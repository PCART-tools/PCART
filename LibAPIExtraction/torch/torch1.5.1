
----------------------------------------/home/zhang/Packages/torch/torch1.5.1/serialization.py----------------------------------------
A:torch.serialization.path->tempfile.mkdtemp()
A:torch.serialization.start->f.tell()
A:torch.serialization.byte->f.read(1)
A:torch.serialization.version_strs->_import_dotted_name(storage_type.__module__).__version__.split('.')
A:torch.serialization.module_version->tuple((type(req_field)(version_strs[idx]) for (idx, req_field) in enumerate(req_version_tuple)))
A:torch.serialization.device->validate_cuda_device(location)
A:torch.serialization.storage_type->normalize_storage_type(type(obj))
A:torch.serialization.location->map_location.get(location, location)
A:torch.serialization.result->pickle_module.Unpickler(data_file, **pickle_load_args).load()
A:torch.serialization.module->_import_dotted_name(storage_type.__module__)
A:torch.serialization.(source_lines, _, source_file)->get_source_lines_and_file(obj)
A:torch.serialization.source->''.join(source_lines)
A:torch.serialization.obj_key->str(obj._cdata)
A:torch.serialization.sys_info->dict(protocol_version=PROTOCOL_VERSION, little_endian=sys.byteorder == 'little', type_sizes=dict(short=SHORT_SIZE, int=INT_SIZE, long=LONG_SIZE))
A:torch.serialization.pickler->pickle_module.Pickler(data_buf, protocol=pickle_protocol)
A:torch.serialization.serialized_storage_keys->sorted(serialized_storages.keys())
A:torch.serialization.data_buf->io.BytesIO()
A:torch.serialization.data_value->io.BytesIO().getvalue()
A:torch.serialization.name->'data/{}'.format(key)
A:torch.serialization.buf->io.BytesIO()
A:torch.serialization.buf_value->io.BytesIO().getvalue()
A:torch.serialization.restore_location->_get_restore_location(map_location)
A:torch.serialization.current_source->''.join(get_source_lines_and_file(container_type)[0])
A:torch.serialization.diff->difflib.unified_diff(current_source.split('\n'), original_source.split('\n'), source_file, source_file, lineterm='')
A:torch.serialization.lines->'\n'.join(diff)
A:torch.serialization.file_size->f.seek(0, 2)
A:torch.serialization.msg->"source code of class '{container_type}' has changed. {msg}".format(container_type=torch.typename(container_type), msg=msg)
A:torch.serialization.num_storages->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.args->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.obj->data_type(size)
A:torch.serialization.storage_views->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.num_tensors->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.tensor_type->storage_to_tensor_type(storage)
A:torch.serialization.(ndim,)->struct.unpack('<i', f.read(4))
A:torch.serialization.size->struct.unpack('<{}q'.format(ndim), f.read(8 * ndim))
A:torch.serialization.stride->struct.unpack('<{}q'.format(ndim), f.read(8 * ndim))
A:torch.serialization.(storage_offset,)->struct.unpack('<q', f.read(8))
A:torch.serialization.tensor->tensor_type().set_(storage, storage_offset, size, stride)
A:torch.serialization.pickle_file->tar.extractfile('pickle')
A:torch.serialization.unpickler->pickle_module.Unpickler(data_file, **pickle_load_args)
A:torch.serialization.typename->_maybe_decode_ascii(saved_id[0])
A:torch.serialization.deserialized_objects[root_key]->restore_location(obj, location)
A:torch.serialization.f_should_read_directly->_should_read_directly(f)
A:torch.serialization.magic_number->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.protocol_version->pickle_module.load(f, **pickle_load_args)
A:torch.serialization._sys_info->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.deserialized_storage_keys->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.offset->f.tell()
A:torch.serialization.loaded_storages[key]->restore_location(obj, location)
A:torch.serialization.size_long->struct.pack('<Q', size)
A:torch.serialization.tensor_file->io.BytesIO(size_long + zip_file.get_record(name))
A:torch.serialization.data_file->io.BytesIO(zip_file.get_record('data.pkl'))
A:torch.serialization.parts->file_name.split(os.sep)
torch.load(f,map_location=None,pickle_module=pickle,**pickle_load_args)
torch.save(obj,f,pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL,_use_new_zipfile_serialization=False)
torch.serialization.SourceChangeWarning(Warning)
torch.serialization._check_dill_version(pickle_module)
torch.serialization._check_seekable(f)
torch.serialization._cpu_deserialize(obj,location)
torch.serialization._cpu_tag(obj)
torch.serialization._cuda_deserialize(obj,location)
torch.serialization._cuda_tag(obj)
torch.serialization._get_layout(name)
torch.serialization._get_restore_location(map_location)
torch.serialization._is_compressed_file(f)
torch.serialization._is_path(name_or_buffer)
torch.serialization._is_torchscript_zip(zip_file)
torch.serialization._is_zipfile(f)
torch.serialization._legacy_load(f,map_location,pickle_module,**pickle_load_args)
torch.serialization._legacy_save(obj,f,pickle_module,pickle_protocol)
torch.serialization._load(zip_file,map_location,pickle_module,**pickle_load_args)
torch.serialization._maybe_decode_ascii(bytes_str)
torch.serialization._open_buffer_reader(self,buffer)
torch.serialization._open_buffer_reader.__init__(self,buffer)
torch.serialization._open_buffer_writer(_opener)
torch.serialization._open_buffer_writer.__exit__(self,*args)
torch.serialization._open_file(self,name,mode)
torch.serialization._open_file.__exit__(self,*args)
torch.serialization._open_file.__init__(self,name,mode)
torch.serialization._open_file_like(name_or_buffer,mode)
torch.serialization._open_zipfile_reader(self,name_or_buffer)
torch.serialization._open_zipfile_reader.__init__(self,name_or_buffer)
torch.serialization._open_zipfile_writer(name_or_buffer)
torch.serialization._open_zipfile_writer_buffer(self,buffer)
torch.serialization._open_zipfile_writer_buffer.__exit__(self,*args)
torch.serialization._open_zipfile_writer_buffer.__init__(self,buffer)
torch.serialization._open_zipfile_writer_file(self,name)
torch.serialization._open_zipfile_writer_file.__exit__(self,*args)
torch.serialization._open_zipfile_writer_file.__init__(self,name)
torch.serialization._opener(self,file_like)
torch.serialization._opener.__enter__(self)
torch.serialization._opener.__exit__(self,*args)
torch.serialization._opener.__init__(self,file_like)
torch.serialization._save(obj,zip_file,pickle_module,pickle_protocol)
torch.serialization._should_read_directly(f)
torch.serialization.check_module_version_greater_or_equal(module,req_version_tuple,error_if_malformed=True)
torch.serialization.default_restore_location(storage,location)
torch.serialization.load(f,map_location=None,pickle_module=pickle,**pickle_load_args)
torch.serialization.location_tag(storage)
torch.serialization.mkdtemp()
torch.serialization.normalize_storage_type(storage_type)
torch.serialization.register_package(priority,tagger,deserializer)
torch.serialization.save(obj,f,pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL,_use_new_zipfile_serialization=False)
torch.serialization.storage_to_tensor_type(storage)
torch.serialization.validate_cuda_device(location)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_utils.py----------------------------------------
A:torch._utils.non_blocking->_get_async_or_non_blocking('cuda', non_blocking, kwargs)
A:torch._utils.dtype->_import_dotted_name(dtype)
A:torch._utils.new_module_name->_import_dotted_name(dtype).__module__.replace('.sparse', '')
A:torch._utils.new_values->torch._values(self).type(new_values_type_name, non_blocking)
A:torch._utils.new_indices->torch._indices(self).type(new_indices_type_name, non_blocking)
A:torch._utils.device->torch.cuda.current_device()
A:torch._utils.new_type->getattr(torch.cuda, self.__class__.__name__)
A:torch._utils.indices->torch._indices(tensor)
A:torch._utils.values->torch._values(tensor)
A:torch._utils.argument->list(kwargs.keys()).pop()
A:torch._utils.t->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype).type()
A:torch._utils.tensor->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype)
A:torch._utils.scales->torch.tensor(scales, dtype=torch.double)
A:torch._utils.zero_points->torch.tensor(zero_points, dtype=torch.long)
A:torch._utils.param->torch.nn.Parameter(data, requires_grad)
A:torch._utils.components->name.split('.')
A:torch._utils.obj->getattr(obj, component)
A:torch._utils.it->iter(iterable)
A:torch._utils.total->fn(total, element)
A:torch._utils.flat->torch.cat([t.contiguous().view(-1) for t in tensors], dim=0)
A:torch._utils.flat_indices->_flatten_dense_tensors([torch._indices(t) for t in tensors])
A:torch._utils.flat_values->_flatten_dense_tensors([torch._values(t) for t in tensors])
A:torch._utils.numel->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype).numel()
A:torch._utils.type_dict->defaultdict(list)
A:torch._utils.buf_dict->defaultdict(lambda : [[], 0])
A:torch._utils.fun.__annotations__->dict(kwargs)
A:torch._utils.exc_info->sys.exc_info()
A:torch._utils.self.exc_msg->''.join(traceback.format_exception(*exc_info))
A:torch._utils.msg->KeyErrorMessage(msg)
torch._import_dotted_name(name)
torch._utils.ExceptionWrapper(self,exc_info=None,where='inbackground')
torch._utils.ExceptionWrapper.__init__(self,exc_info=None,where='inbackground')
torch._utils.ExceptionWrapper.reraise(self)
torch._utils.KeyErrorMessage(str)
torch._utils.KeyErrorMessage.__repr__(self)
torch._utils._accumulate(iterable,fn=lambdax,y:x+y)
torch._utils._cuda(self,device=None,non_blocking=False,**kwargs)
torch._utils._flatten_dense_tensors(tensors)
torch._utils._flatten_sparse_tensors(tensors)
torch._utils._get_async_or_non_blocking(function_name,non_blocking,kwargs)
torch._utils._import_dotted_name(name)
torch._utils._rebuild_parameter(data,requires_grad,backward_hooks)
torch._utils._rebuild_qtensor(storage,storage_offset,size,stride,quantizer_params,requires_grad,backward_hooks)
torch._utils._rebuild_sparse_tensor(layout,data)
torch._utils._rebuild_tensor(storage,storage_offset,size,stride)
torch._utils._rebuild_tensor_v2(storage,storage_offset,size,stride,requires_grad,backward_hooks)
torch._utils._rebuild_xla_tensor(data,dtype,device,requires_grad)
torch._utils._reorder_tensors_as(tensors,ordered_tensors)
torch._utils._take_tensors(tensors,size_limit)
torch._utils._type(self,dtype=None,non_blocking=False,**kwargs)
torch._utils._unflatten_dense_tensors(flat,tensors)
torch._utils._unflatten_sparse_tensors(flat,tensors)
torch._utils.annotate(ret,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/storage.py----------------------------------------
A:torch.storage.memo->memo.setdefault('torch', {}).setdefault('torch', {})
A:torch.storage.new_storage->self.clone()
A:torch.storage.b->io.BytesIO()
A:torch.storage.allocator->torch.cuda._host_allocator()
torch._StorageBase(object)
torch._StorageBase.__copy__(self)
torch._StorageBase.__deepcopy__(self,memo)
torch._StorageBase.__iter__(self)
torch._StorageBase.__reduce__(self)
torch._StorageBase.__repr__(self)
torch._StorageBase.__sizeof__(self)
torch._StorageBase.__str__(self)
torch._StorageBase._new_shared(cls,size)
torch._StorageBase.bfloat16(self)
torch._StorageBase.bool(self)
torch._StorageBase.byte(self)
torch._StorageBase.char(self)
torch._StorageBase.clone(self)
torch._StorageBase.cpu(self)
torch._StorageBase.double(self)
torch._StorageBase.float(self)
torch._StorageBase.half(self)
torch._StorageBase.int(self)
torch._StorageBase.long(self)
torch._StorageBase.pin_memory(self)
torch._StorageBase.share_memory_(self)
torch._StorageBase.short(self)
torch._StorageBase.tolist(self)
torch.storage._StorageBase(object)
torch.storage._StorageBase.__copy__(self)
torch.storage._StorageBase.__deepcopy__(self,memo)
torch.storage._StorageBase.__iter__(self)
torch.storage._StorageBase.__reduce__(self)
torch.storage._StorageBase.__repr__(self)
torch.storage._StorageBase.__sizeof__(self)
torch.storage._StorageBase.__str__(self)
torch.storage._StorageBase._new_shared(cls,size)
torch.storage._StorageBase.bfloat16(self)
torch.storage._StorageBase.bool(self)
torch.storage._StorageBase.byte(self)
torch.storage._StorageBase.char(self)
torch.storage._StorageBase.clone(self)
torch.storage._StorageBase.cpu(self)
torch.storage._StorageBase.double(self)
torch.storage._StorageBase.float(self)
torch.storage._StorageBase.half(self)
torch.storage._StorageBase.int(self)
torch.storage._StorageBase.long(self)
torch.storage._StorageBase.pin_memory(self)
torch.storage._StorageBase.share_memory_(self)
torch.storage._StorageBase.short(self)
torch.storage._StorageBase.tolist(self)
torch.storage._load_from_bytes(b)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/__config__.py----------------------------------------
torch.__config__.parallel_info()
torch.__config__.show()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_ops.py----------------------------------------
A:torch._ops.old_flags->sys.getdlopenflags()
A:torch._ops.qualified_op_name->'{}::{}'.format(self.name, op_name)
A:torch._ops.op->torch._C._jit_get_operation(qualified_op_name)
A:torch._ops.__file__->os.path.join(os.path.dirname(__file__), '_ops.py')
A:torch._ops.self.loaded_libraries->set()
A:torch._ops.namespace->_OpNamespace(name)
A:torch._ops.path->torch._utils_internal.resolve_library_path(path)
A:torch._ops.ops->_Ops()
torch._ops._OpNamespace(self,name)
torch._ops._OpNamespace.__getattr__(self,op_name)
torch._ops._OpNamespace.__init__(self,name)
torch._ops._Ops(self)
torch._ops._Ops.__getattr__(self,name)
torch._ops._Ops.__init__(self)
torch._ops._Ops.load_library(self,path)
torch._ops.dl_open_guard()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_namedtensor_internals.py----------------------------------------
A:torch._namedtensor_internals.namedshape->namedshape.items().items()
A:torch._namedtensor_internals.globbed_names->expand_single_ellipsis(ellipsis_idx, len(names) - ellipsis_idx - 1, tensor_names)
A:torch._namedtensor_internals.ellipsis_idx->single_ellipsis_index(names, fn_name)
A:torch._namedtensor_internals.dim_map->build_dim_map(tensor)
A:torch._namedtensor_internals.has_rename_pairs->bool(rename_map)
torch._namedtensor_internals.build_dim_map(tensor)
torch._namedtensor_internals.check_serializing_named_tensor(tensor)
torch._namedtensor_internals.expand_single_ellipsis(numel_pre_glob,numel_post_glob,names)
torch._namedtensor_internals.is_ellipsis(item)
torch._namedtensor_internals.namer_api_name(inplace)
torch._namedtensor_internals.replace_ellipsis_by_position(ellipsis_idx,names,tensor_names)
torch._namedtensor_internals.resolve_ellipsis(names,tensor_names,fn_name)
torch._namedtensor_internals.single_ellipsis_index(names,fn_name)
torch._namedtensor_internals.unzip_namedshape(namedshape)
torch._namedtensor_internals.update_names(tensor,names,rename_map,inplace)
torch._namedtensor_internals.update_names_with_list(tensor,names,inplace)
torch._namedtensor_internals.update_names_with_mapping(tensor,rename_map,inplace)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_linalg_utils.py----------------------------------------
A:torch._linalg_utils.ndim->len(A.shape)
A:torch._linalg_utils.(Q, _)->torch.qr(A, some=True)
A:torch._linalg_utils.Q->torch.orgqr(*torch.geqrf(A))
A:torch._linalg_utils.(E, Z)->torch.symeig(A, eigenvectors, True)
A:torch._linalg_utils.E->torch.flip(E, dims=(-1,))
A:torch._linalg_utils.Z->torch.flip(Z, dims=(-1,))
torch._linalg_utils.basis(A)
torch._linalg_utils.bform(X,A,Y)
torch._linalg_utils.conjugate(A)
torch._linalg_utils.get_floating_dtype(A)
torch._linalg_utils.is_sparse(A)
torch._linalg_utils.matmul(A,B)
torch._linalg_utils.qform(A,S)
torch._linalg_utils.symeig(A,largest=False,eigenvectors=True)
torch._linalg_utils.transjugate(A)
torch._linalg_utils.transpose(A)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/__init__.py----------------------------------------
A:torch.__init__.is_conda->os.path.exists(os.path.join(sys.prefix, 'conda-meta'))
A:torch.__init__.py_dll_path->os.path.join(sys.exec_prefix, 'Library', 'bin')
A:torch.__init__.th_dll_path->os.path.join(os.path.dirname(__file__), 'lib')
A:torch.__init__.nvtoolsext_dll_path->os.path.join(os.getenv('NVTOOLSEXT_PATH', 'C:\\Program Files\\NVIDIA Corporation\\NvToolsExt'), 'bin', 'x64')
A:torch.__init__.cuda_version_1->version.cuda.replace('.', '_')
A:torch.__init__.cuda_path->os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
A:torch.__init__.dll_paths->list(filter(os.path.exists, [th_dll_path, py_dll_path, nvtoolsext_dll_path, cuda_path]))
A:torch.__init__.os.environ['PATH']->';'.join(dll_paths)
A:torch.__init__.dlls->glob.glob(os.path.join(th_dll_path, '*.dll'))
A:torch.__init__.here->os.path.abspath(__file__)
A:torch.__init__.lib_path->os.path.join(os.path.dirname(here), 'lib', lib_name)
A:torch.__init__.old_flags->sys.getdlopenflags()
A:torch.__init__.t->_import_dotted_name(t)
A:torch.__init__._tensor_classes->set()
A:torch.__init__.path->get_file_path('torch', 'bin', 'torch_shm_manager')
A:torch.__init__.globals()[name]->getattr(_C._VariableFunctions, name)
torch.__init__.BFloat16Storage(_C.BFloat16StorageBase,_StorageBase)
torch.__init__.BoolStorage(_C.BoolStorageBase,_StorageBase)
torch.__init__.ByteStorage(_C.ByteStorageBase,_StorageBase)
torch.__init__.CharStorage(_C.CharStorageBase,_StorageBase)
torch.__init__.DoubleStorage(_C.DoubleStorageBase,_StorageBase)
torch.__init__.FloatStorage(_C.FloatStorageBase,_StorageBase)
torch.__init__.HalfStorage(_C.HalfStorageBase,_StorageBase)
torch.__init__.IntStorage(_C.IntStorageBase,_StorageBase)
torch.__init__.LongStorage(_C.LongStorageBase,_StorageBase)
torch.__init__.QInt32Storage(_C.QInt32StorageBase,_StorageBase)
torch.__init__.QInt8Storage(_C.QInt8StorageBase,_StorageBase)
torch.__init__.QUInt8Storage(_C.QUInt8StorageBase,_StorageBase)
torch.__init__.ShortStorage(_C.ShortStorageBase,_StorageBase)
torch.__init__._load_global_deps()
torch.__init__.compiled_with_cxx11_abi()
torch.__init__.is_storage(obj)
torch.__init__.is_tensor(obj)
torch.__init__.manager_path()
torch.__init__.set_default_dtype(d)
torch.__init__.set_default_tensor_type(t)
torch.__init__.typename(o)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/__init__.pyi----------------------------------------
torch.__init__.BoolTensor(Tensor)
torch.__init__.ByteTensor(Tensor)
torch.__init__.CharTensor(Tensor)
torch.__init__.DoubleTensor(Tensor)
torch.__init__.FloatTensor(Tensor)
torch.__init__.Generator(self,device:Union[_int,str])
torch.__init__.Generator.__init__(self,device:Union[_int,str])
torch.__init__.IntTensor(Tensor)
torch.__init__.LongTensor(Tensor)
torch.__init__.ShortTensor(Tensor)
torch.__init__.Size(Tuple[_int,...])
torch.__init__.Storage
torch.__init__.Tensor
torch.__init__.Tensor.T(self)->Tensor
torch.__init__.Tensor.__abs__(self)->Tensor
torch.__init__.Tensor.__add__(self,other:Any)->Tensor
torch.__init__.Tensor.__and__(self,other:Any)->Tensor
torch.__init__.Tensor.__and__(self,other:Number)->Tensor
torch.__init__.Tensor.__and__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__bool__(self)->builtins.bool
torch.__init__.Tensor.__contains__(self,item:Union[Tensor,Number])->_bool
torch.__init__.Tensor.__div__(self,other:Any)->Tensor
torch.__init__.Tensor.__eq__(self,other:Any)->Tensor
torch.__init__.Tensor.__float__(self)->builtins.float
torch.__init__.Tensor.__floordiv__(self,other:Any)->Tensor
torch.__init__.Tensor.__ge__(self,other:Any)->Tensor
torch.__init__.Tensor.__getitem__(self,indices:Union[None,_int,slice,Tensor,List,Tuple])->Tensor
torch.__init__.Tensor.__gt__(self,other:Any)->Tensor
torch.__init__.Tensor.__iadd__(self,other:Any)->Tensor
torch.__init__.Tensor.__iand__(self,other:Any)->Tensor
torch.__init__.Tensor.__iand__(self,other:Number)->Tensor
torch.__init__.Tensor.__iand__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__idiv__(self,other:Any)->Tensor
torch.__init__.Tensor.__ilshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__ilshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__ilshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__imul__(self,other:Any)->Tensor
torch.__init__.Tensor.__index__(self)->builtins.int
torch.__init__.Tensor.__int__(self)->builtins.int
torch.__init__.Tensor.__invert__(self)->Tensor
torch.__init__.Tensor.__ior__(self,other:Any)->Tensor
torch.__init__.Tensor.__ior__(self,other:Number)->Tensor
torch.__init__.Tensor.__ior__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__irshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__irshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__irshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__isub__(self,other:Any)->Tensor
torch.__init__.Tensor.__iter__(self)->Iterator[Tensor]
torch.__init__.Tensor.__itruediv__(self,other:Any)->Tensor
torch.__init__.Tensor.__ixor__(self,other:Any)->Tensor
torch.__init__.Tensor.__ixor__(self,other:Number)->Tensor
torch.__init__.Tensor.__ixor__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__le__(self,other:Any)->Tensor
torch.__init__.Tensor.__len__(self)->_int
torch.__init__.Tensor.__long__(self)->builtins.int
torch.__init__.Tensor.__lshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__lshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__lshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__lt__(self,other:Any)->Tensor
torch.__init__.Tensor.__matmul__(self,other:Any)->Tensor
torch.__init__.Tensor.__mod__(self,other:Any)->Tensor
torch.__init__.Tensor.__mul__(self,other:Any)->Tensor
torch.__init__.Tensor.__ne__(self,other:Any)->Tensor
torch.__init__.Tensor.__neg__(self)->Tensor
torch.__init__.Tensor.__nonzero__(self)->builtins.bool
torch.__init__.Tensor.__or__(self,other:Any)->Tensor
torch.__init__.Tensor.__or__(self,other:Number)->Tensor
torch.__init__.Tensor.__or__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__pow__(self,other:Any)->Tensor
torch.__init__.Tensor.__radd__(self,other:Any)->Tensor
torch.__init__.Tensor.__rfloordiv__(self,other:Any)->Tensor
torch.__init__.Tensor.__rmul__(self,other:Any)->Tensor
torch.__init__.Tensor.__rpow__(self,other:Any)->Tensor
torch.__init__.Tensor.__rshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__rshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__rshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__rsub__(self,other:Any)->Tensor
torch.__init__.Tensor.__rtruediv__(self,other:Any)->Tensor
torch.__init__.Tensor.__setitem__(self,indices:Union[None,_int,slice,Tensor,List,Tuple],val:Union[Tensor,Number])->None
torch.__init__.Tensor.__sub__(self,other:Any)->Tensor
torch.__init__.Tensor.__truediv__(self,other:Any)->Tensor
torch.__init__.Tensor.__xor__(self,other:Any)->Tensor
torch.__init__.Tensor.__xor__(self,other:Number)->Tensor
torch.__init__.Tensor.__xor__(self,other:Tensor)->Tensor
torch.__init__.Tensor._coalesced_(self,coalesced:_bool)->Tensor
torch.__init__.Tensor._dimI(self)->_int
torch.__init__.Tensor._dimV(self)->_int
torch.__init__.Tensor._indices(self)->Tensor
torch.__init__.Tensor._nnz(self)->_int
torch.__init__.Tensor._values(self)->Tensor
torch.__init__.Tensor.abs(self)->Tensor
torch.__init__.Tensor.abs_(self)->Tensor
torch.__init__.Tensor.acos(self)->Tensor
torch.__init__.Tensor.acos_(self)->Tensor
torch.__init__.Tensor.add(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.add_(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1)->Tensor
torch.__init__.Tensor.addbmm(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addbmm_(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addcdiv(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addcdiv_(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addcmul(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addcmul_(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addmm(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addmm_(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addmv(self,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addmv_(self,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addr(self,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addr_(self,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.align_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.align_to(self,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.align_to(self,order:List[Union[str,None]],ellipsis_idx:_int)->Tensor
torch.__init__.Tensor.all(self)->Tensor
torch.__init__.Tensor.all(self,dim:Union[str,None],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.all(self,dim:_int,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.allclose(self,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->_bool
torch.__init__.Tensor.angle(self)->Tensor
torch.__init__.Tensor.any(self)->Tensor
torch.__init__.Tensor.any(self,dim:Union[str,None],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.any(self,dim:_int,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.apply_(self,callable:Callable)->Tensor
torch.__init__.Tensor.argmax(self,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.argmin(self,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.argsort(self,dim:Union[str,None],descending:_bool=False)->Tensor
torch.__init__.Tensor.argsort(self,dim:_int=-1,descending:_bool=False)->Tensor
torch.__init__.Tensor.as_strided(self,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.Tensor.as_strided_(self,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.Tensor.asin(self)->Tensor
torch.__init__.Tensor.asin_(self)->Tensor
torch.__init__.Tensor.atan(self)->Tensor
torch.__init__.Tensor.atan2(self,other:Tensor)->Tensor
torch.__init__.Tensor.atan2_(self,other:Tensor)->Tensor
torch.__init__.Tensor.atan_(self)->Tensor
torch.__init__.Tensor.backward(self,gradient:Optional[Tensor]=None,keep_graph:_bool=False,create_graph:_bool=False)->None
torch.__init__.Tensor.baddbmm(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.baddbmm_(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.bernoulli(self,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bernoulli(self,p:_float,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bernoulli_(self,p:Tensor,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bernoulli_(self,p:_float=0.5,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bfloat16(self)->Tensor
torch.__init__.Tensor.bincount(self,weights:Optional[Tensor]=None,minlength:_int=0)->Tensor
torch.__init__.Tensor.bitwise_and(self,other:Number)->Tensor
torch.__init__.Tensor.bitwise_and(self,other:Tensor)->Tensor
torch.__init__.Tensor.bitwise_and_(self,other:Number)->Tensor
torch.__init__.Tensor.bitwise_and_(self,other:Tensor)->Tensor
torch.__init__.Tensor.bitwise_not(self)->Tensor
torch.__init__.Tensor.bitwise_not_(self)->Tensor
torch.__init__.Tensor.bitwise_or(self,other:Number)->Tensor
torch.__init__.Tensor.bitwise_or(self,other:Tensor)->Tensor
torch.__init__.Tensor.bitwise_or_(self,other:Number)->Tensor
torch.__init__.Tensor.bitwise_or_(self,other:Tensor)->Tensor
torch.__init__.Tensor.bitwise_xor(self,other:Number)->Tensor
torch.__init__.Tensor.bitwise_xor(self,other:Tensor)->Tensor
torch.__init__.Tensor.bitwise_xor_(self,other:Number)->Tensor
torch.__init__.Tensor.bitwise_xor_(self,other:Tensor)->Tensor
torch.__init__.Tensor.bmm(self,mat2:Tensor)->Tensor
torch.__init__.Tensor.bool(self)->Tensor
torch.__init__.Tensor.byte(self)->Tensor
torch.__init__.Tensor.cauchy_(self,median:_float=0,sigma:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.ceil(self)->Tensor
torch.__init__.Tensor.ceil_(self)->Tensor
torch.__init__.Tensor.char(self)->Tensor
torch.__init__.Tensor.cholesky(self,upper:_bool=False)->Tensor
torch.__init__.Tensor.cholesky_inverse(self,upper:_bool=False)->Tensor
torch.__init__.Tensor.cholesky_solve(self,input2:Tensor,upper:_bool=False)->Tensor
torch.__init__.Tensor.chunk(self,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.clamp(self,min:_float=-inf,max:_float=inf,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.clamp_(self,min:_float=-inf,max:_float=inf)->Tensor
torch.__init__.Tensor.clamp_max(self,max:Number)->Tensor
torch.__init__.Tensor.clamp_max_(self,max:Number)->Tensor
torch.__init__.Tensor.clamp_min(self,min:Number)->Tensor
torch.__init__.Tensor.clamp_min_(self,min:Number)->Tensor
torch.__init__.Tensor.clone(self,*,memory_format:Optional[memory_format]=None)->Tensor
torch.__init__.Tensor.coalesce(self)->Tensor
torch.__init__.Tensor.conj(self)->Tensor
torch.__init__.Tensor.contiguous(self)->Tensor
torch.__init__.Tensor.cos(self)->Tensor
torch.__init__.Tensor.cos_(self)->Tensor
torch.__init__.Tensor.cosh(self)->Tensor
torch.__init__.Tensor.cosh_(self)->Tensor
torch.__init__.Tensor.cpu(self)->Tensor
torch.__init__.Tensor.cross(self,other:Tensor,dim:Optional[_int]=None)->Tensor
torch.__init__.Tensor.cuda(self,device:Optional[Union[_device,_int,str]]=None,non_blocking:_bool=False)->Tensor
torch.__init__.Tensor.cummax(self,dim:Union[str,None])->Tuple[Tensor, Tensor]
torch.__init__.Tensor.cummax(self,dim:_int)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.cummin(self,dim:Union[str,None])->Tuple[Tensor, Tensor]
torch.__init__.Tensor.cummin(self,dim:_int)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.cumprod(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.cumprod(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.cumsum(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.cumsum(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.dense_dim(self)->_int
torch.__init__.Tensor.dequantize(self)->Tensor
torch.__init__.Tensor.det(self)->Tensor
torch.__init__.Tensor.detach(self)->Tensor
torch.__init__.Tensor.detach_(self)->Tensor
torch.__init__.Tensor.device(self)->_device
torch.__init__.Tensor.diag(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.diag_embed(self,offset:_int=0,dim1:_int=-2,dim2:_int=-1)->Tensor
torch.__init__.Tensor.diagflat(self,offset:_int=0)->Tensor
torch.__init__.Tensor.diagonal(self,*,outdim:Union[str,None],dim1:Union[str,None],dim2:Union[str,None],offset:_int=0)->Tensor
torch.__init__.Tensor.diagonal(self,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch.__init__.Tensor.digamma(self)->Tensor
torch.__init__.Tensor.digamma_(self)->Tensor
torch.__init__.Tensor.dim(self)->_int
torch.__init__.Tensor.dist(self,other:Tensor,p:Number=2)->Tensor
torch.__init__.Tensor.div(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.div_(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.dot(self,tensor:Tensor)->Tensor
torch.__init__.Tensor.double(self)->Tensor
torch.__init__.Tensor.dtype(self)->_dtype
torch.__init__.Tensor.eig(self,eigenvectors:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.element_size(self)->_int
torch.__init__.Tensor.eq(self,other:Number)->Tensor
torch.__init__.Tensor.eq(self,other:Tensor)->Tensor
torch.__init__.Tensor.eq_(self,other:Number)->Tensor
torch.__init__.Tensor.eq_(self,other:Tensor)->Tensor
torch.__init__.Tensor.equal(self,other:Tensor)->_bool
torch.__init__.Tensor.erf(self)->Tensor
torch.__init__.Tensor.erf_(self)->Tensor
torch.__init__.Tensor.erfc(self)->Tensor
torch.__init__.Tensor.erfc_(self)->Tensor
torch.__init__.Tensor.erfinv(self)->Tensor
torch.__init__.Tensor.erfinv_(self)->Tensor
torch.__init__.Tensor.exp(self)->Tensor
torch.__init__.Tensor.exp_(self)->Tensor
torch.__init__.Tensor.expand(self,*size:_int,implicit:_bool=False)->Tensor
torch.__init__.Tensor.expand(self,size:_size,*,implicit:_bool=False)->Tensor
torch.__init__.Tensor.expand_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.expm1(self)->Tensor
torch.__init__.Tensor.expm1_(self)->Tensor
torch.__init__.Tensor.exponential_(self,lambd:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.fft(self,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.Tensor.fill_(self,value:Number)->Tensor
torch.__init__.Tensor.fill_(self,value:Tensor)->Tensor
torch.__init__.Tensor.fill_diagonal_(self,fill_value:Number,wrap:_bool=False)->Tensor
torch.__init__.Tensor.flatten(self,dims:List[Union[str,None]],out_dim:Union[str,None])->Tensor
torch.__init__.Tensor.flatten(self,start_dim:Union[str,None],end_dim:Union[str,None],out_dim:Union[str,None])->Tensor
torch.__init__.Tensor.flatten(self,start_dim:_int,end_dim:_int,out_dim:Union[str,None])->Tensor
torch.__init__.Tensor.flatten(self,start_dim:_int=0,end_dim:_int=-1)->Tensor
torch.__init__.Tensor.flip(self,*dims:_int)->Tensor
torch.__init__.Tensor.flip(self,dims:_size)->Tensor
torch.__init__.Tensor.float(self)->Tensor
torch.__init__.Tensor.floor(self)->Tensor
torch.__init__.Tensor.floor_(self)->Tensor
torch.__init__.Tensor.floor_divide(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.floor_divide_(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.fmod(self,other:Number)->Tensor
torch.__init__.Tensor.fmod(self,other:Tensor)->Tensor
torch.__init__.Tensor.fmod_(self,other:Number)->Tensor
torch.__init__.Tensor.fmod_(self,other:Tensor)->Tensor
torch.__init__.Tensor.frac(self)->Tensor
torch.__init__.Tensor.frac_(self)->Tensor
torch.__init__.Tensor.gather(self,dim:Union[str,None],index:Tensor,*,sparse_grad:_bool=False)->Tensor
torch.__init__.Tensor.gather(self,dim:_int,index:Tensor,*,sparse_grad:_bool=False)->Tensor
torch.__init__.Tensor.ge(self,other:Number)->Tensor
torch.__init__.Tensor.ge(self,other:Tensor)->Tensor
torch.__init__.Tensor.ge_(self,other:Number)->Tensor
torch.__init__.Tensor.ge_(self,other:Tensor)->Tensor
torch.__init__.Tensor.geometric_(self,p:_float,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.geqrf(self)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.ger(self,vec2:Tensor)->Tensor
torch.__init__.Tensor.get_device(self)->_int
torch.__init__.Tensor.grad_fn(self)->Optional[Any]
torch.__init__.Tensor.gt(self,other:Number)->Tensor
torch.__init__.Tensor.gt(self,other:Tensor)->Tensor
torch.__init__.Tensor.gt_(self,other:Number)->Tensor
torch.__init__.Tensor.gt_(self,other:Tensor)->Tensor
torch.__init__.Tensor.half(self)->Tensor
torch.__init__.Tensor.hardshrink(self,lambd:Number=0.5)->Tensor
torch.__init__.Tensor.histc(self,bins:_int=100,min:Number=0,max:Number=0)->Tensor
torch.__init__.Tensor.ifft(self,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.Tensor.index_add(self,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_add(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_add_(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy(self,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy_(self,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy_(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_fill(self,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill(self,dim:Union[str,None],index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_fill(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill(self,dim:_int,index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:Union[str,None],index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:_int,index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_put(self,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.Tensor.index_put_(self,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.Tensor.index_select(self,dim:Union[str,None],index:Tensor)->Tensor
torch.__init__.Tensor.index_select(self,dim:_int,index:Tensor)->Tensor
torch.__init__.Tensor.indices(self)->Tensor
torch.__init__.Tensor.int(self)->Tensor
torch.__init__.Tensor.int_repr(self)->Tensor
torch.__init__.Tensor.inverse(self)->Tensor
torch.__init__.Tensor.irfft(self,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True,signal_sizes:_size=())->Tensor
torch.__init__.Tensor.is_coalesced(self)->_bool
torch.__init__.Tensor.is_complex(self)->_bool
torch.__init__.Tensor.is_contiguous(self)->_bool
torch.__init__.Tensor.is_distributed(self)->_bool
torch.__init__.Tensor.is_floating_point(self)->_bool
torch.__init__.Tensor.is_nonzero(self)->_bool
torch.__init__.Tensor.is_pinned(self)->_bool
torch.__init__.Tensor.is_same_size(self,other:Tensor)->_bool
torch.__init__.Tensor.is_set_to(self,tensor:Tensor)->_bool
torch.__init__.Tensor.is_shared(self)->_bool
torch.__init__.Tensor.is_signed(self)->_bool
torch.__init__.Tensor.isclose(self,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->Tensor
torch.__init__.Tensor.item(self)->Number
torch.__init__.Tensor.kthvalue(self,k:_int,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.kthvalue(self,k:_int,dim:_int=-1,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.layout(self)->_layout
torch.__init__.Tensor.le(self,other:Number)->Tensor
torch.__init__.Tensor.le(self,other:Tensor)->Tensor
torch.__init__.Tensor.le_(self,other:Number)->Tensor
torch.__init__.Tensor.le_(self,other:Tensor)->Tensor
torch.__init__.Tensor.lerp(self,end:Tensor,weight:Number)->Tensor
torch.__init__.Tensor.lerp(self,end:Tensor,weight:Tensor)->Tensor
torch.__init__.Tensor.lerp_(self,end:Tensor,weight:Number)->Tensor
torch.__init__.Tensor.lerp_(self,end:Tensor,weight:Tensor)->Tensor
torch.__init__.Tensor.lgamma(self)->Tensor
torch.__init__.Tensor.lgamma_(self)->Tensor
torch.__init__.Tensor.log(self)->Tensor
torch.__init__.Tensor.log10(self)->Tensor
torch.__init__.Tensor.log10_(self)->Tensor
torch.__init__.Tensor.log1p(self)->Tensor
torch.__init__.Tensor.log1p_(self)->Tensor
torch.__init__.Tensor.log2(self)->Tensor
torch.__init__.Tensor.log2_(self)->Tensor
torch.__init__.Tensor.log_(self)->Tensor
torch.__init__.Tensor.log_normal_(self,mean:_float=1,std:_float=2,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.log_softmax(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.log_softmax(self,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.logdet(self)->Tensor
torch.__init__.Tensor.logical_and(self,other:Tensor)->Tensor
torch.__init__.Tensor.logical_and_(self,other:Tensor)->Tensor
torch.__init__.Tensor.logical_not(self)->Tensor
torch.__init__.Tensor.logical_not_(self)->Tensor
torch.__init__.Tensor.logical_or(self,other:Tensor)->Tensor
torch.__init__.Tensor.logical_or_(self,other:Tensor)->Tensor
torch.__init__.Tensor.logical_xor(self,other:Tensor)->Tensor
torch.__init__.Tensor.logical_xor_(self,other:Tensor)->Tensor
torch.__init__.Tensor.logsumexp(self,dim:List[Union[str,None]],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.logsumexp(self,dim:Union[_int,_size],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.long(self)->Tensor
torch.__init__.Tensor.lstsq(self,A:Tensor)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.lt(self,other:Number)->Tensor
torch.__init__.Tensor.lt(self,other:Tensor)->Tensor
torch.__init__.Tensor.lt_(self,other:Number)->Tensor
torch.__init__.Tensor.lt_(self,other:Tensor)->Tensor
torch.__init__.Tensor.lu(self,pivot=True,get_infos=False)
torch.__init__.Tensor.lu_solve(self,LU_data:Tensor,LU_pivots:Tensor)->Tensor
torch.__init__.Tensor.map_(self,tensor:Tensor,callable:Callable)->Tensor
torch.__init__.Tensor.masked_fill(self,mask:Tensor,value:Number)->Tensor
torch.__init__.Tensor.masked_fill(self,mask:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.masked_fill_(self,mask:Tensor,value:Number)->Tensor
torch.__init__.Tensor.masked_fill_(self,mask:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.masked_scatter(self,mask:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.masked_scatter_(self,mask:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.masked_select(self,mask:Tensor)->Tensor
torch.__init__.Tensor.matmul(self,other:Tensor)->Tensor
torch.__init__.Tensor.matrix_power(self,n:_int)->Tensor
torch.__init__.Tensor.max(self)->Tensor
torch.__init__.Tensor.max(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.max(self,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.max(self,other:Tensor)->Tensor
torch.__init__.Tensor.mean(self,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.mean(self,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.mean(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.median(self)->Tensor
torch.__init__.Tensor.median(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.median(self,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.min(self)->Tensor
torch.__init__.Tensor.min(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.min(self,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.min(self,other:Tensor)->Tensor
torch.__init__.Tensor.mm(self,mat2:Tensor)->Tensor
torch.__init__.Tensor.mode(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.mode(self,dim:_int=-1,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.mul(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.mul_(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.multinomial(self,num_samples:_int,replacement:_bool=False,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.mv(self,vec:Tensor)->Tensor
torch.__init__.Tensor.mvlgamma(self,p:_int)->Tensor
torch.__init__.Tensor.mvlgamma_(self,p:_int)->Tensor
torch.__init__.Tensor.narrow(self,dim:_int,start:Tensor,length:_int)->Tensor
torch.__init__.Tensor.narrow(self,dim:_int,start:_int,length:_int)->Tensor
torch.__init__.Tensor.narrow_copy(self,dim:_int,start:_int,length:_int)->Tensor
torch.__init__.Tensor.ndim(self)->_int
torch.__init__.Tensor.ndimension(self)->_int
torch.__init__.Tensor.ne(self,other:Number)->Tensor
torch.__init__.Tensor.ne(self,other:Tensor)->Tensor
torch.__init__.Tensor.ne_(self,other:Number)->Tensor
torch.__init__.Tensor.ne_(self,other:Tensor)->Tensor
torch.__init__.Tensor.neg(self)->Tensor
torch.__init__.Tensor.neg_(self)->Tensor
torch.__init__.Tensor.nelement(self)->_int
torch.__init__.Tensor.new_empty(self,*size:_int,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_empty(self,size:_size,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_full(self,size:_size,fill_value:Number,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_ones(self,size:_size,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_tensor(self,data:Any,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_zeros(self,*size:_int,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_zeros(self,size:_size,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.nonzero(self,*,as_tuple=True)
torch.__init__.Tensor.norm(self,p='fro',dim=None,keepdim=False)
torch.__init__.Tensor.normal_(self,mean:_float=0,std:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.numel(self)->_int
torch.__init__.Tensor.numpy(self)->Any
torch.__init__.Tensor.orgqr(self,input2:Tensor)->Tensor
torch.__init__.Tensor.ormqr(self,input2:Tensor,input3:Tensor,left:_bool=True,transpose:_bool=False)->Tensor
torch.__init__.Tensor.permute(self,*dims:_int)->Tensor
torch.__init__.Tensor.permute(self,dims:_size)->Tensor
torch.__init__.Tensor.pin_memory(self)->Tensor
torch.__init__.Tensor.pinverse(self,rcond:_float=1e-15)->Tensor
torch.__init__.Tensor.polygamma(self,n:_int)->Tensor
torch.__init__.Tensor.polygamma_(self,n:_int)->Tensor
torch.__init__.Tensor.pow(self,exponent:Number)->Tensor
torch.__init__.Tensor.pow(self,exponent:Tensor)->Tensor
torch.__init__.Tensor.pow_(self,exponent:Number)->Tensor
torch.__init__.Tensor.pow_(self,exponent:Tensor)->Tensor
torch.__init__.Tensor.prelu(self,weight:Tensor)->Tensor
torch.__init__.Tensor.prod(self,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.prod(self,dim:Union[str,None],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.prod(self,dim:_int,keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.put_(self,index:Tensor,source:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.Tensor.q_per_channel_axis(self)->_int
torch.__init__.Tensor.q_per_channel_scales(self)->Tensor
torch.__init__.Tensor.q_per_channel_zero_points(self)->Tensor
torch.__init__.Tensor.q_scale(self)->_float
torch.__init__.Tensor.q_zero_point(self)->_int
torch.__init__.Tensor.qr(self,some:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.qscheme(self)->_qscheme
torch.__init__.Tensor.random_(self,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.random_(self,from_:_int,to:Optional[_int],*,generator:Generator=None)->Tensor
torch.__init__.Tensor.random_(self,to:_int,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.reciprocal(self)->Tensor
torch.__init__.Tensor.reciprocal_(self)->Tensor
torch.__init__.Tensor.refine_names(self,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.register_hook(self,hook:Callable)->Any
torch.__init__.Tensor.relu(self)->Tensor
torch.__init__.Tensor.relu_(self)->Tensor
torch.__init__.Tensor.remainder(self,other:Number)->Tensor
torch.__init__.Tensor.remainder(self,other:Tensor)->Tensor
torch.__init__.Tensor.remainder_(self,other:Number)->Tensor
torch.__init__.Tensor.remainder_(self,other:Tensor)->Tensor
torch.__init__.Tensor.rename(self,names:Optional[List[Union[str,None]]])->Tensor
torch.__init__.Tensor.rename_(self,names:Optional[List[Union[str,None]]])->Tensor
torch.__init__.Tensor.renorm(self,p:Number,dim:_int,maxnorm:Number)->Tensor
torch.__init__.Tensor.renorm_(self,p:Number,dim:_int,maxnorm:Number)->Tensor
torch.__init__.Tensor.repeat(self,*repeats:_int)->Tensor
torch.__init__.Tensor.repeat(self,repeats:_size)->Tensor
torch.__init__.Tensor.repeat_interleave(self,repeats:Tensor,dim:Optional[_int]=None)->Tensor
torch.__init__.Tensor.repeat_interleave(self,repeats:_int,dim:Optional[_int]=None)->Tensor
torch.__init__.Tensor.requires_grad_(self,mode:_bool=True)->Tensor
torch.__init__.Tensor.reshape(self,*shape:_int)->Tensor
torch.__init__.Tensor.reshape(self,shape:_size)->Tensor
torch.__init__.Tensor.reshape_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.resize_(self,*size:_int,memory_format:Optional[memory_format]=None)->Tensor
torch.__init__.Tensor.resize_(self,size:_size,*,memory_format:Optional[memory_format]=None)->Tensor
torch.__init__.Tensor.resize_as_(self,the_template:Tensor,*,memory_format:Optional[memory_format]=None)->Tensor
torch.__init__.Tensor.retain_grad(self)->None
torch.__init__.Tensor.rfft(self,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True)->Tensor
torch.__init__.Tensor.roll(self,shifts:Union[_int,_size],dims:Union[_int,_size]=())->Tensor
torch.__init__.Tensor.rot90(self,k:_int=1,dims:_size=(0,1))->Tensor
torch.__init__.Tensor.round(self)->Tensor
torch.__init__.Tensor.round_(self)->Tensor
torch.__init__.Tensor.rsqrt(self)->Tensor
torch.__init__.Tensor.rsqrt_(self)->Tensor
torch.__init__.Tensor.scatter(self,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter(self,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.scatter(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.scatter_(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter_(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.scatter_add(self,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter_add(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter_add_(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.select(self,dim:Union[str,None],index:_int)->Tensor
torch.__init__.Tensor.select(self,dim:_int,index:_int)->Tensor
torch.__init__.Tensor.set_(self)->Tensor
torch.__init__.Tensor.set_(self,source:Storage)->Tensor
torch.__init__.Tensor.set_(self,source:Storage,storage_offset:_int,size:_size,stride:_size=())->Tensor
torch.__init__.Tensor.set_(self,source:Tensor)->Tensor
torch.__init__.Tensor.shape(self)->Size
torch.__init__.Tensor.share_memory_(self)->None
torch.__init__.Tensor.short(self)->Tensor
torch.__init__.Tensor.sigmoid(self)->Tensor
torch.__init__.Tensor.sigmoid_(self)->Tensor
torch.__init__.Tensor.sign(self)->Tensor
torch.__init__.Tensor.sign_(self)->Tensor
torch.__init__.Tensor.sin(self)->Tensor
torch.__init__.Tensor.sin_(self)->Tensor
torch.__init__.Tensor.sinh(self)->Tensor
torch.__init__.Tensor.sinh_(self)->Tensor
torch.__init__.Tensor.size(self)->Size
torch.__init__.Tensor.size(self,_int)->_int
torch.__init__.Tensor.slogdet(self)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.smm(self,mat2:Tensor)->Tensor
torch.__init__.Tensor.softmax(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.softmax(self,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.solve(self,A:Tensor)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.sort(self,dim:Union[str,None],descending:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.sort(self,dim:_int=-1,descending:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.sparse_dim(self)->_int
torch.__init__.Tensor.sparse_mask(self,mask:Tensor)->Tensor
torch.__init__.Tensor.sparse_resize_(self,size:_size,sparse_dim:_int,dense_dim:_int)->Tensor
torch.__init__.Tensor.sparse_resize_and_clear_(self,size:_size,sparse_dim:_int,dense_dim:_int)->Tensor
torch.__init__.Tensor.split(self,split_size,dim=0)
torch.__init__.Tensor.split_with_sizes(self,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.sqrt(self)->Tensor
torch.__init__.Tensor.sqrt_(self)->Tensor
torch.__init__.Tensor.square(self)->Tensor
torch.__init__.Tensor.square_(self)->Tensor
torch.__init__.Tensor.squeeze(self)->Tensor
torch.__init__.Tensor.squeeze(self,dim:Union[str,None])->Tensor
torch.__init__.Tensor.squeeze(self,dim:_int)->Tensor
torch.__init__.Tensor.squeeze_(self)->Tensor
torch.__init__.Tensor.squeeze_(self,dim:Union[str,None])->Tensor
torch.__init__.Tensor.squeeze_(self,dim:_int)->Tensor
torch.__init__.Tensor.sspaddmm(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.std(self,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.std(self,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.std(self,unbiased:_bool=True)->Tensor
torch.__init__.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.__init__.Tensor.storage(self)->Storage
torch.__init__.Tensor.storage_offset(self)->_int
torch.__init__.Tensor.stride(self)->Tuple[_int]
torch.__init__.Tensor.stride(self,_int)->_int
torch.__init__.Tensor.sub(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.sub_(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1)->Tensor
torch.__init__.Tensor.sum(self,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.sum(self,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.sum(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.sum_to_size(self,*size:_int)->Tensor
torch.__init__.Tensor.sum_to_size(self,size:_size)->Tensor
torch.__init__.Tensor.svd(self,some:_bool=True,compute_uv:_bool=True)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.Tensor.symeig(self,eigenvectors:_bool=False,upper:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.t(self)->Tensor
torch.__init__.Tensor.t_(self)->Tensor
torch.__init__.Tensor.take(self,index:Tensor)->Tensor
torch.__init__.Tensor.tan(self)->Tensor
torch.__init__.Tensor.tan_(self)->Tensor
torch.__init__.Tensor.tanh(self)->Tensor
torch.__init__.Tensor.tanh_(self)->Tensor
torch.__init__.Tensor.to(self,device:Optional[Union[_device,str]]=None,dtype:Optional[_dtype]=None,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch.__init__.Tensor.to(self,dtype:_dtype,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch.__init__.Tensor.to(self,other:Tensor,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch.__init__.Tensor.to_dense(self)->Tensor
torch.__init__.Tensor.to_mkldnn(self)->Tensor
torch.__init__.Tensor.to_sparse(self)->Tensor
torch.__init__.Tensor.to_sparse(self,sparse_dim:_int)->Tensor
torch.__init__.Tensor.tolist(self)->List
torch.__init__.Tensor.topk(self,k:_int,dim:_int=-1,largest:_bool=True,sorted:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.trace(self)->Tensor
torch.__init__.Tensor.transpose(self,dim0:Union[str,None],dim1:Union[str,None])->Tensor
torch.__init__.Tensor.transpose(self,dim0:_int,dim1:_int)->Tensor
torch.__init__.Tensor.transpose_(self,dim0:_int,dim1:_int)->Tensor
torch.__init__.Tensor.triangular_solve(self,A:Tensor,upper:_bool=True,transpose:_bool=False,unitriangular:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.tril(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.tril_(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.triu(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.triu_(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.true_divide(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.true_divide_(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.trunc(self)->Tensor
torch.__init__.Tensor.trunc_(self)->Tensor
torch.__init__.Tensor.type(self,dtype:None=None,non_blocking:_bool=False)->str
torch.__init__.Tensor.type(self,dtype:Union[str,_dtype],non_blocking:_bool=False)->Tensor
torch.__init__.Tensor.type_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.unbind(self,dim:Union[str,None])->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.unbind(self,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.unflatten(self,dim:Union[str,None],sizes:_size,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.unflatten(self,dim:_int,sizes:_size,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.unfold(self,dimension:_int,size:_int,step:_int)->Tensor
torch.__init__.Tensor.uniform_(self,from_:_float=0,to:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.unique(self,sorted=True,return_inverse=False,dim=None)
torch.__init__.Tensor.unique_consecutive(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.__init__.Tensor.unsqueeze(self,dim:_int)->Tensor
torch.__init__.Tensor.unsqueeze_(self,dim:_int)->Tensor
torch.__init__.Tensor.values(self)->Tensor
torch.__init__.Tensor.var(self,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.var(self,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.var(self,unbiased:_bool=True)->Tensor
torch.__init__.Tensor.view(self,*size:_int)->Tensor
torch.__init__.Tensor.view(self,size:_size)->Tensor
torch.__init__.Tensor.view_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.where(self,condition:Tensor,other:Tensor)->Tensor
torch.__init__.Tensor.zero_(self)->Tensor
torch.__init__.__and__(self:Tensor,other:Number)->Tensor
torch.__init__.__and__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__lshift__(self:Tensor,other:Number)->Tensor
torch.__init__.__lshift__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__or__(self:Tensor,other:Number)->Tensor
torch.__init__.__or__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__rshift__(self:Tensor,other:Number)->Tensor
torch.__init__.__rshift__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__xor__(self:Tensor,other:Number)->Tensor
torch.__init__.__xor__(self:Tensor,other:Tensor)->Tensor
torch.__init__._adaptive_avg_pool2d(self:Tensor,output_size:Union[_int,_size])->Tensor
torch.__init__._addr(self:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__._addr_(self:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__._amp_non_finite_check_and_unscale_(self:Tensor,found_inf:Tensor,inv_scale:Tensor)->None
torch.__init__._amp_update_scale(growth_tracker:Tensor,current_scale:Tensor,found_inf:Tensor,scale_growth_factor:_float,scale_backoff_factor:_float,growth_interval:_int)->Tensor
torch.__init__._baddbmm_mkl_(self:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__._batch_norm_impl_index(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tuple[Tensor, Tensor, Tensor, Tensor, _int]
torch.__init__._cast_Byte(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Char(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Double(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Float(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Half(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Int(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Long(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Short(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__._convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int,benchmark:_bool,deterministic:_bool,cudnn_enabled:_bool)->Tensor
torch.__init__._convolution_nogroup(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size)->Tensor
torch.__init__._copy_from(self:Tensor,dst:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int=0,zero_infinity:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__._cudnn_ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int,deterministic:_bool,zero_infinity:_bool)->Tuple[Tensor, Tensor]
torch.__init__._cudnn_init_dropout_state(dropout:_float,train:_bool,dropout_seed:_int,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._cudnn_rnn(input:Tensor,weight:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,weight_buf:Optional[Tensor],hx:Tensor,cx:Optional[Tensor],mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,dropout:_float,train:_bool,bidirectional:_bool,batch_sizes:_size,dropout_state:Optional[Tensor])->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]
torch.__init__._cudnn_rnn_flatten_weight(weight_arr:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,input_size:_int,mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,bidirectional:_bool)->Tensor
torch.__init__._cufft_clear_plan_cache(device_index:_int)->None
torch.__init__._cufft_get_plan_cache_max_size(device_index:_int)->_int
torch.__init__._cufft_get_plan_cache_size(device_index:_int)->_int
torch.__init__._cufft_set_plan_cache_max_size(device_index:_int,max_size:_int)->None
torch.__init__._cummax_helper(self:Tensor,values:Tensor,indices:Tensor,dim:_int)->None
torch.__init__._cummin_helper(self:Tensor,values:Tensor,indices:Tensor,dim:_int)->None
torch.__init__._debug_has_internal_overlap(self:Tensor)->_int
torch.__init__._dim_arange(like:Tensor,dim:_int)->Tensor
torch.__init__._dirichlet_grad(x:Tensor,alpha:Tensor,total:Tensor)->Tensor
torch.__init__._embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None,include_last_offset:_bool=False)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__._empty_affine_quantized(*size:_int,scale:_float=1,zero_point:_int=0,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._empty_affine_quantized(size:_size,*,scale:_float=1,zero_point:_int=0,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._empty_per_channel_affine_quantized(*size:_int,scales:Tensor,zero_points:Tensor,axis:_int,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._empty_per_channel_affine_quantized(size:_size,*,scales:Tensor,zero_points:Tensor,axis:_int,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._fft_with_size(self:Tensor,signal_ndim:_int,complex_input:_bool,complex_output:_bool,inverse:_bool,checked_signal_sizes:_size,normalized:_bool,onesided:_bool,output_sizes:_size)->Tensor
torch.__init__._fused_dropout(self:Tensor,p:_float,generator:Generator=None)->Tuple[Tensor, Tensor]
torch.__init__._has_compatible_shallow_copy_type(self:Tensor,from_:Tensor)->_bool
torch.__init__._index_copy_(self:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__._index_put_impl_(self:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False,unsafe:_bool=False)->Tensor
torch.__init__._log_softmax(self:Tensor,dim:_int,half_to_float:_bool)->Tensor
torch.__init__._log_softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,self:Tensor)->Tensor
torch.__init__._lu_solve_helper(self:Tensor,LU_data:Tensor,LU_pivots:Tensor)->Tensor
torch.__init__._lu_with_info(self:Tensor,pivot:_bool=True,check_errors:_bool=True)->Tuple[Tensor, Tensor, Tensor]
torch.__init__._make_per_channel_quantized_tensor(self:Tensor,scale:Tensor,zero_point:Tensor,axis:_int)->Tensor
torch.__init__._make_per_tensor_quantized_tensor(self:Tensor,scale:_float,zero_point:_int)->Tensor
torch.__init__._masked_scale(self:Tensor,mask:Tensor,scale:_float)->Tensor
torch.__init__._max(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__._min(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__._mkldnn_reshape(self:Tensor,shape:_size)->Tensor
torch.__init__._mkldnn_transpose(self:Tensor,dim0:_int,dim1:_int)->Tensor
torch.__init__._mkldnn_transpose_(self:Tensor,dim0:_int,dim1:_int)->Tensor
torch.__init__._mode(self:Tensor,dim:_int=-1,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__._multinomial_alias_draw(J:Tensor,q:Tensor,num_samples:_int,*,generator:Generator=None)->Tensor
torch.__init__._multinomial_alias_setup(probs:Tensor)->Tuple[Tensor, Tensor]
torch.__init__._nnpack_available()->_bool
torch.__init__._nnpack_spatial_convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:Union[_int,_size],stride:Union[_int,_size]=1)->Tensor
torch.__init__._pack_padded_sequence(input:Tensor,lengths:Tensor,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__._pad_packed_sequence(data:Tensor,batch_sizes:Tensor,batch_first:_bool,padding_value:Number,total_length:_int)->Tuple[Tensor, Tensor]
torch.__init__._reshape_from_tensor(self:Tensor,shape:Tensor)->Tensor
torch.__init__._s_where(condition:Tensor,self:Tensor,other:Tensor)->Tensor
torch.__init__._sample_dirichlet(self:Tensor,generator:Generator=None)->Tensor
torch.__init__._shape_as_tensor(self:Tensor)->Tensor
torch.__init__._sobol_engine_draw(quasi:Tensor,n:_int,sobolstate:Tensor,dimension:_int,num_generated:_int,dtype:Optional[_dtype])->Tuple[Tensor, Tensor]
torch.__init__._sobol_engine_ff_(self:Tensor,n:_int,sobolstate:Tensor,dimension:_int,num_generated:_int)->Tensor
torch.__init__._sobol_engine_initialize_state_(self:Tensor,dimension:_int)->Tensor
torch.__init__._sobol_engine_scramble_(self:Tensor,ltm:Tensor,dimension:_int)->Tensor
torch.__init__._softmax(self:Tensor,dim:_int,half_to_float:_bool)->Tensor
torch.__init__._softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,self:Tensor)->Tensor
torch.__init__._sparse_addmm(self:Tensor,sparse:Tensor,dense:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__._sparse_mm(sparse:Tensor,dense:Tensor)->Tensor
torch.__init__._sparse_sum(self:Tensor)->Tensor
torch.__init__._sparse_sum(self:Tensor,*,dtype:_dtype)->Tensor
torch.__init__._sparse_sum(self:Tensor,dim:Union[_int,_size])->Tensor
torch.__init__._sparse_sum(self:Tensor,dim:Union[_int,_size],*,dtype:_dtype)->Tensor
torch.__init__._standard_gamma(self:Tensor,generator:Generator=None)->Tensor
torch.__init__._standard_gamma_grad(self:Tensor,output:Tensor)->Tensor
torch.__init__._std(self:Tensor,unbiased:_bool=True)->Tensor
torch.__init__._trilinear(i1:Tensor,i2:Tensor,i3:Tensor,expand1:_size,expand2:_size,expand3:_size,sumdim:_size,unroll_dim:_int=1)->Tensor
torch.__init__._unique(self:Tensor,sorted:_bool=True,return_inverse:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__._unique2(self:Tensor,sorted:_bool=True,return_inverse:_bool=False,return_counts:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__._use_cudnn_ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int)->_bool
torch.__init__._use_cudnn_rnn_flatten_weight()->_bool
torch.__init__._var(self:Tensor,unbiased:_bool=True)->Tensor
torch.__init__._weight_norm(v:Tensor,g:Tensor,dim:_int=0)->Tensor
torch.__init__._weight_norm_cuda_interface(v:Tensor,g:Tensor,dim:_int=0)->Tuple[Tensor, Tensor]
torch.__init__.abs(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.abs_(self:Tensor)->Tensor
torch.__init__.acos(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.acos_(self:Tensor)->Tensor
torch.__init__.adaptive_avg_pool1d(self:Tensor,output_size:Union[_int,_size])->Tensor
torch.__init__.adaptive_max_pool1d(self:Tensor,output_size:Union[_int,_size])->Tuple[Tensor, Tensor]
torch.__init__.add(input:Union[Tensor,Number],other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.add(self:Tensor,alpha:Number,other:Tensor)->Tensor
torch.__init__.add(self:Tensor,alpha:Number,other:Tensor,*,out:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addbmm(self:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addcdiv(self:Tensor,tensor1:Tensor,tensor2:Tensor,*,value:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addcdiv(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor)->Tensor
torch.__init__.addcdiv(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addcmul(self:Tensor,tensor1:Tensor,tensor2:Tensor,*,value:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addcmul(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor)->Tensor
torch.__init__.addcmul(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmm(self:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,alpha:Number,mat:Tensor,vec:Tensor)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,alpha:Number,mat:Tensor,vec:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,mat:Tensor,vec:Tensor)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,mat:Tensor,vec:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmv(self:Tensor,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addmv_(self:Tensor,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,alpha:Number,vec1:Tensor,vec2:Tensor)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,alpha:Number,vec1:Tensor,vec2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,vec1:Tensor,vec2:Tensor)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,vec1:Tensor,vec2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addr(self:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.affine_grid_generator(theta:Tensor,size:_size,align_corners:_bool)->Tensor
torch.__init__.all(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.all(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.all(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.allclose(self:Tensor,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->_bool
torch.__init__.alpha_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.alpha_dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.angle(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.any(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.any(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.any(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.arange(end:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.arange(start:Number,end:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.arange(start:Number,end:Number,step:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.argmax(self:Tensor,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.argmin(self:Tensor,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.argsort(self:Tensor,dim:Union[str,None],descending:_bool=False)->Tensor
torch.__init__.argsort(self:Tensor,dim:_int=-1,descending:_bool=False)->Tensor
torch.__init__.as_strided(self:Tensor,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.as_strided_(self:Tensor,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.as_tensor(data:Any,dtype:_dtype=None,device:Optional[_device]=None)->Tensor
torch.__init__.asin(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.asin_(self:Tensor)->Tensor
torch.__init__.atan(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.atan2(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.atan_(self:Tensor)->Tensor
torch.__init__.avg_pool1d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,ceil_mode:_bool=False,count_include_pad:_bool=True)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.baddbmm(self:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.bartlett_window(window_length:_int,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.bartlett_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tensor
torch.__init__.batch_norm_backward_elemt(grad_out:Tensor,input:Tensor,mean:Tensor,invstd:Tensor,weight:Optional[Tensor],mean_dy:Tensor,mean_dy_xmu:Tensor)->Tensor
torch.__init__.batch_norm_backward_reduce(grad_out:Tensor,input:Tensor,mean:Tensor,invstd:Tensor,weight:Optional[Tensor],input_g:_bool,weight_g:_bool,bias_g:_bool)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__.batch_norm_elemt(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],mean:Tensor,invstd:Tensor,eps:_float,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.batch_norm_gather_stats(input:Tensor,mean:Tensor,invstd:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float,eps:_float,count:_int)->Tuple[Tensor, Tensor]
torch.__init__.batch_norm_gather_stats_with_counts(input:Tensor,mean:Tensor,invstd:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float,eps:_float,counts:_size)->Tuple[Tensor, Tensor]
torch.__init__.batch_norm_stats(input:Tensor,eps:_float)->Tuple[Tensor, Tensor]
torch.__init__.batch_norm_update_stats(input:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float)->Tuple[Tensor, Tensor]
torch.__init__.bernoulli(self:Tensor,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.bernoulli(self:Tensor,p:_float,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.bilinear(input1:Tensor,input2:Tensor,weight:Tensor,bias:Optional[Tensor])->Tensor
torch.__init__.bincount(self:Tensor,weights:Optional[Tensor]=None,minlength:_int=0)->Tensor
torch.__init__.bitwise_and(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.bitwise_and(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.bitwise_not(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.bitwise_or(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.bitwise_or(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.bitwise_xor(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.bitwise_xor(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.blackman_window(window_length:_int,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.blackman_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.bmm(self:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.can_cast(from_:_dtype,to:_dtype)->_bool
torch.__init__.cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:Union[str,None],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ceil(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ceil_(self:Tensor)->Tensor
torch.__init__.celu(self:Tensor,alpha:Number=1.0)->Tensor
torch.__init__.celu_(self:Tensor,alpha:Number=1.0)->Tensor
torch.__init__.cholesky(self:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cholesky_inverse(self:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cholesky_solve(self:Tensor,input2:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.chunk(self:Tensor,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.clamp(self,min:_float=-inf,max:_float=inf,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.clamp_max(self:Tensor,max:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.clamp_max_(self:Tensor,max:Number)->Tensor
torch.__init__.clamp_min(self:Tensor,min:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.clamp_min_(self:Tensor,min:Number)->Tensor
torch.__init__.clone(self:Tensor,*,memory_format:Optional[memory_format]=None)->Tensor
torch.__init__.combinations(self:Tensor,r:_int=2,with_replacement:_bool=False)->Tensor
torch.__init__.conj(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.constant_pad_nd(self:Tensor,pad:_size,value:Number=0)->Tensor
torch.__init__.conv1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch.__init__.conv2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch.__init__.conv3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch.__init__.conv_tbc(self:Tensor,weight:Tensor,bias:Tensor,pad:_int=0)->Tensor
torch.__init__.conv_transpose1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch.__init__.conv_transpose2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch.__init__.conv_transpose3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch.__init__.convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int)->Tensor
torch.__init__.cos(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cos_(self:Tensor)->Tensor
torch.__init__.cosh(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cosh_(self:Tensor)->Tensor
torch.__init__.cosine_similarity(x1:Tensor,x2:Tensor,dim:_int=1,eps:_float=1e-08)->Tensor
torch.__init__.cross(self:Tensor,other:Tensor,dim:Optional[_int]=None,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cudnn_affine_grid_generator(theta:Tensor,N:_int,C:_int,H:_int,W:_int)->Tensor
torch.__init__.cudnn_batch_norm(input:Tensor,weight:Tensor,bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,exponential_average_factor:_float,epsilon:_float)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__.cudnn_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.cudnn_convolution(self:Tensor,weight:Tensor,padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.cudnn_convolution_transpose(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.cudnn_convolution_transpose(self:Tensor,weight:Tensor,padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.cudnn_grid_sampler(self:Tensor,grid:Tensor)->Tensor
torch.__init__.cudnn_is_acceptable(self:Tensor)->_bool
torch.__init__.cummax(self:Tensor,dim:Union[str,None],*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.cummax(self:Tensor,dim:_int,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.cummin(self:Tensor,dim:Union[str,None],*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.cummin(self:Tensor,dim:_int,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.cumprod(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.cumprod(self:Tensor,dim:_int,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.cumsum(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.cumsum(self:Tensor,dim:_int,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.dequantize(self:Tensor)->Tensor
torch.__init__.det(self:Tensor)->Tensor
torch.__init__.detach(self:Tensor)->Tensor
torch.__init__.detach_(self:Tensor)->Tensor
torch.__init__.device(self,type:str,index:_int)
torch.__init__.device.__init__(self,type:str,index:_int)
torch.__init__.diag(self:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.diag_embed(self:Tensor,offset:_int=0,dim1:_int=-2,dim2:_int=-1)->Tensor
torch.__init__.diagflat(self:Tensor,offset:_int=0)->Tensor
torch.__init__.diagonal(self:Tensor,*,outdim:Union[str,None],dim1:Union[str,None],dim2:Union[str,None],offset:_int=0)->Tensor
torch.__init__.diagonal(self:Tensor,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch.__init__.digamma(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.dist(self:Tensor,other:Tensor,p:Number=2)->Tensor
torch.__init__.div(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.dot(self:Tensor,tensor:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.dtype
torch.__init__.eig(self:Tensor,eigenvectors:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.embedding(weight:Tensor,indices:Tensor,padding_idx:_int=-1,scale_grad_by_freq:_bool=False,sparse:_bool=False)->Tensor
torch.__init__.embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None,include_last_offset:_bool=False)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__.embedding_renorm_(self:Tensor,indices:Tensor,max_norm:_float,norm_type:_float)->Tensor
torch.__init__.empty(*size:_int,memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty(*size:_int,names:Optional[List[Union[str,None]]],memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty(size:_size,*,memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty(size:_size,*,names:Optional[List[Union[str,None]]],memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty_like(self:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty_strided(size:_size,stride:_size,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.eq(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.eq(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.equal(self:Tensor,other:Tensor)->_bool
torch.__init__.erf(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.erf_(self:Tensor)->Tensor
torch.__init__.erfc(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.erfc_(self:Tensor)->Tensor
torch.__init__.erfinv(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.exp(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.exp_(self:Tensor)->Tensor
torch.__init__.expm1(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.expm1_(self:Tensor)->Tensor
torch.__init__.eye(n:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.eye(n:_int,m:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.fake_quantize_per_channel_affine(self:Tensor,scale:Tensor,zero_point:Tensor,axis:_int,quant_min:_int,quant_max:_int)->Tensor
torch.__init__.fake_quantize_per_tensor_affine(self:Tensor,scale:_float,zero_point:_int,quant_min:_int,quant_max:_int)->Tensor
torch.__init__.fbgemm_linear_fp16_weight(input:Tensor,packed_weight:Tensor,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_fp16_weight_fp32_activation(input:Tensor,packed_weight:Tensor,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_int8_weight(input:Tensor,weight:Tensor,packed:Tensor,col_offsets:Tensor,weight_scale:Number,weight_zero_point:Number,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_int8_weight_fp32_activation(input:Tensor,weight:Tensor,packed:Tensor,col_offsets:Tensor,weight_scale:Number,weight_zero_point:Number,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_quantize_weight(input:Tensor)->Tuple[Tensor, Tensor, _float, _int]
torch.__init__.fbgemm_pack_gemm_matrix_fp16(input:Tensor)->Tensor
torch.__init__.fbgemm_pack_quantized_matrix(input:Tensor)->Tensor
torch.__init__.fbgemm_pack_quantized_matrix(input:Tensor,K:_int,N:_int)->Tensor
torch.__init__.feature_alpha_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.feature_alpha_dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.feature_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.feature_dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.fft(self:Tensor,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.fill_(self:Tensor,value:Number)->Tensor
torch.__init__.fill_(self:Tensor,value:Tensor)->Tensor
torch.__init__.flatten(self:Tensor,dims:List[Union[str,None]],out_dim:Union[str,None])->Tensor
torch.__init__.flatten(self:Tensor,start_dim:Union[str,None],end_dim:Union[str,None],out_dim:Union[str,None])->Tensor
torch.__init__.flatten(self:Tensor,start_dim:_int,end_dim:_int,out_dim:Union[str,None])->Tensor
torch.__init__.flatten(self:Tensor,start_dim:_int=0,end_dim:_int=-1)->Tensor
torch.__init__.flip(self:Tensor,dims:_size)->Tensor
torch.__init__.floor(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.floor_(self:Tensor)->Tensor
torch.__init__.floor_divide(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.fmod(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.fmod(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.frac(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.frac_(self:Tensor)->Tensor
torch.__init__.frobenius_norm(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.frobenius_norm(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.from_file(filename:str,shared:Optional[_bool]=None,size:Optional[_int]=0,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.from_numpy(ndarray)->Tensor
torch.__init__.full(size:_size,fill_value:Number,*,names:List[Union[str,None]],dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.full(size:_size,fill_value:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.full_like(self:Tensor,fill_value:Number,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.gather(self:Tensor,dim:Union[str,None],index:Tensor,*,sparse_grad:_bool=False,out:Optional[Tensor]=None)->Tensor
torch.__init__.gather(self:Tensor,dim:_int,index:Tensor,*,sparse_grad:_bool=False,out:Optional[Tensor]=None)->Tensor
torch.__init__.ge(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ge(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.geqrf(self:Tensor,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.ger(self:Tensor,vec2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.get_default_dtype()->_dtype
torch.__init__.get_num_interop_threads()->_int
torch.__init__.get_num_threads()->_int
torch.__init__.grid_sampler(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch.__init__.grid_sampler_2d(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch.__init__.grid_sampler_3d(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch.__init__.group_norm(input:Tensor,num_groups:_int,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:_float=1e-05,cudnn_enabled:_bool=True)->Tensor
torch.__init__.gru(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.gru(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.gru_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch.__init__.gt(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.gt(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.hamming_window(window_length:_int,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hamming_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hamming_window(window_length:_int,periodic:_bool,alpha:_float,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hamming_window(window_length:_int,periodic:_bool,alpha:_float,beta:_float,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hann_window(window_length:_int,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hann_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hardshrink(self:Tensor,lambd:Number=0.5)->Tensor
torch.__init__.histc(self:Tensor,bins:_int=100,min:Number=0,max:Number=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.hspmm(mat1:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ifft(self:Tensor,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.imag(self:Tensor)->Tensor
torch.__init__.index_add(self:Tensor,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.index_add(self:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.index_copy(self:Tensor,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.index_copy(self:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.index_fill(self:Tensor,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.index_fill(self:Tensor,dim:Union[str,None],index:Tensor,value:Tensor)->Tensor
torch.__init__.index_fill(self:Tensor,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.index_fill(self:Tensor,dim:_int,index:Tensor,value:Tensor)->Tensor
torch.__init__.index_put(self:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.index_put_(self:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.index_select(self:Tensor,dim:Union[str,None],index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.index_select(self:Tensor,dim:_int,index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.instance_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],use_input_stats:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tensor
torch.__init__.int_repr(self:Tensor)->Tensor
torch.__init__.inverse(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.irfft(self:Tensor,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True,signal_sizes:_size=())->Tensor
torch.__init__.is_complex(self:Tensor)->_bool
torch.__init__.is_distributed(self:Tensor)->_bool
torch.__init__.is_floating_point(self:Tensor)->_bool
torch.__init__.is_grad_enabled()->_bool
torch.__init__.is_nonzero(self:Tensor)->_bool
torch.__init__.is_same_size(self:Tensor,other:Tensor)->_bool
torch.__init__.is_signed(self:Tensor)->_bool
torch.__init__.isclose(self:Tensor,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->Tensor
torch.__init__.isfinite(self:Tensor)->Tensor
torch.__init__.isinf(self:Tensor)->Tensor
torch.__init__.isnan(self:Tensor)->Tensor
torch.__init__.kthvalue(self:Tensor,k:_int,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.kthvalue(self:Tensor,k:_int,dim:_int=-1,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.layer_norm(input:Tensor,normalized_shape:_size,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:_float=1e-05,cudnn_enable:_bool=True)->Tensor
torch.__init__.layout
torch.__init__.le(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.le(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lerp(self:Tensor,end:Tensor,weight:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lerp(self:Tensor,end:Tensor,weight:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lgamma(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.linspace(start:Number,end:Number,steps:_int=100,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.log(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log10(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log10_(self:Tensor)->Tensor
torch.__init__.log1p(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log1p_(self:Tensor)->Tensor
torch.__init__.log2(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log2_(self:Tensor)->Tensor
torch.__init__.log_(self:Tensor)->Tensor
torch.__init__.log_softmax(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.log_softmax(self:Tensor,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.logdet(self:Tensor)->Tensor
torch.__init__.logical_and(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logical_not(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logical_or(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logical_xor(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logspace(start:Number,end:Number,steps:_int=100,base:_float=10.0,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.logsumexp(self:Tensor,dim:List[Union[str,None]],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logsumexp(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lstm(data:Tensor,batch_sizes:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.lstm(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.lstm_cell(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.lstsq(self:Tensor,A:Tensor,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.lt(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lt(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lu_solve(self:Tensor,LU_data:Tensor,LU_pivots:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.masked_fill(self:Tensor,mask:Tensor,value:Number)->Tensor
torch.__init__.masked_fill(self:Tensor,mask:Tensor,value:Tensor)->Tensor
torch.__init__.masked_scatter(self:Tensor,mask:Tensor,source:Tensor)->Tensor
torch.__init__.masked_select(self:Tensor,mask:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.matmul(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.matrix_power(self:Tensor,n:_int)->Tensor
torch.__init__.matrix_rank(self:Tensor,symmetric:_bool=False)->Tensor
torch.__init__.matrix_rank(self:Tensor,tol:_float,symmetric:_bool=False)->Tensor
torch.__init__.max(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.max(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.max(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.max(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.max_pool1d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.max_pool1d_with_indices(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.max_pool2d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.max_pool3d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.mean(self:Tensor,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.mean(self:Tensor,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.mean(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.median(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.median(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.median(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.memory_format
torch.__init__.min(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.min(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.min(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.min(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.miopen_batch_norm(input:Tensor,weight:Tensor,bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,exponential_average_factor:_float,epsilon:_float)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.miopen_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.miopen_convolution_transpose(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.miopen_depthwise_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.miopen_rnn(input:Tensor,weight:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,hx:Tensor,cx:Optional[Tensor],mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,dropout:_float,train:_bool,bidirectional:_bool,batch_sizes:_size,dropout_state:Optional[Tensor])->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]
torch.__init__.mkldnn_adaptive_avg_pool2d(self:Tensor,output_size:Union[_int,_size])->Tensor
torch.__init__.mkldnn_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int)->Tensor
torch.__init__.mkldnn_convolution_backward_weights(weight_size:_size,grad_output:Tensor,self:Tensor,padding:_size,stride:_size,dilation:_size,groups:_int,bias_defined:_bool)->Tuple[Tensor, Tensor]
torch.__init__.mkldnn_max_pool2d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.mm(self:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.mode(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.mode(self:Tensor,dim:_int=-1,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.mul(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.multinomial(self:Tensor,num_samples:_int,replacement:_bool=False,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.mv(self:Tensor,vec:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.mvlgamma(self:Tensor,p:_int)->Tensor
torch.__init__.narrow(self:Tensor,dim:_int,start:Tensor,length:_int)->Tensor
torch.__init__.narrow(self:Tensor,dim:_int,start:_int,length:_int)->Tensor
torch.__init__.native_batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.native_layer_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],M:_int,N:_int,eps:_float)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.native_norm(self:Tensor,p:Number=2)->Tensor
torch.__init__.ne(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ne(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.neg(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.neg_(self:Tensor)->Tensor
torch.__init__.nonzero(input:Tensor,*,out:Optional[Tensor]=None,as_tuple:Optional[_bool]=None)
torch.__init__.norm_except_dim(v:Tensor,pow:_int=2,dim:_int=0)->Tensor
torch.__init__.normal(mean:Tensor,std:Tensor,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.normal(mean:Tensor,std:_float=1,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.normal(mean:_float,std:Tensor,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.normal(mean:_float,std:_float,size:_size,*,generator:Generator=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.nuclear_norm(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.nuclear_norm(self:Tensor,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.numel(self:Tensor)->_int
torch.__init__.ones(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones_like(self:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.orgqr(self:Tensor,input2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ormqr(self:Tensor,input2:Tensor,input3:Tensor,left:_bool=True,transpose:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pairwise_distance(x1:Tensor,x2:Tensor,p:_float=2,eps:_float=1e-06,keepdim:_bool=False)->Tensor
torch.__init__.pdist(self:Tensor,p:_float=2)->Tensor
torch.__init__.pinverse(self:Tensor,rcond:_float=1e-15)->Tensor
torch.__init__.pixel_shuffle(self:Tensor,upscale_factor:_int)->Tensor
torch.__init__.poisson(self:Tensor,generator:Generator=None)->Tensor
torch.__init__.poisson_nll_loss(input:Tensor,target:Tensor,log_input:_bool,full:_bool,eps:_float,reduction:_int)->Tensor
torch.__init__.polygamma(n:_int,self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pow(self:Number,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pow(self:Tensor,exponent:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pow(self:Tensor,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.prelu(self:Tensor,weight:Tensor)->Tensor
torch.__init__.prod(self:Tensor,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.prod(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.prod(self:Tensor,dim:_int,keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.promote_types(type1:_dtype,type2:_dtype)->_dtype
torch.__init__.q_per_channel_axis(self:Tensor)->_int
torch.__init__.q_per_channel_scales(self:Tensor)->Tensor
torch.__init__.q_per_channel_zero_points(self:Tensor)->Tensor
torch.__init__.q_scale(self:Tensor)->_float
torch.__init__.q_zero_point(self:Tensor)->_int
torch.__init__.qr(self:Tensor,some:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.qscheme
torch.__init__.quantize_per_channel(self:Tensor,scales:Tensor,zero_points:Tensor,axis:_int,dtype:_dtype)->Tensor
torch.__init__.quantize_per_tensor(self:Tensor,scale:_float,zero_point:_int,dtype:_dtype)->Tensor
torch.__init__.quantized_batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],mean:Tensor,var:Tensor,eps:_float,output_scale:_float,output_zero_point:_int)->Tensor
torch.__init__.quantized_gru(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.quantized_gru(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.quantized_gru_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch.__init__.quantized_lstm(data:Tensor,batch_sizes:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,*,dtype:Optional[_dtype]=None,use_dynamic:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.quantized_lstm(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool,*,dtype:Optional[_dtype]=None,use_dynamic:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.quantized_lstm_cell(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tuple[Tensor, Tensor]
torch.__init__.quantized_max_pool2d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.quantized_rnn_relu_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch.__init__.quantized_rnn_tanh_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch.__init__.rand(*size:_int,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(*size:_int,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand_like(self:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint(high:_int,size:_size,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint(low:_int,high:_int,size:_size,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint_like(self:Tensor,high:_int,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint_like(self:Tensor,low:_int,high:_int,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn_like(self:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randperm(n:_int,*,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randperm(n:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.range(start:Number,end:Number,step:Number=1,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.real(self:Tensor)->Tensor
torch.__init__.reciprocal(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.reciprocal_(self:Tensor)->Tensor
torch.__init__.relu(self:Tensor)->Tensor
torch.__init__.relu_(self:Tensor)->Tensor
torch.__init__.remainder(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.remainder(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.renorm(self:Tensor,p:Number,dim:_int,maxnorm:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.repeat_interleave(repeats:Tensor)->Tensor
torch.__init__.repeat_interleave(self:Tensor,repeats:Tensor,dim:Optional[_int]=None)->Tensor
torch.__init__.repeat_interleave(self:Tensor,repeats:_int,dim:Optional[_int]=None)->Tensor
torch.__init__.reshape(self:Tensor,shape:_size)->Tensor
torch.__init__.resize_as_(self:Tensor,the_template:Tensor,*,memory_format:Optional[memory_format]=None)->Tensor
torch.__init__.result_type(scalar1:Number,scalar2:Number)->_dtype
torch.__init__.result_type(scalar:Number,tensor:Tensor)->_dtype
torch.__init__.result_type(tensor:Tensor,other:Number)->_dtype
torch.__init__.result_type(tensor:Tensor,other:Tensor)->_dtype
torch.__init__.rfft(self:Tensor,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True)->Tensor
torch.__init__.rnn_relu(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_relu(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_relu_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch.__init__.rnn_tanh(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_tanh(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_tanh_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch.__init__.roll(self:Tensor,shifts:Union[_int,_size],dims:Union[_int,_size]=())->Tensor
torch.__init__.rot90(self:Tensor,k:_int=1,dims:_size=(0,1))->Tensor
torch.__init__.round(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.round_(self:Tensor)->Tensor
torch.__init__.rrelu(self:Tensor,lower:Number=0.125,upper:Number=0.3333333333333333,training:_bool=False,generator:Generator=None)->Tensor
torch.__init__.rrelu_(self:Tensor,lower:Number=0.125,upper:Number=0.3333333333333333,training:_bool=False,generator:Generator=None)->Tensor
torch.__init__.rsqrt(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.rsqrt_(self:Tensor)->Tensor
torch.__init__.rsub(self:Tensor,other:Number,alpha:Number=1)->Tensor
torch.__init__.rsub(self:Tensor,other:Tensor,*,alpha:Number=1)->Tensor
torch.__init__.scalar_tensor(s:Number,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.scatter(self:Tensor,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.scatter(self:Tensor,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.scatter(self:Tensor,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.scatter(self:Tensor,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.scatter_add(self:Tensor,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.scatter_add(self:Tensor,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.select(self:Tensor,dim:Union[str,None],index:_int)->Tensor
torch.__init__.select(self:Tensor,dim:_int,index:_int)->Tensor
torch.__init__.selu(self:Tensor)->Tensor
torch.__init__.selu_(self:Tensor)->Tensor
torch.__init__.set_flush_denormal(mode:_bool)->_bool
torch.__init__.set_num_interop_threads(num:_int)->None
torch.__init__.set_num_threads(num:_int)->None
torch.__init__.sigmoid(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sigmoid_(self:Tensor)->Tensor
torch.__init__.sign(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sin(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sin_(self:Tensor)->Tensor
torch.__init__.sinh(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sinh_(self:Tensor)->Tensor
torch.__init__.slogdet(self:Tensor)->Tuple[Tensor, Tensor]
torch.__init__.smm(self:Tensor,mat2:Tensor)->Tensor
torch.__init__.softmax(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.softmax(self:Tensor,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.solve(self:Tensor,A:Tensor,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.sort(self:Tensor,dim:Union[str,None],descending:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.sort(self:Tensor,dim:_int=-1,descending:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.sparse_coo_tensor(indices:Tensor,values:Union[Tensor,List],size:Optional[_size]=None,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.split_with_sizes(self:Tensor,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.sqrt(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sqrt_(self:Tensor)->Tensor
torch.__init__.square(self:Tensor)->Tensor
torch.__init__.square_(self:Tensor)->Tensor
torch.__init__.squeeze(self:Tensor)->Tensor
torch.__init__.squeeze(self:Tensor,dim:Union[str,None])->Tensor
torch.__init__.squeeze(self:Tensor,dim:_int)->Tensor
torch.__init__.sspaddmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.sspaddmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.sspaddmm(self:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.stack(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std(self:Tensor,unbiased:_bool=True,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std_mean(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.std_mean(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.std_mean(self:Tensor,unbiased:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.sub(input:Union[Tensor,Number],other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.sub(self:Tensor,alpha:Number,other:Tensor)->Tensor
torch.__init__.sub(self:Tensor,alpha:Number,other:Tensor,*,out:Tensor)->Tensor
torch.__init__.sum(self:Tensor,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.sum(self:Tensor,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.sum(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.svd(self:Tensor,some:_bool=True,compute_uv:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.symeig(self:Tensor,eigenvectors:_bool=False,upper:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.t(self:Tensor)->Tensor
torch.__init__.take(self:Tensor,index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tan(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tan_(self:Tensor)->Tensor
torch.__init__.tanh(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tanh_(self:Tensor)->Tensor
torch.__init__.tensor(data:Any,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.threshold(self:Tensor,threshold:Number,value:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.threshold_(self:Tensor,threshold:Number,value:Number)->Tensor
torch.__init__.topk(self:Tensor,k:_int,dim:_int=-1,largest:_bool=True,sorted:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.trace(self:Tensor)->Tensor
torch.__init__.transpose(self:Tensor,dim0:Union[str,None],dim1:Union[str,None])->Tensor
torch.__init__.transpose(self:Tensor,dim0:_int,dim1:_int)->Tensor
torch.__init__.trapz(y:Tensor,*,dx:_float=1,dim:_int=-1)->Tensor
torch.__init__.trapz(y:Tensor,x:Tensor,*,dim:_int=-1)->Tensor
torch.__init__.triangular_solve(self:Tensor,A:Tensor,upper:_bool=True,transpose:_bool=False,unitriangular:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.tril(self:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tril_indices(row:_int,col:_int,offset:_int=0,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.triu(self:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.triu_indices(row:_int,col:_int,offset:_int=0,*,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.true_divide(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.trunc(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.trunc_(self:Tensor)->Tensor
torch.__init__.unbind(self:Tensor,dim:Union[str,None])->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.unbind(self:Tensor,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.unique_dim(self:Tensor,dim:_int,sorted:_bool=True,return_inverse:_bool=False,return_counts:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.unsqueeze(self:Tensor,dim:_int)->Tensor
torch.__init__.var(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.var(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.var(self:Tensor,unbiased:_bool=True,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.var_mean(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.var_mean(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.var_mean(self:Tensor,unbiased:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.where(condition:Tensor)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.where(condition:Tensor,self:Tensor,other:Tensor)->Tensor
torch.__init__.zero_(self:Tensor)->Tensor
torch.__init__.zeros(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros_like(self:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:_layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_lobpcg.py----------------------------------------
A:torch._lobpcg.dtype->_utils.get_floating_dtype(A)
A:torch._lobpcg.iparams['ortho_i_max']->iparams.get('ortho_i_max', 3)
A:torch._lobpcg.iparams['ortho_j_max']->iparams.get('ortho_j_max', 3)
A:torch._lobpcg.fparams['ortho_tol']->fparams.get('ortho_tol', tol)
A:torch._lobpcg.fparams['ortho_tol_drop']->fparams.get('ortho_tol_drop', tol)
A:torch._lobpcg.fparams['ortho_tol_replace']->fparams.get('ortho_tol_replace', tol)
A:torch._lobpcg.bparams['ortho_use_drop']->bparams.get('ortho_use_drop', False)
A:torch._lobpcg.N->int(torch.prod(torch.tensor(A.shape[:-2])))
A:torch._lobpcg.bA->A.reshape((N,) + A.shape[-2:])
A:torch._lobpcg.bE->torch.empty((N, k), dtype=dtype, device=device)
A:torch._lobpcg.bXret->torch.empty((N, m, k), dtype=dtype, device=device)
A:torch._lobpcg.worker->LOBPCG(A, B, X, iK, iparams, fparams, bparams, method, tracker)
A:torch._lobpcg.self.E->torch.zeros((n,), dtype=X.dtype, device=X.device)
A:torch._lobpcg.self.R->torch.zeros((m, n), dtype=X.dtype, device=X.device)
A:torch._lobpcg.self.S->torch.zeros((m, 3 * n), dtype=X.dtype, device=X.device)
A:torch._lobpcg.X_norm->float(torch.norm(self.X))
A:torch._lobpcg.Ri->self._get_rayleigh_ritz_transform(self.X)
A:torch._lobpcg.M->_utils.qform(_utils.qform(self.A, self.X), Ri)
A:torch._lobpcg.(E, Z)->_utils.symeig(DUBUD, eigenvectors=True)
A:torch._lobpcg.self.X[:]->mm(self.X, mm(Ri, Z))
A:torch._lobpcg.nc->self.update_converged_count()
A:torch._lobpcg.W->self._get_ortho(self.R[:, nc:], self.S[:, :n + np])
A:torch._lobpcg.(E_, Z)->_utils.symeig(_utils.qform(self.A, S_), largest)
A:torch._lobpcg.self.X[:, nc:]->mm(S_, Z[:, :n - nc])
A:torch._lobpcg.P->mm(S_, mm(Z[:, n - nc:], _utils.basis(_utils.transpose(Z[:n - nc, n - nc:]))))
A:torch._lobpcg.self.X->mm(self.X, mm(Ri, Z))
A:torch._lobpcg.SBS->_utils.qform(B, S)
A:torch._lobpcg.d_col->(d ** (-0.5)).reshape(d.shape[0], 1)
A:torch._lobpcg.R->torch.cholesky(SBS * d_row * d_col, upper=True)
A:torch._lobpcg.Rinv->torch.inverse(R)
A:torch._lobpcg.UBU->mm(_utils.transpose(U), BU)
A:torch._lobpcg.d->mm(_utils.transpose(U), BU).diagonal(0, -2, -1)
A:torch._lobpcg.nz->torch.where(abs(d) != 0.0)
A:torch._lobpcg.keep->torch.where(E > t)
A:torch._lobpcg.BV_norm->torch.norm(mm_B(self.B, V))
A:torch._lobpcg.BU->mm_B(self.B, U)
A:torch._lobpcg.VBU->mm(_utils.transpose(V), BU)
A:torch._lobpcg.U->self._get_svqb(U, False, tau_replace)
A:torch._lobpcg.U_norm->torch.norm(U)
A:torch._lobpcg.BU_norm->torch.norm(BU)
A:torch._lobpcg.R_norm->torch.norm(R)
A:torch._lobpcg.vkey->'ortho_VBU_rerr[{}]'.format(i)
A:torch._lobpcg.VBU_norm->torch.norm(VBU)
torch._lobpcg.LOBPCG(self,A,B,X,iK,iparams,fparams,bparams,method,tracker)
torch._lobpcg.LOBPCG.__init__(self,A,B,X,iK,iparams,fparams,bparams,method,tracker)
torch._lobpcg.LOBPCG.__str__(self)
torch._lobpcg.LOBPCG._get_ortho(self,U,V)
torch._lobpcg.LOBPCG._get_rayleigh_ritz_transform(self,S)
torch._lobpcg.LOBPCG._get_svqb(self,U,drop,tau)
torch._lobpcg.LOBPCG._update_basic(self)
torch._lobpcg.LOBPCG._update_ortho(self)
torch._lobpcg.LOBPCG.call_tracker(self)
torch._lobpcg.LOBPCG.run(self)
torch._lobpcg.LOBPCG.stop_iteration(self)
torch._lobpcg.LOBPCG.update(self)
torch._lobpcg.LOBPCG.update_converged_count(self)
torch._lobpcg.LOBPCG.update_residual(self)
torch._lobpcg.LOBPCG_call_tracker(self)
torch._lobpcg.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)
torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_six.py----------------------------------------
A:torch._six.inf->float('inf')
A:torch._six.nan->float('nan')
A:torch._six.exec_->getattr(builtins, 'exec')
A:torch._six.frame->sys._getframe(1)
A:torch._six.method->getattr(cls, name, None)
A:torch._six.t->type(obj)
torch._six.bind_method(fn,obj,obj_type)
torch._six.istuple(obj)
torch._six.with_metaclass(meta,*bases)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_torch_docs.py----------------------------------------
A:torch._torch_docs.regx->re.compile('\\n\\s{4}(?!\\s)')
A:torch._torch_docs.common_args->parse_kwargs('\n    input (Tensor): the input tensor.\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n')
A:torch._torch_docs.reduceops_common_args->merge_dicts(common_args, parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n'))
A:torch._torch_docs.multi_dim_common->merge_dicts(reduceops_common_args, parse_kwargs('\n    dim (int or tuple of ints): the dimension or dimensions to reduce.\n'), {'keepdim_details': '\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n'})
A:torch._torch_docs.single_dim_common->merge_dicts(reduceops_common_args, parse_kwargs('\n    dim (int): the dimension to reduce.\n'), {'keepdim_details': 'If :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\nthe output tensor having 1 fewer dimension than :attr:`input`.'})
A:torch._torch_docs.factory_common_args->merge_dicts(common_args, parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.contiguous_format``.\n'))
A:torch._torch_docs.factory_like_common_args->parse_kwargs('\n    input (Tensor): the size of :attr:`input` will determine size of the output tensor.\n    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n        Default: if ``None``, defaults to the layout of :attr:`input`.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n        Default: if ``None``, defaults to the dtype of :attr:`input`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, defaults to the device of :attr:`input`.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.\n')
A:torch._torch_docs.factory_data_common_args->parse_kwargs('\n    data (array_like): Initial data for the tensor. Can be a list, tuple,\n        NumPy ``ndarray``, scalar, and other types.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, infers data type from :attr:`data`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
torch._torch_docs.merge_dicts(*dicts)
torch._torch_docs.parse_kwargs(desc)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/functional.py----------------------------------------
A:torch.functional.empty_list->torch.jit.annotate(List[int], [])
A:torch.functional.result_temp->torch.jit.annotate(List[List[int]], [])
A:torch.functional.k->min(m, n)
A:torch.functional.U->U.narrow(-2, 0, k).narrow(-2, 0, k)
A:torch.functional.L->L.narrow(-1, 0, k).narrow(-1, 0, k)
A:torch.functional.P->P.index_select(1, torch.as_tensor(final_order, device=LU_pivots.device)).index_select(1, torch.as_tensor(final_order, device=LU_pivots.device))
A:torch.functional.indices->_indices_product(shape[:-2])
A:torch.functional.p_idx->_index_tensor_with_indices_list(P, idx)
A:torch.functional.signal_dim->input.view(input.shape[-signal_dim:]).dim()
A:torch.functional.pad->int(n_fft // 2)
A:torch.functional.input->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:])
A:torch.functional.(output, inverse_indices, counts)->torch._VF.unique_consecutive(input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
A:torch.functional.(output, _, counts)->_unique_impl(input, sorted, return_inverse, return_counts, dim)
A:torch.functional.(output, _, _)->_unique_impl(input, sorted, return_inverse, return_counts, dim)
A:torch.functional.(output, inverse_indices, _)->_unique_impl(input, sorted, return_inverse, return_counts, dim)
A:torch.functional._return_inverse_false->boolean_dispatch(arg_name='return_counts', arg_index=3, default=False, if_true=_return_counts, if_false=_return_output, module_name=__name__, func_name='unique')
A:torch.functional._return_inverse_true->boolean_dispatch(arg_name='return_counts', arg_index=3, default=False, if_true=_unique_impl, if_false=_return_inverse, module_name=__name__, func_name='unique')
A:torch.functional.unique->boolean_dispatch(arg_name='return_inverse', arg_index=2, default=False, if_true=_return_inverse_true, if_false=_return_inverse_false, module_name=__name__, func_name='unique')
A:torch.functional.dims->dims.item().item()
A:torch.functional.dims_a->list(range(-dims, 0))
A:torch.functional.dims_b->list(range(dims))
A:torch.functional.p->str(p)
A:torch.functional.ndim->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:]).dim()
A:torch.functional.result->_lu_impl(A, pivot, get_infos, out)
A:torch.functional.lu->boolean_dispatch(arg_name='get_infos', arg_index=2, default=False, if_true=_lu_with_infos, if_false=_lu_no_infos, module_name=__name__, func_name='lu')
torch._check_list_size(out_len,get_infos,out)
torch._index_tensor_with_indices_list(tensor,indices)
torch._indices_product(indices)
torch._lu_impl(A,pivot=True,get_infos=False,out=None)
torch._lu_no_infos(A,pivot=True,get_infos=False,out=None)
torch._lu_with_infos(A,pivot=True,get_infos=False,out=None)
torch._return_counts(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch._return_inverse(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch._return_output(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch._unique_impl(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.align_tensors(*tensors)
torch.broadcast_tensors(*tensors)
torch.cartesian_prod(*tensors)
torch.cdist(x1,x2,p=2.0,compute_mode='use_mm_for_euclid_dist_if_necessary')
torch.chain_matmul(*matrices)
torch.einsum(equation,*operands)
torch.functional._check_list_size(out_len,get_infos,out)
torch.functional._index_tensor_with_indices_list(tensor,indices)
torch.functional._indices_product(indices)
torch.functional._lu_impl(A,pivot=True,get_infos=False,out=None)
torch.functional._lu_no_infos(A,pivot=True,get_infos=False,out=None)
torch.functional._lu_with_infos(A,pivot=True,get_infos=False,out=None)
torch.functional._return_counts(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional._return_inverse(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional._return_output(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional._unique_impl(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional.align_tensors(*tensors)
torch.functional.broadcast_tensors(*tensors)
torch.functional.cartesian_prod(*tensors)
torch.functional.cdist(x1,x2,p=2.0,compute_mode='use_mm_for_euclid_dist_if_necessary')
torch.functional.chain_matmul(*matrices)
torch.functional.einsum(equation,*operands)
torch.functional.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)
torch.functional.meshgrid(*tensors)
torch.functional.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)
torch.functional.split(tensor,split_size_or_sections,dim=0)
torch.functional.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.functional.tensordot(a,b,dims=2)
torch.functional.unique_consecutive(input,return_inverse=False,return_counts=False,dim=None)
torch.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)
torch.meshgrid(*tensors)
torch.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)
torch.split(tensor,split_size_or_sections,dim=0)
torch.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.tensordot(a,b,dims=2)
torch.unique_consecutive(input,return_inverse=False,return_counts=False,dim=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_lowrank.py----------------------------------------
A:torch._lowrank.dtype->_utils.get_floating_dtype(A)
A:torch._lowrank.R->torch.randn(n, q, dtype=dtype, device=A.device)
A:torch._lowrank.A_H->_utils.transjugate(A)
A:torch._lowrank.(Q, _)->(matmul(A, Q) - matmul(M, Q)).qr()
A:torch._lowrank.M_H->_utils.transjugate(M)
A:torch._lowrank.M_t->_utils.transpose(M)
A:torch._lowrank.A_t->_utils.transpose(A)
A:torch._lowrank.Q->get_approximate_basis(A, q, niter=niter, M=M)
A:torch._lowrank.Q_c->_utils.conjugate(Q)
A:torch._lowrank.B_t->matmul(A, Q_c)
A:torch._lowrank.(U, S, V)->torch.svd(_utils.transpose(B))
A:torch._lowrank.V->get_approximate_basis(A, q, niter=niter, M=M).matmul(V)
A:torch._lowrank.B->matmul(A_t, Q_c)
A:torch._lowrank.U->get_approximate_basis(A, q, niter=niter, M=M).matmul(U)
A:torch._lowrank.q->min(6, m, n)
A:torch._lowrank.indices->torch.zeros(2, len(column_indices), dtype=column_indices.dtype, device=column_indices.device)
A:torch._lowrank.C_t->torch.sparse_coo_tensor(indices, c.values(), (n, 1), dtype=dtype, device=A.device)
A:torch._lowrank.ones_m1_t->torch.ones(A.shape[:-2] + (1, m), dtype=dtype, device=A.device)
A:torch._lowrank.M->torch.ones(A.shape[:-1] + (1,), dtype=dtype, device=A.device).matmul(C)
A:torch._lowrank.C->c.reshape(A.shape[:-2] + (1, n))
A:torch._lowrank.ones_m1->torch.ones(A.shape[:-1] + (1,), dtype=dtype, device=A.device)
torch._lowrank._svd_lowrank(A,q=6,niter=2,M=None)
torch._lowrank.get_approximate_basis(A,q,niter=2,M=None)
torch._lowrank.pca_lowrank(A,q=None,center=True,niter=2)
torch._lowrank.svd_lowrank(A,q=6,niter=2,M=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_utils_internal.py----------------------------------------
A:torch._utils_internal.torch_parent->os.path.dirname(os.path.dirname(__file__))
A:torch._utils_internal.filename->inspect.getsourcefile(obj)
A:torch._utils_internal.(sourcelines, file_lineno)->inspect.getsourcelines(obj)
A:torch._utils_internal.msg->"Can't get source for {}. TorchScript requires source access in order to carry out compilation, make sure original .py files are available. Original error: {}".format(obj, e)
torch._utils_internal.get_file_path(*path_components)
torch._utils_internal.get_file_path_2(*path_components)
torch._utils_internal.get_source_lines_and_file(obj,error_msg=None)
torch._utils_internal.get_writable_path(path)
torch._utils_internal.prepare_multiprocessing_environment(path)
torch._utils_internal.resolve_library_path(path)
torch.get_file_path(*path_components)
torch.get_file_path_2(*path_components)
torch.prepare_multiprocessing_environment(path)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/version.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_overrides.py----------------------------------------
A:torch._overrides.arg_type->type(arg)
A:torch._overrides.index->len(overloaded_args)
A:torch._overrides.overloaded_args->_get_overloaded_args(relevant_args)
A:torch._overrides.types->tuple(map(type, overloaded_args))
A:torch._overrides.result->overloaded_arg.__torch_function__(public_api, types, args, kwargs)
A:torch._overrides.func_name->'{}.{}'.format(public_api.__module__, public_api.__name__)
A:torch._overrides.overridable_funcs->collections.defaultdict(list)
A:torch._overrides.func->getattr(namespace, func_name)
torch._overrides._get_overloaded_args(relevant_args)
torch._overrides.get_ignored_functions()
torch._overrides.get_overridable_functions()
torch._overrides.get_testing_overrides()
torch._overrides.handle_torch_function(public_api,relevant_args,*args,**kwargs)
torch._overrides.has_torch_function(relevant_args)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_storage_docs.py----------------------------------------
A:torch._storage_docs.cls->getattr(torch._C, cls_name)
torch._storage_docs.add_docstr_all(method,docstr)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/__future__.py----------------------------------------
torch.__future__.get_overwrite_module_params_on_conversion()
torch.__future__.set_overwrite_module_params_on_conversion(value)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quasirandom.py----------------------------------------
A:torch.quasirandom.cpu->torch.device('cpu')
A:torch.quasirandom.self.sobolstate->torch.zeros(dimension, self.MAXBIT, device=cpu, dtype=torch.long)
A:torch.quasirandom.g->torch.Generator()
A:torch.quasirandom.shift_ints->torch.randint(2, (self.dimension, self.MAXBIT), device=cpu, generator=g)
A:torch.quasirandom.self.shift->torch.zeros(self.dimension, device=cpu, dtype=torch.long)
A:torch.quasirandom.ltm->torch.randint(2, ltm_dims, device=cpu, generator=g).tril()
A:torch.quasirandom.self.quasi->self.shift.clone(memory_format=torch.contiguous_format)
A:torch.quasirandom.(result, self.quasi)->torch._sobol_engine_draw(self.quasi, n, self.sobolstate, self.dimension, self.num_generated, dtype=dtype)
torch.quasirandom.SobolEngine(self,dimension,scramble=False,seed=None)
torch.quasirandom.SobolEngine.__init__(self,dimension,scramble=False,seed=None)
torch.quasirandom.SobolEngine.__repr__(self)
torch.quasirandom.SobolEngine.draw(self,n=1,out=None,dtype=torch.float32)
torch.quasirandom.SobolEngine.fast_forward(self,n)
torch.quasirandom.SobolEngine.reset(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/random.py----------------------------------------
A:torch.random.seed->torch._C.default_generator.seed()
A:torch.random.num_devices->torch.cuda.device_count()
A:torch.random.devices->list(devices)
A:torch.random.cpu_rng_state->torch.get_rng_state()
torch.get_rng_state()
torch.initial_seed()
torch.manual_seed(seed)
torch.random.fork_rng(devices=None,enabled=True,_caller='fork_rng',_devices_kw='devices')
torch.random.get_rng_state()
torch.random.initial_seed()
torch.random.manual_seed(seed)
torch.random.seed()
torch.random.set_rng_state(new_state)
torch.seed()
torch.set_rng_state(new_state)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_tensor_docs.py----------------------------------------
A:torch._tensor_docs.common_args->parse_kwargs('\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.\n')
A:torch._tensor_docs.new_common_args->parse_kwargs('\n    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n        shape of the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n        Default: if None, same :class:`torch.dtype` as this tensor.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if None, same :class:`torch.device` as this tensor.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
torch._tensor_docs.add_docstr_all(method,docstr)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_classes.py----------------------------------------
A:torch._classes.proxy->torch._C._get_custom_class_python_wrapper(self.name, attr)
A:torch._classes.namespace->_ClassNamespace(name)
A:torch._classes.classes->_Classes()
torch._classes._ClassNamespace(self,name)
torch._classes._ClassNamespace.__getattr__(self,attr)
torch._classes._ClassNamespace.__init__(self,name)
torch._classes._Classes(self)
torch._classes._Classes.__getattr__(self,name)
torch._classes._Classes.__init__(self)
torch._classes._Classes.load_library(self,path)
torch._classes._Classes.loaded_libraries(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/hub.py----------------------------------------
A:torch.hub.HASH_REGEX->re.compile('-([a-f0-9]*)\\.')
A:torch.hub.spec->importlib.util.spec_from_file_location(name, path)
A:torch.hub.module->importlib.util.module_from_spec(spec)
A:torch.hub.torch_home->_get_torch_home()
A:torch.hub.hub_dir->os.path.join(torch_home, 'hub')
A:torch.hub.(repo_info, branch)->github.split(':')
A:torch.hub.(repo_owner, repo_name)->repo_info.split('/')
A:torch.hub.(repo_owner, repo_name, branch)->_parse_repo_info(github)
A:torch.hub.normalized_br->branch.replace('/', '_')
A:torch.hub.repo_dir->_get_cache_or_reload(github, force_reload, verbose)
A:torch.hub.cached_file->os.path.join(model_dir, extraced_name)
A:torch.hub.url->_git_archive_link(repo_owner, repo_name, branch)
A:torch.hub.extracted_repo->os.path.join(hub_dir, extraced_repo_name)
A:torch.hub.result->sys.path_importer_cache.get(item).find_module(name, [item])
A:torch.hub.importer->sys.path_importer_cache.get(item)
A:torch.hub.dependencies->_load_attr_from_module(m, VAR_DEPENDENCY)
A:torch.hub.func->_load_attr_from_module(m, model)
A:torch.hub.hub_module->import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)
A:torch.hub.entry->_load_entry_from_hubconf(hub_module, model)
A:torch.hub.force_reload->kwargs.get('force_reload', False)
A:torch.hub.verbose->kwargs.get('verbose', True)
A:torch.hub.model->entry(*args, **kwargs)
A:torch.hub.u->urlopen(url)
A:torch.hub.meta->urlopen(url).info()
A:torch.hub.content_length->urlopen(url).info().get_all('Content-Length')
A:torch.hub.file_size->int(content_length[0])
A:torch.hub.dst->os.path.expanduser(dst)
A:torch.hub.dst_dir->os.path.dirname(dst)
A:torch.hub.f->tempfile.NamedTemporaryFile(delete=False, dir=dst_dir)
A:torch.hub.sha256->hashlib.sha256()
A:torch.hub.buffer->urlopen(url).read(8192)
A:torch.hub.digest->hashlib.sha256().hexdigest()
A:torch.hub.model_dir->os.path.join(torch_home, 'checkpoints')
A:torch.hub.parts->urlparse(url)
A:torch.hub.filename->os.path.basename(parts.path)
A:torch.hub.members->cached_zipfile.infolist()
torch.hub._check_dependencies(m)
torch.hub._check_module_exists(name)
torch.hub._download_url_to_file(url,dst,hash_prefix=None,progress=True)
torch.hub._get_cache_or_reload(github,force_reload,verbose=True)
torch.hub._get_torch_home()
torch.hub._git_archive_link(repo_owner,repo_name,branch)
torch.hub._load_attr_from_module(module,func_name)
torch.hub._load_entry_from_hubconf(m,model)
torch.hub._parse_repo_info(github)
torch.hub._remove_if_exists(path)
torch.hub._setup_hubdir()
torch.hub.download_url_to_file(url,dst,hash_prefix=None,progress=True)
torch.hub.help(github,model,force_reload=False)
torch.hub.import_module(name,path)
torch.hub.list(github,force_reload=False)
torch.hub.load(github,model,*args,**kwargs)
torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)
torch.hub.set_dir(d)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/tensor.py----------------------------------------
A:torch.tensor.new_tensor->self.new()
A:torch.tensor.new_storage->self.storage().__deepcopy__(memo)
A:torch.tensor.self._backward_hooks->OrderedDict()
A:torch.tensor.handle->torch.utils.hooks.RemovableHandle(self._backward_hooks)
A:torch.tensor.detach->_add_docstr(_C._TensorBase.detach, '\n    Returns a new Tensor, detached from the current graph.\n\n    The result will never require gradient.\n\n    .. note::\n\n      Returned Tensor shares the same storage with the original one.\n      In-place modifications on either of them will be seen, and may trigger\n      errors in correctness checks.\n      IMPORTANT NOTE: Previously, in-place size / stride / storage changes\n      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor\n      also update the original tensor. Now, these in-place changes will not update the\n      original tensor anymore, and will instead trigger an error.\n      For sparse tensors:\n      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the\n      returned tensor will not update the original tensor anymore, and will instead\n      trigger an error.\n    ')
A:torch.tensor.detach_->_add_docstr(_C._TensorBase.detach_, '\n    Detaches the Tensor from the graph that created it, making it a leaf.\n    Views cannot be detached in-place.\n    ')
A:torch.tensor.weak_self->weakref.ref(self)
A:torch.tensor.var->weak_self()
A:torch.tensor.var._grad->grad.clone(memory_format=torch.contiguous_format)
A:torch.tensor.(LU, pivots, infos)->torch._lu_with_info(self, pivot=pivot, check_errors=not get_infos)
A:torch.tensor.split_size->int(split_size)
A:torch.tensor.result->result.trunc().trunc()
A:torch.tensor.__eq__->_wrap_type_error_to_not_implemented(_C._TensorBase.eq)
A:torch.tensor.__ne__->_wrap_type_error_to_not_implemented(_C._TensorBase.ne)
A:torch.tensor.__lt__->_wrap_type_error_to_not_implemented(_C._TensorBase.lt)
A:torch.tensor.__le__->_wrap_type_error_to_not_implemented(_C._TensorBase.le)
A:torch.tensor.__gt__->_wrap_type_error_to_not_implemented(_C._TensorBase.gt)
A:torch.tensor.__ge__->_wrap_type_error_to_not_implemented(_C._TensorBase.ge)
A:torch.tensor.tensor_methods->dir(self.__class__)
A:torch.tensor.attrs->list(self.__dict__.keys())
A:torch.tensor.array->array.astype('uint8').astype('uint8')
A:torch.tensor.itemsize->self.storage().element_size()
A:torch.tensor.shape->tuple(self.shape)
A:torch.tensor.strides->tuple((s * itemsize for s in self.stride()))
A:torch.tensor.names->resolve_ellipsis(names, self.names, 'refine_names')
A:torch.tensor.ellipsis_idx->single_ellipsis_index(names, 'align_to')
A:torch.tensor.(names, sizes)->unzip_namedshape(namedshape)
torch.Tensor(torch._C._TensorBase)
torch.Tensor.__array__(self,dtype=None)
torch.Tensor.__array_wrap__(self,array)
torch.Tensor.__contains__(self,element)
torch.Tensor.__cuda_array_interface__(self)
torch.Tensor.__deepcopy__(self,memo)
torch.Tensor.__dir__(self)
torch.Tensor.__floordiv__(self,other)
torch.Tensor.__format__(self,format_spec)
torch.Tensor.__hash__(self)
torch.Tensor.__ipow__(self,other)
torch.Tensor.__iter__(self)
torch.Tensor.__len__(self)
torch.Tensor.__rdiv__(self,other)
torch.Tensor.__reduce_ex__(self,proto)
torch.Tensor.__repr__(self)
torch.Tensor.__reversed__(self)
torch.Tensor.__rfloordiv__(self,other)
torch.Tensor.__rpow__(self,other)
torch.Tensor.__rsub__(self,other)
torch.Tensor.__setstate__(self,state)
torch.Tensor._update_names(self,names,inplace)
torch.Tensor.align_to(self,*names)
torch.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False)
torch.Tensor.grad(self)
torch.Tensor.grad(self)
torch.Tensor.grad(self,new_grad)
torch.Tensor.is_shared(self)
torch.Tensor.lu(self,pivot=True,get_infos=False)
torch.Tensor.norm(self,p='fro',dim=None,keepdim=False,dtype=None)
torch.Tensor.refine_names(self,*names)
torch.Tensor.register_hook(self,hook)
torch.Tensor.reinforce(self,reward)
torch.Tensor.rename(self,*names,**rename_map)
torch.Tensor.rename_(self,*names,**rename_map)
torch.Tensor.resize(self,*sizes)
torch.Tensor.resize_as(self,tensor)
torch.Tensor.retain_grad(self)
torch.Tensor.share_memory_(self)
torch.Tensor.split(self,split_size,dim=0)
torch.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.Tensor.unflatten(self,dim,namedshape)
torch.Tensor.unique(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.Tensor.unique_consecutive(self,return_inverse=False,return_counts=False,dim=None)
torch.tensor.Tensor(torch._C._TensorBase)
torch.tensor.Tensor.__array__(self,dtype=None)
torch.tensor.Tensor.__array_wrap__(self,array)
torch.tensor.Tensor.__contains__(self,element)
torch.tensor.Tensor.__cuda_array_interface__(self)
torch.tensor.Tensor.__deepcopy__(self,memo)
torch.tensor.Tensor.__dir__(self)
torch.tensor.Tensor.__floordiv__(self,other)
torch.tensor.Tensor.__format__(self,format_spec)
torch.tensor.Tensor.__hash__(self)
torch.tensor.Tensor.__ipow__(self,other)
torch.tensor.Tensor.__iter__(self)
torch.tensor.Tensor.__len__(self)
torch.tensor.Tensor.__rdiv__(self,other)
torch.tensor.Tensor.__reduce_ex__(self,proto)
torch.tensor.Tensor.__repr__(self)
torch.tensor.Tensor.__reversed__(self)
torch.tensor.Tensor.__rfloordiv__(self,other)
torch.tensor.Tensor.__rpow__(self,other)
torch.tensor.Tensor.__rsub__(self,other)
torch.tensor.Tensor.__setstate__(self,state)
torch.tensor.Tensor._update_names(self,names,inplace)
torch.tensor.Tensor.align_to(self,*names)
torch.tensor.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False)
torch.tensor.Tensor.grad(self)
torch.tensor.Tensor.grad(self)
torch.tensor.Tensor.grad(self,new_grad)
torch.tensor.Tensor.is_shared(self)
torch.tensor.Tensor.lu(self,pivot=True,get_infos=False)
torch.tensor.Tensor.norm(self,p='fro',dim=None,keepdim=False,dtype=None)
torch.tensor.Tensor.refine_names(self,*names)
torch.tensor.Tensor.register_hook(self,hook)
torch.tensor.Tensor.reinforce(self,reward)
torch.tensor.Tensor.rename(self,*names,**rename_map)
torch.tensor.Tensor.rename_(self,*names,**rename_map)
torch.tensor.Tensor.resize(self,*sizes)
torch.tensor.Tensor.resize_as(self,tensor)
torch.tensor.Tensor.retain_grad(self)
torch.tensor.Tensor.share_memory_(self)
torch.tensor.Tensor.split(self,split_size,dim=0)
torch.tensor.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.tensor.Tensor.unflatten(self,dim,namedshape)
torch.tensor.Tensor.unique(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.tensor.Tensor.unique_consecutive(self,return_inverse=False,return_counts=False,dim=None)
torch.tensor._wrap_type_error_to_not_implemented(f)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_VF.py----------------------------------------
A:torch._VF.sys.modules[__name__]->VFModule(__name__)
torch._VF.VFModule(self,name)
torch._VF.VFModule.__getattr__(self,attr)
torch._VF.VFModule.__init__(self,name)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_jit_internal.py----------------------------------------
A:torch._jit_internal.boolean_dispatched->weakref.WeakKeyDictionary()
A:torch._jit_internal.parts->qualified_name.split('.')
A:torch._jit_internal.remainding_pieces->'.'.join(parts[1:])
A:torch._jit_internal.module_value->getattr(module, base)
A:torch._jit_internal.frame->inspect.currentframe()
A:torch._jit_internal.closure->get_closure(fn)
A:torch._jit_internal.drop_on_export->kwargs.pop('drop_on_export', None)
A:torch._jit_internal.item->getattr(mod, name)
A:torch._jit_internal.attr->get_torchscript_modifier(orig)
A:torch._jit_internal.mod->get_torchscript_modifier(fn)
A:torch._jit_internal.qual_name->_qualified_name(method)
A:torch._jit_internal.fn_overload_list->_overloaded_fns.get(qual_name)
A:torch._jit_internal.current_frame->inspect.currentframe()
A:torch._jit_internal.class_name_map->_overloaded_methods.get(qual_name, None)
A:torch._jit_internal.(class_name, line_no)->get_class_name_lineno(func)
A:torch._jit_internal.method_overloads->_overloaded_methods.get(qual_name, None).get(class_name, None)
A:torch._jit_internal.overloads->_overloaded_methods.get(qual_name, None).get(mod_class.__name__, None)
A:torch._jit_internal.args->getattr(ann, '__args__', ())
A:torch._jit_internal.Tuple->TupleCls()
A:torch._jit_internal.List->ListCls()
A:torch._jit_internal.Dict->DictCls()
A:torch._jit_internal.Optional->DictCls()
A:torch._jit_internal.Any->AnyCls()
A:torch._jit_internal.Final->FinalCls()
A:torch._jit_internal.T->TypeVar('T')
A:torch._jit_internal.RRef->RRefCls()
A:torch._jit_internal.BroadcastingList1->BroadcastingListCls()
torch._jit_internal.BroadcastingListCls(object)
torch._jit_internal.BroadcastingListCls.__getitem__(self,types)
torch._jit_internal.FunctionModifiers(object)
torch._jit_internal.SourceContext(self,source,filename,file_lineno,leading_whitespace_len,uses_true_division=True)
torch._jit_internal.SourceContext.__init__(self,source,filename,file_lineno,leading_whitespace_len,uses_true_division=True)
torch._jit_internal._clear_fn_overloads(qual_name)
torch._jit_internal._copy_to_script_wrapper(fn)
torch._jit_internal._get_fn_overloads(qual_name)
torch._jit_internal._get_overloaded_methods(method,mod_class)
torch._jit_internal._overload(func)
torch._jit_internal._overload_method(func)
torch._jit_internal._qualified_name(obj)
torch._jit_internal.boolean_dispatch(arg_name,arg_index,default,if_true,if_false,module_name,func_name)
torch._jit_internal.can_compile_class(cls)
torch._jit_internal.copy_torchscript_modifier(orig,new)
torch._jit_internal.createResolutionCallbackForClassMethods(cls)
torch._jit_internal.createResolutionCallbackFromClosure(fn)
torch._jit_internal.createResolutionCallbackFromEnv(lookup_base)
torch._jit_internal.createResolutionCallbackFromFrame(frames_up=0)
torch._jit_internal.export(fn)
torch._jit_internal.fake_range()
torch._jit_internal.get_class_name_lineno(method)
torch._jit_internal.get_closure(fn)
torch._jit_internal.get_torchscript_modifier(fn)
torch._jit_internal.ignore(drop=False,**kwargs)
torch._jit_internal.is_ignored_fn(fn)
torch._jit_internal.module_has_exports(mod)
torch._jit_internal.should_drop(fn)
torch._jit_internal.unused(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/_tensor_str.py----------------------------------------
A:torch._tensor_str.PRINT_OPTS->__PrinterOptions()
A:torch._tensor_str.tensor_view->tensor.reshape(-1)
A:torch._tensor_str.value_str->'{{:.{}f}}'.format(PRINT_OPTS.precision).format(value)
A:torch._tensor_str.self.max_width->max(self.max_width, len(value_str))
A:torch._tensor_str.nonzero_finite_vals->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
A:torch._tensor_str.nonzero_finite_abs->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double()
A:torch._tensor_str.nonzero_finite_min->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().min().double()
A:torch._tensor_str.nonzero_finite_max->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().max().double()
A:torch._tensor_str.ret->'{}'.format(value)
A:torch._tensor_str.elements_per_line->max(1, int(math.floor((PRINT_OPTS.linewidth - indent) / element_length)))
A:torch._tensor_str.dim->self.float().dim()
A:torch._tensor_str.tensor_str->_tensor_str(self, indent)
A:torch._tensor_str.self->self.float().float()
A:torch._tensor_str.formatter->_Formatter(get_summarized_data(self) if summarize else self)
A:torch._tensor_str.suffix_len->len(suffix)
A:torch._tensor_str.indent->len(prefix)
A:torch._tensor_str.indices->self.float().float()._indices().detach()
A:torch._tensor_str.indices_str->_tensor_str(indices, indent + len(indices_prefix))
A:torch._tensor_str.values->self.float().float()._values().detach()
A:torch._tensor_str.values_str->_tensor_str(values, indent + len(values_prefix))
torch._tensor_str._Formatter(self,tensor)
torch._tensor_str._Formatter.__init__(self,tensor)
torch._tensor_str._Formatter.format(self,value)
torch._tensor_str._Formatter.width(self)
torch._tensor_str.__PrinterOptions(object)
torch._tensor_str._add_suffixes(tensor_str,suffixes,indent,force_newline)
torch._tensor_str._scalar_str(self,formatter)
torch._tensor_str._str(self)
torch._tensor_str._tensor_str(self,indent)
torch._tensor_str._tensor_str_with_formatter(self,indent,formatter,summarize)
torch._tensor_str._vector_str(self,indent,formatter,summarize)
torch._tensor_str.get_summarized_data(self)
torch._tensor_str.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)
torch.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/_utils.py----------------------------------------
A:torch.cuda._utils.device->torch.device(device)
torch.cuda._get_device_index(device,optional=False)
torch.cuda._utils._get_device_index(device,optional=False)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/nvtx.py----------------------------------------
A:torch.cuda.nvtx._nvtx->_NVTXStub()
torch.cuda.nvtx.mark(msg)
torch.cuda.nvtx.range_pop()
torch.cuda.nvtx.range_push(msg)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/profiler.py----------------------------------------
A:torch.cuda.profiler.rt->cudart()
torch.cuda.profiler.init(output_file,flags=None,output_mode='key_value')
torch.cuda.profiler.profile()
torch.cuda.profiler.start()
torch.cuda.profiler.stop()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/sparse.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/nccl.py----------------------------------------
A:torch.cuda.nccl.devices->set()
A:torch.cuda.nccl.device->tensor.get_device()
torch.cuda.nccl.all_gather(inputs,outputs,streams=None,comms=None)
torch.cuda.nccl.all_reduce(inputs,outputs=None,op=SUM,streams=None,comms=None)
torch.cuda.nccl.broadcast(inputs,root=0,streams=None,comms=None)
torch.cuda.nccl.init_rank(num_ranks,uid,rank)
torch.cuda.nccl.is_available(tensors)
torch.cuda.nccl.reduce(inputs,outputs=None,root=0,op=SUM,streams=None,comms=None)
torch.cuda.nccl.reduce_scatter(inputs,outputs,op=SUM,streams=None,comms=None)
torch.cuda.nccl.unique_id()
torch.cuda.nccl.version()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/__init__.py----------------------------------------
A:torch.cuda.__init__._tls->threading.local()
A:torch.cuda.__init__._initialization_lock->threading.Lock()
A:torch.cuda.__init__._is_in_bad_fork->getattr(torch._C, '_cuda_isInBadFork', lambda : False)
A:torch.cuda.__init__.CUDA_VERSION->torch._C._cuda_getCompiledVersion()
A:torch.cuda.__init__.capability->get_device_capability(d)
A:torch.cuda.__init__.name->get_device_name(d)
A:torch.cuda.__init__.msg->torch._C._cudart.cudaGetErrorString(code).decode('utf-8')
A:torch.cuda.__init__.self.idx->_get_device_index(device, optional=True)
A:torch.cuda.__init__.self.prev_idx->torch._C._cuda_getDevice()
A:torch.cuda.__init__.device->_get_device_index(device, optional=True)
A:torch.cuda.__init__.prop->get_device_properties(device)
A:torch.cuda.__init__.src_prev_stream->current_stream()
A:torch.cuda.__init__.dst_prev_stream->current_stream()
A:torch.cuda.__init__.storage_name->'Cuda{0}StorageBase'.format(t)
A:torch.cuda.__init__.tensor_name->'Cuda{0}TensorBase'.format(t)
A:torch.cuda.__init__.torch._C.__dict__[storage_name]->_dummy_type(storage_name)
A:torch.cuda.__init__.torch._C.__dict__[tensor_name]->_dummy_type(tensor_name)
A:torch.cuda.__init__.torch._C.__dict__['_CudaStreamBase']->_dummy_type('CudaStreamBase')
A:torch.cuda.__init__.torch._C.__dict__['_CudaEventBase']->_dummy_type('CudaEventBase')
torch.cuda.__init__.BFloat16Storage(_CudaBase,torch._C.CudaBFloat16StorageBase,_StorageBase)
torch.cuda.__init__.BoolStorage(_CudaBase,torch._C.CudaBoolStorageBase,_StorageBase)
torch.cuda.__init__.ByteStorage(_CudaBase,torch._C.CudaByteStorageBase,_StorageBase)
torch.cuda.__init__.CharStorage(_CudaBase,torch._C.CudaCharStorageBase,_StorageBase)
torch.cuda.__init__.CudaError(self,code)
torch.cuda.__init__.CudaError.__init__(self,code)
torch.cuda.__init__.DeferredCudaCallError(Exception)
torch.cuda.__init__.DoubleStorage(_CudaBase,torch._C.CudaDoubleStorageBase,_StorageBase)
torch.cuda.__init__.FloatStorage(_CudaBase,torch._C.CudaFloatStorageBase,_StorageBase)
torch.cuda.__init__.HalfStorage(_CudaBase,torch._C.CudaHalfStorageBase,_StorageBase)
torch.cuda.__init__.IntStorage(_CudaBase,torch._C.CudaIntStorageBase,_StorageBase)
torch.cuda.__init__.LongStorage(_CudaBase,torch._C.CudaLongStorageBase,_StorageBase)
torch.cuda.__init__.ShortStorage(_CudaBase,torch._C.CudaShortStorageBase,_StorageBase)
torch.cuda.__init__._CudaBase(object)
torch.cuda.__init__._CudaBase.type(self,*args,**kwargs)
torch.cuda.__init__._check_capability()
torch.cuda.__init__._check_driver()
torch.cuda.__init__._dummy_type(name)
torch.cuda.__init__._lazy_call(callable)
torch.cuda.__init__._lazy_init()
torch.cuda.__init__._lazy_new(cls,*args,**kwargs)
torch.cuda.__init__._sleep(cycles)
torch.cuda.__init__.check_error(res)
torch.cuda.__init__.cudaStatus(object)
torch.cuda.__init__.cudart()
torch.cuda.__init__.current_blas_handle()
torch.cuda.__init__.current_device()
torch.cuda.__init__.current_stream(device=None)
torch.cuda.__init__.default_stream(device=None)
torch.cuda.__init__.device(self,device)
torch.cuda.__init__.device.__enter__(self)
torch.cuda.__init__.device.__exit__(self,*args)
torch.cuda.__init__.device.__init__(self,device)
torch.cuda.__init__.device_count()
torch.cuda.__init__.device_of(self,obj)
torch.cuda.__init__.device_of.__init__(self,obj)
torch.cuda.__init__.get_device_capability(device=None)
torch.cuda.__init__.get_device_name(device=None)
torch.cuda.__init__.get_device_properties(device)
torch.cuda.__init__.init()
torch.cuda.__init__.ipc_collect()
torch.cuda.__init__.is_available()
torch.cuda.__init__.is_initialized()
torch.cuda.__init__.set_device(device)
torch.cuda.__init__.stream(stream)
torch.cuda.__init__.synchronize(device=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/__init__.pyi----------------------------------------
torch.cuda.__init__._CudaDeviceProperties
torch.cuda.__init__.empty_cache()->None
torch.cuda.__init__.get_rng_state()
torch.cuda.__init__.max_memory_allocated(device:Optional[_device_t]=...)->int
torch.cuda.__init__.max_memory_cached(device:Optional[_device_t]=...)->int
torch.cuda.__init__.memory_allocated(device:Optional[_device_t]=...)->int
torch.cuda.__init__.memory_cached(device:Optional[_device_t]=...)->int
torch.cuda.__init__.reset_max_memory_allocated(device:Optional[_device_t]=...)->None
torch.cuda.__init__.reset_max_memory_cached(device:Optional[_device_t]=...)->None
torch.cuda.__init__.set_rng_state(new_state)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/memory.py----------------------------------------
A:torch.cuda.memory.device->_get_device_index(device, optional=True)
A:torch.cuda.memory.stream->torch.cuda.current_stream(device)
A:torch.cuda.memory.stats->memory_stats(device=device)
torch.cuda._free_mutex()
torch.cuda._host_allocator()
torch.cuda.caching_allocator_alloc(size,device=None,stream=None)
torch.cuda.caching_allocator_delete(mem_ptr)
torch.cuda.empty_cache()
torch.cuda.max_memory_allocated(device=None)
torch.cuda.max_memory_cached(device=None)
torch.cuda.max_memory_reserved(device=None)
torch.cuda.memory._free_mutex()
torch.cuda.memory._host_allocator()
torch.cuda.memory.caching_allocator_alloc(size,device=None,stream=None)
torch.cuda.memory.caching_allocator_delete(mem_ptr)
torch.cuda.memory.empty_cache()
torch.cuda.memory.max_memory_allocated(device=None)
torch.cuda.memory.max_memory_cached(device=None)
torch.cuda.memory.max_memory_reserved(device=None)
torch.cuda.memory.memory_allocated(device=None)
torch.cuda.memory.memory_cached(device=None)
torch.cuda.memory.memory_reserved(device=None)
torch.cuda.memory.memory_snapshot()
torch.cuda.memory.memory_stats(device=None)
torch.cuda.memory.memory_stats_as_nested_dict(device=None)
torch.cuda.memory.memory_summary(device=None,abbreviated=False)
torch.cuda.memory.reset_accumulated_memory_stats(device=None)
torch.cuda.memory.reset_max_memory_allocated(device=None)
torch.cuda.memory.reset_max_memory_cached(device=None)
torch.cuda.memory.reset_peak_memory_stats(device=None)
torch.cuda.memory_allocated(device=None)
torch.cuda.memory_cached(device=None)
torch.cuda.memory_reserved(device=None)
torch.cuda.memory_snapshot()
torch.cuda.memory_stats(device=None)
torch.cuda.memory_stats_as_nested_dict(device=None)
torch.cuda.memory_summary(device=None,abbreviated=False)
torch.cuda.reset_accumulated_memory_stats(device=None)
torch.cuda.reset_max_memory_allocated(device=None)
torch.cuda.reset_max_memory_cached(device=None)
torch.cuda.reset_peak_memory_stats(device=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/comm.py----------------------------------------
A:torch.cuda.comm.destination->torch.cuda.current_device()
A:torch.cuda.comm.input_size->inputs[0].size()
A:torch.cuda.comm.got->'x'.join((str(x) for x in inp.size()))
A:torch.cuda.comm.expected->'x'.join((str(x) for x in input_size))
A:torch.cuda.comm.result->reduce_add(tensor_at_gpus, destination)
A:torch.cuda.comm.input_correct_gpu->inp.cuda(result.get_device())
A:torch.cuda.comm.flat_result->reduce_add(flat_tensors, destination)
torch.cuda.comm.broadcast(tensor,devices)
torch.cuda.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)
torch.cuda.comm.gather(tensors,dim=0,destination=None)
torch.cuda.comm.reduce_add(inputs,destination=None)
torch.cuda.comm.reduce_add_coalesced(inputs,destination=None,buffer_size=10485760)
torch.cuda.comm.scatter(tensor,devices,chunk_sizes=None,dim=0,streams=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/streams.py----------------------------------------
A:torch.cuda.streams.event->Event()
A:torch.cuda.streams.stream->torch.cuda.current_stream()
torch.cuda.Event(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.Event.__repr__(self)
torch.cuda.Event._as_parameter_(self)
torch.cuda.Event.elapsed_time(self,end_event)
torch.cuda.Event.from_ipc_handle(cls,device,handle)
torch.cuda.Event.ipc_handle(self)
torch.cuda.Event.query(self)
torch.cuda.Event.record(self,stream=None)
torch.cuda.Event.synchronize(self)
torch.cuda.Event.wait(self,stream=None)
torch.cuda.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.Stream.__eq__(self,o)
torch.cuda.Stream.__hash__(self)
torch.cuda.Stream.__repr__(self)
torch.cuda.Stream._as_parameter_(self)
torch.cuda.Stream.query(self)
torch.cuda.Stream.record_event(self,event=None)
torch.cuda.Stream.synchronize(self)
torch.cuda.Stream.wait_event(self,event)
torch.cuda.Stream.wait_stream(self,stream)
torch.cuda.streams.Event(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.streams.Event.__new__(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.streams.Event.__repr__(self)
torch.cuda.streams.Event._as_parameter_(self)
torch.cuda.streams.Event.elapsed_time(self,end_event)
torch.cuda.streams.Event.from_ipc_handle(cls,device,handle)
torch.cuda.streams.Event.ipc_handle(self)
torch.cuda.streams.Event.query(self)
torch.cuda.streams.Event.record(self,stream=None)
torch.cuda.streams.Event.synchronize(self)
torch.cuda.streams.Event.wait(self,stream=None)
torch.cuda.streams.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__eq__(self,o)
torch.cuda.streams.Stream.__hash__(self)
torch.cuda.streams.Stream.__new__(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__repr__(self)
torch.cuda.streams.Stream._as_parameter_(self)
torch.cuda.streams.Stream.query(self)
torch.cuda.streams.Stream.record_event(self,event=None)
torch.cuda.streams.Stream.synchronize(self)
torch.cuda.streams.Stream.wait_event(self,event)
torch.cuda.streams.Stream.wait_stream(self,stream)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/error.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/random.py----------------------------------------
A:torch.cuda.random.device->torch.device('cuda', device)
A:torch.cuda.random.idx->current_device()
A:torch.cuda.random.new_state_copy->new_state.clone(memory_format=torch.contiguous_format)
A:torch.cuda.random.seed->int(seed)
A:torch.cuda.random.random_seed->default_generator.initial_seed()
torch.cuda.get_rng_state(device='cuda')
torch.cuda.get_rng_state_all()
torch.cuda.initial_seed()
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.cuda.random.get_rng_state(device='cuda')
torch.cuda.random.get_rng_state_all()
torch.cuda.random.initial_seed()
torch.cuda.random.manual_seed(seed)
torch.cuda.random.manual_seed_all(seed)
torch.cuda.random.seed()
torch.cuda.random.seed_all()
torch.cuda.random.set_rng_state(new_state,device='cuda')
torch.cuda.random.set_rng_state_all(new_states)
torch.cuda.seed()
torch.cuda.seed_all()
torch.cuda.set_rng_state(new_state,device='cuda')
torch.cuda.set_rng_state_all(new_states)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/amp/grad_scaler.py----------------------------------------
A:torch.cuda.amp.grad_scaler.retval->optimizer.step(*args, **kwargs)
A:torch.cuda.amp.grad_scaler.self._per_optimizer_states->defaultdict(lambda : {'stage': self.READY, 'found_inf_per_device': {}})
A:torch.cuda.amp.grad_scaler.self._scale->torch._amp_update_scale(self._growth_tracker, self._scale, found_inf_combined, self._growth_factor, self._backoff_factor, self._growth_interval)
A:torch.cuda.amp.grad_scaler.self._growth_tracker->torch.full((1,), self._init_growth_tracker, dtype=torch.int32, device=dev)
A:torch.cuda.amp.grad_scaler.stash[0]->_MultiDeviceReplicator(self._scale)
A:torch.cuda.amp.grad_scaler.per_device_inv_scale->_MultiDeviceReplicator(inv_scale)
A:torch.cuda.amp.grad_scaler.per_device_found_inf->_MultiDeviceReplicator(found_inf)
A:torch.cuda.amp.grad_scaler.inv_scale->self._scale.double().reciprocal().float()
A:torch.cuda.amp.grad_scaler.found_inf->torch.full((1,), 0.0, dtype=torch.float32, device=self._scale.device)
A:torch.cuda.amp.grad_scaler.optimizer_state['found_inf_per_device']->self._unscale_grads_(optimizer, inv_scale, found_inf, False)
A:torch.cuda.amp.grad_scaler.dummy_inv_scale->torch.full((1,), 1.0, dtype=torch.float32, device=self._scale.device)
A:torch.cuda.amp.grad_scaler.self._per_optimizer_states[id(optimizer)]['found_inf_per_device']->self._unscale_grads_(optimizer, dummy_inv_scale, found_inf, True)
torch.cuda.amp.GradScaler(self,init_scale=2.0**16,growth_factor=2.0,backoff_factor=0.5,growth_interval=2000,enabled=True)
torch.cuda.amp.GradScaler._check_inf_per_device(self,optimizer)
torch.cuda.amp.GradScaler._check_scale_growth_tracker(self,funcname)
torch.cuda.amp.GradScaler._found_inf_per_device(self,optimizer)
torch.cuda.amp.GradScaler._get_growth_tracker(self)
torch.cuda.amp.GradScaler._get_scale_async(self)
torch.cuda.amp.GradScaler._lazy_init_scale_growth_tracker(self,dev)
torch.cuda.amp.GradScaler._unscale_grads_(self,optimizer,inv_scale,found_inf,allow_fp16)
torch.cuda.amp.GradScaler.get_backoff_factor(self)
torch.cuda.amp.GradScaler.get_growth_factor(self)
torch.cuda.amp.GradScaler.get_growth_interval(self)
torch.cuda.amp.GradScaler.get_scale(self)
torch.cuda.amp.GradScaler.is_enabled(self)
torch.cuda.amp.GradScaler.load_state_dict(self,state_dict)
torch.cuda.amp.GradScaler.scale(self,outputs)
torch.cuda.amp.GradScaler.set_backoff_factor(self,new_factor)
torch.cuda.amp.GradScaler.set_growth_factor(self,new_factor)
torch.cuda.amp.GradScaler.set_growth_interval(self,new_interval)
torch.cuda.amp.GradScaler.state_dict(self)
torch.cuda.amp.GradScaler.step(self,optimizer,*args,**kwargs)
torch.cuda.amp.GradScaler.unscale_(self,optimizer)
torch.cuda.amp.GradScaler.update(self,new_scale=None)
torch.cuda.amp.grad_scaler.GradScaler(self,init_scale=2.0**16,growth_factor=2.0,backoff_factor=0.5,growth_interval=2000,enabled=True)
torch.cuda.amp.grad_scaler.GradScaler.__init__(self,init_scale=2.0**16,growth_factor=2.0,backoff_factor=0.5,growth_interval=2000,enabled=True)
torch.cuda.amp.grad_scaler.GradScaler._check_inf_per_device(self,optimizer)
torch.cuda.amp.grad_scaler.GradScaler._check_scale_growth_tracker(self,funcname)
torch.cuda.amp.grad_scaler.GradScaler._found_inf_per_device(self,optimizer)
torch.cuda.amp.grad_scaler.GradScaler._get_growth_tracker(self)
torch.cuda.amp.grad_scaler.GradScaler._get_scale_async(self)
torch.cuda.amp.grad_scaler.GradScaler._lazy_init_scale_growth_tracker(self,dev)
torch.cuda.amp.grad_scaler.GradScaler._unscale_grads_(self,optimizer,inv_scale,found_inf,allow_fp16)
torch.cuda.amp.grad_scaler.GradScaler.get_backoff_factor(self)
torch.cuda.amp.grad_scaler.GradScaler.get_growth_factor(self)
torch.cuda.amp.grad_scaler.GradScaler.get_growth_interval(self)
torch.cuda.amp.grad_scaler.GradScaler.get_scale(self)
torch.cuda.amp.grad_scaler.GradScaler.is_enabled(self)
torch.cuda.amp.grad_scaler.GradScaler.load_state_dict(self,state_dict)
torch.cuda.amp.grad_scaler.GradScaler.scale(self,outputs)
torch.cuda.amp.grad_scaler.GradScaler.set_backoff_factor(self,new_factor)
torch.cuda.amp.grad_scaler.GradScaler.set_growth_factor(self,new_factor)
torch.cuda.amp.grad_scaler.GradScaler.set_growth_interval(self,new_interval)
torch.cuda.amp.grad_scaler.GradScaler.state_dict(self)
torch.cuda.amp.grad_scaler.GradScaler.step(self,optimizer,*args,**kwargs)
torch.cuda.amp.grad_scaler.GradScaler.unscale_(self,optimizer)
torch.cuda.amp.grad_scaler.GradScaler.update(self,new_scale=None)
torch.cuda.amp.grad_scaler._MultiDeviceReplicator(self,master_tensor)
torch.cuda.amp.grad_scaler._MultiDeviceReplicator.__init__(self,master_tensor)
torch.cuda.amp.grad_scaler._MultiDeviceReplicator.get(self,device)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/cuda/amp/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/collect_env.py----------------------------------------
A:torch.utils.collect_env.SystemEnv->namedtuple('SystemEnv', ['torch_version', 'is_debug_build', 'cuda_compiled_version', 'gcc_version', 'cmake_version', 'os', 'python_version', 'is_cuda_available', 'cuda_runtime_version', 'nvidia_driver_version', 'nvidia_gpu_models', 'cudnn_version', 'pip_version', 'pip_packages', 'conda_packages'])
A:torch.utils.collect_env.p->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
A:torch.utils.collect_env.(output, err)->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()
A:torch.utils.collect_env.enc->locale.getpreferredencoding()
A:torch.utils.collect_env.output->get_pretty_env_info()
A:torch.utils.collect_env.err->err.decode(enc).decode(enc)
A:torch.utils.collect_env.(rc, out, _)->run_lambda(cudnn_cmd)
A:torch.utils.collect_env.match->re.search(regex, out)
A:torch.utils.collect_env.conda->os.environ.get('CONDA_EXE', 'conda')
A:torch.utils.collect_env.out->run_and_read_all(run_lambda, conda + ' list | ' + grep_cmd)
A:torch.utils.collect_env.comment_regex->re.compile('^#.*\\n')
A:torch.utils.collect_env.smi->get_nvidia_smi()
A:torch.utils.collect_env.uuid_regex->re.compile(' \\(UUID: .+?\\)')
A:torch.utils.collect_env.l->os.environ.get('CUDNN_LIBRARY')
A:torch.utils.collect_env.files->list(sorted(files))
A:torch.utils.collect_env.fn->os.path.realpath(fn)
A:torch.utils.collect_env.result->'\n'.join(files)
A:torch.utils.collect_env.platform->get_platform()
A:torch.utils.collect_env.version->get_mac_version(run_lambda)
A:torch.utils.collect_env.desc->check_release_file(run_lambda)
A:torch.utils.collect_env.out2->run_with_pip('pip')
A:torch.utils.collect_env.out3->run_with_pip('pip3')
A:torch.utils.collect_env.num_pips->len([x for x in [out2, out3] if x is not None])
A:torch.utils.collect_env.(pip_version, pip_list_output)->get_pip_packages(run_lambda)
A:torch.utils.collect_env.cuda_available_str->torch.cuda.is_available()
A:torch.utils.collect_env.env_info_fmt->'\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\n\nOS: {os}\nGCC version: {gcc_version}\nCMake version: {cmake_version}\n\nPython version: {python_version}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n'.strip()
A:torch.utils.collect_env.lines->text.split('\n')
A:torch.utils.collect_env.mutable_dict->replace_nones(mutable_dict)
A:torch.utils.collect_env.mutable_dict['nvidia_gpu_models']->maybe_start_on_next_line(envinfo.nvidia_gpu_models)
A:torch.utils.collect_env.all_dynamic_cuda_fields_missing->all((mutable_dict[field] is None for field in dynamic_cuda_fields))
A:torch.utils.collect_env.mutable_dict['pip_packages']->prepend(mutable_dict['pip_packages'], '[{}] '.format(envinfo.pip_version))
A:torch.utils.collect_env.mutable_dict['conda_packages']->prepend(mutable_dict['conda_packages'], '[conda] ')
torch.utils.collect_env.check_release_file(run_lambda)
torch.utils.collect_env.get_cmake_version(run_lambda)
torch.utils.collect_env.get_conda_packages(run_lambda)
torch.utils.collect_env.get_cudnn_version(run_lambda)
torch.utils.collect_env.get_env_info()
torch.utils.collect_env.get_gcc_version(run_lambda)
torch.utils.collect_env.get_gpu_info(run_lambda)
torch.utils.collect_env.get_lsb_version(run_lambda)
torch.utils.collect_env.get_mac_version(run_lambda)
torch.utils.collect_env.get_nvidia_driver_version(run_lambda)
torch.utils.collect_env.get_nvidia_smi()
torch.utils.collect_env.get_os(run_lambda)
torch.utils.collect_env.get_pip_packages(run_lambda)
torch.utils.collect_env.get_platform()
torch.utils.collect_env.get_pretty_env_info()
torch.utils.collect_env.get_running_cuda_version(run_lambda)
torch.utils.collect_env.get_windows_version(run_lambda)
torch.utils.collect_env.main()
torch.utils.collect_env.pretty_str(envinfo)
torch.utils.collect_env.run(command)
torch.utils.collect_env.run_and_parse_first_match(run_lambda,command,regex)
torch.utils.collect_env.run_and_read_all(run_lambda,command)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/cpp_extension.py----------------------------------------
A:torch.utils.cpp_extension.nvcc->_join_cuda_home('bin', 'nvcc')
A:torch.utils.cpp_extension.cuda_home->os.path.dirname(os.path.dirname(nvcc))
A:torch.utils.cpp_extension.cuda_homes->glob.glob('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')
A:torch.utils.cpp_extension.hipcc->subprocess.check_output(['which', 'hipcc']).decode().rstrip('\r\n')
A:torch.utils.cpp_extension.rocm_home->os.path.dirname(rocm_home)
A:torch.utils.cpp_extension.ROCM_HOME->_find_rocm_home()
A:torch.utils.cpp_extension.CUDA_HOME->_find_cuda_home()
A:torch.utils.cpp_extension.BUILT_FROM_SOURCE_VERSION_PATTERN->re.compile('\\d+\\.\\d+\\.\\d+\\w+\\+\\w+')
A:torch.utils.cpp_extension.JIT_EXTENSION_VERSIONER->ExtensionVersioner()
A:torch.utils.cpp_extension.which->subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.compiler_path->os.path.realpath(which.decode().strip())
A:torch.utils.cpp_extension.version->ExtensionVersioner().bump_version_if_changed(name, sources, build_arguments=[extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths], build_directory=build_directory, with_cuda=with_cuda)
A:torch.utils.cpp_extension.compiler_info->subprocess.check_output(compiler, stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.match->re.search('(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip())
A:torch.utils.cpp_extension.(_, error, _)->sys.exc_info()
A:torch.utils.cpp_extension.compiler->os.environ.get('CXX', 'c++')
A:torch.utils.cpp_extension.kwargs->kwargs.copy().copy()
A:torch.utils.cpp_extension.self.no_python_abi_suffix->kwargs.copy().copy().get('no_python_abi_suffix', False)
A:torch.utils.cpp_extension.self.use_ninja->kwargs.copy().copy().get('use_ninja', False if IS_HIP_EXTENSION else True)
A:torch.utils.cpp_extension.cpp_flag_prefix->cpp_format_prefix.format('std')
A:torch.utils.cpp_extension.paths[i]->os.path.abspath(paths[i])
A:torch.utils.cpp_extension.cflags->sanitize_flags(cflags)
A:torch.utils.cpp_extension.output_dir->os.path.abspath(output_dir)
A:torch.utils.cpp_extension.(_, objects, extra_postargs, pp_opts, _)->self.compiler._setup_compile(output_dir, macros, include_dirs, sources, depends, extra_postargs)
A:torch.utils.cpp_extension.common_cflags->self.compiler._get_cc_args(pp_opts, debug, extra_preargs)
A:torch.utils.cpp_extension.with_cuda->any(map(_is_cuda_file, sources))
A:torch.utils.cpp_extension.post_cflags->sanitize_flags(post_cflags)
A:torch.utils.cpp_extension.cuda_post_cflags->sanitize_flags(cuda_post_cflags)
A:torch.utils.cpp_extension.self.cflags->copy.deepcopy(extra_postargs)
A:torch.utils.cpp_extension.src_regex->re.compile('/T(p|c)(.*)')
A:torch.utils.cpp_extension.obj_regex->re.compile('/Fo(.*)')
A:torch.utils.cpp_extension.include_regex->re.compile('((\\-|\\/)I.*)')
A:torch.utils.cpp_extension.cuda_cflags->sanitize_flags(cuda_cflags)
A:torch.utils.cpp_extension.ext_filename->'.'.join(without_abi)
A:torch.utils.cpp_extension.ext_filename_parts->'.'.join(without_abi).split('.')
A:torch.utils.cpp_extension.extension.extra_compile_args->copy.deepcopy(extension.extra_compile_args)
A:torch.utils.cpp_extension.names->extension.name.split('.')
A:torch.utils.cpp_extension.define->'-DTORCH_EXTENSION_NAME={}'.format(name)
A:torch.utils.cpp_extension.include_dirs->kwargs.copy().copy().get('include_dirs', [])
A:torch.utils.cpp_extension.library_dirs->kwargs.copy().copy().get('library_dirs', [])
A:torch.utils.cpp_extension.libraries->kwargs.copy().copy().get('libraries', [])
A:torch.utils.cpp_extension.here->os.path.abspath(__file__)
A:torch.utils.cpp_extension.torch_path->os.path.dirname(os.path.dirname(here))
A:torch.utils.cpp_extension.lib_include->os.path.join(torch_path, 'include')
A:torch.utils.cpp_extension.cuda_home_include->_join_cuda_home('include')
A:torch.utils.cpp_extension.lib_path->os.path.join(torch_path, 'lib')
A:torch.utils.cpp_extension.functions->dict(((f, f) for f in functions))
A:torch.utils.cpp_extension.cpp_source_path->os.path.join(build_directory, 'main.cpp')
A:torch.utils.cpp_extension.cuda_source_path->os.path.join(build_directory, 'cuda.cu')
A:torch.utils.cpp_extension.old_version->ExtensionVersioner().get_version(name)
A:torch.utils.cpp_extension.name->'{}_v{}'.format(name, version)
A:torch.utils.cpp_extension.baton->FileBaton(os.path.join(build_directory, 'lock'))
A:torch.utils.cpp_extension.build_file_path->os.path.join(build_directory, 'build.ninja')
A:torch.utils.cpp_extension.extra_ldflags->_prepare_ldflags(extra_ldflags or [], with_cuda, verbose)
A:torch.utils.cpp_extension.python_path->os.path.dirname(sys.executable)
A:torch.utils.cpp_extension.python_lib_path->os.path.join(python_path, 'libs')
A:torch.utils.cpp_extension.named_arches->collections.OrderedDict([('Kepler+Tesla', '3.7'), ('Kepler', '3.5+PTX'), ('Maxwell+Tegra', '5.3'), ('Maxwell', '5.0;5.2+PTX'), ('Pascal', '6.0;6.1+PTX'), ('Volta', '7.0+PTX'), ('Turing', '7.5+PTX')])
A:torch.utils.cpp_extension.arch_list->arch_list.split(';').split(';')
A:torch.utils.cpp_extension.capability->torch.cuda.get_device_capability()
A:torch.utils.cpp_extension.root_extensions_directory->get_default_build_root()
A:torch.utils.cpp_extension.build_directory->os.path.join(root_extensions_directory, name)
A:torch.utils.cpp_extension.max_jobs->os.environ.get('MAX_JOBS')
A:torch.utils.cpp_extension.num_workers->_get_num_workers(verbose)
A:torch.utils.cpp_extension.(file, path, description)->imp.find_module(module_name, [path])
A:torch.utils.cpp_extension.system_includes->include_paths(with_cuda)
A:torch.utils.cpp_extension.cuda_flags->_nt_quote_args(cuda_flags)
A:torch.utils.cpp_extension.target->'{}.o'.format(file_name)
A:torch.utils.cpp_extension.objects->list(map(object_file_path, sources))
A:torch.utils.cpp_extension.ldflags->sanitize_flags(ldflags)
A:torch.utils.cpp_extension.library_target->'{}.{}'.format(name, ext)
A:torch.utils.cpp_extension.source_file->source_file.replace(' ', '$ ').replace(' ', '$ ')
A:torch.utils.cpp_extension.object_file->object_file.replace(' ', '$ ').replace(' ', '$ ')
A:torch.utils.cpp_extension.cl_paths->subprocess.check_output(['where', 'cl']).decode().split('\r\n')
A:torch.utils.cpp_extension.cl_path->os.path.dirname(cl_paths[0]).replace(':', '$:')
A:torch.utils.cpp_extension.lines->'\n'.join(block)
torch.utils.cpp_extension.BuildExtension(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension.__init__(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension._add_compile_flag(self,extension,flag)
torch.utils.cpp_extension.BuildExtension._add_gnu_cpp_abi_flag(self,extension)
torch.utils.cpp_extension.BuildExtension._check_abi(self)
torch.utils.cpp_extension.BuildExtension._define_torch_extension_name(self,extension)
torch.utils.cpp_extension.BuildExtension.build_extensions(self)
torch.utils.cpp_extension.BuildExtension.get_ext_filename(self,ext_name)
torch.utils.cpp_extension.BuildExtension.with_options(cls,**options)
torch.utils.cpp_extension.CUDAExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension.CppExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension._accepted_compilers_for_platform()
torch.utils.cpp_extension._find_cuda_home()
torch.utils.cpp_extension._find_rocm_home()
torch.utils.cpp_extension._get_build_directory(name,verbose)
torch.utils.cpp_extension._get_cuda_arch_flags(cflags=None)
torch.utils.cpp_extension._get_num_workers(verbose)
torch.utils.cpp_extension._get_rocm_arch_flags(cflags=None)
torch.utils.cpp_extension._import_module_from_library(module_name,path,is_python_module)
torch.utils.cpp_extension._is_binary_build()
torch.utils.cpp_extension._is_cuda_file(path)
torch.utils.cpp_extension._is_ninja_available()
torch.utils.cpp_extension._jit_compile(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory,verbose,with_cuda,is_python_module)
torch.utils.cpp_extension._join_cuda_home(*paths)
torch.utils.cpp_extension._join_rocm_home(*paths)
torch.utils.cpp_extension._prepare_ldflags(extra_ldflags,with_cuda,verbose)
torch.utils.cpp_extension._run_ninja_build(build_directory,verbose,error_prefix)
torch.utils.cpp_extension._write_ninja_file(path,cflags,post_cflags,cuda_cflags,cuda_post_cflags,sources,objects,ldflags,library_target,with_cuda)
torch.utils.cpp_extension._write_ninja_file_and_build_library(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory,verbose,with_cuda)
torch.utils.cpp_extension._write_ninja_file_and_compile_objects(sources,objects,cflags,post_cflags,cuda_cflags,cuda_post_cflags,build_directory,verbose,with_cuda)
torch.utils.cpp_extension._write_ninja_file_to_build_library(path,name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,with_cuda)
torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)
torch.utils.cpp_extension.check_compiler_ok_for_platform(compiler)
torch.utils.cpp_extension.get_default_build_root()
torch.utils.cpp_extension.include_paths(cuda=False)
torch.utils.cpp_extension.library_paths(cuda=False)
torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)
torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True,with_pytorch_error_handling=True)
torch.utils.cpp_extension.verify_ninja_availability()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/hooks.py----------------------------------------
A:torch.utils.hooks.self.hooks_dict_ref->weakref.ref(state[0])
A:torch.utils.hooks.hooks_dict->self.hooks_dict_ref()
A:torch.utils.hooks.RemovableHandle.next_id->max(RemovableHandle.next_id, self.id + 1)
torch.utils.hooks.RemovableHandle(self,hooks_dict)
torch.utils.hooks.RemovableHandle.__enter__(self)
torch.utils.hooks.RemovableHandle.__exit__(self,type,value,tb)
torch.utils.hooks.RemovableHandle.__getstate__(self)
torch.utils.hooks.RemovableHandle.__init__(self,hooks_dict)
torch.utils.hooks.RemovableHandle.__setstate__(self,state)
torch.utils.hooks.RemovableHandle.remove(self)
torch.utils.hooks.unserializable_hook(f)
torch.utils.hooks.warn_if_has_hooks(tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/checkpoint.py----------------------------------------
A:torch.utils.checkpoint.x->inp.detach()
A:torch.utils.checkpoint.fwd_gpu_devices->list(set((arg.get_device() for arg in args if isinstance(arg, torch.Tensor) and arg.is_cuda)))
A:torch.utils.checkpoint.ctx.fwd_cpu_state->torch.get_rng_state()
A:torch.utils.checkpoint.(ctx.fwd_gpu_devices, ctx.fwd_gpu_states)->get_device_states(*args)
A:torch.utils.checkpoint.outputs->ctx.run_function(*detached_inputs)
A:torch.utils.checkpoint.detached_inputs->detach_variable(inputs)
A:torch.utils.checkpoint.grads->tuple((inp.grad if isinstance(inp, torch.Tensor) else inp for inp in detached_inputs))
A:torch.utils.checkpoint.preserve->kwargs.pop('preserve_rng_state', True)
A:torch.utils.checkpoint.input->checkpoint(run_function(start, end, functions), input, preserve_rng_state=preserve)
A:torch.utils.checkpoint.functions->list(functions.children())
torch.utils.checkpoint.CheckpointFunction(torch.autograd.Function)
torch.utils.checkpoint.CheckpointFunction.backward(ctx,*args)
torch.utils.checkpoint.CheckpointFunction.forward(ctx,run_function,preserve_rng_state,*args)
torch.utils.checkpoint.check_backward_validity(inputs)
torch.utils.checkpoint.checkpoint(function,*args,**kwargs)
torch.utils.checkpoint.checkpoint_sequential(functions,segments,input,**kwargs)
torch.utils.checkpoint.detach_variable(inputs)
torch.utils.checkpoint.get_device_states(*args)
torch.utils.checkpoint.set_device_states(devices,states)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/throughput_benchmark.py----------------------------------------
A:torch.utils.throughput_benchmark.self._benchmark->torch._C.ThroughputBenchmark(module)
A:torch.utils.throughput_benchmark.config->torch._C.BenchmarkConfig()
A:torch.utils.throughput_benchmark.c_stats->self._benchmark.benchmark(config)
torch.utils.ThroughputBenchmark(self,module)
torch.utils.ThroughputBenchmark.add_input(self,*args,**kwargs)
torch.utils.ThroughputBenchmark.benchmark(self,num_calling_threads=1,num_warmup_iters=10,num_iters=100)
torch.utils.ThroughputBenchmark.run_once(self,*args,**kwargs)
torch.utils.throughput_benchmark.ExecutionStats(self,c_stats,benchmark_config)
torch.utils.throughput_benchmark.ExecutionStats.__init__(self,c_stats,benchmark_config)
torch.utils.throughput_benchmark.ExecutionStats.__str__(self)
torch.utils.throughput_benchmark.ExecutionStats.iters_per_second(self)
torch.utils.throughput_benchmark.ExecutionStats.latency_avg_ms(self)
torch.utils.throughput_benchmark.ExecutionStats.num_iters(self)
torch.utils.throughput_benchmark.ExecutionStats.total_time_seconds(self)
torch.utils.throughput_benchmark.ThroughputBenchmark(self,module)
torch.utils.throughput_benchmark.ThroughputBenchmark.__init__(self,module)
torch.utils.throughput_benchmark.ThroughputBenchmark.add_input(self,*args,**kwargs)
torch.utils.throughput_benchmark.ThroughputBenchmark.benchmark(self,num_calling_threads=1,num_warmup_iters=10,num_iters=100)
torch.utils.throughput_benchmark.ThroughputBenchmark.run_once(self,*args,**kwargs)
torch.utils.throughput_benchmark.format_time(time_us=None,time_ms=None,time_s=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/file_baton.py----------------------------------------
A:torch.utils.file_baton.self.fd->os.open(self.lock_file_path, os.O_CREAT | os.O_EXCL)
torch.utils.file_baton.FileBaton(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.__init__(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.release(self)
torch.utils.file_baton.FileBaton.try_acquire(self)
torch.utils.file_baton.FileBaton.wait(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/dlpack.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/_cpp_extension_versioner.py----------------------------------------
A:torch.utils._cpp_extension_versioner.Entry->collections.namedtuple('Entry', 'version, hash')
A:torch.utils._cpp_extension_versioner.hash_value->update_hash(hash_value, with_cuda)
A:torch.utils._cpp_extension_versioner.entry->self.entries.get(name)
A:torch.utils._cpp_extension_versioner.self.entries[name]entry->Entry(entry.version + 1, hash_value)
torch.utils._cpp_extension_versioner.ExtensionVersioner(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.__init__(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.bump_version_if_changed(self,name,source_files,build_arguments,build_directory,with_cuda)
torch.utils._cpp_extension_versioner.ExtensionVersioner.get_version(self,name)
torch.utils._cpp_extension_versioner.hash_build_arguments(hash_value,build_arguments)
torch.utils._cpp_extension_versioner.hash_source_files(hash_value,source_files)
torch.utils._cpp_extension_versioner.update_hash(seed,value)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/__init__.py----------------------------------------
torch.utils.__init__.set_module(obj,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/model_zoo.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/mkldnn.py----------------------------------------
A:torch.utils.mkldnn.self.weight->state[0].to_mkldnn()
A:torch.utils.mkldnn.self.bias->state[1].to_mkldnn()
A:torch.utils.mkldnn.y_mkldnn->torch._C._nn.mkldnn_linear(x_mkldnn, self.weight, self.bias)
A:torch.utils.mkldnn.weight->self.weight.to_dense()
A:torch.utils.mkldnn.bias->self.bias.to_dense()
A:torch.utils.mkldnn.running_mean->self.running_mean.to_dense()
A:torch.utils.mkldnn.running_var->self.running_var.to_dense()
A:torch.utils.mkldnn.self.running_mean->state[2].to_mkldnn()
A:torch.utils.mkldnn.self.running_var->state[3].to_mkldnn()
A:torch.utils.mkldnn.new_m->m_fn(m)
torch.utils.mkldnn.MkldnnBatchNorm2d(self,dense_module)
torch.utils.mkldnn.MkldnnBatchNorm2d.__getstate__(self)
torch.utils.mkldnn.MkldnnBatchNorm2d.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnBatchNorm2d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnBatchNorm2d.forward(self,x)
torch.utils.mkldnn.MkldnnConv2d(self,dense_module)
torch.utils.mkldnn.MkldnnConv2d.__getstate__(self)
torch.utils.mkldnn.MkldnnConv2d.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnConv2d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnConv2d.forward(self,x)
torch.utils.mkldnn.MkldnnLinear(self,dense_module)
torch.utils.mkldnn.MkldnnLinear.__getstate__(self)
torch.utils.mkldnn.MkldnnLinear.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnLinear.__setstate__(self,state)
torch.utils.mkldnn.MkldnnLinear.forward(self,x)
torch.utils.mkldnn.to_mkldnn(module)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/hipify/cuda_to_hip_mappings.py----------------------------------------
A:torch.utils.hipify.cuda_to_hip_mappings.MATH_TRANSPILATIONS->collections.OrderedDict([('std::max', '::max'), ('std::min', '::min'), ('std::ceil', '::ceil'), ('std::floor', '::floor'), ('std::exp', '::exp'), ('std::log', '::log'), ('std::pow', '::pow'), ('std::fabs', '::fabs'), ('std::fmod', '::fmod'), ('std::remainder', '::remainder')])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_TYPE_NAME_MAP->collections.OrderedDict([('CUresult', ('hipError_t', CONV_TYPE, API_DRIVER)), ('cudaError_t', ('hipError_t', CONV_TYPE, API_RUNTIME)), ('CUDA_ARRAY3D_DESCRIPTOR', ('HIP_ARRAY3D_DESCRIPTOR', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY_DESCRIPTOR', ('HIP_ARRAY_DESCRIPTOR', CONV_TYPE, API_DRIVER)), ('CUDA_MEMCPY2D', ('hip_Memcpy2D', CONV_TYPE, API_DRIVER)), ('CUDA_MEMCPY3D', ('HIP_MEMCPY3D', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_MEMCPY3D_PEER', ('HIP_MEMCPY3D_PEER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_POINTER_ATTRIBUTE_P2P_TOKENS', ('HIP_POINTER_ATTRIBUTE_P2P_TOKENS', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_RESOURCE_DESC', ('HIP_RESOURCE_DESC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_RESOURCE_VIEW_DESC', ('HIP_RESOURCE_VIEW_DESC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUipcEventHandle', ('hipIpcEventHandle', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUipcMemHandle', ('hipIpcMemHandle', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUaddress_mode', ('hipAddress_mode', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUarray_cubemap_face', ('hipArray_cubemap_face', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUarray_format', ('hipArray_format', CONV_TYPE, API_DRIVER)), ('CUcomputemode', ('hipComputemode', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmem_advise', ('hipMemAdvise', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmem_range_attribute', ('hipMemRangeAttribute', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUctx_flags', ('hipCctx_flags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUdevice', ('hipDevice_t', CONV_TYPE, API_DRIVER)), ('CUdevice_attribute_enum', ('hipDeviceAttribute_t', CONV_TYPE, API_DRIVER)), ('CUdevice_attribute', ('hipDeviceAttribute_t', CONV_TYPE, API_DRIVER)), ('CUdeviceptr', ('hipDeviceptr_t', CONV_TYPE, API_DRIVER)), ('CUarray_st', ('hipArray', CONV_TYPE, API_DRIVER)), ('CUarray', ('hipArray *', CONV_TYPE, API_DRIVER)), ('CUdevprop_st', ('hipDeviceProp_t', CONV_TYPE, API_DRIVER)), ('CUdevprop', ('hipDeviceProp_t', CONV_TYPE, API_DRIVER)), ('CUfunction', ('hipFunction_t', CONV_TYPE, API_DRIVER)), ('CUgraphicsResource', ('hipGraphicsResource_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmipmappedArray', ('hipMipmappedArray_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUfunction_attribute', ('hipFuncAttribute_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUfunction_attribute_enum', ('hipFuncAttribute_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsMapResourceFlags', ('hipGraphicsMapFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsMapResourceFlags_enum', ('hipGraphicsMapFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsRegisterFlags', ('hipGraphicsRegisterFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsRegisterFlags_enum', ('hipGraphicsRegisterFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUoccupancy_flags', ('hipOccupancyFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUoccupancy_flags_enum', ('hipOccupancyFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUfunc_cache_enum', ('hipFuncCache', CONV_TYPE, API_DRIVER)), ('CUfunc_cache', ('hipFuncCache', CONV_TYPE, API_DRIVER)), ('CUipcMem_flags', ('hipIpcMemFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUipcMem_flags_enum', ('hipIpcMemFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_cacheMode', ('hipJitCacheMode', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_cacheMode_enum', ('hipJitCacheMode', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_fallback', ('hipJitFallback', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_fallback_enum', ('hipJitFallback', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_option', ('hipJitOption', CONV_JIT, API_DRIVER)), ('CUjit_option_enum', ('hipJitOption', CONV_JIT, API_DRIVER)), ('CUjit_target', ('hipJitTarget', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_target_enum', ('hipJitTarget', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjitInputType', ('hipJitInputType', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjitInputType_enum', ('hipJitInputType', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUlimit', ('hipLimit_t', CONV_TYPE, API_DRIVER)), ('CUlimit_enum', ('hipLimit_t', CONV_TYPE, API_DRIVER)), ('CUmemAttach_flags', ('hipMemAttachFlags_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmemAttach_flags_enum', ('hipMemAttachFlags_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmemorytype', ('hipMemType_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmemorytype_enum', ('hipMemType_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUresourcetype', ('hipResourceType', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CUresourcetype_enum', ('hipResourceType', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CUresourceViewFormat', ('hipResourceViewFormat', CONV_TEX, API_DRIVER)), ('CUresourceViewFormat_enum', ('hipResourceViewFormat', CONV_TEX, API_DRIVER)), ('CUsharedconfig', ('hipSharedMemConfig', CONV_TYPE, API_DRIVER)), ('CUsharedconfig_enum', ('hipSharedMemConfig', CONV_TYPE, API_DRIVER)), ('CUcontext', ('hipCtx_t', CONV_TYPE, API_DRIVER)), ('CUmodule', ('hipModule_t', CONV_TYPE, API_DRIVER)), ('CUstream', ('hipStream_t', CONV_TYPE, API_DRIVER)), ('CUstream_st', ('ihipStream_t', CONV_TYPE, API_DRIVER)), ('CUstreamCallback', ('hipStreamCallback_t', CONV_TYPE, API_DRIVER)), ('CUsurfObject', ('hipSurfaceObject', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUsurfref', ('hipSurfaceReference_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUtexObject', ('hipTextureObject_t', CONV_TYPE, API_DRIVER)), ('CUtexref', ('textureReference', CONV_TYPE, API_DRIVER)), ('CUstream_flags', ('hipStreamFlags', CONV_TYPE, API_DRIVER)), ('CUstreamWaitValue_flags', ('hipStreamWaitValueFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUstreamWriteValue_flags', ('hipStreamWriteValueFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUstreamBatchMemOpType', ('hipStreamBatchMemOpType', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUdevice_P2PAttribute', ('hipDeviceP2PAttribute', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUevent', ('hipEvent_t', CONV_TYPE, API_DRIVER)), ('CUevent_st', ('ihipEvent_t', CONV_TYPE, API_DRIVER)), ('CUevent_flags', ('hipEventFlags', CONV_EVENT, API_DRIVER, HIP_UNSUPPORTED)), ('CUfilter_mode', ('hipTextureFilterMode', CONV_TEX, API_DRIVER)), ('CUGLDeviceList', ('hipGLDeviceList', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CUGLmap_flags', ('hipGLMapFlags', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d9DeviceList', ('hipD3D9DeviceList', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d9map_flags', ('hipD3D9MapFlags', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d9register_flags', ('hipD3D9RegisterFlags', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d10DeviceList', ('hipd3d10DeviceList', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d10map_flags', ('hipD3D10MapFlags', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d10register_flags', ('hipD3D10RegisterFlags', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d11DeviceList', ('hipd3d11DeviceList', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('CUeglStreamConnection_st', ('hipEglStreamConnection', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('CUeglStreamConnection', ('hipEglStreamConnection', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('libraryPropertyType_t', ('hipLibraryPropertyType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('libraryPropertyType', ('hipLibraryPropertyType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamCallback_t', ('hipStreamCallback_t', CONV_TYPE, API_RUNTIME)), ('cudaArray', ('hipArray', CONV_MEM, API_RUNTIME)), ('cudaArray_t', ('hipArray_t', CONV_MEM, API_RUNTIME)), ('cudaArray_const_t', ('hipArray_const_t', CONV_MEM, API_RUNTIME)), ('cudaMipmappedArray_t', ('hipMipmappedArray_t', CONV_MEM, API_RUNTIME)), ('cudaMipmappedArray_const_t', ('hipMipmappedArray_const_t', CONV_MEM, API_RUNTIME)), ('cudaArrayDefault', ('hipArrayDefault', CONV_MEM, API_RUNTIME)), ('cudaArrayLayered', ('hipArrayLayered', CONV_MEM, API_RUNTIME)), ('cudaArraySurfaceLoadStore', ('hipArraySurfaceLoadStore', CONV_MEM, API_RUNTIME)), ('cudaArrayCubemap', ('hipArrayCubemap', CONV_MEM, API_RUNTIME)), ('cudaArrayTextureGather', ('hipArrayTextureGather', CONV_MEM, API_RUNTIME)), ('cudaMemoryAdvise', ('hipMemAdvise', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttribute', ('hipMemRangeAttribute', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyKind', ('hipMemcpyKind', CONV_MEM, API_RUNTIME)), ('cudaMemoryType', ('hipMemoryType', CONV_MEM, API_RUNTIME)), ('cudaExtent', ('hipExtent', CONV_MEM, API_RUNTIME)), ('cudaPitchedPtr', ('hipPitchedPtr', CONV_MEM, API_RUNTIME)), ('cudaPos', ('hipPos', CONV_MEM, API_RUNTIME)), ('cudaEvent_t', ('hipEvent_t', CONV_TYPE, API_RUNTIME)), ('cudaStream_t', ('hipStream_t', CONV_TYPE, API_RUNTIME)), ('cudaPointerAttributes', ('hipPointerAttribute_t', CONV_TYPE, API_RUNTIME)), ('cudaDeviceAttr', ('hipDeviceAttribute_t', CONV_TYPE, API_RUNTIME)), ('cudaDeviceProp', ('hipDeviceProp_t', CONV_TYPE, API_RUNTIME)), ('cudaDeviceP2PAttr', ('hipDeviceP2PAttribute', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeMode', ('hipComputeMode', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFuncCache', ('hipFuncCache_t', CONV_CACHE, API_RUNTIME)), ('cudaFuncAttributes', ('hipFuncAttributes', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSharedMemConfig', ('hipSharedMemConfig', CONV_TYPE, API_RUNTIME)), ('cudaLimit', ('hipLimit_t', CONV_TYPE, API_RUNTIME)), ('cudaOutputMode', ('hipOutputMode', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaTextureReadMode', ('hipTextureReadMode', CONV_TEX, API_RUNTIME)), ('cudaTextureFilterMode', ('hipTextureFilterMode', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKind', ('hipChannelFormatKind', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatDesc', ('hipChannelFormatDesc', CONV_TEX, API_RUNTIME)), ('cudaResourceDesc', ('hipResourceDesc', CONV_TEX, API_RUNTIME)), ('cudaResourceViewDesc', ('hipResourceViewDesc', CONV_TEX, API_RUNTIME)), ('cudaTextureDesc', ('hipTextureDesc', CONV_TEX, API_RUNTIME)), ('surfaceReference', ('hipSurfaceReference', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaTextureObject_t', ('hipTextureObject_t', CONV_TEX, API_RUNTIME)), ('cudaResourceType', ('hipResourceType', CONV_TEX, API_RUNTIME)), ('cudaResourceViewFormat', ('hipResourceViewFormat', CONV_TEX, API_RUNTIME)), ('cudaTextureAddressMode', ('hipTextureAddressMode', CONV_TEX, API_RUNTIME)), ('cudaSurfaceBoundaryMode', ('hipSurfaceBoundaryMode', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSurfaceFormatMode', ('hipSurfaceFormatMode', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaTextureType1D', ('hipTextureType1D', CONV_TEX, API_RUNTIME)), ('cudaTextureType2D', ('hipTextureType2D', CONV_TEX, API_RUNTIME)), ('cudaTextureType3D', ('hipTextureType3D', CONV_TEX, API_RUNTIME)), ('cudaTextureTypeCubemap', ('hipTextureTypeCubemap', CONV_TEX, API_RUNTIME)), ('cudaTextureType1DLayered', ('hipTextureType1DLayered', CONV_TEX, API_RUNTIME)), ('cudaTextureType2DLayered', ('hipTextureType2DLayered', CONV_TEX, API_RUNTIME)), ('cudaTextureTypeCubemapLayered', ('hipTextureTypeCubemapLayered', CONV_TEX, API_RUNTIME)), ('cudaIpcEventHandle_t', ('hipIpcEventHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaIpcEventHandle_st', ('hipIpcEventHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaIpcMemHandle_t', ('hipIpcMemHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaIpcMemHandle_st', ('hipIpcMemHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaGraphicsCubeFace', ('hipGraphicsCubeFace', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlags', ('hipGraphicsMapFlags', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlags', ('hipGraphicsRegisterFlags', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceList', ('hipGLDeviceList', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlags', ('hipGLMapFlags', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceList', ('hipD3D9DeviceList', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlags', ('hipD3D9MapFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterFlags', ('hipD3D9RegisterFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceList', ('hipd3d10DeviceList', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlags', ('hipD3D10MapFlags', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterFlags', ('hipD3D10RegisterFlags', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceList', ('hipd3d11DeviceList', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEglStreamConnection', ('hipEglStreamConnection', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cublasHandle_t', ('rocblas_handle', CONV_TYPE, API_BLAS)), ('cublasOperation_t', ('rocblas_operation', CONV_TYPE, API_BLAS)), ('cublasStatus_t', ('rocblas_status', CONV_TYPE, API_BLAS)), ('cublasFillMode_t', ('rocblas_fill', CONV_TYPE, API_BLAS)), ('cublasDiagType_t', ('rocblas_diagonal', CONV_TYPE, API_BLAS)), ('cublasSideMode_t', ('rocblas_side', CONV_TYPE, API_BLAS)), ('cublasPointerMode_t', ('rocblas_pointer_mode', CONV_TYPE, API_BLAS)), ('cublasAtomicsMode_t', ('rocblas_atomics_mode', CONV_TYPE, API_BLAS, HIP_UNSUPPORTED)), ('cublasDataType_t', ('rocblas_data_type', CONV_TYPE, API_BLAS, HIP_UNSUPPORTED)), ('curandStatus', ('hiprandStatus_t', CONV_TYPE, API_RAND)), ('curandStatus_t', ('hiprandStatus_t', CONV_TYPE, API_RAND)), ('curandRngType', ('hiprandRngType_t', CONV_TYPE, API_RAND)), ('curandRngType_t', ('hiprandRngType_t', CONV_TYPE, API_RAND)), ('curandGenerator_st', ('hiprandGenerator_st', CONV_TYPE, API_RAND)), ('curandGenerator_t', ('hiprandGenerator_t', CONV_TYPE, API_RAND)), ('curandDirectionVectorSet', ('hiprandDirectionVectorSet_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDirectionVectorSet_t', ('hiprandDirectionVectorSet_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandOrdering', ('hiprandOrdering_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandOrdering_t', ('hiprandOrdering_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistribution_st', ('hiprandDistribution_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2V_st', ('hiprandDistribution_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistribution_t', ('hiprandDistribution_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2V_t', ('hiprandDistribution_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionShift_st', ('hiprandDistributionShift_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionShift_t', ('hiprandDistributionShift_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionM2Shift_st', ('hiprandDistributionM2Shift_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionM2Shift_t', ('hiprandDistributionM2Shift_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2_st', ('hiprandHistogramM2_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2_t', ('hiprandHistogramM2_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2K_st', ('hiprandHistogramM2K_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2K_t', ('hiprandHistogramM2K_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDiscreteDistribution_st', ('hiprandDiscreteDistribution_st', CONV_TYPE, API_RAND)), ('curandDiscreteDistribution_t', ('hiprandDiscreteDistribution_t', CONV_TYPE, API_RAND)), ('curandMethod', ('hiprandMethod_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandMethod_t', ('hiprandMethod_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDirectionVectors32_t', ('hiprandDirectionVectors32_t', CONV_TYPE, API_RAND)), ('curandDirectionVectors64_t', ('hiprandDirectionVectors64_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateMtgp32_t', ('hiprandStateMtgp32_t', CONV_TYPE, API_RAND)), ('curandStateMtgp32', ('hiprandStateMtgp32_t', CONV_TYPE, API_RAND)), ('curandStateScrambledSobol64_t', ('hiprandStateScrambledSobol64_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateSobol64_t', ('hiprandStateSobol64_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateScrambledSobol32_t', ('hiprandStateScrambledSobol32_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateSobol32_t', ('hiprandStateSobol32_t', CONV_TYPE, API_RAND)), ('curandStateMRG32k3a_t', ('hiprandStateMRG32k3a_t', CONV_TYPE, API_RAND)), ('curandStatePhilox4_32_10_t', ('hiprandStatePhilox4_32_10_t', CONV_TYPE, API_RAND)), ('curandStateXORWOW_t', ('hiprandStateXORWOW_t', CONV_TYPE, API_RAND)), ('curandState_t', ('hiprandState_t', CONV_TYPE, API_RAND)), ('curandState', ('hiprandState_t', CONV_TYPE, API_RAND))])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_INCLUDE_MAP->collections.OrderedDict([('include <cuda.h', ('include <hip/hip_runtime.h', CONV_INCLUDE_CUDA_MAIN_H, API_DRIVER)), ('include "cuda.h', ('include "hip/hip_runtime.h', CONV_INCLUDE_CUDA_MAIN_H, API_DRIVER)), ('cuda_runtime.h', ('hip/hip_runtime.h', CONV_INCLUDE_CUDA_MAIN_H, API_RUNTIME)), ('cuda_runtime_api.h', ('hip/hip_runtime_api.h', CONV_INCLUDE, API_RUNTIME)), ('channel_descriptor.h', ('hip/channel_descriptor.h', CONV_INCLUDE, API_RUNTIME)), ('device_functions.h', ('hip/device_functions.h', CONV_INCLUDE, API_RUNTIME)), ('driver_types.h', ('hip/driver_types.h', CONV_INCLUDE, API_RUNTIME)), ('cuComplex.h', ('hip/hip_complex.h', CONV_INCLUDE, API_RUNTIME)), ('cuda_fp16.h', ('hip/hip_fp16.h', CONV_INCLUDE, API_RUNTIME)), ('cuda_texture_types.h', ('hip/hip_texture_types.h', CONV_INCLUDE, API_RUNTIME)), ('vector_types.h', ('hip/hip_vector_types.h', CONV_INCLUDE, API_RUNTIME)), ('cublas.h', ('rocblas.h', CONV_INCLUDE_CUDA_MAIN_H, API_BLAS)), ('cublas_v2.h', ('rocblas.h', CONV_INCLUDE_CUDA_MAIN_H, API_BLAS)), ('curand.h', ('hiprand.h', CONV_INCLUDE_CUDA_MAIN_H, API_RAND)), ('curand_kernel.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_discrete.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_discrete2.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_globals.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_lognormal.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mrg32k3a.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32_host.h', ('hiprand_mtgp32_host.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32_kernel.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32dc_p_11213.h', ('rocrand_mtgp32_11213.h', CONV_INCLUDE, API_RAND)), ('curand_normal.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_normal_static.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_philox4x32_x.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_poisson.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_precalc.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_uniform.h', ('hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('cusparse.h', ('hipsparse.h', CONV_INCLUDE, API_RAND)), ('cufft.h', ('hipfft.h', CONV_INCLUDE, API_BLAS)), ('cufftXt.h', ('hipfft.h', CONV_INCLUDE, API_BLAS)), ('<nccl.h>', ('<rccl.h>', CONV_INCLUDE, API_RUNTIME)), ('nvrtc.h', ('hip/hiprtc.h', CONV_INCLUDE, API_RTC)), ('thrust/system/cuda', ('thrust/system/hip', CONV_INCLUDE, API_BLAS)), ('cub/util_allocator.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/block/block_reduce.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/cub.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/block/block_load.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/device/device_reduce.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/device/device_scan.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('nvToolsExt.h', ('roctx.h', CONV_INCLUDE, API_ROCTX))])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_IDENTIFIER_MAP->collections.OrderedDict([('__CUDACC__', ('__HIPCC__', CONV_DEF, API_RUNTIME)), ('CUDA_ERROR_INVALID_CONTEXT', ('hipErrorInvalidContext', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_CONTEXT_ALREADY_CURRENT', ('hipErrorContextAlreadyCurrent', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_ARRAY_IS_MAPPED', ('hipErrorArrayIsMapped', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_ALREADY_MAPPED', ('hipErrorAlreadyMapped', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_ALREADY_ACQUIRED', ('hipErrorAlreadyAcquired', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_MAPPED', ('hipErrorNotMapped', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_MAPPED_AS_ARRAY', ('hipErrorNotMappedAsArray', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_MAPPED_AS_POINTER', ('hipErrorNotMappedAsPointer', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_CONTEXT_ALREADY_IN_USE', ('hipErrorContextAlreadyInUse', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_INVALID_SOURCE', ('hipErrorInvalidSource', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_FILE_NOT_FOUND', ('hipErrorFileNotFound', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_FOUND', ('hipErrorNotFound', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING', ('hipErrorLaunchIncompatibleTexturing', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE', ('hipErrorPrimaryContextActive', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_CONTEXT_IS_DESTROYED', ('hipErrorContextIsDestroyed', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_NOT_PERMITTED', ('hipErrorNotPermitted', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_NOT_SUPPORTED', ('hipErrorNotSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorMissingConfiguration', ('hipErrorMissingConfiguration', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorPriorLaunchFailure', ('hipErrorPriorLaunchFailure', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidDeviceFunction', ('hipErrorInvalidDeviceFunction', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidConfiguration', ('hipErrorInvalidConfiguration', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidPitchValue', ('hipErrorInvalidPitchValue', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidSymbol', ('hipErrorInvalidSymbol', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidHostPointer', ('hipErrorInvalidHostPointer', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidDevicePointer', ('hipErrorInvalidDevicePointer', CONV_TYPE, API_RUNTIME)), ('cudaErrorInvalidTexture', ('hipErrorInvalidTexture', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidTextureBinding', ('hipErrorInvalidTextureBinding', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidChannelDescriptor', ('hipErrorInvalidChannelDescriptor', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidMemcpyDirection', ('hipErrorInvalidMemcpyDirection', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorAddressOfConstant', ('hipErrorAddressOfConstant', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorTextureFetchFailed', ('hipErrorTextureFetchFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorTextureNotBound', ('hipErrorTextureNotBound', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorSynchronizationError', ('hipErrorSynchronizationError', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidFilterSetting', ('hipErrorInvalidFilterSetting', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidNormSetting', ('hipErrorInvalidNormSetting', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorMixedDeviceExecution', ('hipErrorMixedDeviceExecution', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorNotYetImplemented', ('hipErrorNotYetImplemented', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorMemoryValueTooLarge', ('hipErrorMemoryValueTooLarge', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInsufficientDriver', ('hipErrorInsufficientDriver', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorSetOnActiveProcess', ('hipErrorSetOnActiveProcess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidSurface', ('hipErrorInvalidSurface', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDuplicateVariableName', ('hipErrorDuplicateVariableName', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDuplicateTextureName', ('hipErrorDuplicateTextureName', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDuplicateSurfaceName', ('hipErrorDuplicateSurfaceName', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDevicesUnavailable', ('hipErrorDevicesUnavailable', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorIncompatibleDriverContext', ('hipErrorIncompatibleDriverContext', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDeviceAlreadyInUse', ('hipErrorDeviceAlreadyInUse', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchMaxDepthExceeded', ('hipErrorLaunchMaxDepthExceeded', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchFileScopedTex', ('hipErrorLaunchFileScopedTex', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchFileScopedSurf', ('hipErrorLaunchFileScopedSurf', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorSyncDepthExceeded', ('hipErrorSyncDepthExceeded', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchPendingCountExceeded', ('hipErrorLaunchPendingCountExceeded', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorNotPermitted', ('hipErrorNotPermitted', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorNotSupported', ('hipErrorNotSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorStartupFailure', ('hipErrorStartupFailure', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorApiFailureBase', ('hipErrorApiFailureBase', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_SUCCESS', ('hipSuccess', CONV_TYPE, API_DRIVER)), ('cudaSuccess', ('hipSuccess', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_INVALID_VALUE', ('hipErrorInvalidValue', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidValue', ('hipErrorInvalidValue', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_OUT_OF_MEMORY', ('hipErrorMemoryAllocation', CONV_TYPE, API_DRIVER)), ('cudaErrorMemoryAllocation', ('hipErrorMemoryAllocation', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_NOT_INITIALIZED', ('hipErrorNotInitialized', CONV_TYPE, API_DRIVER)), ('cudaErrorInitializationError', ('hipErrorInitializationError', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_DEINITIALIZED', ('hipErrorDeinitialized', CONV_TYPE, API_DRIVER)), ('cudaErrorCudartUnloading', ('hipErrorDeinitialized', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_DISABLED', ('hipErrorProfilerDisabled', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerDisabled', ('hipErrorProfilerDisabled', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_NOT_INITIALIZED', ('hipErrorProfilerNotInitialized', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerNotInitialized', ('hipErrorProfilerNotInitialized', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_ALREADY_STARTED', ('hipErrorProfilerAlreadyStarted', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerAlreadyStarted', ('hipErrorProfilerAlreadyStarted', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_ALREADY_STOPPED', ('hipErrorProfilerAlreadyStopped', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerAlreadyStopped', ('hipErrorProfilerAlreadyStopped', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_NO_DEVICE', ('hipErrorNoDevice', CONV_TYPE, API_DRIVER)), ('cudaErrorNoDevice', ('hipErrorNoDevice', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_INVALID_DEVICE', ('hipErrorInvalidDevice', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidDevice', ('hipErrorInvalidDevice', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_INVALID_IMAGE', ('hipErrorInvalidImage', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidKernelImage', ('hipErrorInvalidImage', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_MAP_FAILED', ('hipErrorMapFailed', CONV_TYPE, API_DRIVER)), ('cudaErrorMapBufferObjectFailed', ('hipErrorMapFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_UNMAP_FAILED', ('hipErrorUnmapFailed', CONV_TYPE, API_DRIVER)), ('cudaErrorUnmapBufferObjectFailed', ('hipErrorUnmapFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_NO_BINARY_FOR_GPU', ('hipErrorNoBinaryForGpu', CONV_TYPE, API_DRIVER)), ('cudaErrorNoKernelImageForDevice', ('hipErrorNoBinaryForGpu', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_ECC_UNCORRECTABLE', ('hipErrorECCNotCorrectable', CONV_TYPE, API_DRIVER)), ('cudaErrorECCUncorrectable', ('hipErrorECCNotCorrectable', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_UNSUPPORTED_LIMIT', ('hipErrorUnsupportedLimit', CONV_TYPE, API_DRIVER)), ('cudaErrorUnsupportedLimit', ('hipErrorUnsupportedLimit', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PEER_ACCESS_UNSUPPORTED', ('hipErrorPeerAccessUnsupported', CONV_TYPE, API_DRIVER)), ('cudaErrorPeerAccessUnsupported', ('hipErrorPeerAccessUnsupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_PTX', ('hipErrorInvalidKernelFile', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidPtx', ('hipErrorInvalidKernelFile', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_GRAPHICS_CONTEXT', ('hipErrorInvalidGraphicsContext', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidGraphicsContext', ('hipErrorInvalidGraphicsContext', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_NVLINK_UNCORRECTABLE', ('hipErrorNvlinkUncorrectable', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorNvlinkUncorrectable', ('hipErrorNvlinkUncorrectable', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND', ('hipErrorSharedObjectSymbolNotFound', CONV_TYPE, API_DRIVER)), ('cudaErrorSharedObjectSymbolNotFound', ('hipErrorSharedObjectSymbolNotFound', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_SHARED_OBJECT_INIT_FAILED', ('hipErrorSharedObjectInitFailed', CONV_TYPE, API_DRIVER)), ('cudaErrorSharedObjectInitFailed', ('hipErrorSharedObjectInitFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_OPERATING_SYSTEM', ('hipErrorOperatingSystem', CONV_TYPE, API_DRIVER)), ('cudaErrorOperatingSystem', ('hipErrorOperatingSystem', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_HANDLE', ('hipErrorInvalidResourceHandle', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidResourceHandle', ('hipErrorInvalidResourceHandle', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_NOT_READY', ('hipErrorNotReady', CONV_TYPE, API_DRIVER)), ('cudaErrorNotReady', ('hipErrorNotReady', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_ILLEGAL_ADDRESS', ('hipErrorIllegalAddress', CONV_TYPE, API_DRIVER)), ('cudaErrorIllegalAddress', ('hipErrorIllegalAddress', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES', ('hipErrorLaunchOutOfResources', CONV_TYPE, API_DRIVER)), ('cudaErrorLaunchOutOfResources', ('hipErrorLaunchOutOfResources', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_LAUNCH_TIMEOUT', ('hipErrorLaunchTimeOut', CONV_TYPE, API_DRIVER)), ('cudaErrorLaunchTimeout', ('hipErrorLaunchTimeOut', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED', ('hipErrorPeerAccessAlreadyEnabled', CONV_TYPE, API_DRIVER)), ('cudaErrorPeerAccessAlreadyEnabled', ('hipErrorPeerAccessAlreadyEnabled', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_PEER_ACCESS_NOT_ENABLED', ('hipErrorPeerAccessNotEnabled', CONV_TYPE, API_DRIVER)), ('cudaErrorPeerAccessNotEnabled', ('hipErrorPeerAccessNotEnabled', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_ASSERT', ('hipErrorAssert', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorAssert', ('hipErrorAssert', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_TOO_MANY_PEERS', ('hipErrorTooManyPeers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorTooManyPeers', ('hipErrorTooManyPeers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED', ('hipErrorHostMemoryAlreadyRegistered', CONV_TYPE, API_DRIVER)), ('cudaErrorHostMemoryAlreadyRegistered', ('hipErrorHostMemoryAlreadyRegistered', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED', ('hipErrorHostMemoryNotRegistered', CONV_TYPE, API_DRIVER)), ('cudaErrorHostMemoryNotRegistered', ('hipErrorHostMemoryNotRegistered', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_HARDWARE_STACK_ERROR', ('hipErrorHardwareStackError', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorHardwareStackError', ('hipErrorHardwareStackError', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_ILLEGAL_INSTRUCTION', ('hipErrorIllegalInstruction', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorIllegalInstruction', ('hipErrorIllegalInstruction', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_MISALIGNED_ADDRESS', ('hipErrorMisalignedAddress', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorMisalignedAddress', ('hipErrorMisalignedAddress', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_ADDRESS_SPACE', ('hipErrorInvalidAddressSpace', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorInvalidAddressSpace', ('hipErrorInvalidAddressSpace', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_PC', ('hipErrorInvalidPc', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorInvalidPc', ('hipErrorInvalidPc', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_LAUNCH_FAILED', ('hipErrorLaunchFailure', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorLaunchFailure', ('hipErrorLaunchFailure', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_UNKNOWN', ('hipErrorUnknown', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorUnknown', ('hipErrorUnknown', CONV_TYPE, API_RUNTIME)), ('CU_TR_ADDRESS_MODE_WRAP', ('HIP_TR_ADDRESS_MODE_WRAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_ADDRESS_MODE_CLAMP', ('HIP_TR_ADDRESS_MODE_CLAMP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_ADDRESS_MODE_MIRROR', ('HIP_TR_ADDRESS_MODE_MIRROR', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_ADDRESS_MODE_BORDER', ('HIP_TR_ADDRESS_MODE_BORDER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_POSITIVE_X', ('HIP_CUBEMAP_FACE_POSITIVE_X', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_NEGATIVE_X', ('HIP_CUBEMAP_FACE_NEGATIVE_X', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_POSITIVE_Y', ('HIP_CUBEMAP_FACE_POSITIVE_Y', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_NEGATIVE_Y', ('HIP_CUBEMAP_FACE_NEGATIVE_Y', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_POSITIVE_Z', ('HIP_CUBEMAP_FACE_POSITIVE_Z', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_NEGATIVE_Z', ('HIP_CUBEMAP_FACE_NEGATIVE_Z', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_AD_FORMAT_UNSIGNED_INT8', ('HIP_AD_FORMAT_UNSIGNED_INT8', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_UNSIGNED_INT16', ('HIP_AD_FORMAT_UNSIGNED_INT16', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_UNSIGNED_INT32', ('HIP_AD_FORMAT_UNSIGNED_INT32', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_SIGNED_INT8', ('HIP_AD_FORMAT_SIGNED_INT8', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_SIGNED_INT16', ('HIP_AD_FORMAT_SIGNED_INT16', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_SIGNED_INT32', ('HIP_AD_FORMAT_SIGNED_INT32', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_HALF', ('HIP_AD_FORMAT_HALF', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_FLOAT', ('HIP_AD_FORMAT_FLOAT', CONV_TYPE, API_DRIVER)), ('CU_COMPUTEMODE_DEFAULT', ('hipComputeModeDefault', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_COMPUTEMODE_EXCLUSIVE', ('hipComputeModeExclusive', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_COMPUTEMODE_PROHIBITED', ('hipComputeModeProhibited', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_COMPUTEMODE_EXCLUSIVE_PROCESS', ('hipComputeModeExclusiveProcess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_SET_READ_MOSTLY', ('hipMemAdviseSetReadMostly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_UNSET_READ_MOSTLY', ('hipMemAdviseUnsetReadMostly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_SET_PREFERRED_LOCATION', ('hipMemAdviseSetPreferredLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION', ('hipMemAdviseUnsetPreferredLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_SET_ACCESSED_BY', ('hipMemAdviseSetAccessedBy', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_UNSET_ACCESSED_BY', ('hipMemAdviseUnsetAccessedBy', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY', ('hipMemRangeAttributeReadMostly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION', ('hipMemRangeAttributePreferredLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY', ('hipMemRangeAttributeAccessedBy', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION', ('hipMemRangeAttributeLastPrefetchLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_AUTO', ('HIP_CTX_SCHED_AUTO', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_SPIN', ('HIP_CTX_SCHED_SPIN', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_YIELD', ('HIP_CTX_SCHED_YIELD', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_BLOCKING_SYNC', ('HIP_CTX_SCHED_BLOCKING_SYNC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_BLOCKING_SYNC', ('HIP_CTX_BLOCKING_SYNC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_MASK', ('HIP_CTX_SCHED_MASK', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_MAP_HOST', ('HIP_CTX_MAP_HOST', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_LMEM_RESIZE_TO_MAX', ('HIP_CTX_LMEM_RESIZE_TO_MAX', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_FLAGS_MASK', ('HIP_CTX_FLAGS_MASK', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LAUNCH_PARAM_BUFFER_POINTER', ('HIP_LAUNCH_PARAM_BUFFER_POINTER', CONV_TYPE, API_DRIVER)), ('CU_LAUNCH_PARAM_BUFFER_SIZE', ('HIP_LAUNCH_PARAM_BUFFER_SIZE', CONV_TYPE, API_DRIVER)), ('CU_LAUNCH_PARAM_END', ('HIP_LAUNCH_PARAM_END', CONV_TYPE, API_DRIVER)), ('CU_IPC_HANDLE_SIZE', ('HIP_LAUNCH_PARAM_END', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTALLOC_DEVICEMAP', ('HIP_MEMHOSTALLOC_DEVICEMAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTALLOC_PORTABLE', ('HIP_MEMHOSTALLOC_PORTABLE', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTALLOC_WRITECOMBINED', ('HIP_MEMHOSTALLOC_WRITECOMBINED', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTREGISTER_DEVICEMAP', ('HIP_MEMHOSTREGISTER_DEVICEMAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTREGISTER_IOMEMORY', ('HIP_MEMHOSTREGISTER_IOMEMORY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTREGISTER_PORTABLE', ('HIP_MEMHOSTREGISTER_PORTABLE', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_PARAM_TR_DEFAULT', ('HIP_PARAM_TR_DEFAULT', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_LEGACY', ('HIP_STREAM_LEGACY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_PER_THREAD', ('HIP_STREAM_PER_THREAD', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSA_OVERRIDE_FORMAT', ('HIP_TRSA_OVERRIDE_FORMAT', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSF_NORMALIZED_COORDINATES', ('HIP_TRSF_NORMALIZED_COORDINATES', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSF_READ_AS_INTEGER', ('HIP_TRSF_READ_AS_INTEGER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSF_SRGB', ('HIP_TRSF_SRGB', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_2DARRAY', ('HIP_ARRAY3D_LAYERED', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_CUBEMAP', ('HIP_ARRAY3D_CUBEMAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_DEPTH_TEXTURE', ('HIP_ARRAY3D_DEPTH_TEXTURE', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_LAYERED', ('HIP_ARRAY3D_LAYERED', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_SURFACE_LDST', ('HIP_ARRAY3D_SURFACE_LDST', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_TEXTURE_GATHER', ('HIP_ARRAY3D_TEXTURE_GATHER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK', ('hipDeviceAttributeMaxThreadsPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X', ('hipDeviceAttributeMaxBlockDimX', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y', ('hipDeviceAttributeMaxBlockDimY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z', ('hipDeviceAttributeMaxBlockDimZ', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X', ('hipDeviceAttributeMaxGridDimX', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y', ('hipDeviceAttributeMaxGridDimY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z', ('hipDeviceAttributeMaxGridDimZ', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK', ('hipDeviceAttributeMaxSharedMemoryPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_SHARED_MEMORY_PER_BLOCK', ('hipDeviceAttributeMaxSharedMemoryPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY', ('hipDeviceAttributeTotalConstantMemory', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_WARP_SIZE', ('hipDeviceAttributeWarpSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_PITCH', ('hipDeviceAttributeMaxPitch', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK', ('hipDeviceAttributeMaxRegistersPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_REGISTERS_PER_BLOCK', ('hipDeviceAttributeMaxRegistersPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CLOCK_RATE', ('hipDeviceAttributeClockRate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT', ('hipDeviceAttributeTextureAlignment', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_GPU_OVERLAP', ('hipDeviceAttributeAsyncEngineCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT', ('hipDeviceAttributeMultiprocessorCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT', ('hipDeviceAttributeKernelExecTimeout', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_INTEGRATED', ('hipDeviceAttributeIntegrated', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY', ('hipDeviceAttributeCanMapHostMemory', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_MODE', ('hipDeviceAttributeComputeMode', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH', ('hipDeviceAttributeMaxTexture1DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_WIDTH', ('hipDeviceAttributeMaxTexture2DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_HEIGHT', ('hipDeviceAttributeMaxTexture2DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH', ('hipDeviceAttributeMaxTexture3DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT', ('hipDeviceAttributeMaxTexture3DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH', ('hipDeviceAttributeMaxTexture3DDepth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH', ('hipDeviceAttributeMaxTexture2DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT', ('hipDeviceAttributeMaxTexture2DLayeredHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS', ('hipDeviceAttributeMaxTexture2DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_WIDTH', ('hipDeviceAttributeMaxTexture2DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_HEIGHT', ('hipDeviceAttributeMaxTexture2DLayeredHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES', ('hipDeviceAttributeMaxTexture2DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_SURFACE_ALIGNMENT', ('hipDeviceAttributeSurfaceAlignment', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS', ('hipDeviceAttributeConcurrentKernels', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_ECC_ENABLED', ('hipDeviceAttributeEccEnabled', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_PCI_BUS_ID', ('hipDeviceAttributePciBusId', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID', ('hipDeviceAttributePciDeviceId', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_TCC_DRIVER', ('hipDeviceAttributeTccDriver', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE', ('hipDeviceAttributeMemoryClockRate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH', ('hipDeviceAttributeMemoryBusWidth', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE', ('hipDeviceAttributeL2CacheSize', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR', ('hipDeviceAttributeMaxThreadsPerMultiProcessor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT', ('hipDeviceAttributeAsyncEngineCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING', ('hipDeviceAttributeUnifiedAddressing', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH', ('hipDeviceAttributeMaxTexture1DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS', ('hipDeviceAttributeMaxTexture1DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CAN_TEX2D_GATHER', ('hipDeviceAttributeCanTex2DGather', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_WIDTH', ('hipDeviceAttributeMaxTexture2DGatherWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_HEIGHT', ('hipDeviceAttributeMaxTexture2DGatherHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE', ('hipDeviceAttributeMaxTexture3DWidthAlternate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE', ('hipDeviceAttributeMaxTexture3DHeightAlternate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE', ('hipDeviceAttributeMaxTexture3DDepthAlternate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID', ('hipDeviceAttributePciDomainId', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT', ('hipDeviceAttributeTexturePitchAlignment', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_WIDTH', ('hipDeviceAttributeMaxTextureCubemapWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH', ('hipDeviceAttributeMaxTextureCubemapLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS', ('hipDeviceAttributeMaxTextureCubemapLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_WIDTH', ('hipDeviceAttributeMaxSurface1DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_WIDTH', ('hipDeviceAttributeMaxSurface2DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_HEIGHT', ('hipDeviceAttributeMaxSurface2DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_WIDTH', ('hipDeviceAttributeMaxSurface3DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_HEIGHT', ('hipDeviceAttributeMaxSurface3DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_DEPTH', ('hipDeviceAttributeMaxSurface3DDepth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_WIDTH', ('hipDeviceAttributeMaxSurface1DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_LAYERS', ('hipDeviceAttributeMaxSurface1DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_WIDTH', ('hipDeviceAttributeMaxSurface2DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_HEIGHT', ('hipDeviceAttributeMaxSurface2DLayeredHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_LAYERS', ('hipDeviceAttributeMaxSurface2DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_WIDTH', ('hipDeviceAttributeMaxSurfaceCubemapWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH', ('hipDeviceAttributeMaxSurfaceCubemapLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS', ('hipDeviceAttributeMaxSurfaceCubemapLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH', ('hipDeviceAttributeMaxTexture1DLinearWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH', ('hipDeviceAttributeMaxTexture2DLinearWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT', ('hipDeviceAttributeMaxTexture2DLinearHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH', ('hipDeviceAttributeMaxTexture2DLinearPitch', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH', ('hipDeviceAttributeMaxTexture2DMipmappedWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT', ('hipDeviceAttributeMaxTexture2DMipmappedHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR', ('hipDeviceAttributeComputeCapabilityMajor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR', ('hipDeviceAttributeComputeCapabilityMinor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH', ('hipDeviceAttributeMaxTexture1DMipmappedWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_STREAM_PRIORITIES_SUPPORTED', ('hipDeviceAttributeStreamPrioritiesSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_GLOBAL_L1_CACHE_SUPPORTED', ('hipDeviceAttributeGlobalL1CacheSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_LOCAL_L1_CACHE_SUPPORTED', ('hipDeviceAttributeLocalL1CacheSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR', ('hipDeviceAttributeMaxSharedMemoryPerMultiprocessor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR', ('hipDeviceAttributeMaxRegistersPerMultiprocessor', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY', ('hipDeviceAttributeManagedMemory', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD', ('hipDeviceAttributeIsMultiGpuBoard', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD_GROUP_ID', ('hipDeviceAttributeMultiGpuBoardGroupId', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_HOST_NATIVE_ATOMIC_SUPPORTED', ('hipDeviceAttributeHostNativeAtomicSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO', ('hipDeviceAttributeSingleToDoublePrecisionPerfRatio', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS', ('hipDeviceAttributePageableMemoryAccess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS', ('hipDeviceAttributeConcurrentManagedAccess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_PREEMPTION_SUPPORTED', ('hipDeviceAttributeComputePreemptionSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM', ('hipDeviceAttributeCanUseHostPointerForRegisteredMem', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX', ('hipDeviceAttributeMax', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_CONTEXT', ('hipPointerAttributeContext', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_MEMORY_TYPE', ('hipPointerAttributeMemoryType', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_DEVICE_POINTER', ('hipPointerAttributeDevicePointer', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_HOST_POINTER', ('hipPointerAttributeHostPointer', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_P2P_TOKENS', ('hipPointerAttributeP2pTokens', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_SYNC_MEMOPS', ('hipPointerAttributeSyncMemops', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_BUFFER_ID', ('hipPointerAttributeBufferId', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_IS_MANAGED', ('hipPointerAttributeIsManaged', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK', ('hipFuncAttributeMaxThreadsPerBlocks', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES', ('hipFuncAttributeSharedSizeBytes', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_CONST_SIZE_BYTES', ('hipFuncAttributeConstSizeBytes', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES', ('hipFuncAttributeLocalSizeBytes', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_NUM_REGS', ('hipFuncAttributeNumRegs', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_PTX_VERSION', ('hipFuncAttributePtxVersion', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_BINARY_VERSION', ('hipFuncAttributeBinaryVersion', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_CACHE_MODE_CA', ('hipFuncAttributeCacheModeCA', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_MAX', ('hipFuncAttributeMax', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_MAP_RESOURCE_FLAGS_NONE', ('hipGraphicsMapFlagsNone', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_MAP_RESOURCE_FLAGS_READ_ONLY', ('hipGraphicsMapFlagsReadOnly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_MAP_RESOURCE_FLAGS_WRITE_DISCARD', ('hipGraphicsMapFlagsWriteDiscard', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_NONE', ('hipGraphicsRegisterFlagsNone', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_READ_ONLY', ('hipGraphicsRegisterFlagsReadOnly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_WRITE_DISCARD', ('hipGraphicsRegisterFlagsWriteDiscard', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_SURFACE_LDST', ('hipGraphicsRegisterFlagsSurfaceLoadStore', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_TEXTURE_GATHER', ('hipGraphicsRegisterFlagsTextureGather', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_OCCUPANCY_DEFAULT', ('hipOccupancyDefault', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE', ('hipOccupancyDisableCachingOverride', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_CACHE_PREFER_NONE', ('hipFuncCachePreferNone', CONV_CACHE, API_DRIVER)), ('CU_FUNC_CACHE_PREFER_SHARED', ('hipFuncCachePreferShared', CONV_CACHE, API_DRIVER)), ('CU_FUNC_CACHE_PREFER_L1', ('hipFuncCachePreferL1', CONV_CACHE, API_DRIVER)), ('CU_FUNC_CACHE_PREFER_EQUAL', ('hipFuncCachePreferEqual', CONV_CACHE, API_DRIVER)), ('CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS', ('hipIpcMemLazyEnablePeerAccess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_IPC_HANDLE_SIZE', ('HIP_IPC_HANDLE_SIZE', CONV_TYPE, API_DRIVER)), ('CU_JIT_CACHE_OPTION_NONE', ('hipJitCacheModeOptionNone', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_CACHE_OPTION_CG', ('hipJitCacheModeOptionCG', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_CACHE_OPTION_CA', ('hipJitCacheModeOptionCA', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_PREFER_PTX', ('hipJitFallbackPreferPtx', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_PREFER_BINARY', ('hipJitFallbackPreferBinary', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_MAX_REGISTERS', ('hipJitOptionMaxRegisters', CONV_JIT, API_DRIVER)), ('CU_JIT_THREADS_PER_BLOCK', ('hipJitOptionThreadsPerBlock', CONV_JIT, API_DRIVER)), ('CU_JIT_WALL_TIME', ('hipJitOptionWallTime', CONV_JIT, API_DRIVER)), ('CU_JIT_INFO_LOG_BUFFER', ('hipJitOptionInfoLogBuffer', CONV_JIT, API_DRIVER)), ('CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES', ('hipJitOptionInfoLogBufferSizeBytes', CONV_JIT, API_DRIVER)), ('CU_JIT_ERROR_LOG_BUFFER', ('hipJitOptionErrorLogBuffer', CONV_JIT, API_DRIVER)), ('CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES', ('hipJitOptionErrorLogBufferSizeBytes', CONV_JIT, API_DRIVER)), ('CU_JIT_OPTIMIZATION_LEVEL', ('hipJitOptionOptimizationLevel', CONV_JIT, API_DRIVER)), ('CU_JIT_TARGET_FROM_CUCONTEXT', ('hipJitOptionTargetFromContext', CONV_JIT, API_DRIVER)), ('CU_JIT_TARGET', ('hipJitOptionTarget', CONV_JIT, API_DRIVER)), ('CU_JIT_FALLBACK_STRATEGY', ('hipJitOptionFallbackStrategy', CONV_JIT, API_DRIVER)), ('CU_JIT_GENERATE_DEBUG_INFO', ('hipJitOptionGenerateDebugInfo', CONV_JIT, API_DRIVER)), ('CU_JIT_LOG_VERBOSE', ('hipJitOptionLogVerbose', CONV_JIT, API_DRIVER)), ('CU_JIT_GENERATE_LINE_INFO', ('hipJitOptionGenerateLineInfo', CONV_JIT, API_DRIVER)), ('CU_JIT_CACHE_MODE', ('hipJitOptionCacheMode', CONV_JIT, API_DRIVER)), ('CU_JIT_NEW_SM3X_OPT', ('hipJitOptionSm3xOpt', CONV_JIT, API_DRIVER)), ('CU_JIT_FAST_COMPILE', ('hipJitOptionFastCompile', CONV_JIT, API_DRIVER)), ('CU_JIT_NUM_OPTIONS', ('hipJitOptionNumOptions', CONV_JIT, API_DRIVER)), ('CU_TARGET_COMPUTE_10', ('hipJitTargetCompute10', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_11', ('hipJitTargetCompute11', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_12', ('hipJitTargetCompute12', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_13', ('hipJitTargetCompute13', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_20', ('hipJitTargetCompute20', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_21', ('hipJitTargetCompute21', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_30', ('hipJitTargetCompute30', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_32', ('hipJitTargetCompute32', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_35', ('hipJitTargetCompute35', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_37', ('hipJitTargetCompute37', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_50', ('hipJitTargetCompute50', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_52', ('hipJitTargetCompute52', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_53', ('hipJitTargetCompute53', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_60', ('hipJitTargetCompute60', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_61', ('hipJitTargetCompute61', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_62', ('hipJitTargetCompute62', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_CUBIN', ('hipJitInputTypeBin', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_PTX', ('hipJitInputTypePtx', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_FATBINARY', ('hipJitInputTypeFatBinary', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_OBJECT', ('hipJitInputTypeObject', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_LIBRARY', ('hipJitInputTypeLibrary', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_NUM_INPUT_TYPES', ('hipJitInputTypeNumInputTypes', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_STACK_SIZE', ('hipLimitStackSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_PRINTF_FIFO_SIZE', ('hipLimitPrintfFifoSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_MALLOC_HEAP_SIZE', ('hipLimitMallocHeapSize', CONV_TYPE, API_DRIVER)), ('CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH', ('hipLimitDevRuntimeSyncDepth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_DEV_RUNTIME_PENDING_LAUNCH_COUNT', ('hipLimitDevRuntimePendingLaunchCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_STACK_SIZE', ('hipLimitStackSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ATTACH_GLOBAL', ('hipMemAttachGlobal', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ATTACH_HOST', ('hipMemAttachHost', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ATTACH_SINGLE', ('hipMemAttachSingle', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_HOST', ('hipMemTypeHost', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_DEVICE', ('hipMemTypeDevice', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_ARRAY', ('hipMemTypeArray', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_UNIFIED', ('hipMemTypeUnified', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_ARRAY', ('hipResourceTypeArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_MIPMAPPED_ARRAY', ('hipResourceTypeMipmappedArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_LINEAR', ('hipResourceTypeLinear', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_PITCH2D', ('hipResourceTypePitch2D', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RES_VIEW_FORMAT_NONE', ('hipResViewFormatNone', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_1X8', ('hipResViewFormatUnsignedChar1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_2X8', ('hipResViewFormatUnsignedChar2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_4X8', ('hipResViewFormatUnsignedChar4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_1X8', ('hipResViewFormatSignedChar1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_2X8', ('hipResViewFormatSignedChar2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_4X8', ('hipResViewFormatSignedChar4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_1X16', ('hipResViewFormatUnsignedShort1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_2X16', ('hipResViewFormatUnsignedShort2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_4X16', ('hipResViewFormatUnsignedShort4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_1X16', ('hipResViewFormatSignedShort1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_2X16', ('hipResViewFormatSignedShort2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_4X16', ('hipResViewFormatSignedShort4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_1X32', ('hipResViewFormatUnsignedInt1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_2X32', ('hipResViewFormatUnsignedInt2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_4X32', ('hipResViewFormatUnsignedInt4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_1X32', ('hipResViewFormatSignedInt1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_2X32', ('hipResViewFormatSignedInt2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_4X32', ('hipResViewFormatSignedInt4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_1X16', ('hipResViewFormatHalf1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_2X16', ('hipResViewFormatHalf2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_4X16', ('hipResViewFormatHalf4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_1X32', ('hipResViewFormatFloat1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_2X32', ('hipResViewFormatFloat2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_4X32', ('hipResViewFormatFloat4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC1', ('hipResViewFormatUnsignedBlockCompressed1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC2', ('hipResViewFormatUnsignedBlockCompressed2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC3', ('hipResViewFormatUnsignedBlockCompressed3', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC4', ('hipResViewFormatUnsignedBlockCompressed4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SIGNED_BC4', ('hipResViewFormatSignedBlockCompressed4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC5', ('hipResViewFormatUnsignedBlockCompressed5', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SIGNED_BC5', ('hipResViewFormatSignedBlockCompressed5', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC6H', ('hipResViewFormatUnsignedBlockCompressed6H', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SIGNED_BC6H', ('hipResViewFormatSignedBlockCompressed6H', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC7', ('hipResViewFormatUnsignedBlockCompressed7', CONV_TEX, API_DRIVER)), ('CU_SHARED_MEM_CONFIG_DEFAULT_BANK_SIZE', ('hipSharedMemBankSizeDefault', CONV_TYPE, API_DRIVER)), ('CU_SHARED_MEM_CONFIG_FOUR_BYTE_BANK_SIZE', ('hipSharedMemBankSizeFourByte', CONV_TYPE, API_DRIVER)), ('CU_SHARED_MEM_CONFIG_EIGHT_BYTE_BANK_SIZE', ('hipSharedMemBankSizeEightByte', CONV_TYPE, API_DRIVER)), ('CU_STREAM_DEFAULT', ('hipStreamDefault', CONV_TYPE, API_DRIVER)), ('CU_STREAM_NON_BLOCKING', ('hipStreamNonBlocking', CONV_TYPE, API_DRIVER)), ('CU_STREAM_WAIT_VALUE_GEQ', ('hipStreamWaitValueGeq', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WAIT_VALUE_EQ', ('hipStreamWaitValueEq', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WAIT_VALUE_AND', ('hipStreamWaitValueAnd', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WAIT_VALUE_FLUSH', ('hipStreamWaitValueFlush', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WRITE_VALUE_DEFAULT', ('hipStreamWriteValueDefault', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WRITE_VALUE_NO_MEMORY_BARRIER', ('hipStreamWriteValueNoMemoryBarrier', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_MEM_OP_WAIT_VALUE_32', ('hipStreamBatchMemOpWaitValue32', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_MEM_OP_WRITE_VALUE_32', ('hipStreamBatchMemOpWriteValue32', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_MEM_OP_FLUSH_REMOTE_WRITES', ('hipStreamBatchMemOpFlushRemoteWrites', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cuGetErrorName', ('hipGetErrorName___', CONV_ERROR, API_DRIVER, HIP_UNSUPPORTED)), ('cuGetErrorString', ('hipGetErrorString___', CONV_ERROR, API_DRIVER, HIP_UNSUPPORTED)), ('cuInit', ('hipInit', CONV_INIT, API_DRIVER)), ('cuDriverGetVersion', ('hipDriverGetVersion', CONV_VERSION, API_DRIVER)), ('cuCtxCreate_v2', ('hipCtxCreate', CONV_CONTEXT, API_DRIVER)), ('cuCtxDestroy_v2', ('hipCtxDestroy', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetApiVersion', ('hipCtxGetApiVersion', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetCacheConfig', ('hipCtxGetCacheConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetCurrent', ('hipCtxGetCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetDevice', ('hipCtxGetDevice', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetFlags', ('hipCtxGetFlags', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetLimit', ('hipCtxGetLimit', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxGetSharedMemConfig', ('hipCtxGetSharedMemConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetStreamPriorityRange', ('hipCtxGetStreamPriorityRange', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxPopCurrent_v2', ('hipCtxPopCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxPushCurrent_v2', ('hipCtxPushCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxSetCacheConfig', ('hipCtxSetCacheConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxSetCurrent', ('hipCtxSetCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxSetLimit', ('hipCtxSetLimit', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxSetSharedMemConfig', ('hipCtxSetSharedMemConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxSynchronize', ('hipCtxSynchronize', CONV_CONTEXT, API_DRIVER)), ('cuCtxAttach', ('hipCtxAttach', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxDetach', ('hipCtxDetach', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxEnablePeerAccess', ('hipCtxEnablePeerAccess', CONV_PEER, API_DRIVER)), ('cuCtxDisablePeerAccess', ('hipCtxDisablePeerAccess', CONV_PEER, API_DRIVER)), ('cuDeviceCanAccessPeer', ('hipDeviceCanAccessPeer', CONV_PEER, API_DRIVER)), ('cuDeviceGetP2PAttribute', ('hipDeviceGetP2PAttribute', CONV_PEER, API_DRIVER, HIP_UNSUPPORTED)), ('cuDevicePrimaryCtxGetState', ('hipDevicePrimaryCtxGetState', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxRelease', ('hipDevicePrimaryCtxRelease', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxReset', ('hipDevicePrimaryCtxReset', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxRetain', ('hipDevicePrimaryCtxRetain', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxSetFlags', ('hipDevicePrimaryCtxSetFlags', CONV_CONTEXT, API_DRIVER)), ('cuDeviceGet', ('hipGetDevice', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetName', ('hipDeviceGetName', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetCount', ('hipGetDeviceCount', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetAttribute', ('hipDeviceGetAttribute', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetPCIBusId', ('hipDeviceGetPCIBusId', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetByPCIBusId', ('hipDeviceGetByPCIBusId', CONV_DEVICE, API_DRIVER)), ('cuDeviceTotalMem_v2', ('hipDeviceTotalMem', CONV_DEVICE, API_DRIVER)), ('cuDeviceComputeCapability', ('hipDeviceComputeCapability', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetProperties', ('hipGetDeviceProperties', CONV_DEVICE, API_DRIVER)), ('cuLinkAddData', ('hipLinkAddData', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkAddFile', ('hipLinkAddFile', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkComplete', ('hipLinkComplete', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkCreate', ('hipLinkCreate', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkDestroy', ('hipLinkDestroy', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuModuleGetFunction', ('hipModuleGetFunction', CONV_MODULE, API_DRIVER)), ('cuModuleGetGlobal_v2', ('hipModuleGetGlobal', CONV_MODULE, API_DRIVER)), ('cuModuleGetSurfRef', ('hipModuleGetSurfRef', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuModuleGetTexRef', ('hipModuleGetTexRef', CONV_MODULE, API_DRIVER)), ('cuModuleLoad', ('hipModuleLoad', CONV_MODULE, API_DRIVER)), ('cuModuleLoadData', ('hipModuleLoadData', CONV_MODULE, API_DRIVER)), ('cuModuleLoadDataEx', ('hipModuleLoadDataEx', CONV_MODULE, API_DRIVER)), ('cuModuleLoadFatBinary', ('hipModuleLoadFatBinary', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuModuleUnload', ('hipModuleUnload', CONV_MODULE, API_DRIVER)), ('CU_DEVICE_P2P_ATTRIBUTE_PERFORMANCE_RANK', ('hipDeviceP2PAttributePerformanceRank', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_P2P_ATTRIBUTE_ACCESS_SUPPORTED', ('hipDeviceP2PAttributeAccessSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_P2P_ATTRIBUTE_NATIVE_ATOMIC_SUPPORTED', ('hipDeviceP2PAttributeNativeAtomicSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_EVENT_DEFAULT', ('hipEventDefault', CONV_EVENT, API_DRIVER)), ('CU_EVENT_BLOCKING_SYNC', ('hipEventBlockingSync', CONV_EVENT, API_DRIVER)), ('CU_EVENT_DISABLE_TIMING', ('hipEventDisableTiming', CONV_EVENT, API_DRIVER)), ('CU_EVENT_INTERPROCESS', ('hipEventInterprocess', CONV_EVENT, API_DRIVER)), ('cuEventCreate', ('hipEventCreate', CONV_EVENT, API_DRIVER)), ('cuEventDestroy_v2', ('hipEventDestroy', CONV_EVENT, API_DRIVER)), ('cuEventElapsedTime', ('hipEventElapsedTime', CONV_EVENT, API_DRIVER)), ('cuEventQuery', ('hipEventQuery', CONV_EVENT, API_DRIVER)), ('cuEventRecord', ('hipEventRecord', CONV_EVENT, API_DRIVER)), ('cuEventSynchronize', ('hipEventSynchronize', CONV_EVENT, API_DRIVER)), ('cuFuncGetAttribute', ('hipFuncGetAttribute', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuFuncSetCacheConfig', ('hipFuncSetCacheConfig', CONV_MODULE, API_DRIVER)), ('cuFuncSetSharedMemConfig', ('hipFuncSetSharedMemConfig', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunchKernel', ('hipModuleLaunchKernel', CONV_MODULE, API_DRIVER)), ('cuFuncSetBlockShape', ('hipFuncSetBlockShape', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuFuncSetSharedSize', ('hipFuncSetSharedSize', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunch', ('hipLaunch', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunchGrid', ('hipLaunchGrid', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunchGridAsync', ('hipLaunchGridAsync', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetf', ('hipParamSetf', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSeti', ('hipParamSeti', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetSize', ('hipParamSetSize', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetSize', ('hipParamSetSize', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetv', ('hipParamSetv', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuOccupancyMaxActiveBlocksPerMultiprocessor', ('hipOccupancyMaxActiveBlocksPerMultiprocessor', CONV_OCCUPANCY, API_DRIVER)), ('cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', ('hipOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', CONV_OCCUPANCY, API_DRIVER, HIP_UNSUPPORTED)), ('cuOccupancyMaxPotentialBlockSize', ('hipOccupancyMaxPotentialBlockSize', CONV_OCCUPANCY, API_DRIVER)), ('cuOccupancyMaxPotentialBlockSizeWithFlags', ('hipOccupancyMaxPotentialBlockSizeWithFlags', CONV_OCCUPANCY, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamAddCallback', ('hipStreamAddCallback', CONV_STREAM, API_DRIVER)), ('cuStreamAttachMemAsync', ('hipStreamAttachMemAsync', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamCreate', ('hipStreamCreate__', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamCreateWithPriority', ('hipStreamCreateWithPriority', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamDestroy_v2', ('hipStreamDestroy', CONV_STREAM, API_DRIVER)), ('cuStreamGetFlags', ('hipStreamGetFlags', CONV_STREAM, API_DRIVER)), ('cuStreamGetPriority', ('hipStreamGetPriority', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamQuery', ('hipStreamQuery', CONV_STREAM, API_DRIVER)), ('cuStreamSynchronize', ('hipStreamSynchronize', CONV_STREAM, API_DRIVER)), ('cuStreamWaitEvent', ('hipStreamWaitEvent', CONV_STREAM, API_DRIVER)), ('cuStreamWaitValue32', ('hipStreamWaitValue32', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamWriteValue32', ('hipStreamWriteValue32', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamBatchMemOp', ('hipStreamBatchMemOp', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArray3DCreate', ('hipArray3DCreate', CONV_MEM, API_DRIVER)), ('cuArray3DGetDescriptor', ('hipArray3DGetDescriptor', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArrayCreate', ('hipArrayCreate', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArrayDestroy', ('hipArrayDestroy', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArrayGetDescriptor', ('hipArrayGetDescriptor', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcCloseMemHandle', ('hipIpcCloseMemHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcGetEventHandle', ('hipIpcGetEventHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcGetMemHandle', ('hipIpcGetMemHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcOpenEventHandle', ('hipIpcOpenEventHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcOpenMemHandle', ('hipIpcOpenMemHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAlloc_v2', ('hipMalloc', CONV_MEM, API_DRIVER)), ('cuMemAllocHost', ('hipMemAllocHost', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAllocManaged', ('hipMemAllocManaged', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAllocPitch', ('hipMemAllocPitch__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy', ('hipMemcpy__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy2D', ('hipMemcpy2D__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy2DAsync', ('hipMemcpy2DAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy2DUnaligned', ('hipMemcpy2DUnaligned', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3D', ('hipMemcpy3D__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3DAsync', ('hipMemcpy3DAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3DPeer', ('hipMemcpy3DPeer__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3DPeerAsync', ('hipMemcpy3DPeerAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAsync', ('hipMemcpyAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoA', ('hipMemcpyAtoA', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoD', ('hipMemcpyAtoD', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoH', ('hipMemcpyAtoH', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoHAsync', ('hipMemcpyAtoHAsync', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyDtoA', ('hipMemcpyDtoA', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyDtoD_v2', ('hipMemcpyDtoD', CONV_MEM, API_DRIVER)), ('cuMemcpyDtoDAsync_v2', ('hipMemcpyDtoDAsync', CONV_MEM, API_DRIVER)), ('cuMemcpyDtoH_v2', ('hipMemcpyDtoH', CONV_MEM, API_DRIVER)), ('cuMemcpyDtoHAsync_v2', ('hipMemcpyDtoHAsync', CONV_MEM, API_DRIVER)), ('cuMemcpyHtoA', ('hipMemcpyHtoA', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyHtoAAsync', ('hipMemcpyHtoAAsync', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyHtoD_v2', ('hipMemcpyHtoD', CONV_MEM, API_DRIVER)), ('cuMemcpyHtoDAsync_v2', ('hipMemcpyHtoDAsync', CONV_MEM, API_DRIVER)), ('cuMemcpyPeerAsync', ('hipMemcpyPeerAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyPeer', ('hipMemcpyPeer__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemFree_v2', ('hipFree', CONV_MEM, API_DRIVER)), ('cuMemFreeHost', ('hipHostFree', CONV_MEM, API_DRIVER)), ('cuMemGetAddressRange', ('hipMemGetAddressRange', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemGetInfo_v2', ('hipMemGetInfo', CONV_MEM, API_DRIVER)), ('cuMemHostAlloc', ('hipHostMalloc', CONV_MEM, API_DRIVER)), ('cuMemHostGetDevicePointer', ('hipMemHostGetDevicePointer', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemHostGetFlags', ('hipMemHostGetFlags', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemHostRegister_v2', ('hipHostRegister', CONV_MEM, API_DRIVER)), ('cuMemHostUnregister', ('hipHostUnregister', CONV_MEM, API_DRIVER)), ('cuMemsetD16_v2', ('hipMemsetD16', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD16Async', ('hipMemsetD16Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D16_v2', ('hipMemsetD2D16', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D16Async', ('hipMemsetD2D16Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D32_v2', ('hipMemsetD2D32', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D32Async', ('hipMemsetD2D32Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D8_v2', ('hipMemsetD2D8', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D8Async', ('hipMemsetD2D8Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD32_v2', ('hipMemset', CONV_MEM, API_DRIVER)), ('cuMemsetD32Async', ('hipMemsetAsync', CONV_MEM, API_DRIVER)), ('cuMemsetD8_v2', ('hipMemsetD8', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD8Async', ('hipMemsetD8Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMipmappedArrayCreate', ('hipMipmappedArrayCreate', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMipmappedArrayDestroy', ('hipMipmappedArrayDestroy', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMipmappedArrayGetLevel', ('hipMipmappedArrayGetLevel', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemPrefetchAsync', ('hipMemPrefetchAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAdvise', ('hipMemAdvise', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemRangeGetAttribute', ('hipMemRangeGetAttribute', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemRangeGetAttributes', ('hipMemRangeGetAttributes', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuPointerGetAttribute', ('hipPointerGetAttribute', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuPointerGetAttributes', ('hipPointerGetAttributes', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuPointerSetAttribute', ('hipPointerSetAttribute', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_FILTER_MODE_POINT', ('hipFilterModePoint', CONV_TEX, API_DRIVER)), ('CU_TR_FILTER_MODE_LINEAR', ('hipFilterModeLinear', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetAddress', ('hipTexRefGetAddress', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetAddressMode', ('hipTexRefGetAddressMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetArray', ('hipTexRefGetArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetBorderColor', ('hipTexRefGetBorderColor', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetFilterMode', ('hipTexRefGetFilterMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetFlags', ('hipTexRefGetFlags', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetFormat', ('hipTexRefGetFormat', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMaxAnisotropy', ('hipTexRefGetMaxAnisotropy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmapFilterMode', ('hipTexRefGetMipmapFilterMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmapLevelBias', ('hipTexRefGetMipmapLevelBias', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmapLevelClamp', ('hipTexRefGetMipmapLevelClamp', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmappedArray', ('hipTexRefGetMipmappedArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetAddress', ('hipTexRefSetAddress', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetAddress2D', ('hipTexRefSetAddress2D', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetAddressMode', ('hipTexRefSetAddressMode', CONV_TEX, API_DRIVER)), ('cuTexRefSetArray', ('hipTexRefSetArray', CONV_TEX, API_DRIVER)), ('cuTexRefSetBorderColor', ('hipTexRefSetBorderColor', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetFilterMode', ('hipTexRefSetFilterMode', CONV_TEX, API_DRIVER)), ('cuTexRefSetFlags', ('hipTexRefSetFlags', CONV_TEX, API_DRIVER)), ('cuTexRefSetFormat', ('hipTexRefSetFormat', CONV_TEX, API_DRIVER)), ('cuTexRefSetMaxAnisotropy', ('hipTexRefSetMaxAnisotropy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmapFilterMode', ('hipTexRefSetMipmapFilterMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmapLevelBias', ('hipTexRefSetMipmapLevelBias', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmapLevelClamp', ('hipTexRefSetMipmapLevelClamp', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmappedArray', ('hipTexRefSetMipmappedArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefCreate', ('hipTexRefCreate', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefDestroy', ('hipTexRefDestroy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfRefGetArray', ('hipSurfRefGetArray', CONV_SURFACE, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfRefSetArray', ('hipSurfRefSetArray', CONV_SURFACE, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectCreate', ('hipTexObjectCreate', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectDestroy', ('hipTexObjectDestroy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectGetResourceDesc', ('hipTexObjectGetResourceDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectGetResourceViewDesc', ('hipTexObjectGetResourceViewDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectGetTextureDesc', ('hipTexObjectGetTextureDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfObjectCreate', ('hipSurfObjectCreate', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfObjectDestroy', ('hipSurfObjectDestroy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfObjectGetResourceDesc', ('hipSurfObjectGetResourceDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsMapResources', ('hipGraphicsMapResources', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceGetMappedMipmappedArray', ('hipGraphicsResourceGetMappedMipmappedArray', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceGetMappedPointer', ('hipGraphicsResourceGetMappedPointer', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceSetMapFlags', ('hipGraphicsResourceSetMapFlags', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsSubResourceGetMappedArray', ('hipGraphicsSubResourceGetMappedArray', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsUnmapResources', ('hipGraphicsUnmapResources', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsUnregisterResource', ('hipGraphicsUnregisterResource', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuProfilerInitialize', ('hipProfilerInitialize', CONV_OTHER, API_DRIVER, HIP_UNSUPPORTED)), ('cuProfilerStart', ('hipProfilerStart', CONV_OTHER, API_DRIVER)), ('cuProfilerStop', ('hipProfilerStop', CONV_OTHER, API_DRIVER)), ('CU_GL_DEVICE_LIST_ALL', ('HIP_GL_DEVICE_LIST_ALL', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_DEVICE_LIST_CURRENT_FRAME', ('HIP_GL_DEVICE_LIST_CURRENT_FRAME', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_DEVICE_LIST_NEXT_FRAME', ('HIP_GL_DEVICE_LIST_NEXT_FRAME', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLGetDevices', ('hipGLGetDevices', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsGLRegisterBuffer', ('hipGraphicsGLRegisterBuffer', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsGLRegisterImage', ('hipGraphicsGLRegisterImage', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuWGLGetDevice', ('hipWGLGetDevice', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_MAP_RESOURCE_FLAGS_NONE', ('HIP_GL_MAP_RESOURCE_FLAGS_NONE', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_MAP_RESOURCE_FLAGS_READ_ONLY', ('HIP_GL_MAP_RESOURCE_FLAGS_READ_ONLY', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD', ('HIP_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLCtxCreate', ('hipGLCtxCreate', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLInit', ('hipGLInit', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLMapBufferObject', ('hipGLMapBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLMapBufferObjectAsync', ('hipGLMapBufferObjectAsync', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLRegisterBufferObject', ('hipGLRegisterBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLSetBufferObjectMapFlags', ('hipGLSetBufferObjectMapFlags', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLUnmapBufferObject', ('hipGLUnmapBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLUnmapBufferObjectAsync', ('hipGLUnmapBufferObjectAsync', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLUnregisterBufferObject', ('hipGLUnregisterBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_DEVICE_LIST_ALL', ('HIP_D3D9_DEVICE_LIST_ALL', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_DEVICE_LIST_CURRENT_FRAME', ('HIP_D3D9_DEVICE_LIST_CURRENT_FRAME', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_DEVICE_LIST_NEXT_FRAME', ('HIP_D3D9_DEVICE_LIST_NEXT_FRAME', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9CtxCreate', ('hipD3D9CtxCreate', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9CtxCreateOnDevice', ('hipD3D9CtxCreateOnDevice', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9GetDevice', ('hipD3D9GetDevice', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9GetDevices', ('hipD3D9GetDevices', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9GetDirect3DDevice', ('hipD3D9GetDirect3DDevice', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsD3D9RegisterResource', ('hipGraphicsD3D9RegisterResource', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_MAPRESOURCE_FLAGS_NONE', ('HIP_D3D9_MAPRESOURCE_FLAGS_NONE', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_MAPRESOURCE_FLAGS_READONLY', ('HIP_D3D9_MAPRESOURCE_FLAGS_READONLY', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD', ('HIP_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_REGISTER_FLAGS_NONE', ('HIP_D3D9_REGISTER_FLAGS_NONE', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_REGISTER_FLAGS_ARRAY', ('HIP_D3D9_REGISTER_FLAGS_ARRAY', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9MapResources', ('hipD3D9MapResources', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9RegisterResource', ('hipD3D9RegisterResource', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedArray', ('hipD3D9ResourceGetMappedArray', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedPitch', ('hipD3D9ResourceGetMappedPitch', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedPointer', ('hipD3D9ResourceGetMappedPointer', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedSize', ('hipD3D9ResourceGetMappedSize', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetSurfaceDimensions', ('hipD3D9ResourceGetSurfaceDimensions', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceSetMapFlags', ('hipD3D9ResourceSetMapFlags', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9UnmapResources', ('hipD3D9UnmapResources', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9UnregisterResource', ('hipD3D9UnregisterResource', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_DEVICE_LIST_ALL', ('HIP_D3D10_DEVICE_LIST_ALL', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_DEVICE_LIST_CURRENT_FRAME', ('HIP_D3D10_DEVICE_LIST_CURRENT_FRAME', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_DEVICE_LIST_NEXT_FRAME', ('HIP_D3D10_DEVICE_LIST_NEXT_FRAME', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10GetDevice', ('hipD3D10GetDevice', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10GetDevices', ('hipD3D10GetDevices', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsD3D10RegisterResource', ('hipGraphicsD3D10RegisterResource', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_MAPRESOURCE_FLAGS_NONE', ('HIP_D3D10_MAPRESOURCE_FLAGS_NONE', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_MAPRESOURCE_FLAGS_READONLY', ('HIP_D3D10_MAPRESOURCE_FLAGS_READONLY', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD', ('HIP_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_REGISTER_FLAGS_NONE', ('HIP_D3D10_REGISTER_FLAGS_NONE', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_REGISTER_FLAGS_ARRAY', ('HIP_D3D10_REGISTER_FLAGS_ARRAY', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10CtxCreate', ('hipD3D10CtxCreate', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10CtxCreateOnDevice', ('hipD3D10CtxCreateOnDevice', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10GetDirect3DDevice', ('hipD3D10GetDirect3DDevice', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10MapResources', ('hipD3D10MapResources', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10RegisterResource', ('hipD3D10RegisterResource', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedArray', ('hipD3D10ResourceGetMappedArray', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedPitch', ('hipD3D10ResourceGetMappedPitch', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedPointer', ('hipD3D10ResourceGetMappedPointer', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedSize', ('hipD3D10ResourceGetMappedSize', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetSurfaceDimensions', ('hipD3D10ResourceGetSurfaceDimensions', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD310ResourceSetMapFlags', ('hipD3D10ResourceSetMapFlags', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10UnmapResources', ('hipD3D10UnmapResources', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10UnregisterResource', ('hipD3D10UnregisterResource', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D11_DEVICE_LIST_ALL', ('HIP_D3D11_DEVICE_LIST_ALL', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D11_DEVICE_LIST_CURRENT_FRAME', ('HIP_D3D11_DEVICE_LIST_CURRENT_FRAME', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D11_DEVICE_LIST_NEXT_FRAME', ('HIP_D3D11_DEVICE_LIST_NEXT_FRAME', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11GetDevice', ('hipD3D11GetDevice', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11GetDevices', ('hipD3D11GetDevices', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsD3D11RegisterResource', ('hipGraphicsD3D11RegisterResource', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11CtxCreate', ('hipD3D11CtxCreate', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11CtxCreateOnDevice', ('hipD3D11CtxCreateOnDevice', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11GetDirect3DDevice', ('hipD3D11GetDirect3DDevice', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsVDPAURegisterOutputSurface', ('hipGraphicsVDPAURegisterOutputSurface', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsVDPAURegisterVideoSurface', ('hipGraphicsVDPAURegisterVideoSurface', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuVDPAUGetDevice', ('hipVDPAUGetDevice', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuVDPAUCtxCreate', ('hipVDPAUCtxCreate', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerAcquireFrame', ('hipEGLStreamConsumerAcquireFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerConnect', ('hipEGLStreamConsumerConnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerConnectWithFlags', ('hipEGLStreamConsumerConnectWithFlags', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerDisconnect', ('hipEGLStreamConsumerDisconnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerReleaseFrame', ('hipEGLStreamConsumerReleaseFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerConnect', ('hipEGLStreamProducerConnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerDisconnect', ('hipEGLStreamProducerDisconnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerPresentFrame', ('hipEGLStreamProducerPresentFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerReturnFrame', ('hipEGLStreamProducerReturnFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsEGLRegisterImage', ('hipGraphicsEGLRegisterImage', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceGetMappedEglFrame', ('hipGraphicsResourceGetMappedEglFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cudaDataType_t', ('hipDataType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDataType', ('hipDataType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_16F', ('hipR16F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_16F', ('hipC16F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_32F', ('hipR32F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_32F', ('hipC32F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_64F', ('hipR64F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_64F', ('hipC64F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_8I', ('hipR8I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_8I', ('hipC8I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_8U', ('hipR8U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_8U', ('hipC8U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_32I', ('hipR32I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_32I', ('hipC32I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_32U', ('hipR32U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_32U', ('hipC32U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('MAJOR_VERSION', ('hipLibraryMajorVersion', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('MINOR_VERSION', ('hipLibraryMinorVersion', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('PATCH_LEVEL', ('hipLibraryPatchVersion', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAttachGlobal', ('hipMemAttachGlobal', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAttachHost', ('hipMemAttachHost', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAttachSingle', ('hipMemAttachSingle', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyDefault', ('hipOccupancyDefault', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyDisableCachingOverride', ('hipOccupancyDisableCachingOverride', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetLastError', ('hipGetLastError', CONV_ERROR, API_RUNTIME)), ('cudaPeekAtLastError', ('hipPeekAtLastError', CONV_ERROR, API_RUNTIME)), ('cudaGetErrorName', ('hipGetErrorName', CONV_ERROR, API_RUNTIME)), ('cudaGetErrorString', ('hipGetErrorString', CONV_ERROR, API_RUNTIME)), ('cudaMemcpy3DParms', ('hipMemcpy3DParms', CONV_MEM, API_RUNTIME)), ('cudaMemcpy3DPeerParms', ('hipMemcpy3DPeerParms', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy', ('hipMemcpy', CONV_MEM, API_RUNTIME)), ('cudaMemcpyToArray', ('hipMemcpyToArray', CONV_MEM, API_RUNTIME)), ('cudaMemcpyToSymbol', ('hipMemcpyToSymbol', CONV_MEM, API_RUNTIME)), ('cudaMemcpyToSymbolAsync', ('hipMemcpyToSymbolAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpyAsync', ('hipMemcpyAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2D', ('hipMemcpy2D', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2DAsync', ('hipMemcpy2DAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2DToArray', ('hipMemcpy2DToArray', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2DArrayToArray', ('hipMemcpy2DArrayToArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy2DFromArray', ('hipMemcpy2DFromArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy2DFromArrayAsync', ('hipMemcpy2DFromArrayAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy2DToArrayAsync', ('hipMemcpy2DToArrayAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy3D', ('hipMemcpy3D', CONV_MEM, API_RUNTIME)), ('cudaMemcpy3DAsync', ('hipMemcpy3DAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy3DPeer', ('hipMemcpy3DPeer', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy3DPeerAsync', ('hipMemcpy3DPeerAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyArrayToArray', ('hipMemcpyArrayToArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyFromArrayAsync', ('hipMemcpyFromArrayAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyFromSymbol', ('hipMemcpyFromSymbol', CONV_MEM, API_RUNTIME)), ('cudaMemcpyFromSymbolAsync', ('hipMemcpyFromSymbolAsync', CONV_MEM, API_RUNTIME)), ('cudaMemAdvise', ('hipMemAdvise', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeGetAttribute', ('hipMemRangeGetAttribute', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeGetAttributes', ('hipMemRangeGetAttributes', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseSetReadMostly', ('hipMemAdviseSetReadMostly', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseUnsetReadMostly', ('hipMemAdviseUnsetReadMostly', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseSetPreferredLocation', ('hipMemAdviseSetPreferredLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseUnsetPreferredLocation', ('hipMemAdviseUnsetPreferredLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseSetAccessedBy', ('hipMemAdviseSetAccessedBy', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseUnsetAccessedBy', ('hipMemAdviseUnsetAccessedBy', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributeReadMostly', ('hipMemRangeAttributeReadMostly', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributePreferredLocation', ('hipMemRangeAttributePreferredLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributeAccessedBy', ('hipMemRangeAttributeAccessedBy', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributeLastPrefetchLocation', ('hipMemRangeAttributeLastPrefetchLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyHostToHost', ('hipMemcpyHostToHost', CONV_MEM, API_RUNTIME)), ('cudaMemcpyHostToDevice', ('hipMemcpyHostToDevice', CONV_MEM, API_RUNTIME)), ('cudaMemcpyDeviceToHost', ('hipMemcpyDeviceToHost', CONV_MEM, API_RUNTIME)), ('cudaMemcpyDeviceToDevice', ('hipMemcpyDeviceToDevice', CONV_MEM, API_RUNTIME)), ('cudaMemcpyDefault', ('hipMemcpyDefault', CONV_MEM, API_RUNTIME)), ('cudaMemset', ('hipMemset', CONV_MEM, API_RUNTIME)), ('cudaMemsetAsync', ('hipMemsetAsync', CONV_MEM, API_RUNTIME)), ('cudaMemset2D', ('hipMemset2D', CONV_MEM, API_RUNTIME)), ('cudaMemset2DAsync', ('hipMemset2DAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemset3D', ('hipMemset3D', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemset3DAsync', ('hipMemset3DAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemGetInfo', ('hipMemGetInfo', CONV_MEM, API_RUNTIME)), ('cudaArrayGetInfo', ('hipArrayGetInfo', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFreeMipmappedArray', ('hipFreeMipmappedArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetMipmappedArrayLevel', ('hipGetMipmappedArrayLevel', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSymbolAddress', ('hipGetSymbolAddress', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSymbolSize', ('hipGetSymbolSize', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemPrefetchAsync', ('hipMemPrefetchAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMallocHost', ('hipHostMalloc', CONV_MEM, API_RUNTIME)), ('cudaMallocArray', ('hipMallocArray', CONV_MEM, API_RUNTIME)), ('cudaMalloc', ('hipMalloc', CONV_MEM, API_RUNTIME)), ('cudaMalloc3D', ('hipMalloc3D', CONV_MEM, API_RUNTIME)), ('cudaMalloc3DArray', ('hipMalloc3DArray', CONV_MEM, API_RUNTIME)), ('cudaMallocManaged', ('hipMallocManaged', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMallocMipmappedArray', ('hipMallocMipmappedArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMallocPitch', ('hipMallocPitch', CONV_MEM, API_RUNTIME)), ('cudaFreeHost', ('hipHostFree', CONV_MEM, API_RUNTIME)), ('cudaFreeArray', ('hipFreeArray', CONV_MEM, API_RUNTIME)), ('cudaFree', ('hipFree', CONV_MEM, API_RUNTIME)), ('cudaHostRegister', ('hipHostRegister', CONV_MEM, API_RUNTIME)), ('cudaHostUnregister', ('hipHostUnregister', CONV_MEM, API_RUNTIME)), ('cudaHostAlloc', ('hipHostMalloc', CONV_MEM, API_RUNTIME)), ('cudaMemoryTypeHost', ('hipMemoryTypeHost', CONV_MEM, API_RUNTIME)), ('cudaMemoryTypeDevice', ('hipMemoryTypeDevice', CONV_MEM, API_RUNTIME)), ('make_cudaExtent', ('make_hipExtent', CONV_MEM, API_RUNTIME)), ('make_cudaPitchedPtr', ('make_hipPitchedPtr', CONV_MEM, API_RUNTIME)), ('make_cudaPos', ('make_hipPos', CONV_MEM, API_RUNTIME)), ('cudaHostAllocDefault', ('hipHostMallocDefault', CONV_MEM, API_RUNTIME)), ('cudaHostAllocPortable', ('hipHostMallocPortable', CONV_MEM, API_RUNTIME)), ('cudaHostAllocMapped', ('hipHostMallocMapped', CONV_MEM, API_RUNTIME)), ('cudaHostAllocWriteCombined', ('hipHostMallocWriteCombined', CONV_MEM, API_RUNTIME)), ('cudaHostGetFlags', ('hipHostGetFlags', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterDefault', ('hipHostRegisterDefault', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterPortable', ('hipHostRegisterPortable', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterMapped', ('hipHostRegisterMapped', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterIoMemory', ('hipHostRegisterIoMemory', CONV_MEM, API_RUNTIME)), ('cudaEventCreate', ('hipEventCreate', CONV_EVENT, API_RUNTIME)), ('cudaEventCreateWithFlags', ('hipEventCreateWithFlags', CONV_EVENT, API_RUNTIME)), ('cudaEventDestroy', ('hipEventDestroy', CONV_EVENT, API_RUNTIME)), ('cudaEventRecord', ('hipEventRecord', CONV_EVENT, API_RUNTIME)), ('cudaEventElapsedTime', ('hipEventElapsedTime', CONV_EVENT, API_RUNTIME)), ('cudaEventSynchronize', ('hipEventSynchronize', CONV_EVENT, API_RUNTIME)), ('cudaEventQuery', ('hipEventQuery', CONV_EVENT, API_RUNTIME)), ('cudaEventDefault', ('hipEventDefault', CONV_EVENT, API_RUNTIME)), ('cudaEventBlockingSync', ('hipEventBlockingSync', CONV_EVENT, API_RUNTIME)), ('cudaEventDisableTiming', ('hipEventDisableTiming', CONV_EVENT, API_RUNTIME)), ('cudaEventInterprocess', ('hipEventInterprocess', CONV_EVENT, API_RUNTIME)), ('cudaStreamCreate', ('hipStreamCreate', CONV_STREAM, API_RUNTIME)), ('cudaStreamCreateWithFlags', ('hipStreamCreateWithFlags', CONV_STREAM, API_RUNTIME)), ('cudaStreamCreateWithPriority', ('hipStreamCreateWithPriority', CONV_STREAM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamDestroy', ('hipStreamDestroy', CONV_STREAM, API_RUNTIME)), ('cudaStreamWaitEvent', ('hipStreamWaitEvent', CONV_STREAM, API_RUNTIME)), ('cudaStreamSynchronize', ('hipStreamSynchronize', CONV_STREAM, API_RUNTIME)), ('cudaStreamGetFlags', ('hipStreamGetFlags', CONV_STREAM, API_RUNTIME)), ('cudaStreamQuery', ('hipStreamQuery', CONV_STREAM, API_RUNTIME)), ('cudaStreamAddCallback', ('hipStreamAddCallback', CONV_STREAM, API_RUNTIME)), ('cudaStreamAttachMemAsync', ('hipStreamAttachMemAsync', CONV_STREAM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamGetPriority', ('hipStreamGetPriority', CONV_STREAM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamDefault', ('hipStreamDefault', CONV_TYPE, API_RUNTIME)), ('cudaStreamNonBlocking', ('hipStreamNonBlocking', CONV_TYPE, API_RUNTIME)), ('cudaDeviceSynchronize', ('hipDeviceSynchronize', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceReset', ('hipDeviceReset', CONV_DEVICE, API_RUNTIME)), ('cudaSetDevice', ('hipSetDevice', CONV_DEVICE, API_RUNTIME)), ('cudaGetDevice', ('hipGetDevice', CONV_DEVICE, API_RUNTIME)), ('cudaGetDeviceCount', ('hipGetDeviceCount', CONV_DEVICE, API_RUNTIME)), ('cudaChooseDevice', ('hipChooseDevice', CONV_DEVICE, API_RUNTIME)), ('cudaThreadExit', ('hipDeviceReset', CONV_THREAD, API_RUNTIME)), ('cudaThreadGetCacheConfig', ('hipDeviceGetCacheConfig', CONV_THREAD, API_RUNTIME)), ('cudaThreadGetLimit', ('hipThreadGetLimit', CONV_THREAD, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaThreadSetCacheConfig', ('hipDeviceSetCacheConfig', CONV_THREAD, API_RUNTIME)), ('cudaThreadSetLimit', ('hipThreadSetLimit', CONV_THREAD, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaThreadSynchronize', ('hipDeviceSynchronize', CONV_THREAD, API_RUNTIME)), ('cudaDeviceGetAttribute', ('hipDeviceGetAttribute', CONV_DEVICE, API_RUNTIME)), ('cudaDevAttrMaxThreadsPerBlock', ('hipDeviceAttributeMaxThreadsPerBlock', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxBlockDimX', ('hipDeviceAttributeMaxBlockDimX', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxBlockDimY', ('hipDeviceAttributeMaxBlockDimY', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxBlockDimZ', ('hipDeviceAttributeMaxBlockDimZ', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxGridDimX', ('hipDeviceAttributeMaxGridDimX', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxGridDimY', ('hipDeviceAttributeMaxGridDimY', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxGridDimZ', ('hipDeviceAttributeMaxGridDimZ', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxSharedMemoryPerBlock', ('hipDeviceAttributeMaxSharedMemoryPerBlock', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrTotalConstantMemory', ('hipDeviceAttributeTotalConstantMemory', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrWarpSize', ('hipDeviceAttributeWarpSize', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxPitch', ('hipDeviceAttributeMaxPitch', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxRegistersPerBlock', ('hipDeviceAttributeMaxRegistersPerBlock', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrClockRate', ('hipDeviceAttributeClockRate', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrTextureAlignment', ('hipDeviceAttributeTextureAlignment', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrGpuOverlap', ('hipDeviceAttributeGpuOverlap', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMultiProcessorCount', ('hipDeviceAttributeMultiprocessorCount', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrKernelExecTimeout', ('hipDeviceAttributeKernelExecTimeout', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrIntegrated', ('hipDeviceAttributeIntegrated', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrCanMapHostMemory', ('hipDeviceAttributeCanMapHostMemory', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrComputeMode', ('hipDeviceAttributeComputeMode', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxTexture1DWidth', ('hipDeviceAttributeMaxTexture1DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DWidth', ('hipDeviceAttributeMaxTexture2DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DHeight', ('hipDeviceAttributeMaxTexture2DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DWidth', ('hipDeviceAttributeMaxTexture3DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DHeight', ('hipDeviceAttributeMaxTexture3DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DDepth', ('hipDeviceAttributeMaxTexture3DDepth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLayeredWidth', ('hipDeviceAttributeMaxTexture2DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLayeredHeight', ('hipDeviceAttributeMaxTexture2DLayeredHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLayeredLayers', ('hipDeviceAttributeMaxTexture2DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrSurfaceAlignment', ('hipDeviceAttributeSurfaceAlignment', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrConcurrentKernels', ('hipDeviceAttributeConcurrentKernels', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrEccEnabled', ('hipDeviceAttributeEccEnabled', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrPciBusId', ('hipDeviceAttributePciBusId', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrPciDeviceId', ('hipDeviceAttributePciDeviceId', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrTccDriver', ('hipDeviceAttributeTccDriver', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMemoryClockRate', ('hipDeviceAttributeMemoryClockRate', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrGlobalMemoryBusWidth', ('hipDeviceAttributeMemoryBusWidth', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrL2CacheSize', ('hipDeviceAttributeL2CacheSize', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxThreadsPerMultiProcessor', ('hipDeviceAttributeMaxThreadsPerMultiProcessor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrAsyncEngineCount', ('hipDeviceAttributeAsyncEngineCount', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrUnifiedAddressing', ('hipDeviceAttributeUnifiedAddressing', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture1DLayeredWidth', ('hipDeviceAttributeMaxTexture1DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture1DLayeredLayers', ('hipDeviceAttributeMaxTexture1DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DGatherWidth', ('hipDeviceAttributeMaxTexture2DGatherWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DGatherHeight', ('hipDeviceAttributeMaxTexture2DGatherHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DWidthAlt', ('hipDeviceAttributeMaxTexture3DWidthAlternate', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DHeightAlt', ('hipDeviceAttributeMaxTexture3DHeightAlternate', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DDepthAlt', ('hipDeviceAttributeMaxTexture3DDepthAlternate', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrPciDomainId', ('hipDeviceAttributePciDomainId', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrTexturePitchAlignment', ('hipDeviceAttributeTexturePitchAlignment', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTextureCubemapWidth', ('hipDeviceAttributeMaxTextureCubemapWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTextureCubemapLayeredWidth', ('hipDeviceAttributeMaxTextureCubemapLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTextureCubemapLayeredLayers', ('hipDeviceAttributeMaxTextureCubemapLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface1DWidth', ('hipDeviceAttributeMaxSurface1DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DWidth', ('hipDeviceAttributeMaxSurface2DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DHeight', ('hipDeviceAttributeMaxSurface2DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface3DWidth', ('hipDeviceAttributeMaxSurface3DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface3DHeight', ('hipDeviceAttributeMaxSurface3DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface3DDepth', ('hipDeviceAttributeMaxSurface3DDepth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface1DLayeredWidth', ('hipDeviceAttributeMaxSurface1DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface1DLayeredLayers', ('hipDeviceAttributeMaxSurface1DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DLayeredWidth', ('hipDeviceAttributeMaxSurface2DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DLayeredHeight', ('hipDeviceAttributeMaxSurface2DLayeredHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DLayeredLayers', ('hipDeviceAttributeMaxSurface2DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurfaceCubemapWidth', ('hipDeviceAttributeMaxSurfaceCubemapWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurfaceCubemapLayeredWidth', ('hipDeviceAttributeMaxSurfaceCubemapLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurfaceCubemapLayeredLayers', ('hipDeviceAttributeMaxSurfaceCubemapLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture1DLinearWidth', ('hipDeviceAttributeMaxTexture1DLinearWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLinearWidth', ('hipDeviceAttributeMaxTexture2DLinearWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLinearHeight', ('hipDeviceAttributeMaxTexture2DLinearHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLinearPitch', ('hipDeviceAttributeMaxTexture2DLinearPitch', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DMipmappedWidth', ('hipDeviceAttributeMaxTexture2DMipmappedWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DMipmappedHeight', ('hipDeviceAttributeMaxTexture2DMipmappedHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrComputeCapabilityMajor', ('hipDeviceAttributeComputeCapabilityMajor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrComputeCapabilityMinor', ('hipDeviceAttributeComputeCapabilityMinor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxTexture1DMipmappedWidth', ('hipDeviceAttributeMaxTexture1DMipmappedWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrStreamPrioritiesSupported', ('hipDeviceAttributeStreamPrioritiesSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrGlobalL1CacheSupported', ('hipDeviceAttributeGlobalL1CacheSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrLocalL1CacheSupported', ('hipDeviceAttributeLocalL1CacheSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSharedMemoryPerMultiprocessor', ('hipDeviceAttributeMaxSharedMemoryPerMultiprocessor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxRegistersPerMultiprocessor', ('hipDeviceAttributeMaxRegistersPerMultiprocessor', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrManagedMemory', ('hipDeviceAttributeManagedMemory', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrIsMultiGpuBoard', ('hipDeviceAttributeIsMultiGpuBoard', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMultiGpuBoardGroupID', ('hipDeviceAttributeMultiGpuBoardGroupID', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrHostNativeAtomicSupported', ('hipDeviceAttributeHostNativeAtomicSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrSingleToDoublePrecisionPerfRatio', ('hipDeviceAttributeSingleToDoublePrecisionPerfRatio', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrPageableMemoryAccess', ('hipDeviceAttributePageableMemoryAccess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrConcurrentManagedAccess', ('hipDeviceAttributeConcurrentManagedAccess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrComputePreemptionSupported', ('hipDeviceAttributeComputePreemptionSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrCanUseHostPointerForRegisteredMem', ('hipDeviceAttributeCanUseHostPointerForRegisteredMem', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaPointerGetAttributes', ('hipPointerGetAttributes', CONV_MEM, API_RUNTIME)), ('cudaHostGetDevicePointer', ('hipHostGetDevicePointer', CONV_MEM, API_RUNTIME)), ('cudaGetDeviceProperties', ('hipGetDeviceProperties', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetPCIBusId', ('hipDeviceGetPCIBusId', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetByPCIBusId', ('hipDeviceGetByPCIBusId', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetStreamPriorityRange', ('hipDeviceGetStreamPriorityRange', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetValidDevices', ('hipSetValidDevices', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevP2PAttrPerformanceRank', ('hipDeviceP2PAttributePerformanceRank', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevP2PAttrAccessSupported', ('hipDeviceP2PAttributeAccessSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevP2PAttrNativeAtomicSupported', ('hipDeviceP2PAttributeNativeAtomicSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceGetP2PAttribute', ('hipDeviceGetP2PAttribute', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeDefault', ('hipComputeModeDefault', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeExclusive', ('hipComputeModeExclusive', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeProhibited', ('hipComputeModeProhibited', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeExclusiveProcess', ('hipComputeModeExclusiveProcess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetDeviceFlags', ('hipGetDeviceFlags', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetDeviceFlags', ('hipSetDeviceFlags', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceScheduleAuto', ('hipDeviceScheduleAuto', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleSpin', ('hipDeviceScheduleSpin', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleYield', ('hipDeviceScheduleYield', CONV_TYPE, API_RUNTIME)), ('cudaDeviceBlockingSync', ('hipDeviceScheduleBlockingSync', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleBlockingSync', ('hipDeviceScheduleBlockingSync', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleMask', ('hipDeviceScheduleMask', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceMapHost', ('hipDeviceMapHost', CONV_TYPE, API_RUNTIME)), ('cudaDeviceLmemResizeToMax', ('hipDeviceLmemResizeToMax', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceMask', ('hipDeviceMask', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceSetCacheConfig', ('hipDeviceSetCacheConfig', CONV_CACHE, API_RUNTIME)), ('cudaDeviceGetCacheConfig', ('hipDeviceGetCacheConfig', CONV_CACHE, API_RUNTIME)), ('cudaFuncSetCacheConfig', ('hipFuncSetCacheConfig', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferNone', ('hipFuncCachePreferNone', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferShared', ('hipFuncCachePreferShared', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferL1', ('hipFuncCachePreferL1', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferEqual', ('hipFuncCachePreferEqual', CONV_CACHE, API_RUNTIME)), ('cudaFuncGetAttributes', ('hipFuncGetAttributes', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFuncSetSharedMemConfig', ('hipFuncSetSharedMemConfig', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetParameterBuffer', ('hipGetParameterBuffer', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetDoubleForDevice', ('hipSetDoubleForDevice', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetDoubleForHost', ('hipSetDoubleForHost', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaConfigureCall', ('hipConfigureCall', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLaunch', ('hipLaunch', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetupArgument', ('hipSetupArgument', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDriverGetVersion', ('hipDriverGetVersion', CONV_VERSION, API_RUNTIME)), ('cudaRuntimeGetVersion', ('hipRuntimeGetVersion', CONV_VERSION, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxPotentialBlockSize', ('hipOccupancyMaxPotentialBlockSize', CONV_OCCUPANCY, API_RUNTIME)), ('cudaOccupancyMaxPotentialBlockSizeWithFlags', ('hipOccupancyMaxPotentialBlockSizeWithFlags', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxActiveBlocksPerMultiprocessor', ('hipOccupancyMaxActiveBlocksPerMultiprocessor', CONV_OCCUPANCY, API_RUNTIME)), ('cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', ('hipOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxPotentialBlockSizeVariableSMem', ('hipOccupancyMaxPotentialBlockSizeVariableSMem', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags', ('hipOccupancyMaxPotentialBlockSizeVariableSMemWithFlags', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceCanAccessPeer', ('hipDeviceCanAccessPeer', CONV_PEER, API_RUNTIME)), ('cudaDeviceDisablePeerAccess', ('hipDeviceDisablePeerAccess', CONV_PEER, API_RUNTIME)), ('cudaDeviceEnablePeerAccess', ('hipDeviceEnablePeerAccess', CONV_PEER, API_RUNTIME)), ('cudaMemcpyPeerAsync', ('hipMemcpyPeerAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpyPeer', ('hipMemcpyPeer', CONV_MEM, API_RUNTIME)), ('cudaIpcMemLazyEnablePeerAccess', ('hipIpcMemLazyEnablePeerAccess', CONV_TYPE, API_RUNTIME)), ('cudaDeviceSetSharedMemConfig', ('hipDeviceSetSharedMemConfig', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetSharedMemConfig', ('hipDeviceGetSharedMemConfig', CONV_DEVICE, API_RUNTIME)), ('cudaSharedMemBankSizeDefault', ('hipSharedMemBankSizeDefault', CONV_TYPE, API_RUNTIME)), ('cudaSharedMemBankSizeFourByte', ('hipSharedMemBankSizeFourByte', CONV_TYPE, API_RUNTIME)), ('cudaSharedMemBankSizeEightByte', ('hipSharedMemBankSizeEightByte', CONV_TYPE, API_RUNTIME)), ('cudaLimitStackSize', ('hipLimitStackSize', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLimitPrintfFifoSize', ('hipLimitPrintfFifoSize', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLimitMallocHeapSize', ('hipLimitMallocHeapSize', CONV_TYPE, API_RUNTIME)), ('cudaLimitDevRuntimeSyncDepth', ('hipLimitDevRuntimeSyncDepth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLimitDevRuntimePendingLaunchCount', ('hipLimitDevRuntimePendingLaunchCount', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceGetLimit', ('hipDeviceGetLimit', CONV_DEVICE, API_RUNTIME)), ('cudaProfilerInitialize', ('hipProfilerInitialize', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaProfilerStart', ('hipProfilerStart', CONV_OTHER, API_RUNTIME)), ('cudaProfilerStop', ('hipProfilerStop', CONV_OTHER, API_RUNTIME)), ('cudaKeyValuePair', ('hipKeyValuePair', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaCSV', ('hipCSV', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaReadModeElementType', ('hipReadModeElementType', CONV_TEX, API_RUNTIME)), ('cudaReadModeNormalizedFloat', ('hipReadModeNormalizedFloat', CONV_TEX, API_RUNTIME)), ('cudaFilterModePoint', ('hipFilterModePoint', CONV_TEX, API_RUNTIME)), ('cudaFilterModeLinear', ('hipFilterModeLinear', CONV_TEX, API_RUNTIME)), ('cudaBindTexture', ('hipBindTexture', CONV_TEX, API_RUNTIME)), ('cudaUnbindTexture', ('hipUnbindTexture', CONV_TEX, API_RUNTIME)), ('cudaBindTexture2D', ('hipBindTexture2D', CONV_TEX, API_RUNTIME)), ('cudaBindTextureToArray', ('hipBindTextureToArray', CONV_TEX, API_RUNTIME)), ('cudaBindTextureToMipmappedArray', ('hipBindTextureToMipmappedArray', CONV_TEX, API_RUNTIME)), ('cudaGetTextureAlignmentOffset', ('hipGetTextureAlignmentOffset', CONV_TEX, API_RUNTIME)), ('cudaGetTextureReference', ('hipGetTextureReference', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindSigned', ('hipChannelFormatKindSigned', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindUnsigned', ('hipChannelFormatKindUnsigned', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindFloat', ('hipChannelFormatKindFloat', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindNone', ('hipChannelFormatKindNone', CONV_TEX, API_RUNTIME)), ('cudaCreateChannelDesc', ('hipCreateChannelDesc', CONV_TEX, API_RUNTIME)), ('cudaGetChannelDesc', ('hipGetChannelDesc', CONV_TEX, API_RUNTIME)), ('cudaResourceTypeArray', ('hipResourceTypeArray', CONV_TEX, API_RUNTIME)), ('cudaResourceTypeMipmappedArray', ('hipResourceTypeMipmappedArray', CONV_TEX, API_RUNTIME)), ('cudaResourceTypeLinear', ('hipResourceTypeLinear', CONV_TEX, API_RUNTIME)), ('cudaResourceTypePitch2D', ('hipResourceTypePitch2D', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatNone', ('hipResViewFormatNone', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedChar1', ('hipResViewFormatUnsignedChar1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedChar2', ('hipResViewFormatUnsignedChar2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedChar4', ('hipResViewFormatUnsignedChar4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedChar1', ('hipResViewFormatSignedChar1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedChar2', ('hipResViewFormatSignedChar2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedChar4', ('hipResViewFormatSignedChar4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedShort1', ('hipResViewFormatUnsignedShort1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedShort2', ('hipResViewFormatUnsignedShort2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedShort4', ('hipResViewFormatUnsignedShort4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedShort1', ('hipResViewFormatSignedShort1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedShort2', ('hipResViewFormatSignedShort2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedShort4', ('hipResViewFormatSignedShort4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedInt1', ('hipResViewFormatUnsignedInt1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedInt2', ('hipResViewFormatUnsignedInt2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedInt4', ('hipResViewFormatUnsignedInt4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedInt1', ('hipResViewFormatSignedInt1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedInt2', ('hipResViewFormatSignedInt2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedInt4', ('hipResViewFormatSignedInt4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatHalf1', ('hipResViewFormatHalf1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatHalf2', ('hipResViewFormatHalf2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatHalf4', ('hipResViewFormatHalf4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatFloat1', ('hipResViewFormatFloat1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatFloat2', ('hipResViewFormatFloat2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatFloat4', ('hipResViewFormatFloat4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed1', ('hipResViewFormatUnsignedBlockCompressed1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed2', ('hipResViewFormatUnsignedBlockCompressed2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed3', ('hipResViewFormatUnsignedBlockCompressed3', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed4', ('hipResViewFormatUnsignedBlockCompressed4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedBlockCompressed4', ('hipResViewFormatSignedBlockCompressed4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed5', ('hipResViewFormatUnsignedBlockCompressed5', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedBlockCompressed5', ('hipResViewFormatSignedBlockCompressed5', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed6H', ('hipResViewFormatUnsignedBlockCompressed6H', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedBlockCompressed6H', ('hipResViewFormatSignedBlockCompressed6H', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed7', ('hipResViewFormatUnsignedBlockCompressed7', CONV_TEX, API_RUNTIME)), ('cudaAddressModeWrap', ('hipAddressModeWrap', CONV_TEX, API_RUNTIME)), ('cudaAddressModeClamp', ('hipAddressModeClamp', CONV_TEX, API_RUNTIME)), ('cudaAddressModeMirror', ('hipAddressModeMirror', CONV_TEX, API_RUNTIME)), ('cudaAddressModeBorder', ('hipAddressModeBorder', CONV_TEX, API_RUNTIME)), ('cudaCreateTextureObject', ('hipCreateTextureObject', CONV_TEX, API_RUNTIME)), ('cudaDestroyTextureObject', ('hipDestroyTextureObject', CONV_TEX, API_RUNTIME)), ('cudaGetTextureObjectResourceDesc', ('hipGetTextureObjectResourceDesc', CONV_TEX, API_RUNTIME)), ('cudaGetTextureObjectResourceViewDesc', ('hipGetTextureObjectResourceViewDesc', CONV_TEX, API_RUNTIME)), ('cudaGetTextureObjectTextureDesc', ('hipGetTextureObjectTextureDesc', CONV_TEX, API_RUNTIME)), ('cudaBindSurfaceToArray', ('hipBindSurfaceToArray', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSurfaceReference', ('hipGetSurfaceReference', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaBoundaryModeZero', ('hipBoundaryModeZero', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaBoundaryModeClamp', ('hipBoundaryModeClamp', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaBoundaryModeTrap', ('hipBoundaryModeTrap', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFormatModeForced', ('hipFormatModeForced', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFormatModeAuto', ('hipFormatModeAuto', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaCreateSurfaceObject', ('hipCreateSurfaceObject', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDestroySurfaceObject', ('hipDestroySurfaceObject', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSurfaceObjectResourceDesc', ('hipGetSurfaceObjectResourceDesc', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaIpcCloseMemHandle', ('hipIpcCloseMemHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcGetEventHandle', ('hipIpcGetEventHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcGetMemHandle', ('hipIpcGetMemHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcOpenEventHandle', ('hipIpcOpenEventHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcOpenMemHandle', ('hipIpcOpenMemHandle', CONV_DEVICE, API_RUNTIME)), ('cudaGLGetDevices', ('hipGLGetDevices', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterBuffer', ('hipGraphicsGLRegisterBuffer', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterImage', ('hipGraphicsGLRegisterImage', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaWGLGetDevice', ('hipWGLGetDevice', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapResources', ('hipGraphicsMapResources', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceGetMappedMipmappedArray', ('hipGraphicsResourceGetMappedMipmappedArray', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceGetMappedPointer', ('hipGraphicsResourceGetMappedPointer', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceSetMapFlags', ('hipGraphicsResourceSetMapFlags', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsSubResourceGetMappedArray', ('hipGraphicsSubResourceGetMappedArray', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsUnmapResources', ('hipGraphicsUnmapResources', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsUnregisterResource', ('hipGraphicsUnregisterResource', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFacePositiveX', ('hipGraphicsCubeFacePositiveX', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFaceNegativeX', ('hipGraphicsCubeFaceNegativeX', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFacePositiveY', ('hipGraphicsCubeFacePositiveY', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFaceNegativeY', ('hipGraphicsCubeFaceNegativeY', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFacePositiveZ', ('hipGraphicsCubeFacePositiveZ', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFaceNegativeZ', ('hipGraphicsCubeFaceNegativeZ', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlagsNone', ('hipGraphicsMapFlagsNone', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlagsReadOnly', ('hipGraphicsMapFlagsReadOnly', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlagsWriteDiscard', ('hipGraphicsMapFlagsWriteDiscard', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsNone', ('hipGraphicsRegisterFlagsNone', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsReadOnly', ('hipGraphicsRegisterFlagsReadOnly', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsWriteDiscard', ('hipGraphicsRegisterFlagsWriteDiscard', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsSurfaceLoadStore', ('hipGraphicsRegisterFlagsSurfaceLoadStore', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsTextureGather', ('hipGraphicsRegisterFlagsTextureGather', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceListAll', ('HIP_GL_DEVICE_LIST_ALL', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceListCurrentFrame', ('HIP_GL_DEVICE_LIST_CURRENT_FRAME', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceListNextFrame', ('HIP_GL_DEVICE_LIST_NEXT_FRAME', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLGetDevices', ('hipGLGetDevices', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterBuffer', ('hipGraphicsGLRegisterBuffer', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterImage', ('hipGraphicsGLRegisterImage', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaWGLGetDevice', ('hipWGLGetDevice', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlagsNone', ('HIP_GL_MAP_RESOURCE_FLAGS_NONE', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlagsReadOnly', ('HIP_GL_MAP_RESOURCE_FLAGS_READ_ONLY', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlagsWriteDiscard', ('HIP_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapBufferObject', ('hipGLMapBufferObject__', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapBufferObjectAsync', ('hipGLMapBufferObjectAsync__', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLRegisterBufferObject', ('hipGLRegisterBufferObject', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLSetBufferObjectMapFlags', ('hipGLSetBufferObjectMapFlags', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLSetGLDevice', ('hipGLSetGLDevice', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLUnmapBufferObject', ('hipGLUnmapBufferObject', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLUnmapBufferObjectAsync', ('hipGLUnmapBufferObjectAsync', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLUnregisterBufferObject', ('hipGLUnregisterBufferObject', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceListAll', ('HIP_D3D9_DEVICE_LIST_ALL', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceListCurrentFrame', ('HIP_D3D9_DEVICE_LIST_CURRENT_FRAME', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceListNextFrame', ('HIP_D3D9_DEVICE_LIST_NEXT_FRAME', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9GetDevice', ('hipD3D9GetDevice', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9GetDevices', ('hipD3D9GetDevices', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9GetDirect3DDevice', ('hipD3D9GetDirect3DDevice', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9SetDirect3DDevice', ('hipD3D9SetDirect3DDevice', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D9RegisterResource', ('hipGraphicsD3D9RegisterResource', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlags', ('hipD3D9MapFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlagsNone', ('HIP_D3D9_MAPRESOURCE_FLAGS_NONE', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlagsReadOnly', ('HIP_D3D9_MAPRESOURCE_FLAGS_READONLY', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlagsWriteDiscard', ('HIP_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterFlagsNone', ('HIP_D3D9_REGISTER_FLAGS_NONE', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterFlagsArray', ('HIP_D3D9_REGISTER_FLAGS_ARRAY', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapResources', ('hipD3D9MapResources', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterResource', ('hipD3D9RegisterResource', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedArray', ('hipD3D9ResourceGetMappedArray', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedPitch', ('hipD3D9ResourceGetMappedPitch', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedPointer', ('hipD3D9ResourceGetMappedPointer', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedSize', ('hipD3D9ResourceGetMappedSize', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetSurfaceDimensions', ('hipD3D9ResourceGetSurfaceDimensions', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceSetMapFlags', ('hipD3D9ResourceSetMapFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9UnmapResources', ('hipD3D9UnmapResources', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9UnregisterResource', ('hipD3D9UnregisterResource', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceListAll', ('HIP_D3D10_DEVICE_LIST_ALL', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceListCurrentFrame', ('HIP_D3D10_DEVICE_LIST_CURRENT_FRAME', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceListNextFrame', ('HIP_D3D10_DEVICE_LIST_NEXT_FRAME', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10GetDevice', ('hipD3D10GetDevice', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10GetDevices', ('hipD3D10GetDevices', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D10RegisterResource', ('hipGraphicsD3D10RegisterResource', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlagsNone', ('HIP_D3D10_MAPRESOURCE_FLAGS_NONE', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlagsReadOnly', ('HIP_D3D10_MAPRESOURCE_FLAGS_READONLY', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlagsWriteDiscard', ('HIP_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterFlagsNone', ('HIP_D3D10_REGISTER_FLAGS_NONE', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterFlagsArray', ('HIP_D3D10_REGISTER_FLAGS_ARRAY', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10GetDirect3DDevice', ('hipD3D10GetDirect3DDevice', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapResources', ('hipD3D10MapResources', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterResource', ('hipD3D10RegisterResource', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedArray', ('hipD3D10ResourceGetMappedArray', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedPitch', ('hipD3D10ResourceGetMappedPitch', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedPointer', ('hipD3D10ResourceGetMappedPointer', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedSize', ('hipD3D10ResourceGetMappedSize', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetSurfaceDimensions', ('hipD3D10ResourceGetSurfaceDimensions', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceSetMapFlags', ('hipD3D10ResourceSetMapFlags', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10SetDirect3DDevice', ('hipD3D10SetDirect3DDevice', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10UnmapResources', ('hipD3D10UnmapResources', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10UnregisterResource', ('hipD3D10UnregisterResource', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceListAll', ('HIP_D3D11_DEVICE_LIST_ALL', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceListCurrentFrame', ('HIP_D3D11_DEVICE_LIST_CURRENT_FRAME', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceListNextFrame', ('HIP_D3D11_DEVICE_LIST_NEXT_FRAME', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevice', ('hipD3D11GetDevice', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevices', ('hipD3D11GetDevices', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D11RegisterResource', ('hipGraphicsD3D11RegisterResource', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevice', ('hipD3D11GetDevice', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevices', ('hipD3D11GetDevices', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D11RegisterResource', ('hipGraphicsD3D11RegisterResource', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsVDPAURegisterOutputSurface', ('hipGraphicsVDPAURegisterOutputSurface', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsVDPAURegisterVideoSurface', ('hipGraphicsVDPAURegisterVideoSurface', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaVDPAUGetDevice', ('hipVDPAUGetDevice', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaVDPAUSetVDPAUDevice', ('hipVDPAUSetDevice', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerAcquireFrame', ('hipEGLStreamConsumerAcquireFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerConnect', ('hipEGLStreamConsumerConnect', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerConnectWithFlags', ('hipEGLStreamConsumerConnectWithFlags', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerReleaseFrame', ('hipEGLStreamConsumerReleaseFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerConnect', ('hipEGLStreamProducerConnect', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerDisconnect', ('hipEGLStreamProducerDisconnect', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerPresentFrame', ('hipEGLStreamProducerPresentFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerReturnFrame', ('hipEGLStreamProducerReturnFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsEGLRegisterImage', ('hipGraphicsEGLRegisterImage', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceGetMappedEglFrame', ('hipGraphicsResourceGetMappedEglFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cublasInit', ('rocblas_init', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasShutdown', ('rocblas_shutdown', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetVersion', ('rocblas_get_version', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetError', ('rocblas_get_error', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasAlloc', ('rocblas_alloc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasFree', ('rocblas_free', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetKernelStream', ('rocblas_set_kernel_stream', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetAtomicsMode', ('rocblas_get_atomics_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetAtomicsMode', ('rocblas_set_atomics_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetMathMode', ('rocblas_get_math_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetMathMode', ('rocblas_set_math_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_OP_N', ('rocblas_operation_none', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_OP_T', ('rocblas_operation_transpose', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_OP_C', ('rocblas_operation_conjugate_transpose', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_SUCCESS', ('rocblas_status_success', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_NOT_INITIALIZED', ('rocblas_status_invalid_handle', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_ALLOC_FAILED', ('rocblas_status_memory_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_INVALID_VALUE', ('rocblas_status_invalid_pointer', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_MAPPING_ERROR', ('rocblas_status_internal_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_EXECUTION_FAILED', ('rocblas_status_internal_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_INTERNAL_ERROR', ('rocblas_status_internal_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_NOT_SUPPORTED', ('rocblas_status_not_implemented', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_ARCH_MISMATCH', ('rocblas_status_not_implemented', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_FILL_MODE_LOWER', ('rocblas_fill_lower', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_FILL_MODE_UPPER', ('rocblas_fill_upper', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_DIAG_NON_UNIT', ('rocblas_diagonal_non_unit', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_DIAG_UNIT', ('rocblas_diagonal_unit', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_SIDE_LEFT', ('rocblas_side_left', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_SIDE_RIGHT', ('rocblas_side_right', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_POINTER_MODE_HOST', ('rocblas_pointer_mode_host', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_POINTER_MODE_DEVICE', ('rocblas_pointer_mode_device', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_ATOMICS_NOT_ALLOWED', ('rocblas_atomics_not_allowed', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_ATOMICS_ALLOWED', ('rocblas_atomics_allowed', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_FLOAT', ('rocblas_precision_float', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_DOUBLE', ('rocblas_precision_double', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_HALF', ('rocblas_precision_half', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_INT8', ('rocblas_precision_int8', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('cublasCreate', ('rocblas_create_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasDestroy', ('rocblas_destroy_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasSetVector', ('rocblas_set_vector', CONV_MATH_FUNC, API_BLAS)), ('cublasGetVector', ('rocblas_get_vector', CONV_MATH_FUNC, API_BLAS)), ('cublasSetVectorAsync', ('rocblas_set_vector_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetVectorAsync', ('rocblas_get_vector_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetMatrix', ('rocblas_set_matrix', CONV_MATH_FUNC, API_BLAS)), ('cublasGetMatrix', ('rocblas_get_matrix', CONV_MATH_FUNC, API_BLAS)), ('cublasGetMatrixAsync', ('rocblas_get_matrix_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetMatrixAsync', ('rocblas_set_matrix_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasXerbla', ('rocblas_xerbla', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSnrm2', ('rocblas_snrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasDnrm2', ('rocblas_dnrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasScnrm2', ('rocblas_scnrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDznrm2', ('rocblas_dznrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasNrm2Ex', ('rocblas_nrm2_ex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSdot', ('rocblas_sdot', CONV_MATH_FUNC, API_BLAS)), ('cublasSdotBatched', ('rocblas_sdot_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDdot', ('rocblas_ddot', CONV_MATH_FUNC, API_BLAS)), ('cublasDdotBatched', ('rocblas_ddot_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdotu', ('rocblas_cdotu', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdotc', ('rocblas_cdotc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdotu', ('rocblas_zdotu', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdotc', ('rocblas_zdotc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSscal', ('rocblas_sscal', CONV_MATH_FUNC, API_BLAS)), ('cublasSscalBatched', ('rocblas_sscal_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDscal', ('rocblas_dscal', CONV_MATH_FUNC, API_BLAS)), ('cublasDscalBatched', ('rocblas_dscal_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCscal', ('rocblas_cscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsscal', ('rocblas_csscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZscal', ('rocblas_zscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdscal', ('rocblas_zdscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSaxpy', ('rocblas_saxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasSaxpyBatched', ('rocblas_saxpy_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDaxpy', ('rocblas_daxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasCaxpy', ('rocblas_caxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZaxpy', ('rocblas_zaxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScopy', ('rocblas_scopy', CONV_MATH_FUNC, API_BLAS)), ('cublasScopyBatched', ('rocblas_scopy_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDcopy', ('rocblas_dcopy', CONV_MATH_FUNC, API_BLAS)), ('cublasDcopyBatched', ('rocblas_dcopy_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCcopy', ('rocblas_ccopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZcopy', ('rocblas_zcopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSswap', ('rocblas_sswap', CONV_MATH_FUNC, API_BLAS)), ('cublasDswap', ('rocblas_dswap', CONV_MATH_FUNC, API_BLAS)), ('cublasCswap', ('rocblas_cswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZswap', ('rocblas_zswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamax', ('rocblas_isamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamax', ('rocblas_idamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamax', ('rocblas_icamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamax', ('rocblas_izamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamin', ('rocblas_isamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamin', ('rocblas_idamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamin', ('rocblas_icamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamin', ('rocblas_izamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSasum', ('rocblas_sasum', CONV_MATH_FUNC, API_BLAS)), ('cublasSasumBatched', ('rocblas_sasum_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDasum', ('rocblas_dasum', CONV_MATH_FUNC, API_BLAS)), ('cublasDasumBatched', ('rocblas_dasum_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScasum', ('rocblas_scasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDzasum', ('rocblas_dzasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrot', ('rocblas_srot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrot', ('rocblas_drot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrot', ('rocblas_crot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsrot', ('rocblas_csrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrot', ('rocblas_zrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdrot', ('rocblas_zdrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotg', ('rocblas_srotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotg', ('rocblas_drotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrotg', ('rocblas_crotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrotg', ('rocblas_zrotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotm', ('rocblas_srotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotm', ('rocblas_drotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotmg', ('rocblas_srotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotmg', ('rocblas_drotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemv', ('rocblas_sgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasSgemvBatched', ('rocblas_sgemv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgemv', ('rocblas_dgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemv', ('rocblas_cgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemv', ('rocblas_zgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgbmv', ('rocblas_sgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgbmv', ('rocblas_dgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgbmv', ('rocblas_cgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgbmv', ('rocblas_zgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmv', ('rocblas_strmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmv', ('rocblas_dtrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmv', ('rocblas_ctrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmv', ('rocblas_ztrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbmv', ('rocblas_stbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbmv', ('rocblas_dtbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbmv', ('rocblas_ctbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbmv', ('rocblas_ztbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpmv', ('rocblas_stpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpmv', ('rocblas_dtpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpmv', ('rocblas_ctpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpmv', ('rocblas_ztpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsv', ('rocblas_strsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsv', ('rocblas_dtrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsv', ('rocblas_ctrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsv', ('rocblas_ztrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpsv', ('rocblas_stpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpsv', ('rocblas_dtpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpsv', ('rocblas_ctpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpsv', ('rocblas_ztpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbsv', ('rocblas_stbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbsv', ('rocblas_dtbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbsv', ('rocblas_ctbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbsv', ('rocblas_ztbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymv', ('rocblas_ssymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymv', ('rocblas_dsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymv', ('rocblas_csymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymv', ('rocblas_zsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemv', ('rocblas_chemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemv', ('rocblas_zhemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsbmv', ('rocblas_ssbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsbmv', ('rocblas_dsbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChbmv', ('rocblas_chbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhbmv', ('rocblas_zhbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspmv', ('rocblas_sspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspmv', ('rocblas_dspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpmv', ('rocblas_chpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpmv', ('rocblas_zhpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSger', ('rocblas_sger', CONV_MATH_FUNC, API_BLAS)), ('cublasDger', ('rocblas_dger', CONV_MATH_FUNC, API_BLAS)), ('cublasCgeru', ('rocblas_cgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgerc', ('rocblas_cgerc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeru', ('rocblas_zgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgerc', ('rocblas_zgerc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr', ('rocblas_ssyr', CONV_MATH_FUNC, API_BLAS)), ('cublasDsyr', ('rocblas_dsyr', CONV_MATH_FUNC, API_BLAS)), ('cublasCher', ('rocblas_cher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher', ('rocblas_zher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr', ('rocblas_sspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr', ('rocblas_dspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr', ('rocblas_chpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr', ('rocblas_zhpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2', ('rocblas_ssyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2', ('rocblas_dsyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2', ('rocblas_cher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2', ('rocblas_zher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr2', ('rocblas_sspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr2', ('rocblas_dspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr2', ('rocblas_chpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr2', ('rocblas_zhpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemmBatched', ('rocblas_sgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgemmBatched', ('rocblas_dgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasHgemmBatched', ('rocblas_hgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemmStridedBatched', ('rocblas_sgemm_strided_batched', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemmStridedBatched', ('rocblas_dgemm_strided_batched', CONV_MATH_FUNC, API_BLAS)), ('cublasHgemmStridedBatched', ('rocblas_hgemm_strided_batched', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemmBatched', ('rocblas_cgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3mBatched', ('rocblas_cgemm_3m_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemmBatched', ('rocblas_zgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemmStridedBatched', ('rocblas_cgemm_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3mStridedBatched', ('rocblas_cgemm_3m_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemmStridedBatched', ('rocblas_zgemm_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasHgemmStridedBatched', ('rocblas_hgemm_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemm', ('rocblas_sgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemm', ('rocblas_dgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemm', ('rocblas_cgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasZgemm', ('rocblas_zgemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasHgemm', ('rocblas_hgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasSsyrk', ('rocblas_ssyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyrk', ('rocblas_dsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrk', ('rocblas_csyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyrk', ('rocblas_zsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherk', ('rocblas_cherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZherk', ('rocblas_zherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2k', ('rocblas_ssyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2k', ('rocblas_dsyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr2k', ('rocblas_csyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr2k', ('rocblas_zyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyrkx', ('rocblas_ssyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyrkx', ('rocblas_dsyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrkx', ('rocblas_csyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyrkx', ('rocblas_zsyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2k', ('rocblas_cher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2k', ('rocblas_zher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherkx', ('rocblas_cherkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZherkx', ('rocblas_zherkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymm', ('rocblas_ssymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymm', ('rocblas_dsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymm', ('rocblas_csymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymm', ('rocblas_zsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemm', ('rocblas_chemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemm', ('rocblas_zhemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsm', ('rocblas_strsm', CONV_MATH_FUNC, API_BLAS)), ('cublasDtrsm', ('rocblas_dtrsm', CONV_MATH_FUNC, API_BLAS)), ('cublasCtrsm', ('rocblas_ctrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsm', ('rocblas_ztrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsmBatched', ('rocblas_strsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsmBatched', ('rocblas_dtrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsmBatched', ('rocblas_ctrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsmBatched', ('rocblas_ztrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmm', ('rocblas_strmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmm', ('rocblas_dtrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmm', ('rocblas_ctrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmm', ('rocblas_ztrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgeam', ('rocblas_sgeam', CONV_MATH_FUNC, API_BLAS)), ('cublasDgeam', ('rocblas_dgeam', CONV_MATH_FUNC, API_BLAS)), ('cublasCgeam', ('rocblas_cgeam', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeam', ('rocblas_zgeam', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgetrfBatched', ('rocblas_sgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgetrfBatched', ('rocblas_dgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgetrfBatched', ('rocblas_cgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgetrfBatched', ('rocblas_zgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgetriBatched', ('rocblas_sgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgetriBatched', ('rocblas_dgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgetriBatched', ('rocblas_cgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgetriBatched', ('rocblas_zgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgetrsBatched', ('rocblas_sgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgetrsBatched', ('rocblas_dgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgetrsBatched', ('rocblas_cgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgetrsBatched', ('rocblas_zgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsmBatched', ('rocblas_strsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsmBatched', ('rocblas_dtrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsmBatched', ('rocblas_ctrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsmBatched', ('rocblas_dtrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSmatinvBatched', ('rocblas_smatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDmatinvBatched', ('rocblas_dmatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCmatinvBatched', ('rocblas_cmatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZmatinvBatched', ('rocblas_zmatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgeqrfBatched', ('rocblas_sgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgeqrfBatched', ('rocblas_dgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgeqrfBatched', ('rocblas_cgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeqrfBatched', ('rocblas_zgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgelsBatched', ('rocblas_sgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgelsBatched', ('rocblas_dgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgelsBatched', ('rocblas_cgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgelsBatched', ('rocblas_zgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSdgmm', ('rocblas_sdgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDdgmm', ('rocblas_ddgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdgmm', ('rocblas_cdgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdgmm', ('rocblas_zdgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpttr', ('rocblas_stpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpttr', ('rocblas_dtpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpttr', ('rocblas_ctpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpttr', ('rocblas_ztpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrttp', ('rocblas_strttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrttp', ('rocblas_dtrttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrttp', ('rocblas_ctrttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrttp', ('rocblas_ztrttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCreate_v2', ('rocblas_create_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasDestroy_v2', ('rocblas_destroy_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasGetVersion_v2', ('rocblas_get_version', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetStream', ('rocblas_set_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasGetStream', ('rocblas_get_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasSetStream_v2', ('rocblas_set_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasGetStream_v2', ('rocblas_get_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasGetPointerMode', ('rocblas_get_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasSetPointerMode', ('rocblas_set_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasGetPointerMode_v2', ('rocblas_get_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasSetPointerMode_v2', ('rocblas_set_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasSgemv_v2', ('rocblas_sgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemv_v2', ('rocblas_dgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemv_v2', ('rocblas_cgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemv_v2', ('rocblas_zgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgbmv_v2', ('rocblas_sgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgbmv_v2', ('rocblas_dgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgbmv_v2', ('rocblas_cgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgbmv_v2', ('rocblas_zgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmv_v2', ('rocblas_strmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmv_v2', ('rocblas_dtrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmv_v2', ('rocblas_ctrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmv_v2', ('rocblas_ztrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbmv_v2', ('rocblas_stbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbmv_v2', ('rocblas_dtbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbmv_v2', ('rocblas_ctbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbmv_v2', ('rocblas_ztbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpmv_v2', ('rocblas_stpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpmv_v2', ('rocblas_dtpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpmv_v2', ('rocblas_ctpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpmv_v2', ('rocblas_ztpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsv_v2', ('rocblas_strsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsv_v2', ('rocblas_dtrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsv_v2', ('rocblas_ctrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsv_v2', ('rocblas_ztrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpsv_v2', ('rocblas_stpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpsv_v2', ('rocblas_dtpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpsv_v2', ('rocblas_ctpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpsv_v2', ('rocblas_ztpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbsv_v2', ('rocblas_stbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbsv_v2', ('rocblas_dtbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbsv_v2', ('rocblas_ctbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbsv_v2', ('rocblas_ztbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymv_v2', ('rocblas_ssymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymv_v2', ('rocblas_dsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymv_v2', ('rocblas_csymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymv_v2', ('rocblas_zsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemv_v2', ('rocblas_chemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemv_v2', ('rocblas_zhemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsbmv_v2', ('rocblas_ssbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsbmv_v2', ('rocblas_dsbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChbmv_v2', ('rocblas_chbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhbmv_v2', ('rocblas_zhbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspmv_v2', ('rocblas_sspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspmv_v2', ('rocblas_dspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpmv_v2', ('rocblas_chpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpmv_v2', ('rocblas_zhpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSger_v2', ('rocblas_sger', CONV_MATH_FUNC, API_BLAS)), ('cublasDger_v2', ('rocblas_dger', CONV_MATH_FUNC, API_BLAS)), ('cublasCgeru_v2', ('rocblas_cgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgerc_v2', ('rocblas_cergc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeru_v2', ('rocblas_zgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgerc_v2', ('rocblas_zgerc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr_v2', ('rocblas_ssyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr_v2', ('rocblas_dsyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr_v2', ('rocblas_csyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr_v2', ('rocblas_zsyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher_v2', ('rocblas_cher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher_v2', ('rocblas_zher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr_v2', ('rocblas_sspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr_v2', ('rocblas_dspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr_v2', ('rocblas_chpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr_v2', ('rocblas_zhpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2_v2', ('rocblas_ssyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2_v2', ('rocblas_dsyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr2_v2', ('rocblas_csyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr2_v2', ('rocblas_zsyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2_v2', ('rocblas_cher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2_v2', ('rocblas_zher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr2_v2', ('rocblas_sspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr2_v2', ('rocblas_dspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr2_v2', ('rocblas_chpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr2_v2', ('rocblas_zhpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemm_v2', ('rocblas_sgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemm_v2', ('rocblas_dgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemm_v2', ('rocblas_cgemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3m', ('rocblas_cgemm_3m', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3mEx', ('rocblas_cgemm_3mex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemm_v2', ('rocblas_zgemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemm3m', ('rocblas_zgemm_3m', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemmEx', ('rocblas_sgemmex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGemmEx', ('rocblas_gemmex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemmEx', ('rocblas_cgemmex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasUint8gemmBias', ('rocblas_uint8gemmbias', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyrk_v2', ('rocblas_ssyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyrk_v2', ('rocblas_dsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrk_v2', ('rocblas_csyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyrk_v2', ('rocblas_zsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrkEx', ('rocblas_csyrkex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrk3mEx', ('rocblas_csyrk3mex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherk_v2', ('rocblas_cherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherkEx', ('rocblas_cherkex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherk3mEx', ('rocblas_cherk3mex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZherk_v2', ('rocblas_zherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2k_v2', ('rocblas_ssyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2k_v2', ('rocblas_dsyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr2k_v2', ('rocblas_csyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr2k_v2', ('rocblas_zsyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2k_v2', ('rocblas_cher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2k_v2', ('rocblas_zher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymm_v2', ('rocblas_ssymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymm_v2', ('rocblas_dsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymm_v2', ('rocblas_csymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymm_v2', ('rocblas_zsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemm_v2', ('rocblas_chemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemm_v2', ('rocblas_zhemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsm_v2', ('rocblas_strsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsm_v2', ('rocblas_dtrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsm_v2', ('rocblas_ctrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsm_v2', ('rocblas_ztrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmm_v2', ('rocblas_strmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmm_v2', ('rocblas_dtrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmm_v2', ('rocblas_ctrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmm_v2', ('rocblas_ztrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSnrm2_v2', ('rocblas_snrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasDnrm2_v2', ('rocblas_dnrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasScnrm2_v2', ('rocblas_scnrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDznrm2_v2', ('rocblas_dznrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDotEx', ('rocblas_dotex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDotcEx', ('rocblas_dotcex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSdot_v2', ('rocblas_sdot', CONV_MATH_FUNC, API_BLAS)), ('cublasDdot_v2', ('rocblas_ddot', CONV_MATH_FUNC, API_BLAS)), ('cublasCdotu_v2', ('rocblas_cdotu', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdotc_v2', ('rocblas_cdotc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdotu_v2', ('rocblas_zdotu', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdotc_v2', ('rocblas_zdotc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScalEx', ('rocblas_scalex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSscal_v2', ('rocblas_sscal', CONV_MATH_FUNC, API_BLAS)), ('cublasDscal_v2', ('rocblas_dscal', CONV_MATH_FUNC, API_BLAS)), ('cublasCscal_v2', ('rocblas_cscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsscal_v2', ('rocblas_csscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZscal_v2', ('rocblas_zcsal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdscal_v2', ('rocblas_zdscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasAxpyEx', ('rocblas_axpyex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSaxpy_v2', ('rocblas_saxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasDaxpy_v2', ('rocblas_daxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasCaxpy_v2', ('rocblas_caxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZaxpy_v2', ('rocblas_zaxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScopy_v2', ('rocblas_scopy', CONV_MATH_FUNC, API_BLAS)), ('cublasDcopy_v2', ('rocblas_dcopy', CONV_MATH_FUNC, API_BLAS)), ('cublasCcopy_v2', ('rocblas_ccopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZcopy_v2', ('rocblas_zcopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSswap_v2', ('rocblas_sswap', CONV_MATH_FUNC, API_BLAS)), ('cublasDswap_v2', ('rocblas_dswap', CONV_MATH_FUNC, API_BLAS)), ('cublasCswap_v2', ('rocblas_cswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZswap_v2', ('rocblas_zswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamax_v2', ('rocblas_isamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamax_v2', ('rocblas_idamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamax_v2', ('rocblas_icamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamax_v2', ('rocblas_izamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamin_v2', ('rocblas_isamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamin_v2', ('rocblas_idamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamin_v2', ('rocblas_icamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamin_v2', ('rocblas_izamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSasum_v2', ('rocblas_sasum', CONV_MATH_FUNC, API_BLAS)), ('cublasDasum_v2', ('rocblas_dasum', CONV_MATH_FUNC, API_BLAS)), ('cublasScasum_v2', ('rocblas_scasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDzasum_v2', ('rocblas_dzasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrot_v2', ('rocblas_srot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrot_v2', ('rocblas_drot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrot_v2', ('rocblas_crot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsrot_v2', ('rocblas_csrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrot_v2', ('rocblas_zrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdrot_v2', ('rocblas_zdrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotg_v2', ('rocblas_srotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotg_v2', ('rocblas_drotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrotg_v2', ('rocblas_crotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrotg_v2', ('rocblas_zrotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotm_v2', ('rocblas_srotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotm_v2', ('rocblas_drotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotmg_v2', ('rocblas_srotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotmg_v2', ('rocblas_drotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('CURAND_STATUS_SUCCESS', ('HIPRAND_STATUS_SUCCESS', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_VERSION_MISMATCH', ('HIPRAND_STATUS_VERSION_MISMATCH', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_NOT_INITIALIZED', ('HIPRAND_STATUS_NOT_INITIALIZED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_ALLOCATION_FAILED', ('HIPRAND_STATUS_ALLOCATION_FAILED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_TYPE_ERROR', ('HIPRAND_STATUS_TYPE_ERROR', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_OUT_OF_RANGE', ('HIPRAND_STATUS_OUT_OF_RANGE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_LENGTH_NOT_MULTIPLE', ('HIPRAND_STATUS_LENGTH_NOT_MULTIPLE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_DOUBLE_PRECISION_REQUIRED', ('HIPRAND_STATUS_DOUBLE_PRECISION_REQUIRED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_LAUNCH_FAILURE', ('HIPRAND_STATUS_LAUNCH_FAILURE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_PREEXISTING_FAILURE', ('HIPRAND_STATUS_PREEXISTING_FAILURE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_INITIALIZATION_FAILED', ('HIPRAND_STATUS_INITIALIZATION_FAILED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_ARCH_MISMATCH', ('HIPRAND_STATUS_ARCH_MISMATCH', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_INTERNAL_ERROR', ('HIPRAND_STATUS_INTERNAL_ERROR', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_TEST', ('HIPRAND_RNG_TEST', CONV_NUMERIC_LITERAL, API_RAND)), ('mtgp32dc_params_fast_11213', ('mtgp32dc_params_fast_11213', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_DEFAULT', ('HIPRAND_RNG_PSEUDO_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_XORWOW', ('HIPRAND_RNG_PSEUDO_XORWOW', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_MRG32K3A', ('HIPRAND_RNG_PSEUDO_MRG32K3A', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_MTGP32', ('HIPRAND_RNG_PSEUDO_MTGP32', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_MT19937', ('HIPRAND_RNG_PSEUDO_MT19937', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_PHILOX4_32_10', ('HIPRAND_RNG_PSEUDO_PHILOX4_32_10', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_DEFAULT', ('HIPRAND_RNG_QUASI_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SOBOL32', ('HIPRAND_RNG_QUASI_SOBOL32', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SCRAMBLED_SOBOL32', ('HIPRAND_RNG_QUASI_SCRAMBLED_SOBOL32', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SOBOL64', ('HIPRAND_RNG_QUASI_SOBOL64', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SCRAMBLED_SOBOL64', ('HIPRAND_RNG_QUASI_SCRAMBLED_SOBOL64', CONV_NUMERIC_LITERAL, API_RAND)), ('curand_ORDERING_PSEUDO_BEST', ('HIPRAND_ORDERING_PSEUDO_BEST', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ORDERING_PSEUDO_DEFAULT', ('HIPRAND_ORDERING_PSEUDO_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ORDERING_PSEUDO_SEEDED', ('HIPRAND_ORDERING_PSEUDO_SEEDED', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ORDERING_QUASI_DEFAULT', ('HIPRAND_ORDERING_QUASI_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DIRECTION_VECTORS_32_JOEKUO6', ('HIPRAND_DIRECTION_VECTORS_32_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_SCRAMBLED_DIRECTION_VECTORS_32_JOEKUO6', ('HIPRAND_SCRAMBLED_DIRECTION_VECTORS_32_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DIRECTION_VECTORS_64_JOEKUO6', ('HIPRAND_DIRECTION_VECTORS_64_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_SCRAMBLED_DIRECTION_VECTORS_64_JOEKUO6', ('HIPRAND_SCRAMBLED_DIRECTION_VECTORS_64_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_CHOOSE_BEST', ('HIPRAND_CHOOSE_BEST', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ITR', ('HIPRAND_ITR', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_KNUTH', ('HIPRAND_KNUTH', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_HITR', ('HIPRAND_HITR', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_M1', ('HIPRAND_M1', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_M2', ('HIPRAND_M2', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_BINARY_SEARCH', ('HIPRAND_BINARY_SEARCH', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DISCRETE_GAUSS', ('HIPRAND_DISCRETE_GAUSS', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_REJECTION', ('HIPRAND_REJECTION', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DEVICE_API', ('HIPRAND_DEVICE_API', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_FAST_REJECTION', ('HIPRAND_FAST_REJECTION', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_3RD', ('HIPRAND_3RD', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DEFINITION', ('HIPRAND_DEFINITION', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_POISSON', ('HIPRAND_POISSON', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curandCreateGenerator', ('hiprandCreateGenerator', CONV_MATH_FUNC, API_RAND)), ('curandCreateGeneratorHost', ('hiprandCreateGeneratorHost', CONV_MATH_FUNC, API_RAND)), ('curandCreatePoissonDistribution', ('hiprandCreatePoissonDistribution', CONV_MATH_FUNC, API_RAND)), ('curandDestroyDistribution', ('hiprandDestroyDistribution', CONV_MATH_FUNC, API_RAND)), ('curandDestroyGenerator', ('hiprandDestroyGenerator', CONV_MATH_FUNC, API_RAND)), ('curandGenerate', ('hiprandGenerate', CONV_MATH_FUNC, API_RAND)), ('curandGenerateLogNormal', ('hiprandGenerateLogNormal', CONV_MATH_FUNC, API_RAND)), ('curandGenerateLogNormalDouble', ('hiprandGenerateLogNormalDouble', CONV_MATH_FUNC, API_RAND)), ('curandGenerateLongLong', ('hiprandGenerateLongLong', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGenerateNormal', ('hiprandGenerateNormal', CONV_MATH_FUNC, API_RAND)), ('curandGenerateNormalDouble', ('hiprandGenerateNormalDouble', CONV_MATH_FUNC, API_RAND)), ('curandGeneratePoisson', ('hiprandGeneratePoisson', CONV_MATH_FUNC, API_RAND)), ('curandGenerateSeeds', ('hiprandGenerateSeeds', CONV_MATH_FUNC, API_RAND)), ('curandGenerateUniform', ('hiprandGenerateUniform', CONV_MATH_FUNC, API_RAND)), ('curandGenerateUniformDouble', ('hiprandGenerateUniformDouble', CONV_MATH_FUNC, API_RAND)), ('curandGetDirectionVectors32', ('hiprandGetDirectionVectors32', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetDirectionVectors64', ('hiprandGetDirectionVectors64', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetProperty', ('hiprandGetProperty', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetScrambleConstants32', ('hiprandGetScrambleConstants32', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetScrambleConstants64', ('hiprandGetScrambleConstants64', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetVersion', ('hiprandGetVersion', CONV_MATH_FUNC, API_RAND)), ('curandSetGeneratorOffset', ('hiprandSetGeneratorOffset', CONV_MATH_FUNC, API_RAND)), ('curandSetGeneratorOrdering', ('hiprandSetGeneratorOrdering', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandSetPseudoRandomGeneratorSeed', ('hiprandSetPseudoRandomGeneratorSeed', CONV_MATH_FUNC, API_RAND)), ('curandSetQuasiRandomGeneratorDimensions', ('hiprandSetQuasiRandomGeneratorDimensions', CONV_MATH_FUNC, API_RAND)), ('curandSetStream', ('hiprandSetStream', CONV_MATH_FUNC, API_RAND)), ('curand', ('hiprand', CONV_DEVICE_FUNC, API_RAND)), ('curand4', ('hiprand4', CONV_DEVICE_FUNC, API_RAND)), ('curand_init', ('hiprand_init', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal', ('hiprand_log_normal', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal_double', ('hiprand_log_normal_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal2', ('hiprand_log_normal2', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal2_double', ('hiprand_log_normal2_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal4', ('hiprand_log_normal4', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal4_double', ('hiprand_log_normal4_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_mtgp32_single', ('hiprand_mtgp32_single', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curand_mtgp32_single_specific', ('hiprand_mtgp32_single_specific', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curand_mtgp32_specific', ('hiprand_mtgp32_specific', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curand_normal', ('hiprand_normal', CONV_DEVICE_FUNC, API_RAND)), ('curandMakeMTGP32Constants', ('hiprandMakeMTGP32Constants', CONV_DEVICE_FUNC, API_RAND)), ('curandMakeMTGP32KernelState', ('hiprandMakeMTGP32KernelState', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal_double', ('hiprand_normal_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal2', ('hiprand_normal2', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal2_double', ('hiprand_normal2_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal4', ('hiprand_normal4', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal4_double', ('hiprand_normal4_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform', ('hiprand_uniform', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform_double', ('hiprand_uniform_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform2_double', ('hiprand_uniform2_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform4', ('hiprand_uniform4', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform4_double', ('hiprand_uniform4_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_discrete', ('hiprand_discrete', CONV_DEVICE_FUNC, API_RAND)), ('curand_discrete4', ('hiprand_discrete4', CONV_DEVICE_FUNC, API_RAND)), ('curand_poisson', ('hiprand_poisson', CONV_DEVICE_FUNC, API_RAND)), ('curand_poisson4', ('hiprand_poisson4', CONV_DEVICE_FUNC, API_RAND)), ('curand_Philox4x32_10', ('hiprand_Philox4x32_10', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('mtgp32_kernel_params', ('mtgp32_kernel_params_t', CONV_MATH_FUNC, API_RAND)), ('CUFFT_FORWARD', ('HIPFFT_FORWARD', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUFFT_INVERSE', ('HIPFFT_BACKWARD', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUFFT_COMPATIBILITY_DEFAULT', ('HIPFFT_COMPATIBILITY_DEFAULT', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('cufftResult_t', ('hipfftResult_t', CONV_TYPE, API_FFT)), ('cufftResult', ('hipfftResult', CONV_TYPE, API_FFT)), ('CUFFT_SUCCESS', ('HIPFFT_SUCCESS', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_PLAN', ('HIPFFT_INVALID_PLAN', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_ALLOC_FAILED', ('HIPFFT_ALLOC_FAILED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_TYPE', ('HIPFFT_INVALID_TYPE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_VALUE', ('HIPFFT_INVALID_VALUE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INTERNAL_ERROR', ('HIPFFT_INTERNAL_ERROR', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_EXEC_FAILED', ('HIPFFT_EXEC_FAILED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_SETUP_FAILED', ('HIPFFT_SETUP_FAILED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_SIZE', ('HIPFFT_INVALID_SIZE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_UNALIGNED_DATA', ('HIPFFT_UNALIGNED_DATA', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INCOMPLETE_PARAMETER_LIST', ('HIPFFT_INCOMPLETE_PARAMETER_LIST', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_DEVICE', ('HIPFFT_INVALID_DEVICE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_PARSE_ERROR', ('HIPFFT_PARSE_ERROR', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_NO_WORKSPACE', ('HIPFFT_NO_WORKSPACE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_NOT_IMPLEMENTED', ('HIPFFT_NOT_IMPLEMENTED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_LICENSE_ERROR', ('HIPFFT_LICENSE_ERROR', CONV_NUMERIC_LITERAL, API_FFT, HIP_UNSUPPORTED)), ('CUFFT_NOT_SUPPORTED', ('HIPFFT_NOT_SUPPORTED', CONV_NUMERIC_LITERAL, API_FFT)), ('cufftType_t', ('hipfftType_t', CONV_TYPE, API_FFT)), ('cufftType', ('hipfftType', CONV_TYPE, API_FFT)), ('CUFFT_R2C', ('HIPFFT_R2C', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_C2R', ('HIPFFT_C2R', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_C2C', ('HIPFFT_C2C', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_D2Z', ('HIPFFT_D2Z', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_Z2D', ('HIPFFT_Z2D', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_Z2Z', ('HIPFFT_Z2Z', CONV_NUMERIC_LITERAL, API_FFT)), ('cufftCompatibility_t', ('hipfftCompatibility_t', CONV_TYPE, API_FFT, HIP_UNSUPPORTED)), ('cufftCompatibility', ('hipfftCompatibility', CONV_TYPE, API_FFT, HIP_UNSUPPORTED)), ('CUFFT_COMPATIBILITY_FFTW_PADDING', ('HIPFFT_COMPATIBILITY_FFTW_PADDING', CONV_NUMERIC_LITERAL, API_FFT, HIP_UNSUPPORTED)), ('cufftReal', ('hipfftReal', CONV_TYPE, API_FFT)), ('cufftDoubleReal', ('hipfftDoubleReal', CONV_TYPE, API_FFT)), ('cufftComplex', ('hipfftComplex', CONV_TYPE, API_FFT)), ('cufftDoubleComplex', ('hipfftDoubleComplex', CONV_TYPE, API_FFT)), ('cufftHandle', ('hipfftHandle', CONV_TYPE, API_FFT)), ('cufftPlan1d', ('hipfftPlan1d', CONV_MATH_FUNC, API_FFT)), ('cufftPlan2d', ('hipfftPlan2d', CONV_MATH_FUNC, API_FFT)), ('cufftPlan3d', ('hipfftPlan3d', CONV_MATH_FUNC, API_FFT)), ('cufftPlanMany', ('hipfftPlanMany', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlan1d', ('hipfftMakePlan1d', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlan2d', ('hipfftMakePlan2d', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlan3d', ('hipfftMakePlan3d', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlanMany', ('hipfftMakePlanMany', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlanMany64', ('hipfftMakePlanMany64', CONV_MATH_FUNC, API_FFT)), ('cufftGetSizeMany64', ('hipfftGetSizeMany64', CONV_MATH_FUNC, API_FFT)), ('cufftEstimate1d', ('hipfftEstimate1d', CONV_MATH_FUNC, API_FFT)), ('cufftEstimate2d', ('hipfftEstimate2d', CONV_MATH_FUNC, API_FFT)), ('cufftEstimate3d', ('hipfftEstimate3d', CONV_MATH_FUNC, API_FFT)), ('cufftEstimateMany', ('hipfftEstimateMany', CONV_MATH_FUNC, API_FFT)), ('cufftCreate', ('hipfftCreate', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize1d', ('hipfftGetSize1d', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize2d', ('hipfftGetSize2d', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize3d', ('hipfftGetSize3d', CONV_MATH_FUNC, API_FFT)), ('cufftGetSizeMany', ('hipfftGetSizeMany', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize', ('hipfftGetSize', CONV_MATH_FUNC, API_FFT)), ('cufftSetWorkArea', ('hipfftSetWorkArea', CONV_MATH_FUNC, API_FFT)), ('cufftSetAutoAllocation', ('hipfftSetAutoAllocation', CONV_MATH_FUNC, API_FFT)), ('cufftExecC2C', ('hipfftExecC2C', CONV_MATH_FUNC, API_FFT)), ('cufftExecR2C', ('hipfftExecR2C', CONV_MATH_FUNC, API_FFT)), ('cufftExecC2R', ('hipfftExecC2R', CONV_MATH_FUNC, API_FFT)), ('cufftExecZ2Z', ('hipfftExecZ2Z', CONV_MATH_FUNC, API_FFT)), ('cufftExecD2Z', ('hipfftExecD2Z', CONV_MATH_FUNC, API_FFT)), ('cufftExecZ2D', ('hipfftExecZ2D', CONV_MATH_FUNC, API_FFT)), ('cufftSetStream', ('hipfftSetStream', CONV_MATH_FUNC, API_FFT)), ('cufftDestroy', ('hipfftDestroy', CONV_MATH_FUNC, API_FFT)), ('cufftGetVersion', ('hipfftGetVersion', CONV_MATH_FUNC, API_FFT)), ('cufftGetProperty', ('hipfftGetProperty', CONV_MATH_FUNC, API_FFT, HIP_UNSUPPORTED)), ('nvrtcResult', ('hiprtcResult', CONV_TYPE, API_RTC)), ('NVRTC_SUCCESS', ('HIPRTC_SUCCESS', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_OUT_OF_MEMORY', ('HIPRTC_ERROR_OUT_OF_MEMORY', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_PROGRAM_CREATION_FAILURE', ('HIPRTC_ERROR_PROGRAM_CREATION_FAILURE', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_INVALID_INPUT', ('HIPRTC_ERROR_INVALID_INPUT', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_INVALID_PROGRAM', ('HIPRTC_ERROR_INVALID_PROGRAM', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_COMPILATION', ('HIPRTC_ERROR_COMPILATION', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_BUILTIN_OPERATION_FAILURE', ('HIPRTC_ERROR_BUILTIN_OPERATION_FAILURE', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION', ('HIPRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID', ('HIPRTC_ERROR_NAME_EXPRESSION_NOT_VALID', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_INTERNAL_ERROR', ('HIPRTC_ERROR_INTERNAL_ERROR', CONV_TYPE, API_RTC)), ('nvrtcGetErrorString', ('hiprtcGetErrorString', CONV_JIT, API_RTC)), ('nvrtcVersion', ('hiprtcVersion', CONV_JIT, API_RTC)), ('nvrtcProgram', ('hiprtcProgram', CONV_TYPE, API_RTC)), ('nvrtcAddNameExpression', ('hiprtcAddNameExpression', CONV_JIT, API_RTC)), ('nvrtcCompileProgram', ('hiprtcCompileProgram', CONV_JIT, API_RTC)), ('nvrtcCreateProgram', ('hiprtcCreateProgram', CONV_JIT, API_RTC)), ('nvrtcDestroyProgram', ('hiprtcDestroyProgram', CONV_JIT, API_RTC)), ('nvrtcGetLoweredName', ('hiprtcGetLoweredName', CONV_JIT, API_RTC)), ('nvrtcGetProgramLog', ('hiprtcGetProgramLog', CONV_JIT, API_RTC)), ('nvrtcGetProgramLogSize', ('hiprtcGetProgramLogSize', CONV_JIT, API_RTC)), ('nvrtcGetPTX', ('hiprtcGetCode', CONV_JIT, API_RTC)), ('nvrtcGetPTXSize', ('hiprtcGetCodeSize', CONV_JIT, API_RTC)), ('thrust::cuda', ('thrust::hip', CONV_MATH_FUNC, API_BLAS)), ('cub::', ('hipcub::', CONV_MATH_FUNC, API_BLAS)), ('nvtxMark', ('roctxMark', CONV_OTHER, API_ROCTX)), ('nvtxMarkA', ('roctxMarkA', CONV_OTHER, API_ROCTX)), ('nvtxRangePushA', ('roctxRangePushA', CONV_OTHER, API_ROCTX)), ('nvtxRangePop', ('roctxRangePop', CONV_OTHER, API_ROCTX))])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_SPARSE_MAP->collections.OrderedDict([('cusparseStatus_t', ('hipsparseStatus_t', CONV_MATH_FUNC, API_SPARSE)), ('cusparseHandle_t', ('hipsparseHandle_t', CONV_MATH_FUNC, API_SPARSE)), ('cusparseOperation_t', ('hipsparseOperation_t', CONV_TYPE, API_SPARSE)), ('cusparseCreateMatDescr', ('hipsparseCreateMatDescr', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCreate', ('hipsparseCreate', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroyMatDescr', ('hipsparseDestroyMatDescr', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroy', ('hipsparseDestroy', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcoo2csr', ('hipsparseXcoo2csr', CONV_MATH_FUNC, API_SPARSE)), ('cusparseMatDescr_t', ('hipsparseMatDescr_t', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrmm2', ('hipsparseScsrmm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrmm2', ('hipsparseDcsrmm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrmm', ('hipsparseScsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrmm', ('hipsparseDcsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcsrsort_bufferSizeExt', ('hipsparseXcsrsort_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcsrsort', ('hipsparseXcsrsort', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcoosort_bufferSizeExt', ('hipsparseXcoosort_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcoosortByRow', ('hipsparseXcoosortByRow', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetStream', ('hipsparseSetStream', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCreateIdentityPermutation', ('hipsparseCreateIdentityPermutation', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetMatIndexBase', ('hipsparseSetMatIndexBase', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetMatType', ('hipsparseSetMatType', CONV_MATH_FUNC, API_SPARSE)), ('CUSPARSE_STATUS_SUCCESS', ('HIPSPARSE_STATUS_SUCCESS', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_NOT_INITIALIZED', ('HIPSPARSE_STATUS_NOT_INITIALIZED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_ALLOC_FAILED', ('HIPSPARSE_STATUS_ALLOC_FAILED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_INVALID_VALUE', ('HIPSPARSE_STATUS_INVALID_VALUE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_MAPPING_ERROR', ('HIPSPARSE_STATUS_MAPPING_ERROR', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_EXECUTION_FAILED', ('HIPSPARSE_STATUS_EXECUTION_FAILED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_INTERNAL_ERROR', ('HIPSPARSE_STATUS_INTERNAL_ERROR', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED', ('HIPSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_ARCH_MISMATCH', ('HIPSPARSE_STATUS_ARCH_MISMATCH', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_ZERO_PIVOT', ('HIPSPARSE_STATUS_ZERO_PIVOT', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_OPERATION_TRANSPOSE', ('HIPSPARSE_OPERATION_TRANSPOSE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_OPERATION_NON_TRANSPOSE', ('HIPSPARSE_OPERATION_NON_TRANSPOSE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE', ('HIPSPARSE_OPERATION_CONJUGATE_TRANSPOSE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_INDEX_BASE_ZERO', ('HIPSPARSE_INDEX_BASE_ZERO', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_INDEX_BASE_ONE', ('HIPSPARSE_INDEX_BASE_ONE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_MATRIX_TYPE_GENERAL', ('HIPSPARSE_MATRIX_TYPE_GENERAL', CONV_NUMERIC_LITERAL, API_SPARSE))])
A:torch.utils.hipify.cuda_to_hip_mappings.PYTORCH_SPECIFIC_MAPPINGS->collections.OrderedDict([('USE_CUDA', ('USE_ROCM', API_PYTORCH)), ('CUDA_VERSION', ('HIP_VERSION', API_PYTORCH)), ('cudaHostAllocator', ('hipHostAllocator', API_PYTORCH)), ('cudaDeviceAllocator', ('hipDeviceAllocator', API_PYTORCH)), ('define MAX_NUM_BLOCKS 200', ('define MAX_NUM_BLOCKS 64', API_PYTORCH)), ('cuda::CUDAGuard', ('hip::HIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('CUDAGuard', ('HIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::OptionalCUDAGuard', ('hip::OptionalHIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('OptionalCUDAGuard', ('OptionalHIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDAStreamGuard', ('hip::HIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('CUDAStreamGuard', ('HIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::OptionalCUDAStreamGuard', ('hip::OptionalHIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('OptionalCUDAStreamGuard', ('OptionalHIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDACachingAllocator::get', ('hip::HIPCachingAllocatorMasqueradingAsCUDA::get', API_PYTORCH)), ('CUDACachingAllocator::get', ('HIPCachingAllocatorMasqueradingAsCUDA::get', API_PYTORCH)), ('cuda::CUDACachingAllocator::recordStream', ('hip::HIPCachingAllocatorMasqueradingAsCUDA::recordStreamMasqueradingAsCUDA', API_PYTORCH)), ('CUDACachingAllocator::recordStream', ('HIPCachingAllocatorMasqueradingAsCUDA::recordStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDAStream', ('hip::HIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('CUDAStream', ('HIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getStreamFromPool', ('hip::getStreamFromPoolMasqueradingAsCUDA', API_PYTORCH)), ('getStreamFromPool', ('getStreamFromPoolMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getDefaultCUDAStream', ('hip::getDefaultHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('getDefaultCUDAStream', ('getDefaultHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getCurrentCUDAStream', ('hip::getCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('getCurrentCUDAStream', ('getCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::setCurrentCUDAStream', ('hip::setCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('setCurrentCUDAStream', ('setCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('c10/cuda/CUDAGuard.h', ('ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h', API_PYTORCH)), ('c10/cuda/CUDACachingAllocator.h', ('ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h', API_PYTORCH)), ('c10/cuda/CUDAStream.h', ('ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h', API_PYTORCH)), ('gloo/cuda.h', ('gloo/hip.h', API_PYTORCH)), ('gloo/cuda_allreduce_halving_doubling.h', ('gloo/hip_allreduce_halving_doubling.h', API_PYTORCH)), ('gloo/cuda_allreduce_halving_doubling_pipelined.h', ('gloo/hip_allreduce_halving_doubling_pipelined.h', API_PYTORCH)), ('gloo/cuda_allreduce_ring.h', ('gloo/hip_allreduce_ring.h', API_PYTORCH)), ('gloo/cuda_broadcast_one_to_all.h', ('gloo/hip_broadcast_one_to_all.h', API_PYTORCH)), ('gloo::CudaAllreduceHalvingDoublingPipelined', ('gloo::HipAllreduceHalvingDoublingPipelined', API_PYTORCH)), ('gloo::CudaBroadcastOneToAll', ('gloo::HipBroadcastOneToAll', API_PYTORCH)), ('gloo::CudaHostWorkspace', ('gloo::HipHostWorkspace', API_PYTORCH)), ('gloo::CudaDeviceWorkspace', ('gloo::HipDeviceWorkspace', API_PYTORCH)), ('CUDNN_RNN_RELU', ('miopenRNNRELU', API_PYTORCH)), ('CUDNN_RNN_TANH', ('miopenRNNTANH', API_PYTORCH)), ('CUDNN_LSTM', ('miopenLSTM', API_PYTORCH)), ('CUDNN_GRU', ('miopenGRU', API_PYTORCH)), ('cudnnRNNMode_t', ('miopenRNNMode_t', API_PYTORCH))])
A:torch.utils.hipify.cuda_to_hip_mappings.CAFFE2_SPECIFIC_MAPPINGS->collections.OrderedDict([('cuda_stream', ('hip_stream', API_CAFFE2)), ('/hip/', ('/hip/', API_CAFFE2)), ('/context_gpu', ('/hip/context_gpu', API_CAFFE2)), ('/common_gpu', ('/hip/common_gpu', API_CAFFE2)), ('/cuda_nccl_gpu', ('/hip/hip_nccl_gpu', API_CAFFE2)), ('/mixed_utils', ('/hip/mixed_utils', API_CAFFE2)), ('/operator_fallback_gpu', ('/hip/operator_fallback_gpu', API_CAFFE2)), ('/spatial_batch_norm_op_impl', ('/hip/spatial_batch_norm_op_impl', API_CAFFE2)), ('/recurrent_network_executor_gpu', ('/hip/recurrent_network_executor_gpu', API_CAFFE2)), ('/generate_proposals_op_util_nms_gpu', ('/hip/generate_proposals_op_util_nms_gpu', API_CAFFE2)), ('/max_pool_with_index_gpu', ('/hip/max_pool_with_index_gpu', API_CAFFE2)), ('/THCCachingAllocator_gpu', ('/hip/THCCachingAllocator_gpu', API_CAFFE2)), ('/top_k_heap_selection', ('/hip/top_k_heap_selection', API_CAFFE2)), ('/top_k_radix_selection', ('/hip/top_k_radix_selection', API_CAFFE2)), ('/GpuDefs', ('/hip/GpuDefs', API_CAFFE2)), ('/GpuScanUtils', ('/hip/GpuScanUtils', API_CAFFE2)), ('/GpuBitonicSort', ('/hip/GpuBitonicSort', API_CAFFE2)), ('/math/reduce.cuh', ('/math/hip/reduce.cuh', API_CAFFE2)), ('/gather_op.cuh', ('/hip/gather_op.cuh', API_CAFFE2)), ('caffe2/core/common_cudnn.h', ('caffe2/core/hip/common_miopen.h', API_CAFFE2)), ('REGISTER_CUDA_OPERATOR', ('REGISTER_HIP_OPERATOR', API_CAFFE2)), ('CUDA_1D_KERNEL_LOOP', ('HIP_1D_KERNEL_LOOP', API_CAFFE2)), ('CUDAContext', ('HIPContext', API_CAFFE2)), ('CAFFE_CUDA_NUM_THREADS', ('CAFFE_HIP_NUM_THREADS', API_CAFFE2)), ('HasCudaGPU', ('HasHipGPU', API_CAFFE2)), ('__expf', ('expf', API_CAFFE2)), ('CUBLAS_ENFORCE', ('ROCBLAS_ENFORCE', API_CAFFE2)), ('CUBLAS_CHECK', ('ROCBLAS_CHECK', API_CAFFE2)), ('cublas_handle', ('rocblashandle', API_CAFFE2)), ('CURAND_ENFORCE', ('HIPRAND_ENFORCE', API_CAFFE2)), ('CURAND_CHECK', ('HIPRAND_CHECK', API_CAFFE2)), ('curandGenerateUniform', ('hiprandGenerateUniform', API_CAFFE2)), ('curand_generator', ('hiprand_generator', API_CAFFE2)), ('CaffeCudaGetDevice', ('CaffeHipGetDevice', API_CAFFE2)), ('CUDA_KERNEL_ASSERT', ('CUDA_KERNEL_ASSERT', API_CAFFE2)), ('CUDA', ('HIP', API_CAFFE2)), ('Cuda', ('Hip', API_CAFFE2)), ('cuda_', ('hip_', API_CAFFE2)), ('_cuda', ('_hip', API_CAFFE2)), ('CUDNN', ('MIOPEN', API_CAFFE2)), ('CuDNN', ('MIOPEN', API_CAFFE2)), ('cudnn', ('miopen', API_CAFFE2)), ('namespace cuda', ('namespace hip', API_CAFFE2)), ('cuda::CUDAGuard', ('hip::HIPGuard', API_CAFFE2)), ('cuda::OptionalCUDAGuard', ('hip::OptionalHIPGuard', API_CAFFE2)), ('cuda::CUDAStreamGuard', ('hip::HIPStreamGuard', API_CAFFE2)), ('cuda::OptionalCUDAStreamGuard', ('hip::OptionalHIPStreamGuard', API_CAFFE2)), ('c10/cuda/CUDAGuard.h', ('c10/hip/HIPGuard.h', API_CAFFE2)), ('gloo/cuda', ('gloo/hip', API_CAFFE2))])
A:torch.utils.hipify.cuda_to_hip_mappings.C10_MAPPINGS->collections.OrderedDict([('cuda::compat::', ('hip::compat::', API_C10)), ('c10/cuda/CUDAException.h', ('c10/hip/HIPException.h', API_C10)), ('c10/cuda/CUDAMacros.h', ('c10/hip/HIPMacros.h', API_C10)), ('c10/cuda/CUDAMathCompat.h', ('c10/hip/HIPMathCompat.h', API_C10)), ('c10/cuda/CUDAFunctions.h', ('c10/hip/HIPFunctions.h', API_C10)), ('c10/cuda/CUDAStream.h', ('c10/hip/HIPStream.h', API_C10)), ('c10/cuda/CUDACachingAllocator.h', ('c10/hip/HIPCachingAllocator.h', API_C10)), ('c10/cuda/impl/CUDATest.h', ('c10/hip/impl/HIPTest.h', API_C10)), ('c10/cuda/impl/CUDAGuardImpl.h', ('c10/hip/impl/HIPGuardImpl.h', API_C10)), ('c10/cuda/impl/cuda_cmake_macros.h', ('c10/hip/impl/hip_cmake_macros.h', API_C10)), ('C10_CUDA_CHECK', ('C10_HIP_CHECK', API_C10)), ('C10_CUDA_CHECK_WARN', ('C10_HIP_CHECK_WARN', API_C10)), ('c10::cuda', ('c10::hip', API_C10)), ('cuda::CUDAStream', ('hip::HIPStream', API_C10)), ('CUDAStream', ('HIPStream', API_C10)), ('cuda::current_device', ('hip::current_device', API_C10)), ('cuda::set_device', ('hip::set_device', API_C10)), ('cuda::getStreamFromPool', ('hip::getStreamFromPool', API_C10)), ('getStreamFromPool', ('getStreamFromPool', API_C10)), ('cuda::getDefaultCUDAStream', ('hip::getDefaultHIPStream', API_C10)), ('getDefaultCUDAStream', ('getDefaultHIPStream', API_C10)), ('cuda::getCurrentCUDAStream', ('hip::getCurrentHIPStream', API_C10)), ('getCurrentCUDAStream', ('getCurrentHIPStream', API_C10)), ('cuda::setCurrentCUDAStream', ('hip::setCurrentHIPStream', API_C10)), ('setCurrentCUDAStream', ('setCurrentHIPStream', API_C10)), ('cuda::CUDACachingAllocator', ('hip::HIPCachingAllocator', API_C10)), ('CUDACachingAllocator', ('HIPCachingAllocator', API_C10))])


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/hipify/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/hipify/hipify_python.py----------------------------------------
A:torch.utils.hipify.hipify_python.exact_matches->set(includes)
A:torch.utils.hipify.hipify_python.rel_dirpath->os.path.relpath(abs_dirpath, root_path)
A:torch.utils.hipify.hipify_python.filepath->os.path.join(rel_dirpath, filename)
A:torch.utils.hipify.hipify_python.result->preprocessor(output_directory, filepath, stats, hip_clang_launch, is_pytorch_extension)
A:torch.utils.hipify.hipify_python.kernel_string->kernel_string.replace('<<<', '').replace('>>>', '').replace('<<<', '').replace('>>>', '')
A:torch.utils.hipify.hipify_python.first_arg_clean->kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace('\n', '').strip(' ')
A:torch.utils.hipify.hipify_python.second_arg_clean->kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace('\n', '').strip(' ')
A:torch.utils.hipify.hipify_python.first_arg_dim3->'dim3({})'.format(first_arg_clean)
A:torch.utils.hipify.hipify_python.second_arg_dim3->'dim3({})'.format(second_arg_clean)
A:torch.utils.hipify.hipify_python.first_arg_raw_dim3->first_arg_raw.replace(first_arg_clean, first_arg_dim3)
A:torch.utils.hipify.hipify_python.second_arg_raw_dim3->second_arg_raw.replace(second_arg_clean, second_arg_dim3)
A:torch.utils.hipify.hipify_python.cuda_kernel->cuda_kernel.replace(first_arg_raw + second_arg_raw, first_arg_raw_dim3 + second_arg_raw_dim3).replace(first_arg_raw + second_arg_raw, first_arg_raw_dim3 + second_arg_raw_dim3)
A:torch.utils.hipify.hipify_python.RE_KERNEL_LAUNCH->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+')
A:torch.utils.hipify.hipify_python.string->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+').sub(lambda inp: '{0}{1}::'.format(inp.group(1), inp.group(2)), string)
A:torch.utils.hipify.hipify_python.kernel_start->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+').sub(lambda inp: '{0}{1}::'.format(inp.group(1), inp.group(2)), string).find('<<<', kernel_end)
A:torch.utils.hipify.hipify_python.get_kernel_positions->list(find_kernel_bounds(string))
A:torch.utils.hipify.hipify_python.params->grab_method_and_template(kernel)
A:torch.utils.hipify.hipify_python.parenthesis->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+').sub(lambda inp: '{0}{1}::'.format(inp.group(1), inp.group(2)), string).find('(', kernel['end'])
A:torch.utils.hipify.hipify_python.cuda_kernel_dim3->add_dim3(kernel_string, cuda_kernel)
A:torch.utils.hipify.hipify_python.num_klp->len(extract_arguments(0, kernel['group'].replace('<<<', '(').replace('>>>', ')')))
A:torch.utils.hipify.hipify_python.output_string->re.compile('extern\\s+([\\w\\(\\)]+)?\\s*__shared__\\s+([\\w:<>\\s]+)\\s+(\\w+)\\s*\\[\\s*\\]\\s*;').sub(lambda inp: 'HIP_DYNAMIC_SHARED({0} {1}, {2})'.format(inp.group(1) or '', inp.group(2), inp.group(3)), output_string)
A:torch.utils.hipify.hipify_python.RE_ASSERT->re.compile('\\bassert[ ]*\\(')
A:torch.utils.hipify.hipify_python.RE_SYNCTHREADS->re.compile('[:]?[:]?\\b(__syncthreads)\\b(\\w*\\()')
A:torch.utils.hipify.hipify_python.RE_EXTERN_SHARED->re.compile('extern\\s+([\\w\\(\\)]+)?\\s*__shared__\\s+([\\w:<>\\s]+)\\s+(\\w+)\\s*\\[\\s*\\]\\s*;')
A:torch.utils.hipify.hipify_python.(dirpath, filename)->os.path.split(filepath)
A:torch.utils.hipify.hipify_python.(root, ext)->os.path.splitext(filename)
A:torch.utils.hipify.hipify_python.dirpath->os.path.join(dirpath, 'hip')
A:torch.utils.hipify.hipify_python.root->root.replace('THC', 'THH').replace('THC', 'THH')
A:torch.utils.hipify.hipify_python.filename->os.path.basename(filepath)
A:torch.utils.hipify.hipify_python.(_, ext)->os.path.splitext(filename)
A:torch.utils.hipify.hipify_python.recurse->self._pattern(data[char])
A:torch.utils.hipify.hipify_python.CAFFE2_TRIE->Trie()
A:torch.utils.hipify.hipify_python.PYTORCH_TRIE->Trie()
A:torch.utils.hipify.hipify_python.RE_CAFFE2_PREPROCESSOR->re.compile(CAFFE2_TRIE.pattern())
A:torch.utils.hipify.hipify_python.RE_PYTORCH_PREPROCESSOR->re.compile('(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))
A:torch.utils.hipify.hipify_python.RE_QUOTE_HEADER->re.compile('#include "([^"]+)"')
A:torch.utils.hipify.hipify_python.RE_ANGLE_HEADER->re.compile('#include <([^>]+)>')
A:torch.utils.hipify.hipify_python.RE_THC_GENERIC_FILE->re.compile('#define THC_GENERIC_FILE "([^"]+)"')
A:torch.utils.hipify.hipify_python.RE_CU_SUFFIX->re.compile('\\.cu\\b')
A:torch.utils.hipify.hipify_python.fin_path->os.path.join(output_directory, filepath)
A:torch.utils.hipify.hipify_python.output_source->replace_extern_shared(output_source)
A:torch.utils.hipify.hipify_python.fout_path->os.path.join(output_directory, get_hip_file_path(filepath))
A:torch.utils.hipify.hipify_python.f->m.group(1)
A:torch.utils.hipify.hipify_python.contents->m.group(1).read()
A:torch.utils.hipify.hipify_python.header->'"{0}"'.format(header)
A:torch.utils.hipify.hipify_python.in_txt->in_txt.replace(' __global__ static', '__global__').replace(' __global__ static', '__global__')
A:torch.utils.hipify.hipify_python.RE_INCLUDE->re.compile('#include .*\\n')
A:torch.utils.hipify.hipify_python.project_directory->os.getcwd()
A:torch.utils.hipify.hipify_python.all_files->list(matched_files_iter(output_directory, includes=includes, ignores=ignores, extensions=extensions, out_of_place_only=out_of_place_only))
torch.utils.hipify.hipify_python.InputError(self,message)
torch.utils.hipify.hipify_python.InputError.__init__(self,message)
torch.utils.hipify.hipify_python.InputError.__str__(self)
torch.utils.hipify.hipify_python.Trie(self)
torch.utils.hipify.hipify_python.Trie.__init__(self)
torch.utils.hipify.hipify_python.Trie._pattern(self,pData)
torch.utils.hipify.hipify_python.Trie.add(self,word)
torch.utils.hipify.hipify_python.Trie.dump(self)
torch.utils.hipify.hipify_python.Trie.pattern(self)
torch.utils.hipify.hipify_python.Trie.quote(self,char)
torch.utils.hipify.hipify_python.add_dim3(kernel_string,cuda_kernel)
torch.utils.hipify.hipify_python.bcolors
torch.utils.hipify.hipify_python.compute_stats(stats)
torch.utils.hipify.hipify_python.extract_arguments(start,string)
torch.utils.hipify.hipify_python.file_add_header(filepath,header)
torch.utils.hipify.hipify_python.file_specific_replacement(filepath,search_string,replace_string,strict=False)
torch.utils.hipify.hipify_python.find_bracket_group(input_string,start)
torch.utils.hipify.hipify_python.find_closure_group(input_string,start,group)
torch.utils.hipify.hipify_python.find_parentheses_group(input_string,start)
torch.utils.hipify.hipify_python.fix_static_global_kernels(in_txt)
torch.utils.hipify.hipify_python.get_hip_file_path(filepath)
torch.utils.hipify.hipify_python.hip_header_magic(input_string)
torch.utils.hipify.hipify_python.hipify(project_directory,show_detailed=False,extensions=('.cu','.cuh','.c','.cc','.cpp','.h','.in','.hpp'),output_directory='',includes=(),out_of_place_only=False,ignores=(),show_progress=True,hip_clang_launch=False,is_pytorch_extension=False)
torch.utils.hipify.hipify_python.is_caffe2_gpu_file(filepath)
torch.utils.hipify.hipify_python.is_out_of_place(filepath)
torch.utils.hipify.hipify_python.is_pytorch_file(filepath)
torch.utils.hipify.hipify_python.matched_files_iter(root_path,includes=('*',),ignores=(),extensions=(),out_of_place_only=False)
torch.utils.hipify.hipify_python.openf(filename,mode)
torch.utils.hipify.hipify_python.preprocess(output_directory,all_files,show_detailed=False,show_progress=True,hip_clang_launch=False,is_pytorch_extension=False)
torch.utils.hipify.hipify_python.preprocessor(output_directory,filepath,stats,hip_clang_launch,is_pytorch_extension)
torch.utils.hipify.hipify_python.processKernelLaunches(string,stats)
torch.utils.hipify.hipify_python.replace_extern_shared(input_string)
torch.utils.hipify.hipify_python.replace_math_functions(input_string)
torch.utils.hipify.hipify_python.str2bool(v)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/hipify/constants.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/ffi/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_utils.py----------------------------------------
A:torch.utils.tensorboard._utils.canvas->numpy.zeros((3, H * nrows, W * ncols), dtype=I.dtype)
A:torch.utils.tensorboard._utils.data->numpy.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)
A:torch.utils.tensorboard._utils.(w, h)->figure.canvas.get_width_height()
A:torch.utils.tensorboard._utils.image_chw->numpy.moveaxis(image_hwc, source=2, destination=0)
A:torch.utils.tensorboard._utils.image->render_to_rgb(figures)
A:torch.utils.tensorboard._utils.len_addition->int(2 ** V.shape[0].bit_length() - V.shape[0])
A:torch.utils.tensorboard._utils.V->numpy.reshape(V, newshape=(t, n_rows * h, n_cols * w, c))
A:torch.utils.tensorboard._utils.I->numpy.concatenate([I, I, I], 1)
A:torch.utils.tensorboard._utils.ncols->min(nimg, ncols)
A:torch.utils.tensorboard._utils.nrows->int(np.ceil(float(nimg) / ncols))
A:torch.utils.tensorboard._utils.input_format->input_format.upper().upper()
A:torch.utils.tensorboard._utils.tensor_NCHW->numpy.stack([tensor, tensor, tensor], 2).transpose(index)
A:torch.utils.tensorboard._utils.tensor_CHW->make_grid(tensor_NCHW)
A:torch.utils.tensorboard._utils.tensor_HWC->numpy.concatenate([tensor_HWC, tensor_HWC, tensor_HWC], 2)
A:torch.utils.tensorboard._utils.tensor->numpy.stack([tensor, tensor, tensor], 2)
torch.utils.tensorboard._utils._prepare_video(V)
torch.utils.tensorboard._utils.convert_to_HWC(tensor,input_format)
torch.utils.tensorboard._utils.figure_to_image(figures,close=True)
torch.utils.tensorboard._utils.make_grid(I,ncols=8)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_convert_np.py----------------------------------------
A:torch.utils.tensorboard._convert_np.x->caffe2.python.workspace.FetchBlob(x)
torch.utils.tensorboard._convert_np._prepare_caffe2(x)
torch.utils.tensorboard._convert_np._prepare_pytorch(x)
torch.utils.tensorboard._convert_np.make_np(x)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/writer.py----------------------------------------
A:torch.utils.tensorboard.writer.log_dir->os.path.join('runs', current_time + '_' + socket.gethostname() + comment)
A:torch.utils.tensorboard.writer.self.event_writer->EventFileWriter(log_dir, max_queue, flush_secs, filename_suffix)
A:torch.utils.tensorboard.writer.event.step->int(step)
A:torch.utils.tensorboard.writer.event->tensorboard.compat.proto.event_pb2.Event(graph_def=current_graph.SerializeToString())
A:torch.utils.tensorboard.writer.trm->tensorboard.compat.proto.event_pb2.TaggedRunMetadata(tag='step1', run_metadata=stepstats.SerializeToString())
A:torch.utils.tensorboard.writer.current_time->datetime.datetime.now().strftime('%b%d_%H-%M-%S')
A:torch.utils.tensorboard.writer.self.file_writer->FileWriter(self.log_dir, self.max_queue, self.flush_secs, self.filename_suffix)
A:torch.utils.tensorboard.writer.(exp, ssi, sei)->hparams(hparam_dict, metric_dict)
A:torch.utils.tensorboard.writer.logdir->os.path.join(self._get_file_writer().get_logdir(), str(time.time()))
A:torch.utils.tensorboard.writer.scalar_value->workspace.FetchBlob(scalar_value)
A:torch.utils.tensorboard.writer.fw_logdir->self._get_file_writer().get_logdir()
A:torch.utils.tensorboard.writer.fw->FileWriter(fw_tag, self.max_queue, self.flush_secs, self.filename_suffix)
A:torch.utils.tensorboard.writer.values->workspace.FetchBlob(values)
A:torch.utils.tensorboard.writer.img_tensor->workspace.FetchBlob(img_tensor)
A:torch.utils.tensorboard.writer.box_tensor->workspace.FetchBlob(box_tensor)
A:torch.utils.tensorboard.writer.snd_tensor->workspace.FetchBlob(snd_tensor)
A:torch.utils.tensorboard.writer.current_graph->model_to_graph_def(model)
A:torch.utils.tensorboard.writer.retval->retval.replace('\\', '%%%02x' % ord('\\')).replace('\\', '%%%02x' % ord('\\'))
A:torch.utils.tensorboard.writer.mat->make_np(mat)
A:torch.utils.tensorboard.writer.save_path->os.path.join(self._get_file_writer().get_logdir(), subdir)
A:torch.utils.tensorboard.writer.fs->tensorboard.compat.tf.io.gfile.get_filesystem(save_path)
A:torch.utils.tensorboard.writer.self._projector_config->ProjectorConfig()
A:torch.utils.tensorboard.writer.embedding_info->get_embedding_info(metadata, label_img, fs, subdir, global_step, tag)
A:torch.utils.tensorboard.writer.config_pbtxt->google.protobuf.text_format.MessageToString(self._projector_config)
torch.utils.tensorboard.FileWriter(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.FileWriter.add_event(self,event,step=None,walltime=None)
torch.utils.tensorboard.FileWriter.add_graph(self,graph_profile,walltime=None)
torch.utils.tensorboard.FileWriter.add_onnx_graph(self,graph,walltime=None)
torch.utils.tensorboard.FileWriter.add_summary(self,summary,global_step=None,walltime=None)
torch.utils.tensorboard.FileWriter.close(self)
torch.utils.tensorboard.FileWriter.flush(self)
torch.utils.tensorboard.FileWriter.get_logdir(self)
torch.utils.tensorboard.FileWriter.reopen(self)
torch.utils.tensorboard.SummaryWriter(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.SummaryWriter.__enter__(self)
torch.utils.tensorboard.SummaryWriter.__exit__(self,exc_type,exc_val,exc_tb)
torch.utils.tensorboard.SummaryWriter._check_caffe2_blob(self,item)
torch.utils.tensorboard.SummaryWriter._encode(rawstr)
torch.utils.tensorboard.SummaryWriter._get_file_writer(self)
torch.utils.tensorboard.SummaryWriter.add_audio(self,tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_custom_scalars(self,layout)
torch.utils.tensorboard.SummaryWriter.add_custom_scalars_marginchart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.SummaryWriter.add_custom_scalars_multilinechart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.SummaryWriter.add_embedding(self,mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)
torch.utils.tensorboard.SummaryWriter.add_figure(self,tag,figure,global_step=None,close=True,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_graph(self,model,input_to_model=None,verbose=False)
torch.utils.tensorboard.SummaryWriter.add_histogram(self,tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)
torch.utils.tensorboard.SummaryWriter.add_histogram_raw(self,tag,min,max,num,sum,sum_squares,bucket_limits,bucket_counts,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_hparams(self,hparam_dict=None,metric_dict=None)
torch.utils.tensorboard.SummaryWriter.add_image(self,tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')
torch.utils.tensorboard.SummaryWriter.add_image_with_boxes(self,tag,img_tensor,box_tensor,global_step=None,walltime=None,rescale=1,dataformats='CHW')
torch.utils.tensorboard.SummaryWriter.add_images(self,tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')
torch.utils.tensorboard.SummaryWriter.add_mesh(self,tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_onnx_graph(self,prototxt)
torch.utils.tensorboard.SummaryWriter.add_pr_curve(self,tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_pr_curve_raw(self,tag,true_positive_counts,false_positive_counts,true_negative_counts,false_negative_counts,precision,recall,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_scalar(self,tag,scalar_value,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_scalars(self,main_tag,tag_scalar_dict,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_text(self,tag,text_string,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_video(self,tag,vid_tensor,global_step=None,fps=4,walltime=None)
torch.utils.tensorboard.SummaryWriter.close(self)
torch.utils.tensorboard.SummaryWriter.flush(self)
torch.utils.tensorboard.SummaryWriter.get_logdir(self)
torch.utils.tensorboard.writer.FileWriter(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.FileWriter.__init__(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.FileWriter.add_event(self,event,step=None,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_graph(self,graph_profile,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_onnx_graph(self,graph,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_summary(self,summary,global_step=None,walltime=None)
torch.utils.tensorboard.writer.FileWriter.close(self)
torch.utils.tensorboard.writer.FileWriter.flush(self)
torch.utils.tensorboard.writer.FileWriter.get_logdir(self)
torch.utils.tensorboard.writer.FileWriter.reopen(self)
torch.utils.tensorboard.writer.SummaryWriter(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.SummaryWriter.__enter__(self)
torch.utils.tensorboard.writer.SummaryWriter.__exit__(self,exc_type,exc_val,exc_tb)
torch.utils.tensorboard.writer.SummaryWriter.__init__(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.SummaryWriter._check_caffe2_blob(self,item)
torch.utils.tensorboard.writer.SummaryWriter._encode(rawstr)
torch.utils.tensorboard.writer.SummaryWriter._get_file_writer(self)
torch.utils.tensorboard.writer.SummaryWriter.add_audio(self,tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars(self,layout)
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars_marginchart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars_multilinechart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.writer.SummaryWriter.add_embedding(self,mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)
torch.utils.tensorboard.writer.SummaryWriter.add_figure(self,tag,figure,global_step=None,close=True,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_graph(self,model,input_to_model=None,verbose=False)
torch.utils.tensorboard.writer.SummaryWriter.add_histogram(self,tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)
torch.utils.tensorboard.writer.SummaryWriter.add_histogram_raw(self,tag,min,max,num,sum,sum_squares,bucket_limits,bucket_counts,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_hparams(self,hparam_dict=None,metric_dict=None)
torch.utils.tensorboard.writer.SummaryWriter.add_image(self,tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')
torch.utils.tensorboard.writer.SummaryWriter.add_image_with_boxes(self,tag,img_tensor,box_tensor,global_step=None,walltime=None,rescale=1,dataformats='CHW')
torch.utils.tensorboard.writer.SummaryWriter.add_images(self,tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')
torch.utils.tensorboard.writer.SummaryWriter.add_mesh(self,tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_onnx_graph(self,prototxt)
torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve(self,tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve_raw(self,tag,true_positive_counts,false_positive_counts,true_negative_counts,false_negative_counts,precision,recall,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_scalar(self,tag,scalar_value,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_scalars(self,main_tag,tag_scalar_dict,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_text(self,tag,text_string,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_video(self,tag,vid_tensor,global_step=None,fps=4,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.close(self)
torch.utils.tensorboard.writer.SummaryWriter.flush(self)
torch.utils.tensorboard.writer.SummaryWriter.get_logdir(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_caffe2_graph.py----------------------------------------
A:torch.utils.tensorboard._caffe2_graph.WEIGHT->re.compile('(_w)$')
A:torch.utils.tensorboard._caffe2_graph.WEIGHT_->re.compile('(_w_)')
A:torch.utils.tensorboard._caffe2_graph.BN->re.compile('(_bn)$')
A:torch.utils.tensorboard._caffe2_graph.BN_->re.compile('(_bn_)')
A:torch.utils.tensorboard._caffe2_graph.BIAS->re.compile('(_b)$')
A:torch.utils.tensorboard._caffe2_graph.BIAS_->re.compile('(_b_)')
A:torch.utils.tensorboard._caffe2_graph.SCALE->re.compile('(_s)$')
A:torch.utils.tensorboard._caffe2_graph.SCALE_->re.compile('(_s_)')
A:torch.utils.tensorboard._caffe2_graph.SUM->re.compile('(_sum)$')
A:torch.utils.tensorboard._caffe2_graph.SUM_->re.compile('(_sum_)')
A:torch.utils.tensorboard._caffe2_graph.BRANCH->re.compile('(_branch)')
A:torch.utils.tensorboard._caffe2_graph.inter_name->re.compile('(_sum_)').sub('/sum_', SUM.sub('/sum', inter_name))
A:torch.utils.tensorboard._caffe2_graph.new_name->_make_unique_name(seen, rename_fn(name))
A:torch.utils.tensorboard._caffe2_graph.ir->caffe2.python.core.IR(ops)
A:torch.utils.tensorboard._caffe2_graph.seen->set(input_blobs)
A:torch.utils.tensorboard._caffe2_graph.inputs->list(op.input)
A:torch.utils.tensorboard._caffe2_graph.outputs->list(op.output)
A:torch.utils.tensorboard._caffe2_graph.names->set()
A:torch.utils.tensorboard._caffe2_graph.op.name->_make_unique_name(seen, name)
A:torch.utils.tensorboard._caffe2_graph.scope->os.path.commonprefix(name_list)
A:torch.utils.tensorboard._caffe2_graph.name->os.path.join(scope, op.type)
A:torch.utils.tensorboard._caffe2_graph.shape_proto->TensorShapeProto()
A:torch.utils.tensorboard._caffe2_graph.dim->tensorboard.compat.proto.tensor_shape_pb2.TensorShapeProto.Dim()
A:torch.utils.tensorboard._caffe2_graph.n->NodeDef()
A:torch.utils.tensorboard._caffe2_graph.n.device->_tf_device(device)
A:torch.utils.tensorboard._caffe2_graph.len_outputs->len(outputs)
A:torch.utils.tensorboard._caffe2_graph.name_list->list(outputs)
A:torch.utils.tensorboard._caffe2_graph.device->_tf_device(op.device_option)
A:torch.utils.tensorboard._caffe2_graph.produced_by->producing_ops.get(name, [])
A:torch.utils.tensorboard._caffe2_graph.in_blobs->set()
A:torch.utils.tensorboard._caffe2_graph.out_blobs->set()
A:torch.utils.tensorboard._caffe2_graph.input_blobs->list(in_blobs.difference(out_blobs))
A:torch.utils.tensorboard._caffe2_graph.output_blobs->list(out_blobs.difference(in_blobs))
A:torch.utils.tensorboard._caffe2_graph.ops->_filter_ops(ops, _check_if_cpu, show_simplified)
A:torch.utils.tensorboard._caffe2_graph.blobs->set()
A:torch.utils.tensorboard._caffe2_graph.(input_blobs, inter_blobs, _)->_compute_in_out(ops)
A:torch.utils.tensorboard._caffe2_graph.current_graph->GraphDef()
A:torch.utils.tensorboard._caffe2_graph.(shapes, _)->caffe2.python.workspace.InferShapesAndTypes(nets)
A:torch.utils.tensorboard._caffe2_graph.shapes->copy.deepcopy(shapes or {})
torch.utils.tensorboard._caffe2_graph._add_gradient_scope(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._add_tf_shape(attr_dict,ints)
torch.utils.tensorboard._caffe2_graph._blob_to_node(producing_ops,shapes,name)
torch.utils.tensorboard._caffe2_graph._check_if_cpu(blob)
torch.utils.tensorboard._caffe2_graph._check_if_forward(blob)
torch.utils.tensorboard._caffe2_graph._clear_debug_info(ops,perform_clear)
torch.utils.tensorboard._caffe2_graph._compute_in_out(ops)
torch.utils.tensorboard._caffe2_graph._convert_to_ssa(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._fill_missing_operator_names(ops)
torch.utils.tensorboard._caffe2_graph._filter_ops(ops,filter_fn,perform_filter)
torch.utils.tensorboard._caffe2_graph._get_blob_names(ops)
torch.utils.tensorboard._caffe2_graph._make_unique_name(seen,name,min_version=0)
torch.utils.tensorboard._caffe2_graph._operator_to_node(shapes,op)
torch.utils.tensorboard._caffe2_graph._operator_to_node_simp(op,inter_blobs,seen)
torch.utils.tensorboard._caffe2_graph._operators_to_graph_def(shapes,ops,colon_replacement='$',with_ssa=True,with_gradient_scope=True,blob_name_tracker=None,show_simplified=False,custom_rename=None)
torch.utils.tensorboard._caffe2_graph._propagate_device_option(net_def)
torch.utils.tensorboard._caffe2_graph._remap_keys(old_dict,rename_fn)
torch.utils.tensorboard._caffe2_graph._rename_all(shapes,blob_name_tracker,ops,rename_fn)
torch.utils.tensorboard._caffe2_graph._rename_tensorflow_style(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._replace_colons(shapes,blob_name_tracker,ops,repl)
torch.utils.tensorboard._caffe2_graph._set_tf_attr(attr_dict,arg)
torch.utils.tensorboard._caffe2_graph._tf_device(device_option)
torch.utils.tensorboard._caffe2_graph._try_get_shapes(nets)
torch.utils.tensorboard._caffe2_graph.model_to_graph_def(model,**kwargs)
torch.utils.tensorboard._caffe2_graph.nets_to_graph_def(nets,shapes=None,**kwargs)
torch.utils.tensorboard._caffe2_graph.protos_to_graph_def(net_defs,shapes=None,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_proto_graph.py----------------------------------------
A:torch.utils.tensorboard._proto_graph.attr['attr']->AttrValue(s=s.encode(encoding='utf_8'))
A:torch.utils.tensorboard._proto_graph.shapeproto->tensor_shape_proto(shape)
A:torch.utils.tensorboard._proto_graph.attr['_output_shapes']->AttrValue(list=AttrValue.ListValue(shape=[shapeproto]))
torch.utils.tensorboard._proto_graph.attr_value_proto(dtype,shape,s)
torch.utils.tensorboard._proto_graph.node_proto(name,op='UnSpecified',input=None,dtype=None,shape=None,outputsize=None,attributes='')
torch.utils.tensorboard._proto_graph.tensor_shape_proto(outputsize)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_embedding.py----------------------------------------
A:torch.utils.tensorboard._embedding.metadata_bytes->tensorboard.compat.tf.compat.as_bytes('\n'.join(metadata) + '\n')
A:torch.utils.tensorboard._embedding.fs->tensorboard.compat.tf.io.gfile.get_filesystem(save_path)
A:torch.utils.tensorboard._embedding.nrow->int(math.ceil(label_img.size(0) ** 0.5))
A:torch.utils.tensorboard._embedding.arranged_img_CHW->make_grid(make_np(label_img), ncols=nrow)
A:torch.utils.tensorboard._embedding.arranged_augment_square_HWC->numpy.ndarray((arranged_img_CHW.shape[2], arranged_img_CHW.shape[2], 3))
A:torch.utils.tensorboard._embedding.arranged_img_HWC->make_grid(make_np(label_img), ncols=nrow).transpose(1, 2, 0)
A:torch.utils.tensorboard._embedding.im->PIL.Image.fromarray(np.uint8((arranged_augment_square_HWC * 255).clip(0, 255)))
A:torch.utils.tensorboard._embedding.im_bytes->buf.getvalue()
A:torch.utils.tensorboard._embedding.info->EmbeddingInfo()
A:torch.utils.tensorboard._embedding.info.tensor_name->'{}:{}'.format(tag, str(global_step).zfill(5))
A:torch.utils.tensorboard._embedding.info.tensor_path->filesys.join(subdir, 'tensors.tsv')
A:torch.utils.tensorboard._embedding.info.metadata_path->filesys.join(subdir, 'metadata.tsv')
A:torch.utils.tensorboard._embedding.info.sprite.image_path->filesys.join(subdir, 'sprite.png')
A:torch.utils.tensorboard._embedding.config_path->tensorboard.compat.tf.io.gfile.get_filesystem(save_path).join(save_path, 'projector_config.pbtxt')
torch.utils.tensorboard._embedding.get_embedding_info(metadata,label_img,filesys,subdir,global_step,tag)
torch.utils.tensorboard._embedding.make_mat(matlist,save_path)
torch.utils.tensorboard._embedding.make_sprite(label_img,save_path)
torch.utils.tensorboard._embedding.make_tsv(metadata,save_path,metadata_header=None)
torch.utils.tensorboard._embedding.write_pbtxt(save_path,contents)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_pytorch_graph.py----------------------------------------
A:torch.utils.tensorboard._pytorch_graph.list_of_node->list(getattr(node_cpp, m)())
A:torch.utils.tensorboard._pytorch_graph.tensor_size->node_cpp.type().sizes()
A:torch.utils.tensorboard._pytorch_graph.self.attributes->str({k: node_cpp[k] for k in node_cpp.attributeNames()}).replace("'", ' ')
A:torch.utils.tensorboard._pytorch_graph.self.kind->node_cpp.kind()
A:torch.utils.tensorboard._pytorch_graph.self.nodes_io->OrderedDict()
A:torch.utils.tensorboard._pytorch_graph.self.nodes_io[node_output]->NodeBase(node_output, node.inputs, node.scopeName, outputSize, op_type=node.kind, attributes=node.attributes)
A:torch.utils.tensorboard._pytorch_graph.n_inputs->len(args)
A:torch.utils.tensorboard._pytorch_graph.nodes_py->GraphPy()
A:torch.utils.tensorboard._pytorch_graph.attr_to_scope->dict()
A:torch.utils.tensorboard._pytorch_graph.attr_name->node.s('name')
A:torch.utils.tensorboard._pytorch_graph.parent->node.input().node()
A:torch.utils.tensorboard._pytorch_graph.parent_attr_name->node.input().node().s('name')
A:torch.utils.tensorboard._pytorch_graph.attr_to_scope[attr_name]->'__module.{}'.format(attr_name)
A:torch.utils.tensorboard._pytorch_graph.node_py->NodePyIO(node, 'output')
A:torch.utils.tensorboard._pytorch_graph.node_py.debugName->'output.{}'.format(i + 1)
A:torch.utils.tensorboard._pytorch_graph.module_name->getattr(module, 'original_name', 'Module')
A:torch.utils.tensorboard._pytorch_graph.alias_to_name->dict()
A:torch.utils.tensorboard._pytorch_graph.base_name->parse_traced_name(trace)
A:torch.utils.tensorboard._pytorch_graph.mod_name->parse_traced_name(module)
A:torch.utils.tensorboard._pytorch_graph.alias_to_name[name]->'{}[{}]'.format(mod_name, attr_name)
A:torch.utils.tensorboard._pytorch_graph.module_aliases->node.scopeName.split('/')
A:torch.utils.tensorboard._pytorch_graph.trace->torch.jit.trace(model, args)
A:torch.utils.tensorboard._pytorch_graph.list_of_nodes->parse(graph, trace, args)
A:torch.utils.tensorboard._pytorch_graph.stepstats->RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device='/device:CPU:0')]))
torch.utils.tensorboard._pytorch_graph.GraphPy(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.__init__(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.append(self,x)
torch.utils.tensorboard._pytorch_graph.GraphPy.find_common_root(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.populate_namespace_from_OP_to_IO(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.printall(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.to_proto(self)
torch.utils.tensorboard._pytorch_graph.NodeBase(self,debugName=None,inputs=None,scope=None,tensor_size=None,op_type='UnSpecified',attributes='')
torch.utils.tensorboard._pytorch_graph.NodeBase.__init__(self,debugName=None,inputs=None,scope=None,tensor_size=None,op_type='UnSpecified',attributes='')
torch.utils.tensorboard._pytorch_graph.NodeBase.__repr__(self)
torch.utils.tensorboard._pytorch_graph.NodePy(self,node_cpp,valid_methods)
torch.utils.tensorboard._pytorch_graph.NodePy.__init__(self,node_cpp,valid_methods)
torch.utils.tensorboard._pytorch_graph.NodePyIO(self,node_cpp,input_or_output=None)
torch.utils.tensorboard._pytorch_graph.NodePyIO.__init__(self,node_cpp,input_or_output=None)
torch.utils.tensorboard._pytorch_graph.NodePyOP(self,node_cpp)
torch.utils.tensorboard._pytorch_graph.NodePyOP.__init__(self,node_cpp)
torch.utils.tensorboard._pytorch_graph.graph(model,args,verbose=False)
torch.utils.tensorboard._pytorch_graph.parse(graph,trace,args=None,omit_useless_nodes=True)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/summary.py----------------------------------------
A:torch.utils.tensorboard.summary.font->PIL.ImageFont.load_default()
A:torch.utils.tensorboard.summary.draw->PIL.ImageDraw.Draw(image)
A:torch.utils.tensorboard.summary.(text_width, text_height)->PIL.ImageFont.load_default().getsize(display_str)
A:torch.utils.tensorboard.summary.margin->numpy.ceil(0.05 * text_height)
A:torch.utils.tensorboard.summary.exp->Summary(value=[Summary.Value(tag=EXPERIMENT_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.content->HParamsPluginData(session_end_info=sei, version=PLUGIN_DATA_VERSION)
A:torch.utils.tensorboard.summary.smd->SummaryMetadata(plugin_data=plugin_data)
A:torch.utils.tensorboard.summary.ssi->Summary(value=[Summary.Value(tag=SESSION_START_INFO_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.sei->Summary(value=[Summary.Value(tag=SESSION_END_INFO_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.scalar->float(scalar)
A:torch.utils.tensorboard.summary.hist->make_histogram(values.astype(float), bins, max_bins)
A:torch.utils.tensorboard.summary.values->values.reshape(-1).reshape(-1)
A:torch.utils.tensorboard.summary.(counts, limits)->numpy.histogram(values, bins=bins)
A:torch.utils.tensorboard.summary.num_bins->len(counts)
A:torch.utils.tensorboard.summary.counts->counts.reshape(-1, subsampling).sum(axis=-1).reshape(-1, subsampling).sum(axis=-1)
A:torch.utils.tensorboard.summary.new_limits->numpy.empty((counts.size + 1,), limits.dtype)
A:torch.utils.tensorboard.summary.cum_counts->numpy.cumsum(np.greater(counts, 0, dtype=np.int32))
A:torch.utils.tensorboard.summary.(start, end)->numpy.searchsorted(cum_counts, [0, cum_counts[-1] - 1], side='right')
A:torch.utils.tensorboard.summary.start->int(start)
A:torch.utils.tensorboard.summary.sum_sq->values.reshape(-1).reshape(-1).dot(values)
A:torch.utils.tensorboard.summary.tensor->TensorProto(dtype='DT_FLOAT', float_val=tensor.reshape(-1).tolist(), tensor_shape=TensorShapeProto(dim=[TensorShapeProto.Dim(size=tensor.shape[0]), TensorShapeProto.Dim(size=tensor.shape[1]), TensorShapeProto.Dim(size=tensor.shape[2])]))
A:torch.utils.tensorboard.summary.scale_factor->_calc_scale_factor(tensor)
A:torch.utils.tensorboard.summary.image->image.resize((scaled_width, scaled_height), Image.ANTIALIAS).resize((scaled_width, scaled_height), Image.ANTIALIAS)
A:torch.utils.tensorboard.summary.tensor_image->convert_to_HWC(tensor_image, dataformats)
A:torch.utils.tensorboard.summary.tensor_boxes->make_np(tensor_boxes)
A:torch.utils.tensorboard.summary.list_gt->range(num_boxes)
A:torch.utils.tensorboard.summary.disp_image->_draw_single_box(disp_image, boxes[i, 0], boxes[i, 1], boxes[i, 2], boxes[i, 3], display_str=None, color='Red')
A:torch.utils.tensorboard.summary.scaled_height->int(height * rescale)
A:torch.utils.tensorboard.summary.scaled_width->int(width * rescale)
A:torch.utils.tensorboard.summary.output->io.BytesIO()
A:torch.utils.tensorboard.summary.image_string->io.BytesIO().getvalue()
A:torch.utils.tensorboard.summary.video->make_video(tensor, fps)
A:torch.utils.tensorboard.summary.clip->moviepy.editor.ImageSequenceClip(list(tensor), fps=fps)
A:torch.utils.tensorboard.summary.tensor_string->f.read()
A:torch.utils.tensorboard.summary.fio->io.BytesIO()
A:torch.utils.tensorboard.summary.wave_write->wave.open(fio, 'wb')
A:torch.utils.tensorboard.summary.audio_string->io.BytesIO().getvalue()
A:torch.utils.tensorboard.summary.audio->tensorboard.compat.proto.summary_pb2.Summary.Audio(sample_rate=sample_rate, num_channels=1, length_frames=len(tensor_list), encoded_audio_string=audio_string, content_type='audio/wav')
A:torch.utils.tensorboard.summary.mgcc->tensorboard.plugins.custom_scalar.layout_pb2.MarginChartContent(series=[layout_pb2.MarginChartContent.Series(value=tags[0], lower=tags[1], upper=tags[2])])
A:torch.utils.tensorboard.summary.chart->tensorboard.plugins.custom_scalar.layout_pb2.Chart(title=chart_name, multiline=mlcc)
A:torch.utils.tensorboard.summary.mlcc->tensorboard.plugins.custom_scalar.layout_pb2.MultilineChartContent(tag=tags)
A:torch.utils.tensorboard.summary.layout->tensorboard.plugins.custom_scalar.layout_pb2.Layout(category=categories)
A:torch.utils.tensorboard.summary.plugin_data->tensorboard.compat.proto.summary_pb2.SummaryMetadata.PluginData(plugin_name='pr_curves', content=pr_curve_plugin_data)
A:torch.utils.tensorboard.summary.data->compute_curve(labels, predictions, num_thresholds=num_thresholds, weights=weights)
A:torch.utils.tensorboard.summary.pr_curve_plugin_data->PrCurvePluginData(version=0, num_thresholds=num_thresholds).SerializeToString()
A:torch.utils.tensorboard.summary.num_thresholds->min(num_thresholds, 127)
A:torch.utils.tensorboard.summary.bucket_indices->numpy.int32(np.floor(predictions * (num_thresholds - 1)))
A:torch.utils.tensorboard.summary.float_labels->labels.astype(np.float)
A:torch.utils.tensorboard.summary.(tp_buckets, _)->numpy.histogram(bucket_indices, bins=num_thresholds, range=histogram_range, weights=float_labels * weights)
A:torch.utils.tensorboard.summary.(fp_buckets, _)->numpy.histogram(bucket_indices, bins=num_thresholds, range=histogram_range, weights=(1.0 - float_labels) * weights)
A:torch.utils.tensorboard.summary.tensor_metadata->tensorboard.plugins.mesh.metadata.create_summary_metadata(name, display_name, content_type, components, tensor.shape, description, json_config=json_config)
A:torch.utils.tensorboard.summary.tensor_summary->tensorboard.compat.proto.summary_pb2.Summary.Value(tag=metadata.get_instance_name(name, content_type), tensor=tensor, metadata=tensor_metadata)
A:torch.utils.tensorboard.summary.json_config->_get_json_config(config_dict)
A:torch.utils.tensorboard.summary.components->tensorboard.plugins.mesh.metadata.get_components_bitmask([content_type for (tensor, content_type) in tensors])
torch.utils.tensorboard.summary._calc_scale_factor(tensor)
torch.utils.tensorboard.summary._draw_single_box(image,xmin,ymin,xmax,ymax,display_str,color='black',color_text='black',thickness=2)
torch.utils.tensorboard.summary._get_json_config(config_dict)
torch.utils.tensorboard.summary._get_tensor_summary(name,display_name,description,tensor,content_type,components,json_config)
torch.utils.tensorboard.summary.audio(tag,tensor,sample_rate=44100)
torch.utils.tensorboard.summary.compute_curve(labels,predictions,num_thresholds=None,weights=None)
torch.utils.tensorboard.summary.custom_scalars(layout)
torch.utils.tensorboard.summary.draw_boxes(disp_image,boxes)
torch.utils.tensorboard.summary.histogram(name,values,bins,max_bins=None)
torch.utils.tensorboard.summary.histogram_raw(name,min,max,num,sum,sum_squares,bucket_limits,bucket_counts)
torch.utils.tensorboard.summary.hparams(hparam_dict=None,metric_dict=None)
torch.utils.tensorboard.summary.image(tag,tensor,rescale=1,dataformats='NCHW')
torch.utils.tensorboard.summary.image_boxes(tag,tensor_image,tensor_boxes,rescale=1,dataformats='CHW')
torch.utils.tensorboard.summary.make_histogram(values,bins,max_bins=None)
torch.utils.tensorboard.summary.make_image(tensor,rescale=1,rois=None)
torch.utils.tensorboard.summary.make_video(tensor,fps)
torch.utils.tensorboard.summary.mesh(tag,vertices,colors,faces,config_dict,display_name=None,description=None)
torch.utils.tensorboard.summary.pr_curve(tag,labels,predictions,num_thresholds=127,weights=None)
torch.utils.tensorboard.summary.pr_curve_raw(tag,tp,fp,tn,fn,precision,recall,num_thresholds=127,weights=None)
torch.utils.tensorboard.summary.scalar(name,scalar,collections=None)
torch.utils.tensorboard.summary.text(tag,text)
torch.utils.tensorboard.summary.video(tag,tensor,fps=4)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/tensorboard/_onnx_graph.py----------------------------------------
A:torch.utils.tensorboard._onnx_graph.m->onnx.load(fname)
A:torch.utils.tensorboard._onnx_graph.shapeproto->TensorShapeProto(dim=[TensorShapeProto.Dim(size=d.dim_value) for d in node.type.tensor_type.shape.dim])
A:torch.utils.tensorboard._onnx_graph.attr->', '.join(attr).encode(encoding='utf_8')
torch.utils.tensorboard._onnx_graph.load_onnx_graph(fname)
torch.utils.tensorboard._onnx_graph.parse(graph)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/bottleneck/__main__.py----------------------------------------
A:torch.utils.bottleneck.__main__.env_summary->run_env_analysis()
A:torch.utils.bottleneck.__main__.info->get_env_info()
A:torch.utils.bottleneck.__main__.prof->cProfile.Profile()
A:torch.utils.bottleneck.__main__.cprof_summary->'\n--------------------------------------------------------------------------------\n  cProfile output\n--------------------------------------------------------------------------------\n'.strip()
A:torch.utils.bottleneck.__main__.cprofile_stats->pstats.Stats(prof).sort_stats(sortby)
A:torch.utils.bottleneck.__main__.autograd_prof_summary->'\n--------------------------------------------------------------------------------\n  autograd profiler output ({mode} mode)\n--------------------------------------------------------------------------------\n        {description}\n{cuda_warning}\n{output}\n'.strip()
A:torch.utils.bottleneck.__main__.sorted_events->sorted(prof.function_events, key=lambda x: getattr(x, sortby), reverse=True)
A:torch.utils.bottleneck.__main__.descript->"\n`bottleneck` is a tool that can be used as an initial step for debugging\nbottlenecks in your program.\n\nIt summarizes runs of your script with the Python profiler and PyTorch's\nautograd profiler. Because your script will be profiled, please ensure that it\nexits in a finite amount of time.\n\nFor more complicated uses of the profilers, please see\nhttps://docs.python.org/3/library/profile.html and\nhttps://pytorch.org/docs/master/autograd.html#profiler for more information.\n".strip()
A:torch.utils.bottleneck.__main__.parser->argparse.ArgumentParser(description=descript)
A:torch.utils.bottleneck.__main__.args->parse_args()
A:torch.utils.bottleneck.__main__.code->compile(stream.read(), scriptfile, 'exec')
A:torch.utils.bottleneck.__main__.cprofile_prof->run_cprofile(code, globs)
A:torch.utils.bottleneck.__main__.(autograd_prof_cpu, autograd_prof_cuda)->run_autograd_prof(code, globs)
A:torch.utils.bottleneck.__main__.cuda_prof_exec_time->cpu_time_total(autograd_prof_cuda)
A:torch.utils.bottleneck.__main__.cpu_prof_exec_time->cpu_time_total(autograd_prof_cpu)
torch.utils.bottleneck.__main__.compiled_with_cuda(sysinfo)
torch.utils.bottleneck.__main__.cpu_time_total(autograd_prof)
torch.utils.bottleneck.__main__.main()
torch.utils.bottleneck.__main__.parse_args()
torch.utils.bottleneck.__main__.print_autograd_prof_summary(prof,mode,sortby='cpu_time',topk=15)
torch.utils.bottleneck.__main__.print_cprofile_summary(prof,sortby='tottime',topk=15)
torch.utils.bottleneck.__main__.redirect_argv(new_argv)
torch.utils.bottleneck.__main__.run_autograd_prof(code,globs)
torch.utils.bottleneck.__main__.run_cprofile(code,globs,launch_blocking=False)
torch.utils.bottleneck.__main__.run_env_analysis()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/bottleneck/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/backcompat/__init__.py----------------------------------------
A:torch.utils.backcompat.__init__.enabled->property(get_enabled, set_enabled)
A:torch.utils.backcompat.__init__.broadcast_warning->Warning(_set_backcompat_broadcast_warn, _get_backcompat_broadcast_warn)
A:torch.utils.backcompat.__init__.keepdim_warning->Warning(_set_backcompat_keepdim_warn, _get_backcompat_keepdim_warn)
torch.utils.backcompat.__init__.Warning(self,setter,getter)
torch.utils.backcompat.__init__.Warning.__init__(self,setter,getter)
torch.utils.backcompat.__init__.Warning.get_enabled(self)
torch.utils.backcompat.__init__.Warning.set_enabled(self,value)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/dataset.py----------------------------------------
A:torch.utils.data.dataset.l->len(e)
A:torch.utils.data.dataset.self.datasets->list(datasets)
A:torch.utils.data.dataset.self.cumulative_sizes->self.cumsum(self.datasets)
A:torch.utils.data.dataset.dataset_idx->bisect.bisect_right(self.cumulative_sizes, idx)
A:torch.utils.data.dataset.indices->randperm(sum(lengths)).tolist()
torch.utils.data.ChainDataset(self,datasets)
torch.utils.data.ChainDataset.__iter__(self)
torch.utils.data.ChainDataset.__len__(self)
torch.utils.data.ConcatDataset(self,datasets)
torch.utils.data.ConcatDataset.__getitem__(self,idx)
torch.utils.data.ConcatDataset.__len__(self)
torch.utils.data.ConcatDataset.cummulative_sizes(self)
torch.utils.data.ConcatDataset.cumsum(sequence)
torch.utils.data.Dataset(object)
torch.utils.data.Dataset.__add__(self,other)
torch.utils.data.Dataset.__getitem__(self,index)
torch.utils.data.IterableDataset(Dataset)
torch.utils.data.IterableDataset.__add__(self,other)
torch.utils.data.IterableDataset.__iter__(self)
torch.utils.data.Subset(self,dataset,indices)
torch.utils.data.Subset.__getitem__(self,idx)
torch.utils.data.Subset.__len__(self)
torch.utils.data.TensorDataset(self,*tensors)
torch.utils.data.TensorDataset.__getitem__(self,index)
torch.utils.data.TensorDataset.__len__(self)
torch.utils.data.dataset.ChainDataset(self,datasets)
torch.utils.data.dataset.ChainDataset.__init__(self,datasets)
torch.utils.data.dataset.ChainDataset.__iter__(self)
torch.utils.data.dataset.ChainDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset(self,datasets)
torch.utils.data.dataset.ConcatDataset.__getitem__(self,idx)
torch.utils.data.dataset.ConcatDataset.__init__(self,datasets)
torch.utils.data.dataset.ConcatDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset.cummulative_sizes(self)
torch.utils.data.dataset.ConcatDataset.cumsum(sequence)
torch.utils.data.dataset.Dataset(object)
torch.utils.data.dataset.Dataset.__add__(self,other)
torch.utils.data.dataset.Dataset.__getitem__(self,index)
torch.utils.data.dataset.IterableDataset(Dataset)
torch.utils.data.dataset.IterableDataset.__add__(self,other)
torch.utils.data.dataset.IterableDataset.__iter__(self)
torch.utils.data.dataset.Subset(self,dataset,indices)
torch.utils.data.dataset.Subset.__getitem__(self,idx)
torch.utils.data.dataset.Subset.__init__(self,dataset,indices)
torch.utils.data.dataset.Subset.__len__(self)
torch.utils.data.dataset.TensorDataset(self,*tensors)
torch.utils.data.dataset.TensorDataset.__getitem__(self,index)
torch.utils.data.dataset.TensorDataset.__init__(self,*tensors)
torch.utils.data.dataset.TensorDataset.__len__(self)
torch.utils.data.dataset.random_split(dataset,lengths)
torch.utils.data.random_split(dataset,lengths)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/dataset.pyi----------------------------------------
torch.utils.data.Dataset.__len__(self)->int
torch.utils.data.dataset.Dataset.__len__(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/sampler.py----------------------------------------
A:torch.utils.data.sampler.n->len(self.data_source)
A:torch.utils.data.sampler.self.weights->torch.as_tensor(weights, dtype=torch.double)
torch.utils.data.BatchSampler(self,sampler,batch_size,drop_last)
torch.utils.data.BatchSampler.__iter__(self)
torch.utils.data.BatchSampler.__len__(self)
torch.utils.data.RandomSampler(self,data_source,replacement=False,num_samples=None)
torch.utils.data.RandomSampler.__iter__(self)
torch.utils.data.RandomSampler.__len__(self)
torch.utils.data.RandomSampler.num_samples(self)
torch.utils.data.Sampler(self,data_source)
torch.utils.data.Sampler.__iter__(self)
torch.utils.data.SequentialSampler(self,data_source)
torch.utils.data.SequentialSampler.__iter__(self)
torch.utils.data.SequentialSampler.__len__(self)
torch.utils.data.SubsetRandomSampler(self,indices)
torch.utils.data.SubsetRandomSampler.__iter__(self)
torch.utils.data.SubsetRandomSampler.__len__(self)
torch.utils.data.WeightedRandomSampler(self,weights,num_samples,replacement=True)
torch.utils.data.WeightedRandomSampler.__iter__(self)
torch.utils.data.WeightedRandomSampler.__len__(self)
torch.utils.data.sampler.BatchSampler(self,sampler,batch_size,drop_last)
torch.utils.data.sampler.BatchSampler.__init__(self,sampler,batch_size,drop_last)
torch.utils.data.sampler.BatchSampler.__iter__(self)
torch.utils.data.sampler.BatchSampler.__len__(self)
torch.utils.data.sampler.RandomSampler(self,data_source,replacement=False,num_samples=None)
torch.utils.data.sampler.RandomSampler.__init__(self,data_source,replacement=False,num_samples=None)
torch.utils.data.sampler.RandomSampler.__iter__(self)
torch.utils.data.sampler.RandomSampler.__len__(self)
torch.utils.data.sampler.RandomSampler.num_samples(self)
torch.utils.data.sampler.Sampler(self,data_source)
torch.utils.data.sampler.Sampler.__init__(self,data_source)
torch.utils.data.sampler.Sampler.__iter__(self)
torch.utils.data.sampler.SequentialSampler(self,data_source)
torch.utils.data.sampler.SequentialSampler.__init__(self,data_source)
torch.utils.data.sampler.SequentialSampler.__iter__(self)
torch.utils.data.sampler.SequentialSampler.__len__(self)
torch.utils.data.sampler.SubsetRandomSampler(self,indices)
torch.utils.data.sampler.SubsetRandomSampler.__init__(self,indices)
torch.utils.data.sampler.SubsetRandomSampler.__iter__(self)
torch.utils.data.sampler.SubsetRandomSampler.__len__(self)
torch.utils.data.sampler.WeightedRandomSampler(self,weights,num_samples,replacement=True)
torch.utils.data.sampler.WeightedRandomSampler.__init__(self,weights,num_samples,replacement=True)
torch.utils.data.sampler.WeightedRandomSampler.__iter__(self)
torch.utils.data.sampler.WeightedRandomSampler.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/sampler.pyi----------------------------------------
torch.utils.data.Sampler.__len__(self)->int
torch.utils.data.sampler.Sampler.__len__(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/dataloader.py----------------------------------------
A:torch.utils.data.dataloader.sampler->SequentialSampler(dataset)
A:torch.utils.data.dataloader.batch_sampler->BatchSampler(sampler, batch_size, drop_last)
A:torch.utils.data.dataloader.valid_start_methods->torch.multiprocessing.get_all_start_methods()
A:torch.utils.data.dataloader.multiprocessing_context->torch.multiprocessing.get_context(multiprocessing_context)
A:torch.utils.data.dataloader.lengthself._IterableDataset_len_called->len(self.dataset)
A:torch.utils.data.dataloader.self._sampler_iter->iter(self._index_sampler)
A:torch.utils.data.dataloader.self._base_seed->torch.empty((), dtype=torch.int64).random_().item()
A:torch.utils.data.dataloader.data->self._data_queue.get(timeout=timeout)
A:torch.utils.data.dataloader.warn_msg->'Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} samples have been fetched. '.format(self._dataset, self._IterableDataset_len_called, self._num_yielded)
A:torch.utils.data.dataloader.self._dataset_fetcher->_DatasetKind.create_fetcher(self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)
A:torch.utils.data.dataloader.index->self._next_index()
A:torch.utils.data.dataloader.self._worker_queue_idx_cycle->itertools.cycle(range(self._num_workers))
A:torch.utils.data.dataloader.self._worker_result_queue->torch.multiprocessing.get_context(multiprocessing_context).Queue()
A:torch.utils.data.dataloader.self._workers_done_event->torch.multiprocessing.get_context(multiprocessing_context).Event()
A:torch.utils.data.dataloader.index_queue->torch.multiprocessing.get_context(multiprocessing_context).Queue()
A:torch.utils.data.dataloader.w->torch.multiprocessing.get_context(multiprocessing_context).Process(target=_utils.worker._worker_loop, args=(self._dataset_kind, self._dataset, index_queue, self._worker_result_queue, self._workers_done_event, self._auto_collation, self._collate_fn, self._drop_last, self._base_seed + i, self._worker_init_fn, i, self._num_workers))
A:torch.utils.data.dataloader.self._pin_memory_thread_done_event->threading.Event()
A:torch.utils.data.dataloader.self._data_queue->torch._six.queue.Queue()
A:torch.utils.data.dataloader.pin_memory_thread->threading.Thread(target=_utils.pin_memory._pin_memory_loop, args=(self._worker_result_queue, self._data_queue, torch.cuda.current_device(), self._pin_memory_thread_done_event))
A:torch.utils.data.dataloader.pids_str->', '.join((str(w.pid) for w in failed_workers))
A:torch.utils.data.dataloader.(success, data)->self._try_get_data()
A:torch.utils.data.dataloader.(idx, data)->self._get_data()
A:torch.utils.data.dataloader.worker_queue_idx->next(self._worker_queue_idx_cycle)
torch.utils.data.DataLoader(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)
torch.utils.data.DataLoader.__iter__(self)
torch.utils.data.DataLoader.__len__(self)
torch.utils.data.DataLoader.__setattr__(self,attr,val)
torch.utils.data.DataLoader._auto_collation(self)
torch.utils.data.DataLoader._index_sampler(self)
torch.utils.data.DataLoader.multiprocessing_context(self)
torch.utils.data.DataLoader.multiprocessing_context(self,multiprocessing_context)
torch.utils.data._DatasetKind(object)
torch.utils.data._DatasetKind.create_fetcher(kind,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data.dataloader.DataLoader(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)
torch.utils.data.dataloader.DataLoader.__init__(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)
torch.utils.data.dataloader.DataLoader.__iter__(self)
torch.utils.data.dataloader.DataLoader.__len__(self)
torch.utils.data.dataloader.DataLoader.__setattr__(self,attr,val)
torch.utils.data.dataloader.DataLoader._auto_collation(self)
torch.utils.data.dataloader.DataLoader._index_sampler(self)
torch.utils.data.dataloader.DataLoader.multiprocessing_context(self)
torch.utils.data.dataloader.DataLoader.multiprocessing_context(self,multiprocessing_context)
torch.utils.data.dataloader._BaseDataLoaderIter(self,loader)
torch.utils.data.dataloader._BaseDataLoaderIter.__getstate__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._BaseDataLoaderIter.__iter__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__len__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__next__(self)
torch.utils.data.dataloader._BaseDataLoaderIter._next_data(self)
torch.utils.data.dataloader._BaseDataLoaderIter._next_index(self)
torch.utils.data.dataloader._DatasetKind(object)
torch.utils.data.dataloader._DatasetKind.create_fetcher(kind,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data.dataloader._InfiniteConstantSampler(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__init__(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__iter__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter(self,loader)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__del__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._get_data(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._next_data(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._process_data(self,data)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._shutdown_worker(self,worker_id)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._shutdown_workers(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._try_get_data(self,timeout=_utils.MP_STATUS_CHECK_INTERVAL)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._try_put_index(self)
torch.utils.data.dataloader._SingleProcessDataLoaderIter(self,loader)
torch.utils.data.dataloader._SingleProcessDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._SingleProcessDataLoaderIter._next_data(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/dataloader.pyi----------------------------------------
torch.utils.data.dataloader.default_collate(batch:List[T])->Any


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/distributed.py----------------------------------------
A:torch.utils.data.distributed.num_replicas->torch.distributed.get_world_size()
A:torch.utils.data.distributed.rank->torch.distributed.get_rank()
A:torch.utils.data.distributed.self.num_samples->int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
A:torch.utils.data.distributed.g->torch.Generator()
A:torch.utils.data.distributed.indices->list(range(len(self.dataset)))
torch.utils.data.DistributedSampler(self,dataset,num_replicas=None,rank=None,shuffle=True)
torch.utils.data.DistributedSampler.__iter__(self)
torch.utils.data.DistributedSampler.__len__(self)
torch.utils.data.DistributedSampler.set_epoch(self,epoch)
torch.utils.data.distributed.DistributedSampler(self,dataset,num_replicas=None,rank=None,shuffle=True)
torch.utils.data.distributed.DistributedSampler.__init__(self,dataset,num_replicas=None,rank=None,shuffle=True)
torch.utils.data.distributed.DistributedSampler.__iter__(self)
torch.utils.data.distributed.DistributedSampler.__len__(self)
torch.utils.data.distributed.DistributedSampler.set_epoch(self,epoch)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/distributed.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/_utils/worker.py----------------------------------------
A:torch.utils.data._utils.worker.self.manager_pid->os.getppid()
A:torch.utils.data._utils.worker.self.kernel32->ctypes.WinDLL('kernel32', use_last_error=True)
A:torch.utils.data._utils.worker.self.manager_handle->self.kernel32.OpenProcess(SYNCHRONIZE, 0, self.manager_pid)
A:torch.utils.data._utils.worker._IterableDatasetStopIteration->namedtuple('_IterableDatasetStopIteration', ['worker_id'])
A:torch.utils.data._utils.worker._worker_info->WorkerInfo(id=worker_id, num_workers=num_workers, seed=seed, dataset=dataset)
A:torch.utils.data._utils.worker.fetcher->torch.utils.data._DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)
A:torch.utils.data._utils.worker.init_exception->ExceptionWrapper(where='in DataLoader worker process {}'.format(worker_id))
A:torch.utils.data._utils.worker.watchdog->ManagerWatchdog()
A:torch.utils.data._utils.worker.r->index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data._utils.worker.data->ExceptionWrapper(where='in DataLoader worker process {}'.format(worker_id))
torch.utils.data._utils.worker.WorkerInfo(self,**kwargs)
torch.utils.data._utils.worker.WorkerInfo.__init__(self,**kwargs)
torch.utils.data._utils.worker.WorkerInfo.__setattr__(self,key,val)
torch.utils.data._utils.worker._worker_loop(dataset_kind,dataset,index_queue,data_queue,done_event,auto_collation,collate_fn,drop_last,seed,init_fn,worker_id,num_workers)
torch.utils.data._utils.worker.get_worker_info()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/_utils/pin_memory.py----------------------------------------
A:torch.utils.data._utils.pin_memory.r->in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data._utils.pin_memory.data->ExceptionWrapper(where='in pin memory thread for device {}'.format(device_id))
torch.utils.data._utils.pin_memory._pin_memory_loop(in_queue,out_queue,device_id,done_event)
torch.utils.data._utils.pin_memory.pin_memory(data)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/_utils/collate.py----------------------------------------
A:torch.utils.data._utils.collate.np_str_obj_array_pattern->re.compile('[SaUO]')
A:torch.utils.data._utils.collate.elem_type->type(elem)
A:torch.utils.data._utils.collate.numel->sum([x.numel() for x in batch])
A:torch.utils.data._utils.collate.storage->elem.storage()._new_shared(numel)
A:torch.utils.data._utils.collate.out->elem.new(storage)
A:torch.utils.data._utils.collate.transposed->zip(*batch)
torch.utils.data._utils.collate.default_collate(batch)
torch.utils.data._utils.collate.default_convert(data)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/_utils/signal_handling.py----------------------------------------
A:torch.utils.data._utils.signal_handling.previous_handler->signal.getsignal(signal.SIGCHLD)
torch.utils.data._utils.signal_handling._set_SIGCHLD_handler()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/_utils/__init__.py----------------------------------------
torch.utils.data._utils.__init__._set_python_exit_flag()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/utils/data/_utils/fetch.py----------------------------------------
A:torch.utils.data._utils.fetch.self.dataset_iter->iter(dataset)
A:torch.utils.data._utils.fetch.data->next(self.dataset_iter)
torch.utils.data._utils.fetch._BaseDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._BaseDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._BaseDatasetFetcher.fetch(self,possibly_batched_index)
torch.utils.data._utils.fetch._IterableDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._IterableDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._IterableDatasetFetcher.fetch(self,possibly_batched_index)
torch.utils.data._utils.fetch._MapDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._MapDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._MapDatasetFetcher.fetch(self,possibly_batched_index)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/multiprocessing/pool.py----------------------------------------
A:torch.multiprocessing.pool.self._inqueue->SimpleQueue()
A:torch.multiprocessing.pool.self._outqueue->SimpleQueue()
A:torch.multiprocessing.pool.w->self.Process(target=clean_worker, args=args)
A:torch.multiprocessing.pool.w.name->self.Process(target=clean_worker, args=args).name.replace('Process', 'PoolWorker')
torch.multiprocessing.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.Pool._repopulate_pool(self)
torch.multiprocessing.Pool._setup_queues(self)
torch.multiprocessing.pool.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.pool.Pool._repopulate_pool(self)
torch.multiprocessing.pool.Pool._setup_queues(self)
torch.multiprocessing.pool.clean_worker(*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/multiprocessing/queue.py----------------------------------------
A:torch.multiprocessing.queue.buf->self.recv_bytes()
A:torch.multiprocessing.queue.self._reader->ConnectionWrapper(self._reader)
A:torch.multiprocessing.queue.self._writer->ConnectionWrapper(self._writer)
torch.multiprocessing.Queue(self,*args,**kwargs)
torch.multiprocessing.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.SimpleQueue._make_methods(self)
torch.multiprocessing.queue.ConnectionWrapper(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.__getattr__(self,name)
torch.multiprocessing.queue.ConnectionWrapper.__init__(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.recv(self)
torch.multiprocessing.queue.ConnectionWrapper.send(self,obj)
torch.multiprocessing.queue.Queue(self,*args,**kwargs)
torch.multiprocessing.queue.Queue.__init__(self,*args,**kwargs)
torch.multiprocessing.queue.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.queue.SimpleQueue._make_methods(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/multiprocessing/_atfork.py----------------------------------------
torch.multiprocessing._atfork.register_after_fork(func)
torch.register_after_fork(func)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/multiprocessing/reductions.py----------------------------------------
A:torch.multiprocessing.reductions.self.cdata->cls._new_shared_filename(manager, handle, size)._weak_ref()
A:torch.multiprocessing.reductions.self.lock->threading.Lock()
A:torch.multiprocessing.reductions.self.limit->max(128, live * 2)
A:torch.multiprocessing.reductions.shared_cache->SharedCache()
A:torch.multiprocessing.reductions.handle->event.ipc_handle()
A:torch.multiprocessing.reductions.t->torch.nn.parameter.Parameter(t)
A:torch.multiprocessing.reductions.storage->cls._new_shared_filename(manager, handle, size)
A:torch.multiprocessing.reductions.shared_cache[storage_handle, storage_offset_bytes]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.(device, handle, storage_size_bytes, storage_offset_bytes, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required)->cls._new_shared_filename(manager, handle, size)._share_cuda_()
A:torch.multiprocessing.reductions.tensor_offset->tensor.storage_offset()
A:torch.multiprocessing.reductions.shared_cache[handle]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.stat->os.fstat(fd)
A:torch.multiprocessing.reductions.storage_ref->SharedCache().get(key)
A:torch.multiprocessing.reductions.fd->multiprocessing.reduction.DupFd(fd).detach()
A:torch.multiprocessing.reductions.shared_cache[fd_id(fd)]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.metadata->cls._new_shared_filename(manager, handle, size)._share_filename_()
A:torch.multiprocessing.reductions.(fd, size)->cls._new_shared_filename(manager, handle, size)._share_fd_()
A:torch.multiprocessing.reductions.df->multiprocessing.reduction.DupFd(fd)
A:torch.multiprocessing.reductions.cache_key->fd_id(fd)
A:torch.multiprocessing.reductions.shared_cache[cache_key]->StorageWeakRef(storage)
torch.multiprocessing.init_reductions()
torch.multiprocessing.reductions.SharedCache(self)
torch.multiprocessing.reductions.SharedCache.__init__(self)
torch.multiprocessing.reductions.SharedCache.__setitem__(self,key,storage_ref)
torch.multiprocessing.reductions.SharedCache._after_fork(self)
torch.multiprocessing.reductions.SharedCache.free_dead_references(self)
torch.multiprocessing.reductions.StorageWeakRef(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.__del__(self)
torch.multiprocessing.reductions.StorageWeakRef.__init__(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.expired(self)
torch.multiprocessing.reductions.fd_id(fd)
torch.multiprocessing.reductions.init_reductions()
torch.multiprocessing.reductions.rebuild_cuda_tensor(tensor_cls,tensor_size,tensor_stride,tensor_offset,storage_cls,storage_device,storage_handle,storage_size_bytes,storage_offset_bytes,requires_grad,ref_counter_handle,ref_counter_offset,event_handle,event_sync_required)
torch.multiprocessing.reductions.rebuild_event(device,handle)
torch.multiprocessing.reductions.rebuild_storage_empty(cls)
torch.multiprocessing.reductions.rebuild_storage_fd(cls,df,size)
torch.multiprocessing.reductions.rebuild_storage_filename(cls,manager,handle,size)
torch.multiprocessing.reductions.rebuild_tensor(cls,storage,metadata)
torch.multiprocessing.reductions.reduce_event(event)
torch.multiprocessing.reductions.reduce_storage(storage)
torch.multiprocessing.reductions.reduce_tensor(tensor)
torch.multiprocessing.reductions.storage_from_cache(cls,key)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/multiprocessing/__init__.py----------------------------------------
torch.multiprocessing.__init__.get_all_sharing_strategies()
torch.multiprocessing.__init__.get_sharing_strategy()
torch.multiprocessing.__init__.set_sharing_strategy(new_strategy)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/multiprocessing/spawn.py----------------------------------------
A:torch.multiprocessing.spawn.ready->multiprocessing.connection.wait(self.sentinels.keys(), timeout=timeout)
A:torch.multiprocessing.spawn.index->self.sentinels.pop(sentinel)
A:torch.multiprocessing.spawn.original_trace->self.error_queues[error_index].get()
A:torch.multiprocessing.spawn.mp->multiprocessing.get_context(start_method)
A:torch.multiprocessing.spawn.error_queue->multiprocessing.get_context(start_method).SimpleQueue()
A:torch.multiprocessing.spawn.process->multiprocessing.get_context(start_method).Process(target=_wrap, args=(fn, i, args, error_queue), daemon=daemon)
A:torch.multiprocessing.spawn.context->ProcessContext(processes, error_queues)
torch.multiprocessing.ProcessContext(self,processes,error_queues)
torch.multiprocessing.ProcessContext.join(self,timeout=None)
torch.multiprocessing.ProcessContext.pids(self)
torch.multiprocessing.SpawnContext(self,processes,error_queues)
torch.multiprocessing.spawn(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')
torch.multiprocessing.spawn.ProcessContext(self,processes,error_queues)
torch.multiprocessing.spawn.ProcessContext.__init__(self,processes,error_queues)
torch.multiprocessing.spawn.ProcessContext.join(self,timeout=None)
torch.multiprocessing.spawn.ProcessContext.pids(self)
torch.multiprocessing.spawn.SpawnContext(self,processes,error_queues)
torch.multiprocessing.spawn.SpawnContext.__init__(self,processes,error_queues)
torch.multiprocessing.spawn._python_version_check()
torch.multiprocessing.spawn._wrap(fn,i,args,error_queue)
torch.multiprocessing.spawn.spawn(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')
torch.multiprocessing.spawn.start_processes(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')
torch.multiprocessing.start_processes(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/__init__.py----------------------------------------
torch.backends.__init__.ContextProp(self,getter,setter)
torch.backends.__init__.ContextProp.__get__(self,obj,objtype)
torch.backends.__init__.ContextProp.__init__(self,getter,setter)
torch.backends.__init__.ContextProp.__set__(self,obj,val)
torch.backends.__init__.PropModule(self,m,name)
torch.backends.__init__.PropModule.__getattr__(self,attr)
torch.backends.__init__.PropModule.__init__(self,m,name)
torch.backends.__init__.__allow_nonbracketed_mutation()
torch.backends.__init__.disable_global_flags()
torch.backends.__init__.flags_frozen()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/cuda/__init__.py----------------------------------------
A:torch.backends.cuda.__init__.size->cuFFTPlanCacheAttrContextProp(torch._cufft_get_plan_cache_size, '.size is a read-only property showing the number of plans currently in the cache. To change the cache capacity, set cufft_plan_cache.max_size.')
A:torch.backends.cuda.__init__.max_size->cuFFTPlanCacheAttrContextProp(torch._cufft_get_plan_cache_max_size, torch._cufft_set_plan_cache_max_size)
A:torch.backends.cuda.__init__.index->torch.cuda._utils._get_device_index(device)
A:torch.backends.cuda.__init__.cufft_plan_cache->cuFFTPlanCacheManager()
A:torch.backends.cuda.__init__.sys.modules[__name__]->CUDAModule(sys.modules[__name__])
torch.backends.cuda.__init__.CUDAModule(self,m)
torch.backends.cuda.__init__.CUDAModule.__init__(self,m)
torch.backends.cuda.__init__.cuFFTPlanCache(self,device_index)
torch.backends.cuda.__init__.cuFFTPlanCache.__init__(self,device_index)
torch.backends.cuda.__init__.cuFFTPlanCache.clear(self)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp(self,getter,setter)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__get__(self,obj,objtype)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__init__(self,getter,setter)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__set__(self,obj,val)
torch.backends.cuda.__init__.cuFFTPlanCacheManager(self)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__getattr__(self,name)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__getitem__(self,device)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__init__(self)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__setattr__(self,name,value)
torch.backends.cuda.__init__.is_built()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/cudnn/__init__.py----------------------------------------
A:torch.backends.cudnn.__init__.__cudnn_version->torch._C._cudnn.getVersionInt()
A:torch.backends.cudnn.__init__.runtime_version->torch._C._cudnn.getRuntimeVersion()
A:torch.backends.cudnn.__init__.compile_version->torch._C._cudnn.getCompileVersion()
A:torch.backends.cudnn.__init__.orig_flags->set_flags(enabled, benchmark, deterministic, verbose)
A:torch.backends.cudnn.__init__.enabled->ContextProp(torch._C._get_cudnn_enabled, torch._C._set_cudnn_enabled)
A:torch.backends.cudnn.__init__.deterministic->ContextProp(torch._C._get_cudnn_deterministic, torch._C._set_cudnn_deterministic)
A:torch.backends.cudnn.__init__.benchmark->ContextProp(torch._C._get_cudnn_benchmark, torch._C._set_cudnn_benchmark)
A:torch.backends.cudnn.__init__.sys.modules[__name__]->CudnnModule(sys.modules[__name__], __name__)
torch.backends.cudnn.__init__.CudnnModule(self,m,name)
torch.backends.cudnn.__init__.CudnnModule.__init__(self,m,name)
torch.backends.cudnn.__init__.flags(enabled=False,benchmark=False,deterministic=False,verbose=False)
torch.backends.cudnn.__init__.is_acceptable(tensor)
torch.backends.cudnn.__init__.is_available()
torch.backends.cudnn.__init__.set_flags(_enabled,_benchmark,_deterministic,_verbose)
torch.backends.cudnn.__init__.version()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/cudnn/rnn.py----------------------------------------
A:torch.backends.cudnn.rnn.dropout_state[dropout_desc_name]->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda')))
A:torch.backends.cudnn.rnn.dropout_ts->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda'))).get()
torch.backends.cudnn.rnn.Unserializable(self,inner)
torch.backends.cudnn.rnn.Unserializable.__getstate__(self)
torch.backends.cudnn.rnn.Unserializable.__init__(self,inner)
torch.backends.cudnn.rnn.Unserializable.__setstate__(self,state)
torch.backends.cudnn.rnn.Unserializable.get(self)
torch.backends.cudnn.rnn.get_cudnn_mode(mode)
torch.backends.cudnn.rnn.init_dropout_state(dropout,train,dropout_seed,dropout_state)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/openmp/__init__.py----------------------------------------
torch.backends.openmp.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/mkl/__init__.py----------------------------------------
torch.backends.mkl.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/mkldnn/__init__.py----------------------------------------
A:torch.backends.mkldnn.__init__.orig_flags->set_flags(enabled)
A:torch.backends.mkldnn.__init__.enabled->ContextProp(torch._C._get_mkldnn_enabled, torch._C._set_mkldnn_enabled)
A:torch.backends.mkldnn.__init__.sys.modules[__name__]->MkldnnModule(sys.modules[__name__], __name__)
torch.backends.mkldnn.__init__.MkldnnModule(self,m,name)
torch.backends.mkldnn.__init__.MkldnnModule.__init__(self,m,name)
torch.backends.mkldnn.__init__.flags(enabled=False)
torch.backends.mkldnn.__init__.is_available()
torch.backends.mkldnn.__init__.set_flags(_enabled)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/xnnpack/__init__.py----------------------------------------
A:torch.backends.xnnpack.__init__.enabled->_XNNPACKEnabled()
A:torch.backends.xnnpack.__init__.sys.modules[__name__]->XNNPACKEngine(sys.modules[__name__], __name__)
torch.backends.xnnpack.__init__.XNNPACKEngine(self,m,name)
torch.backends.xnnpack.__init__.XNNPACKEngine.__getattr__(self,attr)
torch.backends.xnnpack.__init__.XNNPACKEngine.__init__(self,m,name)
torch.backends.xnnpack.__init__._XNNPACKEnabled(object)
torch.backends.xnnpack.__init__._XNNPACKEnabled.__get__(self,obj,objtype)
torch.backends.xnnpack.__init__._XNNPACKEnabled.__set__(self,obj,val)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/backends/quantized/__init__.py----------------------------------------
A:torch.backends.quantized.__init__.qengines->torch._C._supported_qengines()
A:torch.backends.quantized.__init__.engine->_QEngineProp()
A:torch.backends.quantized.__init__.supported_engines->_SupportedQEnginesProp()
A:torch.backends.quantized.__init__.sys.modules[__name__]->QuantizedEngine(sys.modules[__name__], __name__)
torch.backends.quantized.__init__.QuantizedEngine(self,m,name)
torch.backends.quantized.__init__.QuantizedEngine.__getattr__(self,attr)
torch.backends.quantized.__init__.QuantizedEngine.__init__(self,m,name)
torch.backends.quantized.__init__._QEngineProp(object)
torch.backends.quantized.__init__._QEngineProp.__get__(self,obj,objtype)
torch.backends.quantized.__init__._QEngineProp.__set__(self,obj,val)
torch.backends.quantized.__init__._SupportedQEnginesProp(object)
torch.backends.quantized.__init__._SupportedQEnginesProp.__get__(self,obj,objtype)
torch.backends.quantized.__init__._SupportedQEnginesProp.__set__(self,obj,val)
torch.backends.quantized.__init__._get_qengine_id(qengine)
torch.backends.quantized.__init__._get_qengine_str(qengine)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/for_onnx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/sparse/__init__.py----------------------------------------
torch.sparse.__init__.addmm(mat,mat1,mat2,beta=1,alpha=1)
torch.sparse.__init__.mm(mat1,mat2)
torch.sparse.__init__.sum(input,dim=None,dtype=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/anomaly_mode.py----------------------------------------
A:torch.autograd.anomaly_mode.self.prev->torch.is_anomaly_enabled()
torch.autograd.anomaly_mode.detect_anomaly(self)
torch.autograd.anomaly_mode.detect_anomaly.__enter__(self)
torch.autograd.anomaly_mode.detect_anomaly.__exit__(self,*args)
torch.autograd.anomaly_mode.detect_anomaly.__init__(self)
torch.autograd.anomaly_mode.set_detect_anomaly(self,mode)
torch.autograd.anomaly_mode.set_detect_anomaly.__enter__(self)
torch.autograd.anomaly_mode.set_detect_anomaly.__exit__(self,*args)
torch.autograd.anomaly_mode.set_detect_anomaly.__init__(self,mode)
torch.autograd.detect_anomaly(self)
torch.autograd.detect_anomaly.__enter__(self)
torch.autograd.detect_anomaly.__exit__(self,*args)
torch.autograd.set_detect_anomaly(self,mode)
torch.autograd.set_detect_anomaly.__enter__(self)
torch.autograd.set_detect_anomaly.__exit__(self,*args)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/profiler.py----------------------------------------
A:torch.autograd.profiler.use_cuda->kwargs.pop('use_cuda', True)
A:torch.autograd.profiler.events->EventList(sorted(events, key=lambda evt: getattr(evt, sort_by), reverse=True), use_cuda=use_cuda)
A:torch.autograd.profiler.threads->itertools.groupby(events, key=attrgetter('thread'))
A:torch.autograd.profiler.thread_events->sorted(thread_events, key=lambda event: [event.cpu_interval.start, -event.cpu_interval.end])
A:torch.autograd.profiler.stats->defaultdict(FunctionEventAvg)
A:torch.autograd.profiler.total_stat->FunctionEventAvg()
A:torch.autograd.profiler.records->torch.autograd._disable_profiler()
A:torch.autograd.profiler.self.function_events->EventList(parse_cpu_trace(records), use_cuda=self.use_cuda)
A:torch.autograd.profiler.self.handle->torch.ops.profiler._record_function_enter(self.name)
A:torch.autograd.profiler.cpu_time_str->attr_formatter('cpu_time')
A:torch.autograd.profiler.cuda_time_str->attr_formatter('cuda_time')
A:torch.autograd.profiler.cpu_time_total_str->attr_formatter('cpu_time_total')
A:torch.autograd.profiler.cuda_time_total_str->attr_formatter('cuda_time_total')
A:torch.autograd.profiler.self_cpu_time_total_str->attr_formatter('self_cpu_time_total')
A:torch.autograd.profiler.Kernel->namedtuple('Kernel', ['name', 'device', 'interval'])
A:torch.autograd.profiler.self.cpu_interval->Interval(cpu_start, cpu_end)
A:torch.autograd.profiler.self[key]->torch._C._demangle(key)
A:torch.autograd.profiler.string_table->StringTable()
A:torch.autograd.profiler.(function_id, start)->record_stack.pop()
A:torch.autograd.profiler.fe->FunctionEvent(id=function_id, name=string_table[start.name()], thread=start.thread_id(), cpu_start=start_record.cpu_elapsed_us(start), cpu_end=start_record.cpu_elapsed_us(record), input_shapes=start.shapes())
A:torch.autograd.profiler.cuda_start->adjusted_time(start)
A:torch.autograd.profiler.cuda_end->adjusted_time(record)
A:torch.autograd.profiler.self.seen->set()
A:torch.autograd.profiler.conn->sqlite3.connect(path)
A:torch.autograd.profiler.strings[r['id']]->torch._C._demangle(r['value'])
A:torch.autograd.profiler.unique->EnforceUnique()
A:torch.autograd.profiler.evt->FunctionEvent(id=row['marker_id'], name=strings[row['name']], cpu_start=row['start_time'], cpu_end=row['end_time'], thread=0)
A:torch.autograd.profiler.has_input_shapes->any([event.input_shapes is not None for event in events])
A:torch.autograd.profiler.self_cpu_time_total->sum([event.self_cpu_time_total for event in events])
A:torch.autograd.profiler.cuda_time_total->sum([evt.cuda_time_total for evt in events])
torch.autograd.profiler.EnforceUnique(self)
torch.autograd.profiler.EnforceUnique.__init__(self)
torch.autograd.profiler.EnforceUnique.see(self,*key)
torch.autograd.profiler.EventList(self,*args,**kwargs)
torch.autograd.profiler.EventList.__init__(self,*args,**kwargs)
torch.autograd.profiler.EventList.__str__(self)
torch.autograd.profiler.EventList.cpu_children_populated(self)
torch.autograd.profiler.EventList.export_chrome_trace(self,path)
torch.autograd.profiler.EventList.key_averages(self,group_by_input_shapes=False)
torch.autograd.profiler.EventList.populate_cpu_children(self)
torch.autograd.profiler.EventList.self_cpu_time_total(self)
torch.autograd.profiler.EventList.table(self,sort_by=None,row_limit=100,header=None)
torch.autograd.profiler.EventList.total_average(self)
torch.autograd.profiler.FormattedTimesMixin(object)
torch.autograd.profiler.FormattedTimesMixin.cpu_time(self)
torch.autograd.profiler.FormattedTimesMixin.cuda_time(self)
torch.autograd.profiler.FunctionEvent(self,id,name,thread,cpu_start,cpu_end,input_shapes=None)
torch.autograd.profiler.FunctionEvent.__init__(self,id,name,thread,cpu_start,cpu_end,input_shapes=None)
torch.autograd.profiler.FunctionEvent.__repr__(self)
torch.autograd.profiler.FunctionEvent.append_cpu_child(self,child)
torch.autograd.profiler.FunctionEvent.append_kernel(self,name,device,start,end)
torch.autograd.profiler.FunctionEvent.cpu_time_total(self)
torch.autograd.profiler.FunctionEvent.cuda_time_total(self)
torch.autograd.profiler.FunctionEvent.key(self)
torch.autograd.profiler.FunctionEvent.self_cpu_time_total(self)
torch.autograd.profiler.FunctionEventAvg(self)
torch.autograd.profiler.FunctionEventAvg.__iadd__(self,other)
torch.autograd.profiler.FunctionEventAvg.__init__(self)
torch.autograd.profiler.FunctionEventAvg.__repr__(self)
torch.autograd.profiler.FunctionEventAvg.add(self,other,group_by_input_shapes=False)
torch.autograd.profiler.Interval(self,start,end)
torch.autograd.profiler.Interval.__init__(self,start,end)
torch.autograd.profiler.Interval.elapsed_us(self)
torch.autograd.profiler.StringTable(defaultdict)
torch.autograd.profiler.StringTable.__missing__(self,key)
torch.autograd.profiler.attr_formatter(name)
torch.autograd.profiler.build_table(events,sort_by=None,header=None,row_limit=100,use_cuda=True)
torch.autograd.profiler.emit_nvtx(self,enabled=True,record_shapes=False)
torch.autograd.profiler.emit_nvtx.__enter__(self)
torch.autograd.profiler.emit_nvtx.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.emit_nvtx.__init__(self,enabled=True,record_shapes=False)
torch.autograd.profiler.format_time(time_us)
torch.autograd.profiler.format_time_share(time_us,total_time_us)
torch.autograd.profiler.load_nvprof(path)
torch.autograd.profiler.parse_cpu_trace(thread_records)
torch.autograd.profiler.parse_nvprof_trace(path)
torch.autograd.profiler.profile(self,enabled=True,use_cuda=False,record_shapes=False)
torch.autograd.profiler.profile.__enter__(self)
torch.autograd.profiler.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.profile.__init__(self,enabled=True,use_cuda=False,record_shapes=False)
torch.autograd.profiler.profile.__repr__(self)
torch.autograd.profiler.profile.__str__(self)
torch.autograd.profiler.profile._check_finish(self)
torch.autograd.profiler.profile.export_chrome_trace(self,path)
torch.autograd.profiler.profile.key_averages(self,group_by_input_shape=False)
torch.autograd.profiler.profile.self_cpu_time_total(self)
torch.autograd.profiler.profile.table(self,sort_by=None,row_limit=100,header=None)
torch.autograd.profiler.profile.total_average(self)
torch.autograd.profiler.record_function(self,name)
torch.autograd.profiler.record_function.__enter__(self)
torch.autograd.profiler.record_function.__exit__(self,*args)
torch.autograd.profiler.record_function.__init__(self,name)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/__init__.py----------------------------------------
A:torch.autograd.__init__.grad_tensors->_make_grads(tensors, grad_tensors)
A:torch.autograd.__init__.grad_outputs->_make_grads(outputs, grad_outputs)
torch.autograd.__init__._is_checkpoint_valid()
torch.autograd.__init__._make_grads(outputs,grads)
torch.autograd.__init__.backward(tensors,grad_tensors=None,retain_graph=None,create_graph=False,grad_variables=None)
torch.autograd.__init__.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)
torch.autograd.__init__.variable(*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/__init__.pyi----------------------------------------
torch.autograd.__init__.Function
torch.autograd.__init__.Function.backward(ctx:Any,*grad_outputs:Any)->Any
torch.autograd.__init__.Function.forward(ctx:Any,*args:Any,**kwargs:Any)->Any
torch.autograd.__init__.NestedIOFunction(Function)
torch.autograd.__init__.NestedIOFunction.backward(self,*gradients:Any)->Any
torch.autograd.__init__.NestedIOFunction.backward_extended(self,*grad_output:Any)->None
torch.autograd.__init__.NestedIOFunction.forward(self,*args:Any)->tuple
torch.autograd.__init__.NestedIOFunction.forward_extended(self,*input:Any)->None
torch.autograd.__init__.NestedIOFunction.mark_dirty(self,*args:Any,**kwargs:Any)->None
torch.autograd.__init__.NestedIOFunction.mark_non_differentiable(self,*args:Any,**kwargs:Any)->None
torch.autograd.__init__.NestedIOFunction.save_for_backward(self,*args:Any)->None
torch.autograd.__init__.Variable(tensor:Tensor,requires_grad:bool=...)->Tensor
torch.autograd.__init__.detect_anomaly
torch.autograd.__init__.detect_anomaly.__enter__(self)->None
torch.autograd.__init__.detect_anomaly.__exit__(self,*args:Any)->bool
torch.autograd.__init__.gradcheck(func:Callable[...,Union[Tensor,Tuple[Tensor,...]]],inputs:Union[Tensor,Tuple[Tensor,...]],eps:float=...,atol:float=...,rtol:float=...,raise_exception:bool=...,check_sparse_nnz:bool=...)->bool
torch.autograd.__init__.gradgradcheck(func:Callable[...,Union[Tensor,Tuple[Tensor,...]]],inputs:Union[Tensor,Tuple[Tensor,...]],eps:float=...,atol:float=...,rtol:float=...,gen_non_contig_grad_outputs:bool=...,raise_exception:bool=...)->bool
torch.autograd.__init__.set_detect_anomaly(self,mode:bool)
torch.autograd.__init__.set_detect_anomaly.__enter__(self)->None
torch.autograd.__init__.set_detect_anomaly.__exit__(self,*args:Any)->bool
torch.autograd.__init__.set_detect_anomaly.__init__(self,mode:bool)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/variable.py----------------------------------------
A:torch.autograd.variable.Variable._execution_engine->ImperativeEngine()
torch.autograd.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.VariableMeta(type)
torch.autograd.VariableMeta.__instancecheck__(cls,other)
torch.autograd.variable.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.variable.VariableMeta(type)
torch.autograd.variable.VariableMeta.__instancecheck__(cls,other)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/grad_mode.py----------------------------------------
A:torch.autograd.grad_mode.gen->func(*args, **kwargs)
A:torch.autograd.grad_mode.x->next(gen)
A:torch.autograd.grad_mode.self.prev->torch.is_grad_enabled()
torch.autograd.grad_mode._DecoratorContextManager(self,func)
torch.autograd.grad_mode._DecoratorContextManager.__call__(self,func)
torch.autograd.grad_mode._DecoratorContextManager._wrap_generator(self,func)
torch.autograd.grad_mode.enable_grad(_DecoratorContextManager)
torch.autograd.grad_mode.enable_grad.__enter__(self)
torch.autograd.grad_mode.enable_grad.__exit__(self,*args)
torch.autograd.grad_mode.no_grad(_DecoratorContextManager)
torch.autograd.grad_mode.no_grad.__enter__(self)
torch.autograd.grad_mode.no_grad.__exit__(self,*args)
torch.autograd.grad_mode.set_grad_enabled(self,mode)
torch.autograd.grad_mode.set_grad_enabled.__enter__(self)
torch.autograd.grad_mode.set_grad_enabled.__exit__(self,*args)
torch.autograd.grad_mode.set_grad_enabled.__init__(self,mode)
torch.enable_grad(_DecoratorContextManager)
torch.enable_grad.__enter__(self)
torch.enable_grad.__exit__(self,*args)
torch.no_grad(_DecoratorContextManager)
torch.no_grad.__enter__(self)
torch.no_grad.__exit__(self,*args)
torch.set_grad_enabled(self,mode)
torch.set_grad_enabled.__enter__(self)
torch.set_grad_enabled.__exit__(self,*args)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/grad_mode.pyi----------------------------------------
torch.autograd.grad_mode.enable_grad.__call__(self,func:T)
torch.autograd.grad_mode.no_grad.__call__(self,func:T)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/functional.py----------------------------------------
A:torch.autograd.functional.res->jacobian(jac_func, inputs, create_graph=create_graph, strict=strict)
A:torch.autograd.functional.prepend->'Entry {} in '.format(idx)
A:torch.autograd.functional.new_outputs->tuple()
A:torch.autograd.functional.new_grad_outputs->tuple()
A:torch.autograd.functional.grads_i->torch.zeros_like(refs[i])
A:torch.autograd.functional.(is_inputs_tuple, inputs)->_as_tuple(inputs, 'inputs', 'hvp')
A:torch.autograd.functional.inputs->_grad_preprocess(inputs, create_graph=create_graph, need_graph=True)
A:torch.autograd.functional.outputs->_grad_postprocess(outputs, create_graph)
A:torch.autograd.functional.(is_outputs_tuple, outputs)->_as_tuple(outputs, 'outputs of the user-provided function', 'hvp')
A:torch.autograd.functional.(_, v)->_as_tuple(v, 'v', 'hvp')
A:torch.autograd.functional.v->_grad_preprocess(v, create_graph=create_graph, need_graph=False)
A:torch.autograd.functional.grad_res->_autograd_grad(double_back, grad_jac, v, create_graph=create_graph)
A:torch.autograd.functional.vjp->_grad_postprocess(vjp, create_graph)
A:torch.autograd.functional.grad_outputs->tuple((torch.zeros_like(out, requires_grad=True) for out in outputs))
A:torch.autograd.functional.grad_inputs->_autograd_grad(outputs, inputs, grad_outputs, create_graph=True)
A:torch.autograd.functional.jvp->_grad_postprocess(jvp, create_graph)
A:torch.autograd.functional.jacobian->_grad_postprocess(jacobian, create_graph)
A:torch.autograd.functional.jac_i->tuple(([] for _ in range(len(inputs))))
A:torch.autograd.functional.vj->_autograd_grad((out.reshape(-1)[j],), inputs, retain_graph=True, create_graph=create_graph)
A:torch.autograd.functional.out->func(*inp)
A:torch.autograd.functional.(is_out_tuple, t_out)->_as_tuple(out, 'outputs of the user-provided function', 'hessian')
A:torch.autograd.functional.jac->_autograd_grad(outputs, inputs, create_graph=True)
A:torch.autograd.functional.vhp->_grad_postprocess(vhp, create_graph)
A:torch.autograd.functional.grad_jac->tuple((torch.zeros_like(inp, requires_grad=True) for inp in inputs))
A:torch.autograd.functional.double_back->_autograd_grad(jac, inputs, grad_jac, create_graph=True)
A:torch.autograd.functional.hvp->_grad_postprocess(hvp, create_graph)
torch.autograd._as_tuple(inp,arg_name,fn_name)
torch.autograd._autograd_grad(outputs,inputs,grad_outputs=None,create_graph=False,retain_graph=None)
torch.autograd._check_requires_grad(inputs,input_type,strict)
torch.autograd._fill_in_zeros(grads,refs,strict,create_graph,stage)
torch.autograd._grad_postprocess(inputs,create_graph)
torch.autograd._grad_preprocess(inputs,create_graph,need_graph)
torch.autograd._tuple_postprocess(res,to_unpack)
torch.autograd._validate_v(v,other,is_other_tuple)
torch.autograd.functional._as_tuple(inp,arg_name,fn_name)
torch.autograd.functional._autograd_grad(outputs,inputs,grad_outputs=None,create_graph=False,retain_graph=None)
torch.autograd.functional._check_requires_grad(inputs,input_type,strict)
torch.autograd.functional._fill_in_zeros(grads,refs,strict,create_graph,stage)
torch.autograd.functional._grad_postprocess(inputs,create_graph)
torch.autograd.functional._grad_preprocess(inputs,create_graph,need_graph)
torch.autograd.functional._tuple_postprocess(res,to_unpack)
torch.autograd.functional._validate_v(v,other,is_other_tuple)
torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)
torch.autograd.functional.hvp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.functional.jacobian(func,inputs,create_graph=False,strict=False)
torch.autograd.functional.jvp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.functional.vhp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.functional.vjp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.hessian(func,inputs,create_graph=False,strict=False)
torch.autograd.hvp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.jacobian(func,inputs,create_graph=False,strict=False)
torch.autograd.jvp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.vhp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.vjp(func,inputs,v=None,create_graph=False,strict=False)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/function.py----------------------------------------
A:torch.autograd.function.backward_hooks->OrderedDict()
A:torch.autograd.function.handle->torch.utils.hooks.RemovableHandle(backward_hooks)
A:torch.autograd.function.forward->super_cls.__dict__.get('forward')
A:torch.autograd.function.backward_fn->type(name + 'Backward', (BackwardCFunction,), {'_forward_cls': cls})
A:torch.autograd.function.outputs->fn(ctx, *args)
A:torch.autograd.function.requires_grad->any((isinstance(arg, torch.Tensor) and arg.requires_grad for arg in args))
A:torch.autograd.function.err_fn->torch._C._functions.DelayedError(b'trying to differentiate twice a function that was markedwith @once_differentiable', len(outputs))
A:torch.autograd.function.var->var.detach().detach()
A:torch.autograd.function.obj->conversion(obj)
A:torch.autograd.function.(res_e, input)->unflatten_helper(input, e)
A:torch.autograd.function._iter_jit_values->_iter_filter(lambda o: o is None or isinstance(o, torch._C.Value), condition_msg="jit's Values or None")
A:torch.autograd.function._iter_tensors->_iter_filter(lambda x: isinstance(x, torch.Tensor), condition_msg='Tensors', conversion=_jit_unwrap_structured)
A:torch.autograd.function._iter_tensors_permissive->_iter_filter(lambda x: isinstance(x, torch.Tensor), allow_unknown=True, condition_msg='Tensors (permissive)')
A:torch.autograd.function._iter_None_tensors->_iter_filter(lambda o: o is None or isinstance(o, torch.Tensor), condition_msg='Tensors or None')
A:torch.autograd.function._map_tensor_data->_nested_map(lambda x: isinstance(x, torch.Tensor), lambda o: o.data, condition_msg='Tensors')
A:torch.autograd.function.flat_input->tuple(_iter_tensors(input))
A:torch.autograd.function.flat_output->super(NestedIOFunction, self)._do_forward(*flat_input)
A:torch.autograd.function.nested_tensors->_map_tensor_data(self._nested_input)
A:torch.autograd.function.result->self.forward_extended(*nested_tensors)
A:torch.autograd.function.nested_gradients->_unflatten(gradients, self._nested_output)
A:torch.autograd.function.self.to_save->tuple(_iter_tensors(args))
A:torch.autograd.function.self.dirty_tensors->tuple(_iter_tensors((args, kwargs)))
A:torch.autograd.function.self.non_differentiable->tuple(_iter_tensors((args, kwargs)))
torch.autograd.Function(self,*args,**kwargs)
torch.autograd.Function.backward(ctx,*grad_outputs)
torch.autograd.Function.forward(ctx,*args,**kwargs)
torch.autograd.FunctionMeta(cls,name,bases,attrs)
torch.autograd.NestedIOFunction(Function)
torch.autograd.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.NestedIOFunction._do_forward(self,*input)
torch.autograd.NestedIOFunction.backward(self,*gradients)
torch.autograd.NestedIOFunction.backward_extended(self,*grad_output)
torch.autograd.NestedIOFunction.forward(self,*args)
torch.autograd.NestedIOFunction.forward_extended(self,*input)
torch.autograd.NestedIOFunction.mark_dirty(self,*args,**kwargs)
torch.autograd.NestedIOFunction.mark_non_differentiable(self,*args,**kwargs)
torch.autograd.NestedIOFunction.save_for_backward(self,*args)
torch.autograd.NestedIOFunction.saved_tensors(self)
torch.autograd.function.BackwardCFunction(_C._FunctionBase,_ContextMethodMixin,_HookMixin)
torch.autograd.function.BackwardCFunction.apply(self,*args)
torch.autograd.function.Function(self,*args,**kwargs)
torch.autograd.function.Function.__call__(self,*args,**kwargs)
torch.autograd.function.Function.backward(ctx,*grad_outputs)
torch.autograd.function.Function.forward(ctx,*args,**kwargs)
torch.autograd.function.FunctionMeta(cls,name,bases,attrs)
torch.autograd.function.FunctionMeta.__init__(cls,name,bases,attrs)
torch.autograd.function.InplaceFunction(self,inplace=False)
torch.autograd.function.InplaceFunction.__init__(self,inplace=False)
torch.autograd.function.NestedIOFunction(Function)
torch.autograd.function.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.function.NestedIOFunction._do_forward(self,*input)
torch.autograd.function.NestedIOFunction.backward(self,*gradients)
torch.autograd.function.NestedIOFunction.backward_extended(self,*grad_output)
torch.autograd.function.NestedIOFunction.forward(self,*args)
torch.autograd.function.NestedIOFunction.forward_extended(self,*input)
torch.autograd.function.NestedIOFunction.mark_dirty(self,*args,**kwargs)
torch.autograd.function.NestedIOFunction.mark_non_differentiable(self,*args,**kwargs)
torch.autograd.function.NestedIOFunction.save_for_backward(self,*args)
torch.autograd.function.NestedIOFunction.saved_tensors(self)
torch.autograd.function._ContextMethodMixin(object)
torch.autograd.function._ContextMethodMixin.mark_dirty(self,*args)
torch.autograd.function._ContextMethodMixin.mark_non_differentiable(self,*args)
torch.autograd.function._ContextMethodMixin.mark_shared_storage(self,*pairs)
torch.autograd.function._ContextMethodMixin.save_for_backward(self,*tensors)
torch.autograd.function._HookMixin(object)
torch.autograd.function._HookMixin._register_hook(backward_hooks,hook)
torch.autograd.function._iter_filter(condition,allow_unknown=False,condition_msg=None,conversion=None)
torch.autograd.function._jit_unwrap_structured(obj)
torch.autograd.function._nested_map(condition,fn,condition_msg=None)
torch.autograd.function._unflatten(input,proto)
torch.autograd.function.once_differentiable(fn)
torch.autograd.function.traceable(fn_cls)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/gradcheck.py----------------------------------------
A:torch.autograd.gradcheck.jacobians->list(filter(lambda x: x is not None, (make_jacobian(elem, num_out) for elem in input)))
A:torch.autograd.gradcheck.output_size->fn(input).numel()
A:torch.autograd.gradcheck.jacobian->make_jacobian(input, output.numel())
A:torch.autograd.gradcheck.x_tensors->iter_tensors(target, True)
A:torch.autograd.gradcheck.j_tensors->iter_tensors(jacobian)
A:torch.autograd.gradcheck.dim->len(size)
A:torch.autograd.gradcheck.x_nnz->x_tensor._nnz()
A:torch.autograd.gradcheck.x_size->list(x_tensor.size())
A:torch.autograd.gradcheck.x_indices->x_tensor._indices().t()
A:torch.autograd.gradcheck.x_values->x_tensor._values()
A:torch.autograd.gradcheck.x_stride->get_stride(x_size)
A:torch.autograd.gradcheck.d_idx->sum((indices[k] * x_stride[k] for k in range(len(x_size))))
A:torch.autograd.gradcheck.orig->x_tensor[x_idx].item()
A:torch.autograd.gradcheck.outa->fn(input).clone()
A:torch.autograd.gradcheck.outb->fn(input).clone()
A:torch.autograd.gradcheck.d_tensor[d_idx]->r.detach().reshape(-1)
A:torch.autograd.gradcheck.x_tensor_dense->x_tensor.to_dense()
A:torch.autograd.gradcheck.x_tensor_mkl->x_tensor.to_dense().to_mkldnn()
A:torch.autograd.gradcheck.diff_input_list->list(iter_tensors(tupled_inputs, True))
A:torch.autograd.gradcheck.jacobian_reentrant->make_jacobian(input, output.numel())
A:torch.autograd.gradcheck.grad_output->torch.zeros_like(output, memory_format=torch.legacy_contiguous_format)
A:torch.autograd.gradcheck.flat_grad_output->torch.zeros_like(output, memory_format=torch.legacy_contiguous_format).view(-1)
A:torch.autograd.gradcheck.grads_input->torch.autograd.grad(output, diff_input_list, [torch.zeros_like(o, memory_format=torch.legacy_contiguous_format) for o in output], allow_unused=True)
A:torch.autograd.gradcheck.jacobian_x[:, i]->d_x_dense.contiguous().view(-1)
A:torch.autograd.gradcheck.tupled_inputs->_as_tuple(inputs)
A:torch.autograd.gradcheck.func_out->func(*tupled_inputs)
A:torch.autograd.gradcheck.output->_differentiable_outputs(func(*tupled_inputs))
A:torch.autograd.gradcheck.numerical->get_numerical_jacobian(fn, tupled_inputs, eps=eps)
A:torch.autograd.gradcheck.(analytical, reentrant, correct_grad_sizes)->get_analytical_jacobian(tupled_inputs, o, nondet_tol=nondet_tol)
A:torch.autograd.gradcheck.gi->gi.to_dense().to_dense()
A:torch.autograd.gradcheck.i->i.to_dense().to_dense()
A:torch.autograd.gradcheck.y->torch.testing.make_non_contiguous(y)
A:torch.autograd.gradcheck.outputs->_differentiable_outputs(func(*input_args))
A:torch.autograd.gradcheck.tupled_grad_outputs->_as_tuple(grad_outputs)
A:torch.autograd.gradcheck.num_outputs->len(tupled_grad_outputs)
A:torch.autograd.gradcheck.input_args->tuple((x for x in input_args if isinstance(x, torch.Tensor) and x.requires_grad))
A:torch.autograd.gradcheck.grad_inputs->torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)
torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)
torch.autograd.gradcheck._as_tuple(x)
torch.autograd.gradcheck._differentiable_outputs(x)
torch.autograd.gradcheck.get_analytical_jacobian(input,output,nondet_tol=0.0)
torch.autograd.gradcheck.get_numerical_jacobian(fn,input,target=None,eps=0.001)
torch.autograd.gradcheck.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)
torch.autograd.gradcheck.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)
torch.autograd.gradcheck.iter_tensors(x,only_requiring_grad=False)
torch.autograd.gradcheck.make_jacobian(input,num_out)
torch.autograd.gradcheck.zero_gradients(x)
torch.autograd.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/_functions/utils.py----------------------------------------
A:torch.autograd._functions.utils.tensor->tensor.sum(dim, keepdim=True).sum(dim, keepdim=True)
A:torch.autograd._functions.utils.len1->len(dims1)
A:torch.autograd._functions.utils.len2->len(dims2)
A:torch.autograd._functions.utils.numel1->reduce(lambda x, y: x * y, dims1)
A:torch.autograd._functions.utils.numel2->reduce(lambda x, y: x * y, dims2)
torch.autograd._functions.utils.check_onnx_broadcast(dims1,dims2)
torch.autograd._functions.utils.maybe_unexpand(tensor,old_size,check_same_size=True)
torch.autograd._functions.utils.maybe_view(tensor,size,check_same_size=True)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/_functions/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/autograd/_functions/tensor.py----------------------------------------
A:torch.autograd._functions.tensor.ctx.input_type->type(i)
A:torch.autograd._functions.tensor.ctx.numel->reduce(lambda x, y: x * y, sizes, 1)
A:torch.autograd._functions.tensor.ctx.input_sizes->tensor.size()
A:torch.autograd._functions.tensor.result->tensor.new(tensor).contiguous().view(*sizes)
torch.autograd._functions.Resize(Function)
torch.autograd._functions.Resize.backward(ctx,grad_output)
torch.autograd._functions.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.Type(Function)
torch.autograd._functions.Type.backward(ctx,grad_output)
torch.autograd._functions.Type.forward(ctx,i,dest_type)
torch.autograd._functions.tensor.Resize(Function)
torch.autograd._functions.tensor.Resize.backward(ctx,grad_output)
torch.autograd._functions.tensor.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.tensor.Type(Function)
torch.autograd._functions.tensor.Type.backward(ctx,grad_output)
torch.autograd._functions.tensor.Type.forward(ctx,i,dest_type)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/__init__.py----------------------------------------
A:torch.testing.__init__.actual->torch.tensor(actual)
A:torch.testing.__init__.expected->expected.expand_as(actual).expand_as(actual)
A:torch.testing.__init__.(rtol, atol)->_get_default_tolerance(actual, expected)
A:torch.testing.__init__.close->torch.isclose(actual, expected, rtol, atol, equal_nan)
A:torch.testing.__init__.error->(expected - actual).abs()
A:torch.testing.__init__.(_, index)->delta.reshape(-1).max(0)
A:torch.testing.__init__.index->_unravel_index(index.item(), actual.shape)
A:torch.testing.__init__.count->(~close).long().sum()
A:torch.testing.__init__.osize->list(tensor.size())
A:torch.testing.__init__.dim->random.randint(0, len(osize) - 1)
A:torch.testing.__init__.add->random.randint(4, 15)
A:torch.testing.__init__.input->input.narrow(i, bounds, tensor.size(i)).narrow(i, bounds, tensor.size(i))
A:torch.testing.__init__.bounds->random.randint(1, input.size(i) - tensor.size(i))
A:torch.testing.__init__.a_tol->_get_default_tolerance(a)
A:torch.testing.__init__.b_tol->_get_default_tolerance(b)
torch.testing.__init__._get_default_tolerance(a,b=None)
torch.testing.__init__.assert_allclose(actual,expected,rtol=None,atol=None,equal_nan=True)
torch.testing.__init__.get_all_device_types()
torch.testing.__init__.get_all_dtypes()
torch.testing.__init__.get_all_math_dtypes(device)
torch.testing.__init__.make_non_contiguous(tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_distributed.py----------------------------------------
A:torch.testing._internal.common_distributed.TestSkip->namedtuple('TestSkip', 'exit_code, message')
A:torch.testing._internal.common_distributed.values->torch.ones([rank + 1] + [2 for _ in range(dense_dims)])
A:torch.testing._internal.common_distributed.fn->getattr(cls, attr)
A:torch.testing._internal.common_distributed.process->proc(target=self.__class__._run, name='process ' + str(rank), args=(rank, self._current_test_name(), self.file_name))
A:torch.testing._internal.common_distributed.self->cls(test_name)
A:torch.testing._internal.common_distributed.timeout->get_timeout(self.id())
A:torch.testing._internal.common_distributed.start_time->time.time()
A:torch.testing._internal.common_distributed.active_children->torch.multiprocessing.active_children()
A:torch.testing._internal.common_distributed.error->'Processes {} exited with error code {}'.format(' '.join([str(i) for (i, _) in errored_processes]), MultiProcessTestCase.TEST_ERROR_EXIT_CODE)
torch.testing._internal.common_distributed.MultiProcessTestCase(TestCase)
torch.testing._internal.common_distributed.MultiProcessTestCase._check_no_test_errors(self,elapsed_time)
torch.testing._internal.common_distributed.MultiProcessTestCase._check_return_codes(self,elapsed_time)
torch.testing._internal.common_distributed.MultiProcessTestCase._current_test_name(self)
torch.testing._internal.common_distributed.MultiProcessTestCase._fork_processes(self)
torch.testing._internal.common_distributed.MultiProcessTestCase._join_processes(self,fn)
torch.testing._internal.common_distributed.MultiProcessTestCase._run(cls,rank,test_name,file_name)
torch.testing._internal.common_distributed.MultiProcessTestCase._spawn_processes(self)
torch.testing._internal.common_distributed.MultiProcessTestCase._start_processes(self,proc)
torch.testing._internal.common_distributed.MultiProcessTestCase.is_master(self)
torch.testing._internal.common_distributed.MultiProcessTestCase.join_or_run(fn)
torch.testing._internal.common_distributed.MultiProcessTestCase.setUp(self)
torch.testing._internal.common_distributed.MultiProcessTestCase.setUpClass(cls)
torch.testing._internal.common_distributed.MultiProcessTestCase.tearDown(self)
torch.testing._internal.common_distributed.MultiProcessTestCase.world_size(self)
torch.testing._internal.common_distributed.get_timeout(test_id)
torch.testing._internal.common_distributed.requires_gloo()
torch.testing._internal.common_distributed.requires_mpi()
torch.testing._internal.common_distributed.requires_nccl()
torch.testing._internal.common_distributed.requires_nccl_version(version,msg)
torch.testing._internal.common_distributed.simple_sparse_reduce_tests(rank,world_size,num_inputs=1)
torch.testing._internal.common_distributed.skip_for_known_issues(func)
torch.testing._internal.common_distributed.skip_if_lt_x_gpu(x)
torch.testing._internal.common_distributed.skip_if_not_multigpu(func)
torch.testing._internal.common_distributed.skip_if_rocm(func)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/expecttest.py----------------------------------------
A:torch.testing._internal.expecttest.ACCEPT->os.getenv('EXPECTTEST_ACCEPT')
A:torch.testing._internal.expecttest.pos->src.find('\n', pos + 1)
A:torch.testing._internal.expecttest.EDIT_HISTORY->EditHistory()
A:torch.testing._internal.expecttest.RE_EXPECT->re.compile('^(?P<suffix>[^\\n]*?)(?P<quote>\'\'\'|""")(?P<body>.*?)(?P=quote)(?P<raw>r?)', re.DOTALL)
A:torch.testing._internal.expecttest.i->nth_eol(src, lineno)
A:torch.testing._internal.expecttest.new_string->normalize_nl(new_string)
A:torch.testing._internal.expecttest.s->escape_trailing_quote(s, '"').replace('"""', '\\"\\"\\"')
A:torch.testing._internal.expecttest.tb->traceback.extract_stack(limit=2 + skip)
A:torch.testing._internal.expecttest.old->f.read()
A:torch.testing._internal.expecttest.lineno->EditHistory().adjust_lineno(fn, lineno)
A:torch.testing._internal.expecttest.(new, delta)->replace_string_literal(old, lineno, actual)
torch.testing._internal.expecttest.EditHistory(self)
torch.testing._internal.expecttest.EditHistory.__init__(self)
torch.testing._internal.expecttest.EditHistory.adjust_lineno(self,fn,lineno)
torch.testing._internal.expecttest.EditHistory.record_edit(self,fn,lineno,delta)
torch.testing._internal.expecttest.EditHistory.seen_file(self,fn)
torch.testing._internal.expecttest.TestCase(unittest.TestCase)
torch.testing._internal.expecttest.TestCase.assertExpectedInline(self,actual,expect,skip=0)
torch.testing._internal.expecttest.escape_trailing_quote(s,quote)
torch.testing._internal.expecttest.normalize_nl(t)
torch.testing._internal.expecttest.nth_eol(src,lineno)
torch.testing._internal.expecttest.nth_line(src,lineno)
torch.testing._internal.expecttest.ok_for_raw_triple_quoted_string(s,quote)
torch.testing._internal.expecttest.replace_string_literal(src,lineno,new_string)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/dist_utils.py----------------------------------------
A:torch.testing._internal.dist_utils.TEST_CONFIG->TestConfig()
A:torch.testing._internal.dist_utils.return_value->old_test_method(self, *arg, **kwargs)
A:torch.testing._internal.dist_utils.error_regex->''.join(['({})|'.format(error_str) for error_str in error_regexes])
A:torch.testing._internal.dist_utils.num_pending_users->int(_rref_context_get_debug_info()['num_pending_users'])
torch.testing._internal.dist_utils.TestConfig(self,*args,**kwargs)
torch.testing._internal.dist_utils.TestConfig.__init__(self,*args,**kwargs)
torch.testing._internal.dist_utils.dist_init(old_test_method=None,setup_rpc=True,clean_shutdown=True)
torch.testing._internal.dist_utils.get_shutdown_error_regex(rpc_backend)
torch.testing._internal.dist_utils.initialize_pg(init_method,rank,world_size)
torch.testing._internal.dist_utils.noop()
torch.testing._internal.dist_utils.wait_until_node_failure(rank,expected_error_regex='.*')
torch.testing._internal.dist_utils.wait_until_pending_users_flushed()
torch.testing._internal.dist_utils.worker_name(rank)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/hypothesis_utils.py----------------------------------------
A:torch.testing._internal.hypothesis_utils._ENFORCED_ZERO_POINT->defaultdict(lambda : None, {torch.quint8: None, torch.qint8: None, torch.qint32: 0})
A:torch.testing._internal.hypothesis_utils._long_type_info->torch.iinfo(torch.long)
A:torch.testing._internal.hypothesis_utils.min_value->max((long_min - zero_point) * scale, long_min / scale + zero_point)
A:torch.testing._internal.hypothesis_utils.max_value->min((long_max - zero_point) * scale, long_max / scale + zero_point)
A:torch.testing._internal.hypothesis_utils.(min_value, max_value)->_get_valid_min_max(qparams)
A:torch.testing._internal.hypothesis_utils.quantized_type->draw(st.sampled_from(dtypes))
A:torch.testing._internal.hypothesis_utils._type_info->torch.iinfo(quantized_type)
A:torch.testing._internal.hypothesis_utils.zero_point->draw(st.integers(min_value=_zp_min, max_value=_zp_max))
A:torch.testing._internal.hypothesis_utils.scale->draw(floats(min_value=scale_min, max_value=scale_max, width=32))
A:torch.testing._internal.hypothesis_utils.max_dims->min(min_dims + 2, 32)
A:torch.testing._internal.hypothesis_utils._shape->draw(st.sampled_from(shapes))
A:torch.testing._internal.hypothesis_utils.elements->floats(min_value, max_value, allow_infinity=False, allow_nan=False, width=32)
A:torch.testing._internal.hypothesis_utils.X->draw(tensor(shapes=((batch_size, input_channels) + tuple(feature_map_shape),), elements=elements, qparams=qparams[0]))
A:torch.testing._internal.hypothesis_utils.qparams->draw(qparams)
A:torch.testing._internal.hypothesis_utils.(scale, zp)->_calculate_dynamic_per_channel_qparams(X, qparams[2])
A:torch.testing._internal.hypothesis_utils.enforced_zp->defaultdict(lambda : None, {torch.quint8: None, torch.qint8: None, torch.qint32: 0}).get(qparams[2], None)
A:torch.testing._internal.hypothesis_utils.axis->int(np.random.randint(0, X.ndim, 1))
A:torch.testing._internal.hypothesis_utils.permute_axes->numpy.arange(X.ndim)
A:torch.testing._internal.hypothesis_utils.batch_size->draw(st.integers(*batch_size_range))
A:torch.testing._internal.hypothesis_utils.input_channels_per_group->draw(st.integers(*input_channels_per_group_range))
A:torch.testing._internal.hypothesis_utils.output_channels_per_group->draw(st.integers(*output_channels_per_group_range))
A:torch.testing._internal.hypothesis_utils.groups->draw(st.integers(1, max_groups))
A:torch.testing._internal.hypothesis_utils.W->draw(tensor(shapes=((output_channels, input_channels_per_group) + tuple(kernels),), elements=elements, qparams=qparams[1]))
A:torch.testing._internal.hypothesis_utils.b->draw(tensor(shapes=(output_channels,), elements=elements, qparams=qparams[2]))
A:torch.testing._internal.hypothesis_utils.warning_message->'Your version of hypothesis is outdated. To avoid `DeadlineExceeded` errors, please update. Current hypothesis version: {}'.format(hypothesis.__version__)
torch.testing._internal.hypothesis_utils._floats_wrapper(*args,**kwargs)
torch.testing._internal.hypothesis_utils._get_valid_min_max(qparams)
torch.testing._internal.hypothesis_utils.array_shapes(draw,min_dims=1,max_dims=None,min_side=1,max_side=None)
torch.testing._internal.hypothesis_utils.assert_deadline_disabled()
torch.testing._internal.hypothesis_utils.assume_not_overflowing(tensor,qparams)
torch.testing._internal.hypothesis_utils.floats(*args,**kwargs)
torch.testing._internal.hypothesis_utils.per_channel_tensor(draw,shapes=None,elements=None,qparams=None)
torch.testing._internal.hypothesis_utils.qparams(draw,dtypes=None,scale_min=None,scale_max=None,zero_point_min=None,zero_point_max=None)
torch.testing._internal.hypothesis_utils.tensor(draw,shapes=None,elements=None,qparams=None)
torch.testing._internal.hypothesis_utils.tensor_conv(draw,spatial_dim=2,batch_size_range=(1,4),input_channels_per_group_range=(3,7),output_channels_per_group_range=(3,7),feature_map_range=(6,12),kernel_range=(3,7),max_groups=1,elements=None,qparams=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_utils.py----------------------------------------
A:torch.testing._internal.common_utils.old_prof_exec_state->kwargs.get('torch', globals()['torch'])._C._jit_set_profiling_executor(True)
A:torch.testing._internal.common_utils.old_prof_mode_state->kwargs.get('torch', globals()['torch'])._C._jit_set_profiling_mode(True)
A:torch.testing._internal.common_utils.parser->argparse.ArgumentParser(add_help=False)
A:torch.testing._internal.common_utils.(args, remaining)->argparse.ArgumentParser(add_help=False).parse_known_args()
A:torch.testing._internal.common_utils.p->subprocess.Popen(command, universal_newlines=True, cwd=cwd, env=env)
A:torch.testing._internal.common_utils.exit_status->subprocess.Popen(command, universal_newlines=True, cwd=cwd, env=env).wait(timeout=5)
A:torch.testing._internal.common_utils.IS_PYTORCH_CI->bool(os.environ.get('IS_PYTORCH_CI'))
A:torch.testing._internal.common_utils.IN_CIRCLECI->bool(os.environ.get('IN_CIRCLECI'))
A:torch.testing._internal.common_utils.TEST_REPORT_SOURCE_OVERRIDE->os.environ.get('TEST_REPORT_SOURCE_OVERRIDE')
A:torch.testing._internal.common_utils.suite->unittest.TestLoader().loadTestsFromModule(__main__)
A:torch.testing._internal.common_utils.exitcode->shell([sys.executable] + argv + [test_case_full_name])
A:torch.testing._internal.common_utils.test_report_path->os.path.join('test-reports', test_source)
A:torch.testing._internal.common_utils.f->tempfile.NamedTemporaryFile(delete=False)
A:torch.testing._internal.common_utils.loader->importlib.find_loader(name)
A:torch.testing._internal.common_utils.spec->importlib.util.find_spec(name)
A:torch.testing._internal.common_utils.TEST_NUMPY->_check_module_exists('numpy')
A:torch.testing._internal.common_utils.TEST_SCIPY->_check_module_exists('scipy')
A:torch.testing._internal.common_utils.TEST_MKL->kwargs.get('torch', globals()['torch']).backends.mkl.is_available()
A:torch.testing._internal.common_utils.TEST_NUMBA->_check_module_exists('numba')
A:torch.testing._internal.common_utils.skipper->unittest.skip('Cannot import `caffe2.python.core`')
A:torch.testing._internal.common_utils.(module, name)->'{}.{}'.format(type_name.__module__, type_name.__name__).rsplit('.', 1)
A:torch.testing._internal.common_utils.type_name->'{}.{}'.format(type_name.__module__, type_name.__name__)
A:torch.testing._internal.common_utils.t->type_map.get(obj.type(), get_gpu_type(obj.type()))
A:torch.testing._internal.common_utils.res->obj.clone().type(t)
A:torch.testing._internal.common_utils.rng_state->kwargs.get('torch', globals()['torch']).get_rng_state()
A:torch.testing._internal.common_utils.cuda_rng_state->kwargs.get('torch', globals()['torch']).cuda.get_rng_state()
A:torch.testing._internal.common_utils.beforeDevice->kwargs.get('torch', globals()['torch']).cuda.current_device()
A:torch.testing._internal.common_utils.deviceStream->kwargs.get('torch', globals()['torch']).cuda.Stream(device=d)
A:torch.testing._internal.common_utils.num_devices->kwargs.get('torch', globals()['torch']).cuda.device_count()
A:torch.testing._internal.common_utils.self.befores->self.get_cuda_memory_usage()
A:torch.testing._internal.common_utils.afters->self.get_cuda_memory_usage()
A:torch.testing._internal.common_utils.contents->urlopen(url, timeout=1).read().decode('utf-8')
A:torch.testing._internal.common_utils.the_response->json.loads(contents)
A:torch.testing._internal.common_utils.test_name->title[len(key):].strip()
A:torch.testing._internal.common_utils.test_method->getattr(self, method_name)
A:torch.testing._internal.common_utils.fullname->self.id().lower()
A:torch.testing._internal.common_utils.max_err->diff.abs().max().item()
A:torch.testing._internal.common_utils.v->kwargs.get('torch', globals()['torch']).full(shape, fv, dtype=dtype, layout=layout, device=device, requires_grad=rg)
A:torch.testing._internal.common_utils.i->random.randint(0, matrix_size - 1)
A:torch.testing._internal.common_utils.x->self.safeCoalesce(x)
A:torch.testing._internal.common_utils.r->self.safeCoalesce(t)
A:torch.testing._internal.common_utils.tc->type_map.get(obj.type(), get_gpu_type(obj.type())).coalesce()
A:torch.testing._internal.common_utils.idx_tup->tuple(idx.tolist())
A:torch.testing._internal.common_utils.new_indices->type_map.get(obj.type(), get_gpu_type(obj.type()))._indices().new(new_indices).t()
A:torch.testing._internal.common_utils.new_values->kwargs.get('torch', globals()['torch']).stack(new_values)
A:torch.testing._internal.common_utils.tg->type_map.get(obj.type(), get_gpu_type(obj.type())).new(new_indices, new_values, t.size())
A:torch.testing._internal.common_utils.a->a.to(torch.int).to(torch.int)
A:torch.testing._internal.common_utils.b->b.to(torch.int).to(torch.int)
A:torch.testing._internal.common_utils.nan_mask->kwargs.get('torch', globals()['torch']).isnan(a)
A:torch.testing._internal.common_utils.inf_mask->kwargs.get('torch', globals()['torch']).isinf(a)
A:torch.testing._internal.common_utils.inf_sign->kwargs.get('torch', globals()['torch']).isinf(a).sign()
A:torch.testing._internal.common_utils.diff->diff.abs().abs()
A:torch.testing._internal.common_utils.y->y.type_as(x).type_as(x)
A:torch.testing._internal.common_utils.key_list->list(x.keys())
A:torch.testing._internal.common_utils.found->any((re.search(regex, str(w.message)) is not None for w in ws))
A:torch.testing._internal.common_utils.backup[name]->reg.copy()
A:torch.testing._internal.common_utils.munged_id->remove_prefix(self.id(), module_id + '.')
A:torch.testing._internal.common_utils.test_file->os.path.realpath(sys.modules[module_id].__file__)
A:torch.testing._internal.common_utils.expected_file->os.path.join(os.path.dirname(test_file), 'expect', munged_id)
A:torch.testing._internal.common_utils.subname_output->' ({})'.format(subname)
A:torch.testing._internal.common_utils.expected->re.sub('CppOp\\[(.+?)\\]', 'CppOp[]', expected)
A:torch.testing._internal.common_utils.s->kwargs.get('torch', globals()['torch']).zeros(rows, columns, dtype=dtype, device=device)
A:torch.testing._internal.common_utils.env->os.environ.copy()
A:torch.testing._internal.common_utils.pipes->subprocess.Popen([sys.executable, '-c', code], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
A:torch.testing._internal.common_utils.filename->os.path.basename(urlsplit(url)[2])
A:torch.testing._internal.common_utils.data_dir->get_writable_path(os.path.join(os.path.dirname(__file__), 'data'))
A:torch.testing._internal.common_utils.path->os.path.join(data_dir, filename)
A:torch.testing._internal.common_utils.data->dict([((i, i), float(i + 1) / matrix_size) for i in range(matrix_size)])
A:torch.testing._internal.common_utils.msg->"could not download test file '{}'".format(url)
A:torch.testing._internal.common_utils.sock->socket.socket(socket.AF_INET, socket.SOCK_STREAM)
A:torch.testing._internal.common_utils.sockname->socket.socket(socket.AF_INET, socket.SOCK_STREAM).getsockname()
A:torch.testing._internal.common_utils.result->kwargs.get('torch', globals()['torch']).randn(dim_size, dim_size)
A:torch.testing._internal.common_utils.A->kwargs.get('torch', globals()['torch']).sparse_coo_tensor(indices, values, (rows, columns), device=device)
A:torch.testing._internal.common_utils.(u, s, v)->kwargs.get('torch', globals()['torch']).sparse_coo_tensor(indices, values, (rows, columns), device=device).svd()
A:torch.testing._internal.common_utils.dtype->kwargs.get('dtype', torch.double)
A:torch.testing._internal.common_utils.device->kwargs.get('device', 'cpu')
A:torch.testing._internal.common_utils.det->det.item().item()
A:torch.testing._internal.common_utils.cond->((det < 0) ^ (sign < 0)).nonzero()
A:torch.testing._internal.common_utils.silent->kwargs.get('silent', False)
A:torch.testing._internal.common_utils.(u, _, v)->kwargs.get('torch', globals()['torch']).sparse_coo_tensor(indices, values, (rows, columns), device=device).svd(some=False)
A:torch.testing._internal.common_utils.singular->kwargs.get('singular', False)
A:torch.testing._internal.common_utils.k->min(rows, columns)
A:torch.testing._internal.common_utils.B->random_matrix(rows, rank, *batch_dims, **kwargs)
A:torch.testing._internal.common_utils.C->random_matrix(rank, columns, *batch_dims, **kwargs)
A:torch.testing._internal.common_utils.nonzero_elements->max(min(rows, columns), int(rows * columns * density))
A:torch.testing._internal.common_utils.values->kwargs.get('torch', globals()['torch']).randn(nonzero_elements, dtype=dtype, device=device)
A:torch.testing._internal.common_utils.torch->kwargs.get('torch', globals()['torch'])
A:torch.testing._internal.common_utils.j->random.randint(0, matrix_size - 1)
A:torch.testing._internal.common_utils.theta->random.uniform(0, 2 * math.pi)
A:torch.testing._internal.common_utils.cs->math.cos(theta)
A:torch.testing._internal.common_utils.sn->math.sin(theta)
A:torch.testing._internal.common_utils.out->kwargs.get('torch', globals()['torch']).full(shape, fv, dtype=dtype, layout=layout, device=device, requires_grad=rg).new()
A:torch.testing._internal.common_utils.shape->kwargs.get('torch', globals()['torch']).Size([2, 3])
A:torch.testing._internal.common_utils.fill->tensor.new(shape).fill_(value)
A:torch.testing._internal.common_utils.module->'.'.join(str(dtype).split('.')[1:-1])
A:torch.testing._internal.common_utils.default_dtype->kwargs.get('torch', globals()['torch']).get_default_dtype()
A:torch.testing._internal.common_utils.int64_dtype->get_int64_dtype(dtype)
A:torch.testing._internal.common_utils.running_file->os.path.abspath(os.path.realpath(sys.argv[0]))
A:torch.testing._internal.common_utils.test_case_class_file->os.path.abspath(os.path.realpath(inspect.getfile(test_case.__class__)))
A:torch.testing._internal.common_utils.test_suite->unittest.TestSuite()
torch.testing._internal.common_utils.BytesIOContext(io.BytesIO)
torch.testing._internal.common_utils.BytesIOContext.__enter__(self)
torch.testing._internal.common_utils.BytesIOContext.__exit__(self,*args)
torch.testing._internal.common_utils.CudaMemoryLeakCheck(self,testcase,name=None)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.__enter__(self)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.__exit__(self,exec_type,exec_value,traceback)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.__init__(self,testcase,name=None)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.get_cuda_memory_usage()
torch.testing._internal.common_utils.CudaNonDefaultStream
torch.testing._internal.common_utils.CudaNonDefaultStream.__enter__(self)
torch.testing._internal.common_utils.CudaNonDefaultStream.__exit__(self,exec_type,exec_value,traceback)
torch.testing._internal.common_utils.ProfilingMode(Enum)
torch.testing._internal.common_utils.TestCase(self,method_name='runTest')
torch.testing._internal.common_utils.TestCase.__init__(self,method_name='runTest')
torch.testing._internal.common_utils.TestCase._reset_warning_registry(self)
torch.testing._internal.common_utils.TestCase.assertAlmostEqual(self,x,y,places=None,msg=None,delta=None,allow_inf=None)
torch.testing._internal.common_utils.TestCase.assertEqual(self,x,y,prec=None,message='',allow_inf=False,exact_dtype=None)
torch.testing._internal.common_utils.TestCase.assertExpected(self,s,subname=None)
torch.testing._internal.common_utils.TestCase.assertExpectedRaises(self,exc_type,callable,*args,**kwargs)
torch.testing._internal.common_utils.TestCase.assertExpectedStripMangled(self,s,subname=None)
torch.testing._internal.common_utils.TestCase.assertLeaksNoCudaTensors(self,name=None)
torch.testing._internal.common_utils.TestCase.assertNotEqual(self,x,y,prec=None,message='')
torch.testing._internal.common_utils.TestCase.assertNotWarn(self,callable,msg='')
torch.testing._internal.common_utils.TestCase.assertObjectIn(self,obj,iterable)
torch.testing._internal.common_utils.TestCase.assertTensorsSlowEqual(self,x,y,prec=None,message='')
torch.testing._internal.common_utils.TestCase.assertWarns(self,callable,msg='')
torch.testing._internal.common_utils.TestCase.assertWarnsRegex(self,callable,regex,msg='')
torch.testing._internal.common_utils.TestCase.enforceNonDefaultStream(self)
torch.testing._internal.common_utils.TestCase.genSparseTensor(self,size,sparse_dim,nnz,is_uncoalesced,device='cpu')
torch.testing._internal.common_utils.TestCase.maybeWarnsRegex(self,category,regex='')
torch.testing._internal.common_utils.TestCase.runWithPytorchAPIUsageStderr(code)
torch.testing._internal.common_utils.TestCase.safeCoalesce(self,t)
torch.testing._internal.common_utils.TestCase.safeToDense(self,t)
torch.testing._internal.common_utils.TestCase.setUp(self)
torch.testing._internal.common_utils.TestCase.wrap_method_with_cuda_policy(self,method,policy)
torch.testing._internal.common_utils.TestCase.wrap_with_cuda_memory_check(self,method)
torch.testing._internal.common_utils.TestCase.wrap_with_cuda_policy(self,method_name,policy)
torch.testing._internal.common_utils._assertGradAndGradgradChecks(test_case,apply_fn,inputs)
torch.testing._internal.common_utils._check_module_exists(name)
torch.testing._internal.common_utils._test_function(fn,device)
torch.testing._internal.common_utils.check_disabled(test_name)
torch.testing._internal.common_utils.check_test_defined_in_running_script(test_case)
torch.testing._internal.common_utils.do_test_dtypes(self,dtypes,layout,device)
torch.testing._internal.common_utils.do_test_empty_full(self,dtypes,layout,device)
torch.testing._internal.common_utils.download_file(url,binary=True)
torch.testing._internal.common_utils.enable_profiling_mode()
torch.testing._internal.common_utils.find_free_port()
torch.testing._internal.common_utils.freeze_rng_state()
torch.testing._internal.common_utils.get_cpu_type(type_name)
torch.testing._internal.common_utils.get_function_arglist(func)
torch.testing._internal.common_utils.get_gpu_type(type_name)
torch.testing._internal.common_utils.is_iterable(obj)
torch.testing._internal.common_utils.iter_indices(tensor)
torch.testing._internal.common_utils.load_tests(loader,tests,pattern)
torch.testing._internal.common_utils.make_nonzero_det(A,sign=None,min_singular_value=0.1)
torch.testing._internal.common_utils.prod_single_zero(dim_size)
torch.testing._internal.common_utils.prof_callable(callable,*args,**kwargs)
torch.testing._internal.common_utils.prof_func_call(*args,**kwargs)
torch.testing._internal.common_utils.prof_meth_call(*args,**kwargs)
torch.testing._internal.common_utils.random_fullrank_matrix_distinct_singular_value(matrix_size,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_lowrank_matrix(rank,rows,columns,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_matrix(rows,columns,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_sparse_matrix(rows,columns,density=0.01,**kwargs)
torch.testing._internal.common_utils.random_sparse_pd_matrix(matrix_size,density=0.01,**kwargs)
torch.testing._internal.common_utils.random_square_matrix_of_rank(l,rank,dtype=torch.double,device='cpu')
torch.testing._internal.common_utils.random_symmetric_matrix(l,*batches,**kwargs)
torch.testing._internal.common_utils.random_symmetric_pd_matrix(matrix_size,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_symmetric_psd_matrix(l,*batches,**kwargs)
torch.testing._internal.common_utils.repeat_test_for_types(dtypes)
torch.testing._internal.common_utils.retry(ExceptionToCheck,tries=3,delay=3)
torch.testing._internal.common_utils.retry_on_connect_failures(func=None,connect_errors=ADDRESS_IN_USE)
torch.testing._internal.common_utils.run_tests(argv=UNITTEST_ARGS)
torch.testing._internal.common_utils.set_rng_seed(seed)
torch.testing._internal.common_utils.set_running_script_path()
torch.testing._internal.common_utils.shell(command,cwd=None,env=None)
torch.testing._internal.common_utils.skipCUDAMemoryLeakCheckIf(condition)
torch.testing._internal.common_utils.skipCUDANonDefaultStreamIf(condition)
torch.testing._internal.common_utils.skipIfCompiledWithoutNumpy(fn)
torch.testing._internal.common_utils.skipIfNoLapack(fn)
torch.testing._internal.common_utils.skipIfNotRegistered(op_name,message)
torch.testing._internal.common_utils.skipIfRocm(fn)
torch.testing._internal.common_utils.slowTest(fn)
torch.testing._internal.common_utils.suppress_warnings(fn)
torch.testing._internal.common_utils.to_gpu(obj,type_map=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_nn.py----------------------------------------
A:torch.testing._internal.common_nn.result->getattr(m, 'weight', None)
A:torch.testing._internal.common_nn.total->reduce(mul, size, 1)
A:torch.testing._internal.common_nn.t->torch.rand(5).mul(8).floor().long()
A:torch.testing._internal.common_nn.weights->torch.rand(10)
A:torch.testing._internal.common_nn.sigmoid->torch.nn.Sigmoid()
A:torch.testing._internal.common_nn.i->torch.rand(10, 10).log()
A:torch.testing._internal.common_nn.target->self._get_target()
A:torch.testing._internal.common_nn.weight->input.half().cuda().new(len(input)).fill_(1)
A:torch.testing._internal.common_nn.random_samples->torch.DoubleTensor(2, 4, 3).uniform_()
A:torch.testing._internal.common_nn.padding->tuple(range(1, d + 1))
A:torch.testing._internal.common_nn.safe_target_log->(safe_target + (target <= 0).type_as(target)).log()
A:torch.testing._internal.common_nn.N->input.half().cuda().size(0)
A:torch.testing._internal.common_nn.C->input.half().cuda().size(1)
A:torch.testing._internal.common_nn.output->module(input)
A:torch.testing._internal.common_nn.input_index->list(tup)
A:torch.testing._internal.common_nn.(losses, weights)->zip(*losses_and_weights)
A:torch.testing._internal.common_nn.losses_tensor->input.half().cuda().new_tensor(losses)
A:torch.testing._internal.common_nn.abs_diff->(input - target).abs()
A:torch.testing._internal.common_nn.ge_one_mask->(abs_diff >= 1).type_as(abs_diff)
A:torch.testing._internal.common_nn.lt_one_mask->(abs_diff < 1).type_as(abs_diff)
A:torch.testing._internal.common_nn.input_dim->input.half().cuda().dim()
A:torch.testing._internal.common_nn.n->input.half().cuda().size(0)
A:torch.testing._internal.common_nn.dim->input.half().cuda().size(1)
A:torch.testing._internal.common_nn.output[i]->_multilabelmarginloss_reference(input[i], target[i])
A:torch.testing._internal.common_nn.margin_clamp->(margin - input).clamp(min=0).type_as(input)
A:torch.testing._internal.common_nn.target_dim->self._get_target().dim()
A:torch.testing._internal.common_nn.output[x]->_multimarginloss_reference(input[x], target[x], p, margin, weight)
A:torch.testing._internal.common_nn.cos->a.new(a.size(0))
A:torch.testing._internal.common_nn.d_p->torch.pairwise_distance(anchor, positive, p, eps)
A:torch.testing._internal.common_nn.d_n->torch.min(d_n, d_s)
A:torch.testing._internal.common_nn.d_s->torch.pairwise_distance(positive, negative, p, eps)
A:torch.testing._internal.common_nn.input_lengths->torch.as_tensor(input_lengths, dtype=torch.long)
A:torch.testing._internal.common_nn.target_lengths->torch.as_tensor(target_lengths, dtype=torch.long)
A:torch.testing._internal.common_nn.log_probs->log_probs.double().double()
A:torch.testing._internal.common_nn.targets->targets.long().long()
A:torch.testing._internal.common_nn.cum_target_lengths->torch.as_tensor(target_lengths, dtype=torch.long).cumsum(0)
A:torch.testing._internal.common_nn.input_length->input_lengths[i].item()
A:torch.testing._internal.common_nn.target_length->target_lengths[i].item()
A:torch.testing._internal.common_nn.cum_target_length->cum_target_lengths[i].item()
A:torch.testing._internal.common_nn.targets_prime->targets.long().long().new_full((2 * target_length + 1,), blank)
A:torch.testing._internal.common_nn.probs->log_probs[:input_length, i].exp()
A:torch.testing._internal.common_nn.alpha->log_probs.double().double().new_zeros((target_length * 2 + 1,))
A:torch.testing._internal.common_nn.alpha_next->log_probs.double().double().new_zeros((target_length * 2 + 1,)).clone()
A:torch.testing._internal.common_nn.input->input.half().cuda().half().cuda()
A:torch.testing._internal.common_nn.output_size->module(input).nelement()
A:torch.testing._internal.common_nn.jacobian_inp->self._jacobian(input, output_size)
A:torch.testing._internal.common_nn.flat_jacobian_input->list(iter_tensors(jacobian_inp))
A:torch.testing._internal.common_nn.num_param->sum((p.numel() for p in self._get_parameters(module)[0]))
A:torch.testing._internal.common_nn.jacobian_param->torch.zeros(num_param, output_size)
A:torch.testing._internal.common_nn.(param, d_param)->self._get_parameters(module)
A:torch.testing._internal.common_nn.d_out->torch.zeros_like(output)
A:torch.testing._internal.common_nn.flat_d_out->torch.zeros_like(output).view(-1)
A:torch.testing._internal.common_nn.d_input->deepcopy(test_case._backward(module, input, output, grad_output))
A:torch.testing._internal.common_nn.jacobian_x[:, i]->d_x.contiguous().view(-1)
A:torch.testing._internal.common_nn.jacobian_param[:, i]->torch.cat(self._flatten_tensors(d_param), 0)
A:torch.testing._internal.common_nn.res->tuple()
A:torch.testing._internal.common_nn.(param, _)->self._get_parameters(module)
A:torch.testing._internal.common_nn.jacobian_parameters->bool(self._get_parameters(module)[0])
A:torch.testing._internal.common_nn.analytical->self._analytical_jacobian(module, input, jacobian_input, jacobian_parameters)
A:torch.testing._internal.common_nn.numerical->self._numerical_jacobian(module, input, jacobian_input, jacobian_parameters)
A:torch.testing._internal.common_nn.analytical_t->list(iter_tensors(analytical_d_x))
A:torch.testing._internal.common_nn.numerical_t->list(iter_tensors(numerical_d_x))
A:torch.testing._internal.common_nn.analytical_d_x->self._backward_criterion(criterion, input, target)
A:torch.testing._internal.common_nn.numerical_d_x->deepcopy(analytical_d_x)
A:torch.testing._internal.common_nn.input_t->iter_tensors(input)
A:torch.testing._internal.common_nn.original->x[i].item()
A:torch.testing._internal.common_nn.fx1->self._forward_criterion(criterion, input, target)
A:torch.testing._internal.common_nn.fx2->self._forward_criterion(criterion, input, target)
A:torch.testing._internal.common_nn.d_x[i]->float(deriv)
A:torch.testing._internal.common_nn.kwargs[name]->tuple()
A:torch.testing._internal.common_nn.self._arg_cache[name]->map_tensor_sizes(self._extra_kwargs[size_name])
A:torch.testing._internal.common_nn.self.jacobian_input->kwargs.get('jacobian_input', True)
A:torch.testing._internal.common_nn.self.should_test_cuda->kwargs.get('test_cuda', True)
A:torch.testing._internal.common_nn.self.should_test_pickle->kwargs.get('pickle', True)
A:torch.testing._internal.common_nn.self.check_gradgrad->kwargs.get('check_gradgrad', True)
A:torch.testing._internal.common_nn.self.FIXME_no_cuda_gradgrad_comparison->kwargs.get('FIXME_no_cuda_gradgrad_comparison', False)
A:torch.testing._internal.common_nn.self.precision->kwargs.get('precision', 0.0002)
A:torch.testing._internal.common_nn.self.check_forward_only->kwargs.get('check_forward_only', True)
A:torch.testing._internal.common_nn.module->self.constructor(*self.constructor_args)
A:torch.testing._internal.common_nn.out->test_case._forward_criterion(module, input, target, extra_args=self.extra_args)
A:torch.testing._internal.common_nn.ref_input->deepcopy(input)
A:torch.testing._internal.common_nn.ref_module->deepcopy(module)
A:torch.testing._internal.common_nn.expected_out->self.reference_fn(*ref_args)
A:torch.testing._internal.common_nn.module_copy->torch.load(f)
A:torch.testing._internal.common_nn.ndim->tensor.dim()
A:torch.testing._internal.common_nn.noncontig->torch.stack([torch.empty_like(tensor), tensor], dim).select(dim, 1).detach()
A:torch.testing._internal.common_nn.grad_output->module(input).new(output.shape).normal_()
A:torch.testing._internal.common_nn.d_param->deepcopy(test_case._get_parameters(module)[1])
A:torch.testing._internal.common_nn.nc_input->self.noncontiguize(input)
A:torch.testing._internal.common_nn.nc_grad_output->self.noncontiguize(grad_output)
A:torch.testing._internal.common_nn.go->deepcopy(grad_output if contig_g else nc_grad_output)
A:torch.testing._internal.common_nn.grad->module(input).data.clone().normal_()
A:torch.testing._internal.common_nn.cpu_input->self._get_input()
A:torch.testing._internal.common_nn.gpu_input->to_gpu(cpu_input)
A:torch.testing._internal.common_nn.cpu_module->self.constructor(*self.constructor_args)
A:torch.testing._internal.common_nn.gpu_module->self.constructor(*self.constructor_args)
A:torch.testing._internal.common_nn.cpu_param->test_case._get_parameters(cpu_module)
A:torch.testing._internal.common_nn.gpu_param->test_case._get_parameters(gpu_module)
A:torch.testing._internal.common_nn.cpu_output->test_case._forward_criterion(cpu_module, cpu_input, cpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.gpu_output->test_case._forward_criterion(gpu_module, gpu_input, gpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.cpu_gradOutput->torch.randn_like(cpu_output, requires_grad=True)
A:torch.testing._internal.common_nn.gpu_gradOutput->torch.randn_like(cpu_output, requires_grad=True).type_as(gpu_output).detach()
A:torch.testing._internal.common_nn.cpu_gradInput->test_case._backward_criterion(cpu_module, cpu_input, cpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.gpu_gradInput->test_case._backward_criterion(gpu_module, gpu_input, gpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.cpu_gradInputs->torch.autograd.grad(cpu_output, (cpu_input,) + tuple(cpu_module.parameters()), cpu_gradOutput, create_graph=True)
A:torch.testing._internal.common_nn.gpu_gradInputs->torch.autograd.grad(gpu_output, (gpu_input,) + tuple(gpu_module.parameters()), gpu_gradOutput, create_graph=True)
A:torch.testing._internal.common_nn.cpu_gg->torch.autograd.grad(cpu_output.sum() + sum(map(lambda x: x.sum(), cpu_gradInputs)), (cpu_input, cpu_gradOutput) + tuple(cpu_module.parameters()), retain_graph=True)
A:torch.testing._internal.common_nn.gpu_gg->torch.autograd.grad(gpu_output.sum() + sum(map(lambda x: x.sum(), gpu_gradInputs)), (gpu_input, gpu_gradOutput) + tuple(gpu_module.parameters()), retain_graph=True)
A:torch.testing._internal.common_nn._required_arg_names->TestBase._required_arg_names.union({'target'})
A:torch.testing._internal.common_nn.cpu_target->self._get_target()
A:torch.testing._internal.common_nn.gpu_target->to_gpu(cpu_target)
A:torch.testing._internal.common_nn.gradOutput->torch.randn(())
A:torch.testing._internal.common_nn.self.cudnn->kwargs.get('cudnn', False)
A:torch.testing._internal.common_nn.self.check_inplace->kwargs.get('check_inplace', False)
A:torch.testing._internal.common_nn.self.skip_double->kwargs.get('skip_double', False)
A:torch.testing._internal.common_nn.params->tuple((x for x in module.parameters()))
A:torch.testing._internal.common_nn.module_ip->self.constructor(*self.constructor_args, inplace=True)
A:torch.testing._internal.common_nn.input_ip->deepcopy(input)
A:torch.testing._internal.common_nn.input_ip_clone->deepcopy(input).clone()
A:torch.testing._internal.common_nn.output_ip->module_ip(input_ip_clone)
A:torch.testing._internal.common_nn.self.check_half->kwargs.get('check_half', True)
A:torch.testing._internal.common_nn.self.check_bfloat16->kwargs.get('check_bfloat16', False)
A:torch.testing._internal.common_nn.self.convert_target->kwargs.get('convert_target', True)
torch.testing._internal.common_nn.CriterionTest(self,*args,**kwargs)
torch.testing._internal.common_nn.CriterionTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.CriterionTest._do_extra_tests(self,test_case,module,input,target)
torch.testing._internal.common_nn.CriterionTest._get_target(self)
torch.testing._internal.common_nn.CriterionTest.test_cuda(self,test_case)
torch.testing._internal.common_nn.InputVariableMixin(object)
torch.testing._internal.common_nn.InputVariableMixin._get_input(self)
torch.testing._internal.common_nn.ModuleTest(self,*args,**kwargs)
torch.testing._internal.common_nn.ModuleTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.ModuleTest.noncontiguize(self,obj)
torch.testing._internal.common_nn.ModuleTest.test_cuda(self,test_case)
torch.testing._internal.common_nn.ModuleTest.test_noncontig(self,test_case,module,input)
torch.testing._internal.common_nn.NNTestCase(TestCase)
torch.testing._internal.common_nn.NNTestCase._analytical_jacobian(self,module,input,jacobian_input=True,jacobian_parameters=True)
torch.testing._internal.common_nn.NNTestCase._flatten_tensors(self,x)
torch.testing._internal.common_nn.NNTestCase._jacobian(self,input,num_out)
torch.testing._internal.common_nn.NNTestCase._numerical_jacobian(self,module,input,jacobian_input=True,jacobian_parameters=True)
torch.testing._internal.common_nn.NNTestCase._zero_grad_input(self,input)
torch.testing._internal.common_nn.NNTestCase.check_criterion_jacobian(self,criterion,input,target)
torch.testing._internal.common_nn.NNTestCase.check_jacobian(self,module,input,jacobian_input=True)
torch.testing._internal.common_nn.NewCriterionTest(self,*args,**kwargs)
torch.testing._internal.common_nn.NewCriterionTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.NewCriterionTest._do_extra_tests(self,test_case,module,input,target)
torch.testing._internal.common_nn.NewCriterionTest._get_target(self)
torch.testing._internal.common_nn.NewCriterionTest.constructor_args(self)
torch.testing._internal.common_nn.NewCriterionTest.extra_args(self)
torch.testing._internal.common_nn.NewCriterionTest.test_cuda(self,test_case,dtype=None,extra_args=None)
torch.testing._internal.common_nn.NewModuleTest(self,*args,**kwargs)
torch.testing._internal.common_nn.NewModuleTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.NewModuleTest._do_test(self,test_case,module,input)
torch.testing._internal.common_nn.NewModuleTest._get_target(self)
torch.testing._internal.common_nn.NewModuleTest.constructor_args(self)
torch.testing._internal.common_nn.TestBase(self,constructor,desc='',reference_fn=None,fullname=None,**kwargs)
torch.testing._internal.common_nn.TestBase.__init__(self,constructor,desc='',reference_fn=None,fullname=None,**kwargs)
torch.testing._internal.common_nn.TestBase._get_arg(self,name,unpack)
torch.testing._internal.common_nn.TestBase._get_input(self,unpack=True)
torch.testing._internal.common_nn.TestBase._unpack(self,value)
torch.testing._internal.common_nn.TestBase.constructor_args(self)
torch.testing._internal.common_nn.TestBase.extra_args(self)
torch.testing._internal.common_nn.TestBase.get_name(self)
torch.testing._internal.common_nn._multilabelmarginloss_reference(input,target)
torch.testing._internal.common_nn._multimarginloss_reference(input,target_idx,p,margin,weight)
torch.testing._internal.common_nn._rand_tensor_non_equal(*size)
torch.testing._internal.common_nn.bce_with_logistic_legacy_enum_test()
torch.testing._internal.common_nn.bce_with_logistic_no_reduce_scalar_test()
torch.testing._internal.common_nn.bce_with_logistic_no_reduce_test()
torch.testing._internal.common_nn.bceloss_no_reduce_scalar_test()
torch.testing._internal.common_nn.bceloss_no_reduce_test()
torch.testing._internal.common_nn.bceloss_weights_no_reduce_scalar_test()
torch.testing._internal.common_nn.bceloss_weights_no_reduce_test()
torch.testing._internal.common_nn.cosineembeddingloss_reference(input1,input2,target,margin=0,reduction='mean')
torch.testing._internal.common_nn.ctcloss_reference(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean')
torch.testing._internal.common_nn.fractional_max_pool2d_test(test_case)
torch.testing._internal.common_nn.fractional_max_pool3d_test(test_case)
torch.testing._internal.common_nn.get_reduction(m)
torch.testing._internal.common_nn.get_weight(m)
torch.testing._internal.common_nn.hingeembeddingloss_margin_no_reduce_test()
torch.testing._internal.common_nn.hingeembeddingloss_no_reduce_test()
torch.testing._internal.common_nn.hingeembeddingloss_reference(input,target,margin=1.0,reduction='mean')
torch.testing._internal.common_nn.kldivloss_no_reduce_scalar_test()
torch.testing._internal.common_nn.kldivloss_no_reduce_test()
torch.testing._internal.common_nn.kldivloss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.kldivloss_with_target_no_reduce_test()
torch.testing._internal.common_nn.l1loss_no_reduce_scalar_test()
torch.testing._internal.common_nn.l1loss_no_reduce_test()
torch.testing._internal.common_nn.marginrankingloss_reference(input1,input2,target,margin=0,reduction='mean')
torch.testing._internal.common_nn.mseloss_no_reduce_scalar_test()
torch.testing._internal.common_nn.mseloss_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_0d_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_1d_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_index_neg_test()
torch.testing._internal.common_nn.multilabelmarginloss_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.multilabelsoftmarginloss_no_reduce_test()
torch.testing._internal.common_nn.multilabelsoftmarginloss_weights_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_1d_input_0d_target_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_1d_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_margin_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_p_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_reference(input,target,p=1,margin=1,weight=None,reduction='mean')
torch.testing._internal.common_nn.multimarginloss_weights_no_reduce_test()
torch.testing._internal.common_nn.nllloss2d_no_reduce_ignore_index_test()
torch.testing._internal.common_nn.nllloss2d_no_reduce_test()
torch.testing._internal.common_nn.nllloss2d_no_reduce_weights_test()
torch.testing._internal.common_nn.nlllossNd_no_reduce_ignore_index_test()
torch.testing._internal.common_nn.nlllossNd_no_reduce_test()
torch.testing._internal.common_nn.nlllossNd_no_reduce_weights_test()
torch.testing._internal.common_nn.nlllossNd_reference(input,target,weight=None,ignore_index=-100,reduction='mean')
torch.testing._internal.common_nn.nllloss_no_reduce_ignore_index_test()
torch.testing._internal.common_nn.nllloss_no_reduce_test()
torch.testing._internal.common_nn.nllloss_no_reduce_weights_ignore_index_neg_test()
torch.testing._internal.common_nn.nllloss_no_reduce_weights_ignore_index_test()
torch.testing._internal.common_nn.nllloss_no_reduce_weights_test()
torch.testing._internal.common_nn.nllloss_reference(input,target,weight=None,ignore_index=-100,reduction='mean')
torch.testing._internal.common_nn.padding1d_circular(input,pad)
torch.testing._internal.common_nn.padding2d_circular(input,pad)
torch.testing._internal.common_nn.padding3d_circular(input,pad)
torch.testing._internal.common_nn.poissonnllloss_no_reduce_test()
torch.testing._internal.common_nn.smoothl1loss_no_reduce_scalar_test()
torch.testing._internal.common_nn.smoothl1loss_no_reduce_test()
torch.testing._internal.common_nn.smoothl1loss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.softmarginloss_no_reduce_test()
torch.testing._internal.common_nn.softmarginloss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.tripletmarginloss_reference(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,reduction='mean')
torch.testing._internal.common_nn.wrap_functional(fn,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_cuda.py----------------------------------------
A:torch.testing._internal.common_cuda.TEST_CUDA->torch.cuda.is_available()
A:torch.testing._internal.common_cuda.TEST_NUMBA_CUDA->numba.cuda.is_available()
torch.testing._internal.common_cuda.initialize_cuda_context_rng()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_quantization.py----------------------------------------
A:torch.testing._internal.common_quantization.output->model(data)
A:torch.testing._internal.common_quantization.(_, predicted)->torch.max(output, 1)
A:torch.testing._internal.common_quantization._default_loss_fn->torch.nn.CrossEntropyLoss()
A:torch.testing._internal.common_quantization.optimizer->torch.optim.Adam(model.parameters(), lr=0.001)
A:torch.testing._internal.common_quantization.loss->loss_fn(output, target)
A:torch.testing._internal.common_quantization.scripted->torch.jit.script(orig_mod)
A:torch.testing._internal.common_quantization.traced->torch.jit.trace(orig_mod, calib_data[0][0], check_trace=False)
A:torch.testing._internal.common_quantization.buffer->io.BytesIO()
A:torch.testing._internal.common_quantization.loaded_mod->torch.jit.load(buffer)
A:torch.testing._internal.common_quantization.ref_output->orig_mod(inp)
A:torch.testing._internal.common_quantization.scripted_output->test_mod(inp)
A:torch.testing._internal.common_quantization.self.fc1->torch.nn.Linear(5, 5).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.x->self.no_quant_module(x)
A:torch.testing._internal.common_quantization.self.lstm->torch.nn.LSTM(2, 2).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.conv->torch.nn.Conv2d(2, 2, 1, bias=None).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.quant->QuantStub()
A:torch.testing._internal.common_quantization.self.dequant->DeQuantStub()
A:torch.testing._internal.common_quantization.self.bn->torch.nn.BatchNorm2d(2).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.fc2->torch.nn.Linear(10, 10).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.fc2.qconfig->torch.quantization.get_default_qconfig('fbgemm')
A:torch.testing._internal.common_quantization.self.fc->torch.nn.Linear(12, 6)
A:torch.testing._internal.common_quantization.self.relu->torch.nn.ReLU(inplace=False).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.sub1->SubModelForFusion()
A:torch.testing._internal.common_quantization.self.sub2->SubModelWithoutFusion()
A:torch.testing._internal.common_quantization.self.fc3->torch.nn.Linear(5, 5).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.sub2.fc1->QuantWrapper(self.sub2.fc1)
A:torch.testing._internal.common_quantization.custom_qconfig->QConfig(activation=default_observer.with_args(**custom_options), weight=default_weight_observer)
A:torch.testing._internal.common_quantization.self.sub2.fc2->QuantWrapper(self.sub2.fc2)
A:torch.testing._internal.common_quantization.self.sub->QuantWrapper(InnerModule())
A:torch.testing._internal.common_quantization.self.qconfig->torch.quantization.get_default_qat_qconfig('qnnpack')
A:torch.testing._internal.common_quantization.self.conv1->torch.nn.Conv2d(inplanes, inplanes, (1, 1), bias=False)
A:torch.testing._internal.common_quantization.self.bn1->norm_layer(inplanes)
A:torch.testing._internal.common_quantization.self.relu1->torch.nn.ReLU()
A:torch.testing._internal.common_quantization.self.features->torch.nn.Sequential(*layers)
A:torch.testing._internal.common_quantization.self.classifier->torch.nn.Sequential(*head)
A:torch.testing._internal.common_quantization.self.seq->torch.nn.Sequential()
A:torch.testing._internal.common_quantization.self.mycat->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.myadd->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.myadd_relu->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.y->self.mycat.cat([x, x, x])
A:torch.testing._internal.common_quantization.z->self.myadd.add(y, y)
A:torch.testing._internal.common_quantization.w->self.myadd_relu.add_relu(z, z)
A:torch.testing._internal.common_quantization.self.relu2->torch.nn.ReLU()
A:torch.testing._internal.common_quantization.self.downsample->torch.nn.Identity()
A:torch.testing._internal.common_quantization.self.myop->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.avgpool->torch.nn.AdaptiveAvgPool2d((4, 4))
A:torch.testing._internal.common_quantization.out->self.fc(out)
A:torch.testing._internal.common_quantization.identity->self.downsample(x)
A:torch.testing._internal.common_quantization.self.conv2->torch.nn.Conv2d(inplanes, inplanes, (1, 1), bias=False)
A:torch.testing._internal.common_quantization.self.skip_add->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.cat->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.maxpool->torch.nn.MaxPool2d((4, 4))
A:torch.testing._internal.common_quantization.skip->self.conv2(x)
A:torch.testing._internal.common_quantization.self.no_quant_module->self.ListOutModule()
torch.testing._internal.common_quantization.AnnotatedConvBnModel(self)
torch.testing._internal.common_quantization.AnnotatedConvBnModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedConvBnModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedConvModel(self)
torch.testing._internal.common_quantization.AnnotatedConvModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedConvModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedCustomConfigNestedModel(self)
torch.testing._internal.common_quantization.AnnotatedCustomConfigNestedModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedCustomConfigNestedModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedNestedModel(self)
torch.testing._internal.common_quantization.AnnotatedNestedModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedNestedModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedSingleLayerLinearModel(self)
torch.testing._internal.common_quantization.AnnotatedSingleLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedSingleLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedSubNestedModel(self)
torch.testing._internal.common_quantization.AnnotatedSubNestedModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedSubNestedModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedTwoLayerLinearModel(self)
torch.testing._internal.common_quantization.AnnotatedTwoLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedTwoLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.ConvBNReLU(self)
torch.testing._internal.common_quantization.ConvBNReLU.__init__(self)
torch.testing._internal.common_quantization.ConvBnModel(self)
torch.testing._internal.common_quantization.ConvBnModel.__init__(self)
torch.testing._internal.common_quantization.ConvBnModel.forward(self,x)
torch.testing._internal.common_quantization.ConvModel(self)
torch.testing._internal.common_quantization.ConvModel.__init__(self)
torch.testing._internal.common_quantization.ConvModel.forward(self,x)
torch.testing._internal.common_quantization.DummyObserver(torch.nn.Module)
torch.testing._internal.common_quantization.DummyObserver.calculate_qparams(self)
torch.testing._internal.common_quantization.DummyObserver.forward(self,x)
torch.testing._internal.common_quantization.InnerModule(self)
torch.testing._internal.common_quantization.InnerModule.__init__(self)
torch.testing._internal.common_quantization.InnerModule.forward(self,x)
torch.testing._internal.common_quantization.LSTMDynamicModel(self)
torch.testing._internal.common_quantization.LSTMDynamicModel.__init__(self)
torch.testing._internal.common_quantization.LSTMDynamicModel.forward(self,x)
torch.testing._internal.common_quantization.LinearReluModel(self)
torch.testing._internal.common_quantization.LinearReluModel.__init__(self)
torch.testing._internal.common_quantization.LinearReluModel.forward(self,x)
torch.testing._internal.common_quantization.ManualConvLinearQATModel(self)
torch.testing._internal.common_quantization.ManualConvLinearQATModel.__init__(self)
torch.testing._internal.common_quantization.ManualConvLinearQATModel.forward(self,x)
torch.testing._internal.common_quantization.ManualLinearQATModel(self)
torch.testing._internal.common_quantization.ManualLinearQATModel.__init__(self)
torch.testing._internal.common_quantization.ManualLinearQATModel.forward(self,x)
torch.testing._internal.common_quantization.ModelForFusion(self,qconfig)
torch.testing._internal.common_quantization.ModelForFusion.__init__(self,qconfig)
torch.testing._internal.common_quantization.ModelForFusion.forward(self,x)
torch.testing._internal.common_quantization.ModelMultipleOps(self)
torch.testing._internal.common_quantization.ModelMultipleOps.__init__(self)
torch.testing._internal.common_quantization.ModelMultipleOps.forward(self,x)
torch.testing._internal.common_quantization.ModelMultipleOpsNoAvgPool(self)
torch.testing._internal.common_quantization.ModelMultipleOpsNoAvgPool.__init__(self)
torch.testing._internal.common_quantization.ModelMultipleOpsNoAvgPool.forward(self,x)
torch.testing._internal.common_quantization.ModelWithFunctionals(self)
torch.testing._internal.common_quantization.ModelWithFunctionals.__init__(self)
torch.testing._internal.common_quantization.ModelWithFunctionals.forward(self,x)
torch.testing._internal.common_quantization.ModelWithNoQconfigPropagation(self)
torch.testing._internal.common_quantization.ModelWithNoQconfigPropagation.ListOutModule(self)
torch.testing._internal.common_quantization.ModelWithNoQconfigPropagation.ListOutModule.__init__(self)
torch.testing._internal.common_quantization.ModelWithNoQconfigPropagation.ListOutModule.forward(self,x)
torch.testing._internal.common_quantization.ModelWithNoQconfigPropagation.__init__(self)
torch.testing._internal.common_quantization.ModelWithNoQconfigPropagation.forward(self,x)
torch.testing._internal.common_quantization.ModelWithSequentialFusion(self)
torch.testing._internal.common_quantization.ModelWithSequentialFusion.__init__(self)
torch.testing._internal.common_quantization.ModelWithSequentialFusion.forward(self,x)
torch.testing._internal.common_quantization.NestedModel(self)
torch.testing._internal.common_quantization.NestedModel.__init__(self)
torch.testing._internal.common_quantization.NestedModel.forward(self,x)
torch.testing._internal.common_quantization.QuantStubModel(self)
torch.testing._internal.common_quantization.QuantStubModel.__init__(self)
torch.testing._internal.common_quantization.QuantStubModel.forward(self,x)
torch.testing._internal.common_quantization.QuantSubModel(self)
torch.testing._internal.common_quantization.QuantSubModel.__init__(self)
torch.testing._internal.common_quantization.QuantSubModel.forward(self,x)
torch.testing._internal.common_quantization.QuantizationTestCase(TestCase)
torch.testing._internal.common_quantization.QuantizationTestCase._checkModuleCorrectnessAgainstOrig(self,orig_mod,test_mod,calib_data)
torch.testing._internal.common_quantization.QuantizationTestCase._checkScriptable(self,orig_mod,script_mod,calib_data,check_save_load)
torch.testing._internal.common_quantization.QuantizationTestCase.checkDynamicQuantizedLinear(self,mod,dtype)
torch.testing._internal.common_quantization.QuantizationTestCase.checkHasPrepModules(self,module)
torch.testing._internal.common_quantization.QuantizationTestCase.checkLinear(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.checkNoPrepModules(self,module)
torch.testing._internal.common_quantization.QuantizationTestCase.checkObservers(self,module)
torch.testing._internal.common_quantization.QuantizationTestCase.checkQuantDequant(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.checkQuantizedLinear(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.checkScriptable(self,orig_mod,calib_data,check_save_load=False)
torch.testing._internal.common_quantization.QuantizationTestCase.checkWrappedQuantizedLinear(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.setUp(self)
torch.testing._internal.common_quantization.ResNetBase(self)
torch.testing._internal.common_quantization.ResNetBase.__init__(self)
torch.testing._internal.common_quantization.ResNetBase.forward(self,x)
torch.testing._internal.common_quantization.SingleLayerLinearDynamicModel(self)
torch.testing._internal.common_quantization.SingleLayerLinearDynamicModel.__init__(self)
torch.testing._internal.common_quantization.SingleLayerLinearDynamicModel.forward(self,x)
torch.testing._internal.common_quantization.SingleLayerLinearModel(self)
torch.testing._internal.common_quantization.SingleLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.SingleLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.SkipQuantModel(self)
torch.testing._internal.common_quantization.SkipQuantModel.__init__(self)
torch.testing._internal.common_quantization.SkipQuantModel.forward(self,x)
torch.testing._internal.common_quantization.SubModelForFusion(self)
torch.testing._internal.common_quantization.SubModelForFusion.__init__(self)
torch.testing._internal.common_quantization.SubModelForFusion.forward(self,x)
torch.testing._internal.common_quantization.SubModelWithoutFusion(self)
torch.testing._internal.common_quantization.SubModelWithoutFusion.__init__(self)
torch.testing._internal.common_quantization.SubModelWithoutFusion.forward(self,x)
torch.testing._internal.common_quantization.TwoLayerLinearModel(self)
torch.testing._internal.common_quantization.TwoLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.TwoLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.convert_dynamic(module)
torch.testing._internal.common_quantization.prepare_dynamic(model,qconfig_dict=None)
torch.testing._internal.common_quantization.test_only_eval_fn(model,calib_data)
torch.testing._internal.common_quantization.test_only_train_fn(model,train_data,loss_fn=_default_loss_fn)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_methods_invocations.py----------------------------------------
A:torch.testing._internal.common_methods_invocations.index->torch.LongTensor(*shape)
A:torch.testing._internal.common_methods_invocations.result->torch.randn(dim_size, dim_size, dim_size)
A:torch.testing._internal.common_methods_invocations.v->maybe_non_contig(arg).detach().to(device=device).clone()
A:torch.testing._internal.common_methods_invocations.non_differentiable->collections.namedtuple('non_differentiable', ['tensor'])
A:torch.testing._internal.common_methods_invocations.NO_ARGS->NoArgsClass()
A:torch.testing._internal.common_methods_invocations.var->torch.randn((), dtype=torch.double, device=device)
A:torch.testing._internal.common_methods_invocations.arg->arg.double().double()
A:torch.testing._internal.common_methods_invocations.args_out->tuple((map_arg(arg) for arg in call_args))
A:torch.testing._internal.common_methods_invocations.l->torch.ones(3, 3, dtype=torch.long, device=device, layout=torch.strided).tril(0).nonzero().transpose(0, 1)
A:torch.testing._internal.common_methods_invocations.x->torch.ones(3, 3, dtype=torch.long, device=device, layout=torch.strided)
A:torch.testing._internal.common_methods_invocations.u->torch.ones(3, 3, dtype=torch.long, device=device, layout=torch.strided).triu(0).nonzero().transpose(0, 1)
torch.testing._internal.common_methods_invocations.NoArgsClass(object)
torch.testing._internal.common_methods_invocations.NoArgsClass.__iter__(self)
torch.testing._internal.common_methods_invocations.NoArgsClass.__len__(self)
torch.testing._internal.common_methods_invocations.NoArgsClass.__next__(self)
torch.testing._internal.common_methods_invocations._compare_large_trilu_indices(self,row,col,offset=0,dtype=torch.long,device='cpu')
torch.testing._internal.common_methods_invocations._compare_trilu_indices(self,row,col,offset=0,dtype=torch.long,device='cpu')
torch.testing._internal.common_methods_invocations.bernoulli_scalar()
torch.testing._internal.common_methods_invocations.create_input(call_args,requires_grad=True,non_contiguous=False,call_kwargs=None,device=None)
torch.testing._internal.common_methods_invocations.dont_convert(tuple)
torch.testing._internal.common_methods_invocations.exclude_tensor_method(name,test_name)
torch.testing._internal.common_methods_invocations.gather_variable(shape,index_dim,max_indices,duplicate=False)
torch.testing._internal.common_methods_invocations.ident(x)
torch.testing._internal.common_methods_invocations.index_perm_variable(shape,max_indices)
torch.testing._internal.common_methods_invocations.index_variable(shape,max_indices)
torch.testing._internal.common_methods_invocations.mask_not_all_zeros(shape)
torch.testing._internal.common_methods_invocations.method_tests()
torch.testing._internal.common_methods_invocations.normal_scalar_clamp(amin,amax,requires_grad=False)
torch.testing._internal.common_methods_invocations.prod_zeros(dim_size,dim_select)
torch.testing._internal.common_methods_invocations.run_additional_tri_tests(self,device)
torch.testing._internal.common_methods_invocations.uniform_scalar(offset=0,requires_grad=False)
torch.testing._internal.common_methods_invocations.unpack_variables(args)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/jit_utils.py----------------------------------------
A:torch.testing._internal.jit_utils.RUN_CUDA->torch.cuda.is_available()
A:torch.testing._internal.jit_utils.torch.jit._recursive.concrete_type_store->torch.jit._recursive.ConcreteTypeStore()
A:torch.testing._internal.jit_utils.execution_plans->list(graph_executor_state.execution_plans.values())
A:torch.testing._internal.jit_utils.num_plans->len(execution_plans)
A:torch.testing._internal.jit_utils.self.stringio->StringIO()
A:torch.testing._internal.jit_utils.se->str(e)
A:torch.testing._internal.jit_utils.archive->zipfile.ZipFile(buffer)
A:torch.testing._internal.jit_utils.files->list(filter(lambda x: x.startswith('archive/code/'), archive.namelist()))
A:torch.testing._internal.jit_utils.code_files->map(lambda file: ''.join([line.decode() for line in file]), code_files)
A:torch.testing._internal.jit_utils.debug_files->map(lambda f: pickle.load(f), debug_files)
A:torch.testing._internal.jit_utils.buffer->io.BytesIO()
A:torch.testing._internal.jit_utils.buffer_copy->io.BytesIO().getvalue()
A:torch.testing._internal.jit_utils.(code_files, debug_files)->extract_files(buffer)
A:torch.testing._internal.jit_utils.buffer2->io.BytesIO(buffer_copy)
A:torch.testing._internal.jit_utils.imported->torch.jit.load(buffer, map_location=map_location)
A:torch.testing._internal.jit_utils.saved_module_buffer_2->io.BytesIO()
A:torch.testing._internal.jit_utils.(code_files_2, debug_files_2)->extract_files(saved_module_buffer_2)
A:torch.testing._internal.jit_utils.f->tempfile.NamedTemporaryFile(delete=False)
A:torch.testing._internal.jit_utils.result->getattr(torch._C, '_jit_pass_' + name)(graph)
A:torch.testing._internal.jit_utils.strgraph->str(graph)
A:torch.testing._internal.jit_utils.g->torch.onnx._optimize_trace(g, operator_export_type=OperatorExportTypes.ONNX)
A:torch.testing._internal.jit_utils.graph->trace.graph()
A:torch.testing._internal.jit_utils.diff_nodes->trace.graph().findAllNodes('prim::DifferentiableGraph')
A:torch.testing._internal.jit_utils.fusion_nodes->list(chain.from_iterable([g.findAllNodes('prim::FusionGroup') for g in diff_subgraphs]))
A:torch.testing._internal.jit_utils.frame->self.get_frame_vars(frames_up)
A:torch.testing._internal.jit_utils.source->textwrap.dedent(inspect.getsource(script))
A:torch.testing._internal.jit_utils.cu->torch.jit.CompilationUnit(script, _frames_up=frames_up)
A:torch.testing._internal.jit_utils.ge->self.getExportImportCopy(ge)
A:torch.testing._internal.jit_utils.state->model.get_debug_state()
A:torch.testing._internal.jit_utils.plan->get_execution_plan(state)
A:torch.testing._internal.jit_utils.num_bailouts->get_execution_plan(state).code.num_bailouts()
A:torch.testing._internal.jit_utils.bailout_outputs->model(*inputs)
A:torch.testing._internal.jit_utils.scripted_fn->torch.jit.script(script, _frames_up=1)
A:torch.testing._internal.jit_utils.recording_inputs->do_input_map(lambda t: Variable(t, requires_grad=True), reference_tensors)
A:torch.testing._internal.jit_utils.script_outputs->scripted_fn(*recording_inputs)
A:torch.testing._internal.jit_utils.opt_script_outputs->scripted_fn(*recording_inputs)
A:torch.testing._internal.jit_utils.python_outputs->python_fn(*inputs)
A:torch.testing._internal.jit_utils.flattened_recording_inputs->flatten_inputs(recording_inputs)
A:torch.testing._internal.jit_utils.outputs->func(*recording_inputs)
A:torch.testing._internal.jit_utils.outputs_ge->ge(*recording_inputs)
A:torch.testing._internal.jit_utils.grads->torch.autograd.grad(l1, flattened_recording_inputs, create_graph=True, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.grads_ge->torch.autograd.grad(l1_ge, flattened_recording_inputs, create_graph=True, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.l1->allSum(outputs)
A:torch.testing._internal.jit_utils.grads2->torch.autograd.grad(l2, flattened_recording_inputs, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.l1_ge->allSum(outputs_ge)
A:torch.testing._internal.jit_utils.grads2_ge->torch.autograd.grad(l2_ge, flattened_recording_inputs, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.m->self.createFunctionFromGraph(trace)
A:torch.testing._internal.jit_utils.m_import->self.getExportImportCopy(m)
A:torch.testing._internal.jit_utils.a->self.runAndSaveRNG(m, inputs)
A:torch.testing._internal.jit_utils.b->self.runAndSaveRNG(m_import, inputs)
A:torch.testing._internal.jit_utils.results->func(*inputs, **kwargs)
A:torch.testing._internal.jit_utils.sm->torch.jit.script(nn_module)
A:torch.testing._internal.jit_utils.eager_out->nn_module(*args)
A:torch.testing._internal.jit_utils.script_out->sm(*args)
A:torch.testing._internal.jit_utils.old->torch._C._jit_get_inline_everything_mode()
torch.testing._internal.jit_utils.JitTestCase(TestCase)
torch.testing._internal.jit_utils.JitTestCase._compared_saved_loaded(self,m)
torch.testing._internal.jit_utils.JitTestCase._isHookExceptionOk(self,e)
torch.testing._internal.jit_utils.JitTestCase.assertAutodiffNode(self,graph,should_autodiff_node,nonfusible_nodes,fusible_nodes)
torch.testing._internal.jit_utils.JitTestCase.assertExpectedGraph(self,trace,*args,**kwargs)
torch.testing._internal.jit_utils.JitTestCase.assertExpectedONNXGraph(self,g,*args,**kwargs)
torch.testing._internal.jit_utils.JitTestCase.assertExportImport(self,trace,inputs)
torch.testing._internal.jit_utils.JitTestCase.assertExportImportModule(self,m,inputs)
torch.testing._internal.jit_utils.JitTestCase.assertGraphContains(self,graph,kind)
torch.testing._internal.jit_utils.JitTestCase.assertGraphContainsExactly(self,graph,kind,num_kind_nodes,consider_subgraphs=False)
torch.testing._internal.jit_utils.JitTestCase.capture_stdout(list)
torch.testing._internal.jit_utils.JitTestCase.capture_stdout.__enter__(self)
torch.testing._internal.jit_utils.JitTestCase.capture_stdout.__exit__(self,*args)
torch.testing._internal.jit_utils.JitTestCase.checkBailouts(self,model,inputs,expected)
torch.testing._internal.jit_utils.JitTestCase.checkModule(self,nn_module,args)
torch.testing._internal.jit_utils.JitTestCase.checkScript(self,script,inputs,name='func',optimize=True,inputs_requires_grad=False,capture_output=False,frames_up=1,profiling=ProfilingMode.PROFILING)
torch.testing._internal.jit_utils.JitTestCase.checkScriptRaisesRegex(self,script,inputs,exception,regex,outputs=None,capture_output=False,profiling=ProfilingMode.PROFILING)
torch.testing._internal.jit_utils.JitTestCase.checkTrace(self,func,reference_tensors,input_tensors=None,drop=None,allow_unused=False,verbose=False,inputs_require_grads=True,check_tolerance=1e-05,export_import=True,_force_outplace=False)
torch.testing._internal.jit_utils.JitTestCase.clearHooks(self)
torch.testing._internal.jit_utils.JitTestCase.createFunctionFromGraph(self,trace)
torch.testing._internal.jit_utils.JitTestCase.emitFunctionHook(self,func)
torch.testing._internal.jit_utils.JitTestCase.emitModuleHook(self,module)
torch.testing._internal.jit_utils.JitTestCase.getExportImportCopy(self,m,also_test_file=True,map_location=None)
torch.testing._internal.jit_utils.JitTestCase.getExportImportCopyWithPacking(self,m,also_test_file=True,map_location=None)
torch.testing._internal.jit_utils.JitTestCase.get_frame_vars(self,frames_up)
torch.testing._internal.jit_utils.JitTestCase.runAndSaveRNG(self,func,inputs,kwargs=None)
torch.testing._internal.jit_utils.JitTestCase.run_pass(self,name,trace)
torch.testing._internal.jit_utils.JitTestCase.setHooks(self)
torch.testing._internal.jit_utils.JitTestCase.setUp(self)
torch.testing._internal.jit_utils.JitTestCase.tearDown(self)
torch.testing._internal.jit_utils._inline_everything(fn)
torch.testing._internal.jit_utils._tmp_donotuse_dont_inline_everything(fn)
torch.testing._internal.jit_utils._trace(*args,**kwargs)
torch.testing._internal.jit_utils.attrs_with_prefix(module,prefix)
torch.testing._internal.jit_utils.clear_class_registry()
torch.testing._internal.jit_utils.disable_autodiff_subgraph_inlining(enabled=True)
torch.testing._internal.jit_utils.do_input_map(fn,input)
torch.testing._internal.jit_utils.enable_cpu_fuser(fn)
torch.testing._internal.jit_utils.enable_cpu_fuser_if(cond)
torch.testing._internal.jit_utils.execWrapper(code,glob,loc)
torch.testing._internal.jit_utils.get_execution_plan(graph_executor_state)
torch.testing._internal.jit_utils.get_forward(c)
torch.testing._internal.jit_utils.get_forward_graph(c)
torch.testing._internal.jit_utils.get_module_method(m,module,method)
torch.testing._internal.jit_utils.inline_everything_mode(should_inline)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_device_type.py----------------------------------------
A:torch.testing._internal.common_device_type._tls->threading.local()
A:torch.testing._internal.common_device_type.dtypes->cls._get_dtypes(test)
A:torch.testing._internal.common_device_type.self.precision->self._get_precision_override(test, dtype)
A:torch.testing._internal.common_device_type.result->test(self, device_arg, dtype)
A:torch.testing._internal.common_device_type.primary_device_idx->int(cls.get_primary_device().split(':')[1])
A:torch.testing._internal.common_device_type.num_devices->torch.cuda.device_count()
A:torch.testing._internal.common_device_type.prim_device->cls.get_primary_device()
A:torch.testing._internal.common_device_type.t->torch.ones(1).cuda()
A:torch.testing._internal.common_device_type.cls.primary_device->'cuda:{0}'.format(torch.cuda.current_device())
A:torch.testing._internal.common_device_type.empty_class->type(empty_name, generic_test_class.__bases__, {})
A:torch.testing._internal.common_device_type.device_type_test_class->type(class_name, (base, empty_class), {})
A:torch.testing._internal.common_device_type.test->getattr(generic_test_class, name)
A:torch.testing._internal.common_device_type.nontest->getattr(generic_test_class, name)
A:torch.testing._internal.common_device_type.reason->'cuDNN version {0} is available but {1} required'.format(self.cudnn_version, version)
A:torch.testing._internal.common_device_type.self.device_type->kwargs.get('device_type', 'all')
A:torch.testing._internal.common_device_type.d->getattr(fn, 'dtypes', {})
torch.testing._internal.common_device_type.CPUTestBase(DeviceTypeTestBase)
torch.testing._internal.common_device_type.CUDATestBase(DeviceTypeTestBase)
torch.testing._internal.common_device_type.CUDATestBase.get_all_devices(cls)
torch.testing._internal.common_device_type.CUDATestBase.get_primary_device(cls)
torch.testing._internal.common_device_type.CUDATestBase.has_cudnn(self)
torch.testing._internal.common_device_type.CUDATestBase.setUpClass(cls)
torch.testing._internal.common_device_type.DeviceTypeTestBase(TestCase)
torch.testing._internal.common_device_type.DeviceTypeTestBase._get_dtypes(cls,test)
torch.testing._internal.common_device_type.DeviceTypeTestBase._get_precision_override(self,test,dtype)
torch.testing._internal.common_device_type.DeviceTypeTestBase.get_all_devices(cls)
torch.testing._internal.common_device_type.DeviceTypeTestBase.get_primary_device(cls)
torch.testing._internal.common_device_type.DeviceTypeTestBase.instantiate_test(cls,name,test)
torch.testing._internal.common_device_type.DeviceTypeTestBase.precision(self)
torch.testing._internal.common_device_type.DeviceTypeTestBase.precision(self,prec)
torch.testing._internal.common_device_type.deviceCountAtLeast(self,num_required_devices)
torch.testing._internal.common_device_type.deviceCountAtLeast.__init__(self,num_required_devices)
torch.testing._internal.common_device_type.dtypes(self,*args,**kwargs)
torch.testing._internal.common_device_type.dtypes.__init__(self,*args,**kwargs)
torch.testing._internal.common_device_type.dtypesIfCPU(self,*args)
torch.testing._internal.common_device_type.dtypesIfCPU.__init__(self,*args)
torch.testing._internal.common_device_type.dtypesIfCUDA(self,*args)
torch.testing._internal.common_device_type.dtypesIfCUDA.__init__(self,*args)
torch.testing._internal.common_device_type.expectedFailure(self,device_type)
torch.testing._internal.common_device_type.expectedFailure.__init__(self,device_type)
torch.testing._internal.common_device_type.expectedFailureCUDA(fn)
torch.testing._internal.common_device_type.instantiate_device_type_tests(generic_test_class,scope,except_for=None)
torch.testing._internal.common_device_type.largeCUDATensorTest(size)
torch.testing._internal.common_device_type.onlyCPU(fn)
torch.testing._internal.common_device_type.onlyCUDA(fn)
torch.testing._internal.common_device_type.onlyOn(self,device_type)
torch.testing._internal.common_device_type.onlyOn.__init__(self,device_type)
torch.testing._internal.common_device_type.onlyOnCPUAndCUDA(fn)
torch.testing._internal.common_device_type.precisionOverride(self,d)
torch.testing._internal.common_device_type.precisionOverride.__init__(self,d)
torch.testing._internal.common_device_type.skipCPUIf(self,dep,reason)
torch.testing._internal.common_device_type.skipCPUIf.__init__(self,dep,reason)
torch.testing._internal.common_device_type.skipCPUIfNoLapack(fn)
torch.testing._internal.common_device_type.skipCPUIfNoMkl(fn)
torch.testing._internal.common_device_type.skipCUDAIf(self,dep,reason)
torch.testing._internal.common_device_type.skipCUDAIf.__init__(self,dep,reason)
torch.testing._internal.common_device_type.skipCUDAIfCudnnVersionLessThan(version=0)
torch.testing._internal.common_device_type.skipCUDAIfNoCudnn(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoMagma(fn)
torch.testing._internal.common_device_type.skipCUDAIfNotRocm(fn)
torch.testing._internal.common_device_type.skipCUDAIfRocm(fn)
torch.testing._internal.common_device_type.skipIf(self,dep,reason,device_type=None)
torch.testing._internal.common_device_type.skipIf.__init__(self,dep,reason,device_type=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/common_quantized.py----------------------------------------
A:torch.testing._internal.common_quantized.qx->numpy.clip(qx, qmin, qmax).astype(qtype)
A:torch.testing._internal.common_quantized.X->X.numpy().numpy()
A:torch.testing._internal.common_quantized.min_val->min(min_val, 0.0)
A:torch.testing._internal.common_quantized.max_val->max(max_val, 0.0)
A:torch.testing._internal.common_quantized.scale->numpy.zeros(X.shape[0], dtype=np.float64)
A:torch.testing._internal.common_quantized.zero_point->numpy.zeros(X.shape[0], dtype=np.int64)
A:torch.testing._internal.common_quantized.scale[i]->max(scale[i], np.finfo(np.float32).eps)
A:torch.testing._internal.common_quantized.zero_point[i]->min(qmax, zero_point[i])
torch.testing._internal.common_quantized._calculate_dynamic_per_channel_qparams(X,dtype)
torch.testing._internal.common_quantized._calculate_dynamic_qparams(X,dtype,reduce_range=False)
torch.testing._internal.common_quantized._conv_output_shape(input_size,kernel_size,padding,stride,dilation,output_padding=0)
torch.testing._internal.common_quantized._dequantize(qx,scale,zero_point)
torch.testing._internal.common_quantized._quantize(x,scale,zero_point,qmin=None,qmax=None,dtype=np.uint8)
torch.testing._internal.common_quantized._requantize(x,multiplier,zero_point,qmin=0,qmax=255,qtype=np.uint8)
torch.testing._internal.common_quantized.override_quantized_engine(qengine)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/dist_autograd_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.dist_autograd_test.known_context_ids->set()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.requires_grad_tensor->torch.ones(3, 3, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.grads->torch.distributed.autograd.get_gradients(context_id)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ret->self._verify_backwards(exec_mode, [loss], context_id, local_grads, t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t1->torch.rand((3, 3))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.start->time.time()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.context_id_to_raised->set()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.fut->torch.distributed.rpc.rpc_async(worker_name(dst), method, args=args)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.send_functions->torch.distributed.autograd._current_context()._send_functions()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t2->torch.rand((3, 3))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ctx->torch.distributed.autograd._current_context()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.recv_functions->torch.distributed.autograd._current_context()._recv_functions()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.dst_rank->self._next_rank()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.worker_ids->torch.distributed.autograd._current_context()._known_worker_ids()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.success->_all_contexts_cleaned_up()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.nargs->len(args)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.loss->torch.distributed.rpc.remote(dst, DistAutogradTest._slow_add, args=(t1, t2)).to_here().sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.loss_local->torch.add(t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_ret->torch.add(t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.rref_t1->torch.distributed.rpc.remote(worker_name(self.rank), create_ref_fn, args=())
A:torch.testing._internal.distributed.rpc.dist_autograd_test.rref->torch.distributed.rpc.remote(dst, DistAutogradTest._slow_add, args=(t1, t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.callee->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.rref_owner->worker_name((self.rank + 2) % self.world_size)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_t1->torch.distributed.rpc.remote(worker_name(self.rank), create_ref_fn, args=()).to_here()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t3->torch.distributed.rpc.rpc_sync(worker_name(self._next_rank()), torch.matmul, args=(t1, t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t4->torch.distributed.rpc.rpc_sync('worker0', torch.mul, args=(t2, t3))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t5->torch.distributed.rpc.rpc_sync('worker0', torch.matmul, args=(t3, t4))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.val->torch.distributed.rpc.rpc_sync(worker_name(self._next_rank()), torch.div, args=(val, t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.s1->self._exec_func(exec_mode, torch.stack, (t4, val))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.s2->self._exec_func(exec_mode, torch.stack, (t5, val))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.s->self._exec_func(exec_mode, torch.stack, (t1, t2, t3))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t->threading.Thread(target=DistAutogradTest._workload_thread)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.tensor_list->self._exec_func(exec_mode, torch.split, t, 2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.res->torch.distributed.rpc.rpc_sync(worker_name(dst), DistAutogradTest._call_remote_embedding, args=(remote_embedding, input, offsets, per_sample_weights))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.shutdown_error_regex->get_shutdown_error_regex(dist_utils.TEST_CONFIG.rpc_backend_name)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r1->self._exec_func(exec_mode, torch.add, t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r2->self._exec_func(exec_mode, torch.mul, t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r3->self._exec_func(exec_mode, torch.cos, t1).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r4->self._exec_func(exec_mode, torch.div, t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_grads->self._verify_backwards(exec_mode, [loss], context_id, local_grads, t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.forward_ret->self._exec_func(exec_mode, my_script_add, t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.dst->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.context->torch.distributed.autograd._new_context()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.context_id->torch.distributed.autograd._new_context()._context_id()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.embedding->embedding_rref.local_value()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.grad_map->torch.distributed.autograd.get_gradients(context_id)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.remote_embedding->torch.distributed.rpc.remote(worker_name(dst), torch.nn.EmbeddingBag, args=(16, 16), kwargs={'mode': 'sum', 'sparse': True})
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_embedding->torch.nn.EmbeddingBag(16, 16, mode='sum', sparse=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.input->torch.LongTensor([1, 2, 4, 5, 4, 3, 2, 9])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.per_sample_weights->torch.rand(8, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.offsets->torch.LongTensor([0, 4])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_res->local_embedding(input, offsets, per_sample_weights)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.remote_grad->torch.distributed.rpc.rpc_sync(worker_name(dst), DistAutogradTest._get_grad, args=(remote_embedding, context_id))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.debug_info->torch.distributed.autograd._get_debug_info()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.backward_passes->int(debug_info['num_current_backward_passes'])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.res[i + 1]->torch.distributed.rpc.rpc_sync(worker_name(rank), torch.add, args=(res[i], t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.num_autograd_context->int(debug_info['num_autograd_contexts'])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t6->torch.distributed.rpc.rpc_sync('worker0', torch.add, args=(t4, t5))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.MyFunc.static_grad_ptr->grad.data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.MyFuncSingleGrad.static_grad_ptr->grad.data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ctx.size->inp1.size()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.a->torch.randn(5, 6, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.b->torch.randn(5, 6, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.p_a->grads[a].data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.p_b->grads[b].data_ptr()
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.MyBackwardFunc(Function)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.MyBackwardFunc.backward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.MyBackwardFunc.forward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.TestDebugInfoFunc(Function)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.TestDebugInfoFunc.backward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.TestDebugInfoFunc.forward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._call_remote_embedding(cls,embedding_rref,input,offsets,per_sample_weights)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._check_rpc_done(self,rank_distance)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._complex_python_udf(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._exec_func(self,exec_mode,method,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._exec_func_with_dst(self,dst,exec_mode,method,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._get_grad(cls,embedding_rref,context_id)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._mixed_requires_grad(cls,t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._nested_python_udf(t1,t2,dst)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._nested_rpc_call_backward_error(t1,t2,dst)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._next_rank(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._python_udf_with_backward_error(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._run_test_backward_unused_send_function_in_thread(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._set_backward_done()
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._slow_add(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_backward_rref(self,callee,rref_owner)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_backward_simple(self,dst)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_grad_only_on_return_value(self,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_graph(self,fn,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_graph_for_py_nested_call(self,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_graph_for_py_nested_call_itself(self,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_nested_backward_accumulate_grads(t1,t2,dst_rank)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_no_graph_with_tensors_not_require_grad(self,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_rpc_complex_args(self,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_trainer_ps(self,create_ref_fn,trainer_fn)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._verify_backwards(self,exec_mode,tensors,context_id,local_grads,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._verify_backwards_remote(self,tensors,context_id,local_grads,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._verify_graph_for_first_rpc_call(self,send_function,recv_function,t1,t2,ret)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._verify_graph_for_nested_rpc_call(self,ctx)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._verify_graph_for_rpc_call_exec(self,send_function)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._wait_backward_done()
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._workload_thread()
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.context_cleanup_test_helper(self,rpc_args,func,nested=False)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_async_dist_autograd(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_autograd_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_accumulate_grads(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_autograd_engine_error(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_complex_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_different_dtypes(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_different_tensor_dims(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_invalid_args(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_multiple_output_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_multiple_roots(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_multiple_round_trips(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_no_grad_on_tensor(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_node_failure(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_node_failure_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_python_udf_error(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_rref(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_rref_multi(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_rref_nested(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple_script_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple_self(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_unused_send_function(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_unused_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_verify_hooks(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_without_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_without_rpc(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backwards_nested_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_clean_context_during_backward(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_nested_rpc(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_no_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_tensor_no_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_tensor_with_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_debug_info(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_embedding_bag_with_no_grad_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_error_in_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_grad_only_on_return_value(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_grad_only_on_return_value_remote(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_builtin_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_builtin_remote_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_call_itself(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_remote_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_remote_call_itself(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_python_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_python_remote_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_mixed_requires_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_multiple_backward(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_multiple_backward_with_errors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_nested_backward_accumulate_grads(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_nested_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_grad_copy(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_graph_with_tensors_not_require_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_graph_with_tensors_not_require_grad_remote(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_remote_complex_args(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_rpc_complex_args(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_thread_local_context_id(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_trainer_ps(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_trainer_ps_torchscript_functions(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_worker_ids_recorded(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.ExecMode(Enum)
torch.testing._internal.distributed.rpc.dist_autograd_test.SimulateBackwardError(Function)
torch.testing._internal.distributed.rpc.dist_autograd_test.SimulateBackwardError.backward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.SimulateBackwardError.forward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test._all_contexts_cleaned_up(timeout_seconds=10)
torch.testing._internal.distributed.rpc.dist_autograd_test._check_rpc_done(rank_distance)
torch.testing._internal.distributed.rpc.dist_autograd_test._compare_owner_value(context_id,rref,grad)
torch.testing._internal.distributed.rpc.dist_autograd_test._run_trainer(rref_t1,t2,ps,rank_diff)
torch.testing._internal.distributed.rpc.dist_autograd_test._run_trainer_torchscript(rref_t1,t2,ps,rank_diff)
torch.testing._internal.distributed.rpc.dist_autograd_test._set_rpc_done(ctx_id,rank_distance)
torch.testing._internal.distributed.rpc.dist_autograd_test._torch_ones(sizes,requires_grad=False)
torch.testing._internal.distributed.rpc.dist_autograd_test.create_tensor()
torch.testing._internal.distributed.rpc.dist_autograd_test.create_torchscript_tensor()
torch.testing._internal.distributed.rpc.dist_autograd_test.my_nested_rref_add(dst,rref_t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_py_add(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_py_nested_call(t1,t2,dst,world_size,hops)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_rref_add(rref_t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_scalar_add(a,b)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_script_add(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_script_ref_add(ref_t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.ret_requires_grad()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/dist_optimizer_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.lock->threading.Lock()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.self.w->torch.rand((3, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_module1->torch.distributed.rpc.remote(owner1, MyModule)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_module2->torch.distributed.rpc.remote(owner2, MyModule)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_param1->remote_method(MyModule.get_w, remote_module1)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_param2->remote_method(MyModule.get_w, remote_module2)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.dist_optim->DistributedOptimizer(optim.SGD, [remote_param1, remote_param2], lr=0.05)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.t1->torch.rand((3, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.t2->torch.rand((3, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.output1->rpc_async_method(MyModule.forward, remote_module1, t2)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.output2->rpc_async_method(MyModule.forward, remote_module2, output1.wait())
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.loss->torch.add(output2.wait(), t1)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.module1->MyModule()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.module2->MyModule()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.local_optim->torch.optim.SGD(params, lr=0.05)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.old_w1->MyModule().w.clone().detach()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.old_w2->MyModule().w.clone().detach()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.old_w1_remote->remote_method(MyModule.get_w, remote_module1).to_here()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.new_w1->rpc_async_method(MyModule.get_w, remote_module1).wait()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.new_w2->rpc_async_method(MyModule.get_w, remote_module2).wait()
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim_exception(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim_exception_on_constructor(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.FailingOptimizer(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.FailingOptimizer.__init__(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.FailingOptimizer.step(self,closure=None)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule.__init__(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule.forward(self,t1)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule.get_w(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.OptimizerFailingOnConstructor(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.OptimizerFailingOnConstructor.__init__(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.OptimizerFailingOnConstructor.step(self,closure=None)
torch.testing._internal.distributed.rpc.dist_optimizer_test._call_method(method,obj_rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_method(method,obj_rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.dist_optimizer_test.rpc_async_method(method,obj_rref,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py----------------------------------------
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture(object)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.init_method(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.rpc_backend(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.rpc_backend_options(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.world_size(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/rpc_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.rpc_test.VALUE_FUTURE->concurrent.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.DONE_FUTURE->concurrent.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.TensorClass->namedtuple('TensorClass', ['tensors'])
A:torch.testing._internal.distributed.rpc.rpc_test.(pickled_python_udf, tensors)->torch.distributed.rpc.internal._internal_rpc_pickler.serialize(PythonUDF(my_tensor_function, (torch.ones(2, 2), torch.ones(2, 2)), None))
A:torch.testing._internal.distributed.rpc.rpc_test.python_udf->torch.distributed.rpc.internal._internal_rpc_pickler.deserialize(obj[0], obj[1])
A:torch.testing._internal.distributed.rpc.rpc_test.result->torch.distributed.rpc.rpc_sync(dst_worker, _call_method_on_rref, args=(MyClass.get_value, rref))
A:torch.testing._internal.distributed.rpc.rpc_test.a->MyClass(1)
A:torch.testing._internal.distributed.rpc.rpc_test.current_dst->worker_name(dst)
A:torch.testing._internal.distributed.rpc.rpc_test.rref->self._create_rref()
A:torch.testing._internal.distributed.rpc.rpc_test.ret_rref->torch.distributed.rpc.remote('worker{}'.format(dst_rank), check_rref_confirmed, args=(rref,))
A:torch.testing._internal.distributed.rpc.rpc_test.self_worker_info->torch.distributed.rpc.get_worker_info()
A:torch.testing._internal.distributed.rpc.rpc_test.peer_worker_info->torch.distributed.rpc.get_worker_info(worker_name(peer_rank))
A:torch.testing._internal.distributed.rpc.rpc_test.unknown_worker_id->torch.distributed.rpc.get_worker_info('WorkerUnknown')
A:torch.testing._internal.distributed.rpc.rpc_test.worker_infos->torch.distributed.rpc.api._get_current_rpc_agent().get_worker_infos()
A:torch.testing._internal.distributed.rpc.rpc_test.expected_worker_ids->set(range(self.world_size))
A:torch.testing._internal.distributed.rpc.rpc_test.self_worker_name->worker_name(self.rank)
A:torch.testing._internal.distributed.rpc.rpc_test.fut->torch.distributed.rpc.rpc_async(dst_worker, torch.add, args=(torch.ones(1), 3))
A:torch.testing._internal.distributed.rpc.rpc_test.ret->torch.distributed.rpc.rpc_sync('worker{}'.format(dst_rank), check_rref_confirmed, args=(rref,))
A:torch.testing._internal.distributed.rpc.rpc_test.dst->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.backend->torch.distributed.rpc.backend_registry.register_backend(backend_name, _stub_construct_rpc_backend_options_handler, _stub_init_rpc_backend_handler)
A:torch.testing._internal.distributed.rpc.rpc_test.(store, _, _)->next(torch.distributed.rendezvous(self.init_method, rank=self.rank, world_size=self.world_size))
A:torch.testing._internal.distributed.rpc.rpc_test.info->torch.distributed.rpc.api._get_current_rpc_agent().get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.workder_info->torch.distributed.rpc.get_worker_info(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.rpc_test.x->dict()
A:torch.testing._internal.distributed.rpc.rpc_test.value->concurrent.futures.Future().result()
A:torch.testing._internal.distributed.rpc.rpc_test.record_function->torch.autograd.profiler.record_function('foo')
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_name->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.foo_event_ix->next((i for (i, event) in enumerate(events) if 'foo' in event.name))
A:torch.testing._internal.distributed.rpc.rpc_test.rpc_event_idx->next((i for (i, event) in enumerate(events) if rpc_exec_mode.value in event.name))
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_info->torch.distributed.rpc.get_worker_info(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.rpc_test.fut1->torch.distributed.rpc.rpc_async(worker_name(dst_rank), my_sleep_func, args=(1,))
A:torch.testing._internal.distributed.rpc.rpc_test.fut2->torch.distributed.rpc.rpc_async(worker_name(dst_rank), my_sleep_func, args=(1,))
A:torch.testing._internal.distributed.rpc.rpc_test.b->MyClass(2)
A:torch.testing._internal.distributed.rpc.rpc_test.m->MyPickleClass()
A:torch.testing._internal.distributed.rpc.rpc_test.tik->time.time()
A:torch.testing._internal.distributed.rpc.rpc_test.tok->time.time()
A:torch.testing._internal.distributed.rpc.rpc_test.rref_a->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(n, n), 2))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_b->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(n, n), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_c->torch.distributed.rpc.remote(worker_name(dst_rank), my_rref_function, args=(rref_a, rref_b))
A:torch.testing._internal.distributed.rpc.rpc_test.c->torch.distributed.rpc.rpc_sync(worker_name(dst_rank), my_rref_function, args=(rref_a, rref_b))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_of_rrefs->torch.distributed.rpc.remote(worker_name(dst_rank1), nested_rref, args=(worker_name(dst_rank2),))
A:torch.testing._internal.distributed.rpc.rpc_test.rrefs->torch.distributed.rpc.remote(worker_name(dst_rank1), nested_rref, args=(worker_name(dst_rank2),)).to_here()
A:torch.testing._internal.distributed.rpc.rpc_test.local_rref->RRef(35)
A:torch.testing._internal.distributed.rpc.rpc_test.rref_list->torch.distributed.rpc.rpc_sync(worker_name(dst_rank), get_rref_list, args=([1, 2, 3],))
A:torch.testing._internal.distributed.rpc.rpc_test.other_a->torch.distributed.rpc.remote(worker_name(other_rank), torch.add, args=(torch.ones(1), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.other_b->torch.distributed.rpc.remote(worker_name(other_rank), torch.add, args=(torch.ones(1), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker->worker_name(dst_rank)
A:torch.testing._internal.distributed.rpc.rpc_test.rref1->RRef(self.rank)
A:torch.testing._internal.distributed.rpc.rpc_test.rref2->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(2, 2), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.rref3->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(2, 2), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.num_idle_threads->int(info['agent.num_idle_threads'])
A:torch.testing._internal.distributed.rpc.rpc_test.rref_info->_rref_context_get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.agent_info->torch.distributed.rpc.api._get_current_rpc_agent().get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.autograd_info->torch.distributed.autograd._get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.error_str->get_shutdown_error_regex(dist_utils.TEST_CONFIG.rpc_backend_name)
A:torch.testing._internal.distributed.rpc.rpc_test.timeout->timedelta(seconds=1)
A:torch.testing._internal.distributed.rpc.rpc_test.set_timeout->torch.distributed.rpc.get_rpc_timeout()
A:torch.testing._internal.distributed.rpc.rpc_test.rpc_backend_options->torch.distributed.rpc.ProcessGroupRpcBackendOptions(init_method=self.rpc_backend_options.init_method, num_send_recv_threads=NUM_THREADS)
A:torch.testing._internal.distributed.rpc.rpc_test.test_pickler->TestPickler()
A:torch.testing._internal.distributed.rpc.rpc_test.a.rref->torch.distributed.rpc.remote(dst_worker_name, torch.add, args=(torch.ones(n, n), 2))
A:torch.testing._internal.distributed.rpc.rpc_test.t1->torch.rand(3, 3).cuda(0)
A:torch.testing._internal.distributed.rpc.rpc_test.t2->torch.rand(3, 3).cuda(1)
A:torch.testing._internal.distributed.rpc.rpc_test.t3->torch.rand(3, 3)
torch.testing._internal.distributed.rpc.rpc_test.MyClass(self,a)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.__init__(self,a)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.get_value(self)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.increment_value(self,increment)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_class_method(cls,d,e)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_instance_method(self,b)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_static_method(f)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass(self)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.__getstate__(self)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.__init__(self)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.__setstate__(self,obj)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.set(self,val)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._create_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._gpu_tensor_list_arg(tensor_list)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._profiler_test_with_rpc(self,rpc_exec_mode,func,args,use_record_function=False)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._return_gpu_tensor()
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._return_gpu_tensor_list()
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_uneven_workload(self,num_repeat=30)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._stress_test_rpc(self,f,repeat=1000,args=())
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_multi_remote_call(self,fn,args_fn=lambdax:(),kwargs_fn=lambdax:{})
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_leak(self,_mock_delete_all_user_rrefs,ignore_leak)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_self_remote_rref_as_remote_arg(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_self_remote_rref_as_rpc_arg(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_add_with_id(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_builtin_remote_ret(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_builtin_remote_self(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_call_method_on_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_cuda(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_debug_info(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_disable_gil_profiling(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_dist_init_decorator(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_duplicate_name(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_expected_src(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_function_not_on_callee(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_get_rpc_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_get_worker_infos(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_graceful_shutdown_with_uneven_workload(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_handle_send_exceptions(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_ignore_rref_leak(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_invalid_names(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_rref_no_fork(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_shutdown(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_shutdown_with_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_value_not_on_owner(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_builtin_remote_ret(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_layer_nested_async_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_py_udf_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_rref_stress(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_non_garbage_collected_user_rref_due_to_local_circular_dependency(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nonzero(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_owner_equality(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_pass_local_rrefs(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_process_group_debug_info(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_async_rpc_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_async_rpc_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_remote_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_remote_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_sync_rpc_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_sync_rpc_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_built_in(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_constructor(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_instance_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_static_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_function_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_multi_async_call(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_nested_pickle(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_no_return_result(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_raise_in_user_func(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_rpc_rref_args(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_rref_args(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_rref_args_user_share(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_tensors(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_tensors_in_container(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_tensors_multi_async_call(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_udf_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_user_defined(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_register_rpc_backend_and_set_and_start_rpc_backend(self,mock_rpc_agent,mock_dist_autograd_init)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_reinit(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_remote_same_worker(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_remote_with_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_requires_process_group_agent_decorator(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_return_local_rrefs(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_return_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_timeouts(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_context_debug_info(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_forward_chain(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_leak(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_py_pickle_not_supported(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_str(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_scalar_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_py_udf_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_remote_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_rpc_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_self_remote_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_self_rpc_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_set_and_get_num_send_recv_threads(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_shutdown_followed_by_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_stress_heavy_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_stress_heavy_rpc_torchscript(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_stress_light_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_use_rpc_pickler(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_use_rref_after_shutdown(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_user_rrefs_confirmed(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_user_rrefs_confirmed_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_workers(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_workers_twice(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_worker_id(self)
torch.testing._internal.distributed.rpc.rpc_test.StubRpcAgent(self,world_size)
torch.testing._internal.distributed.rpc.rpc_test.StubRpcAgent.__init__(self,world_size)
torch.testing._internal.distributed.rpc.rpc_test.StubRpcAgent.get_worker_infos(self)
torch.testing._internal.distributed.rpc.rpc_test._call_method_on_rref(method,rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.rpc_test._stub_construct_rpc_backend_options_handler(**kwargs)
torch.testing._internal.distributed.rpc.rpc_test._stub_init_rpc_backend_handler(store,name,rank,world_size,rpc_backend_options)
torch.testing._internal.distributed.rpc.rpc_test.add_rref_to_value(rref,value)
torch.testing._internal.distributed.rpc.rpc_test.build_complex_tensors()
torch.testing._internal.distributed.rpc.rpc_test.check_rref_confirmed(rref)
torch.testing._internal.distributed.rpc.rpc_test.clear_global_rref()
torch.testing._internal.distributed.rpc.rpc_test.foo_add()
torch.testing._internal.distributed.rpc.rpc_test.get_rref_list(values)
torch.testing._internal.distributed.rpc.rpc_test.heavy_rpc(tensor)
torch.testing._internal.distributed.rpc.rpc_test.heavy_rpc_torchscript(tensor)
torch.testing._internal.distributed.rpc.rpc_test.light_rpc()
torch.testing._internal.distributed.rpc.rpc_test.multi_layer_nested_async_rpc(dst,world_size,ttl)
torch.testing._internal.distributed.rpc.rpc_test.my_complex_tensor_function(list_input,tensor_class_input,dict_input)
torch.testing._internal.distributed.rpc.rpc_test.my_function(a,b,c)
torch.testing._internal.distributed.rpc.rpc_test.my_rref_function(rref_a,rref_b)
torch.testing._internal.distributed.rpc.rpc_test.my_sleep_func(seconds=1)
torch.testing._internal.distributed.rpc.rpc_test.my_tensor_function(a,b)
torch.testing._internal.distributed.rpc.rpc_test.nested_remote(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_rpc(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_rref(dst)
torch.testing._internal.distributed.rpc.rpc_test.no_result()
torch.testing._internal.distributed.rpc.rpc_test.raise_func()
torch.testing._internal.distributed.rpc.rpc_test.requires_process_group_agent(message='')
torch.testing._internal.distributed.rpc.rpc_test.rpc_return_rref(dst)
torch.testing._internal.distributed.rpc.rpc_test.rref_forward_chain(dst,world_size,rref,ttl)
torch.testing._internal.distributed.rpc.rpc_test.run_nested_pickle(pickle_cls_instance,tensor)
torch.testing._internal.distributed.rpc.rpc_test.set_and_check_done(value)
torch.testing._internal.distributed.rpc.rpc_test.set_global_rref(rref)
torch.testing._internal.distributed.rpc.rpc_test.set_value(value)
torch.testing._internal.distributed.rpc.rpc_test.wait_for_value_future()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/jit/dist_autograd_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.t1->torch.rand((3, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.t2->torch.rand((3, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.t3->torch.add(t1, t2)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.grads->dist_get_gradients(context_id)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest.test_get_gradients(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/jit/rpc_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.jit.rpc_test.res_tensor->torch.ones(2, 2)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.self.a->torch.ones(rank)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_script_class->torch.distributed.rpc.RRef(MyScriptClass(self.rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.ret->torch.distributed.rpc.rpc_sync('worker{}'.format(dst_rank), script_check_rref_confirmed, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_script_module->torch.distributed.rpc.RRef(MyScriptModule(self.rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.dst_worker_name->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref->self._create_rref()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.fut->torch.distributed.rpc.rpc_async(dst_worker_name, nonexisting_script, args, kwargs)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.use_rref_on_owner_script->torch.jit.script(use_rref_on_owner)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.module->ref_script_module.to_here()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.local_ret->one_arg(torch.ones(2, 2))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.local_rref->torch.distributed.rpc.remote(worker_name(self.rank), one_arg, args=(torch.ones(2, 2),))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.my_local_script_module->MyScriptModule(self.rank)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref1->torch.distributed.rpc.rpc_sync(worker_name(dst_rank), return_rref, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref2->torch.distributed.rpc.remote(worker_name(dst_rank), rref_to_here, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref3->torch.distributed.rpc.remote(worker_name(dst_rank), return_rref, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.remote_ref->torch.distributed.rpc.remote(worker_name(dst_rank), construct_my_script_module, args=(self.rank,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_var->rpc_return_rref(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.res->rref_script_annotation(rref_var)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.module_with_rrefs->MyScriptModuleWithRRefs(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.ret_rref->torch.distributed.rpc.remote('worker{}'.format(dst_rank), script_check_rref_confirmed, args=(rref,))
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_all_kwargs_are_populated_by_defaults(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_args_and_kwargs_contain_different_types(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_args_kwargs_are_neither_passed(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_call_python_function_remotely_from_script_not_supported(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_call_script_function_that_not_exists_remotely_from_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_call_script_function_that_raises_remotely_from_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_kwargs_in_the_front_can_be_specified_by_extra_args(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_kwargs_not_passed(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_less_than_needed_args_are_specified(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_more_than_needed_args_are_specified(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_no_kwargs_are_populated_by_defaults(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_some_kwargs_are_populated_by_defaults(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcAsyncOpTest.test_unexepected_kwarg_is_specified(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest(LocalRRefTest,JitRpcAsyncOpTest,RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest._create_rref(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_my_script_module_with_rrefs(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_remote_script_module(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rref_as_arg_and_return(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rref_is_owner(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rref_jit_pickle_not_supported(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rref_python_annotation(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_torchscript_function(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_torchscript_function_exception(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_torchscript_functions_not_supported(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_user_rrefs_confirmed(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_user_rrefs_confirmed_remote(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_create_local_script_class_rref_in_py(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_create_local_script_module_rref_in_py(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_return_local_script_class_rref_in_py_and_use_in_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_return_local_script_module_rref_in_py_and_use_in_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyModuleInterface(torch.nn.Module)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyModuleInterface.forward(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptClass(self,a)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptClass.__init__(self,a)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptClass.get_value(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule(self,rank)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule.__init__(self,rank)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule.forward(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModuleWithRRefs(self,dst_worker)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModuleWithRRefs.__init__(self,dst_worker)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModuleWithRRefs.forward(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.assorted_types_args_kwargs(tensor_arg:Tensor,str_arg:str,int_arg:int,tensor_kwarg:Tensor=torch.tensor([2,2]),str_kwarg:str='str_kwarg',int_kwarg:int=2)
torch.testing._internal.distributed.rpc.jit.rpc_test.construct_my_script_module(rank)
torch.testing._internal.distributed.rpc.jit.rpc_test.my_script_module_init(rank)
torch.testing._internal.distributed.rpc.jit.rpc_test.no_arg()
torch.testing._internal.distributed.rpc.jit.rpc_test.one_arg(value)
torch.testing._internal.distributed.rpc.jit.rpc_test.owner_create_rref_my_script_class(a)
torch.testing._internal.distributed.rpc.jit.rpc_test.owner_create_rref_my_script_module(a)
torch.testing._internal.distributed.rpc.jit.rpc_test.python_function()
torch.testing._internal.distributed.rpc.jit.rpc_test.raise_script()
torch.testing._internal.distributed.rpc.jit.rpc_test.return_rref(rref_var)
torch.testing._internal.distributed.rpc.jit.rpc_test.rpc_async_call_remote_torchscript_in_torchscript(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor])
torch.testing._internal.distributed.rpc.jit.rpc_test.rpc_return_rref(dst)
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_python_annotation(rref_var)
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_script_annotation(rref_var)
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_to_here(rref_var)
torch.testing._internal.distributed.rpc.jit.rpc_test.run_ref_script_module(ref_script_module,t)
torch.testing._internal.distributed.rpc.jit.rpc_test.save_rref(rref_var,fname)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_check_rref_confirmed(rref)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_run_forward_rref_my_script_module(rref)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_run_get_value_rref_my_script_class(rref)
torch.testing._internal.distributed.rpc.jit.rpc_test.two_args_two_kwargs(first_arg,second_arg,first_kwarg=torch.tensor([3,3]),second_kwarg=torch.tensor([4,4]))


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/distributed/rpc/jit/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/test_module/no_future_div.py----------------------------------------
torch.testing._internal.test_module.no_future_div.div_float_nofuture()
torch.testing._internal.test_module.no_future_div.div_int_nofuture()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/test_module/future_div.py----------------------------------------
torch.testing._internal.test_module.future_div.div_float_future()
torch.testing._internal.test_module.future_div.div_int_future()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/test_module/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/data/network1.py----------------------------------------
A:torch.testing._internal.data.network1.self.linear->torch.nn.Linear(10, 20)
torch.testing._internal.data.network1.Net(self)
torch.testing._internal.data.network1.Net.__init__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/data/network2.py----------------------------------------
A:torch.testing._internal.data.network2.self.linear->torch.nn.Linear(10, 20)
A:torch.testing._internal.data.network2.self.relu->torch.nn.ReLU()
torch.testing._internal.data.network2.Net(self)
torch.testing._internal.data.network2.Net.__init__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/testing/_internal/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/contrib/_tensorboard_vis.py----------------------------------------
A:torch.contrib._tensorboard_vis.pb_graph->visualize(graph_executor)
A:torch.contrib._tensorboard_vis.evt->tensorflow.core.util.event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
A:torch.contrib._tensorboard_vis.input_node->visualize(graph_executor).node.add(op='input', name=name_prefix + 'input')
A:torch.contrib._tensorboard_vis.return_node->visualize(graph_executor).node.add(op='output', name=name_prefix + 'output')
A:torch.contrib._tensorboard_vis.input_kinds->visualize(graph_executor).node.add(op='INPUT_KIND', name=subgraph_name)
A:torch.contrib._tensorboard_vis.input_kinds.attr['inputs'].s->repr(arg_spec).encode('ascii')
A:torch.contrib._tensorboard_vis.op_id_counter->defaultdict(int)
A:torch.contrib._tensorboard_vis.(op, name)->name_for(node)
A:torch.contrib._tensorboard_vis.ge->next(executors_it)
A:torch.contrib._tensorboard_vis.pb_node->visualize(graph_executor).node.add(op=op, name=name)
torch.contrib._tensorboard_vis.dump_tensorboard_summary(graph_executor,logdir)
torch.contrib._tensorboard_vis.visualize(graph,name_prefix='',pb_graph=None,executors_it=None)
torch.contrib._tensorboard_vis.visualize_graph_executor(state,name_prefix,pb_graph,inline_graph)
torch.contrib._tensorboard_vis.visualize_rec(graph,value_map,name_prefix,pb_graph,executors_it=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/contrib/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/frontend.py----------------------------------------
A:torch.jit.frontend._identifier_chars->set(string.ascii_lowercase + string.ascii_uppercase + string.digits)
A:torch.jit.frontend.self.error_report->torch._C.ErrorReport(self.source_range)
A:torch.jit.frontend.node_type->type(offending_node)
A:torch.jit.frontend.range_len->len(node_start_tokens.get(node_type, ' '))
A:torch.jit.frontend.source_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(offending_node.lineno, offending_node.col_offset, offending_node.col_offset + range_len)
A:torch.jit.frontend.feature_name->pretty_node_names.get(node_type, node_type.__name__)
A:torch.jit.frontend.msg->"{} {}aren't supported".format(feature_name, reason + ' ' if reason else '')
A:torch.jit.frontend.methods->inspect.getmembers(cls, predicate=lambda m: (inspect.ismethod(m) or inspect.isfunction(m)) and m.__name__ in cls.__dict__)
A:torch.jit.frontend.(sourcelines, file_lineno, filename)->get_source_lines_and_file(fn, torch._C.ErrorReport.call_stack())
A:torch.jit.frontend.source->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).source.encode('utf-8')
A:torch.jit.frontend.dedent_src->dedent(source)
A:torch.jit.frontend.py_ast->ast.parse(dedent_src)
A:torch.jit.frontend.ctx->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn))
A:torch.jit.frontend.type_line->torch.jit.annotations.get_type_line(source)
A:torch.jit.frontend.method->getattr(self, 'build_' + node.__class__.__name__, None)
A:torch.jit.frontend.r->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(expr.lineno, expr.col_offset, expr.col_offset + 1)
A:torch.jit.frontend.param_list->build_param_list(ctx, py_def.args, self_name)
A:torch.jit.frontend.return_type->build_expr(ctx, py_def.returns)
A:torch.jit.frontend.decl->torch._C.merge_type_from_type_comment(decl, type_comment_decl, is_method)
A:torch.jit.frontend.type_comment_decl->torch._C.parse_type_comment(type_line)
A:torch.jit.frontend.ctx_range->build_expr(ctx, arg).range()
A:torch.jit.frontend.annotation_expr->EmptyTypeAnnotation(r)
A:torch.jit.frontend.argspec->inspect.getargspec(fn)
A:torch.jit.frontend.signature->inspect.signature(fn)
A:torch.jit.frontend.rhs->build_expr(ctx, expr.right)
A:torch.jit.frontend.lhs->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.the_type->build_expr(ctx, stmt.annotation)
A:torch.jit.frontend.expr->build_expr(ctx, stmt.exc)
A:torch.jit.frontend.test->build_expr(ctx, stmt.test)
A:torch.jit.frontend.op->type(op_)
A:torch.jit.frontend.base->build_expr(ctx, expr.value)
A:torch.jit.frontend.name_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_raw_range(start_pos, end_pos)
A:torch.jit.frontend.func->build_expr(ctx, expr.func)
A:torch.jit.frontend.stararg_expr->build_expr(ctx, expr.starargs)
A:torch.jit.frontend.kw_expr->build_expr(ctx, kw.value)
A:torch.jit.frontend.err_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_raw_range(sub_exprs[0].range().end, sub_exprs[1].range().start)
A:torch.jit.frontend.op_token->ExprBuilder.cmpop_map.get(op)
A:torch.jit.frontend.sub_expr->build_expr(ctx, expr.operand)
A:torch.jit.frontend.in_expr->BinOp('in', lhs, rhs)
A:torch.jit.frontend.cmp_expr->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.result->BinOp('and', result, cmp_expr)
A:torch.jit.frontend.sub_type->type(expr.slice)
A:torch.jit.frontend.value->str(expr.s)
A:torch.jit.frontend.error_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(expr.lineno, expr.col_offset, expr.col_offset + len(str(value)))
A:torch.jit.frontend.elt_expr->build_expr(ctx, stmt.elt)
A:torch.jit.frontend.target_expr->build_expr(ctx, stmt.generators[0].target)
A:torch.jit.frontend.iter_expr->build_expr(ctx, stmt.generators[0].iter)
A:torch.jit.frontend.build_expr->ExprBuilder()
A:torch.jit.frontend.build_stmt->StmtBuilder()
A:torch.jit.frontend.new_pos->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).source[:pos].rindex(substr)
torch.jit.frontend.Builder(self,ctx,node)
torch.jit.frontend.Builder.__call__(self,ctx,node)
torch.jit.frontend.ExprBuilder(Builder)
torch.jit.frontend.ExprBuilder.build_Attribute(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BinOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BoolOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Call(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Compare(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Constant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Dict(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Ellipsis(ctx,expr)
torch.jit.frontend.ExprBuilder.build_IfExp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_JoinedStr(ctx,expr)
torch.jit.frontend.ExprBuilder.build_List(ctx,expr)
torch.jit.frontend.ExprBuilder.build_ListComp(ctx,stmt)
torch.jit.frontend.ExprBuilder.build_Name(ctx,expr)
torch.jit.frontend.ExprBuilder.build_NameConstant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Num(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Starred(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Str(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Subscript(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Tuple(ctx,expr)
torch.jit.frontend.ExprBuilder.build_UnaryOp(ctx,expr)
torch.jit.frontend.FrontendError(self,source_range,msg)
torch.jit.frontend.FrontendError.__init__(self,source_range,msg)
torch.jit.frontend.FrontendError.__str__(self)
torch.jit.frontend.FrontendTypeError(FrontendError)
torch.jit.frontend.NotSupportedError(FrontendError)
torch.jit.frontend.StmtBuilder(Builder)
torch.jit.frontend.StmtBuilder.build_AnnAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assert(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_AugAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Break(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Continue(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Delete(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Expr(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_For(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_If(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Pass(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Print(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Raise(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Return(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_While(ctx,stmt)
torch.jit.frontend.UnsupportedNodeError(self,ctx,offending_node,reason='')
torch.jit.frontend.UnsupportedNodeError.__init__(self,ctx,offending_node,reason='')
torch.jit.frontend._uses_true_division(fn)
torch.jit.frontend.build_class_def(ctx,py_def,methods,self_name)
torch.jit.frontend.build_def(ctx,py_def,type_line,self_name=None)
torch.jit.frontend.build_param(ctx,py_arg,self_name,kwarg_only)
torch.jit.frontend.build_param_list(ctx,py_args,self_name)
torch.jit.frontend.build_stmts(ctx,stmts)
torch.jit.frontend.find_before(ctx,pos,substr,offsets=(0,0))
torch.jit.frontend.get_default_args(fn)
torch.jit.frontend.get_jit_class_def(cls,self_name)
torch.jit.frontend.get_jit_def(fn,self_name=None)
torch.jit.frontend.is_reserved_name(name)
torch.jit.get_default_args(fn)
torch.jit.get_jit_class_def(cls,self_name)
torch.jit.get_jit_def(fn,self_name=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/annotations.py----------------------------------------
A:torch.jit.annotations.signature->parse_type_line(type_line, rcb, loc)
A:torch.jit.annotations.source->dedent(''.join(get_source_lines_and_file(fn)[0]))
A:torch.jit.annotations.type_line->get_type_line(source)
A:torch.jit.annotations.py_ast->ast.parse(source)
A:torch.jit.annotations.(arg_ann_str, ret_ann_str)->split_type_line(type_line)
A:torch.jit.annotations.arg_ann->eval(arg_ann_str, {}, EvalEnv(rcb))
A:torch.jit.annotations.ret_ann->eval(ret_ann_str, {}, EvalEnv(rcb))
A:torch.jit.annotations.lines->dedent(''.join(get_source_lines_and_file(fn)[0])).split('\n')
A:torch.jit.annotations.type_lines->list(filter(lambda line: type_comment in line[1], lines))
A:torch.jit.annotations.lines_with_type->list(filter(lambda line: 'type' in line[1], lines))
A:torch.jit.annotations.type_pattern->re.compile('#[\t ]*type[\t ]*:')
A:torch.jit.annotations.wrong_type_lines->list(filter(lambda line: type_pattern.search(line[1]), lines))
A:torch.jit.annotations.types->map(get_parameter_type, parameter_type_lines)
A:torch.jit.annotations.parameter_types->', '.join(types)
A:torch.jit.annotations.start_offset->len('# type:')
A:torch.jit.annotations.arrow_pos->get_type_line(source).index('->')
A:torch.jit.annotations.sig->inspect.signature(fn)
A:torch.jit.annotations.return_type->ann_to_type(as_ann(sig.return_annotation), loc)
A:torch.jit.annotations.key->try_ann_to_type(ann.__args__[0], loc)
A:torch.jit.annotations.value->try_ann_to_type(ann.__args__[1], loc)
A:torch.jit.annotations.the_type->try_ann_to_type(ann, loc)
torch.jit.annotations.EvalEnv(self,rcb)
torch.jit.annotations.EvalEnv.__getitem__(self,name)
torch.jit.annotations.EvalEnv.__init__(self,rcb)
torch.jit.annotations.Module(self,name,members)
torch.jit.annotations.Module.__getattr__(self,name)
torch.jit.annotations.Module.__init__(self,name,members)
torch.jit.annotations.ann_to_type(ann,loc)
torch.jit.annotations.check_fn(fn,loc)
torch.jit.annotations.get_param_names(fn,n_args)
torch.jit.annotations.get_signature(fn,rcb,loc,is_method)
torch.jit.annotations.get_type_line(source)
torch.jit.annotations.is_function_or_method(the_callable)
torch.jit.annotations.is_vararg(the_callable)
torch.jit.annotations.parse_type_line(type_line,rcb,loc)
torch.jit.annotations.split_type_line(type_line)
torch.jit.annotations.try_ann_to_type(ann,loc)
torch.jit.annotations.try_real_annotations(fn,loc)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/_recursive.py----------------------------------------
A:torch.jit._recursive.ScriptMethodStub->collections.namedtuple('ScriptMethodStub', ('resolution_callback', 'def_', 'original_method'))
A:torch.jit._recursive.rcb->torch._jit_internal.createResolutionCallbackFromClosure(fn)
A:torch.jit._recursive.ast->torch.jit.get_jit_def(func, self_name='RecursiveScriptModule')
A:torch.jit._recursive.func->get_function_from_type(type(nn_module), method)
A:torch.jit._recursive.constants->', '.join((typ.__name__ for typ in _constant_types))
A:torch.jit._recursive.concrete_type_builder->infer_concrete_type_builder(nn_module)
A:torch.jit._recursive.class_annotations->getattr(nn_module, '__annotations__', {})
A:torch.jit._recursive.attr_type->infer_type(name, value)
A:torch.jit._recursive.added_names->set()
A:torch.jit._recursive.sub_concrete_type->ConcreteTypeStore().get_or_create_concrete_type(item)
A:torch.jit._recursive.constants_set->getattr(nn_module, '__constants__', set())
A:torch.jit._recursive.value->getattr(nn_module, name)
A:torch.jit._recursive.overloads->getattr(nn_module, '__overloads__', {})
A:torch.jit._recursive.scripted_fn->torch.jit.script(value)
A:torch.jit._recursive.hint->"(This attribute exists on the Python module, but we failed to convert Python type: '{}' to a TorchScript type.)".format(type(value).__name__)
A:torch.jit._recursive.builtin_symbol_name->_find_builtin(value)
A:torch.jit._recursive.self.methods_compiled->set()
A:torch.jit._recursive.nn_module_type->type(nn_module)
A:torch.jit._recursive.concrete_type->infer_concrete_type_builder(nn_module).build()
A:torch.jit._recursive.concrete_type_store->ConcreteTypeStore()
A:torch.jit._recursive.cpp_module->torch._C._create_module_with_type(concrete_type.jit_type)
A:torch.jit._recursive.stubs->stubs_fn(nn_module)
A:torch.jit._recursive.orig_value->getattr(nn_module, name)
A:torch.jit._recursive.scripted->create_script_module_impl(orig_value, sub_concrete_type, infer_methods_to_compile)
A:torch.jit._recursive.item->getattr(orig_class, name, None)
A:torch.jit._recursive.script_module->torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
A:torch.jit._recursive.script_method->functools.wraps(stub.original_method)(script_method)
A:torch.jit._recursive.script_attr->getattr(script_model, attr, None)
A:torch.jit._recursive.default_attr->get_function_from_type(torch.jit.RecursiveScriptModule, attr)
A:torch.jit._recursive.method_overloads->torch._jit_internal._get_overloaded_methods(item, mod.__class__)
A:torch.jit._recursive.overloads[item]->list(zip(names, method_overloads))
A:torch.jit._recursive.signature->torch.jit.annotations.get_signature(func, None, None, inspect.ismethod(func))
A:torch.jit._recursive.qual_name->torch.jit._qualified_name(func)
A:torch.jit._recursive.orig_ast->torch.jit.get_jit_def(orig_fn, self_name='RecursiveScriptModule')
A:torch.jit._recursive.over_ast->torch.jit.get_jit_def(overload_fn, self_name='RecursiveScriptModule')
A:torch.jit._recursive.new_ast->torch._C._replace_overloaded_method_decl(over_ast.decl(), orig_ast, overload_name)
A:torch.jit._recursive._rcb->torch._jit_internal.createResolutionCallbackFromClosure(orig_fn)
A:torch.jit._recursive.forward_func->getattr(nn_module.forward, '__func__', None)
A:torch.jit._recursive.module_forward->get_function_from_type(torch.nn.Module, 'forward')
A:torch.jit._recursive.overload_name_mappings->dict(getattr(nn_module, '__overloads__', {}))
A:torch.jit._recursive.overload_info->get_overload_annotations(nn_module)
A:torch.jit._recursive.overload_stubs->make_stubs_for_overloads(overload_info)
A:torch.jit._recursive.filtered_methods->filter(ignore_overloaded, methods)
A:torch.jit._recursive.uniquer->set()
A:torch.jit._recursive.script_module._concrete_type->torch._C.ConcreteModuleType.from_jit_type(script_module._c._type())
A:torch.jit._recursive.stub->make_stub(fn)
A:torch.jit._recursive.method->bind_method(unbound_method, script_module, torch.jit.RecursiveScriptModule)
torch.jit._recursive.ConcreteTypeStore(self)
torch.jit._recursive.ConcreteTypeStore.__init__(self)
torch.jit._recursive.ConcreteTypeStore.get_or_create_concrete_type(self,nn_module)
torch.jit._recursive.SourceContext(self,source,filename,file_lineno,leading_whitespace_len)
torch.jit._recursive.SourceContext.__init__(self,source,filename,file_lineno,leading_whitespace_len)
torch.jit._recursive._check_no_signature(func)
torch.jit._recursive._get_valid_constant(attr,v)
torch.jit._recursive.add_python_attr_to_scripted_model(script_model,orig,attr)
torch.jit._recursive.check_module_initialized(mod)
torch.jit._recursive.compile_unbound_method(concrete_type,fn)
torch.jit._recursive.create_methods_from_stubs(concrete_type,stubs)
torch.jit._recursive.create_script_module(nn_module,stubs_fn,share_types=True)
torch.jit._recursive.create_script_module_impl(nn_module,concrete_type,stubs_fn)
torch.jit._recursive.get_overload_annotations(mod)
torch.jit._recursive.get_overload_name_mapping(overload_info)
torch.jit._recursive.infer_concrete_type_builder(nn_module)
torch.jit._recursive.infer_methods_to_compile(nn_module)
torch.jit._recursive.interface_script(mod_interface,nn_module)
torch.jit._recursive.lazy_bind(concrete_type,unbound_method)
torch.jit._recursive.make_stub(func)
torch.jit._recursive.make_stub_from_method(nn_module,method)
torch.jit._recursive.make_stubs_for_overloads(overload_info)
torch.jit._recursive.script_model_defines_attr(script_model,attr)
torch.jit._recursive.try_compile_fn(fn,loc)
torch.jit._recursive.wrap_cpp_module(cpp_module)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/supported_ops.py----------------------------------------
A:torch.jit.supported_ops.v->'\n{}{}'.format(' ' * indent, v)
A:torch.jit.supported_ops.qualified_name->'{}.{}'.format(mod, name)
A:torch.jit.supported_ops.schema->_emit_schema(mod.__name__, fn.__name__, schema)
A:torch.jit.supported_ops.schemas->torch._C._jit_get_schemas_for_operator(op_name)
A:torch.jit.supported_ops.attr->getattr(mod, elem)
A:torch.jit.supported_ops.scripted->torch.jit.script(attr)
A:torch.jit.supported_ops.builtin->torch.jit._find_builtin(fn)
A:torch.jit.supported_ops.mod->inspect.getmodule(fn)
A:torch.jit.supported_ops.builtins->list(builtins)
A:torch.jit.supported_ops.op_name->'aten::{}'.format(fn)
A:torch.jit.supported_ops.table_row->'":any:`{}`", "{}"'.format(fn, schemaless_op_explanations[fn])
A:torch.jit.supported_ops.schematized_ops->textwrap.indent(schematized_ops, '\t')
A:torch.jit.supported_ops.schemaless_ops->textwrap.indent(schemaless_ops, '\t')
A:torch.jit.supported_ops.magic_methods_rows->textwrap.indent(magic_methods_rows, '\t')
A:torch.jit.supported_ops.section->'{}\n{}\n{}'.format(header, '~' * len(header), emit_block(items))
A:torch.jit.supported_ops.(header, items)->fn()
A:torch.jit.supported_ops.link_target->header.replace('`', '').replace('-', '').lower().replace(' ', '-')
A:torch.jit.supported_ops.__doc__->_list_supported_ops()
torch.jit.supported_ops._emit_arg(indent,i,arg)
torch.jit.supported_ops._emit_args(indent,arguments)
torch.jit.supported_ops._emit_ret(ret)
torch.jit.supported_ops._emit_rets(returns)
torch.jit.supported_ops._emit_schema(mod,name,schema,arg_start=0,padding=4)
torch.jit.supported_ops._emit_type(type)
torch.jit.supported_ops._get_builtins_helper()
torch.jit.supported_ops._get_global_builtins()
torch.jit.supported_ops._get_math_builtins()
torch.jit.supported_ops._get_nn_functional_ops()
torch.jit.supported_ops._get_tensor_ops()
torch.jit.supported_ops._get_torchscript_builtins()
torch.jit.supported_ops._hidden(name)
torch.jit.supported_ops._is_math_fn(fn)
torch.jit.supported_ops._list_supported_ops()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/_pickle.py----------------------------------------
torch.jit._pickle.build_boollist(data)
torch.jit._pickle.build_doublelist(data)
torch.jit._pickle.build_intlist(data)
torch.jit._pickle.build_tensor_from_id(data)
torch.jit._pickle.build_tensorlist(data)
torch.jit._pickle.restore_type_tag(value,type_str)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/quantized.py----------------------------------------
A:torch.jit.quantized.(self.weight, self.col_offsets, self.scale, self.zero_point)->torch.fbgemm_linear_quantize_weight(other.weight.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.self.weight->torch.fbgemm_pack_gemm_matrix_fp16(other.weight.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.self.col_offsets->torch.nn.Parameter(self.col_offsets, requires_grad=False)
A:torch.jit.quantized.self.bias->torch.nn.Parameter(other.bias.clone(memory_format=torch.contiguous_format).float(), requires_grad=False)
A:torch.jit.quantized.out->torch.fbgemm_linear_fp16_weight_fp32_activation(input.float(), self.packed_weight, self.bias)
A:torch.jit.quantized.repr->'in_features={in_features}, out_features={out_features}, '.format(**self.__dict__)
A:torch.jit.quantized.(weight_ih, col_offsets_ih, self.scale_ih, self.zero_point_ih)->torch.fbgemm_linear_quantize_weight(other.weight_ih.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.(weight_hh, col_offsets_hh, self.scale_hh, self.zero_point_hh)->torch.fbgemm_linear_quantize_weight(other.weight_hh.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.packed_ih->torch.fbgemm_pack_quantized_matrix(self.weight_ih)
A:torch.jit.quantized.packed_hh->torch.fbgemm_pack_quantized_matrix(self.weight_hh)
A:torch.jit.quantized.self.bias_ih->torch.nn.Parameter(other.bias_ih.clone(memory_format=torch.contiguous_format).float(), requires_grad=False)
A:torch.jit.quantized.self.bias_hh->torch.nn.Parameter(other.bias_hh.clone(memory_format=torch.contiguous_format).float(), requires_grad=False)
A:torch.jit.quantized.hx->self.permute_hidden(hx, sorted_indices)
A:torch.jit.quantized.ret->torch._VF.quantized_rnn_relu_cell(input, hx, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh, self.packed_ih, self.packed_hh, self.col_offsets_ih, self.col_offsets_hh, self.scale_ih, self.scale_hh, self.zero_point_ih, self.zero_point_hh)
A:torch.jit.quantized.zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.jit.quantized.weight_name->'weight_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.jit.quantized.bias_name->'bias_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.jit.quantized.weight->getattr(other, weight_name)
A:torch.jit.quantized.bias->getattr(other, bias_name)
A:torch.jit.quantized.(qweight, col_offsets, scale, zero_point)->torch.fbgemm_linear_quantize_weight(weight.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.packed_weight->torch.fbgemm_pack_gemm_matrix_fp16(weight.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.(ih_params, ih_param_names)->process_weights('ih', layer, suffix, dtype)
A:torch.jit.quantized.(hh_params, hh_param_names)->process_weights('hh', layer, suffix, dtype)
A:torch.jit.quantized.self._packed_weights->torch.jit.Attribute([getattr(self, weight) for weight in self._packed_weights_names], List[Tensor])
A:torch.jit.quantized.self._quantized_weights->torch.jit.Attribute([getattr(self, weight) for weight in self._quantized_weights_names], List[Tensor])
A:torch.jit.quantized.self._orig_weights->torch.jit.Attribute([getattr(self, weight) for weight in self._orig_weights_names], List[Tensor])
A:torch.jit.quantized.self.all_weights->torch.jit.Attribute([getattr(self, weight) for weight in self._all_weights_names], List[Tensor])
A:torch.jit.quantized.mini_batch->int(mini_batch)
A:torch.jit.quantized.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.jit.quantized.idx->self._quantized_weights_names.index(attr)
A:torch.jit.quantized.result->torch._VF.quantized_gru(input, batch_sizes, hx, self.all_weights, self.bias, self.num_layers, float(self.dropout), self.training, self.bidirectional)
A:torch.jit.quantized.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.jit.quantized.max_batch_size->int(max_batch_size)
A:torch.jit.quantized.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.jit.quantized.new_mod->quantize_rnn_modules(mod, dtype)
torch.jit.quantized.QuantizedGRU(QuantizedRNNBase)
torch.jit.quantized.QuantizedGRU.forward(self,input,hx=None)
torch.jit.quantized.QuantizedGRU.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.jit.quantized.QuantizedGRU.forward_packed(self,input,hx=None)
torch.jit.quantized.QuantizedGRU.forward_tensor(self,input,hx=None)
torch.jit.quantized.QuantizedGRUCell(self,other)
torch.jit.quantized.QuantizedGRUCell.__init__(self,other)
torch.jit.quantized.QuantizedGRUCell.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM(self,other,dtype)
torch.jit.quantized.QuantizedLSTM.__init__(self,other,dtype)
torch.jit.quantized.QuantizedLSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.jit.quantized.QuantizedLSTM.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.jit.quantized.QuantizedLSTM.forward_packed(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.forward_tensor(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.permute_hidden(self,hx,permutation)
torch.jit.quantized.QuantizedLSTMCell(self,other)
torch.jit.quantized.QuantizedLSTMCell.__init__(self,other)
torch.jit.quantized.QuantizedLSTMCell.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLinear(self,other)
torch.jit.quantized.QuantizedLinear.__init__(self,other)
torch.jit.quantized.QuantizedLinear._pack(self)
torch.jit.quantized.QuantizedLinear._unpack(self)
torch.jit.quantized.QuantizedLinear.extra_repr(self)
torch.jit.quantized.QuantizedLinear.forward(self,input)
torch.jit.quantized.QuantizedLinearFP16(self,other)
torch.jit.quantized.QuantizedLinearFP16.__init__(self,other)
torch.jit.quantized.QuantizedLinearFP16._pack(self)
torch.jit.quantized.QuantizedLinearFP16._unpack(self)
torch.jit.quantized.QuantizedLinearFP16.extra_repr(self)
torch.jit.quantized.QuantizedLinearFP16.forward(self,input)
torch.jit.quantized.QuantizedRNNBase(self,other,dtype=torch.int8)
torch.jit.quantized.QuantizedRNNBase.__init__(self,other,dtype=torch.int8)
torch.jit.quantized.QuantizedRNNBase.__setattr__(self,attr,value)
torch.jit.quantized.QuantizedRNNBase._pack(self)
torch.jit.quantized.QuantizedRNNBase._unpack(self)
torch.jit.quantized.QuantizedRNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.jit.quantized.QuantizedRNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.jit.quantized.QuantizedRNNBase.check_input(self,input,batch_sizes)
torch.jit.quantized.QuantizedRNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.jit.quantized.QuantizedRNNBase.permute_hidden(self,hx,permutation)
torch.jit.quantized.QuantizedRNNCell(self,other)
torch.jit.quantized.QuantizedRNNCell.__init__(self,other)
torch.jit.quantized.QuantizedRNNCell.forward(self,input,hx=None)
torch.jit.quantized.QuantizedRNNCellBase(self,other)
torch.jit.quantized.QuantizedRNNCellBase.__init__(self,other)
torch.jit.quantized.QuantizedRNNCellBase._pack(self)
torch.jit.quantized.QuantizedRNNCellBase._unpack(self)
torch.jit.quantized.QuantizedRNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.jit.quantized.QuantizedRNNCellBase.check_forward_input(self,input)
torch.jit.quantized.QuantizedRNNCellBase.extra_repr(self)
torch.jit.quantized.apply_permutation(tensor,permutation,dim=1)
torch.jit.quantized.quantize_linear_modules(module,dtype=torch.int8)
torch.jit.quantized.quantize_rnn_cell_modules(module)
torch.jit.quantized.quantize_rnn_modules(module,dtype=torch.int8)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/unsupported_tensor_ops.py----------------------------------------
A:torch.jit.unsupported_tensor_ops.tensor_attrs->set(filter(lambda x: x[0] != '_', dir(torch.Tensor)))
A:torch.jit.unsupported_tensor_ops.tensor->torch.tensor([2])
A:torch.jit.unsupported_tensor_ops.funcs_template->dedent('\n    def func(x):\n        return x.{op}()\n    ')
A:torch.jit.unsupported_tensor_ops.deprecated_apis->set(['volatile', 'resize', 'reinforce', 'new', 'name', 'map2_', 'has_names', 'grad_fn', 'resize_as'])
A:torch.jit.unsupported_tensor_ops.sorted_tensor_attrs->sorted(list(tensor_attrs), key=lambda x: x.lower())
A:torch.jit.unsupported_tensor_ops.funcs_str->dedent('\n    def func(x):\n        return x.{op}()\n    ').format(op=attr)
A:torch.jit.unsupported_tensor_ops.cu->torch.jit.CompilationUnit(funcs_str)
A:torch.jit.unsupported_tensor_ops.attr_repr->repr(getattr(tensor, attr))
A:torch.jit.unsupported_tensor_ops.methods->map(lambda x: '\t*  :meth:`~torch.Tensor.' + x + '`', methods)
A:torch.jit.unsupported_tensor_ops.properties->map(lambda x: '\t*  :attr:`~torch.Tensor.' + x + '`', properties)
A:torch.jit.unsupported_tensor_ops.(methods, properties)->_gen_unsupported_methods_properties()
A:torch.jit.unsupported_tensor_ops.__doc__->_list_unsupported_tensor_ops()
torch.jit.unsupported_tensor_ops._gen_unsupported_methods_properties()
torch.jit.unsupported_tensor_ops._list_unsupported_tensor_ops()
torch.jit.unsupported_tensor_ops.execWrapper(code,glob,loc)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/__init__.py----------------------------------------
A:torch.jit.__init__.value->os.environ.get(name)
A:torch.jit.__init__._enabled->_parse_env('PYTORCH_JIT', True, '> Using PyTorch JIT', '> PyTorch JIT DISABLED')
A:torch.jit.__init__._python_cu->torch._C.CompilationUnit()
A:torch.jit.__init__.Attribute->collections.namedtuple('Attribute', ['value', 'type'])
A:torch.jit.__init__.stored_flag->torch._C._get_graph_executor_optimize()
A:torch.jit.__init__.DEFAULT_EXTRA_FILES_MAP->torch._C.ExtraFilesMap()
A:torch.jit.__init__.ret->m.save_to_buffer(_extra_files=_extra_files)
A:torch.jit.__init__.map_location->torch.device(map_location)
A:torch.jit.__init__.cu->torch._C.CompilationUnit()
A:torch.jit.__init__.cpp_module->torch._C.import_ir_module_from_buffer(cu, f.read(), map_location, _extra_files)
A:torch.jit.__init__.outs->wrap_retval(mod(*_clone_inputs(inputs)))
A:torch.jit.__init__.state_dict->make_module(mod, _module_class, _compilation_unit).state_dict(keep_vars=True)
A:torch.jit.__init__.filtered_dict->type(state_dict)()
A:torch.jit.__init__.seen_ids->set()
A:torch.jit.__init__.filtered_dict[k]->a.detach().clone(memory_format=torch.preserve_format).requires_grad_(a.requires_grad).detach()
A:torch.jit.__init__.frame->inspect.currentframe()
A:torch.jit.__init__.(in_vars, in_desc)->_flatten(args)
A:torch.jit.__init__.module_state->list(_unique_state_dict(self, keep_vars=True).values())
A:torch.jit.__init__.trace_inputs->_unflatten(args[:len(in_vars)], in_desc)
A:torch.jit.__init__.(out_vars, _)->_flatten(out)
A:torch.jit.__init__.(graph, out)->torch._C._create_graph_by_tracing(wrapper, in_vars + module_state, _create_interpreter_name_lookup_fn(), self._force_outplace)
A:torch.jit.__init__.v->a.detach().clone(memory_format=torch.preserve_format).requires_grad_(a.requires_grad)
A:torch.jit.__init__.v.grad->clone_input(v.grad)
A:torch.jit.__init__._JIT_TIME->os.environ.get('PYTORCH_JIT_TIME', False)
A:torch.jit.__init__._JIT_DISABLE->os.environ.get('PYTORCH_JIT_DISABLE', False)
A:torch.jit.__init__._JIT_STATS->os.environ.get('PYTORCH_JIT_STATS', False)
A:torch.jit.__init__.stream->torch.cuda.current_stream()
A:torch.jit.__init__.start->torch.cuda.Event(enable_timing=True)
A:torch.jit.__init__.end->torch.cuda.Event(enable_timing=True)
A:torch.jit.__init__.is_module->isinstance(model, Module)
A:torch.jit.__init__.saved_args->_clone_inputs(args)
A:torch.jit.__init__.saved_state->copy.deepcopy(model.state_dict())
A:torch.jit.__init__.(in_vars, _)->_flatten((args, params))
A:torch.jit.__init__.out->model(*args)
A:torch.jit.__init__.loss->loss_fn(*out)
A:torch.jit.__init__.grads->torch.autograd.grad([loss], in_vars)
A:torch.jit.__init__.(uncompiled_outs, uncompiled_grads)->run_fwd_bwd(args, force_trace=True)
A:torch.jit.__init__.(compiled_outs, compiled_grads)->run_fwd_bwd(args, assert_compiled=True)
A:torch.jit.__init__.copied_dict[name]->_clone_inputs(data)
A:torch.jit.__init__.check_mod->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, _force_outplace=force_outplace, _module_class=_module_class)
A:torch.jit.__init__.check_mod_func->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, _force_outplace=force_outplace, _module_class=_module_class)._c._get_method(traced_func.name)
A:torch.jit.__init__.mod_canonicalized->torch._C._jit_pass_canonicalize(traced_func.graph)
A:torch.jit.__init__.mod_str->re.sub('___torch_mangle_[0-9]+\\.', '', mod_str)
A:torch.jit.__init__.check_canonicalized->torch._C._jit_pass_canonicalize(check_mod_func.graph)
A:torch.jit.__init__.check_str->re.sub('___torch_mangle_[0-9]+\\.', '', check_str)
A:torch.jit.__init__.graph_diff->difflib.ndiff(mod_str.splitlines(True), check_str.splitlines(True))
A:torch.jit.__init__.node_diff->difflib.ndiff(str(n_mod).splitlines(True), str(n_check).splitlines(True))
A:torch.jit.__init__.mod_stack->n_mod.sourceRange()
A:torch.jit.__init__.check_stack->n_check.sourceRange()
A:torch.jit.__init__.mod_tensor_val->n_mod.t('value')
A:torch.jit.__init__.check_tensor_val->n_check.t('value')
A:torch.jit.__init__.compare_stack->n_mod.sourceRange()
A:torch.jit.__init__.orig->orig.dequantize().dequantize()
A:torch.jit.__init__.ref->ref.dequantize().dequantize()
A:torch.jit.__init__.traced_outs->run_mod_and_filter_tensor_outputs(traced_func, inputs, 'trace')
A:torch.jit.__init__.fn_outs->run_mod_and_filter_tensor_outputs(func, inputs, 'Python function')
A:torch.jit.__init__.check_outs->run_mod_and_filter_tensor_outputs(check_mod_func, inputs, 'repeated trace')
A:torch.jit.__init__.diag_info->graph_diagnostic_info()
A:torch.jit.__init__.item->getattr(mod, name, None)
A:torch.jit.__init__.example_inputs->make_tuple(example_inputs)
A:torch.jit.__init__.var_lookup_fn->_create_interpreter_name_lookup_fn(0)
A:torch.jit.__init__.name->_qualified_name(func)
A:torch.jit.__init__.traced->torch._C._create_function_from_trace(name, func, example_inputs, var_lookup_fn, _force_outplace)
A:torch.jit.__init__.module->make_module(mod, _module_class, _compilation_unit)
A:torch.jit.__init__.check_trace_method->make_module(mod, _module_class, _compilation_unit)._c._get_method(method_name)
A:torch.jit.__init__.self._c->torch._C.CompilationUnit()
A:torch.jit.__init__.rcb->torch._jit_internal.createResolutionCallbackFromFrame(frames_up=1)
A:torch.jit.__init__.r->self._python_modules.items()
A:torch.jit.__init__.hooks->torch._C._jit_get_emit_hooks()
A:torch.jit.__init__.module_name->getattr(obj, '__module__', None)
A:torch.jit.__init__.ast->torch._C._parse_source_def(src)
A:torch.jit.__init__.qualified_name->_qualified_name(obj)
A:torch.jit.__init__._rcb->torch._jit_internal.createResolutionCallbackFromClosure(impl_fn)
A:torch.jit.__init__.maybe_already_compiled_fn->_try_get_jit_cached_function(obj)
A:torch.jit.__init__.fn->torch._C._jit_script_compile_overload(qual_name, overload_decl, impl_ast, _rcb, implementation_defaults, overload_signature)
A:torch.jit.__init__.cls._constants_set->type(module)._constants_set.union(base_constants)
A:torch.jit.__init__.base_constants->getattr(base, '_constants_set', set())
A:torch.jit.__init__.original_init->getattr(cls, '__init__', lambda self: None)
A:torch.jit.__init__.cls->type(module)
A:torch.jit.__init__.self.__dict__['_actual_script_module']->torch.jit._recursive.create_script_module(self, make_stubs)
A:torch.jit.__init__.forward->_CachedForward()
A:torch.jit.__init__.self._methods[ast.name().name]->ScriptMethodStub(rcb, ast, None)
A:torch.jit.__init__.script_module->torch.jit._recursive.create_script_module(tmp_module, lambda module: (), share_types=False)
A:torch.jit.__init__.script_module._parameters->OrderedDictWrapper(torch._C.ParameterDict(script_module._c))
A:torch.jit.__init__.script_module._buffers->OrderedDictWrapper(torch._C.BufferDict(script_module._c))
A:torch.jit.__init__.script_module._modules->OrderedModuleDict(script_module._c, script_module._modules)
A:torch.jit.__init__.script_method->self._c._get_method(attr)
A:torch.jit.__init__.self_method->getattr(self, method_name)
A:torch.jit.__init__.id_set->set()
A:torch.jit.__init__.QualnameWrapper._jit_override_qualname->torch._jit_internal._qualified_name(type(orig))
A:torch.jit.__init__.tmp_module->QualnameWrapper()
A:torch.jit.__init__.tmp_module._modules[name]->make_module(submodule, TracedModule, _compilation_unit=None)
A:torch.jit.__init__._jit_caching_layer->weakref.WeakKeyDictionary()
A:torch.jit.__init__._jit_function_overload_caching->weakref.WeakKeyDictionary()
A:torch.jit.__init__.qual_names->weakref.WeakKeyDictionary().get(key, None)
A:torch.jit.__init__.qual_name->_qualified_name(obj)
A:torch.jit.__init__.overload_decl->torch.jit.get_jit_def(overload_fn).decl()
A:torch.jit.__init__.overload_signature->torch.jit.annotations.get_signature(overload_fn, None, None, inspect.ismethod(overload_fn))
A:torch.jit.__init__.impl_ast->torch.jit.get_jit_def(impl_fn)
A:torch.jit.__init__.overload_defaults->get_default_args(overload_fn)
A:torch.jit.__init__.implementation_defaults->get_default_args(impl_fn)
A:torch.jit.__init__.existing_compiled_fns->_try_get_jit_cached_overloads(obj)
A:torch.jit.__init__.uncompiled_overloads->torch._jit_internal._get_fn_overloads(qual_name)
A:torch.jit.__init__.fields->list(obj._fields)
A:torch.jit.__init__.has_annotations->hasattr(obj, '__annotations__')
A:torch.jit.__init__.the_type->torch.jit.annotations.ann_to_type(obj.__annotations__[field], _jit_internal.fake_range())
A:torch.jit.__init__.TupleType->collections.namedtuple(unqual_name, field_names)
A:torch.jit.__init__.self.state->torch._C._get_tracing_state()
torch.jit.__init__.CompilationUnit(self,lang=None,_frames_up=0)
torch.jit.__init__.CompilationUnit.__getattr__(self,attr)
torch.jit.__init__.CompilationUnit.__init__(self,lang=None,_frames_up=0)
torch.jit.__init__.CompilationUnit.define(self,lang,rcb=None,_frames_up=0)
torch.jit.__init__.ONNXTracedModule(self,inner,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit.__init__.ONNXTracedModule.__init__(self,inner,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit.__init__.ONNXTracedModule.forward(self,*args)
torch.jit.__init__.OrderedDictWrapper(self,_c)
torch.jit.__init__.OrderedDictWrapper.__contains__(self,k)
torch.jit.__init__.OrderedDictWrapper.__delitem__(self,k)
torch.jit.__init__.OrderedDictWrapper.__getitem__(self,k)
torch.jit.__init__.OrderedDictWrapper.__init__(self,_c)
torch.jit.__init__.OrderedDictWrapper.__len__(self)
torch.jit.__init__.OrderedDictWrapper.__setitem__(self,k,v)
torch.jit.__init__.OrderedDictWrapper.items(self)
torch.jit.__init__.OrderedDictWrapper.keys(self)
torch.jit.__init__.OrderedDictWrapper.values(self)
torch.jit.__init__.OrderedModuleDict(self,module,python_dict)
torch.jit.__init__.OrderedModuleDict.__contains__(self,k)
torch.jit.__init__.OrderedModuleDict.__getitem__(self,k)
torch.jit.__init__.OrderedModuleDict.__init__(self,module,python_dict)
torch.jit.__init__.OrderedModuleDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedModuleDict.items(self)
torch.jit.__init__.ScriptMeta(cls,name,bases,attrs)
torch.jit.__init__.ScriptMeta.__init__(cls,name,bases,attrs)
torch.jit.__init__.ScriptWarning(Warning)
torch.jit.__init__.TracedModule(self,orig,id_set=None,_compilation_unit=None)
torch.jit.__init__.TracedModule.__getattr__(self,attr)
torch.jit.__init__.TracedModule.__init__(self,orig,id_set=None,_compilation_unit=None)
torch.jit.__init__.TracedModule.__setattr__(self,attr,value)
torch.jit.__init__.TracedModule._get_name(self)
torch.jit.__init__.TracedModule.extra_repr(self)
torch.jit.__init__.TracedModule.forward(self,*args,**kwargs)
torch.jit.__init__.TracerWarning(Warning)
torch.jit.__init__.TracerWarning.ignore_lib_warnings()
torch.jit.__init__.TracingCheckError(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit.__init__.TracingCheckError.__init__(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit.__init__._add_script_class(cls,name)
torch.jit.__init__._check_directly_compile_overloaded(obj)
torch.jit.__init__._check_overload_defaults(impl_defaults,overload_defaults,loc)
torch.jit.__init__._check_trace(check_inputs,func,traced_func,check_tolerance,force_outplace,is_trace_module,_module_class)
torch.jit.__init__._clone_inputs(args)
torch.jit.__init__._compile_and_register_class(obj,rcb,qualified_name)
torch.jit.__init__._compile_function_with_overload(overload_fn,qual_name,impl_fn)
torch.jit.__init__._create_interpreter_name_lookup_fn(frames_up=1)
torch.jit.__init__._create_named_tuple(t,unqual_name,field_names)
torch.jit.__init__._disable_emit_hooks()
torch.jit.__init__._disable_tracing(object)
torch.jit.__init__._disable_tracing.__enter__(self)
torch.jit.__init__._disable_tracing.__exit__(self,*args)
torch.jit.__init__._get_named_tuple_properties(obj)
torch.jit.__init__._get_overloads(obj)
torch.jit.__init__._get_script_class(name)
torch.jit.__init__._get_trace_graph(f,args=(),kwargs=None,_force_outplace=False,return_inputs=False,_return_inputs_states=False)
torch.jit.__init__._graph_for(self,*args,**kwargs)
torch.jit.__init__._is_new_style_class(cls)
torch.jit.__init__._parse_env(name,default,true_message,false_message)
torch.jit.__init__._set_jit_function_cache(key,value)
torch.jit.__init__._set_jit_overload_cache(key,compiled_fns)
torch.jit.__init__._time(trace_name,name,time=True)
torch.jit.__init__._try_get_dispatched_fn(fn)
torch.jit.__init__._try_get_jit_cached_function(key)
torch.jit.__init__._try_get_jit_cached_overloads(key)
torch.jit.__init__._try_get_overloaded_fn(mod,field)
torch.jit.__init__._unique_state_dict(module,keep_vars=False)
torch.jit.__init__._unwrap_optional(x)
torch.jit.__init__._verify_equal(xs,ys)
torch.jit.__init__.annotate(the_type,the_value)
torch.jit.__init__.export_opnames(m)
torch.jit.__init__.indent(s)
torch.jit.__init__.interface(obj)
torch.jit.__init__.is_scripting()
torch.jit.__init__.load(f,map_location=None,_extra_files=DEFAULT_EXTRA_FILES_MAP)
torch.jit.__init__.make_module(mod,_module_class,_compilation_unit)
torch.jit.__init__.make_tuple(example_inputs)
torch.jit.__init__.optimized_execution(should_optimize)
torch.jit.__init__.save(m,f,_extra_files=DEFAULT_EXTRA_FILES_MAP)
torch.jit.__init__.script(obj,optimize=None,_frames_up=0,_rcb=None)
torch.jit.__init__.script_method(fn)
torch.jit.__init__.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit.__init__.trace_module(mod,inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit.__init__.verify(model,args,loss_fn=torch.sum,devices=None)
torch.jit.__init__.whichmodule(obj)
torch.jit.__init__.wrap_check_inputs(check_inputs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/_builtins.py----------------------------------------
A:torch.jit._builtins._functional_registered_ops->_gen_torch_functional_registered_ops()
A:torch.jit._builtins.v->getattr(mod, name)
torch.jit._builtins._find_builtin(fn)
torch.jit._builtins._gen_torch_functional_registered_ops()
torch.jit._builtins._get_builtin_table()
torch.jit._builtins._is_special_functional_bound_op(fn)
torch.jit._builtins._register_builtin(fn,op)
torch.jit._find_builtin(fn)
torch.jit._get_builtin_table()
torch.jit._register_builtin(fn,op)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/jit/_logging.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_opset8.py----------------------------------------
A:torch.onnx.symbolic_opset8.vars()[black_listed_op]->_black_list_in_opset(black_listed_op)
A:torch.onnx.symbolic_opset8.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset8.align_corners->torch.onnx.symbolic_helper._maybe_get_const(align_corners, 'b')
A:torch.onnx.symbolic_opset8.output_size->torch.onnx.symbolic_helper._maybe_get_const(output_size, 'is')
A:torch.onnx.symbolic_opset8.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset8.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset8.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset8.(scales, mode)->torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g, input, size, scale_factor, mode, align_corners)
A:torch.onnx.symbolic_opset8.arg0_type->args[0].type().scalarType()
A:torch.onnx.symbolic_opset8.args->tuple((_cast_Float(g, arg, False) for arg in args))
A:torch.onnx.symbolic_opset8.other->torch.onnx.symbolic_helper._if_scalar_type_as(g, other, input)
A:torch.onnx.symbolic_opset8.(_, input, other)->_try_cast_integer_to_float(g, input, other)
A:torch.onnx.symbolic_opset8.(old_type, self, other)->_try_cast_integer_to_float(g, self, other)
A:torch.onnx.symbolic_opset8.self_sizes->self.type().sizes()
A:torch.onnx.symbolic_opset8.weight->g.op('Unsqueeze', weight, axes_i=list(range(1, len(self_sizes) - 1)))
A:torch.onnx.symbolic_opset8.(old_type, self, weight)->_try_cast_integer_to_float(g, self, weight)
A:torch.onnx.symbolic_opset8.ty->torch.onnx.symbolic_helper._try_get_scalar_type(self, other).lower()
A:torch.onnx.symbolic_opset8.C->g.constant(0, [1], ty)
A:torch.onnx.symbolic_opset8.(old_type, self, other, C)->_try_cast_integer_to_float(g, self, other, C)
A:torch.onnx.symbolic_opset8.(old_type, self, mat1, mat2)->_try_cast_integer_to_float(g, self, mat1, mat2)
A:torch.onnx.symbolic_opset8.size->torch.onnx.symbolic_helper._maybe_get_const(size, 'is')
A:torch.onnx.symbolic_opset8.(old_type, self)->_try_cast_integer_to_float(g, self)
A:torch.onnx.symbolic_opset8.shape->g.op('Shape', input)
A:torch.onnx.symbolic_opset8.start_dim_i->torch.onnx.symbolic_helper._get_const(start_dim, 'i', 'start_dim')
A:torch.onnx.symbolic_opset8.end_dim_i->torch.onnx.symbolic_helper._get_const(end_dim, 'i', 'end_dim')
A:torch.onnx.symbolic_opset8.dim->input.type().dim()
A:torch.onnx.symbolic_opset8.(old_type, input)->_try_cast_integer_to_float(g, input)
A:torch.onnx.symbolic_opset8.result->g.op('ConstantFill', sizes, dtype_i=sym_help.cast_pytorch_to_onnx['Float'], input_as_shape_i=1, value_f=const_value)
A:torch.onnx.symbolic_opset8.const_value->torch.onnx.symbolic_helper._maybe_get_const(value, 't')
A:torch.onnx.symbolic_opset8.tmp->zeros(g, sizes, dtype, layout, device)
A:torch.onnx.symbolic_opset8.dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
torch.onnx.symbolic_opset8.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor)
torch.onnx.symbolic_opset8._cast_to_type(g,input,to_type)
torch.onnx.symbolic_opset8._comparison_operator(g,input,other,op_name)
torch.onnx.symbolic_opset8._constant_fill(g,sizes,dtype,const_value)
torch.onnx.symbolic_opset8._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset8._try_cast_integer_to_float(g,*args)
torch.onnx.symbolic_opset8.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic_opset8.bmm(g,self,other)
torch.onnx.symbolic_opset8.empty(g,sizes,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.empty_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset8.full(g,sizes,value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.full_like(g,input,fill_value,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.gt(g,input,other)
torch.onnx.symbolic_opset8.lt(g,input,other)
torch.onnx.symbolic_opset8.matmul(g,self,other)
torch.onnx.symbolic_opset8.mm(g,self,other)
torch.onnx.symbolic_opset8.ones(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.ones_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.prelu(g,self,weight)
torch.onnx.symbolic_opset8.view(g,self,size)
torch.onnx.symbolic_opset8.zeros(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.zeros_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/utils.py----------------------------------------
A:torch.onnx.utils.output_type->node.output().type()
A:torch.onnx.utils.lc->g.create('prim::ListConstruct', inputs).insertBefore(node).output().setType(ListType.ofTensors())
A:torch.onnx.utils.graph->_assign_output_shapes(graph, out_vars)
A:torch.onnx.utils.val_use_external_data_format->_resolve_args_by_export_type('use_external_data_format', use_external_data_format, operator_export_type)
A:torch.onnx.utils.(trace_graph, torch_out, inputs_states)->torch.jit._get_trace_graph(model, args, _force_outplace=False, _return_inputs_states=True)
A:torch.onnx.utils.trace_graph->_optimize_graph(trace_graph, operator_export_type)
A:torch.onnx.utils.orig_state_dict_keys->_unique_state_dict(model).keys()
A:torch.onnx.utils.(method_graph, params)->torch._C._jit_pass_lower_graph(model.forward.graph, model._c)
A:torch.onnx.utils.(in_vars, in_desc)->torch.jit._flatten(tuple(args))
A:torch.onnx.utils.(graph, torch_out)->_trace_and_get_graph_from_model(model, args, training)
A:torch.onnx.utils.state_dict->_unique_state_dict(model)
A:torch.onnx.utils.params->list(state_dict.values())
A:torch.onnx.utils.graph_inputs->list(graph.inputs())
A:torch.onnx.utils.param_names->list(state_dict.keys())
A:torch.onnx.utils.params_dict->torch._C._jit_pass_onnx_constant_fold(graph, params_dict, _export_onnx_opset_version)
A:torch.onnx.utils.(out_vars, _)->torch.jit._flatten(tuple(example_outputs))
A:torch.onnx.utils.(output_tensors, _)->torch._C._jit_flatten(torch_out)
A:torch.onnx.utils.(flatten_args, _)->torch._C._jit_flatten(args)
A:torch.onnx.utils.val_keep_init_as_ip->_decide_keep_init_as_input(keep_initializers_as_inputs, operator_export_type, opset_version)
A:torch.onnx.utils.val_add_node_names->_decide_add_node_names(add_node_names, operator_export_type)
A:torch.onnx.utils.val_do_constant_folding->_decide_constant_folding(do_constant_folding, operator_export_type)
A:torch.onnx.utils.(graph, params_dict, torch_out)->_model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, val_do_constant_folding, fixed_batch_size=fixed_batch_size)
A:torch.onnx.utils.(val_use_external_data_format, model_file_location)->_decide_external_data_format(use_external_data_format, operator_export_type, f)
A:torch.onnx.utils.(proto, export_map)->_assign_output_shapes(graph, out_vars)._export_onnx({}, opset_version, dynamic_axes, False, operator_export_type, strip_doc_string, val_keep_init_as_ip, custom_opsets, val_add_node_names, val_use_external_data_format, model_file_location)
A:torch.onnx.utils.model_proto_file->os.path.join(f, ONNX_ARCHIVE_MODEL_PROTO_NAME)
A:torch.onnx.utils.weight_proto_file->os.path.join(f, k)
A:torch.onnx.utils.attr_pattern->re.compile('^(.+)_([ifstgz])$')
A:torch.onnx.utils.m->re.compile('^(.+)_([ifstgz])$').match(key)
A:torch.onnx.utils.value->_scalar(value)
A:torch.onnx.utils.aten->dict(((k, v) for (k, v) in kwargs.items() if v is not None)).pop('aten', False)
A:torch.onnx.utils.n->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs))
A:torch.onnx.utils.outputs->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).outputsSize()
A:torch.onnx.utils.kwargs->dict(((k, v) for (k, v) in kwargs.items() if v is not None))
A:torch.onnx.utils.args->list((const_if_tensor(arg) for arg in raw_args))
A:torch.onnx.utils.ns_op_name->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).kind()
A:torch.onnx.utils.(ns, op_name)->symbolic_name.split('::')
A:torch.onnx.utils.is_exportable_aten_op->torch.onnx.symbolic_registry.is_registered_op(op_name, '', opset_version)
A:torch.onnx.utils.op_fn->torch.onnx.symbolic_registry.get_registered_op(op_name, domain, opset_version)
A:torch.onnx.utils.new_op_outputs->g.op(op_name, *inputs, outputs=n.outputsSize())
A:torch.onnx.utils.new_block->new_node.addBlock()
A:torch.onnx.utils.is_exportable->torch.onnx.symbolic_registry.is_registered_op(symbolic_name, '', opset_version)
A:torch.onnx.utils.symbolic_fn->torch.onnx.symbolic_registry.get_registered_op(op_name, ns, opset_version)
A:torch.onnx.utils.type->type.lower().lower()
A:torch.onnx.utils.tensor->torch.DoubleTensor(*dims)
A:torch.onnx.utils.sel->self.kindOf(k)
A:torch.onnx.utils.valid_names->set((input_names or []) + (output_names or []))
torch.onnx.utils._add_attribute(node,key,value,aten)
torch.onnx.utils._decide_add_node_names(add_node_names,operator_export_type)
torch.onnx.utils._decide_constant_folding(do_constant_folding,operator_export_type)
torch.onnx.utils._decide_external_data_format(use_external_data_format,operator_export_type,f)
torch.onnx.utils._decide_keep_init_as_input(keep_initializers_as_inputs,operator_export_type,opset_version)
torch.onnx.utils._export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=None,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,opset_version=None,_retain_param_name=False,do_constant_folding=True,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,fixed_batch_size=False,custom_opsets=None,add_node_names=True,enable_onnx_checker=True,use_external_data_format=False)
torch.onnx.utils._export_to_pretty_string(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,google_printer=False,opset_version=None,_retain_param_name=False,do_constant_folding=True,keep_initializers_as_inputs=None,fixed_batch_size=False,custom_opsets=None,add_node_names=True)
torch.onnx.utils._graph_at(g,opname,*args,**kwargs)
torch.onnx.utils._graph_constant(g,value,dims,type,*args,**kwargs)
torch.onnx.utils._graph_op(g,opname,*raw_args,**kwargs)
torch.onnx.utils._is_onnx_list(value)
torch.onnx.utils._model_to_graph(model,args,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,example_outputs=None,propagate=False,_retain_param_name=False,do_constant_folding=True,_disable_torch_constant_prop=False,fixed_batch_size=False)
torch.onnx.utils._newNode(g,opname,outputs,*args,**kwargs)
torch.onnx.utils._node_getitem(self,k)
torch.onnx.utils._optimize_graph(graph,operator_export_type,_disable_torch_constant_prop=False,fixed_batch_size=False,params_dict=None)
torch.onnx.utils._resolve_args_by_export_type(arg_name,arg_value,operator_export_type)
torch.onnx.utils._run_symbolic_function(g,n,inputs,env,operator_export_type=OperatorExportTypes.ONNX)
torch.onnx.utils._run_symbolic_method(op_name,symbolic_fn,args)
torch.onnx.utils._scalar(x)
torch.onnx.utils._set_input_and_output_names(graph,input_names,output_names)
torch.onnx.utils._split_tensor_list_constants(g,block)
torch.onnx.utils._trace(func,args,operator_export_type,return_outs=False)
torch.onnx.utils._trace_and_get_graph_from_model(model,args,training)
torch.onnx.utils._validate_dynamic_axes(dynamic_axes,model,input_names,output_names)
torch.onnx.utils.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)
torch.onnx.utils.export_to_pretty_string(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,google_printer=False,opset_version=None,_retain_param_name=True,keep_initializers_as_inputs=None,custom_opsets=None,add_node_names=True,do_constant_folding=True)
torch.onnx.utils.is_in_onnx_export()
torch.onnx.utils.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)
torch.onnx.utils.set_training(model,mode)
torch.onnx.utils.warn_on_static_input_change(input_states)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_helper.py----------------------------------------
A:torch.onnx.symbolic_helper.value_t->_maybe_get_const(value, 't')
A:torch.onnx.symbolic_helper.list_node->list_value.node()
A:torch.onnx.symbolic_helper.wrapper->wraps(fn)(wrapper)
A:torch.onnx.symbolic_helper.scalar_type->tensor.type().scalarType()
A:torch.onnx.symbolic_helper.ty->tensor.type().scalarType().lower()
A:torch.onnx.symbolic_helper.type->scalar_type_to_pytorch_type.index(torch.get_default_dtype())
A:torch.onnx.symbolic_helper.shape_->g.op('Shape', input)
A:torch.onnx.symbolic_helper.dim_size_->g.op('Gather', shape_, g.op('Constant', value_t=torch.tensor([dim], dtype=torch.int64)))
A:torch.onnx.symbolic_helper.k->g.op('Reshape', k, g.op('Constant', value_t=torch.tensor([1])))
A:torch.onnx.symbolic_helper.output_size->_maybe_get_const(output_size, 'is')
A:torch.onnx.symbolic_helper.offsets->g.op('Constant', value_t=torch.ones(2, dtype=torch.float32))
A:torch.onnx.symbolic_helper.dividend->g.op('Cast', output_size, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.divisor->g.op('Cast', divisor, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.scale_dims->g.op('Div', dividend, divisor)
A:torch.onnx.symbolic_helper.scales->_interpolate_get_scales_if_available(g, scales)
A:torch.onnx.symbolic_helper.unsqueezed_scale->g.op('Cast', unsqueezed_scale, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.scale_factor->_interpolate_size_to_scales(g, input, size, dim)
A:torch.onnx.symbolic_helper.mode->_maybe_get_const(mode, 's')
A:torch.onnx.symbolic_helper.align_corners->_maybe_get_const(align_corners, 'b')
A:torch.onnx.symbolic_helper.dim->input.type().dim()
A:torch.onnx.symbolic_helper.size->g.op('Concat', *size, axis_i=0)
A:torch.onnx.symbolic_helper.full_shape->g.op('Shape', self)
A:torch.onnx.symbolic_helper.self_dim->self.type().dim()
A:torch.onnx.symbolic_helper.dim_value->_parse_arg(dim, 'i')
A:torch.onnx.symbolic_helper.unsqueezed_index->g.op('Unsqueeze', index, axes_i=[i for i in range(self_dim) if i != dim_value])
A:torch.onnx.symbolic_helper.expanded_index_shape->scatter(g, g.op('Shape', self), 0, g.op('Unsqueeze', dim, axes_i=[0]), g.op('Shape', index))
A:torch.onnx.symbolic_helper.expanded_index->expand(g, unsqueezed_index, expanded_index_shape, None)
A:torch.onnx.symbolic_helper.padding->tuple(tuple_fn(padding))
A:torch.onnx.symbolic_helper._quantized_ops->set()
torch.onnx.symbolic_helper._arange_cast_helper(g,end,start=None,step=None,dtype=None)
torch.onnx.symbolic_helper._avgpool_helper(tuple_fn,padding,kernel_size,stride,divisor_override,name)
torch.onnx.symbolic_helper._black_list_in_opset(name)
torch.onnx.symbolic_helper._cast_func_template(to_i,g,input,non_blocking)
torch.onnx.symbolic_helper._get_const(value,desc,arg_name)
torch.onnx.symbolic_helper._get_interpolate_attributes(g,mode,args)
torch.onnx.symbolic_helper._if_scalar_type_as(g,self,tensor)
torch.onnx.symbolic_helper._index_fill_reshape_helper(g,self,dim,index)
torch.onnx.symbolic_helper._interpolate_get_scales(g,scale_factor,dim)
torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g,input,size,scale_factor,mode,align_corners)
torch.onnx.symbolic_helper._interpolate_get_scales_if_available(g,scales)
torch.onnx.symbolic_helper._interpolate_size_to_scales(g,input,output_size,dim)
torch.onnx.symbolic_helper._interpolate_warning(interpolate_mode)
torch.onnx.symbolic_helper._is_fp(value)
torch.onnx.symbolic_helper._is_none(x)
torch.onnx.symbolic_helper._is_packed_list(list_value)
torch.onnx.symbolic_helper._is_tensor_list(x)
torch.onnx.symbolic_helper._is_value(x)
torch.onnx.symbolic_helper._maybe_get_const(value,desc)
torch.onnx.symbolic_helper._maybe_get_scalar(value)
torch.onnx.symbolic_helper._parse_arg(value,desc)
torch.onnx.symbolic_helper._scalar(x)
torch.onnx.symbolic_helper._scatter_helper(g,self,dim,index,src)
torch.onnx.symbolic_helper._set_operator_export_type(operator_export_type)
torch.onnx.symbolic_helper._set_opset_version(opset_version)
torch.onnx.symbolic_helper._size_helper(g,self,dim)
torch.onnx.symbolic_helper._slice_helper(g,input,axes,starts,ends,steps=None,dynamic_slice=False)
torch.onnx.symbolic_helper._sort_helper(g,input,dim,decending=True,out=None)
torch.onnx.symbolic_helper._topk_helper(g,input,k,dim,largest=True,sorted=False,out=None)
torch.onnx.symbolic_helper._try_get_scalar_type(*args)
torch.onnx.symbolic_helper._unimplemented(op,msg)
torch.onnx.symbolic_helper._unpack_list(list_value)
torch.onnx.symbolic_helper._unsqueeze_helper(g,input,dim)
torch.onnx.symbolic_helper.parse_args(*arg_descriptors)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_registry.py----------------------------------------
A:torch.onnx.symbolic_registry.module->importlib.import_module('torch.onnx.symbolic_opset{}'.format(opset_version))
A:torch.onnx.symbolic_registry.version_ops->get_ops_in_version(iter_version)
A:torch.onnx.symbolic_registry.supported_version->get_op_supported_version(opname, domain, version)
torch.onnx.symbolic_registry.get_op_supported_version(opname,domain,version)
torch.onnx.symbolic_registry.get_ops_in_version(version)
torch.onnx.symbolic_registry.get_registered_op(opname,domain,version)
torch.onnx.symbolic_registry.is_registered_op(opname,domain,version)
torch.onnx.symbolic_registry.is_registered_version(domain,version)
torch.onnx.symbolic_registry.register_op(opname,op,domain,version)
torch.onnx.symbolic_registry.register_ops_helper(domain,version,iter_version)
torch.onnx.symbolic_registry.register_ops_in_version(domain,version)
torch.onnx.symbolic_registry.register_version(domain,version)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_opset11.py----------------------------------------
A:torch.onnx.symbolic_opset11.dtype->_get_arange_dtype(args[3])
A:torch.onnx.symbolic_opset11.min_val->g.op('Constant', value_t=torch.tensor(min_val, dtype=sym_help.scalar_type_to_pytorch_type[dtype]))
A:torch.onnx.symbolic_opset11.max_val->g.op('Constant', value_t=torch.tensor(max_val, dtype=sym_help.scalar_type_to_pytorch_type[dtype]))
A:torch.onnx.symbolic_opset11.min->_cast_if_not_none(min, dtype)
A:torch.onnx.symbolic_opset11.max->_cast_if_not_none(max, dtype)
A:torch.onnx.symbolic_opset11.indices_list->torch.onnx.symbolic_helper._unpack_list(indices_list_value)
A:torch.onnx.symbolic_opset11.accumulate->torch.onnx.symbolic_helper._parse_arg(accumulate, 'b')
A:torch.onnx.symbolic_opset11.index->nonzero(g, expand_as(g, mask, self))
A:torch.onnx.symbolic_opset11.broadcast_index_shape->g.op('Shape', index)
A:torch.onnx.symbolic_opset11.sub_data_shape->torch.onnx.symbolic_helper._slice_helper(g, g.op('Shape', self), axes=[0], starts=[len(indices_list)], ends=[maxsize])
A:torch.onnx.symbolic_opset11.values_shape->g.op('Concat', broadcast_index_shape, sub_data_shape, axis_i=0)
A:torch.onnx.symbolic_opset11.values->g.op('Reshape', values, values_shape)
A:torch.onnx.symbolic_opset11.zeros->g.op('ConstantOfShape', g.op('Shape', self), value_t=torch.tensor([0], dtype=dtype))
A:torch.onnx.symbolic_opset11.result->g.op('ScatterND', self, index, values)
A:torch.onnx.symbolic_opset11.dims->self.type().sizes()
A:torch.onnx.symbolic_opset11.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset11.align_corners->torch.onnx.symbolic_helper._maybe_get_const(align_corners, 'b')
A:torch.onnx.symbolic_opset11.empty_tensor->g.op('Constant', value_t=torch.tensor([], dtype=torch.float32))
A:torch.onnx.symbolic_opset11.input_size->torch.onnx.symbolic_helper._slice_helper(g, input_size, axes=[0], ends=[2], starts=[0])
A:torch.onnx.symbolic_opset11.input_size_beg->torch.onnx.symbolic_helper._slice_helper(g, input_size, axes=[0], ends=[2], starts=[0])
A:torch.onnx.symbolic_opset11.output_size->g.op('Concat', input_size_beg, output_size, axis_i=0)
A:torch.onnx.symbolic_opset11.scales->torch.onnx.symbolic_helper._interpolate_get_scales(g, scale_factor, input.type().dim())
A:torch.onnx.symbolic_opset11.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset11.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset11.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset11.upsample_bicubic2d->_interpolate('upsample_bicubic2d', 4, 'cubic')
A:torch.onnx.symbolic_opset11.mode->torch.onnx.symbolic_helper._maybe_get_const(mode, 's')
A:torch.onnx.symbolic_opset11.roi->g.op('Constant', value_t=torch.tensor([], dtype=torch.float32))
A:torch.onnx.symbolic_opset11.size->g.op('Concat', input_size, size, axis_i=0)
A:torch.onnx.symbolic_opset11.dim_tensor->g.op('Constant', value_t=torch.tensor(dim, dtype=torch.int))
A:torch.onnx.symbolic_opset11.csum->g.op('Cast', csum, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset11.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset11.source->torch.onnx.symbolic_helper._slice_helper(g, source, axes=torch.LongTensor([0]), starts=torch.LongTensor([0]), ends=size(g, index, torch.LongTensor([0])), dynamic_slice=True)
A:torch.onnx.symbolic_opset11.dim->g.op('Pad', input, g.op('Constant', value_t=torch.tensor(((0,) * 2 + padding) * 2)), mode_s='constant').type().dim()
A:torch.onnx.symbolic_opset11.(u, indices, inverse_indices, counts)->g.op('Unique', self, axis_i=dim, sorted_i=sorted, outputs=4)
A:torch.onnx.symbolic_opset11.padding->torch.onnx.symbolic_helper._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
A:torch.onnx.symbolic_opset11.input->g.op('Pad', input, g.op('Constant', value_t=torch.tensor(((0,) * 2 + padding) * 2)), mode_s='constant')
A:torch.onnx.symbolic_opset11.output->g.op('Transpose', output, perm_i=[0, 1, 2, 4, 3, 5])
A:torch.onnx.symbolic_opset11.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset11.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset11.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset11.pad_len->torch.onnx.symbolic_opset9.size(g, pad, g.op('Constant', value_t=torch.tensor([0])))
A:torch.onnx.symbolic_opset11.extension->g.op('Sub', g.op('Mul', g.op('Constant', value_t=torch.tensor(dim, dtype=torch.int64)), g.op('Constant', value_t=torch.tensor(2, dtype=torch.int64))), pad_len)
A:torch.onnx.symbolic_opset11.pad->g.op('Constant', value_t=torch.LongTensor([0, 0, padding_h, padding_w] * 2))
A:torch.onnx.symbolic_opset11.paddings->_prepare_onnx_paddings(g, input.type().dim(), padding)
A:torch.onnx.symbolic_opset11.padding_c->g.op('Cast', paddings, to_i=sym_help.cast_pytorch_to_onnx['Long'])
A:torch.onnx.symbolic_opset11.value->torch.onnx.symbolic_helper._if_scalar_type_as(g, value, self)
A:torch.onnx.symbolic_opset11.(type, end, start, step)->torch.onnx.symbolic_helper._arange_cast_helper(g, start=args[0], end=args[1], step=args[2], dtype=dtype)
A:torch.onnx.symbolic_opset11.start_default->g.op('Constant', value_t=torch.tensor(0, dtype=sym_help.scalar_type_to_pytorch_type[type]))
A:torch.onnx.symbolic_opset11.delta_default->g.op('Constant', value_t=torch.tensor(1, dtype=sym_help.scalar_type_to_pytorch_type[type]))
A:torch.onnx.symbolic_opset11.arange_tensor->g.op('Range', start, end, step)
A:torch.onnx.symbolic_opset11.like_shape->g.op('Shape', like)
A:torch.onnx.symbolic_opset11.stop->g.op('Gather', like_shape, g.op('Constant', value_t=torch.tensor(dim)), axis_i=0)
A:torch.onnx.symbolic_opset11.dim_value->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset11.(expanded_index_shape, expanded_index)->torch.onnx.symbolic_helper._index_fill_reshape_helper(g, self, dim, index)
A:torch.onnx.symbolic_opset11.expanded_value->expand(g, value, expanded_index_shape, None)
A:torch.onnx.symbolic_opset11.other->g.op('Cast', other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_opset11.two->g.op('Constant', value_t=torch.tensor(2, dtype=torch.float32))
A:torch.onnx.symbolic_opset11.two_pow->g.op('Pow', two, other)
A:torch.onnx.symbolic_opset11.rshift->g.op('Div', self, two_pow)
A:torch.onnx.symbolic_opset11.lshift->g.op('Mul', self, two_pow)
A:torch.onnx.symbolic_opset11.blocks_d->g.op('Sub', blocks_d, g.op('Constant', value_t=torch.tensor(dilation_d * (kernel_size_d - 1))))
A:torch.onnx.symbolic_opset11.blocks_d_indices->g.op('Unsqueeze', blocks_d_indices, axes_i=[0])
A:torch.onnx.symbolic_opset11.kernel_grid->g.op('Constant', value_t=torch.tensor([kernel_grid]))
A:torch.onnx.symbolic_opset11.kernel_mask->g.op('Reshape', kernel_grid, g.op('Constant', value_t=torch.tensor([-1, 1])))
A:torch.onnx.symbolic_opset11.block_mask->g.op('Add', blocks_d_indices, kernel_mask)
A:torch.onnx.symbolic_opset11.batch_dim->size(g, input, g.op('Constant', value_t=torch.tensor(0)))
A:torch.onnx.symbolic_opset11.channel_dim->size(g, input, g.op('Constant', value_t=torch.tensor(1)))
A:torch.onnx.symbolic_opset11.channel_unfolded->g.op('Mul', channel_dim, g.op('Constant', value_t=torch.tensor(kernel_h * kernel_w)))
A:torch.onnx.symbolic_opset11.input_h->size(g, input, g.op('Constant', value_t=torch.tensor(2)))
A:torch.onnx.symbolic_opset11.input_w->size(g, input, g.op('Constant', value_t=torch.tensor(3)))
A:torch.onnx.symbolic_opset11.blocks_row_indices->_get_im2col_indices_along_dim(g, input_h, kernel_h, dilation_h, padding_h, stride_h)
A:torch.onnx.symbolic_opset11.blocks_col_indices->_get_im2col_indices_along_dim(g, input_w, kernel_w, dilation_w, padding_w, stride_w)
A:torch.onnx.symbolic_opset11.output_shape->_get_im2col_output_shape(g, input, kernel_h, kernel_w)
A:torch.onnx.symbolic_opset11.padded_input->_get_im2col_padded_input(g, input, padding_h, padding_w)
A:torch.onnx.symbolic_opset11.input_dims->g.op('Pad', input, g.op('Constant', value_t=torch.tensor(((0,) * 2 + padding) * 2)), mode_s='constant').type().sizes()
A:torch.onnx.symbolic_opset11.shape->g.op('Constant', value_t=torch.LongTensor(output_dims))
A:torch.onnx.symbolic_opset11.p->_reshape_from_tensor(g, input, shape)
torch.onnx.symbolic_opset11.__getitem_(g,self,i)
torch.onnx.symbolic_opset11.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor)
torch.onnx.symbolic_opset11.__lshift_(g,self,other)
torch.onnx.symbolic_opset11.__rshift_(g,self,other)
torch.onnx.symbolic_opset11._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset11._dim_arange(g,like,dim)
torch.onnx.symbolic_opset11._get_im2col_indices_along_dim(g,input_d,kernel_size_d,dilation_d,padding_d,stride_d)
torch.onnx.symbolic_opset11._get_im2col_output_shape(g,input,kernel_h,kernel_w)
torch.onnx.symbolic_opset11._get_im2col_padded_input(g,input,padding_h,padding_w)
torch.onnx.symbolic_opset11._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset11._len(g,self)
torch.onnx.symbolic_opset11._prepare_onnx_paddings(g,dim,pad)
torch.onnx.symbolic_opset11._unique2(g,self,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset11.append(g,self,tensor)
torch.onnx.symbolic_opset11.arange(g,*args)
torch.onnx.symbolic_opset11.cat(g,tensor_list,dim)
torch.onnx.symbolic_opset11.clamp(g,self,min,max)
torch.onnx.symbolic_opset11.constant_pad_nd(g,input,padding,value=None)
torch.onnx.symbolic_opset11.cumsum(g,self,dim,dtype=None)
torch.onnx.symbolic_opset11.det(g,self)
torch.onnx.symbolic_opset11.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset11.gather(g,self,dim,index,sparse_grad=False)
torch.onnx.symbolic_opset11.hardtanh(g,self,min_val,max_val)
torch.onnx.symbolic_opset11.im2col(g,input,kernel_size,dilation,padding,stride)
torch.onnx.symbolic_opset11.index_copy(g,self,dim,index,source)
torch.onnx.symbolic_opset11.index_fill(g,self,dim,index,value)
torch.onnx.symbolic_opset11.index_put(g,self,indices_list_value,values,accumulate=False)
torch.onnx.symbolic_opset11.insert(g,self,pos,tensor)
torch.onnx.symbolic_opset11.logdet(g,input)
torch.onnx.symbolic_opset11.masked_scatter(g,self,mask,source)
torch.onnx.symbolic_opset11.masked_select(g,self,mask)
torch.onnx.symbolic_opset11.mm(g,self,other)
torch.onnx.symbolic_opset11.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic_opset11.pop(g,tensor_list,dim)
torch.onnx.symbolic_opset11.reflection_pad(g,input,padding)
torch.onnx.symbolic_opset11.replication_pad(g,input,padding)
torch.onnx.symbolic_opset11.round(g,self)
torch.onnx.symbolic_opset11.scatter(g,self,dim,index,src)
torch.onnx.symbolic_opset11.size(g,self,dim=None)
torch.onnx.symbolic_opset11.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset11.split_with_sizes(g,self,split_sizes,dim)
torch.onnx.symbolic_opset11.squeeze(g,self,dim=None)
torch.onnx.symbolic_opset11.stack(g,tensor_list,dim)
torch.onnx.symbolic_opset11.topk(g,self,k,dim,largest,sorted,out=None)
torch.onnx.symbolic_opset11.unique_dim(g,self,dim,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset11.unsqueeze(g,self,dim)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_caffe2.py----------------------------------------
A:torch.onnx.symbolic_caffe2.module->importlib.import_module('torch.onnx.symbolic_caffe2')
A:torch.onnx.symbolic_caffe2.quant_version_ops->getmembers(sym_registry._symbolic_versions['caffe2'])
A:torch.onnx.symbolic_caffe2.output->g.op('_caffe2::Int8Sigmoid', input, **kwargs)
A:torch.onnx.symbolic_caffe2.output_size->torch.onnx.symbolic_helper._parse_arg(output_size, 'is')
A:torch.onnx.symbolic_caffe2.input->nchw2nhwc(g, input)
A:torch.onnx.symbolic_caffe2.start->torch.onnx.symbolic_helper._parse_arg(start, 'i')
A:torch.onnx.symbolic_caffe2.end->torch.onnx.symbolic_helper._parse_arg(end, 'i')
A:torch.onnx.symbolic_caffe2.dim->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_caffe2.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
torch.onnx.symbolic_caffe2._empty_affine_quantized(g,input,shape,scale,zero_point,dtype,pin_memory,memory_format,layout)
torch.onnx.symbolic_caffe2._permute_helper(g,input,axes)
torch.onnx.symbolic_caffe2.add(g,input_a,input_b,scale,zero_point)
torch.onnx.symbolic_caffe2.avg_pool2d(g,input,kernel_size,stride,padding,ceil_mode,count_include_pad,divisor_override=None)
torch.onnx.symbolic_caffe2.cat(g,tensor_list,dim,scale=None,zero_point=None)
torch.onnx.symbolic_caffe2.conv2d(g,input,weight,bias,stride,padding,dilation,groups,scale,zero_point)
torch.onnx.symbolic_caffe2.conv2d_relu(g,input,weight,bias,stride,padding,dilation,groups,scale,zero_point)
torch.onnx.symbolic_caffe2.conv_prepack(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_caffe2.dequantize(g,input)
torch.onnx.symbolic_caffe2.linear(g,input,weight,bias,scale,zero_point)
torch.onnx.symbolic_caffe2.linear_prepack(g,weight,bias)
torch.onnx.symbolic_caffe2.max_pool2d(g,input,kernel_size,stride,padding,dilation,ceil_mode)
torch.onnx.symbolic_caffe2.nchw2nhwc(g,input)
torch.onnx.symbolic_caffe2.nhwc2nchw(g,input)
torch.onnx.symbolic_caffe2.quantize_per_tensor(g,input,scale,zero_point,dtype)
torch.onnx.symbolic_caffe2.register_quantized_ops(domain,version)
torch.onnx.symbolic_caffe2.relu(g,input)
torch.onnx.symbolic_caffe2.reshape(g,input,shape)
torch.onnx.symbolic_caffe2.sigmoid(g,input)
torch.onnx.symbolic_caffe2.slice(g,input,dim,start,end,step)
torch.onnx.symbolic_caffe2.upsample_nearest2d(g,input,output_size,align_corners=None,scales_h=None,scales_w=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/operators.py----------------------------------------
torch.onnx.operators.reshape_from_tensor_shape(x,shape)
torch.onnx.operators.shape_as_tensor(x)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_opset9.py----------------------------------------
A:torch.onnx.symbolic_opset9.n->g.op('ConvTranspose' if transposed else 'Conv', *args, **kwargs)
A:torch.onnx.symbolic_opset9.shape->g.op('Shape', self)
A:torch.onnx.symbolic_opset9.out->reshape_as(g, out, index)
A:torch.onnx.symbolic_opset9.scalar_type->torch.get_default_dtype()
A:torch.onnx.symbolic_opset9.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
A:torch.onnx.symbolic_opset9.C->g.op('Constant', value_t=torch.tensor([1]))
A:torch.onnx.symbolic_opset9.overloads->fn(g, *args)
A:torch.onnx.symbolic_opset9.symbolic->_reduce_op_symbolic(onnx_op, allow_multi_dim_support=allow_multi_dim_support)
A:torch.onnx.symbolic_opset9.sum->g.op('ReduceSum', exp, axes_i=[dim])
A:torch.onnx.symbolic_opset9.mean->g.op('ReduceMean', input, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.prod->_reduce_with_dtype('ReduceProd', 'prod', allow_multi_dim_support=False)
A:torch.onnx.symbolic_opset9.size->torch.onnx.symbolic_helper._maybe_get_const(size, 'is')
A:torch.onnx.symbolic_opset9.ones->ones_like(g, size, dtype)
A:torch.onnx.symbolic_opset9.neg_ones->mul(g, ones, g.op('Constant', value_t=torch.tensor(-1)))
A:torch.onnx.symbolic_opset9.rank->weight_v.type().dim()
A:torch.onnx.symbolic_opset9.dim->_parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset9.axes->list(range(rank))
A:torch.onnx.symbolic_opset9.self_sizes->g.op('Transpose', self, perm_i=adv_idx_permute).type().sizes()
A:torch.onnx.symbolic_opset9.split_size->torch.onnx.symbolic_helper._get_const(split_size_or_sizes, 'i', 'split_size')
A:torch.onnx.symbolic_opset9.index->squeeze(g, nonzero(g, index), dim=1)
A:torch.onnx.symbolic_opset9.slice_node->torch.onnx.symbolic_helper._slice_helper(g, self, axes=[dim], starts=[index], ends=[end_index])
A:torch.onnx.symbolic_opset9.weight->g.op('Constant', value_t=weight_value)
A:torch.onnx.symbolic_opset9.negative_slope->torch.onnx.symbolic_helper._get_const(negative_slope, 't', 'negative_slope')
A:torch.onnx.symbolic_opset9.(first, second)->g.op('Split', input, axis_i=dim, outputs=2)
A:torch.onnx.symbolic_opset9.input_dim->g.op('Transpose', input, perm_i=[1, 0, 2]).type().dim()
A:torch.onnx.symbolic_opset9.softmax->g.op('Cast', softmax, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset9.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset9.exp->g.op('Exp', input)
A:torch.onnx.symbolic_opset9.padding->torch.onnx.symbolic_helper._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
A:torch.onnx.symbolic_opset9.padding_ceil->get_pool_ceil_padding(input, kernel_size, stride, padding)
A:torch.onnx.symbolic_opset9.(r, indices)->g.op('MaxPool', input, outputs=2, **kwargs)
A:torch.onnx.symbolic_opset9.(_, flattened_indices)->g.op('MaxPool', input, outputs=2, kernel_shape_i=[1 for _ in range(ndims)], strides_i=[1 for _ in range(ndims)])
A:torch.onnx.symbolic_opset9.s->torch.onnx.symbolic_helper._slice_helper(g, flattened_indices, axes=[2 + i for i in range(ndims)], starts=tuple_fn(0), ends=tuple_fn(1))
A:torch.onnx.symbolic_opset9.indices->torch.onnx.symbolic_helper._unpack_list(index)
A:torch.onnx.symbolic_opset9.r->g.op('MaxPool', input, outputs=1, **kwargs)
A:torch.onnx.symbolic_opset9.max_pool1d->_max_pool('max_pool1d', _single, 1, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool2d->_max_pool('max_pool2d', _pair, 2, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool3d->_max_pool('max_pool3d', _triple, 3, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool1d_with_indices->_max_pool('max_pool1d_with_indices', _single, 1, return_indices=True)
A:torch.onnx.symbolic_opset9.max_pool2d_with_indices->_max_pool('max_pool2d_with_indices', _pair, 2, return_indices=True)
A:torch.onnx.symbolic_opset9.max_pool3d_with_indices->_max_pool('max_pool3d_with_indices', _triple, 3, return_indices=True)
A:torch.onnx.symbolic_opset9.input->g.op('Transpose', input, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.output->g.op('IsNaN', input)
A:torch.onnx.symbolic_opset9.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset9.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset9.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool1d->_adaptive_pool('adaptive_avg_pool1d', 'AveragePool', _single)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool2d->_adaptive_pool('adaptive_avg_pool2d', 'AveragePool', _pair)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool3d->_adaptive_pool('adaptive_avg_pool3d', 'AveragePool', _triple)
A:torch.onnx.symbolic_opset9.adaptive_max_pool1d->_adaptive_pool('adaptive_max_pool1d', 'MaxPool', _single, max_pool1d_with_indices)
A:torch.onnx.symbolic_opset9.adaptive_max_pool2d->_adaptive_pool('adaptive_max_pool2d', 'MaxPool', _pair, max_pool2d_with_indices)
A:torch.onnx.symbolic_opset9.adaptive_max_pool3d->_adaptive_pool('adaptive_max_pool3d', 'MaxPool', _triple, max_pool3d_with_indices)
A:torch.onnx.symbolic_opset9.paddings->_prepare_onnx_paddings(input.type().dim(), padding)
A:torch.onnx.symbolic_opset9.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset9.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset9.scales->torch.onnx.symbolic_helper._interpolate_size_to_scales(g, input, output_size, dim)
A:torch.onnx.symbolic_opset9.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset9.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset9.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset9.(scales, mode)->torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g, input, size, scale_factor, mode, align_corners)
A:torch.onnx.symbolic_opset9.from_cast_func->wrap_logical_op_with_cast_to(input.type().scalarType())(fn)
A:torch.onnx.symbolic_opset9.other->g.op('Cast', other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_opset9.two->g.op('Constant', value_t=torch.tensor(2, dtype=torch.float32))
A:torch.onnx.symbolic_opset9.two_pow->g.op('Pow', two, other)
A:torch.onnx.symbolic_opset9.rshift->g.op('Div', self, two_pow)
A:torch.onnx.symbolic_opset9.lshift->g.op('Mul', self, two_pow)
A:torch.onnx.symbolic_opset9.return_op->g.op('Transpose', return_op, perm_i=axes)
A:torch.onnx.symbolic_opset9.weight_size->g.op('Constant', value_t=weight_value).type().sizes()
A:torch.onnx.symbolic_opset9.input_sizes->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic_opset9.weight_value->torch.tensor([1.0]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_opset9.bias_value->torch.tensor([0.0]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_opset9.bias->g.op('Constant', value_t=bias_value)
A:torch.onnx.symbolic_opset9.two_cst->g.op('Constant', value_t=torch.tensor(2.0))
A:torch.onnx.symbolic_opset9.eps_cst->g.op('Constant', value_t=torch.tensor(eps))
A:torch.onnx.symbolic_opset9.numerator->sub(g, input, mean)
A:torch.onnx.symbolic_opset9.variance->g.op('ReduceMean', pow(g, numerator, two_cst), axes_i=axes)
A:torch.onnx.symbolic_opset9.denominator->sqrt(g, add(g, variance, eps_cst))
A:torch.onnx.symbolic_opset9.layer_norm->add(g, layer_norm, bias)
A:torch.onnx.symbolic_opset9.low_indices->range(0, sizedim, step)
A:torch.onnx.symbolic_opset9.hi_indices->range(size, sizedim + 1, step)
A:torch.onnx.symbolic_opset9.ndim->g.op('Transpose', input, perm_i=[1, 0, 2]).type().dim()
A:torch.onnx.symbolic_opset9.perm->list(range(0, ndim))
A:torch.onnx.symbolic_opset9.index_const->torch.onnx.symbolic_helper._maybe_get_scalar(index)
A:torch.onnx.symbolic_opset9.index_dim->squeeze(g, nonzero(g, index), dim=1).type().dim()
A:torch.onnx.symbolic_opset9.indices_list->torch.onnx.symbolic_helper._unpack_list(indices_list_value)
A:torch.onnx.symbolic_opset9.dim_value->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset9.(expanded_index_shape, expanded_index)->torch.onnx.symbolic_helper._index_fill_reshape_helper(g, self, dim, index)
A:torch.onnx.symbolic_opset9.value->torch.onnx.symbolic_helper._maybe_get_scalar(value)
A:torch.onnx.symbolic_opset9.expanded_value->expand(g, value, expanded_index_shape, None)
A:torch.onnx.symbolic_opset9.other_type_name->g.op('Cast', other, to_i=sym_help.cast_pytorch_to_onnx['Float']).type().scalarType()
A:torch.onnx.symbolic_opset9.f_dtypeself_dtype->g.op('Transpose', self, perm_i=adv_idx_permute).type().scalarType()
A:torch.onnx.symbolic_opset9.self->g.op('Transpose', self, perm_i=adv_idx_permute)
A:torch.onnx.symbolic_opset9.exponent->g.op('Cast', exponent, to_i=sym_help.cast_pytorch_to_onnx[f_dtype])
A:torch.onnx.symbolic_opset9.pow->g.op('Cast', pow, to_i=sym_help.cast_pytorch_to_onnx[self_dtype])
A:torch.onnx.symbolic_opset9.min->g.op('ReduceMin', self, axes_i=[dim], keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.max->g.op('ReduceMax', self, axes_i=[dim], keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.keepdim->_parse_arg(keepdim, 'i')
A:torch.onnx.symbolic_opset9.(r, _)->g.op('Dropout', input, ratio_f=p, outputs=2)
A:torch.onnx.symbolic_opset9.feature_dropout->_unsupported_dropout('feature_dropout')
A:torch.onnx.symbolic_opset9.alpha_dropout->_unsupported_dropout('alpha_dropout')
A:torch.onnx.symbolic_opset9.feature_alpha_dropout->_unsupported_dropout('feature_alpha_dropout')
A:torch.onnx.symbolic_opset9.f->_reduce_op_symbolic('ReduceL2')
A:torch.onnx.symbolic_opset9.name->'_cast_{}'.format(k)
A:torch.onnx.symbolic_opset9.globals()[name]->parse_args('v', 'i')(partial(sym_help._cast_func_template, v))
A:torch.onnx.symbolic_opset9.dtype->g.op('Transpose', self, perm_i=adv_idx_permute).type().scalarType()
A:torch.onnx.symbolic_opset9.scalar->g.op('Cast', scalar, to_i=sym_help.scalar_type_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.const_value->torch.onnx.symbolic_helper._maybe_get_const(value, 't')
A:torch.onnx.symbolic_opset9.tmp->zeros(g, sizes, dtype, layout, device)
A:torch.onnx.symbolic_opset9.start_unsqueezed->g.op('Unsqueeze', start, axes_i=[0])
A:torch.onnx.symbolic_opset9.end_unsqueezed->g.op('Unsqueeze', end, axes_i=[0])
A:torch.onnx.symbolic_opset9.dim_unsqueezed->g.op('Unsqueeze', dim, axes_i=[0])
A:torch.onnx.symbolic_opset9.start->g.op('Unsqueeze', args[0], axes_i=[0])
A:torch.onnx.symbolic_opset9.end->g.op('Unsqueeze', args[1], axes_i=[0])
A:torch.onnx.symbolic_opset9.repeats->g.op('Constant', value_t=torch.LongTensor(repeats))
A:torch.onnx.symbolic_opset9.const_repeats->torch.onnx.symbolic_helper._maybe_get_const(repeats, 'is')
A:torch.onnx.symbolic_opset9.sizes->g.op('Transpose', self, perm_i=adv_idx_permute).type().sizes()
A:torch.onnx.symbolic_opset9.dims->g.op('Transpose', self, perm_i=adv_idx_permute).type().sizes()
A:torch.onnx.symbolic_opset9.after_view->view(g, self, [-1, output_channel, upscale_factor, upscale_factor, dims[2], dims[3]])
A:torch.onnx.symbolic_opset9.after_transpose->g.op('Transpose', after_view, perm_i=[0, 1, 4, 2, 5, 3])
A:torch.onnx.symbolic_opset9.variantToOnnxActivationMap->dict(zip([act_fun.lower() for act_fun in onnxActivations], onnxActivations))
A:torch.onnx.symbolic_opset9.bias_concat->g.op('Concat', bias_f, bias_b, axis_i=0)
A:torch.onnx.symbolic_opset9.(weight_ih, weight_hh, bias_concat)->transform_weights(i)
A:torch.onnx.symbolic_opset9.(weight_ih_f, weight_hh_f, bias_f)->transform_weights(2 * i)
A:torch.onnx.symbolic_opset9.(weight_ih_b, weight_hh_b, bias_b)->transform_weights(2 * i + 1)
A:torch.onnx.symbolic_opset9.weight_ih->g.op('Concat', weight_ih_f, weight_ih_b, axis_i=0)
A:torch.onnx.symbolic_opset9.weight_hh->g.op('Concat', weight_hh_f, weight_hh_b, axis_i=0)
A:torch.onnx.symbolic_opset9.(prev_output, h_out)->g.op('GRU', *inputs, outputs=2, hidden_size_i=hidden_size, linear_before_reset_i=1, **extra_kwargs)
A:torch.onnx.symbolic_opset9.(prev_output, h_out, c_out)->g.op('LSTM', *inputs, outputs=3, hidden_size_i=hidden_size, **extra_kwargs)
A:torch.onnx.symbolic_opset9.prev_output->g.op('Transpose', prev_output, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.gru->_one_hidden_rnn('GRU')
A:torch.onnx.symbolic_opset9.rnn_tanh->_one_hidden_rnn('RNN_TANH')
A:torch.onnx.symbolic_opset9.rnn_relu->_one_hidden_rnn('RNN_RELU')
A:torch.onnx.symbolic_opset9.like_shape->g.op('Shape', like)
A:torch.onnx.symbolic_opset9.stop->g.op('Gather', like_shape, g.op('Constant', value_t=torch.tensor(dim)), axis_i=0)
A:torch.onnx.symbolic_opset9.lengths->_cast_Int(g, lengths, False)
A:torch.onnx.symbolic_opset9.(data, lengths)->g.op('prim::PadPacked', data, batch_sizes, outputs=2)
A:torch.onnx.symbolic_opset9.data->g.op('Transpose', data, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.shape_const->g.op('ConstantOfShape', shapes, value_t=torch.tensor([0], dtype=sym_help.scalar_type_to_pytorch_type[6]))
A:torch.onnx.symbolic_opset9.p->_reshape_from_tensor(g, input, shape)
A:torch.onnx.symbolic_opset9.input_dims->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic_opset9.flattened->reshape(g, input, (-1,))
A:torch.onnx.symbolic_opset9.to_add->torch.onnx.symbolic_helper._scatter_helper(g, to_add, dim, index, src)
A:torch.onnx.symbolic_opset9.values->g.op('Constant', value_t=torch.LongTensor([0, 1]))
A:torch.onnx.symbolic_opset9.depth->size(g, self, g.op('Constant', value_t=torch.LongTensor([dim])))
A:torch.onnx.symbolic_opset9.mul->g.op('Mul', var, g.op('Constant', value_t=torch.tensor(count, dtype=torch.float)))
A:torch.onnx.symbolic_opset9.sqrd->g.op('Mul', input, input)
A:torch.onnx.symbolic_opset9.sqrdmean->g.op('ReduceMean', sqrd, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.redudced_dims->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic_opset9.meansqrd->g.op('Mul', mean, mean)
A:torch.onnx.symbolic_opset9.var->g.op('Div', mul, g.op('Constant', value_t=torch.tensor(count - 1, dtype=torch.float)))
A:torch.onnx.symbolic_opset9.count->numpy.prod(redudced_dims)
A:torch.onnx.symbolic_opset9.std->g.op('Sqrt', var)
A:torch.onnx.symbolic_opset9.arange_tensor->g.op('Add', g.op('Mul', arange_tensor, step), start)
A:torch.onnx.symbolic_opset9.range_tensor->g.op('Div', g.op('Sub', end, start), step)
A:torch.onnx.symbolic_opset9.step->g.op('Unsqueeze', args[2], axes_i=[0])
A:torch.onnx.symbolic_opset9.mask->_cast_Bool(g, mask, False)
A:torch.onnx.symbolic_opset9.adv_idx_count->len(adv_idx_indices)
A:torch.onnx.symbolic_opset9.shape_tensor->_shape_as_tensor(g, self)
A:torch.onnx.symbolic_opset9.adv_index->g.op('Mul', indices[adv_idx_indices[i]], multiplier)
A:torch.onnx.symbolic_opset9.cum_adv_index->g.op('Add', cum_adv_index, adv_index)
A:torch.onnx.symbolic_opset9.multiplier->g.op('Mul', multiplier, dim_tensor_list[adv_idx_indices[i]])
A:torch.onnx.symbolic_opset9.cum_adv_index_shape_tensor->_shape_as_tensor(g, cum_adv_index)
A:torch.onnx.symbolic_opset9.folded_adv_idx_shape->g.op('Concat', *folded_adv_idx_shape_list, axis_i=0)
A:torch.onnx.symbolic_opset9.final_shape->g.op('Concat', cum_adv_index_shape_tensor, *[dim_tensor_list[i] for i in range(rank) if i not in adv_idx_indices], axis_i=0)
A:torch.onnx.symbolic_opset9.sqr->g.op('Mul', self, self)
A:torch.onnx.symbolic_opset9.sumsqr->g.op('ReduceSum', sqr, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.log_input->log(g, input)
A:torch.onnx.symbolic_opset9.batch_mul->matmul(g, batch1, batch2)
A:torch.onnx.symbolic_opset9.mul_a->mul(g, batch_mul, g.op('Cast', alpha, to_i=sym_help.cast_pytorch_to_onnx[dtype]))
A:torch.onnx.symbolic_opset9.mul_b->mul(g, self, g.op('Cast', beta, to_i=sym_help.cast_pytorch_to_onnx[dtype]))
A:torch.onnx.symbolic_opset9.out_shape->g.op('Concat', *tensors_shape, axis_i=0)
A:torch.onnx.symbolic_opset9.t_reshaped->_reshape_from_tensor(g, t, g.op('Concat', *shape_i, axis_i=0))
A:torch.onnx.symbolic_opset9.div->g.op('Div', weight_v, norm_v)
A:torch.onnx.symbolic_opset9.quo->g.op('Mul', div, other)
A:torch.onnx.symbolic_opset9.erf->g.op('Erf', div(g, self, torch.tensor(_sqrt2)))
A:torch.onnx.symbolic_opset9.erf_plusone->add(g, erf, g.op('Constant', value_t=torch.tensor(1, dtype=torch.float)))
A:torch.onnx.symbolic_opset9.input_reshaped->g.op('Reshape', input, g.op('Constant', value_t=torch.LongTensor(shape)))
A:torch.onnx.symbolic_opset9.weight_->g.op('Constant', value_t=torch.tensor([1.0] * num_groups).type('torch.' + input.type().scalarType() + 'Tensor'))
A:torch.onnx.symbolic_opset9.bias_->g.op('Constant', value_t=torch.tensor([0.0] * num_groups).type('torch.' + input.type().scalarType() + 'Tensor'))
A:torch.onnx.symbolic_opset9.norm_reshaped->g.op('InstanceNormalization', input_reshaped, weight_, bias_, epsilon_f=eps)
A:torch.onnx.symbolic_opset9.norm->g.op('Reshape', norm_reshaped, g.op('Shape', input))
A:torch.onnx.symbolic_opset9.norm_v->norm(g, weight_v, 2, axes, 1)
A:torch.onnx.symbolic_opset9.self_flattened->g.op('Reshape', self, g.op('Constant', value_t=torch.tensor([-1], dtype=torch.int64)))
torch.onnx.symbolic_opset9.__and_(g,input,other)
torch.onnx.symbolic_opset9.__getitem_(g,self,i)
torch.onnx.symbolic_opset9.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor)
torch.onnx.symbolic_opset9.__lshift_(g,self,other)
torch.onnx.symbolic_opset9.__or_(g,input,other)
torch.onnx.symbolic_opset9.__rshift_(g,self,other)
torch.onnx.symbolic_opset9._adaptive_pool(name,type,tuple_fn,fn=None)
torch.onnx.symbolic_opset9._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset9._convolution(g,input,weight,bias,stride,padding,dilation,transposed,output_padding,groups,benchmark,deterministic,cudnn_enabled)
torch.onnx.symbolic_opset9._dim_arange(g,like,dim)
torch.onnx.symbolic_opset9._generic_rnn(g,variant,input,initial_states,all_weights,has_biases,num_layers,dropout,train,bidirectional,batch_first=None,batch_sizes=None)
torch.onnx.symbolic_opset9._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset9._lstm_full(g,input,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional,batch_first)
torch.onnx.symbolic_opset9._lstm_packed(g,input,batch_sizes,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional)
torch.onnx.symbolic_opset9._max_pool(name,tuple_fn,ndims,return_indices)
torch.onnx.symbolic_opset9._one_hidden_rnn(kind)
torch.onnx.symbolic_opset9._pack_padded_sequence(g,input,lengths,batch_first)
torch.onnx.symbolic_opset9._pad_packed_sequence(g,data,batch_sizes,batch_first,padding_value,total_length)
torch.onnx.symbolic_opset9._prepare_onnx_paddings(dim,pad)
torch.onnx.symbolic_opset9._reduce_op_symbolic(onnx_op_name,allow_multi_dim_support=True)
torch.onnx.symbolic_opset9._reduce_with_dtype(onnx_op,name,allow_multi_dim_support=True)
torch.onnx.symbolic_opset9._reshape_from_tensor(g,input,shape)
torch.onnx.symbolic_opset9._sample_dirichlet(g,self,generator)
torch.onnx.symbolic_opset9._shape_as_tensor(g,input)
torch.onnx.symbolic_opset9._slice(g,input,axes,starts,ends)
torch.onnx.symbolic_opset9._standard_gamma(g,self,generator)
torch.onnx.symbolic_opset9._std(g,input,dim,unbiased,keepdim)
torch.onnx.symbolic_opset9._unique(g,input,sorted,return_inverse)
torch.onnx.symbolic_opset9._unique2(g,input,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset9._unsupported_dropout(name)
torch.onnx.symbolic_opset9._weight_norm(g,weight_v,weight_g,dim)
torch.onnx.symbolic_opset9.abs(g,self)
torch.onnx.symbolic_opset9.acos(g,self)
torch.onnx.symbolic_opset9.add(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic_opset9.alias(g,self)
torch.onnx.symbolic_opset9.arange(g,*args)
torch.onnx.symbolic_opset9.argmax(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.argmin(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.asin(g,self)
torch.onnx.symbolic_opset9.atan(g,self)
torch.onnx.symbolic_opset9.baddbmm(g,self,batch1,batch2,beta,alpha)
torch.onnx.symbolic_opset9.batch_norm(g,input,weight,bias,running_mean,running_var,training,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.bitwise_not(g,inp)
torch.onnx.symbolic_opset9.bmm(g,self,other)
torch.onnx.symbolic_opset9.cat(g,tensor_list,dim)
torch.onnx.symbolic_opset9.ceil(g,input)
torch.onnx.symbolic_opset9.clamp(g,self,min,max)
torch.onnx.symbolic_opset9.clamp_max(g,self,max)
torch.onnx.symbolic_opset9.clamp_min(g,self,min)
torch.onnx.symbolic_opset9.clone(g,input,unused_memory_format)
torch.onnx.symbolic_opset9.constant_pad_nd(g,input,padding,value)
torch.onnx.symbolic_opset9.contiguous(g,input,memory_format)
torch.onnx.symbolic_opset9.conv1d(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_opset9.conv2d(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_opset9.conv3d(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_opset9.conv_tbc(g,input,weight,bias,pad)
torch.onnx.symbolic_opset9.conv_transpose1d(g,input,weight,bias,stride,padding,output_padding,groups,dilation)
torch.onnx.symbolic_opset9.conv_transpose2d(g,input,weight,bias,stride,padding,output_padding,groups,dilation)
torch.onnx.symbolic_opset9.conv_transpose3d(g,input,weight,bias,stride,padding,output_padding,groups,dilation)
torch.onnx.symbolic_opset9.cos(g,self)
torch.onnx.symbolic_opset9.cosine_similarity(g,x1,x2,dim,eps)
torch.onnx.symbolic_opset9.cumsum(g,input,dim,dtype)
torch.onnx.symbolic_opset9.detach(g,input)
torch.onnx.symbolic_opset9.dim(g,self)
torch.onnx.symbolic_opset9.div(g,self,other)
torch.onnx.symbolic_opset9.dropout(g,input,p,train)
torch.onnx.symbolic_opset9.elu(g,input,alpha,scale,input_scale)
torch.onnx.symbolic_opset9.embedding(g,weight,indices,padding_idx,scale_grad_by_freq,sparse)
torch.onnx.symbolic_opset9.embedding_bag(g,embedding_matrix,indices,offsets,scale_grad_by_freq,mode,sparse,per_sample_weights,include_last_offset)
torch.onnx.symbolic_opset9.empty(g,sizes,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.empty_like(g,input,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.eq(g,self,other)
torch.onnx.symbolic_opset9.erf(g,input)
torch.onnx.symbolic_opset9.exp(g,self)
torch.onnx.symbolic_opset9.expand(g,self,size,implicit)
torch.onnx.symbolic_opset9.expand_as(g,self,other)
torch.onnx.symbolic_opset9.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset9.floor(g,input)
torch.onnx.symbolic_opset9.floor_divide(g,self,other)
torch.onnx.symbolic_opset9.frobenius_norm(g,self,dim=None,keepdim=False)
torch.onnx.symbolic_opset9.full(g,sizes,value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.full_like(g,input,fill_value,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.gather(g,self,dim,index,sparse_grad=False)
torch.onnx.symbolic_opset9.ge(g,input,other)
torch.onnx.symbolic_opset9.gelu(g,self)
torch.onnx.symbolic_opset9.get_pool_ceil_padding(input,kernel_size,stride,padding)
torch.onnx.symbolic_opset9.glu(g,input,dim)
torch.onnx.symbolic_opset9.group_norm(g,input,num_groups,weight,bias,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.gt(g,input,other)
torch.onnx.symbolic_opset9.gt_impl(g,input,other)
torch.onnx.symbolic_opset9.hardtanh(g,self,min_val,max_val)
torch.onnx.symbolic_opset9.index(g,self,index)
torch.onnx.symbolic_opset9.index_copy(g,self,dim,index,source)
torch.onnx.symbolic_opset9.index_fill(g,self,dim,index,value)
torch.onnx.symbolic_opset9.index_put(g,self,indices_list_value,values,accumulate)
torch.onnx.symbolic_opset9.index_select(g,self,dim,index)
torch.onnx.symbolic_opset9.instance_norm(g,input,weight,bias,running_mean,running_var,use_input_stats,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.isnan(g,input)
torch.onnx.symbolic_opset9.layer_norm(g,input,normalized_shape,weight,bias,eps,cudnn_enable)
torch.onnx.symbolic_opset9.le(g,input,other)
torch.onnx.symbolic_opset9.leaky_relu(g,input,negative_slope,inplace=False)
torch.onnx.symbolic_opset9.log(g,self)
torch.onnx.symbolic_opset9.log1p(g,self)
torch.onnx.symbolic_opset9.log2(g,self)
torch.onnx.symbolic_opset9.log_sigmoid(g,input)
torch.onnx.symbolic_opset9.log_softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset9.logsumexp(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.lstm(g,*args)
torch.onnx.symbolic_opset9.lt(g,input,other)
torch.onnx.symbolic_opset9.lt_impl(g,input,other)
torch.onnx.symbolic_opset9.masked_fill(g,self,mask,value)
torch.onnx.symbolic_opset9.matmul(g,self,other)
torch.onnx.symbolic_opset9.max(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset9.meshgrid(g,tensor_list)
torch.onnx.symbolic_opset9.min(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset9.mm(g,self,other)
torch.onnx.symbolic_opset9.mul(g,self,other)
torch.onnx.symbolic_opset9.multinomial(g,input,num_samples,replacement=False,generator=None)
torch.onnx.symbolic_opset9.narrow(g,input,dim,start,length)
torch.onnx.symbolic_opset9.ne(g,self,other)
torch.onnx.symbolic_opset9.neg(g,self)
torch.onnx.symbolic_opset9.new_zeros(g,self,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.nonzero(g,input)
torch.onnx.symbolic_opset9.norm(g,self,p,dim,keepdim)
torch.onnx.symbolic_opset9.one_hot(g,self,num_classes)
torch.onnx.symbolic_opset9.ones(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.ones_like(g,input,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.overload_by_arg_count(fn)
torch.onnx.symbolic_opset9.permute(g,self,dims)
torch.onnx.symbolic_opset9.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic_opset9.pow(g,self,exponent)
torch.onnx.symbolic_opset9.prelu(g,self,weight)
torch.onnx.symbolic_opset9.prim_ConstantChunk(g,self,chunks,dim)
torch.onnx.symbolic_opset9.prim_ConstantSplit(g,self,split_size,dim)
torch.onnx.symbolic_opset9.prim_shape(g,self)
torch.onnx.symbolic_opset9.rand(g,shapes,dtype,*options)
torch.onnx.symbolic_opset9.rand_like(g,self,dtype,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.randn(g,shapes,dtype,*options)
torch.onnx.symbolic_opset9.randn_like(g,self,dtype,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.reciprocal(g,self)
torch.onnx.symbolic_opset9.reflection_pad(g,input,padding)
torch.onnx.symbolic_opset9.relu(g,input)
torch.onnx.symbolic_opset9.remainder(g,input,other)
torch.onnx.symbolic_opset9.repeat(g,self,repeats)
torch.onnx.symbolic_opset9.replication_pad(g,input,padding)
torch.onnx.symbolic_opset9.reshape(g,self,shape)
torch.onnx.symbolic_opset9.reshape_as(g,self,other)
torch.onnx.symbolic_opset9.rrelu(g,input,lower,upper,training,generator)
torch.onnx.symbolic_opset9.rsqrt(g,self)
torch.onnx.symbolic_opset9.rsub(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.scalar_tensor(g,scalar,dtype,*options)
torch.onnx.symbolic_opset9.scatter(g,self,dim,index,src)
torch.onnx.symbolic_opset9.scatter_add(g,self,dim,index,src)
torch.onnx.symbolic_opset9.select(g,self,dim,index)
torch.onnx.symbolic_opset9.selu(g,input)
torch.onnx.symbolic_opset9.sigmoid(g,self)
torch.onnx.symbolic_opset9.sign(g,self)
torch.onnx.symbolic_opset9.sin(g,self)
torch.onnx.symbolic_opset9.size(g,self,dim)
torch.onnx.symbolic_opset9.slice(g,self,dim,start,end,step)
torch.onnx.symbolic_opset9.softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset9.softplus(g,self,beta,threshold)
torch.onnx.symbolic_opset9.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset9.split(g,self,split_size_or_sizes,dim)
torch.onnx.symbolic_opset9.split_with_sizes(g,self,split_sizes,dim)
torch.onnx.symbolic_opset9.sqrt(g,self)
torch.onnx.symbolic_opset9.squeeze(g,self,dim=None)
torch.onnx.symbolic_opset9.stack(g,tensor_list,dim)
torch.onnx.symbolic_opset9.std(g,input,*args)
torch.onnx.symbolic_opset9.sub(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.t(g,self)
torch.onnx.symbolic_opset9.take(g,self,index)
torch.onnx.symbolic_opset9.tan(g,self)
torch.onnx.symbolic_opset9.tanh(g,self)
torch.onnx.symbolic_opset9.threshold(g,self,threshold,value)
torch.onnx.symbolic_opset9.to(g,self,*args)
torch.onnx.symbolic_opset9.topk(g,self,k,dim,largest,sorted,out=None)
torch.onnx.symbolic_opset9.transpose(g,self,dim0,dim1)
torch.onnx.symbolic_opset9.true_divide(g,self,other)
torch.onnx.symbolic_opset9.type_as(g,self,other)
torch.onnx.symbolic_opset9.unbind(g,self,dim=0)
torch.onnx.symbolic_opset9.unfold(g,input,dimension,size,step)
torch.onnx.symbolic_opset9.unsqueeze(g,self,dim)
torch.onnx.symbolic_opset9.unused(g)
torch.onnx.symbolic_opset9.view(g,self,size)
torch.onnx.symbolic_opset9.where(g,condition,self,other)
torch.onnx.symbolic_opset9.wrap_logical_op_with_cast_to(to_type)
torch.onnx.symbolic_opset9.wrap_logical_op_with_cast_to_and_from(to_type)
torch.onnx.symbolic_opset9.wrap_logical_op_with_negation(func)
torch.onnx.symbolic_opset9.zeros(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.zeros_like(g,input,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/__init__.py----------------------------------------
A:torch.onnx.__init__.result->torch.onnx.utils._export(*args, **kwargs)
torch.onnx.__init__.ExportTypes
torch.onnx.__init__._export(*args,**kwargs)
torch.onnx.__init__._export_to_pretty_string(*args,**kwargs)
torch.onnx.__init__._optimize_trace(graph,operator_export_type)
torch.onnx.__init__._run_symbolic_function(*args,**kwargs)
torch.onnx.__init__._run_symbolic_method(*args,**kwargs)
torch.onnx.__init__.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)
torch.onnx.__init__.export_to_pretty_string(*args,**kwargs)
torch.onnx.__init__.is_in_onnx_export()
torch.onnx.__init__.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)
torch.onnx.__init__.set_training(model,mode)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_opset10.py----------------------------------------
A:torch.onnx.symbolic_opset10.kwargs['dilations_i']->tuple_fn(dilation)
A:torch.onnx.symbolic_opset10.(r, indices)->g.op('MaxPool', input, outputs=2, **kwargs)
A:torch.onnx.symbolic_opset10.(_, flattened_indices)->g.op('MaxPool', input, outputs=2, kernel_shape_i=[1 for _ in range(ndims)], strides_i=[1 for _ in range(ndims)])
A:torch.onnx.symbolic_opset10.s->torch.onnx.symbolic_helper._slice_helper(g, flattened_indices, axes=[2 + i for i in range(ndims)], starts=tuple_fn(0), ends=tuple_fn(1))
A:torch.onnx.symbolic_opset10.indices->sub(g, indices, s)
A:torch.onnx.symbolic_opset10.r->g.op('MaxPool', input, outputs=1, **kwargs)
A:torch.onnx.symbolic_opset10.max_pool1d->_max_pool('max_pool1d', _single, 1, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool2d->_max_pool('max_pool2d', _pair, 2, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool3d->_max_pool('max_pool3d', _triple, 3, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool1d_with_indices->_max_pool('max_pool1d_with_indices', _single, 1, return_indices=True)
A:torch.onnx.symbolic_opset10.max_pool2d_with_indices->_max_pool('max_pool2d_with_indices', _pair, 2, return_indices=True)
A:torch.onnx.symbolic_opset10.max_pool3d_with_indices->_max_pool('max_pool3d_with_indices', _triple, 3, return_indices=True)
A:torch.onnx.symbolic_opset10.padding->torch.onnx.symbolic_helper._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
A:torch.onnx.symbolic_opset10.input->g.op('Pad', input, pads_i=((0,) * 2 + padding) * 2, mode_s='constant', value_f=0.0)
A:torch.onnx.symbolic_opset10.output->g.op('AveragePool', input, kernel_shape_i=tuple_fn(kernel_size), strides_i=tuple_fn(stride), pads_i=padding * 2, ceil_mode_i=ceil_mode)
A:torch.onnx.symbolic_opset10.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset10.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset10.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset10.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset10.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset10.scales->torch.onnx.symbolic_helper._interpolate_size_to_scales(g, input, output_size, dim)
A:torch.onnx.symbolic_opset10.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset10.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset10.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset10.(scales, mode)->torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g, input, size, scale_factor, mode, align_corners)
A:torch.onnx.symbolic_opset10.starts->g.op('Constant', value_t=torch.tensor(starts))
A:torch.onnx.symbolic_opset10.ends->g.op('Constant', value_t=torch.tensor(ends))
A:torch.onnx.symbolic_opset10.axes->g.op('Constant', value_t=torch.tensor(axes))
A:torch.onnx.symbolic_opset10.steps->g.op('Constant', value_t=torch.tensor(steps))
torch.onnx.symbolic_opset10.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor)
torch.onnx.symbolic_opset10._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset10._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset10._max_pool(name,tuple_fn,ndims,return_indices)
torch.onnx.symbolic_opset10._slice(g,input,axes,starts,ends,steps=None,dynamic_slice=False)
torch.onnx.symbolic_opset10.flip(g,input,dims)
torch.onnx.symbolic_opset10.fmod(g,input,other)
torch.onnx.symbolic_opset10.slice(g,self,dim,start,end,step)
torch.onnx.symbolic_opset10.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset10.topk(g,self,k,dim,largest,sorted,out=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_opset7.py----------------------------------------
A:torch.onnx.symbolic_opset7.vars()[black_listed_op]->_black_list_in_opset(black_listed_op)
torch.onnx.symbolic_opset7.max(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset7.min(g,self,dim_or_y=None,keepdim=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/onnx/symbolic_opset12.py----------------------------------------
A:torch.onnx.symbolic_opset12.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
A:torch.onnx.symbolic_opset12.reduction->torch.onnx.symbolic_helper._maybe_get_const(reduction, 'i')
A:torch.onnx.symbolic_opset12.nllloss->g.op('Div', nllloss, denominator)
A:torch.onnx.symbolic_opset12.zeros->zeros_like(g, target)
A:torch.onnx.symbolic_opset12.ignored_mask->eq(g, target, ignore_index)
A:torch.onnx.symbolic_opset12.ones->ones_like(g, target)
A:torch.onnx.symbolic_opset12.denominator->g.op('ReduceSum', denominator)
A:torch.onnx.symbolic_opset12.weight->index_select(g, weight, 0, target)
torch.onnx.symbolic_opset12.einsum(g,equation,tensor_list)
torch.onnx.symbolic_opset12.nll_loss(g,self,target,weight,reduction,ignore_index)
torch.onnx.symbolic_opset12.nll_loss2d(g,self,target,weight,reduction,ignore_index)
torch.onnx.symbolic_opset12.pow(g,self,exponent)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/rendezvous.py----------------------------------------
A:torch.distributed.rendezvous.result->urlparse(url)
A:torch.distributed.rendezvous.query_dict->dict((pair.split('=') for pair in filter(None, result.query.split('&'))))
A:torch.distributed.rendezvous.url->urlunparse(result)
A:torch.distributed.rendezvous.query->dict((pair.split('=') for pair in filter(None, result.query.split('&'))))
A:torch.distributed.rendezvous.rank->int(rank)
A:torch.distributed.rendezvous.world_size->int(world_size)
A:torch.distributed.rendezvous.store->TCPStore(master_addr, master_port, world_size, start_daemon, timeout)
A:torch.distributed.rendezvous.master_addr->os.environ.get('MASTER_ADDR', None)
A:torch.distributed.rendezvous.master_port->int(master_port)
torch.distributed.rendezvous._env_rendezvous_handler(url,timeout=default_pg_timeout,**kwargs)
torch.distributed.rendezvous._file_rendezvous_handler(url,**kwargs)
torch.distributed.rendezvous._rendezvous_error(msg)
torch.distributed.rendezvous._tcp_rendezvous_handler(url,timeout=default_pg_timeout,**kwargs)
torch.distributed.rendezvous.register_rendezvous_handler(scheme,handler)
torch.distributed.rendezvous.rendezvous(url,rank=-1,world_size=-1,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/distributed_c10d.py----------------------------------------
A:torch.distributed.distributed_c10d.value->getattr(Backend, name.upper(), Backend.UNDEFINED)
A:torch.distributed.distributed_c10d.reduce_op->reduce_op()
A:torch.distributed.distributed_c10d.WORLD->object()
A:torch.distributed.distributed_c10d.NON_GROUP_MEMBER->object()
A:torch.distributed.distributed_c10d.backend->Backend(backend)
A:torch.distributed.distributed_c10d._default_pg->_new_process_group_helper(world_size, rank, [], backend, store, group_name=group_name, timeout=timeout)
A:torch.distributed.distributed_c10d.rendezvous_iterator->rendezvous(init_method, rank, world_size, timeout=timeout)
A:torch.distributed.distributed_c10d.(store, rank, world_size)->next(rendezvous_iterator)
A:torch.distributed.distributed_c10d.group_name->str(_group_count)
A:torch.distributed.distributed_c10d.pg->_new_process_group_helper(group_world_size, group_rank, ranks, backend, default_store, timeout=timeout)
A:torch.distributed.distributed_c10d.global_rank->_new_process_group_helper(world_size, rank, [], backend, store, group_name=group_name, timeout=timeout).rank()
A:torch.distributed.distributed_c10d.prefix_store->PrefixStore(group_name, store)
A:torch.distributed.distributed_c10d.group_dst_rank->_get_group_rank(group, dst)
A:torch.distributed.distributed_c10d.group_src_rank->_get_group_rank(group, src)
A:torch.distributed.distributed_c10d.work->group.barrier()
A:torch.distributed.distributed_c10d.src_rank->group.barrier().source_rank()
A:torch.distributed.distributed_c10d.opts->ReduceScatterOptions()
A:torch.distributed.distributed_c10d.my_rank->get_rank()
A:torch.distributed.distributed_c10d.global_world_size->_new_process_group_helper(world_size, rank, [], backend, store, group_name=group_name, timeout=timeout).size()
A:torch.distributed.distributed_c10d.ranks->list(range(global_world_size))
A:torch.distributed.distributed_c10d.group_world_size->len(ranks)
A:torch.distributed.distributed_c10d.group_rank->list(range(global_world_size)).index(global_rank)
torch.distributed.Backend(cls,name)
torch.distributed.GroupMember(object)
torch.distributed._check_default_pg()
torch.distributed._check_single_tensor(param,param_name)
torch.distributed._check_tensor_list(param,param_name)
torch.distributed._get_default_group()
torch.distributed._get_default_store()
torch.distributed._get_global_rank(group,group_rank)
torch.distributed._get_group_rank(group,rank)
torch.distributed._get_group_size(group)
torch.distributed._new_process_group_helper(world_size,rank,group_ranks,backend,store,group_name=None,timeout=default_pg_timeout)
torch.distributed._rank_not_in_group(group)
torch.distributed.all_gather(tensor_list,tensor,group=group.WORLD,async_op=False)
torch.distributed.all_gather_coalesced(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.all_reduce_coalesced(tensors,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.barrier(group=group.WORLD,async_op=False)
torch.distributed.broadcast(tensor,src,group=group.WORLD,async_op=False)
torch.distributed.broadcast_multigpu(tensor_list,src,group=group.WORLD,async_op=False,src_tensor=0)
torch.distributed.destroy_process_group(group=group.WORLD)
torch.distributed.distributed_c10d.Backend(cls,name)
torch.distributed.distributed_c10d.Backend.__new__(cls,name)
torch.distributed.distributed_c10d.GroupMember(object)
torch.distributed.distributed_c10d._check_default_pg()
torch.distributed.distributed_c10d._check_single_tensor(param,param_name)
torch.distributed.distributed_c10d._check_tensor_list(param,param_name)
torch.distributed.distributed_c10d._get_default_group()
torch.distributed.distributed_c10d._get_default_store()
torch.distributed.distributed_c10d._get_global_rank(group,group_rank)
torch.distributed.distributed_c10d._get_group_rank(group,rank)
torch.distributed.distributed_c10d._get_group_size(group)
torch.distributed.distributed_c10d._new_process_group_helper(world_size,rank,group_ranks,backend,store,group_name=None,timeout=default_pg_timeout)
torch.distributed.distributed_c10d._rank_not_in_group(group)
torch.distributed.distributed_c10d.all_gather(tensor_list,tensor,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_gather_coalesced(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce(tensor,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce_coalesced(tensors,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.barrier(group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.broadcast(tensor,src,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.broadcast_multigpu(tensor_list,src,group=group.WORLD,async_op=False,src_tensor=0)
torch.distributed.distributed_c10d.destroy_process_group(group=group.WORLD)
torch.distributed.distributed_c10d.gather(tensor,gather_list=None,dst=0,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.get_backend(group=group.WORLD)
torch.distributed.distributed_c10d.get_rank(group=group.WORLD)
torch.distributed.distributed_c10d.get_world_size(group=group.WORLD)
torch.distributed.distributed_c10d.group(object)
torch.distributed.distributed_c10d.init_process_group(backend,init_method=None,timeout=default_pg_timeout,world_size=-1,rank=-1,store=None,group_name='')
torch.distributed.distributed_c10d.irecv(tensor,src,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.is_gloo_available()
torch.distributed.distributed_c10d.is_initialized()
torch.distributed.distributed_c10d.is_mpi_available()
torch.distributed.distributed_c10d.is_nccl_available()
torch.distributed.distributed_c10d.isend(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.new_group(ranks=None,timeout=default_pg_timeout,backend=None)
torch.distributed.distributed_c10d.recv(tensor,src=None,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.reduce(tensor,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False,dst_tensor=0)
torch.distributed.distributed_c10d.reduce_op(self)
torch.distributed.distributed_c10d.reduce_op.__getattribute__(self,key)
torch.distributed.distributed_c10d.reduce_op.__init__(self)
torch.distributed.distributed_c10d.reduce_scatter(output,input_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.reduce_scatter_multigpu(output_tensor_list,input_tensor_lists,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.scatter(tensor,scatter_list=None,src=0,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.send(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.gather(tensor,gather_list=None,dst=0,group=group.WORLD,async_op=False)
torch.distributed.get_backend(group=group.WORLD)
torch.distributed.get_rank(group=group.WORLD)
torch.distributed.get_world_size(group=group.WORLD)
torch.distributed.group(object)
torch.distributed.init_process_group(backend,init_method=None,timeout=default_pg_timeout,world_size=-1,rank=-1,store=None,group_name='')
torch.distributed.irecv(tensor,src,group=group.WORLD,tag=0)
torch.distributed.is_gloo_available()
torch.distributed.is_initialized()
torch.distributed.is_mpi_available()
torch.distributed.is_nccl_available()
torch.distributed.isend(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.new_group(ranks=None,timeout=default_pg_timeout,backend=None)
torch.distributed.recv(tensor,src=None,group=group.WORLD,tag=0)
torch.distributed.reduce(tensor,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False,dst_tensor=0)
torch.distributed.reduce_op(self)
torch.distributed.reduce_op.__getattribute__(self,key)
torch.distributed.reduce_scatter(output,input_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.reduce_scatter_multigpu(output_tensor_list,input_tensor_lists,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.scatter(tensor,scatter_list=None,src=0,group=group.WORLD,async_op=False)
torch.distributed.send(tensor,dst,group=group.WORLD,tag=0)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/launch.py----------------------------------------
A:torch.distributed.launch.parser->ArgumentParser(description='PyTorch distributed training launch helper utility that will spawn up multiple distributed processes')
A:torch.distributed.launch.args->parse_args()
A:torch.distributed.launch.current_env->os.environ.copy()
A:torch.distributed.launch.current_env['MASTER_PORT']->str(args.master_port)
A:torch.distributed.launch.current_env['WORLD_SIZE']->str(dist_world_size)
A:torch.distributed.launch.current_env['OMP_NUM_THREADS']->str(1)
A:torch.distributed.launch.current_env['RANK']->str(dist_rank)
A:torch.distributed.launch.current_env['LOCAL_RANK']->str(local_rank)
A:torch.distributed.launch.process->subprocess.Popen(cmd, env=current_env)
torch.distributed.launch.main()
torch.distributed.launch.parse_args()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/__init__.py----------------------------------------
torch.distributed.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/constants.py----------------------------------------
A:torch.distributed.constants.default_pg_timeout->timedelta(minutes=30)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/autograd/__init__.py----------------------------------------
A:torch.distributed.autograd.__init__.self.autograd_context->_new_context()
torch.distributed.autograd.__init__.context(object)
torch.distributed.autograd.__init__.context.__enter__(self)
torch.distributed.autograd.__init__.context.__exit__(self,type,value,traceback)
torch.distributed.autograd.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/rpc/internal.py----------------------------------------
A:torch.distributed.rpc.internal._thread_local_tensor_tables->threading.local()
A:torch.distributed.rpc.internal.self._dispatch_table->copyreg.dispatch_table.copy()
A:torch.distributed.rpc.internal.rref_fork_data->rref._serialize()
A:torch.distributed.rpc.internal.f->io.BytesIO()
A:torch.distributed.rpc.internal.p->pickle.Pickler(f)
A:torch.distributed.rpc.internal.ret->AttributeError(except_str)
A:torch.distributed.rpc.internal._internal_rpc_pickler->_InternalRPCPickler()
A:torch.distributed.rpc.internal.result->RemoteException(except_str, type(e))
A:torch.distributed.rpc.internal.except_str->'{}\n{}'.format(repr(e), traceback.format_exc())
A:torch.distributed.rpc.internal.profile_key->'rpc_{}#{}({} -> {})'.format(exec_type.value, str(func_name), current_worker_name, dest_worker_name)
A:torch.distributed.rpc.internal.rf->torch.autograd._RecordFunction()
A:torch.distributed.rpc.internal.PythonUDF->collections.namedtuple('PythonUDF', ['func', 'args', 'kwargs'])
A:torch.distributed.rpc.internal.RemoteException->collections.namedtuple('RemoteException', ['msg', 'exception_type'])
torch.distributed.rpc.internal.RPCExecMode(Enum)
torch.distributed.rpc.internal._InternalRPCPickler(self)
torch.distributed.rpc.internal._InternalRPCPickler.__init__(self)
torch.distributed.rpc.internal._InternalRPCPickler._rref_receiver(cls,rref_fork_data)
torch.distributed.rpc.internal._InternalRPCPickler._rref_reducer(self,rref)
torch.distributed.rpc.internal._InternalRPCPickler._tensor_receiver(cls,tensor_index)
torch.distributed.rpc.internal._InternalRPCPickler._tensor_reducer(self,tensor)
torch.distributed.rpc.internal._InternalRPCPickler.deserialize(self,binary_data,tensor_table)
torch.distributed.rpc.internal._InternalRPCPickler.serialize(self,obj)
torch.distributed.rpc.internal._handle_exception(result)
torch.distributed.rpc.internal._run_function(python_udf)
torch.distributed.rpc.internal._start_record_function(exec_type,func_name,current_worker_name,dest_worker_name)
torch.distributed.rpc.internal.deserialize(binary_data,tensor_table)
torch.distributed.rpc.internal.serialize(obj)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/rpc/api.py----------------------------------------
A:torch.distributed.rpc.api.logger->logging.getLogger(__name__)
A:torch.distributed.rpc.api.self.intent_worker_names->set()
A:torch.distributed.rpc.api.self.proceed_signal->threading.Event()
A:torch.distributed.rpc.api._wait_all_workers_dict_lock->threading.Lock()
A:torch.distributed.rpc.api._wait_all_workers_sequence_id_to_states->collections.defaultdict(WaitAllWorkersStates)
A:torch.distributed.rpc.api.timeout->timedelta(seconds=5)
A:torch.distributed.rpc.api.worker_name_to_response_future_dict->dict()
A:torch.distributed.rpc.api.fut->_invoke_rpc(to, func, RPCExecMode.SYNC, args, kwargs)
A:torch.distributed.rpc.api.rpc_agent->backend_registry.init_backend(backend, store=store, name=name, rank=rank, world_size=world_size, rpc_backend_options=rpc_backend_options)
A:torch.distributed.rpc.api.worker_infos->backend_registry.init_backend(backend, store=store, name=name, rank=rank, world_size=world_size, rpc_backend_options=rpc_backend_options).get_worker_infos()
A:torch.distributed.rpc.api.qualified_name->torch.jit._find_builtin(func)
A:torch.distributed.rpc.api.dst_worker_info->_to_worker_info(to)
A:torch.distributed.rpc.api.rf->_start_record_function(rpc_type, str(qualified_name) if qualified_name is not None else func.__qualname__, get_worker_info().name, dst_worker_info.name)
A:torch.distributed.rpc.api.(pickled_python_udf, tensors)->_default_pickler.serialize(PythonUDF(func, args, kwargs))
torch.distributed.rpc.WaitAllWorkersStates(self)
torch.distributed.rpc._init_rpc_backend(backend=backend_registry.BackendType.PROCESS_GROUP,store=None,name=None,rank=-1,world_size=-1,rpc_backend_options=None)
torch.distributed.rpc._invoke_rpc(to,func,rpc_type,args=None,kwargs=None)
torch.distributed.rpc._on_leader_follower_report_shutdown_intent(sequence_id,worker_name)
torch.distributed.rpc._require_initialized(func)
torch.distributed.rpc._set_proceed_shutdown_signal(sequence_id)
torch.distributed.rpc._to_worker_info(name_or_info)
torch.distributed.rpc._use_rpc_pickler(rpc_pickler)
torch.distributed.rpc._validate_rpc_args(backend,store,name,rank,world_size,rpc_backend_options)
torch.distributed.rpc._wait_all_workers()
torch.distributed.rpc.api.WaitAllWorkersStates(self)
torch.distributed.rpc.api.WaitAllWorkersStates.__init__(self)
torch.distributed.rpc.api._init_rpc_backend(backend=backend_registry.BackendType.PROCESS_GROUP,store=None,name=None,rank=-1,world_size=-1,rpc_backend_options=None)
torch.distributed.rpc.api._invoke_rpc(to,func,rpc_type,args=None,kwargs=None)
torch.distributed.rpc.api._on_leader_follower_report_shutdown_intent(sequence_id,worker_name)
torch.distributed.rpc.api._require_initialized(func)
torch.distributed.rpc.api._set_proceed_shutdown_signal(sequence_id)
torch.distributed.rpc.api._to_worker_info(name_or_info)
torch.distributed.rpc.api._use_rpc_pickler(rpc_pickler)
torch.distributed.rpc.api._validate_rpc_args(backend,store,name,rank,world_size,rpc_backend_options)
torch.distributed.rpc.api._wait_all_workers()
torch.distributed.rpc.api.get_worker_info(worker_name=None)
torch.distributed.rpc.api.remote(to,func,args=None,kwargs=None)
torch.distributed.rpc.api.rpc_async(to,func,args=None,kwargs=None)
torch.distributed.rpc.api.rpc_sync(to,func,args=None,kwargs=None)
torch.distributed.rpc.api.shutdown(graceful=True)
torch.distributed.rpc.get_worker_info(worker_name=None)
torch.distributed.rpc.remote(to,func,args=None,kwargs=None)
torch.distributed.rpc.rpc_async(to,func,args=None,kwargs=None)
torch.distributed.rpc.rpc_sync(to,func,args=None,kwargs=None)
torch.distributed.rpc.shutdown(graceful=True)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/rpc/backend_registry.py----------------------------------------
A:torch.distributed.rpc.backend_registry.BackendValue->collections.namedtuple('BackendValue', ['construct_rpc_backend_options_handler', 'init_backend_handler'])
A:torch.distributed.rpc.backend_registry.BackendType->enum.Enum(value='BackendType', names=extended_enum_dict)
A:torch.distributed.rpc.backend_registry.extended_enum_dict->dict({backend_name: BackendValue(construct_rpc_backend_options_handler=construct_rpc_backend_options_handler, init_backend_handler=init_backend_handler)}, **existing_enum_dict)
A:torch.distributed.rpc.backend_registry.group->torch.distributed.distributed_c10d._get_default_group()
torch.distributed.rpc.backend_registry._backend_type_repr(self)
torch.distributed.rpc.backend_registry._process_group_construct_rpc_backend_options_handler(rpc_timeout,init_method,num_send_recv_threads=rpc_constants.DEFAULT_NUM_SEND_RECV_THREADS,**kwargs)
torch.distributed.rpc.backend_registry._process_group_init_backend_handler(store,name,rank,world_size,rpc_backend_options)
torch.distributed.rpc.backend_registry.construct_rpc_backend_options(backend,rpc_timeout=rpc_constants.DEFAULT_RPC_TIMEOUT,init_method=rpc_constants.DEFAULT_INIT_METHOD,**kwargs)
torch.distributed.rpc.backend_registry.init_backend(backend,*args,**kwargs)
torch.distributed.rpc.backend_registry.register_backend(backend_name,construct_rpc_backend_options_handler,init_backend_handler)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/rpc/__init__.py----------------------------------------
A:torch.distributed.rpc.__init__.rpc_backend_options->backend_registry.construct_rpc_backend_options(backend)
A:torch.distributed.rpc.__init__.rendezvous_iterator->torch.distributed.rendezvous(rpc_backend_options.init_method, rank=rank, world_size=world_size)
A:torch.distributed.rpc.__init__.(store, _, _)->next(rendezvous_iterator)
A:torch.distributed.rpc.__init__.info->_rref_context_get_debug_info()
torch.distributed.rpc.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/rpc/constants.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/optim/optimizer.py----------------------------------------
A:torch.distributed.optim.optimizer.global_lock->Lock()
A:torch.distributed.optim.optimizer.self.optim->optim_cls([rref.local_value() for rref in local_params_rref], *args, **kwargs)
A:torch.distributed.optim.optimizer.all_local_grads->torch.distributed.autograd.get_gradients(autograd_ctx_id)
A:torch.distributed.optim.optimizer.local_optim->local_optim_rref.local_value()
A:torch.distributed.optim.optimizer.per_worker_params_rref->defaultdict(list)
A:torch.distributed.optim.optimizer.remote_optim_rref_fut->torch.distributed.rpc.rpc_async(worker, _new_local_optimizer, args=(optimizer_class, param_rrefs) + args, kwargs=kwargs)
A:torch.distributed.optim.optimizer.self.remote_optimizers->_wait_for_all(remote_optim_futs)
torch.distributed.optim.DistributedOptimizer(self,optimizer_class,params_rref,*args,**kwargs)
torch.distributed.optim.DistributedOptimizer.step(self,context_id)
torch.distributed.optim.optimizer.DistributedOptimizer(self,optimizer_class,params_rref,*args,**kwargs)
torch.distributed.optim.optimizer.DistributedOptimizer.__init__(self,optimizer_class,params_rref,*args,**kwargs)
torch.distributed.optim.optimizer.DistributedOptimizer.step(self,context_id)
torch.distributed.optim.optimizer._LocalOptimizer(self,optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._LocalOptimizer.__init__(self,optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._LocalOptimizer.step(self,autograd_ctx_id)
torch.distributed.optim.optimizer._local_optimizer_step(local_optim_rref,autograd_ctx_id)
torch.distributed.optim.optimizer._new_local_optimizer(optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._wait_for_all(rpc_futs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributed/optim/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/fuse_modules.py----------------------------------------
A:torch.quantization.fuse_modules.tokens->submodule_key.split('.')
A:torch.quantization.fuse_modules.cur_mod->getattr(cur_mod, s)
A:torch.quantization.fuse_modules.types->tuple((type(m) for m in mod_list))
A:torch.quantization.fuse_modules.fuser_method->OP_LIST_TO_FUSER_METHOD.get(types, None)
A:torch.quantization.fuse_modules.new_mod[0]->fuser_method(*mod_list)
A:torch.quantization.fuse_modules.new_mod[i]->torch.nn.Identity()
A:torch.quantization.fuse_modules.new_mod_list->fuser_func(mod_list)
A:torch.quantization.fuse_modules.model->copy.deepcopy(model)
torch.quantization.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules)
torch.quantization.fuse_modules._fuse_modules(model,modules_to_fuse,fuser_func=fuse_known_modules)
torch.quantization.fuse_modules._get_module(model,submodule_key)
torch.quantization.fuse_modules._set_module(model,submodule_key,module)
torch.quantization.fuse_modules.fuse_conv_bn(conv,bn)
torch.quantization.fuse_modules.fuse_conv_bn_relu(conv,bn,relu)
torch.quantization.fuse_modules.fuse_known_modules(mod_list)
torch.quantization.fuse_modules.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/default_mappings.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/observer.py----------------------------------------
A:torch.quantization.observer.r->_PartialWrapper(partial(cls_or_self, **kwargs))
A:torch.quantization.observer.ABC->ABCMeta(str('ABC'), (object,), {})
A:torch.quantization.observer.with_args->classmethod(_with_args)
A:torch.quantization.observer.scales->torch.max(scales, torch.tensor([self.eps], device=scales.device))
A:torch.quantization.observer.zero_points->zero_points.to(dtype=torch.int64).to(dtype=torch.int64)
A:torch.quantization.observer.min_vals->torch.min(torch.min(y, 1)[0], min_vals)
A:torch.quantization.observer.max_vals->torch.max(torch.max(y, 1)[0], max_vals)
A:torch.quantization.observer.min_val->torch.min(x)
A:torch.quantization.observer.max_val->torch.max(x)
A:torch.quantization.observer.scale->max(scale, self.eps)
A:torch.quantization.observer.zero_point->int(zero_point)
A:torch.quantization.observer.x->x_orig.detach()
A:torch.quantization.observer.x_dim->x_orig.detach().size()
A:torch.quantization.observer.new_axis_list->list(range(len(x_dim)))
A:torch.quantization.observer.y->torch.flatten(y, start_dim=1)
A:torch.quantization.observer.(scales, zero_points)->self.calculate_qparams()
A:torch.quantization.observer.dst_bin_of_begin->min(self.dst_nbins - 1, max(0.0, math.floor(src_bin_begin / dst_bin_width)))
A:torch.quantization.observer.dst_bin_of_end->min(self.dst_nbins - 1, max(0.0, math.floor(src_bin_end / dst_bin_width)))
A:torch.quantization.observer.total->sum(self.histogram)
A:torch.quantization.observer.cSum->torch.cumsum(self.histogram, dim=0)
A:torch.quantization.observer.norm_min->float('inf')
A:torch.quantization.observer.norm->_compute_quantization_error(next_start_bin, next_end_bin, 'L2')
A:torch.quantization.observer.downsample_rate->torch.ceil((combined_max - combined_min) / (self.bins * hist_bin_width)).to(torch.int).item()
A:torch.quantization.observer.start_idx->torch.round((self.min_val - combined_min) / hist_bin_width).to(torch.int).item()
A:torch.quantization.observer.upsampled_histogram->new_hist.repeat_interleave(upsample_rate)
A:torch.quantization.observer.histogram_with_output_range->torch.zeros(Nbins * downsample_rate, device=orig_hist.device)
A:torch.quantization.observer.shifted_integral_histogram->torch.zeros(Nbins, device=orig_hist.device)
A:torch.quantization.observer.self.histogram->torch.histc(x, self.bins, min=min_val, max=max_val)
A:torch.quantization.observer.new_min->torch.min(x)
A:torch.quantization.observer.new_max->torch.max(x)
A:torch.quantization.observer.combined_min->torch.min(new_min, min_val)
A:torch.quantization.observer.combined_max->torch.max(new_max, max_val)
A:torch.quantization.observer.(combined_min, combined_max, downsample_rate, start_idx)->self._adjust_min_max(combined_min, combined_max, self.upsample_rate)
A:torch.quantization.observer.combined_histogram->self._combine_histograms(combined_histogram, self.histogram, self.upsample_rate, downsample_rate, start_idx, self.bins)
A:torch.quantization.observer.(new_min, new_max)->self._non_linear_param_search()
A:torch.quantization.observer.default_observer->MinMaxObserver.with_args(reduce_range=True)
A:torch.quantization.observer.default_weight_observer->MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)
A:torch.quantization.observer.default_histogram_observer->HistogramObserver.with_args(reduce_range=True)
A:torch.quantization.observer.default_per_channel_weight_observer->PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
torch.quantization.HistogramObserver(self,bins=2048,upsample_rate=128,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.HistogramObserver._adjust_min_max(self,combined_min,combined_max,upsample_rate)
torch.quantization.HistogramObserver._combine_histograms(self,orig_hist,new_hist,upsample_rate,downsample_rate,start_idx,Nbins)
torch.quantization.HistogramObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.HistogramObserver._non_linear_param_search(self)
torch.quantization.HistogramObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.HistogramObserver.calculate_qparams(self)
torch.quantization.HistogramObserver.forward(self,x_orig)
torch.quantization.MinMaxObserver(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.MinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.MinMaxObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.MinMaxObserver.calculate_qparams(self)
torch.quantization.MinMaxObserver.extra_repr(self)
torch.quantization.MinMaxObserver.forward(self,x_orig)
torch.quantization.MovingAverageMinMaxObserver(self,averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.MovingAverageMinMaxObserver.forward(self,x_orig)
torch.quantization.MovingAveragePerChannelMinMaxObserver(self,averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)
torch.quantization.MovingAveragePerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.NoopObserver(self,dtype=torch.float16)
torch.quantization.NoopObserver.calculate_qparams(self)
torch.quantization.NoopObserver.forward(self,x)
torch.quantization.NoopObserver.get_qparams(self)
torch.quantization.ObserverBase(self,dtype)
torch.quantization.ObserverBase.calculate_qparams(self,**kwargs)
torch.quantization.ObserverBase.forward(self,x)
torch.quantization.ObserverBase.get_qparams(self,**kwargs)
torch.quantization.PerChannelMinMaxObserver(self,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)
torch.quantization.PerChannelMinMaxObserver._forward(self,x_orig)
torch.quantization.PerChannelMinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.PerChannelMinMaxObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.PerChannelMinMaxObserver.calculate_qparams(self)
torch.quantization.PerChannelMinMaxObserver.extra_repr(self)
torch.quantization.PerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.PerChannelMinMaxObserver.get_qparams(self)
torch.quantization.RecordingObserver(self,**kwargs)
torch.quantization.RecordingObserver.calculate_qparams(self)
torch.quantization.RecordingObserver.forward(self,x)
torch.quantization.RecordingObserver.get_tensor_value(self)
torch.quantization._ObserverBase(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization._ObserverBase._calculate_per_channel_qparams(self,min_vals,max_vals)
torch.quantization._ObserverBase._calculate_qparams(self,min_val,max_val)
torch.quantization._ObserverBase.get_qparams(self)
torch.quantization._with_args(cls_or_self,**kwargs)
torch.quantization.observer.HistogramObserver(self,bins=2048,upsample_rate=128,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer.HistogramObserver.__init__(self,bins=2048,upsample_rate=128,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer.HistogramObserver._adjust_min_max(self,combined_min,combined_max,upsample_rate)
torch.quantization.observer.HistogramObserver._combine_histograms(self,orig_hist,new_hist,upsample_rate,downsample_rate,start_idx,Nbins)
torch.quantization.observer.HistogramObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.observer.HistogramObserver._non_linear_param_search(self)
torch.quantization.observer.HistogramObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.observer.HistogramObserver.calculate_qparams(self)
torch.quantization.observer.HistogramObserver.forward(self,x_orig)
torch.quantization.observer.MinMaxObserver(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer.MinMaxObserver.__init__(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer.MinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.observer.MinMaxObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.observer.MinMaxObserver.calculate_qparams(self)
torch.quantization.observer.MinMaxObserver.extra_repr(self)
torch.quantization.observer.MinMaxObserver.forward(self,x_orig)
torch.quantization.observer.MovingAverageMinMaxObserver(self,averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer.MovingAverageMinMaxObserver.__init__(self,averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer.MovingAverageMinMaxObserver.forward(self,x_orig)
torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(self,averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)
torch.quantization.observer.MovingAveragePerChannelMinMaxObserver.__init__(self,averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)
torch.quantization.observer.MovingAveragePerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.observer.NoopObserver(self,dtype=torch.float16)
torch.quantization.observer.NoopObserver.__init__(self,dtype=torch.float16)
torch.quantization.observer.NoopObserver.calculate_qparams(self)
torch.quantization.observer.NoopObserver.forward(self,x)
torch.quantization.observer.NoopObserver.get_qparams(self)
torch.quantization.observer.ObserverBase(self,dtype)
torch.quantization.observer.ObserverBase.__init__(self,dtype)
torch.quantization.observer.ObserverBase.calculate_qparams(self,**kwargs)
torch.quantization.observer.ObserverBase.forward(self,x)
torch.quantization.observer.ObserverBase.get_qparams(self,**kwargs)
torch.quantization.observer.PerChannelMinMaxObserver(self,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)
torch.quantization.observer.PerChannelMinMaxObserver.__init__(self,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)
torch.quantization.observer.PerChannelMinMaxObserver._forward(self,x_orig)
torch.quantization.observer.PerChannelMinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.observer.PerChannelMinMaxObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.observer.PerChannelMinMaxObserver.calculate_qparams(self)
torch.quantization.observer.PerChannelMinMaxObserver.extra_repr(self)
torch.quantization.observer.PerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.observer.PerChannelMinMaxObserver.get_qparams(self)
torch.quantization.observer.RecordingObserver(self,**kwargs)
torch.quantization.observer.RecordingObserver.__init__(self,**kwargs)
torch.quantization.observer.RecordingObserver.calculate_qparams(self)
torch.quantization.observer.RecordingObserver.forward(self,x)
torch.quantization.observer.RecordingObserver.get_tensor_value(self)
torch.quantization.observer._ObserverBase(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer._ObserverBase.__init__(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer._ObserverBase._calculate_per_channel_qparams(self,min_vals,max_vals)
torch.quantization.observer._ObserverBase._calculate_qparams(self,min_val,max_val)
torch.quantization.observer._ObserverBase.get_qparams(self)
torch.quantization.observer._with_args(cls_or_self,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/qconfig.py----------------------------------------
A:torch.quantization.qconfig.default_qconfig->QConfig(activation=default_observer, weight=default_weight_observer)
A:torch.quantization.qconfig.default_debug_qconfig->QConfig(weight=default_weight_observer, activation=default_debug_observer)
A:torch.quantization.qconfig.default_per_channel_qconfig->QConfig(activation=default_observer, weight=default_per_channel_weight_observer)
A:torch.quantization.qconfig.default_dynamic_qconfig->QConfigDynamic(weight=default_weight_observer)
A:torch.quantization.qconfig.float16_dynamic_qconfig->QConfigDynamic(weight=NoopObserver.with_args(dtype=torch.float16))
A:torch.quantization.qconfig.per_channel_dynamic_qconfig->QConfigDynamic(weight=default_per_channel_weight_observer)
A:torch.quantization.qconfig.default_qat_qconfig->QConfig(activation=default_fake_quant, weight=default_weight_fake_quant)
A:torch.quantization.qconfig.default_weight_only_qconfig->QConfig(activation=torch.nn.Identity, weight=default_weight_fake_quant)
A:torch.quantization.qconfig.default_activation_only_qconfig->QConfig(activation=default_fake_quant, weight=torch.nn.Identity)
A:torch.quantization.qconfig.qconfig->QConfig(activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, reduce_range=False), weight=default_weight_fake_quant)
torch.quantization.QConfig(cls,activation,weight)
torch.quantization.QConfigDynamic(cls,weight)
torch.quantization.get_default_qat_qconfig(backend='fbgemm')
torch.quantization.get_default_qconfig(backend='fbgemm')
torch.quantization.qconfig.QConfig(cls,activation,weight)
torch.quantization.qconfig.QConfig.__new__(cls,activation,weight)
torch.quantization.qconfig.QConfigDynamic(cls,weight)
torch.quantization.qconfig.QConfigDynamic.__new__(cls,weight)
torch.quantization.qconfig.get_default_qat_qconfig(backend='fbgemm')
torch.quantization.qconfig.get_default_qconfig(backend='fbgemm')


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/fake_quantize.py----------------------------------------
A:torch.quantization.fake_quantize.self.activation_post_process->observer(**observer_kwargs)
A:torch.quantization.fake_quantize.(_scale, _zero_point)->self.calculate_qparams()
A:torch.quantization.fake_quantize.X->torch.fake_quantize_per_tensor_affine(X, float(self.scale), int(self.zero_point), self.quant_min, self.quant_max)
A:torch.quantization.fake_quantize.with_args->classmethod(_with_args)
A:torch.quantization.fake_quantize.default_fake_quant->FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
A:torch.quantization.fake_quantize.default_weight_fake_quant->FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False)
A:torch.quantization.fake_quantize.default_per_channel_weight_fake_quant->FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0)
A:torch.quantization.fake_quantize.default_histogram_fake_quant->FakeQuantize.with_args(observer=HistogramObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
torch.quantization.FakeQuantize(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.quantization.FakeQuantize._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.FakeQuantize._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.FakeQuantize.calculate_qparams(self)
torch.quantization.FakeQuantize.disable_fake_quant(self)
torch.quantization.FakeQuantize.disable_observer(self)
torch.quantization.FakeQuantize.enable_fake_quant(self,enabled=True)
torch.quantization.FakeQuantize.enable_observer(self,enabled=True)
torch.quantization.FakeQuantize.extra_repr(self)
torch.quantization.FakeQuantize.forward(self,X)
torch.quantization.disable_fake_quant(mod)
torch.quantization.disable_observer(mod)
torch.quantization.enable_fake_quant(mod)
torch.quantization.enable_observer(mod)
torch.quantization.fake_quantize.FakeQuantize(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.quantization.fake_quantize.FakeQuantize.__init__(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.quantization.fake_quantize.FakeQuantize._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.fake_quantize.FakeQuantize._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.fake_quantize.FakeQuantize.calculate_qparams(self)
torch.quantization.fake_quantize.FakeQuantize.disable_fake_quant(self)
torch.quantization.fake_quantize.FakeQuantize.disable_observer(self)
torch.quantization.fake_quantize.FakeQuantize.enable_fake_quant(self,enabled=True)
torch.quantization.fake_quantize.FakeQuantize.enable_observer(self,enabled=True)
torch.quantization.fake_quantize.FakeQuantize.extra_repr(self)
torch.quantization.fake_quantize.FakeQuantize.forward(self,X)
torch.quantization.fake_quantize.disable_fake_quant(mod)
torch.quantization.fake_quantize.disable_observer(mod)
torch.quantization.fake_quantize.enable_fake_quant(mod)
torch.quantization.fake_quantize.enable_observer(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/__init__.py----------------------------------------
torch.quantization.__init__.default_eval_fn(model,calib_data)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/stubs.py----------------------------------------
A:torch.quantization.stubs.X->self.module(X)
torch.quantization.DeQuantStub(self)
torch.quantization.DeQuantStub.forward(self,x)
torch.quantization.QuantStub(self,qconfig=None)
torch.quantization.QuantStub.forward(self,x)
torch.quantization.QuantWrapper(self,module)
torch.quantization.QuantWrapper.forward(self,X)
torch.quantization.stubs.DeQuantStub(self)
torch.quantization.stubs.DeQuantStub.__init__(self)
torch.quantization.stubs.DeQuantStub.forward(self,x)
torch.quantization.stubs.QuantStub(self,qconfig=None)
torch.quantization.stubs.QuantStub.__init__(self,qconfig=None)
torch.quantization.stubs.QuantStub.forward(self,x)
torch.quantization.stubs.QuantWrapper(self,module)
torch.quantization.stubs.QuantWrapper.__init__(self,module)
torch.quantization.stubs.QuantWrapper.forward(self,X)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/quantize.py----------------------------------------
A:torch.quantization.quantize.module_qconfig->getattr(module, 'qconfig', module_qconfig)
A:torch.quantization.quantize.child.activation_post_process->child.qconfig.activation()
A:torch.quantization.quantize.module._modules[name]->add_quant_dequant(child)
A:torch.quantization.quantize.model->copy.deepcopy(model)
A:torch.quantization.quantize.qconfig_spec->dict(zip(qconfig_spec, itertools.repeat(default_qconfig)))
A:torch.quantization.quantize.module->copy.deepcopy(module)
A:torch.quantization.quantize.reassign[name]->swap_module(mod, mapping)
A:torch.quantization.quantize.new_mod->mapping[type(mod)].from_float(mod)
torch.quantization._observer_forward_hook(self,input,output)
torch.quantization._propagate_qconfig_helper(module,qconfig_dict,white_list=None,qconfig_parent=None,prefix='')
torch.quantization.add_observer_(module)
torch.quantization.add_quant_dequant(module)
torch.quantization.convert(module,mapping=None,inplace=False)
torch.quantization.get_observer_dict(mod,target_dict,prefix='')
torch.quantization.prepare(model,inplace=False)
torch.quantization.prepare_qat(model,mapping=None,inplace=False)
torch.quantization.propagate_qconfig_(module,qconfig_dict=None)
torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)
torch.quantization.quantize._observer_forward_hook(self,input,output)
torch.quantization.quantize._propagate_qconfig_helper(module,qconfig_dict,white_list=None,qconfig_parent=None,prefix='')
torch.quantization.quantize.add_observer_(module)
torch.quantization.quantize.add_quant_dequant(module)
torch.quantization.quantize.convert(module,mapping=None,inplace=False)
torch.quantization.quantize.get_observer_dict(mod,target_dict,prefix='')
torch.quantization.quantize.prepare(model,inplace=False)
torch.quantization.quantize.prepare_qat(model,mapping=None,inplace=False)
torch.quantization.quantize.propagate_qconfig_(module,qconfig_dict=None)
torch.quantization.quantize.quantize(model,run_fn,run_args,mapping=None,inplace=False)
torch.quantization.quantize.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)
torch.quantization.quantize.quantize_qat(model,run_fn,run_args,inplace=False)
torch.quantization.quantize.swap_module(mod,mapping)
torch.quantization.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)
torch.quantization.quantize_qat(model,run_fn,run_args,inplace=False)
torch.quantization.swap_module(mod,mapping)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/quantization/_quantize_script.py----------------------------------------
A:torch.quantization._quantize_script.wq->torch._empty_affine_quantized([1, 1, 1, 1], scale=1.0, zero_point=0, dtype=torch.qint8)
A:torch.quantization._quantize_script.self._packed_params->torch.ops.quantized.conv2d_prepack(weight, bias, self.stride, self.padding, self.dilation, self.groups)
A:torch.quantization._quantize_script.(qweight, bias)->self._weight_bias()
A:torch.quantization._quantize_script.model->convert_script(model, True, debug)
torch.quantization._quantize_script.ConvPackedParams(self)
torch.quantization._quantize_script.ConvPackedParams.__getstate__(self)
torch.quantization._quantize_script.ConvPackedParams.__init__(self)
torch.quantization._quantize_script.ConvPackedParams.__setstate__(self,state)
torch.quantization._quantize_script.ConvPackedParams._weight_bias(self)
torch.quantization._quantize_script.ConvPackedParams.forward(self,x)
torch.quantization._quantize_script.ConvPackedParams.set_conv_params(self,stride,padding,dilation,groups)
torch.quantization._quantize_script.ConvPackedParams.set_weight_bias(self,weight,bias)
torch.quantization._quantize_script._check_is_script_module(model)
torch.quantization._quantize_script.convert_script(model,inplace=False,debug=False)
torch.quantization._quantize_script.prepare_script(model,qconfig_dict,inplace=False)
torch.quantization._quantize_script.quantize_script(model,qconfig_dict,run_fn,run_args,inplace=False,debug=False)
torch.quantization._quantize_script.script_qconfig(qconfig)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/init.py----------------------------------------
A:torch.nn.init.dimensions->tensor.dim()
A:torch.nn.init.sizes->tensor.size()
A:torch.nn.init.min_dim->min(out_chans_per_grp, sizes[1])
A:torch.nn.init.num_input_fmaps->tensor.size(1)
A:torch.nn.init.num_output_fmaps->tensor.size(0)
A:torch.nn.init.receptive_field_size->tensor[0][0].numel()
A:torch.nn.init.(fan_in, fan_out)->_calculate_fan_in_and_fan_out(tensor)
A:torch.nn.init.mode->mode.lower().lower()
A:torch.nn.init.fan->_calculate_correct_fan(tensor, mode)
A:torch.nn.init.gain->calculate_gain(nonlinearity, a)
A:torch.nn.init.rows->tensor.size(0)
A:torch.nn.init.flattened->tensor.new(rows, cols).normal_(0, 1)
A:torch.nn.init.(q, r)->torch.qr(flattened)
A:torch.nn.init.d->torch.diag(r, 0)
A:torch.nn.init.ph->torch.diag(r, 0).sign()
A:torch.nn.init.num_zeros->int(math.ceil(sparsity * rows))
A:torch.nn.init.row_indices->torch.randperm(rows)
A:torch.nn.init.deprecated_init.__doc__->'\n    {old_name}(...)\n\n    .. warning::\n        This method is now deprecated in favor of :func:`torch.nn.init.{new_name}`.\n\n    See :func:`~torch.nn.init.{new_name}` for details.'.format(old_name=old_name, new_name=new_name)
A:torch.nn.init.uniform->_make_deprecate(uniform_)
A:torch.nn.init.normal->_make_deprecate(normal_)
A:torch.nn.init.constant->_make_deprecate(constant_)
A:torch.nn.init.eye->_make_deprecate(eye_)
A:torch.nn.init.dirac->_make_deprecate(dirac_)
A:torch.nn.init.xavier_uniform->_make_deprecate(xavier_uniform_)
A:torch.nn.init.xavier_normal->_make_deprecate(xavier_normal_)
A:torch.nn.init.kaiming_uniform->_make_deprecate(kaiming_uniform_)
A:torch.nn.init.kaiming_normal->_make_deprecate(kaiming_normal_)
A:torch.nn.init.orthogonal->_make_deprecate(orthogonal_)
A:torch.nn.init.sparse->_make_deprecate(sparse_)
torch.nn.init._calculate_correct_fan(tensor,mode)
torch.nn.init._calculate_fan_in_and_fan_out(tensor)
torch.nn.init._make_deprecate(meth)
torch.nn.init._no_grad_fill_(tensor,val)
torch.nn.init._no_grad_normal_(tensor,mean,std)
torch.nn.init._no_grad_uniform_(tensor,a,b)
torch.nn.init._no_grad_zero_(tensor)
torch.nn.init.calculate_gain(nonlinearity,param=None)
torch.nn.init.constant_(tensor,val)
torch.nn.init.dirac_(tensor,groups=1)
torch.nn.init.eye_(tensor)
torch.nn.init.kaiming_normal_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.kaiming_uniform_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.normal_(tensor,mean=0.0,std=1.0)
torch.nn.init.ones_(tensor)
torch.nn.init.orthogonal_(tensor,gain=1)
torch.nn.init.sparse_(tensor,sparsity,std=0.01)
torch.nn.init.uniform_(tensor,a=0.0,b=1.0)
torch.nn.init.xavier_normal_(tensor,gain=1.0)
torch.nn.init.xavier_uniform_(tensor,gain=1.0)
torch.nn.init.zeros_(tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parameter.py----------------------------------------
A:torch.nn.parameter.data->torch.Tensor()
A:torch.nn.parameter.result->type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)
torch.nn.Parameter(cls,data=None,requires_grad=True)
torch.nn.Parameter.__deepcopy__(self,memo)
torch.nn.Parameter.__reduce_ex__(self,proto)
torch.nn.Parameter.__repr__(self)
torch.nn.parameter.Parameter(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__deepcopy__(self,memo)
torch.nn.parameter.Parameter.__new__(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__reduce_ex__(self,proto)
torch.nn.parameter.Parameter.__repr__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parameter.pyi----------------------------------------
torch.nn.parameter.Parameter.__init__(self,data:Tensor=...,requires_grad:builtins.bool=...)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/grad.py----------------------------------------
A:torch.nn.grad.input_size->list(input_size)
A:torch.nn.grad.stride->_triple(stride)
A:torch.nn.grad.padding->_triple(padding)
A:torch.nn.grad.dilation->_triple(dilation)
A:torch.nn.grad.grad_input_padding->_grad_input_padding(grad_output, input_size, stride, padding, kernel_size)
A:torch.nn.grad.grad_output->grad_output.contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4]).contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4])
A:torch.nn.grad.input->input.contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4]).contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4])
A:torch.nn.grad.grad_weight->grad_weight.contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4]).contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4])
torch.nn.grad._grad_input_padding(grad_output,input_size,stride,padding,kernel_size)
torch.nn.grad.conv1d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv1d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/functional.py----------------------------------------
A:torch.nn.functional.conv1d->_add_docstr(torch.conv1d, '\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or\n      a one-element tuple `(sW,)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a\n      single number or a one-element tuple `(padW,)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a one-element tuple `(dW,)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3)\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> F.conv1d(inputs, filters)\n')
A:torch.nn.functional.conv2d->_add_docstr(torch.conv2d, '\nconv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8,4,3,3)\n    >>> inputs = torch.randn(1,4,5,5)\n    >>> F.conv2d(inputs, filters, padding=1)\n')
A:torch.nn.functional.conv3d->_add_docstr(torch.conv3d, '\nconv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 3D convolution over an input image composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv3d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3, 3, 3)\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> F.conv3d(inputs, filters)\n')
A:torch.nn.functional.conv_transpose1d->_add_docstr(torch.conv_transpose1d, '\nconv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called "deconvolution".\n\nSee :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sW,)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padW,)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dW,)``. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> weights = torch.randn(16, 33, 5)\n    >>> F.conv_transpose1d(inputs, weights)\n')
A:torch.nn.functional.conv_transpose2d->_add_docstr(torch.conv_transpose2d, '\nconv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution".\n\nSee :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n      Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dH, dW)``. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> weights = torch.randn(4, 8, 3, 3)\n    >>> F.conv_transpose2d(inputs, weights, padding=1)\n')
A:torch.nn.functional.conv_transpose3d->_add_docstr(torch.conv_transpose3d, '\nconv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution"\n\nSee :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sT, sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padT, padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple\n      ``(out_padT, out_padH, out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> weights = torch.randn(16, 33, 3, 3, 3)\n    >>> F.conv_transpose3d(inputs, weights)\n')
A:torch.nn.functional.conv_tbc->_add_docstr(torch.conv_tbc, '\nApplies a 1-dimensional sequence convolution over an input sequence.\nInput and output dimensions are (Time, Batch, Channels) - hence TBC.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n    bias: bias of shape (:math:`\\text{out\\_channels}`)\n    pad: number of timesteps to pad. Default: 0\n')
A:torch.nn.functional.avg_pool1d->_add_docstr(torch.avg_pool1d, '\navg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple `(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])\n\n')
A:torch.nn.functional.avg_pool2d->_add_docstr(torch._C._nn.avg_pool2d, '\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\n\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n    divisor_override: if specified, it will be used as divisor, otherwise\n         size of the pooling region will be used. Default: None\n')
A:torch.nn.functional.avg_pool3d->_add_docstr(torch._C._nn.avg_pool3d, '\navg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\nsize :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n\nSee :class:`~torch.nn.AvgPool3d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kT, kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`, Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation\n    divisor_override: if specified, it will be used as divisor, otherwise\n        size of the pooling region will be used. Default: None\n')
A:torch.nn.functional._output_ratio->_triple(output_ratio)
A:torch.nn.functional._random_samples->torch.rand(input.size(0), input.size(1), 3, dtype=input.dtype, device=input.device)
A:torch.nn.functional.fractional_max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool2d_with_indices, if_false=_fractional_max_pool2d, module_name=__name__, func_name='fractional_max_pool2d')
A:torch.nn.functional.fractional_max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool3d_with_indices, if_false=_fractional_max_pool3d, module_name=__name__, func_name='fractional_max_pool3d')
A:torch.nn.functional.stride->torch.jit.annotate(List[int], [])
A:torch.nn.functional.max_pool1d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool1d_with_indices, if_false=_max_pool1d, module_name=__name__, func_name='max_pool1d')
A:torch.nn.functional.max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool2d_with_indices, if_false=_max_pool2d, module_name=__name__, func_name='max_pool2d')
A:torch.nn.functional.max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool3d_with_indices, if_false=_max_pool3d, module_name=__name__, func_name='max_pool3d')
A:torch.nn.functional.input_size->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size()
A:torch.nn.functional.default_size->torch.jit.annotate(List[int], [])
A:torch.nn.functional.kernel_size->_triple(kernel_size)
A:torch.nn.functional._stride->_triple(stride)
A:torch.nn.functional.padding->_triple(padding)
A:torch.nn.functional.output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.(kw, kh)->modules.utils._pair(kernel_size)
A:torch.nn.functional.out->torch._C._nn.nll_loss2d(input, target, weight, reduction_enum, ignore_index)
A:torch.nn.functional.adaptive_max_pool1d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool1d_with_indices, if_false=_adaptive_max_pool1d, module_name=__name__, func_name='adaptive_max_pool1d')
A:torch.nn.functional.adaptive_max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool2d_with_indices, if_false=_adaptive_max_pool2d, module_name=__name__, func_name='adaptive_max_pool2d')
A:torch.nn.functional.adaptive_max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool3d_with_indices, if_false=_adaptive_max_pool3d, module_name=__name__, func_name='adaptive_max_pool3d')
A:torch.nn.functional.adaptive_avg_pool1d->_add_docstr(torch.adaptive_avg_pool1d, '\nadaptive_avg_pool1d(input, output_size) -> Tensor\n\nApplies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\n\nSee :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n\nArgs:\n    output_size: the target output size (single integer)\n')
A:torch.nn.functional._output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.result->torch.rrelu(input, lower, upper, training)
A:torch.nn.functional.threshold_->_add_docstr(_VF.threshold_, '\nthreshold_(input, threshold, value) -> Tensor\n\nIn-place version of :func:`~threshold`.\n')
A:torch.nn.functional.relu_->_add_docstr(torch.relu_, '\nrelu_(input) -> Tensor\n\nIn-place version of :func:`~relu`.\n')
A:torch.nn.functional.hardtanh_->_add_docstr(torch._C._nn.hardtanh_, '\nhardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n\nIn-place version of :func:`~hardtanh`.\n')
A:torch.nn.functional.elu_->_add_docstr(torch._C._nn.elu_, '\nelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~elu`.\n')
A:torch.nn.functional.selu_->_add_docstr(torch.selu_, '\nselu_(input) -> Tensor\n\nIn-place version of :func:`~selu`.\n')
A:torch.nn.functional.celu_->_add_docstr(torch.celu_, '\ncelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~celu`.\n')
A:torch.nn.functional.leaky_relu_->_add_docstr(torch._C._nn.leaky_relu_, '\nleaky_relu_(input, negative_slope=0.01) -> Tensor\n\nIn-place version of :func:`~leaky_relu`.\n')
A:torch.nn.functional.rrelu_->_add_docstr(torch.rrelu_, '\nrrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n\nIn-place version of :func:`~rrelu`.\n')
A:torch.nn.functional.logsigmoid->_add_docstr(torch._C._nn.log_sigmoid, '\nlogsigmoid(input) -> Tensor\n\nApplies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n\nSee :class:`~torch.nn.LogSigmoid` for more details.\n')
A:torch.nn.functional.softplus->_add_docstr(torch._C._nn.softplus, '\nsoftplus(input, beta=1, threshold=20) -> Tensor\n\nApplies element-wise, the function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))`.\n\nFor numerical stability the implementation reverts to the linear function\nwhen :math:`input \\times \\beta > threshold`.\n\nSee :class:`~torch.nn.Softplus` for more details.\n')
A:torch.nn.functional.dim->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).dim()
A:torch.nn.functional.ret->loss.sum()
A:torch.nn.functional.y_soft->gumbels.softmax(dim)
A:torch.nn.functional.y_hard->torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)
A:torch.nn.functional.softshrink->_add_docstr(torch._C._nn.softshrink, '\nsoftshrink(input, lambd=0.5) -> Tensor\n\nApplies the soft shrinkage function elementwise\n\nSee :class:`~torch.nn.Softshrink` for more details.\n')
A:torch.nn.functional.output->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).matmul(weight.t())
A:torch.nn.functional.input->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4)
A:torch.nn.functional.offsets->torch.arange(0, input.numel(), input.size(1), dtype=torch.long, device=input.device)
A:torch.nn.functional.per_sample_weights->per_sample_weights.reshape(-1).reshape(-1)
A:torch.nn.functional.(ret, _, _, _)->torch.embedding_bag(weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset)
A:torch.nn.functional.div->div.mul(alpha).add(k).pow(beta).mul(alpha).add(k).pow(beta)
A:torch.nn.functional.sizes->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size()
A:torch.nn.functional.reduction->_Reduction.legacy_get_string(size_average, reduce)
A:torch.nn.functional.n->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size(0)
A:torch.nn.functional.c->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size(1)
A:torch.nn.functional.target->target.view(n, 0, 0).view(n, 0, 0)
A:torch.nn.functional.reduction_enum->_Reduction.get_enum(reduction)
A:torch.nn.functional.reduced->torch.kl_div(input, target, reduction_enum)
A:torch.nn.functional.new_size->_infer_size(target.size(), weight.size())
A:torch.nn.functional.weight->weight.expand(new_size).expand(new_size)
A:torch.nn.functional.d->lambd(input, target)
A:torch.nn.functional.(expanded_input, expanded_target)->torch.broadcast_tensors(input, target)
A:torch.nn.functional.t->torch.abs(input - target)
A:torch.nn.functional.pixel_shuffle->_add_docstr(torch.pixel_shuffle, '\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)`.\n\nSee :class:`~torch.nn.PixelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    upscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n')
A:torch.nn.functional.scale_factor_list->torch.jit.annotate(List[Optional[float]], [elem for elem in _scale_factor_repeated])
A:torch.nn.functional.pdist->_add_docstr(torch.pdist, "\npdist(input, p=2) -> Tensor\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape :math:`N \\times M` then the output will have shape\n:math:`\\frac{1}{2} N (N - 1)`.\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n'minkowski', p=p)` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\nequivalent to `scipy.spatial.distance.pdist(input, 'hamming') * M`.\nWhen :math:`p = \\infty`, the closest scipy function is\n`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.\n\nArgs:\n    input: input tensor of shape :math:`N \\times M`.\n    p: p value for the p-norm distance to calculate between each vector pair\n        :math:`\\in [0, \\infty]`.\n")
A:torch.nn.functional.cosine_similarity->_add_docstr(torch.cosine_similarity, '\ncosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between x1 and x2, computed along dim.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input (of size matching x1).\n    dim (int, optional): Dimension of vectors. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nShape:\n    - Input: :math:`(\\ast_1, D, \\ast_2)` where D is at position `dim`.\n    - Output: :math:`(\\ast_1, \\ast_2)` where 1 is at position `dim`.\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)\n')
A:torch.nn.functional.one_hot->_add_docstr(torch._C._nn.one_hot, '\none_hot(tensor, num_classes=-1) -> LongTensor\n\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.\n\nSee also `One-hot on Wikipedia`_ .\n\n.. _One-hot on Wikipedia:\n    https://en.wikipedia.org/wiki/One-hot\n\nArguments:\n    tensor (LongTensor): class values of any shape.\n    num_classes (int):  Total number of classes. If set to -1, the number\n        of classes will be inferred as one greater than the largest class\n        value in the input tensor.\n\nReturns:\n    LongTensor that has one more dimension with 1 values at the\n    index of last dimension indicated by the input, and 0 everywhere\n    else.\n\nExamples:\n    >>> F.one_hot(torch.arange(0, 5) % 3)\n    tensor([[1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 1, 0]])\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n    tensor([[1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0]])\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n    tensor([[[1, 0, 0],\n             [0, 1, 0]],\n            [[0, 0, 1],\n             [1, 0, 0]],\n            [[0, 1, 0],\n             [0, 0, 1]]])\n')
A:torch.nn.functional.denom->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).norm(p, dim, keepdim=True).clamp_min_(eps).expand_as(input)
A:torch.nn.functional.(tgt_len, bsz, embed_dim)->query.size()
A:torch.nn.functional.(q, k, v)->linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)
A:torch.nn.functional.q->q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1).contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
A:torch.nn.functional.(k, v)->linear(key, _w, _b).chunk(2, dim=-1)
A:torch.nn.functional.k->torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)
A:torch.nn.functional.v->torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)
A:torch.nn.functional.q_proj_weight_non_opt->torch.jit._unwrap_optional(q_proj_weight)
A:torch.nn.functional.(len1, len2)->torch.jit._unwrap_optional(v_proj_weight).size()
A:torch.nn.functional.k_proj_weight_non_opt->torch.jit._unwrap_optional(k_proj_weight)
A:torch.nn.functional.v_proj_weight_non_opt->torch.jit._unwrap_optional(v_proj_weight)
A:torch.nn.functional.attn_mask->pad(attn_mask, (0, 1))
A:torch.nn.functional.key_padding_mask->pad(key_padding_mask, (0, 1))
A:torch.nn.functional.src_len->torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1).size(1)
A:torch.nn.functional.attn_output_weights->attn_output_weights.view(bsz, num_heads, tgt_len, src_len).view(bsz, num_heads, tgt_len, src_len)
A:torch.nn.functional.attn_output->linear(attn_output, out_proj_weight, out_proj_bias)
torch.nn._adaptive_max_pool1d(input,output_size,return_indices=False)
torch.nn._adaptive_max_pool2d(input,output_size,return_indices=False)
torch.nn._adaptive_max_pool3d(input,output_size,return_indices=False)
torch.nn._fractional_max_pool2d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn._fractional_max_pool3d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn._get_softmax_dim(name,ndim,stacklevel)
torch.nn._interp_output_size(dim,closed_over_args)
torch.nn._max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._max_pool3d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._no_grad_embedding_renorm_(weight,input,max_norm,norm_type)
torch.nn._pad(input,pad,mode='constant',value=0)
torch.nn._pad_circular(input,padding)
torch.nn._pointwise_loss(lambd,lambd_optimized,input,target,reduction='mean')
torch.nn._smooth_l1_loss(input,target)
torch.nn._unpool_output_size(input,kernel_size,stride,padding,output_size)
torch.nn._verify_batch_size(size)
torch.nn.adaptive_avg_pool2d(input,output_size)
torch.nn.adaptive_avg_pool3d(input,output_size)
torch.nn.adaptive_max_pool1d_with_indices(input,output_size,return_indices=False)
torch.nn.adaptive_max_pool2d_with_indices(input,output_size,return_indices=False)
torch.nn.adaptive_max_pool3d_with_indices(input,output_size,return_indices=False)
torch.nn.affine_grid(theta,size,align_corners=None)
torch.nn.alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.assert_int_or_pair(arg,arg_name,message)
torch.nn.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)
torch.nn.bilinear(input1,input2,weight,bias=None)
torch.nn.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.celu(input,alpha=1.0,inplace=False)
torch.nn.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean',zero_infinity=False)
torch.nn.dropout(input,p=0.5,training=True,inplace=False)
torch.nn.dropout2d(input,p=0.5,training=True,inplace=False)
torch.nn.dropout3d(input,p=0.5,training=True,inplace=False)
torch.nn.elu(input,alpha=1.0,inplace=False)
torch.nn.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False,per_sample_weights=None,include_last_offset=False)
torch.nn.feature_alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.fractional_max_pool2d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.fractional_max_pool3d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._adaptive_max_pool1d(input,output_size,return_indices=False)
torch.nn.functional._adaptive_max_pool2d(input,output_size,return_indices=False)
torch.nn.functional._adaptive_max_pool3d(input,output_size,return_indices=False)
torch.nn.functional._fractional_max_pool2d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._fractional_max_pool3d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._get_softmax_dim(name,ndim,stacklevel)
torch.nn.functional._interp_output_size(dim,closed_over_args)
torch.nn.functional._max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._max_pool3d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._no_grad_embedding_renorm_(weight,input,max_norm,norm_type)
torch.nn.functional._pad(input,pad,mode='constant',value=0)
torch.nn.functional._pad_circular(input,padding)
torch.nn.functional._pointwise_loss(lambd,lambd_optimized,input,target,reduction='mean')
torch.nn.functional._smooth_l1_loss(input,target)
torch.nn.functional._unpool_output_size(input,kernel_size,stride,padding,output_size)
torch.nn.functional._verify_batch_size(size)
torch.nn.functional.adaptive_avg_pool2d(input,output_size)
torch.nn.functional.adaptive_avg_pool3d(input,output_size)
torch.nn.functional.adaptive_max_pool1d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.adaptive_max_pool2d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.adaptive_max_pool3d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.affine_grid(theta,size,align_corners=None)
torch.nn.functional.alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.functional.assert_int_or_pair(arg,arg_name,message)
torch.nn.functional.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)
torch.nn.functional.bilinear(input1,input2,weight,bias=None)
torch.nn.functional.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.functional.celu(input,alpha=1.0,inplace=False)
torch.nn.functional.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.functional.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean',zero_infinity=False)
torch.nn.functional.dropout(input,p=0.5,training=True,inplace=False)
torch.nn.functional.dropout2d(input,p=0.5,training=True,inplace=False)
torch.nn.functional.dropout3d(input,p=0.5,training=True,inplace=False)
torch.nn.functional.elu(input,alpha=1.0,inplace=False)
torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.functional.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False,per_sample_weights=None,include_last_offset=False)
torch.nn.functional.feature_alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.functional.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.functional.fractional_max_pool2d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional.fractional_max_pool3d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional.gelu(input)
torch.nn.functional.glu(input,dim=-1)
torch.nn.functional.grid_sample(input,grid,mode='bilinear',padding_mode='zeros',align_corners=None)
torch.nn.functional.group_norm(input,num_groups,weight=None,bias=None,eps=1e-05)
torch.nn.functional.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)
torch.nn.functional.hardshrink(input,lambd=0.5)
torch.nn.functional.hardsigmoid(input,inplace=False)
torch.nn.functional.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.functional.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)
torch.nn.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None,recompute_scale_factor=None)
torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)
torch.nn.functional.leaky_relu(input,negative_slope=0.01,inplace=False)
torch.nn.functional.linear(input,weight,bias=None)
torch.nn.functional.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.functional.log_softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.functional.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.functional.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.max_pool1d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_pool2d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_pool3d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multi_head_attention_forward(query,key,value,embed_dim_to_check,num_heads,in_proj_weight,in_proj_bias,bias_k,bias_v,add_zero_attn,dropout_p,out_proj_weight,out_proj_bias,training=True,key_padding_mask=None,need_weights=True,attn_mask=None,use_separate_proj_weight=False,q_proj_weight=None,k_proj_weight=None,v_proj_weight=None,static_k=None,static_v=None)
torch.nn.functional.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multilabel_soft_margin_loss(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.functional.normalize(input,p=2,dim=1,eps=1e-12,out=None)
torch.nn.functional.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)
torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.functional.prelu(input,weight)
torch.nn.functional.relu(input,inplace=False)
torch.nn.functional.relu6(input,inplace=False)
torch.nn.functional.rrelu(input,lower=1.0/8,upper=1.0/3,training=False,inplace=False)
torch.nn.functional.selu(input,inplace=False)
torch.nn.functional.sigmoid(input)
torch.nn.functional.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.softmin(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.softsign(input)
torch.nn.functional.tanh(input)
torch.nn.functional.tanhshrink(input)
torch.nn.functional.threshold(input,threshold,value,inplace=False)
torch.nn.functional.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.unfold(input,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.functional.upsample_nearest(input,size=None,scale_factor=None)
torch.nn.gelu(input)
torch.nn.glu(input,dim=-1)
torch.nn.grid_sample(input,grid,mode='bilinear',padding_mode='zeros',align_corners=None)
torch.nn.group_norm(input,num_groups,weight=None,bias=None,eps=1e-05)
torch.nn.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)
torch.nn.hardshrink(input,lambd=0.5)
torch.nn.hardsigmoid(input,inplace=False)
torch.nn.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)
torch.nn.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None,recompute_scale_factor=None)
torch.nn.kl_div(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)
torch.nn.leaky_relu(input,negative_slope=0.01,inplace=False)
torch.nn.linear(input,weight,bias=None)
torch.nn.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.log_softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.max_pool1d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_pool2d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_pool3d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.multi_head_attention_forward(query,key,value,embed_dim_to_check,num_heads,in_proj_weight,in_proj_bias,bias_k,bias_v,add_zero_attn,dropout_p,out_proj_weight,out_proj_bias,training=True,key_padding_mask=None,need_weights=True,attn_mask=None,use_separate_proj_weight=False,q_proj_weight=None,k_proj_weight=None,v_proj_weight=None,static_k=None,static_v=None)
torch.nn.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.multilabel_soft_margin_loss(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.normalize(input,p=2,dim=1,eps=1e-12,out=None)
torch.nn.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)
torch.nn.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.prelu(input,weight)
torch.nn.relu(input,inplace=False)
torch.nn.relu6(input,inplace=False)
torch.nn.rrelu(input,lower=1.0/8,upper=1.0/3,training=False,inplace=False)
torch.nn.selu(input,inplace=False)
torch.nn.sigmoid(input)
torch.nn.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.softmin(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.softsign(input)
torch.nn.tanh(input)
torch.nn.tanhshrink(input)
torch.nn.threshold(input,threshold,value,inplace=False)
torch.nn.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.unfold(input,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/functional.pyi----------------------------------------
torch.nn.functional.pad(input:Tensor,pad:List[int],mode:str=...,value:float=...)->Tensor
torch.nn.pad(input:Tensor,pad:List[int],mode:str=...,value:float=...)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/_reduction.py----------------------------------------
torch.nn._reduction.get_enum(reduction)
torch.nn._reduction.legacy_get_enum(size_average,reduce,emit_warning=True)
torch.nn._reduction.legacy_get_string(size_average,reduce,emit_warning=True)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/common_types.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/cpp.py----------------------------------------
A:torch.nn.cpp.self._parameters->OrderedDictWrapper(cpp_module, '_parameters')
A:torch.nn.cpp.self._buffers->OrderedDictWrapper(cpp_module, '_buffers')
A:torch.nn.cpp.self._modules->OrderedDictWrapper(cpp_module, '_modules')
A:torch.nn.cpp.param.data->fn(param.data)
A:torch.nn.cpp.param._grad.data->fn(param._grad.data)
A:torch.nn.cpp.buf.data->fn(buf.data)
torch.nn.cpp.ModuleWrapper(self,cpp_module)
torch.nn.cpp.ModuleWrapper.__init__(self,cpp_module)
torch.nn.cpp.ModuleWrapper.__repr__(self)
torch.nn.cpp.ModuleWrapper._apply(self,fn)
torch.nn.cpp.ModuleWrapper.training(self)
torch.nn.cpp.ModuleWrapper.training(self,mode)
torch.nn.cpp.OrderedDictWrapper(self,cpp_module,attr)
torch.nn.cpp.OrderedDictWrapper.__contains__(self,key)
torch.nn.cpp.OrderedDictWrapper.__getitem__(self,key)
torch.nn.cpp.OrderedDictWrapper.__init__(self,cpp_module,attr)
torch.nn.cpp.OrderedDictWrapper.__iter__(self)
torch.nn.cpp.OrderedDictWrapper.__len__(self)
torch.nn.cpp.OrderedDictWrapper.cpp_dict(self)
torch.nn.cpp.OrderedDictWrapper.items(self)
torch.nn.cpp.OrderedDictWrapper.keys(self)
torch.nn.cpp.OrderedDictWrapper.values(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/weight_norm.py----------------------------------------
A:torch.nn.utils.weight_norm.g->getattr(module, self.name + '_g')
A:torch.nn.utils.weight_norm.v->getattr(module, self.name + '_v')
A:torch.nn.utils.weight_norm.fn->WeightNorm(name, dim)
A:torch.nn.utils.weight_norm.weight->self.compute_weight(module)
torch.nn.utils.remove_weight_norm(module,name='weight')
torch.nn.utils.weight_norm(module,name='weight',dim=0)
torch.nn.utils.weight_norm.WeightNorm(self,name,dim)
torch.nn.utils.weight_norm.WeightNorm.__init__(self,name,dim)
torch.nn.utils.weight_norm.WeightNorm.apply(module,name,dim)
torch.nn.utils.weight_norm.WeightNorm.compute_weight(self,module)
torch.nn.utils.weight_norm.WeightNorm.remove(self,module)
torch.nn.utils.weight_norm.remove_weight_norm(module,name='weight')
torch.nn.utils.weight_norm.weight_norm(module,name='weight',dim=0)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/memory_format.py----------------------------------------
A:torch.nn.utils.memory_format.weight_data->module.weight.detach().clone().contiguous(memory_format=memory_format)
A:torch.nn.utils.memory_format.module.weight.data->module.weight.detach().clone().contiguous(memory_format=memory_format).resize_(weight_data.size(), memory_format=memory_format)
torch.nn.utils.convert_conv2d_weight_memory_format(module,memory_format)
torch.nn.utils.memory_format.convert_conv2d_weight_memory_format(module,memory_format)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/fusion.py----------------------------------------
A:torch.nn.utils.fusion.fused_conv->copy.deepcopy(conv)
A:torch.nn.utils.fusion.(fused_conv.weight, fused_conv.bias)->fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias, bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)
A:torch.nn.utils.fusion.conv_b->bn_rm.new_zeros(bn_rm.shape)
A:torch.nn.utils.fusion.bn_var_rsqrt->torch.rsqrt(bn_rv + bn_eps)
torch.nn.utils.fuse_conv_bn_eval(conv,bn)
torch.nn.utils.fuse_conv_bn_weights(conv_w,conv_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b)
torch.nn.utils.fusion.fuse_conv_bn_eval(conv,bn)
torch.nn.utils.fusion.fuse_conv_bn_weights(conv_w,conv_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/prune.py----------------------------------------
A:torch.nn.utils.prune.ABC->ABCMeta('ABC', (), {})
A:torch.nn.utils.prune.mask->make_mask(t, self.dim, topk.indices)
A:torch.nn.utils.prune.orig->getattr(module, name + '_orig')
A:torch.nn.utils.prune.method->pruning_method(**kwargs)
A:torch.nn.utils.prune.container->PruningContainer()
A:torch.nn.utils.prune.default_mask->torch.nn.utils.parameters_to_vector([getattr(module, name + '_mask', torch.ones_like(getattr(module, name))) for (module, name) in parameters])
A:torch.nn.utils.prune.weight->self.apply_mask(module)
A:torch.nn.utils.prune.self._pruning_methods->tuple()
A:torch.nn.utils.prune.new_mask->new_mask.to(dtype=t.dtype).to(dtype=t.dtype)
A:torch.nn.utils.prune.n_dims->len(t.shape)
A:torch.nn.utils.prune.partial_mask->pruning_method(**kwargs).compute_mask(t[slc], default_mask=mask[slc])
A:torch.nn.utils.prune.new_mask[slc]->pruning_method(**kwargs).compute_mask(t[slc], default_mask=mask[slc]).to(dtype=new_mask.dtype)
A:torch.nn.utils.prune.tensor_size->torch.nn.utils.parameters_to_vector([getattr(*p) for p in parameters]).nelement()
A:torch.nn.utils.prune.nparams_toprune->_compute_nparams_toprune(self.amount, tensor_size)
A:torch.nn.utils.prune.prob->torch.rand(nchannels)
A:torch.nn.utils.prune.topk->torch.topk(norm, k=nparams_tokeep, largest=True)
A:torch.nn.utils.prune.norm->torch.norm(t, p=n, dim=dims)
A:torch.nn.utils.prune.t->torch.nn.utils.parameters_to_vector([getattr(*p) for p in parameters])
A:torch.nn.utils.prune.final_mask->PruningContainer().compute_mask(t, default_mask)
A:torch.nn.utils.prune.param->getattr(module, name)
A:torch.nn.utils.prune.num_param->getattr(module, name).numel()
A:torch.nn.utils.prune.param_mask->final_mask[pointer:pointer + num_param].view_as(param)
A:torch.nn.utils.prune.dims->list(range(t.dim()))
torch.nn.utils.prune.BasePruningMethod(self)
torch.nn.utils.prune.BasePruningMethod.__init__(self)
torch.nn.utils.prune.BasePruningMethod.apply(cls,module,name,*args,**kwargs)
torch.nn.utils.prune.BasePruningMethod.apply_mask(self,module)
torch.nn.utils.prune.BasePruningMethod.compute_mask(self,t,default_mask)
torch.nn.utils.prune.BasePruningMethod.prune(self,t,default_mask=None)
torch.nn.utils.prune.BasePruningMethod.remove(self,module)
torch.nn.utils.prune.CustomFromMask(self,mask)
torch.nn.utils.prune.CustomFromMask.__init__(self,mask)
torch.nn.utils.prune.CustomFromMask.apply(cls,module,name,mask)
torch.nn.utils.prune.CustomFromMask.compute_mask(self,t,default_mask)
torch.nn.utils.prune.Identity(BasePruningMethod)
torch.nn.utils.prune.Identity.apply(cls,module,name)
torch.nn.utils.prune.Identity.compute_mask(self,t,default_mask)
torch.nn.utils.prune.L1Unstructured(self,amount)
torch.nn.utils.prune.L1Unstructured.__init__(self,amount)
torch.nn.utils.prune.L1Unstructured.apply(cls,module,name,amount)
torch.nn.utils.prune.L1Unstructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune.LnStructured(self,amount,n,dim=-1)
torch.nn.utils.prune.LnStructured.__init__(self,amount,n,dim=-1)
torch.nn.utils.prune.LnStructured.apply(cls,module,name,amount,n,dim)
torch.nn.utils.prune.LnStructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune.PruningContainer(self,*args)
torch.nn.utils.prune.PruningContainer.__getitem__(self,idx)
torch.nn.utils.prune.PruningContainer.__init__(self,*args)
torch.nn.utils.prune.PruningContainer.__iter__(self)
torch.nn.utils.prune.PruningContainer.__len__(self)
torch.nn.utils.prune.PruningContainer.add_pruning_method(self,method)
torch.nn.utils.prune.PruningContainer.compute_mask(self,t,default_mask)
torch.nn.utils.prune.RandomStructured(self,amount,dim=-1)
torch.nn.utils.prune.RandomStructured.__init__(self,amount,dim=-1)
torch.nn.utils.prune.RandomStructured.apply(cls,module,name,amount,dim=-1)
torch.nn.utils.prune.RandomStructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune.RandomUnstructured(self,amount)
torch.nn.utils.prune.RandomUnstructured.__init__(self,amount)
torch.nn.utils.prune.RandomUnstructured.apply(cls,module,name,amount)
torch.nn.utils.prune.RandomUnstructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune._compute_norm(t,n,dim)
torch.nn.utils.prune._compute_nparams_toprune(amount,tensor_size)
torch.nn.utils.prune._validate_pruning_amount(amount,tensor_size)
torch.nn.utils.prune._validate_pruning_amount_init(amount)
torch.nn.utils.prune._validate_pruning_dim(t,dim)
torch.nn.utils.prune._validate_structured_pruning(t)
torch.nn.utils.prune.custom_from_mask(module,name,mask)
torch.nn.utils.prune.global_unstructured(parameters,pruning_method,**kwargs)
torch.nn.utils.prune.identity(module,name)
torch.nn.utils.prune.is_pruned(module)
torch.nn.utils.prune.l1_unstructured(module,name,amount)
torch.nn.utils.prune.ln_structured(module,name,amount,n,dim)
torch.nn.utils.prune.random_structured(module,name,amount,dim)
torch.nn.utils.prune.random_unstructured(module,name,amount)
torch.nn.utils.prune.remove(module,name)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/clip_grad.py----------------------------------------
A:torch.nn.utils.clip_grad.parameters->list(filter(lambda p: p.grad is not None, parameters))
A:torch.nn.utils.clip_grad.max_norm->float(max_norm)
A:torch.nn.utils.clip_grad.norm_type->float(norm_type)
A:torch.nn.utils.clip_grad.total_norm->torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
A:torch.nn.utils.clip_grad.clip_value->float(clip_value)
torch.nn.utils.clip_grad.clip_grad_norm(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad.clip_grad_norm_(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad.clip_grad_value_(parameters,clip_value)
torch.nn.utils.clip_grad_norm(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad_value_(parameters,clip_value)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/convert_parameters.py----------------------------------------
A:torch.nn.utils.convert_parameters.param_device->_check_param_device(param, param_device)
A:torch.nn.utils.convert_parameters.num_param->param.numel()
torch.nn.utils.convert_parameters._check_param_device(param,old_param_device)
torch.nn.utils.convert_parameters.parameters_to_vector(parameters)
torch.nn.utils.convert_parameters.vector_to_parameters(vec,parameters)
torch.nn.utils.parameters_to_vector(parameters)
torch.nn.utils.vector_to_parameters(vec,parameters)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/spectral_norm.py----------------------------------------
A:torch.nn.utils.spectral_norm.weight_mat->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig)
A:torch.nn.utils.spectral_norm.height->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size(0)
A:torch.nn.utils.spectral_norm.weight->state_dict.pop(weight_key)
A:torch.nn.utils.spectral_norm.u->normalize(weight.new_empty(h).normal_(0, 1), dim=0, eps=fn.eps)
A:torch.nn.utils.spectral_norm.v->SpectralNorm(name, n_power_iterations, dim, eps)._solve_v_and_rescale(weight_mat, u, sigma)
A:torch.nn.utils.spectral_norm.sigma->(weight_orig / weight).mean()
A:torch.nn.utils.spectral_norm.fn->SpectralNorm(name, n_power_iterations, dim, eps)
A:torch.nn.utils.spectral_norm.(h, w)->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size()
A:torch.nn.utils.spectral_norm.version->local_metadata.get('spectral_norm', {}).get(fn.name + '.version', None)
torch.nn.utils.remove_spectral_norm(module,name='weight')
torch.nn.utils.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)
torch.nn.utils.spectral_norm.SpectralNorm(self,name='weight',n_power_iterations=1,dim=0,eps=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm.__init__(self,name='weight',n_power_iterations=1,dim=0,eps=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm._solve_v_and_rescale(self,weight_mat,u,target_sigma)
torch.nn.utils.spectral_norm.SpectralNorm.apply(module,name,n_power_iterations,dim,eps)
torch.nn.utils.spectral_norm.SpectralNorm.compute_weight(self,module,do_power_iteration)
torch.nn.utils.spectral_norm.SpectralNorm.remove(self,module)
torch.nn.utils.spectral_norm.SpectralNorm.reshape_weight_to_matrix(self,weight)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook.__init__(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook.__init__(self,fn)
torch.nn.utils.spectral_norm.remove_spectral_norm(module,name='weight')
torch.nn.utils.spectral_norm.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/utils/rnn.py----------------------------------------
A:torch.nn.utils.rnn.PackedSequence_->namedtuple('PackedSequence', ['data', 'batch_sizes', 'sorted_indices', 'unsorted_indices'])
A:torch.nn.utils.rnn.ex->torch.tensor((), dtype=self.data.dtype, device=self.data.device).to(*args, **kwargs)
A:torch.nn.utils.rnn.data->self.data.to(*args, **kwargs)
A:torch.nn.utils.rnn.sorted_indices->sorted_indices.to(input.device).to(input.device)
A:torch.nn.utils.rnn.unsorted_indices->invert_permutation(sorted_indices)
A:torch.nn.utils.rnn.(data, batch_sizes, sorted_indices, unsorted_indices)->_packed_sequence_init_args(data, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.utils.rnn.output->torch.empty_like(permutation, memory_format=torch.legacy_contiguous_format)
A:torch.nn.utils.rnn.lengths->torch.as_tensor(lengths, dtype=torch.int64)
A:torch.nn.utils.rnn.(lengths, sorted_indices)->torch.sort(lengths, descending=True)
A:torch.nn.utils.rnn.input->input.index_select(batch_dim, sorted_indices).index_select(batch_dim, sorted_indices)
A:torch.nn.utils.rnn.(data, batch_sizes)->_VF._pack_padded_sequence(input, lengths, batch_first)
A:torch.nn.utils.rnn.max_seq_length->sequence.batch_sizes.size(0)
A:torch.nn.utils.rnn.(padded_output, lengths)->_VF._pad_packed_sequence(sequence.data, sequence.batch_sizes, batch_first, padding_value, max_seq_length)
A:torch.nn.utils.rnn.max_size->sequences[0].size()
A:torch.nn.utils.rnn.max_len->max([s.size(0) for s in sequences])
A:torch.nn.utils.rnn.out_tensor->sequences[0].data.new(*out_dims).fill_(padding_value)
A:torch.nn.utils.rnn.length->tensor.size(0)
torch.nn.utils.rnn.PackedSequence(cls,data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.PackedSequence.__new__(cls,data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.PackedSequence.byte(self)
torch.nn.utils.rnn.PackedSequence.char(self)
torch.nn.utils.rnn.PackedSequence.cpu(self,*args,**kwargs)
torch.nn.utils.rnn.PackedSequence.cuda(self,*args,**kwargs)
torch.nn.utils.rnn.PackedSequence.double(self)
torch.nn.utils.rnn.PackedSequence.float(self)
torch.nn.utils.rnn.PackedSequence.half(self)
torch.nn.utils.rnn.PackedSequence.int(self)
torch.nn.utils.rnn.PackedSequence.is_cuda(self)
torch.nn.utils.rnn.PackedSequence.is_pinned(self)
torch.nn.utils.rnn.PackedSequence.long(self)
torch.nn.utils.rnn.PackedSequence.pin_memory(self)
torch.nn.utils.rnn.PackedSequence.short(self)
torch.nn.utils.rnn.PackedSequence.to(self,*args,**kwargs)
torch.nn.utils.rnn._packed_sequence_init(data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn._packed_sequence_init_args(data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.bind(optional,fn)
torch.nn.utils.rnn.invert_permutation(permutation)
torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)
torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)
torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)
torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/backends/thnn.py----------------------------------------
torch.nn.backends.thnn._get_thnn_function_backend()


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/backends/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/replicate.py----------------------------------------
A:torch.nn.parallel.replicate.gen->module.modules()
A:torch.nn.parallel.replicate.memo->set()
A:torch.nn.parallel.replicate.tensor_copies->_functions.Broadcast.apply(devices, *tensors)
A:torch.nn.parallel.replicate.devices->list(map(lambda x: _get_device_index(x, True), devices))
A:torch.nn.parallel.replicate.num_replicas->len(devices)
A:torch.nn.parallel.replicate.params->list(network.parameters())
A:torch.nn.parallel.replicate.param_copies->_broadcast_coalesced_reshape(params, devices, detach)
A:torch.nn.parallel.replicate.buffers->list(network.buffers())
A:torch.nn.parallel.replicate.buffer_copies_rg->_broadcast_coalesced_reshape(buffers_rg, devices, detach=detach)
A:torch.nn.parallel.replicate.buffer_copies_not_rg->_broadcast_coalesced_reshape(buffers_not_rg, devices, detach=True)
A:torch.nn.parallel.replicate.modules->list(network.modules())
A:torch.nn.parallel.replicate.replica->module._replicate_for_data_parallel()
A:torch.nn.parallel.replicate.replica._former_parameters->OrderedDict()
torch.nn.parallel.replicate(network,devices,detach=False)
torch.nn.parallel.replicate._broadcast_coalesced_reshape(tensors,devices,detach=False)
torch.nn.parallel.replicate._init_script_module()
torch.nn.parallel.replicate._is_jit_enabled()
torch.nn.parallel.replicate._is_script_method(module)
torch.nn.parallel.replicate._is_script_module(module)
torch.nn.parallel.replicate._replicatable_module(module,memo=None)
torch.nn.parallel.replicate.replicate(network,devices,detach=False)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/replicate.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/parallel_apply.py----------------------------------------
A:torch.nn.parallel.parallel_apply.devices->list(map(lambda x: _get_device_index(x, True), devices))
A:torch.nn.parallel.parallel_apply.lock->threading.Lock()
A:torch.nn.parallel.parallel_apply.grad_enabled->torch.is_grad_enabled()
A:torch.nn.parallel.parallel_apply.device->get_a_var(input).get_device()
A:torch.nn.parallel.parallel_apply.output->module(*input, **kwargs)
A:torch.nn.parallel.parallel_apply.results[i]->ExceptionWrapper(where='in replica {} on device {}'.format(i, device))
torch.nn.parallel.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)
torch.nn.parallel.parallel_apply.get_a_var(obj)
torch.nn.parallel.parallel_apply.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/parallel_apply.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/data_parallel.py----------------------------------------
A:torch.nn.parallel.data_parallel.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.data_parallel.(min_pos, min_val)->min(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.(max_pos, max_val)->max(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.self.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.data_parallel.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.self.src_device_obj->torch.device('cuda:{}'.format(self.device_ids[0]))
A:torch.nn.parallel.data_parallel.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.data_parallel.replicas->replicate(module, used_device_ids)
A:torch.nn.parallel.data_parallel.outputs->parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
A:torch.nn.parallel.data_parallel.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.src_device_obj->torch.device('cuda:{}'.format(device_ids[0]))
A:torch.nn.parallel.data_parallel.(inputs, module_kwargs)->scatter_kwargs(inputs, module_kwargs, device_ids, dim)
torch.nn.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.DataParallel.gather(self,outputs,output_device)
torch.nn.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.DataParallel.replicate(self,module,device_ids)
torch.nn.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)
torch.nn.parallel.data_parallel.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.data_parallel.DataParallel.gather(self,outputs,output_device)
torch.nn.parallel.data_parallel.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.data_parallel.DataParallel.replicate(self,module,device_ids)
torch.nn.parallel.data_parallel.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel._check_balance(device_ids)
torch.nn.parallel.data_parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/data_parallel.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/__init__.py----------------------------------------
torch.nn.parallel.__init__.DistributedDataParallelCPU(*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/distributed.py----------------------------------------
A:torch.nn.parallel.distributed.self.is_cuda->all([p.device.type == 'cuda' for p in module.parameters()])
A:torch.nn.parallel.distributed.device_ids->list(range(torch.cuda.device_count()))
A:torch.nn.parallel.distributed.self.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.distributed.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.distributed.self.process_group->_get_default_group()
A:torch.nn.parallel.distributed.self.broadcast_bucket_size->int(250 * MB)
A:torch.nn.parallel.distributed.self.bucket_bytes_cap->int(bucket_cap_mb * MB)
A:torch.nn.parallel.distributed.module_states->list(self.module.state_dict().values())
A:torch.nn.parallel.distributed.self._module_copies->replicate(self.module, self.device_ids, detach=True)
A:torch.nn.parallel.distributed.bucket_indices->torch.distributed._compute_bucket_assignment_by_size(parameters[0], [1024 * 1024, self.bucket_bytes_cap], expect_sparse_gradient[0])
A:torch.nn.parallel.distributed.self.reducer->torch.distributed.Reducer(parameters, list(reversed(bucket_indices)), self.process_group, expect_sparse_gradient)
A:torch.nn.parallel.distributed.attrs->copy.copy(self.__dict__)
A:torch.nn.parallel.distributed.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.distributed.output->self.module(*inputs, **kwargs)
A:torch.nn.parallel.distributed.outputs->self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
A:torch.nn.parallel.distributed.result->torch.cuda.comm.broadcast_coalesced(self.modules_buffers[0], self.device_ids, self.broadcast_bucket_size)
torch.nn.parallel.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)
torch.nn.parallel.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.DistributedDataParallel._ddp_init_helper(self)
torch.nn.parallel.DistributedDataParallel._distributed_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.DistributedDataParallel._passing_sync_batchnorm_handle(self,module_copies)
torch.nn.parallel.DistributedDataParallel._sync_params(self)
torch.nn.parallel.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.DistributedDataParallel.no_sync(self)
torch.nn.parallel.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.distributed.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)
torch.nn.parallel.distributed.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.distributed.DistributedDataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)
torch.nn.parallel.distributed.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.distributed.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.distributed.DistributedDataParallel._ddp_init_helper(self)
torch.nn.parallel.distributed.DistributedDataParallel._distributed_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.distributed.DistributedDataParallel._passing_sync_batchnorm_handle(self,module_copies)
torch.nn.parallel.distributed.DistributedDataParallel._sync_params(self)
torch.nn.parallel.distributed.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.distributed.DistributedDataParallel.no_sync(self)
torch.nn.parallel.distributed.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.distributed.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.distributed._find_tensors(obj)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/distributed.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/scatter_gather.py----------------------------------------
A:torch.nn.parallel.scatter_gather.res->gather_map(outputs)
A:torch.nn.parallel.scatter_gather.inputs->tuple(inputs)
A:torch.nn.parallel.scatter_gather.kwargs->tuple(kwargs)
torch.nn.parallel.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter_gather.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)
torch.nn.parallel.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/scatter_gather.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/common_types.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/parallel/_functions.py----------------------------------------
A:torch.nn.parallel._functions.target_gpus->list(map(lambda x: _get_device_index(x, True), target_gpus))
A:torch.nn.parallel._functions.ctx.num_inputs->len(inputs)
A:torch.nn.parallel._functions.ctx.input_device->inputs[0].get_device()
A:torch.nn.parallel._functions.outputs->torch.cuda.comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
A:torch.nn.parallel._functions.target_device->_get_device_index(target_device, True)
A:torch.nn.parallel._functions.ctx.input_gpus->tuple(map(lambda i: i.get_device(), inputs))
A:torch.nn.parallel._functions.inputs->tuple((t.view(1) for t in inputs))
A:torch.nn.parallel._functions.ctx.input_sizes->tuple(map(lambda i: i.size(ctx.dim), inputs))
A:torch.nn.parallel._functions.scattered_grads->tuple((g[0] for g in scattered_grads))
A:torch.nn.parallel._functions.main_stream->torch.cuda.current_stream()
A:torch.nn.parallel._functions._streams[device]->torch.cuda.Stream(device)
torch.nn.parallel._functions.Broadcast(Function)
torch.nn.parallel._functions.Broadcast.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.Broadcast.forward(ctx,target_gpus,*inputs)
torch.nn.parallel._functions.Gather(Function)
torch.nn.parallel._functions.Gather.backward(ctx,grad_output)
torch.nn.parallel._functions.Gather.forward(ctx,target_device,dim,*inputs)
torch.nn.parallel._functions.ReduceAddCoalesced(Function)
torch.nn.parallel._functions.ReduceAddCoalesced.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.ReduceAddCoalesced.forward(ctx,destination,num_inputs,*grads)
torch.nn.parallel._functions.Scatter(Function)
torch.nn.parallel._functions.Scatter.backward(ctx,*grad_output)
torch.nn.parallel._functions.Scatter.forward(ctx,target_gpus,chunk_sizes,dim,input)
torch.nn.parallel._functions._get_stream(device)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/linear.py----------------------------------------
A:torch.nn.modules.linear.self.weight->Parameter(torch.Tensor(out_features, in1_features, in2_features))
A:torch.nn.modules.linear.self.bias->Parameter(torch.Tensor(out_features))
A:torch.nn.modules.linear.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
torch.nn.Bilinear(self,in1_features,in2_features,out_features,bias=True)
torch.nn.Bilinear.extra_repr(self)
torch.nn.Bilinear.forward(self,input1,input2)
torch.nn.Bilinear.reset_parameters(self)
torch.nn.Identity(self,*args,**kwargs)
torch.nn.Identity.forward(self,input)
torch.nn.Linear(self,in_features,out_features,bias=True)
torch.nn.Linear.extra_repr(self)
torch.nn.Linear.forward(self,input)
torch.nn.Linear.reset_parameters(self)
torch.nn.modules.linear.Bilinear(self,in1_features,in2_features,out_features,bias=True)
torch.nn.modules.linear.Bilinear.__init__(self,in1_features,in2_features,out_features,bias=True)
torch.nn.modules.linear.Bilinear.extra_repr(self)
torch.nn.modules.linear.Bilinear.forward(self,input1,input2)
torch.nn.modules.linear.Bilinear.reset_parameters(self)
torch.nn.modules.linear.Identity(self,*args,**kwargs)
torch.nn.modules.linear.Identity.__init__(self,*args,**kwargs)
torch.nn.modules.linear.Identity.forward(self,input)
torch.nn.modules.linear.Linear(self,in_features,out_features,bias=True)
torch.nn.modules.linear.Linear.__init__(self,in_features,out_features,bias=True)
torch.nn.modules.linear.Linear.extra_repr(self)
torch.nn.modules.linear.Linear.forward(self,input)
torch.nn.modules.linear.Linear.reset_parameters(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/linear.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/container.py----------------------------------------
A:torch.nn.modules.container.size->len(self)
A:torch.nn.modules.container.idx->self._get_abs_string_index(idx)
A:torch.nn.modules.container.key->self._get_item_by_idx(self._modules.keys(), idx)
A:torch.nn.modules.container.keys->super(ParameterList, self).__dir__()
A:torch.nn.modules.container.input->module(input)
A:torch.nn.modules.container.self._modules->OrderedDict(list(zip(str_indices, self._modules.values())))
A:torch.nn.modules.container.offset->len(self)
A:torch.nn.modules.container.size_str->'x'.join((str(size) for size in p.size()))
A:torch.nn.modules.container.parastr->'Parameter containing: [{} of size {}{}]'.format(torch.typename(p), size_str, device_str)
A:torch.nn.modules.container.tmpstr->'\n'.join(child_lines)
torch.nn.Container(self,**kwargs)
torch.nn.ModuleDict(self,modules=None)
torch.nn.ModuleDict.__contains__(self,key)
torch.nn.ModuleDict.__delitem__(self,key)
torch.nn.ModuleDict.__getitem__(self,key)
torch.nn.ModuleDict.__iter__(self)
torch.nn.ModuleDict.__len__(self)
torch.nn.ModuleDict.__setitem__(self,key,module)
torch.nn.ModuleDict.clear(self)
torch.nn.ModuleDict.forward(self)
torch.nn.ModuleDict.items(self)
torch.nn.ModuleDict.keys(self)
torch.nn.ModuleDict.pop(self,key)
torch.nn.ModuleDict.update(self,modules)
torch.nn.ModuleDict.values(self)
torch.nn.ModuleList(self,modules=None)
torch.nn.ModuleList.__delitem__(self,idx)
torch.nn.ModuleList.__dir__(self)
torch.nn.ModuleList.__getitem__(self,idx)
torch.nn.ModuleList.__iadd__(self,modules)
torch.nn.ModuleList.__iter__(self)
torch.nn.ModuleList.__len__(self)
torch.nn.ModuleList.__setitem__(self,idx,module)
torch.nn.ModuleList._get_abs_string_index(self,idx)
torch.nn.ModuleList.append(self,module)
torch.nn.ModuleList.extend(self,modules)
torch.nn.ModuleList.forward(self)
torch.nn.ModuleList.insert(self,index,module)
torch.nn.ParameterDict(self,parameters=None)
torch.nn.ParameterDict.__contains__(self,key)
torch.nn.ParameterDict.__delitem__(self,key)
torch.nn.ParameterDict.__getitem__(self,key)
torch.nn.ParameterDict.__iter__(self)
torch.nn.ParameterDict.__len__(self)
torch.nn.ParameterDict.__setitem__(self,key,parameter)
torch.nn.ParameterDict.clear(self)
torch.nn.ParameterDict.extra_repr(self)
torch.nn.ParameterDict.items(self)
torch.nn.ParameterDict.keys(self)
torch.nn.ParameterDict.pop(self,key)
torch.nn.ParameterDict.update(self,parameters)
torch.nn.ParameterDict.values(self)
torch.nn.ParameterList(self,parameters=None)
torch.nn.ParameterList.__dir__(self)
torch.nn.ParameterList.__getitem__(self,idx)
torch.nn.ParameterList.__iadd__(self,parameters)
torch.nn.ParameterList.__iter__(self)
torch.nn.ParameterList.__len__(self)
torch.nn.ParameterList.__setitem__(self,idx,param)
torch.nn.ParameterList._get_abs_string_index(self,idx)
torch.nn.ParameterList.append(self,parameter)
torch.nn.ParameterList.extend(self,parameters)
torch.nn.ParameterList.extra_repr(self)
torch.nn.Sequential(self,*args)
torch.nn.Sequential.__delitem__(self,idx)
torch.nn.Sequential.__dir__(self)
torch.nn.Sequential.__getitem__(self,idx)
torch.nn.Sequential.__iter__(self)
torch.nn.Sequential.__len__(self)
torch.nn.Sequential.__setitem__(self,idx,module)
torch.nn.Sequential._get_item_by_idx(self,iterator,idx)
torch.nn.Sequential.forward(self,input)
torch.nn.modules.container.Container(self,**kwargs)
torch.nn.modules.container.Container.__init__(self,**kwargs)
torch.nn.modules.container.ModuleDict(self,modules=None)
torch.nn.modules.container.ModuleDict.__contains__(self,key)
torch.nn.modules.container.ModuleDict.__delitem__(self,key)
torch.nn.modules.container.ModuleDict.__getitem__(self,key)
torch.nn.modules.container.ModuleDict.__init__(self,modules=None)
torch.nn.modules.container.ModuleDict.__iter__(self)
torch.nn.modules.container.ModuleDict.__len__(self)
torch.nn.modules.container.ModuleDict.__setitem__(self,key,module)
torch.nn.modules.container.ModuleDict.clear(self)
torch.nn.modules.container.ModuleDict.forward(self)
torch.nn.modules.container.ModuleDict.items(self)
torch.nn.modules.container.ModuleDict.keys(self)
torch.nn.modules.container.ModuleDict.pop(self,key)
torch.nn.modules.container.ModuleDict.update(self,modules)
torch.nn.modules.container.ModuleDict.values(self)
torch.nn.modules.container.ModuleList(self,modules=None)
torch.nn.modules.container.ModuleList.__delitem__(self,idx)
torch.nn.modules.container.ModuleList.__dir__(self)
torch.nn.modules.container.ModuleList.__getitem__(self,idx)
torch.nn.modules.container.ModuleList.__iadd__(self,modules)
torch.nn.modules.container.ModuleList.__init__(self,modules=None)
torch.nn.modules.container.ModuleList.__iter__(self)
torch.nn.modules.container.ModuleList.__len__(self)
torch.nn.modules.container.ModuleList.__setitem__(self,idx,module)
torch.nn.modules.container.ModuleList._get_abs_string_index(self,idx)
torch.nn.modules.container.ModuleList.append(self,module)
torch.nn.modules.container.ModuleList.extend(self,modules)
torch.nn.modules.container.ModuleList.forward(self)
torch.nn.modules.container.ModuleList.insert(self,index,module)
torch.nn.modules.container.ParameterDict(self,parameters=None)
torch.nn.modules.container.ParameterDict.__contains__(self,key)
torch.nn.modules.container.ParameterDict.__delitem__(self,key)
torch.nn.modules.container.ParameterDict.__getitem__(self,key)
torch.nn.modules.container.ParameterDict.__init__(self,parameters=None)
torch.nn.modules.container.ParameterDict.__iter__(self)
torch.nn.modules.container.ParameterDict.__len__(self)
torch.nn.modules.container.ParameterDict.__setitem__(self,key,parameter)
torch.nn.modules.container.ParameterDict.clear(self)
torch.nn.modules.container.ParameterDict.extra_repr(self)
torch.nn.modules.container.ParameterDict.items(self)
torch.nn.modules.container.ParameterDict.keys(self)
torch.nn.modules.container.ParameterDict.pop(self,key)
torch.nn.modules.container.ParameterDict.update(self,parameters)
torch.nn.modules.container.ParameterDict.values(self)
torch.nn.modules.container.ParameterList(self,parameters=None)
torch.nn.modules.container.ParameterList.__dir__(self)
torch.nn.modules.container.ParameterList.__getitem__(self,idx)
torch.nn.modules.container.ParameterList.__iadd__(self,parameters)
torch.nn.modules.container.ParameterList.__init__(self,parameters=None)
torch.nn.modules.container.ParameterList.__iter__(self)
torch.nn.modules.container.ParameterList.__len__(self)
torch.nn.modules.container.ParameterList.__setitem__(self,idx,param)
torch.nn.modules.container.ParameterList._get_abs_string_index(self,idx)
torch.nn.modules.container.ParameterList.append(self,parameter)
torch.nn.modules.container.ParameterList.extend(self,parameters)
torch.nn.modules.container.ParameterList.extra_repr(self)
torch.nn.modules.container.Sequential(self,*args)
torch.nn.modules.container.Sequential.__delitem__(self,idx)
torch.nn.modules.container.Sequential.__dir__(self)
torch.nn.modules.container.Sequential.__getitem__(self,idx)
torch.nn.modules.container.Sequential.__init__(self,*args)
torch.nn.modules.container.Sequential.__iter__(self)
torch.nn.modules.container.Sequential.__len__(self)
torch.nn.modules.container.Sequential.__setitem__(self,idx,module)
torch.nn.modules.container.Sequential._get_item_by_idx(self,iterator,idx)
torch.nn.modules.container.Sequential.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/container.pyi----------------------------------------
torch.nn.ParameterList.__delitem__(self,idx:Union[int,slice])->None
torch.nn.modules.container.ParameterList.__delitem__(self,idx:Union[int,slice])->None


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/instancenorm.py----------------------------------------
A:torch.nn.modules.instancenorm.version->local_metadata.get('version', None)
torch.nn.InstanceNorm1d(_InstanceNorm)
torch.nn.InstanceNorm1d._check_input_dim(self,input)
torch.nn.InstanceNorm2d(_InstanceNorm)
torch.nn.InstanceNorm2d._check_input_dim(self,input)
torch.nn.InstanceNorm3d(_InstanceNorm)
torch.nn.InstanceNorm3d._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.instancenorm._InstanceNorm.forward(self,input)
torch.nn.modules.instancenorm.InstanceNorm1d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm1d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm2d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm2d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm3d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm3d._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.modules.instancenorm._InstanceNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.modules.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.instancenorm._InstanceNorm.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/instancenorm.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/activation.py----------------------------------------
A:torch.nn.modules.activation.self.q_proj_weight->Parameter(torch.Tensor(embed_dim, embed_dim))
A:torch.nn.modules.activation.self.k_proj_weight->Parameter(torch.Tensor(embed_dim, self.kdim))
A:torch.nn.modules.activation.self.v_proj_weight->Parameter(torch.Tensor(embed_dim, self.vdim))
A:torch.nn.modules.activation.self.in_proj_weight->Parameter(torch.empty(3 * embed_dim, embed_dim))
A:torch.nn.modules.activation.self.in_proj_bias->Parameter(torch.empty(3 * embed_dim))
A:torch.nn.modules.activation.self.out_proj->Linear(embed_dim, embed_dim, bias=bias)
A:torch.nn.modules.activation.self.bias_k->Parameter(torch.empty(1, 1, embed_dim))
A:torch.nn.modules.activation.self.bias_v->Parameter(torch.empty(1, 1, embed_dim))
A:torch.nn.modules.activation.self.weight->Parameter(torch.Tensor(num_parameters).fill_(init))
torch.nn.CELU(self,alpha=1.0,inplace=False)
torch.nn.CELU.extra_repr(self)
torch.nn.CELU.forward(self,input)
torch.nn.ELU(self,alpha=1.0,inplace=False)
torch.nn.ELU.extra_repr(self)
torch.nn.ELU.forward(self,input)
torch.nn.GELU(Module)
torch.nn.GELU.forward(self,input)
torch.nn.GLU(self,dim=-1)
torch.nn.GLU.extra_repr(self)
torch.nn.GLU.forward(self,input)
torch.nn.Hardshrink(self,lambd=0.5)
torch.nn.Hardshrink.extra_repr(self)
torch.nn.Hardshrink.forward(self,input)
torch.nn.Hardsigmoid(Module)
torch.nn.Hardsigmoid.forward(self,input)
torch.nn.Hardtanh(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.Hardtanh.extra_repr(self)
torch.nn.Hardtanh.forward(self,input)
torch.nn.LeakyReLU(self,negative_slope=0.01,inplace=False)
torch.nn.LeakyReLU.extra_repr(self)
torch.nn.LeakyReLU.forward(self,input)
torch.nn.LogSigmoid(Module)
torch.nn.LogSigmoid.forward(self,input)
torch.nn.LogSoftmax(self,dim=None)
torch.nn.LogSoftmax.__setstate__(self,state)
torch.nn.LogSoftmax.forward(self,input)
torch.nn.MultiheadAttention(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)
torch.nn.MultiheadAttention.__setstate__(self,state)
torch.nn.MultiheadAttention._reset_parameters(self)
torch.nn.MultiheadAttention.forward(self,query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)
torch.nn.PReLU(self,num_parameters=1,init=0.25)
torch.nn.PReLU.extra_repr(self)
torch.nn.PReLU.forward(self,input)
torch.nn.RReLU(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.RReLU.extra_repr(self)
torch.nn.RReLU.forward(self,input)
torch.nn.ReLU(self,inplace=False)
torch.nn.ReLU.extra_repr(self)
torch.nn.ReLU.forward(self,input)
torch.nn.ReLU6(self,inplace=False)
torch.nn.ReLU6.extra_repr(self)
torch.nn.SELU(self,inplace=False)
torch.nn.SELU.extra_repr(self)
torch.nn.SELU.forward(self,input)
torch.nn.Sigmoid(Module)
torch.nn.Sigmoid.forward(self,input)
torch.nn.Softmax(self,dim=None)
torch.nn.Softmax.__setstate__(self,state)
torch.nn.Softmax.extra_repr(self)
torch.nn.Softmax.forward(self,input)
torch.nn.Softmax2d(Module)
torch.nn.Softmax2d.forward(self,input)
torch.nn.Softmin(self,dim=None)
torch.nn.Softmin.forward(self,input)
torch.nn.Softplus(self,beta=1,threshold=20)
torch.nn.Softplus.extra_repr(self)
torch.nn.Softplus.forward(self,input)
torch.nn.Softshrink(self,lambd=0.5)
torch.nn.Softshrink.extra_repr(self)
torch.nn.Softshrink.forward(self,input)
torch.nn.Softsign(Module)
torch.nn.Softsign.forward(self,input)
torch.nn.Tanh(Module)
torch.nn.Tanh.forward(self,input)
torch.nn.Tanhshrink(Module)
torch.nn.Tanhshrink.forward(self,input)
torch.nn.Threshold(self,threshold,value,inplace=False)
torch.nn.Threshold.extra_repr(self)
torch.nn.Threshold.forward(self,input)
torch.nn.modules.activation.CELU(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.CELU.__init__(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.CELU.extra_repr(self)
torch.nn.modules.activation.CELU.forward(self,input)
torch.nn.modules.activation.ELU(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.ELU.__init__(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.ELU.extra_repr(self)
torch.nn.modules.activation.ELU.forward(self,input)
torch.nn.modules.activation.GELU(Module)
torch.nn.modules.activation.GELU.forward(self,input)
torch.nn.modules.activation.GLU(self,dim=-1)
torch.nn.modules.activation.GLU.__init__(self,dim=-1)
torch.nn.modules.activation.GLU.extra_repr(self)
torch.nn.modules.activation.GLU.forward(self,input)
torch.nn.modules.activation.Hardshrink(self,lambd=0.5)
torch.nn.modules.activation.Hardshrink.__init__(self,lambd=0.5)
torch.nn.modules.activation.Hardshrink.extra_repr(self)
torch.nn.modules.activation.Hardshrink.forward(self,input)
torch.nn.modules.activation.Hardsigmoid(Module)
torch.nn.modules.activation.Hardsigmoid.forward(self,input)
torch.nn.modules.activation.Hardtanh(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.modules.activation.Hardtanh.__init__(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.modules.activation.Hardtanh.extra_repr(self)
torch.nn.modules.activation.Hardtanh.forward(self,input)
torch.nn.modules.activation.LeakyReLU(self,negative_slope=0.01,inplace=False)
torch.nn.modules.activation.LeakyReLU.__init__(self,negative_slope=0.01,inplace=False)
torch.nn.modules.activation.LeakyReLU.extra_repr(self)
torch.nn.modules.activation.LeakyReLU.forward(self,input)
torch.nn.modules.activation.LogSigmoid(Module)
torch.nn.modules.activation.LogSigmoid.forward(self,input)
torch.nn.modules.activation.LogSoftmax(self,dim=None)
torch.nn.modules.activation.LogSoftmax.__init__(self,dim=None)
torch.nn.modules.activation.LogSoftmax.__setstate__(self,state)
torch.nn.modules.activation.LogSoftmax.forward(self,input)
torch.nn.modules.activation.MultiheadAttention(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)
torch.nn.modules.activation.MultiheadAttention.__init__(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)
torch.nn.modules.activation.MultiheadAttention.__setstate__(self,state)
torch.nn.modules.activation.MultiheadAttention._reset_parameters(self)
torch.nn.modules.activation.MultiheadAttention.forward(self,query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)
torch.nn.modules.activation.PReLU(self,num_parameters=1,init=0.25)
torch.nn.modules.activation.PReLU.__init__(self,num_parameters=1,init=0.25)
torch.nn.modules.activation.PReLU.extra_repr(self)
torch.nn.modules.activation.PReLU.forward(self,input)
torch.nn.modules.activation.RReLU(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.modules.activation.RReLU.__init__(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.modules.activation.RReLU.extra_repr(self)
torch.nn.modules.activation.RReLU.forward(self,input)
torch.nn.modules.activation.ReLU(self,inplace=False)
torch.nn.modules.activation.ReLU.__init__(self,inplace=False)
torch.nn.modules.activation.ReLU.extra_repr(self)
torch.nn.modules.activation.ReLU.forward(self,input)
torch.nn.modules.activation.ReLU6(self,inplace=False)
torch.nn.modules.activation.ReLU6.__init__(self,inplace=False)
torch.nn.modules.activation.ReLU6.extra_repr(self)
torch.nn.modules.activation.SELU(self,inplace=False)
torch.nn.modules.activation.SELU.__init__(self,inplace=False)
torch.nn.modules.activation.SELU.extra_repr(self)
torch.nn.modules.activation.SELU.forward(self,input)
torch.nn.modules.activation.Sigmoid(Module)
torch.nn.modules.activation.Sigmoid.forward(self,input)
torch.nn.modules.activation.Softmax(self,dim=None)
torch.nn.modules.activation.Softmax.__init__(self,dim=None)
torch.nn.modules.activation.Softmax.__setstate__(self,state)
torch.nn.modules.activation.Softmax.extra_repr(self)
torch.nn.modules.activation.Softmax.forward(self,input)
torch.nn.modules.activation.Softmax2d(Module)
torch.nn.modules.activation.Softmax2d.forward(self,input)
torch.nn.modules.activation.Softmin(self,dim=None)
torch.nn.modules.activation.Softmin.__init__(self,dim=None)
torch.nn.modules.activation.Softmin.forward(self,input)
torch.nn.modules.activation.Softplus(self,beta=1,threshold=20)
torch.nn.modules.activation.Softplus.__init__(self,beta=1,threshold=20)
torch.nn.modules.activation.Softplus.extra_repr(self)
torch.nn.modules.activation.Softplus.forward(self,input)
torch.nn.modules.activation.Softshrink(self,lambd=0.5)
torch.nn.modules.activation.Softshrink.__init__(self,lambd=0.5)
torch.nn.modules.activation.Softshrink.extra_repr(self)
torch.nn.modules.activation.Softshrink.forward(self,input)
torch.nn.modules.activation.Softsign(Module)
torch.nn.modules.activation.Softsign.forward(self,input)
torch.nn.modules.activation.Tanh(Module)
torch.nn.modules.activation.Tanh.forward(self,input)
torch.nn.modules.activation.Tanhshrink(Module)
torch.nn.modules.activation.Tanhshrink.forward(self,input)
torch.nn.modules.activation.Threshold(self,threshold,value,inplace=False)
torch.nn.modules.activation.Threshold.__init__(self,threshold,value,inplace=False)
torch.nn.modules.activation.Threshold.extra_repr(self)
torch.nn.modules.activation.Threshold.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/activation.pyi----------------------------------------
torch.nn.modules.activation.GELU.__call__(self,input:Tensor)
torch.nn.modules.activation.LogSigmoid.__call__(self,input:Tensor)
torch.nn.modules.activation.Sigmoid.__call__(self,input:Tensor)
torch.nn.modules.activation.Softmax2d.__call__(self,input:Tensor)
torch.nn.modules.activation.Softsign.__call__(self,input:Tensor)
torch.nn.modules.activation.Tanh.__call__(self,input:Tensor)
torch.nn.modules.activation.Tanhshrink.__call__(self,input:Tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/distance.py----------------------------------------
torch.nn.CosineSimilarity(self,dim=1,eps=1e-08)
torch.nn.CosineSimilarity.forward(self,x1,x2)
torch.nn.PairwiseDistance(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.PairwiseDistance.forward(self,x1,x2)
torch.nn.modules.distance.CosineSimilarity(self,dim=1,eps=1e-08)
torch.nn.modules.distance.CosineSimilarity.__init__(self,dim=1,eps=1e-08)
torch.nn.modules.distance.CosineSimilarity.forward(self,x1,x2)
torch.nn.modules.distance.PairwiseDistance(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.modules.distance.PairwiseDistance.__init__(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.modules.distance.PairwiseDistance.forward(self,x1,x2)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/distance.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/transformer.py----------------------------------------
A:torch.nn.modules.transformer.encoder_layer->TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
A:torch.nn.modules.transformer.encoder_norm->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.encoder->TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
A:torch.nn.modules.transformer.decoder_layer->TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
A:torch.nn.modules.transformer.decoder_norm->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.decoder->TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)
A:torch.nn.modules.transformer.memory->self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)
A:torch.nn.modules.transformer.output->self.norm(output)
A:torch.nn.modules.transformer.mask->mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
A:torch.nn.modules.transformer.self.layers->_get_clones(decoder_layer, num_layers)
A:torch.nn.modules.transformer.self.self_attn->MultiheadAttention(d_model, nhead, dropout=dropout)
A:torch.nn.modules.transformer.self.linear1->Linear(d_model, dim_feedforward)
A:torch.nn.modules.transformer.self.dropout->Dropout(dropout)
A:torch.nn.modules.transformer.self.linear2->Linear(dim_feedforward, d_model)
A:torch.nn.modules.transformer.self.norm1->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.norm2->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.dropout1->Dropout(dropout)
A:torch.nn.modules.transformer.self.dropout2->Dropout(dropout)
A:torch.nn.modules.transformer.self.activation->_get_activation_fn(activation)
A:torch.nn.modules.transformer.src->self.norm2(src)
A:torch.nn.modules.transformer.src2->self.linear2(self.dropout(self.activation(self.linear1(src))))
A:torch.nn.modules.transformer.self.multihead_attn->MultiheadAttention(d_model, nhead, dropout=dropout)
A:torch.nn.modules.transformer.self.norm3->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.dropout3->Dropout(dropout)
A:torch.nn.modules.transformer.tgt->self.norm3(tgt)
A:torch.nn.modules.transformer.tgt2->self.linear2(self.dropout(self.activation(self.linear1(tgt))))
torch.nn.Transformer(self,d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)
torch.nn.Transformer._reset_parameters(self)
torch.nn.Transformer.forward(self,src,tgt,src_mask=None,tgt_mask=None,memory_mask=None,src_key_padding_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.Transformer.generate_square_subsequent_mask(self,sz)
torch.nn.TransformerDecoder(self,decoder_layer,num_layers,norm=None)
torch.nn.TransformerDecoder.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.TransformerDecoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.TransformerDecoderLayer.__setstate__(self,state)
torch.nn.TransformerDecoderLayer.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.TransformerEncoder(self,encoder_layer,num_layers,norm=None)
torch.nn.TransformerEncoder.forward(self,src,mask=None,src_key_padding_mask=None)
torch.nn.TransformerEncoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.TransformerEncoderLayer.__setstate__(self,state)
torch.nn.TransformerEncoderLayer.forward(self,src,src_mask=None,src_key_padding_mask=None)
torch.nn.modules.transformer.Transformer(self,d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)
torch.nn.modules.transformer.Transformer.__init__(self,d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)
torch.nn.modules.transformer.Transformer._reset_parameters(self)
torch.nn.modules.transformer.Transformer.forward(self,src,tgt,src_mask=None,tgt_mask=None,memory_mask=None,src_key_padding_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.modules.transformer.Transformer.generate_square_subsequent_mask(self,sz)
torch.nn.modules.transformer.TransformerDecoder(self,decoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerDecoder.__init__(self,decoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerDecoder.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.modules.transformer.TransformerDecoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerDecoderLayer.__init__(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerDecoderLayer.__setstate__(self,state)
torch.nn.modules.transformer.TransformerDecoderLayer.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.modules.transformer.TransformerEncoder(self,encoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerEncoder.__init__(self,encoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerEncoder.forward(self,src,mask=None,src_key_padding_mask=None)
torch.nn.modules.transformer.TransformerEncoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerEncoderLayer.__init__(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerEncoderLayer.__setstate__(self,state)
torch.nn.modules.transformer.TransformerEncoderLayer.forward(self,src,src_mask=None,src_key_padding_mask=None)
torch.nn.modules.transformer._get_activation_fn(activation)
torch.nn.modules.transformer._get_clones(module,N)
torch.nn.transformer._get_activation_fn(activation)
torch.nn.transformer._get_clones(module,N)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/transformer.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/flatten.py----------------------------------------
torch.nn.Flatten(self,start_dim=1,end_dim=-1)
torch.nn.Flatten.forward(self,input)
torch.nn.modules.flatten.Flatten(self,start_dim=1,end_dim=-1)
torch.nn.modules.flatten.Flatten.__init__(self,start_dim=1,end_dim=-1)
torch.nn.modules.flatten.Flatten.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/flatten.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/dropout.py----------------------------------------
torch.nn.AlphaDropout(_DropoutNd)
torch.nn.AlphaDropout.forward(self,input)
torch.nn.Dropout(_DropoutNd)
torch.nn.Dropout.forward(self,input)
torch.nn.Dropout2d(_DropoutNd)
torch.nn.Dropout2d.forward(self,input)
torch.nn.Dropout3d(_DropoutNd)
torch.nn.Dropout3d.forward(self,input)
torch.nn.FeatureAlphaDropout(_DropoutNd)
torch.nn.FeatureAlphaDropout.forward(self,input)
torch.nn.dropout._DropoutNd(self,p=0.5,inplace=False)
torch.nn.dropout._DropoutNd.extra_repr(self)
torch.nn.modules.dropout.AlphaDropout(_DropoutNd)
torch.nn.modules.dropout.AlphaDropout.forward(self,input)
torch.nn.modules.dropout.Dropout(_DropoutNd)
torch.nn.modules.dropout.Dropout.forward(self,input)
torch.nn.modules.dropout.Dropout2d(_DropoutNd)
torch.nn.modules.dropout.Dropout2d.forward(self,input)
torch.nn.modules.dropout.Dropout3d(_DropoutNd)
torch.nn.modules.dropout.Dropout3d.forward(self,input)
torch.nn.modules.dropout.FeatureAlphaDropout(_DropoutNd)
torch.nn.modules.dropout.FeatureAlphaDropout.forward(self,input)
torch.nn.modules.dropout._DropoutNd(self,p=0.5,inplace=False)
torch.nn.modules.dropout._DropoutNd.__init__(self,p=0.5,inplace=False)
torch.nn.modules.dropout._DropoutNd.extra_repr(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/dropout.pyi----------------------------------------
torch.nn.modules.dropout.AlphaDropout.__call__(self,input:Tensor)
torch.nn.modules.dropout.Dropout.__call__(self,input:Tensor)
torch.nn.modules.dropout.Dropout2d.__call__(self,input:Tensor)
torch.nn.modules.dropout.Dropout3d.__call__(self,input:Tensor)
torch.nn.modules.dropout.FeatureAlphaDropout.__call__(self,input:Tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/upsampling.py----------------------------------------
A:torch.nn.modules.upsampling.self.scale_factor->tuple((float(factor) for factor in scale_factor))
torch.nn.Upsample(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.Upsample.extra_repr(self)
torch.nn.Upsample.forward(self,input)
torch.nn.UpsamplingBilinear2d(self,size=None,scale_factor=None)
torch.nn.UpsamplingNearest2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.Upsample(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.modules.upsampling.Upsample.__init__(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.modules.upsampling.Upsample.extra_repr(self)
torch.nn.modules.upsampling.Upsample.forward(self,input)
torch.nn.modules.upsampling.UpsamplingBilinear2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingBilinear2d.__init__(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingNearest2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingNearest2d.__init__(self,size=None,scale_factor=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/upsampling.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/batchnorm.py----------------------------------------
A:torch.nn.modules.batchnorm.self.weight->Parameter(torch.Tensor(num_features))
A:torch.nn.modules.batchnorm.self.bias->Parameter(torch.Tensor(num_features))
A:torch.nn.modules.batchnorm.version->local_metadata.get('version', None)
A:torch.nn.modules.batchnorm.state_dict[num_batches_tracked_key]->torch.tensor(0, dtype=torch.long)
A:torch.nn.modules.batchnorm.world_size->torch.distributed.get_world_size(process_group)
A:torch.nn.modules.batchnorm.module_output->torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group)
torch.nn.BatchNorm1d(_BatchNorm)
torch.nn.BatchNorm1d._check_input_dim(self,input)
torch.nn.BatchNorm2d(_BatchNorm)
torch.nn.BatchNorm2d._check_input_dim(self,input)
torch.nn.BatchNorm3d(_BatchNorm)
torch.nn.BatchNorm3d._check_input_dim(self,input)
torch.nn.SyncBatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)
torch.nn.SyncBatchNorm._check_input_dim(self,input)
torch.nn.SyncBatchNorm._specify_ddp_gpu_num(self,gpu_size)
torch.nn.SyncBatchNorm.convert_sync_batchnorm(cls,module,process_group=None)
torch.nn.SyncBatchNorm.forward(self,input)
torch.nn.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.batchnorm._BatchNorm.forward(self,input)
torch.nn.batchnorm._NormBase(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.batchnorm._NormBase._check_input_dim(self,input)
torch.nn.batchnorm._NormBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.batchnorm._NormBase.extra_repr(self)
torch.nn.batchnorm._NormBase.reset_parameters(self)
torch.nn.batchnorm._NormBase.reset_running_stats(self)
torch.nn.modules.batchnorm.BatchNorm1d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm1d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm2d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm2d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm3d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm3d._check_input_dim(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm._check_input_dim(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm._specify_ddp_gpu_num(self,gpu_size)
torch.nn.modules.batchnorm.SyncBatchNorm.convert_sync_batchnorm(cls,module,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm.forward(self,input)
torch.nn.modules.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._BatchNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._BatchNorm.forward(self,input)
torch.nn.modules.batchnorm._NormBase(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._NormBase.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._NormBase._check_input_dim(self,input)
torch.nn.modules.batchnorm._NormBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.batchnorm._NormBase.extra_repr(self)
torch.nn.modules.batchnorm._NormBase.reset_parameters(self)
torch.nn.modules.batchnorm._NormBase.reset_running_stats(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/batchnorm.pyi----------------------------------------
torch.nn.batchnorm._BatchNorm.reset_parameters(self)->None
torch.nn.batchnorm._BatchNorm.reset_running_stats(self)->None
torch.nn.modules.batchnorm._BatchNorm.reset_parameters(self)->None
torch.nn.modules.batchnorm._BatchNorm.reset_running_stats(self)->None


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/padding.py----------------------------------------
A:torch.nn.modules.padding.self.padding->_ntuple(6)(padding)
torch.nn.ConstantPad1d(self,padding,value)
torch.nn.ConstantPad2d(self,padding,value)
torch.nn.ConstantPad3d(self,padding,value)
torch.nn.ReflectionPad1d(self,padding)
torch.nn.ReflectionPad2d(self,padding)
torch.nn.ReplicationPad1d(self,padding)
torch.nn.ReplicationPad2d(self,padding)
torch.nn.ReplicationPad3d(self,padding)
torch.nn.ZeroPad2d(self,padding)
torch.nn.modules.padding.ConstantPad1d(self,padding,value)
torch.nn.modules.padding.ConstantPad1d.__init__(self,padding,value)
torch.nn.modules.padding.ConstantPad2d(self,padding,value)
torch.nn.modules.padding.ConstantPad2d.__init__(self,padding,value)
torch.nn.modules.padding.ConstantPad3d(self,padding,value)
torch.nn.modules.padding.ConstantPad3d.__init__(self,padding,value)
torch.nn.modules.padding.ReflectionPad1d(self,padding)
torch.nn.modules.padding.ReflectionPad1d.__init__(self,padding)
torch.nn.modules.padding.ReflectionPad2d(self,padding)
torch.nn.modules.padding.ReflectionPad2d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad1d(self,padding)
torch.nn.modules.padding.ReplicationPad1d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad2d(self,padding)
torch.nn.modules.padding.ReplicationPad2d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad3d(self,padding)
torch.nn.modules.padding.ReplicationPad3d.__init__(self,padding)
torch.nn.modules.padding.ZeroPad2d(self,padding)
torch.nn.modules.padding.ZeroPad2d.__init__(self,padding)
torch.nn.modules.padding._ConstantPadNd(self,value)
torch.nn.modules.padding._ConstantPadNd.__init__(self,value)
torch.nn.modules.padding._ConstantPadNd.extra_repr(self)
torch.nn.modules.padding._ConstantPadNd.forward(self,input)
torch.nn.modules.padding._ReflectionPadNd(Module)
torch.nn.modules.padding._ReflectionPadNd.extra_repr(self)
torch.nn.modules.padding._ReflectionPadNd.forward(self,input)
torch.nn.modules.padding._ReplicationPadNd(Module)
torch.nn.modules.padding._ReplicationPadNd.extra_repr(self)
torch.nn.modules.padding._ReplicationPadNd.forward(self,input)
torch.nn.padding._ConstantPadNd(self,value)
torch.nn.padding._ConstantPadNd.extra_repr(self)
torch.nn.padding._ConstantPadNd.forward(self,input)
torch.nn.padding._ReflectionPadNd(Module)
torch.nn.padding._ReflectionPadNd.extra_repr(self)
torch.nn.padding._ReflectionPadNd.forward(self,input)
torch.nn.padding._ReplicationPadNd(Module)
torch.nn.padding._ReplicationPadNd.extra_repr(self)
torch.nn.padding._ReplicationPadNd.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/padding.pyi----------------------------------------
torch.nn.modules.padding._ReflectionPadNd.__call__(self,input:Tensor)
torch.nn.modules.padding._ReplicationPadNd.__call__(self,input:Tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/module.py----------------------------------------
A:torch.nn.modules.module.s->'\n'.join(s)
A:torch.nn.modules.module.first->'\n'.join(s).pop(0)
A:torch.nn.modules.module.self._parameters->OrderedDict()
A:torch.nn.modules.module.self._buffers->OrderedDict()
A:torch.nn.modules.module.self._backward_hooks->OrderedDict()
A:torch.nn.modules.module.self._forward_hooks->OrderedDict()
A:torch.nn.modules.module.self._forward_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._state_dict_hooks->OrderedDict()
A:torch.nn.modules.module.self._load_state_dict_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._modules->OrderedDict()
A:torch.nn.modules.module.param_applied->fn(param)
A:torch.nn.modules.module.should_use_set_data->compute_should_use_set_data(param.grad, grad_applied)
A:torch.nn.modules.module.self._parameters[key]->Parameter(param_applied, param.requires_grad)
A:torch.nn.modules.module.grad_applied->fn(param.grad)
A:torch.nn.modules.module.self._parameters[key].grad->fn(param.grad).requires_grad_(param.grad.requires_grad)
A:torch.nn.modules.module.self._buffers[key]->fn(buf)
A:torch.nn.modules.module.(device, dtype, non_blocking, convert_to_format)->torch._C._nn._parse_to(*args, **kwargs)
A:torch.nn.modules.module.handle->torch.utils.hooks.RemovableHandle(self._load_state_dict_pre_hooks)
A:torch.nn.modules.module.tracing_state->torch._C._get_tracing_state()
A:torch.nn.modules.module.cur_scope_name->torch._C._get_tracing_state().current_scope()
A:torch.nn.modules.module.result->self.forward(*input, **kwargs)
A:torch.nn.modules.module.hook_result->hook(self, destination, prefix, local_metadata)
A:torch.nn.modules.module.var->next((v for v in var.values() if isinstance(v, torch.Tensor)))
A:torch.nn.modules.module.wrapper->functools.partial(hook, self)
A:torch.nn.modules.module.params->self.__dict__.get('_parameters')
A:torch.nn.modules.module.modules->list(self._modules.keys())
A:torch.nn.modules.module.buffers->list(self._buffers.keys())
A:torch.nn.modules.module.destination->OrderedDict()
A:torch.nn.modules.module.destination._metadata->OrderedDict()
A:torch.nn.modules.module.destination._metadata[prefix[:-1]]local_metadata->dict(version=self._version)
A:torch.nn.modules.module.local_name_params->itertools.chain(self._parameters.items(), self._buffers.items())
A:torch.nn.modules.module.metadata->getattr(state_dict, '_metadata', None)
A:torch.nn.modules.module.state_dict->state_dict.copy().copy()
A:torch.nn.modules.module.memo->set()
A:torch.nn.modules.module.members->get_members_fn(module)
A:torch.nn.modules.module.gen->self._named_members(lambda module: module._buffers.items(), prefix=prefix, recurse=recurse)
A:torch.nn.modules.module.extra_repr->self.extra_repr()
A:torch.nn.modules.module.extra_lines->self.extra_repr().split('\n')
A:torch.nn.modules.module.mod_str->_addindent(mod_str, 2)
A:torch.nn.modules.module.module_attrs->dir(self.__class__)
A:torch.nn.modules.module.attrs->list(self.__dict__.keys())
A:torch.nn.modules.module.parameters->list(self._parameters.keys())
A:torch.nn.modules.module.replica->weak_self()
A:torch.nn.modules.module.replica.__dict__->self.__dict__.copy()
A:torch.nn.modules.module.replica._parameters->weak_self()._parameters.copy()
A:torch.nn.modules.module.replica._buffers->weak_self()._buffers.copy()
A:torch.nn.modules.module.replica._modules->weak_self()._modules.copy()
A:torch.nn.modules.module.weak_self->weakref.ref(replica)
torch.nn.Module(self)
torch.nn.Module.__delattr__(self,name)
torch.nn.Module.__dir__(self)
torch.nn.Module.__getattr__(self,name)
torch.nn.Module.__repr__(self)
torch.nn.Module.__setattr__(self,name,value)
torch.nn.Module.__setstate__(self,state)
torch.nn.Module._apply(self,fn)
torch.nn.Module._get_name(self)
torch.nn.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.Module._register_load_state_dict_pre_hook(self,hook)
torch.nn.Module._register_state_dict_hook(self,hook)
torch.nn.Module._replicate_for_data_parallel(self)
torch.nn.Module._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.Module._slow_forward(self,*input,**kwargs)
torch.nn.Module.add_module(self,name,module)
torch.nn.Module.apply(self,fn)
torch.nn.Module.bfloat16(self)
torch.nn.Module.buffers(self,recurse=True)
torch.nn.Module.children(self)
torch.nn.Module.cpu(self)
torch.nn.Module.cuda(self,device=None)
torch.nn.Module.double(self)
torch.nn.Module.eval(self)
torch.nn.Module.extra_repr(self)
torch.nn.Module.float(self)
torch.nn.Module.forward(self,*input)
torch.nn.Module.half(self)
torch.nn.Module.load_state_dict(self,state_dict,strict=True)
torch.nn.Module.modules(self)
torch.nn.Module.named_buffers(self,prefix='',recurse=True)
torch.nn.Module.named_children(self)
torch.nn.Module.named_modules(self,memo=None,prefix='')
torch.nn.Module.named_parameters(self,prefix='',recurse=True)
torch.nn.Module.parameters(self,recurse=True)
torch.nn.Module.register_backward_hook(self,hook)
torch.nn.Module.register_buffer(self,name,tensor)
torch.nn.Module.register_forward_hook(self,hook)
torch.nn.Module.register_forward_pre_hook(self,hook)
torch.nn.Module.register_parameter(self,name,param)
torch.nn.Module.requires_grad_(self,requires_grad=True)
torch.nn.Module.share_memory(self)
torch.nn.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.Module.to(self,*args,**kwargs)
torch.nn.Module.train(self,mode=True)
torch.nn.Module.type(self,dst_type)
torch.nn.Module.zero_grad(self)
torch.nn.module._IncompatibleKeys(namedtuple('IncompatibleKeys',['missing_keys','unexpected_keys']))
torch.nn.module._IncompatibleKeys.__repr__(self)
torch.nn.module._addindent(s_,numSpaces)
torch.nn.modules.module.Module(self)
torch.nn.modules.module.Module.__delattr__(self,name)
torch.nn.modules.module.Module.__dir__(self)
torch.nn.modules.module.Module.__getattr__(self,name)
torch.nn.modules.module.Module.__init__(self)
torch.nn.modules.module.Module.__repr__(self)
torch.nn.modules.module.Module.__setattr__(self,name,value)
torch.nn.modules.module.Module.__setstate__(self,state)
torch.nn.modules.module.Module._apply(self,fn)
torch.nn.modules.module.Module._get_name(self)
torch.nn.modules.module.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.module.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self,hook)
torch.nn.modules.module.Module._register_state_dict_hook(self,hook)
torch.nn.modules.module.Module._replicate_for_data_parallel(self)
torch.nn.modules.module.Module._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.modules.module.Module._slow_forward(self,*input,**kwargs)
torch.nn.modules.module.Module.add_module(self,name,module)
torch.nn.modules.module.Module.apply(self,fn)
torch.nn.modules.module.Module.bfloat16(self)
torch.nn.modules.module.Module.buffers(self,recurse=True)
torch.nn.modules.module.Module.children(self)
torch.nn.modules.module.Module.cpu(self)
torch.nn.modules.module.Module.cuda(self,device=None)
torch.nn.modules.module.Module.double(self)
torch.nn.modules.module.Module.eval(self)
torch.nn.modules.module.Module.extra_repr(self)
torch.nn.modules.module.Module.float(self)
torch.nn.modules.module.Module.forward(self,*input)
torch.nn.modules.module.Module.half(self)
torch.nn.modules.module.Module.load_state_dict(self,state_dict,strict=True)
torch.nn.modules.module.Module.modules(self)
torch.nn.modules.module.Module.named_buffers(self,prefix='',recurse=True)
torch.nn.modules.module.Module.named_children(self)
torch.nn.modules.module.Module.named_modules(self,memo=None,prefix='')
torch.nn.modules.module.Module.named_parameters(self,prefix='',recurse=True)
torch.nn.modules.module.Module.parameters(self,recurse=True)
torch.nn.modules.module.Module.register_backward_hook(self,hook)
torch.nn.modules.module.Module.register_buffer(self,name,tensor)
torch.nn.modules.module.Module.register_forward_hook(self,hook)
torch.nn.modules.module.Module.register_forward_pre_hook(self,hook)
torch.nn.modules.module.Module.register_parameter(self,name,param)
torch.nn.modules.module.Module.requires_grad_(self,requires_grad=True)
torch.nn.modules.module.Module.share_memory(self)
torch.nn.modules.module.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.modules.module.Module.to(self,*args,**kwargs)
torch.nn.modules.module.Module.train(self,mode=True)
torch.nn.modules.module.Module.type(self,dst_type)
torch.nn.modules.module.Module.zero_grad(self)
torch.nn.modules.module._IncompatibleKeys(namedtuple('IncompatibleKeys',['missing_keys','unexpected_keys']))
torch.nn.modules.module._IncompatibleKeys.__repr__(self)
torch.nn.modules.module._addindent(s_,numSpaces)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/module.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/sparse.py----------------------------------------
A:torch.nn.modules.sparse.self.weight->Parameter(_weight)
A:torch.nn.modules.sparse.embedding->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, padding_idx=padding_idx, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, sparse=sparse)
A:torch.nn.modules.sparse.embeddingbag->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, mode=mode, sparse=sparse, include_last_offset=include_last_offset)
torch.nn.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.Embedding.extra_repr(self)
torch.nn.Embedding.forward(self,input)
torch.nn.Embedding.from_pretrained(cls,embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.Embedding.reset_parameters(self)
torch.nn.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None,include_last_offset=False)
torch.nn.EmbeddingBag.extra_repr(self)
torch.nn.EmbeddingBag.forward(self,input,offsets=None,per_sample_weights=None)
torch.nn.EmbeddingBag.from_pretrained(cls,embeddings,freeze=True,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,include_last_offset=False)
torch.nn.EmbeddingBag.reset_parameters(self)
torch.nn.modules.sparse.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.modules.sparse.Embedding.__init__(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.modules.sparse.Embedding.extra_repr(self)
torch.nn.modules.sparse.Embedding.forward(self,input)
torch.nn.modules.sparse.Embedding.from_pretrained(cls,embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.modules.sparse.Embedding.reset_parameters(self)
torch.nn.modules.sparse.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None,include_last_offset=False)
torch.nn.modules.sparse.EmbeddingBag.__init__(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None,include_last_offset=False)
torch.nn.modules.sparse.EmbeddingBag.extra_repr(self)
torch.nn.modules.sparse.EmbeddingBag.forward(self,input,offsets=None,per_sample_weights=None)
torch.nn.modules.sparse.EmbeddingBag.from_pretrained(cls,embeddings,freeze=True,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,include_last_offset=False)
torch.nn.modules.sparse.EmbeddingBag.reset_parameters(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/sparse.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/utils.py----------------------------------------
A:torch.nn.modules.utils._single->_ntuple(1)
A:torch.nn.modules.utils._pair->_ntuple(2)
A:torch.nn.modules.utils._triple->_ntuple(3)
A:torch.nn.modules.utils._quadruple->_ntuple(4)
torch.nn.modules.utils._list_with_default(out_size,defaults)
torch.nn.modules.utils._ntuple(n)
torch.nn.modules.utils._reverse_repeat_tuple(t,n)
torch.nn.utils._list_with_default(out_size,defaults)
torch.nn.utils._ntuple(n)
torch.nn.utils._reverse_repeat_tuple(t,n)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/loss.py----------------------------------------
A:torch.nn.modules.loss.self.reduction->_Reduction.legacy_get_string(size_average, reduce)
torch.nn.BCELoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.BCELoss.forward(self,input,target)
torch.nn.BCEWithLogitsLoss(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.BCEWithLogitsLoss.forward(self,input,target)
torch.nn.CTCLoss(self,blank=0,reduction='mean',zero_infinity=False)
torch.nn.CTCLoss.forward(self,log_probs,targets,input_lengths,target_lengths)
torch.nn.CosineEmbeddingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.CosineEmbeddingLoss.forward(self,input1,input2,target)
torch.nn.CrossEntropyLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.CrossEntropyLoss.forward(self,input,target)
torch.nn.HingeEmbeddingLoss(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.HingeEmbeddingLoss.forward(self,input,target)
torch.nn.KLDivLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.KLDivLoss.forward(self,input,target)
torch.nn.L1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.L1Loss.forward(self,input,target)
torch.nn.MSELoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.MSELoss.forward(self,input,target)
torch.nn.MarginRankingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.MarginRankingLoss.forward(self,input1,input2,target)
torch.nn.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiLabelMarginLoss.forward(self,input,target)
torch.nn.MultiLabelSoftMarginLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiLabelSoftMarginLoss.forward(self,input,target)
torch.nn.MultiMarginLoss(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiMarginLoss.forward(self,input,target)
torch.nn.NLLLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.NLLLoss.forward(self,input,target)
torch.nn.NLLLoss2d(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.PoissonNLLLoss(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.PoissonNLLLoss.forward(self,log_input,target)
torch.nn.SmoothL1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.SmoothL1Loss.forward(self,input,target)
torch.nn.SoftMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.SoftMarginLoss.forward(self,input,target)
torch.nn.TripletMarginLoss(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.TripletMarginLoss.forward(self,anchor,positive,negative)
torch.nn.loss._Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.loss._WeightedLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss.forward(self,input,target)
torch.nn.modules.loss.BCEWithLogitsLoss(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.modules.loss.BCEWithLogitsLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.modules.loss.BCEWithLogitsLoss.forward(self,input,target)
torch.nn.modules.loss.CTCLoss(self,blank=0,reduction='mean',zero_infinity=False)
torch.nn.modules.loss.CTCLoss.__init__(self,blank=0,reduction='mean',zero_infinity=False)
torch.nn.modules.loss.CTCLoss.forward(self,log_probs,targets,input_lengths,target_lengths)
torch.nn.modules.loss.CosineEmbeddingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.__init__(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.forward(self,input1,input2,target)
torch.nn.modules.loss.CrossEntropyLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.CrossEntropyLoss.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.CrossEntropyLoss.forward(self,input,target)
torch.nn.modules.loss.HingeEmbeddingLoss(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.__init__(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.forward(self,input,target)
torch.nn.modules.loss.KLDivLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.KLDivLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.KLDivLoss.forward(self,input,target)
torch.nn.modules.loss.L1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.L1Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.L1Loss.forward(self,input,target)
torch.nn.modules.loss.MSELoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MSELoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MSELoss.forward(self,input,target)
torch.nn.modules.loss.MarginRankingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MarginRankingLoss.__init__(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MarginRankingLoss.forward(self,input1,input2,target)
torch.nn.modules.loss.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.forward(self,input,target)
torch.nn.modules.loss.MultiLabelSoftMarginLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.forward(self,input,target)
torch.nn.modules.loss.MultiMarginLoss(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiMarginLoss.__init__(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiMarginLoss.forward(self,input,target)
torch.nn.modules.loss.NLLLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss.forward(self,input,target)
torch.nn.modules.loss.NLLLoss2d(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss2d.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss.__init__(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss.forward(self,log_input,target)
torch.nn.modules.loss.SmoothL1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SmoothL1Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SmoothL1Loss.forward(self,input,target)
torch.nn.modules.loss.SoftMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SoftMarginLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SoftMarginLoss.forward(self,input,target)
torch.nn.modules.loss.TripletMarginLoss(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.TripletMarginLoss.__init__(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.TripletMarginLoss.forward(self,anchor,positive,negative)
torch.nn.modules.loss._Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._WeightedLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._WeightedLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/loss.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/normalization.py----------------------------------------
A:torch.nn.modules.normalization.self.normalized_shape->tuple(normalized_shape)
A:torch.nn.modules.normalization.self.weight->Parameter(torch.Tensor(num_channels))
A:torch.nn.modules.normalization.self.bias->Parameter(torch.Tensor(num_channels))
torch.nn.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.CrossMapLRN2d.extra_repr(self)
torch.nn.CrossMapLRN2d.forward(self,input)
torch.nn.GroupNorm(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.GroupNorm.extra_repr(self)
torch.nn.GroupNorm.forward(self,input)
torch.nn.GroupNorm.reset_parameters(self)
torch.nn.LayerNorm(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.LayerNorm.extra_repr(self)
torch.nn.LayerNorm.forward(self,input)
torch.nn.LayerNorm.reset_parameters(self)
torch.nn.LocalResponseNorm(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.LocalResponseNorm.extra_repr(self)
torch.nn.LocalResponseNorm.forward(self,input)
torch.nn.modules.normalization.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules.normalization.CrossMapLRN2d.__init__(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules.normalization.CrossMapLRN2d.extra_repr(self)
torch.nn.modules.normalization.CrossMapLRN2d.forward(self,input)
torch.nn.modules.normalization.GroupNorm(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.modules.normalization.GroupNorm.__init__(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.modules.normalization.GroupNorm.extra_repr(self)
torch.nn.modules.normalization.GroupNorm.forward(self,input)
torch.nn.modules.normalization.GroupNorm.reset_parameters(self)
torch.nn.modules.normalization.LayerNorm(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.modules.normalization.LayerNorm.__init__(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.modules.normalization.LayerNorm.extra_repr(self)
torch.nn.modules.normalization.LayerNorm.forward(self,input)
torch.nn.modules.normalization.LayerNorm.reset_parameters(self)
torch.nn.modules.normalization.LocalResponseNorm(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.modules.normalization.LocalResponseNorm.__init__(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.modules.normalization.LocalResponseNorm.extra_repr(self)
torch.nn.modules.normalization.LocalResponseNorm.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/normalization.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/rnn.py----------------------------------------
A:torch.nn.modules.rnn.self.dropout->float(dropout)
A:torch.nn.modules.rnn.w_ih->Parameter(torch.Tensor(gate_size, layer_input_size))
A:torch.nn.modules.rnn.w_hh->Parameter(torch.Tensor(gate_size, hidden_size))
A:torch.nn.modules.rnn.b_ih->Parameter(torch.Tensor(gate_size))
A:torch.nn.modules.rnn.b_hh->Parameter(torch.Tensor(gate_size))
A:torch.nn.modules.rnn.idx->self._flat_weights_names.index(attr)
A:torch.nn.modules.rnn.unique_data_ptrs->set((p.data_ptr() for p in self._flat_weights))
A:torch.nn.modules.rnn.ret->_VF.rnn_relu_cell(input, hx, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh)
A:torch.nn.modules.rnn.mini_batch->int(mini_batch)
A:torch.nn.modules.rnn.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.nn.modules.rnn.is_packed->isinstance(input, PackedSequence)
A:torch.nn.modules.rnn.max_batch_size->int(max_batch_size)
A:torch.nn.modules.rnn.hx->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.result->_VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias, self.num_layers, self.dropout, self.training, self.bidirectional)
A:torch.nn.modules.rnn.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.modules.rnn.replica->super(RNNBase, self)._replicate_for_data_parallel()
A:torch.nn.modules.rnn.self.nonlinearity->kwargs.pop('nonlinearity', 'tanh')
A:torch.nn.modules.rnn.zeros->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.output_packed->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.modules.rnn.self.weight_ih->Parameter(torch.Tensor(num_chunks * hidden_size, input_size))
A:torch.nn.modules.rnn.self.weight_hh->Parameter(torch.Tensor(num_chunks * hidden_size, hidden_size))
A:torch.nn.modules.rnn.self.bias_ih->Parameter(torch.Tensor(num_chunks * hidden_size))
A:torch.nn.modules.rnn.self.bias_hh->Parameter(torch.Tensor(num_chunks * hidden_size))
torch.nn.GRU(self,*args,**kwargs)
torch.nn.GRU.forward(self,input,hx=None)
torch.nn.GRUCell(self,input_size,hidden_size,bias=True)
torch.nn.GRUCell.forward(self,input,hx=None)
torch.nn.LSTM(self,*args,**kwargs)
torch.nn.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.LSTM.forward(self,input,hx=None)
torch.nn.LSTM.permute_hidden(self,hx,permutation)
torch.nn.LSTMCell(self,input_size,hidden_size,bias=True)
torch.nn.LSTMCell.forward(self,input,hx=None)
torch.nn.RNN(self,*args,**kwargs)
torch.nn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)
torch.nn.RNNBase.__setattr__(self,attr,value)
torch.nn.RNNBase.__setstate__(self,d)
torch.nn.RNNBase._apply(self,fn)
torch.nn.RNNBase._replicate_for_data_parallel(self)
torch.nn.RNNBase.all_weights(self)
torch.nn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.RNNBase.extra_repr(self)
torch.nn.RNNBase.flatten_parameters(self)
torch.nn.RNNBase.forward(self,input,hx=None)
torch.nn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.RNNBase.reset_parameters(self)
torch.nn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.RNNCell.forward(self,input,hx=None)
torch.nn.RNNCellBase(self,input_size,hidden_size,bias,num_chunks)
torch.nn.RNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.nn.RNNCellBase.check_forward_input(self,input)
torch.nn.RNNCellBase.extra_repr(self)
torch.nn.RNNCellBase.reset_parameters(self)
torch.nn.modules.rnn.GRU(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.forward(self,input,hx=None)
torch.nn.modules.rnn.GRUCell(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.GRUCell.__init__(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.GRUCell.forward(self,input,hx=None)
torch.nn.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.modules.rnn.LSTM.forward(self,input,hx=None)
torch.nn.modules.rnn.LSTM.permute_hidden(self,hx,permutation)
torch.nn.modules.rnn.LSTMCell(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.LSTMCell.__init__(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.LSTMCell.forward(self,input,hx=None)
torch.nn.modules.rnn.RNN(self,*args,**kwargs)
torch.nn.modules.rnn.RNN.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)
torch.nn.modules.rnn.RNNBase.__init__(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)
torch.nn.modules.rnn.RNNBase.__setattr__(self,attr,value)
torch.nn.modules.rnn.RNNBase.__setstate__(self,d)
torch.nn.modules.rnn.RNNBase._apply(self,fn)
torch.nn.modules.rnn.RNNBase._replicate_for_data_parallel(self)
torch.nn.modules.rnn.RNNBase.all_weights(self)
torch.nn.modules.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.modules.rnn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.modules.rnn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.modules.rnn.RNNBase.extra_repr(self)
torch.nn.modules.rnn.RNNBase.flatten_parameters(self)
torch.nn.modules.rnn.RNNBase.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.modules.rnn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.modules.rnn.RNNBase.reset_parameters(self)
torch.nn.modules.rnn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.modules.rnn.RNNCell.__init__(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.modules.rnn.RNNCell.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNCellBase(self,input_size,hidden_size,bias,num_chunks)
torch.nn.modules.rnn.RNNCellBase.__init__(self,input_size,hidden_size,bias,num_chunks)
torch.nn.modules.rnn.RNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.nn.modules.rnn.RNNCellBase.check_forward_input(self,input)
torch.nn.modules.rnn.RNNCellBase.extra_repr(self)
torch.nn.modules.rnn.RNNCellBase.reset_parameters(self)
torch.nn.modules.rnn.apply_permutation(tensor,permutation,dim=1)
torch.nn.rnn.apply_permutation(tensor,permutation,dim=1)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/rnn.pyi----------------------------------------
torch.nn.LSTM.forward_impl(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.LSTM.forward_packed(self,input:Tuple[Tensor,Tensor,Optional[Tensor],Optional[Tensor]],hx:Optional[Tuple[Tensor,Tensor]]=...)->Tuple[Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]], Tuple[Tensor, Tensor]]
torch.nn.LSTM.forward_tensor(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=...)->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.RNN.forward(self,input:Tensor,hx:Optional[Tensor]=...)->Tensor
torch.nn.RNNBase.get_flat_weights(self)
torch.nn.modules.rnn.LSTM.forward_impl(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.modules.rnn.LSTM.forward_packed(self,input:Tuple[Tensor,Tensor,Optional[Tensor],Optional[Tensor]],hx:Optional[Tuple[Tensor,Tensor]]=...)->Tuple[Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]], Tuple[Tensor, Tensor]]
torch.nn.modules.rnn.LSTM.forward_tensor(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=...)->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.modules.rnn.RNN.forward(self,input:Tensor,hx:Optional[Tensor]=...)->Tensor
torch.nn.modules.rnn.RNNBase.get_flat_weights(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/pixelshuffle.py----------------------------------------
torch.nn.PixelShuffle(self,upscale_factor)
torch.nn.PixelShuffle.extra_repr(self)
torch.nn.PixelShuffle.forward(self,input)
torch.nn.modules.pixelshuffle.PixelShuffle(self,upscale_factor)
torch.nn.modules.pixelshuffle.PixelShuffle.__init__(self,upscale_factor)
torch.nn.modules.pixelshuffle.PixelShuffle.extra_repr(self)
torch.nn.modules.pixelshuffle.PixelShuffle.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/pixelshuffle.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/adaptive.py----------------------------------------
A:torch.nn.modules.adaptive._ASMoutput->namedtuple('ASMoutput', ['output', 'loss'])
A:torch.nn.modules.adaptive.cutoffs->list(cutoffs)
A:torch.nn.modules.adaptive.self.head->Linear(self.in_features, self.head_size, bias=self.head_bias)
A:torch.nn.modules.adaptive.self.tail->ModuleList()
A:torch.nn.modules.adaptive.hsz->int(self.in_features // self.div_value ** (i + 1))
A:torch.nn.modules.adaptive.projection->Sequential(Linear(self.in_features, hsz, bias=False), Linear(hsz, osz, bias=False))
A:torch.nn.modules.adaptive.batch_size->target.size(0)
A:torch.nn.modules.adaptive.output->torch.argmax(head_output, dim=1)
A:torch.nn.modules.adaptive.gather_inds->target.new_empty(batch_size)
A:torch.nn.modules.adaptive.row_indices->target_mask.nonzero().squeeze()
A:torch.nn.modules.adaptive.input_subset->input.index_select(0, row_indices)
A:torch.nn.modules.adaptive.cluster_output->self.tail[i](input)
A:torch.nn.modules.adaptive.cluster_logprob->log_softmax(cluster_output, dim=1)
A:torch.nn.modules.adaptive.local_logprob->log_softmax(cluster_output, dim=1).gather(1, relative_target.unsqueeze(1))
A:torch.nn.modules.adaptive.head_output->self.head(input)
A:torch.nn.modules.adaptive.head_logprob->log_softmax(head_output, dim=1)
A:torch.nn.modules.adaptive.loss->(-output).mean()
A:torch.nn.modules.adaptive.out->input.new_empty((head_output.size(0), self.n_classes))
A:torch.nn.modules.adaptive.log_prob->self._get_full_log_prob(input[not_in_shortlist], head_output[not_in_shortlist])
A:torch.nn.modules.adaptive.output[not_in_shortlist]->torch.argmax(log_prob, dim=1)
torch.nn.AdaptiveLogSoftmaxWithLoss(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.AdaptiveLogSoftmaxWithLoss.forward(self,input,target)
torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob(self,input)
torch.nn.AdaptiveLogSoftmaxWithLoss.predict(self,input)
torch.nn.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.__init__(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.forward(self,input,target)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.log_prob(self,input)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.predict(self,input)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/adaptive.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/fold.py----------------------------------------
torch.nn.Fold(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.Fold.extra_repr(self)
torch.nn.Fold.forward(self,input)
torch.nn.Unfold(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.Unfold.extra_repr(self)
torch.nn.Unfold.forward(self,input)
torch.nn.modules.fold.Fold(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Fold.__init__(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Fold.extra_repr(self)
torch.nn.modules.fold.Fold.forward(self,input)
torch.nn.modules.fold.Unfold(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Unfold.__init__(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Unfold.extra_repr(self)
torch.nn.modules.fold.Unfold.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/fold.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/pooling.py----------------------------------------
A:torch.nn.modules.pooling.self.kernel_size->_triple(kernel_size)
A:torch.nn.modules.pooling.self.stride->_single(stride if stride is not None else kernel_size)
A:torch.nn.modules.pooling.self.padding->_single(padding)
torch.nn.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool1d.forward(self,input)
torch.nn.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool2d.forward(self,input)
torch.nn.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool3d.forward(self,input)
torch.nn.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool1d.forward(self,input)
torch.nn.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool2d.forward(self,input)
torch.nn.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool3d.forward(self,input)
torch.nn.AvgPool1d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.AvgPool1d.forward(self,input)
torch.nn.AvgPool2d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.AvgPool2d.forward(self,input)
torch.nn.AvgPool3d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.AvgPool3d.__setstate__(self,d)
torch.nn.AvgPool3d.forward(self,input)
torch.nn.FractionalMaxPool2d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.FractionalMaxPool2d.forward(self,input)
torch.nn.FractionalMaxPool3d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.FractionalMaxPool3d.forward(self,input)
torch.nn.LPPool1d(_LPPoolNd)
torch.nn.LPPool1d.forward(self,input)
torch.nn.LPPool2d(_LPPoolNd)
torch.nn.LPPool2d.forward(self,input)
torch.nn.MaxPool1d(_MaxPoolNd)
torch.nn.MaxPool1d.forward(self,input)
torch.nn.MaxPool2d(_MaxPoolNd)
torch.nn.MaxPool2d.forward(self,input)
torch.nn.MaxPool3d(_MaxPoolNd)
torch.nn.MaxPool3d.forward(self,input)
torch.nn.MaxUnpool1d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool1d.forward(self,input,indices,output_size=None)
torch.nn.MaxUnpool2d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool2d.forward(self,input,indices,output_size=None)
torch.nn.MaxUnpool3d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool3d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool1d.forward(self,input)
torch.nn.modules.pooling.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool2d.forward(self,input)
torch.nn.modules.pooling.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool3d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool1d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool2d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool3d.forward(self,input)
torch.nn.modules.pooling.AvgPool1d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool1d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool1d.forward(self,input)
torch.nn.modules.pooling.AvgPool2d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool2d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool2d.forward(self,input)
torch.nn.modules.pooling.AvgPool3d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool3d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool3d.__setstate__(self,d)
torch.nn.modules.pooling.AvgPool3d.forward(self,input)
torch.nn.modules.pooling.FractionalMaxPool2d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.__init__(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.forward(self,input)
torch.nn.modules.pooling.FractionalMaxPool3d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool3d.__init__(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool3d.forward(self,input)
torch.nn.modules.pooling.LPPool1d(_LPPoolNd)
torch.nn.modules.pooling.LPPool1d.forward(self,input)
torch.nn.modules.pooling.LPPool2d(_LPPoolNd)
torch.nn.modules.pooling.LPPool2d.forward(self,input)
torch.nn.modules.pooling.MaxPool1d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool1d.forward(self,input)
torch.nn.modules.pooling.MaxPool2d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool2d.forward(self,input)
torch.nn.modules.pooling.MaxPool3d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool3d.forward(self,input)
torch.nn.modules.pooling.MaxUnpool1d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool1d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool1d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.MaxUnpool2d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool2d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool2d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.MaxUnpool3d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool3d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool3d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling._AdaptiveAvgPoolNd(self,output_size)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.__init__(self,output_size)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.extra_repr(self)
torch.nn.modules.pooling._AdaptiveMaxPoolNd(self,output_size,return_indices=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.__init__(self,output_size,return_indices=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.extra_repr(self)
torch.nn.modules.pooling._AvgPoolNd(Module)
torch.nn.modules.pooling._AvgPoolNd.extra_repr(self)
torch.nn.modules.pooling._LPPoolNd(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.modules.pooling._LPPoolNd.__init__(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.modules.pooling._LPPoolNd.extra_repr(self)
torch.nn.modules.pooling._MaxPoolNd(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.modules.pooling._MaxPoolNd.__init__(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.modules.pooling._MaxPoolNd.extra_repr(self)
torch.nn.modules.pooling._MaxUnpoolNd(Module)
torch.nn.modules.pooling._MaxUnpoolNd.extra_repr(self)
torch.nn.pooling._AdaptiveAvgPoolNd(self,output_size)
torch.nn.pooling._AdaptiveAvgPoolNd.extra_repr(self)
torch.nn.pooling._AdaptiveMaxPoolNd(self,output_size,return_indices=False)
torch.nn.pooling._AdaptiveMaxPoolNd.extra_repr(self)
torch.nn.pooling._AvgPoolNd(Module)
torch.nn.pooling._AvgPoolNd.extra_repr(self)
torch.nn.pooling._LPPoolNd(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.pooling._LPPoolNd.extra_repr(self)
torch.nn.pooling._MaxPoolNd(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.pooling._MaxPoolNd.extra_repr(self)
torch.nn.pooling._MaxUnpoolNd(Module)
torch.nn.pooling._MaxUnpoolNd.extra_repr(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/pooling.pyi----------------------------------------
torch.nn.modules.pooling.AdaptiveAvgPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveAvgPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveAvgPool3d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool3d.__call__(self,input:Tensor)
torch.nn.modules.pooling.LPPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.LPPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.MaxPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.MaxPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.MaxPool3d.__call__(self,input:Tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/_functions.py----------------------------------------
A:torch.nn.modules._functions.input->input.contiguous().contiguous()
A:torch.nn.modules._functions.count->torch.Tensor([size]).to(input.device)
A:torch.nn.modules._functions.(mean, invstd)->torch.batch_norm_gather_stats_with_counts(input, mean_all, invstd_all, running_mean, running_var, momentum, eps, count_all.view(-1).long().tolist())
A:torch.nn.modules._functions.count_all->torch.empty(world_size, 1, dtype=count.dtype, device=count.device)
A:torch.nn.modules._functions.mean_all->torch.empty(world_size, mean.size(0), dtype=mean.dtype, device=mean.device)
A:torch.nn.modules._functions.invstd_all->torch.empty(world_size, invstd.size(0), dtype=invstd.dtype, device=invstd.device)
A:torch.nn.modules._functions.count_l->list(count_all.unbind(0))
A:torch.nn.modules._functions.mean_l->list(mean_all.unbind(0))
A:torch.nn.modules._functions.invstd_l->list(invstd_all.unbind(0))
A:torch.nn.modules._functions.count_all_reduce->torch.distributed.all_gather(count_l, count, process_group, async_op=True)
A:torch.nn.modules._functions.mean_all_reduce->torch.distributed.all_gather(mean_l, mean, process_group, async_op=True)
A:torch.nn.modules._functions.invstd_all_reduce->torch.distributed.all_gather(invstd_l, invstd, process_group, async_op=True)
A:torch.nn.modules._functions.out->torch.batch_norm_elemt(input, weight, bias, mean, invstd, eps)
A:torch.nn.modules._functions.grad_output->grad_output.contiguous().contiguous()
A:torch.nn.modules._functions.(sum_dy, sum_dy_xmu, grad_weight, grad_bias)->torch.batch_norm_backward_reduce(grad_output, saved_input, mean, invstd, weight, self.needs_input_grad[0], self.needs_input_grad[1], self.needs_input_grad[2])
A:torch.nn.modules._functions.sum_dy_all_reduce->torch.distributed.all_reduce(sum_dy, torch.distributed.ReduceOp.SUM, process_group, async_op=True)
A:torch.nn.modules._functions.sum_dy_xmu_all_reduce->torch.distributed.all_reduce(sum_dy_xmu, torch.distributed.ReduceOp.SUM, process_group, async_op=True)
A:torch.nn.modules._functions.divisor->count_tensor.sum()
A:torch.nn.modules._functions.grad_input->grad_output.contiguous().contiguous().new()
A:torch.nn.modules._functions.output->input.contiguous().contiguous().new()
A:torch.nn.modules._functions.batch_size->input.contiguous().contiguous().size(0)
A:torch.nn.modules._functions.channels->input.contiguous().contiguous().size(1)
A:torch.nn.modules._functions.input_height->input.contiguous().contiguous().size(2)
A:torch.nn.modules._functions.input_width->input.contiguous().contiguous().size(3)
A:torch.nn.modules._functions.pre_pad->int((ctx.size - 1) / 2 + 1)
A:torch.nn.modules._functions.scale_first->ctx.scale.select(1, 0)
A:torch.nn.modules._functions.scale_previous->ctx.scale.select(1, c - 1)
A:torch.nn.modules._functions.scale_current->ctx.scale.select(1, c)
A:torch.nn.modules._functions.square_next->input_square.select(1, c + pre_pad - 1)
A:torch.nn.modules._functions.square_previous->input_square.select(1, c - pre_pad)
A:torch.nn.modules._functions.paddded_ratio->input.contiguous().contiguous().new(channels + ctx.size - 1, input_height, input_width)
A:torch.nn.modules._functions.accum_ratio->input.contiguous().contiguous().new(input_height, input_width)
A:torch.nn.modules._functions.inversePrePad->int(ctx.size - (ctx.size - 1) / 2)
A:torch.nn.modules._functions.padded_ratio_center->input.contiguous().contiguous().new(channels + ctx.size - 1, input_height, input_width).narrow(0, inversePrePad, channels)
torch.nn._functions.CrossMapLRN2d(Function)
torch.nn._functions.CrossMapLRN2d.backward(ctx,grad_output)
torch.nn._functions.CrossMapLRN2d.forward(ctx,input,size,alpha=0.0001,beta=0.75,k=1)
torch.nn._functions.SyncBatchNorm(Function)
torch.nn._functions.SyncBatchNorm.backward(self,grad_output)
torch.nn._functions.SyncBatchNorm.forward(self,input,weight,bias,running_mean,running_var,eps,momentum,process_group,world_size)
torch.nn.modules._functions.CrossMapLRN2d(Function)
torch.nn.modules._functions.CrossMapLRN2d.backward(ctx,grad_output)
torch.nn.modules._functions.CrossMapLRN2d.forward(ctx,input,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules._functions.SyncBatchNorm(Function)
torch.nn.modules._functions.SyncBatchNorm.backward(self,grad_output)
torch.nn.modules._functions.SyncBatchNorm.forward(self,input,weight,bias,running_mean,running_var,eps,momentum,process_group,world_size)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/conv.py----------------------------------------
A:torch.nn.modules.conv.self._reversed_padding_repeated_twice->_reverse_repeat_tuple(self.padding, 2)
A:torch.nn.modules.conv.self.weight->Parameter(torch.Tensor(out_channels, in_channels // groups, *kernel_size))
A:torch.nn.modules.conv.self.bias->Parameter(torch.Tensor(out_channels))
A:torch.nn.modules.conv.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
A:torch.nn.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.modules.conv.stride->_triple(stride)
A:torch.nn.modules.conv.padding->_triple(padding)
A:torch.nn.modules.conv.dilation->_triple(dilation)
A:torch.nn.modules.conv.ret->_single(self.output_padding)
A:torch.nn.modules.conv.min_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.max_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.res->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.output_padding->self._output_padding(input, output_size, self.stride, self.padding, self.kernel_size)
torch.nn.Conv1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.Conv1d.forward(self,input)
torch.nn.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.Conv2d._conv_forward(self,input,weight)
torch.nn.Conv2d.forward(self,input)
torch.nn.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.Conv3d.forward(self,input)
torch.nn.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.ConvTranspose1d.forward(self,input,output_size=None)
torch.nn.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.ConvTranspose2d.forward(self,input,output_size=None)
torch.nn.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.ConvTranspose3d.forward(self,input,output_size=None)
torch.nn.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.conv._ConvNd.__setstate__(self,state)
torch.nn.conv._ConvNd.extra_repr(self)
torch.nn.conv._ConvNd.reset_parameters(self)
torch.nn.conv._ConvTransposeMixin(self,*args,**kwargs)
torch.nn.conv._ConvTransposeNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.conv._ConvTransposeNd._output_padding(self,input,output_size,stride,padding,kernel_size)
torch.nn.modules.conv.Conv1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv1d.forward(self,input)
torch.nn.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv2d._conv_forward(self,input,weight)
torch.nn.modules.conv.Conv2d.forward(self,input)
torch.nn.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv3d.forward(self,input)
torch.nn.modules.conv.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose1d.forward(self,input,output_size=None)
torch.nn.modules.conv.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose2d.forward(self,input,output_size=None)
torch.nn.modules.conv.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose3d.forward(self,input,output_size=None)
torch.nn.modules.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.modules.conv._ConvNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.modules.conv._ConvNd.__setstate__(self,state)
torch.nn.modules.conv._ConvNd.extra_repr(self)
torch.nn.modules.conv._ConvNd.reset_parameters(self)
torch.nn.modules.conv._ConvTransposeMixin(self,*args,**kwargs)
torch.nn.modules.conv._ConvTransposeMixin.__init__(self,*args,**kwargs)
torch.nn.modules.conv._ConvTransposeNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.modules.conv._ConvTransposeNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.modules.conv._ConvTransposeNd._output_padding(self,input,output_size,stride,padding,kernel_size)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/modules/conv.pyi----------------------------------------
torch.nn.conv._ConvTransposeMixin.forward(self,input:Tensor,output_size:Optional[List[int]]=...)
torch.nn.modules.conv._ConvTransposeMixin.__call__(self,input:Tensor,output_size:Optional[List[int]]=...)
torch.nn.modules.conv._ConvTransposeMixin.forward(self,input:Tensor,output_size:Optional[List[int]]=...)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/qat/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/qat/modules/linear.py----------------------------------------
A:torch.nn.qat.modules.linear.self.activation_post_process->qconfig.activation()
A:torch.nn.qat.modules.linear.self.weight_fake_quant->qconfig.weight()
A:torch.nn.qat.modules.linear.qat_linear->cls(mod.in_features, mod.out_features, bias=mod.bias is not None, qconfig=qconfig)
torch.nn.qat.Linear(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.qat.Linear.forward(self,input)
torch.nn.qat.Linear.from_float(cls,mod,qconfig=None)
torch.nn.qat.modules.linear.Linear(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.qat.modules.linear.Linear.__init__(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.qat.modules.linear.Linear.forward(self,input)
torch.nn.qat.modules.linear.Linear.from_float(cls,mod,qconfig=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/qat/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/qat/modules/conv.py----------------------------------------
A:torch.nn.qat.modules.conv.self.activation_post_process->qconfig.activation()
A:torch.nn.qat.modules.conv.self.weight_fake_quant->qconfig.weight()
A:torch.nn.qat.modules.conv.qat_conv->cls(mod.in_channels, mod.out_channels, mod.kernel_size, stride=mod.stride, padding=mod.padding, dilation=mod.dilation, groups=mod.groups, bias=mod.bias is not None, padding_mode=mod.padding_mode, qconfig=qconfig)
torch.nn.qat.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.qat.Conv2d.forward(self,input)
torch.nn.qat.Conv2d.from_float(cls,mod,qconfig=None)
torch.nn.qat.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.qat.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.qat.modules.conv.Conv2d.forward(self,input)
torch.nn.qat.modules.conv.Conv2d.from_float(cls,mod,qconfig=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/functional.py----------------------------------------
A:torch.nn.quantized.functional.stride->torch.jit.annotate(_List[int], [])
A:torch.nn.quantized.functional.padding->_triple(padding)
A:torch.nn.quantized.functional.dilation->_triple(dilation)
A:torch.nn.quantized.functional.prepacked_weight->torch.ops.quantized.conv3d_prepack(weight, bias, stride, padding, dilation, groups)
A:torch.nn.quantized.functional.scale->input.q_scale()
A:torch.nn.quantized.functional.zero_point->input.q_zero_point()
A:torch.nn.quantized.functional._packed_params->torch.ops.quantized.linear_prepack(weight, bias)
A:torch.nn.quantized.functional.output->torch.quantize_per_tensor(torch.zeros(input.shape), scale, int(zero_point), input.dtype)
A:torch.nn.quantized.functional.result->torch._C._nn.leaky_relu(input, negative_slope)
torch.nn.quantized.adaptive_avg_pool2d(input,output_size)
torch.nn.quantized.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.clamp(input,min_,max_)
torch.nn.quantized.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.elu(input,alpha=1.0,inplace=False,scale=None,zero_point=None)
torch.nn.quantized.functional.adaptive_avg_pool2d(input,output_size)
torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.functional.clamp(input,min_,max_)
torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.elu(input,alpha=1.0,inplace=False,scale=None,zero_point=None)
torch.nn.quantized.functional.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.functional.leaky_relu(input,negative_slope=0.01,inplace=False,scale=None,zero_point=None)
torch.nn.quantized.functional.linear(input,weight,bias=None,scale=None,zero_point=None)
torch.nn.quantized.functional.max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.quantized.functional.relu(input,inplace=False)
torch.nn.quantized.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.quantized.functional.upsample_nearest(input,size=None,scale_factor=None)
torch.nn.quantized.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.quantized.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.leaky_relu(input,negative_slope=0.01,inplace=False,scale=None,zero_point=None)
torch.nn.quantized.linear(input,weight,bias=None,scale=None,zero_point=None)
torch.nn.quantized.max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.quantized.relu(input,inplace=False)
torch.nn.quantized.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.quantized.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/activation.py----------------------------------------
torch.nn.quantized.ReLU(self,inplace=False)
torch.nn.quantized.ReLU._get_name(self)
torch.nn.quantized.ReLU.forward(self,input)
torch.nn.quantized.ReLU.from_float(mod)
torch.nn.quantized.ReLU6(self,inplace=False)
torch.nn.quantized.ReLU6._get_name(self)
torch.nn.quantized.ReLU6.forward(self,input)
torch.nn.quantized.ReLU6.from_float(mod)
torch.nn.quantized.modules.activation.ReLU(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU.__init__(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU._get_name(self)
torch.nn.quantized.modules.activation.ReLU.forward(self,input)
torch.nn.quantized.modules.activation.ReLU.from_float(mod)
torch.nn.quantized.modules.activation.ReLU6(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU6.__init__(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU6._get_name(self)
torch.nn.quantized.modules.activation.ReLU6.forward(self,input)
torch.nn.quantized.modules.activation.ReLU6.from_float(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/utils.py----------------------------------------
A:torch.nn.quantized.modules.utils.(wt_scale, wt_zp)->observer.calculate_qparams()
A:torch.nn.quantized.modules.utils.qweight->torch.quantize_per_channel(float_wt, wt_scale.to(torch.double), wt_zp.to(torch.int64), wt_axis, torch.qint8)
torch.nn.quantized.modules.utils._quantize_weight(float_wt,observer)
torch.nn.quantized.utils._quantize_weight(float_wt,observer)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/linear.py----------------------------------------
A:torch.nn.quantized.modules.linear.wq->torch.zeros([1, 1], dtype=torch.float)
A:torch.nn.quantized.modules.linear.self._packed_params->LinearPackedParams(dtype)
A:torch.nn.quantized.modules.linear.(w, b)->self._weight_bias()
A:torch.nn.quantized.modules.linear.version->local_metadata.get('version', None)
A:torch.nn.quantized.modules.linear.(qweight, bias)->self._weight_bias()
A:torch.nn.quantized.modules.linear.bias->state_dict.pop(prefix + 'bias')
A:torch.nn.quantized.modules.linear.qweight->_quantize_weight(mod.weight.float(), weight_post_process)
A:torch.nn.quantized.modules.linear.extra_repr->self.extra_repr()
A:torch.nn.quantized.modules.linear.extra_lines->self.extra_repr().split('\n')
A:torch.nn.quantized.modules.linear.mod_str->_addindent(mod_str, 2)
A:torch.nn.quantized.modules.linear.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.linear.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.linear.self.scale->float(state_dict[prefix + 'scale'])
A:torch.nn.quantized.modules.linear.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.nn.quantized.modules.linear.weight->state_dict.pop(prefix + 'weight')
A:torch.nn.quantized.modules.linear.weight_post_process->mod.qconfig.weight()
A:torch.nn.quantized.modules.linear.(act_scale, act_zp)->activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.linear.qlinear->cls(mod.in_features, mod.out_features, dtype=dtype)
A:torch.nn.quantized.modules.linear.qlinear.scale->float(act_scale)
A:torch.nn.quantized.modules.linear.qlinear.zero_point->int(act_zp)
torch.nn.quantized.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.Linear.__repr__(self)
torch.nn.quantized.Linear._get_name(self)
torch.nn.quantized.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.Linear._weight_bias(self)
torch.nn.quantized.Linear.bias(self)
torch.nn.quantized.Linear.extra_repr(self)
torch.nn.quantized.Linear.forward(self,x)
torch.nn.quantized.Linear.from_float(cls,mod)
torch.nn.quantized.Linear.set_weight_bias(self,w,b)
torch.nn.quantized.Linear.weight(self)
torch.nn.quantized.LinearPackedParams(self,dtype=torch.qint8)
torch.nn.quantized.LinearPackedParams.__getstate__(self)
torch.nn.quantized.LinearPackedParams.__repr__(self)
torch.nn.quantized.LinearPackedParams.__setstate__(self,state)
torch.nn.quantized.LinearPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.LinearPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.LinearPackedParams._weight_bias(self)
torch.nn.quantized.LinearPackedParams.forward(self,x)
torch.nn.quantized.LinearPackedParams.set_weight_bias(self,weight,bias)
torch.nn.quantized.modules.linear.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.modules.linear.Linear.__init__(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.modules.linear.Linear.__repr__(self)
torch.nn.quantized.modules.linear.Linear._get_name(self)
torch.nn.quantized.modules.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.linear.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.linear.Linear._weight_bias(self)
torch.nn.quantized.modules.linear.Linear.bias(self)
torch.nn.quantized.modules.linear.Linear.extra_repr(self)
torch.nn.quantized.modules.linear.Linear.forward(self,x)
torch.nn.quantized.modules.linear.Linear.from_float(cls,mod)
torch.nn.quantized.modules.linear.Linear.set_weight_bias(self,w,b)
torch.nn.quantized.modules.linear.Linear.weight(self)
torch.nn.quantized.modules.linear.LinearPackedParams(self,dtype=torch.qint8)
torch.nn.quantized.modules.linear.LinearPackedParams.__getstate__(self)
torch.nn.quantized.modules.linear.LinearPackedParams.__init__(self,dtype=torch.qint8)
torch.nn.quantized.modules.linear.LinearPackedParams.__repr__(self)
torch.nn.quantized.modules.linear.LinearPackedParams.__setstate__(self,state)
torch.nn.quantized.modules.linear.LinearPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.linear.LinearPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.linear.LinearPackedParams._weight_bias(self)
torch.nn.quantized.modules.linear.LinearPackedParams.forward(self,x)
torch.nn.quantized.modules.linear.LinearPackedParams.set_weight_bias(self,weight,bias)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/functional_modules.py----------------------------------------
A:torch.nn.quantized.modules.functional_modules.self.activation_post_process->torch.nn.Identity()
A:torch.nn.quantized.modules.functional_modules.r->self.activation_post_process(r)
A:torch.nn.quantized.modules.functional_modules.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.functional_modules.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.functional_modules.self.scale->float(state_dict.pop(prefix + 'scale'))
A:torch.nn.quantized.modules.functional_modules.self.zero_point->int(state_dict.pop(prefix + 'zero_point'))
A:torch.nn.quantized.modules.functional_modules.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.functional_modules.new_mod->QFunctional()
A:torch.nn.quantized.modules.functional_modules.new_mod.scale->float(scale)
A:torch.nn.quantized.modules.functional_modules.new_mod.zero_point->int(zero_point)
torch.nn.quantized.FloatFunctional(self)
torch.nn.quantized.FloatFunctional.add(self,x,y)
torch.nn.quantized.FloatFunctional.add_relu(self,x,y)
torch.nn.quantized.FloatFunctional.add_scalar(self,x,y)
torch.nn.quantized.FloatFunctional.cat(self,x,dim=0)
torch.nn.quantized.FloatFunctional.forward(self,x)
torch.nn.quantized.FloatFunctional.mul(self,x,y)
torch.nn.quantized.FloatFunctional.mul_scalar(self,x,y)
torch.nn.quantized.QFunctional(self)
torch.nn.quantized.QFunctional._get_name(self)
torch.nn.quantized.QFunctional._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.QFunctional._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.QFunctional.add(self,x,y)
torch.nn.quantized.QFunctional.add_relu(self,x,y)
torch.nn.quantized.QFunctional.add_scalar(self,x,y)
torch.nn.quantized.QFunctional.cat(self,x,dim=0)
torch.nn.quantized.QFunctional.extra_repr(self)
torch.nn.quantized.QFunctional.forward(self,x)
torch.nn.quantized.QFunctional.from_float(cls,mod)
torch.nn.quantized.QFunctional.mul(self,x,y)
torch.nn.quantized.QFunctional.mul_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional(self)
torch.nn.quantized.modules.functional_modules.FloatFunctional.__init__(self)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add_relu(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.cat(self,x,dim=0)
torch.nn.quantized.modules.functional_modules.FloatFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.FloatFunctional.mul(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.mul_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional(self)
torch.nn.quantized.modules.functional_modules.QFunctional.__init__(self)
torch.nn.quantized.modules.functional_modules.QFunctional._get_name(self)
torch.nn.quantized.modules.functional_modules.QFunctional._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.functional_modules.QFunctional._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.functional_modules.QFunctional.add(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.add_relu(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.add_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.cat(self,x,dim=0)
torch.nn.quantized.modules.functional_modules.QFunctional.extra_repr(self)
torch.nn.quantized.modules.functional_modules.QFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.QFunctional.from_float(cls,mod)
torch.nn.quantized.modules.functional_modules.QFunctional.mul(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.mul_scalar(self,x,y)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/batchnorm.py----------------------------------------
A:torch.nn.quantized.modules.batchnorm.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.batchnorm.new_mod->BatchNorm3d(mod.num_features, mod.eps)
A:torch.nn.quantized.modules.batchnorm.new_mod.scale->float(scale)
A:torch.nn.quantized.modules.batchnorm.new_mod.zero_point->int(zero_point)
torch.nn.quantized.BatchNorm2d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.quantized.BatchNorm2d._get_name(self)
torch.nn.quantized.BatchNorm2d.forward(self,input)
torch.nn.quantized.BatchNorm2d.from_float(cls,mod)
torch.nn.quantized.BatchNorm3d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.quantized.BatchNorm3d._get_name(self)
torch.nn.quantized.BatchNorm3d.forward(self,input)
torch.nn.quantized.BatchNorm3d.from_float(cls,mod)
torch.nn.quantized.modules.batchnorm.BatchNorm2d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.quantized.modules.batchnorm.BatchNorm2d.__init__(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.quantized.modules.batchnorm.BatchNorm2d._get_name(self)
torch.nn.quantized.modules.batchnorm.BatchNorm2d.forward(self,input)
torch.nn.quantized.modules.batchnorm.BatchNorm2d.from_float(cls,mod)
torch.nn.quantized.modules.batchnorm.BatchNorm3d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.quantized.modules.batchnorm.BatchNorm3d.__init__(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.quantized.modules.batchnorm.BatchNorm3d._get_name(self)
torch.nn.quantized.modules.batchnorm.BatchNorm3d.forward(self,input)
torch.nn.quantized.modules.batchnorm.BatchNorm3d.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/__init__.py----------------------------------------
A:torch.nn.quantized.modules.__init__.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
torch.nn.quantized.__init__.DeQuantize(self)
torch.nn.quantized.__init__.DeQuantize.forward(self,Xq)
torch.nn.quantized.__init__.DeQuantize.from_float(mod)
torch.nn.quantized.__init__.Quantize(self,scale,zero_point,dtype)
torch.nn.quantized.__init__.Quantize.extra_repr(self)
torch.nn.quantized.__init__.Quantize.forward(self,X)
torch.nn.quantized.__init__.Quantize.from_float(mod)
torch.nn.quantized.modules.__init__.DeQuantize(self)
torch.nn.quantized.modules.__init__.DeQuantize.__init__(self)
torch.nn.quantized.modules.__init__.DeQuantize.forward(self,Xq)
torch.nn.quantized.modules.__init__.DeQuantize.from_float(mod)
torch.nn.quantized.modules.__init__.Quantize(self,scale,zero_point,dtype)
torch.nn.quantized.modules.__init__.Quantize.__init__(self,scale,zero_point,dtype)
torch.nn.quantized.modules.__init__.Quantize.extra_repr(self)
torch.nn.quantized.modules.__init__.Quantize.forward(self,X)
torch.nn.quantized.modules.__init__.Quantize.from_float(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/modules/conv.py----------------------------------------
A:torch.nn.quantized.modules.conv.qweight->_quantize_weight(mod.weight.float(), weight_post_process)
A:torch.nn.quantized.modules.conv.(w, b)->self._weight_bias()
A:torch.nn.quantized.modules.conv.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.conv.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.conv.self.scale->float(state_dict[prefix + 'scale'])
A:torch.nn.quantized.modules.conv.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.nn.quantized.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.quantized.modules.conv.stride->_triple(stride)
A:torch.nn.quantized.modules.conv.padding->_triple(padding)
A:torch.nn.quantized.modules.conv.dilation->_triple(dilation)
A:torch.nn.quantized.modules.conv.self._packed_params->torch.ops.quantized.conv3d_prepack(w, b, self.stride, self.padding, self.dilation, self.groups)
A:torch.nn.quantized.modules.conv.(w, _)->torch.ops.quantized.conv3d_unpack(self._packed_params)
A:torch.nn.quantized.modules.conv.(_, b)->torch.ops.quantized.conv3d_unpack(self._packed_params)
A:torch.nn.quantized.modules.conv.(mod.weight, mod.bias)->fuse_conv_bn_weights(mod.weight, mod.bias, mod.running_mean, mod.running_var, mod.eps, mod.gamma, mod.beta)
A:torch.nn.quantized.modules.conv.weight_post_process->mod.qconfig.weight()
A:torch.nn.quantized.modules.conv.(act_scale, act_zp)->activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.conv.qconv->cls(mod.in_channels, mod.out_channels, mod.kernel_size, mod.stride, mod.padding, mod.dilation, mod.groups, mod.bias is not None, mod.padding_mode)
A:torch.nn.quantized.modules.conv.qconv.scale->float(act_scale)
A:torch.nn.quantized.modules.conv.qconv.zero_point->int(act_zp)
torch.nn.quantized.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.Conv2d._get_name(self)
torch.nn.quantized.Conv2d._weight_bias(self)
torch.nn.quantized.Conv2d.bias(self)
torch.nn.quantized.Conv2d.forward(self,input)
torch.nn.quantized.Conv2d.from_float(cls,mod)
torch.nn.quantized.Conv2d.set_weight_bias(self,w,b)
torch.nn.quantized.Conv2d.weight(self)
torch.nn.quantized.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.Conv3d._get_name(self)
torch.nn.quantized.Conv3d._weight_bias(self)
torch.nn.quantized.Conv3d.bias(self)
torch.nn.quantized.Conv3d.forward(self,input)
torch.nn.quantized.Conv3d.from_float(cls,mod)
torch.nn.quantized.Conv3d.set_weight_bias(self,w,b)
torch.nn.quantized.Conv3d.weight(self)
torch.nn.quantized.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.conv._ConvNd.__getstate__(self)
torch.nn.quantized.conv._ConvNd.__setstate__(self,state)
torch.nn.quantized.conv._ConvNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.conv._ConvNd._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.conv._ConvNd.extra_repr(self)
torch.nn.quantized.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv.Conv2d._get_name(self)
torch.nn.quantized.modules.conv.Conv2d._weight_bias(self)
torch.nn.quantized.modules.conv.Conv2d.bias(self)
torch.nn.quantized.modules.conv.Conv2d.forward(self,input)
torch.nn.quantized.modules.conv.Conv2d.from_float(cls,mod)
torch.nn.quantized.modules.conv.Conv2d.set_weight_bias(self,w,b)
torch.nn.quantized.modules.conv.Conv2d.weight(self)
torch.nn.quantized.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv.Conv3d._get_name(self)
torch.nn.quantized.modules.conv.Conv3d._weight_bias(self)
torch.nn.quantized.modules.conv.Conv3d.bias(self)
torch.nn.quantized.modules.conv.Conv3d.forward(self,input)
torch.nn.quantized.modules.conv.Conv3d.from_float(cls,mod)
torch.nn.quantized.modules.conv.Conv3d.set_weight_bias(self,w,b)
torch.nn.quantized.modules.conv.Conv3d.weight(self)
torch.nn.quantized.modules.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv._ConvNd.__getstate__(self)
torch.nn.quantized.modules.conv._ConvNd.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv._ConvNd.__setstate__(self,state)
torch.nn.quantized.modules.conv._ConvNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.conv._ConvNd._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.conv._ConvNd.extra_repr(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/dynamic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/dynamic/modules/linear.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.linear.Y->torch.ops.quantized.linear_dynamic_fp16(x, self._packed_params._packed_params)
A:torch.nn.quantized.dynamic.modules.linear.weight_observer->torch.quantization.qconfig.default_dynamic_qconfig.weight()
A:torch.nn.quantized.dynamic.modules.linear.qweight->mod.weight.float()
A:torch.nn.quantized.dynamic.modules.linear.qlinear->Linear(mod.in_features, mod.out_features, dtype=dtype)
torch.nn.quantized.dynamic.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.Linear._get_name(self)
torch.nn.quantized.dynamic.Linear.extra_repr(self)
torch.nn.quantized.dynamic.Linear.forward(self,x)
torch.nn.quantized.dynamic.Linear.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.linear.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.linear.Linear.__init__(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.linear.Linear._get_name(self)
torch.nn.quantized.dynamic.modules.linear.Linear.extra_repr(self)
torch.nn.quantized.dynamic.modules.linear.Linear.forward(self,x)
torch.nn.quantized.dynamic.modules.linear.Linear.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/dynamic/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/quantized/dynamic/modules/rnn.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.rnn.self.param->torch.ops.quantized.linear_prepack(weight, bias)
A:torch.nn.quantized.dynamic.modules.rnn.(w, b)->self.unpack()
A:torch.nn.quantized.dynamic.modules.rnn.self.dropout->float(dropout)
A:torch.nn.quantized.dynamic.modules.rnn.packed_weight->torch.fbgemm_pack_gemm_matrix_fp16(weight.float())
A:torch.nn.quantized.dynamic.modules.rnn.w_ih->torch.Tensor(gate_size, layer_input_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.w_hh->torch.Tensor(gate_size, hidden_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.b_ih->torch.Tensor(gate_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.b_hh->torch.Tensor(gate_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.(ih_params, ih_param_names)->process_weights('ih', layer, suffix, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.(hh_params, hh_param_names)->process_weights('hh', layer, suffix, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.self._all_weight_values->torch.nn.ModuleList(_all_weight_values)
A:torch.nn.quantized.dynamic.modules.rnn.extra_repr->self.extra_repr()
A:torch.nn.quantized.dynamic.modules.rnn.extra_lines->self.extra_repr().split('\n')
A:torch.nn.quantized.dynamic.modules.rnn.mod_str->torch.nn.modules.module._addindent(mod_str, 2)
A:torch.nn.quantized.dynamic.modules.rnn.mini_batch->int(mini_batch)
A:torch.nn.quantized.dynamic.modules.rnn.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.nn.quantized.dynamic.modules.rnn.result->torch._VF.quantized_lstm(input, batch_sizes, hx, weight_values, self.bias, self.num_layers, float(self.dropout), self.training, self.bidirectional, dtype=self.dtype, use_dynamic=True)
A:torch.nn.quantized.dynamic.modules.rnn.result[name]->self._all_weight_values[idx].unpack()
A:torch.nn.quantized.dynamic.modules.rnn.qRNNBase->LSTM(mod.input_size, mod.hidden_size, mod.num_layers, mod.bias, mod.batch_first, mod.dropout, mod.bidirectional, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.weight_name->'weight_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.nn.quantized.dynamic.modules.rnn.bias_name->'bias_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.nn.quantized.dynamic.modules.rnn.weight->getattr(mod, weight_name)
A:torch.nn.quantized.dynamic.modules.rnn.bias->getattr(mod, bias_name)
A:torch.nn.quantized.dynamic.modules.rnn.weight_observer->weight_observer_method()
A:torch.nn.quantized.dynamic.modules.rnn.(wt_scale, wt_zp)->weight_observer_method().calculate_qparams()
A:torch.nn.quantized.dynamic.modules.rnn.qweight->torch.quantize_per_tensor(weight.float(), float(wt_scale), int(wt_zp), torch.qint8)
A:torch.nn.quantized.dynamic.modules.rnn.qRNNBase._all_weight_values->torch.nn.ModuleList(_all_weight_values)
A:torch.nn.quantized.dynamic.modules.rnn.zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.quantized.dynamic.modules.rnn.hx->self.permute_hidden(hx, sorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.max_batch_size->int(max_batch_size)
A:torch.nn.quantized.dynamic.modules.rnn.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
torch.nn.quantized.dynamic.LSTM(self,*args,**kwargs)
torch.nn.quantized.dynamic.LSTM._get_name(self)
torch.nn.quantized.dynamic.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.LSTM.forward(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.quantized.dynamic.LSTM.forward_packed(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.forward_tensor(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.from_float(cls,mod)
torch.nn.quantized.dynamic.LSTM.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTM._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_packed(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_tensor(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.LSTM.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter(self,param)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.__getstate__(self)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.__init__(self,param)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.__repr__(self)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.__setstate__(self,state)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.forward(self)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.unpack(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__init__(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__repr__(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.all_weights(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.extra_repr(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.modules.rnn.apply_permutation(tensor,permutation,dim=1)
torch.nn.quantized.dynamic.rnn.PackedParameter(self,param)
torch.nn.quantized.dynamic.rnn.PackedParameter.__getstate__(self)
torch.nn.quantized.dynamic.rnn.PackedParameter.__repr__(self)
torch.nn.quantized.dynamic.rnn.PackedParameter.__setstate__(self,state)
torch.nn.quantized.dynamic.rnn.PackedParameter._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.rnn.PackedParameter._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.dynamic.rnn.PackedParameter.forward(self)
torch.nn.quantized.dynamic.rnn.PackedParameter.unpack(self)
torch.nn.quantized.dynamic.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.rnn.RNNBase.__repr__(self)
torch.nn.quantized.dynamic.rnn.RNNBase._get_name(self)
torch.nn.quantized.dynamic.rnn.RNNBase.all_weights(self)
torch.nn.quantized.dynamic.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.rnn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.quantized.dynamic.rnn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.quantized.dynamic.rnn.RNNBase.extra_repr(self)
torch.nn.quantized.dynamic.rnn.RNNBase.from_float(cls,mod)
torch.nn.quantized.dynamic.rnn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.quantized.dynamic.rnn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.rnn.apply_permutation(tensor,permutation,dim=1)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/modules/fused.py----------------------------------------
torch.nn.intrinsic.ConvBn2d(self,conv,bn)
torch.nn.intrinsic.ConvBnReLU2d(self,conv,bn,relu)
torch.nn.intrinsic.ConvReLU2d(self,conv,relu)
torch.nn.intrinsic.ConvReLU3d(self,conv,relu)
torch.nn.intrinsic.LinearReLU(self,linear,relu)
torch.nn.intrinsic.modules.fused.ConvBn2d(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn2d.__init__(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBnReLU2d(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU2d.__init__(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvReLU2d(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU2d.__init__(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU3d(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU3d.__init__(self,conv,relu)
torch.nn.intrinsic.modules.fused.LinearReLU(self,linear,relu)
torch.nn.intrinsic.modules.fused.LinearReLU.__init__(self,linear,relu)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/qat/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/qat/modules/conv_fused.py----------------------------------------
A:torch.nn.intrinsic.qat.modules.conv_fused.self.gamma->torch.nn.Parameter(torch.Tensor(out_channels))
A:torch.nn.intrinsic.qat.modules.conv_fused.self.beta->torch.nn.Parameter(torch.Tensor(out_channels))
A:torch.nn.intrinsic.qat.modules.conv_fused.self.activation_post_process->self.qconfig.activation()
A:torch.nn.intrinsic.qat.modules.conv_fused.self.weight_fake_quant->self.qconfig.weight()
A:torch.nn.intrinsic.qat.modules.conv_fused.running_std->torch.sqrt(self.running_var + self.eps)
A:torch.nn.intrinsic.qat.modules.conv_fused.conv->self._conv_forward(input, self.weight_fake_quant(scaled_weight))
A:torch.nn.intrinsic.qat.modules.conv_fused.batch_mean->torch.mean(conv_orig, dim=[0, 2, 3])
A:torch.nn.intrinsic.qat.modules.conv_fused.batch_var->torch.var(conv_orig, dim=[0, 2, 3], unbiased=False)
A:torch.nn.intrinsic.qat.modules.conv_fused.n->float(conv_orig.numel() / conv_orig.size()[1])
A:torch.nn.intrinsic.qat.modules.conv_fused.qat_convbn->cls(conv.in_channels, conv.out_channels, conv.kernel_size, conv.stride, conv.padding, conv.dilation, conv.groups, conv.padding_mode, bn.eps, bn.momentum, False, qconfig)
A:torch.nn.intrinsic.qat.modules.conv_fused.kernel_size->_pair(kernel_size)
A:torch.nn.intrinsic.qat.modules.conv_fused.stride->_pair(stride)
A:torch.nn.intrinsic.qat.modules.conv_fused.padding->_pair(padding)
A:torch.nn.intrinsic.qat.modules.conv_fused.dilation->_pair(dilation)
torch.nn.intrinsic.qat.ConvBn2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU2d.forward(self,input)
torch.nn.intrinsic.qat.ConvBnReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.qat.ConvReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,padding_mode,eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd._forward(self,input)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.extra_repr(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.forward(self,input)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.freeze_bn_stats(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.reset_bn_parameters(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.reset_parameters(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.reset_running_stats(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.update_bn_stats(self)
torch.nn.intrinsic.qat.freeze_bn_stats(mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,padding_mode,eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,padding_mode,eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd._forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.extra_repr(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.freeze_bn_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.reset_bn_parameters(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.reset_parameters(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.reset_running_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.update_bn_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused.freeze_bn_stats(mod)
torch.nn.intrinsic.qat.modules.conv_fused.update_bn_stats(mod)
torch.nn.intrinsic.qat.update_bn_stats(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/qat/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/qat/modules/linear_relu.py----------------------------------------
torch.nn.intrinsic.qat.LinearReLU(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.LinearReLU.forward(self,input)
torch.nn.intrinsic.qat.LinearReLU.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.forward(self,input)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.from_float(cls,mod,qconfig=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/quantized/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/quantized/modules/conv_relu.py----------------------------------------
A:torch.nn.intrinsic.quantized.modules.conv_relu.(mod.weight, mod.bias)->fuse_conv_bn_weights(mod.weight, mod.bias, mod.running_mean, mod.running_var, mod.eps, mod.gamma, mod.beta)
torch.nn.intrinsic.quantized.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.ConvReLU2d._get_name(self)
torch.nn.intrinsic.quantized.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.ConvReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.ConvReLU3d._get_name(self)
torch.nn.intrinsic.quantized.ConvReLU3d.forward(self,input)
torch.nn.intrinsic.quantized.ConvReLU3d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d._get_name(self)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d._get_name(self)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.forward(self,input)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/quantized/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/nn/intrinsic/quantized/modules/linear_relu.py----------------------------------------
A:torch.nn.intrinsic.quantized.modules.linear_relu.Y_q->torch.ops.quantized.linear_relu(input, self._packed_params._packed_params, float(self.scale), int(self.zero_point))
torch.nn.intrinsic.quantized.LinearReLU(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.LinearReLU.forward(self,input)
torch.nn.intrinsic.quantized.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.forward(self,input)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/poisson.py----------------------------------------
A:torch.distributions.poisson.(self.rate,)->broadcast_all(rate)
A:torch.distributions.poisson.batch_shape->torch.Size(batch_shape)
A:torch.distributions.poisson.new->self._get_checked_instance(Poisson, _instance)
A:torch.distributions.poisson.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.poisson.shape->self._extended_shape(sample_shape)
A:torch.distributions.poisson.(rate, value)->broadcast_all(self.rate, value)
torch.distributions.Poisson(self,rate,validate_args=None)
torch.distributions.Poisson._log_normalizer(self,x)
torch.distributions.Poisson._natural_params(self)
torch.distributions.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.Poisson.log_prob(self,value)
torch.distributions.Poisson.mean(self)
torch.distributions.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.Poisson.variance(self)
torch.distributions.poisson.Poisson(self,rate,validate_args=None)
torch.distributions.poisson.Poisson.__init__(self,rate,validate_args=None)
torch.distributions.poisson.Poisson._log_normalizer(self,x)
torch.distributions.poisson.Poisson._natural_params(self)
torch.distributions.poisson.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.poisson.Poisson.log_prob(self,value)
torch.distributions.poisson.Poisson.mean(self)
torch.distributions.poisson.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.poisson.Poisson.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/pareto.py----------------------------------------
A:torch.distributions.pareto.(self.scale, self.alpha)->broadcast_all(scale, alpha)
A:torch.distributions.pareto.base_dist->Exponential(self.alpha)
A:torch.distributions.pareto.new->self._get_checked_instance(Pareto, _instance)
A:torch.distributions.pareto.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.pareto.new.alpha->self.alpha.expand(batch_shape)
A:torch.distributions.pareto.a->self.alpha.clamp(min=2)
torch.distributions.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.Pareto.entropy(self)
torch.distributions.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.Pareto.mean(self)
torch.distributions.Pareto.support(self)
torch.distributions.Pareto.variance(self)
torch.distributions.pareto.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.__init__(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.entropy(self)
torch.distributions.pareto.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.pareto.Pareto.mean(self)
torch.distributions.pareto.Pareto.support(self)
torch.distributions.pareto.Pareto.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/cauchy.py----------------------------------------
A:torch.distributions.cauchy.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.cauchy.batch_shape->torch.Size(batch_shape)
A:torch.distributions.cauchy.new->self._get_checked_instance(Cauchy, _instance)
A:torch.distributions.cauchy.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.cauchy.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.cauchy.shape->self._extended_shape(sample_shape)
A:torch.distributions.cauchy.eps->self.loc.new(shape).cauchy_()
torch.distributions.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.Cauchy.cdf(self,value)
torch.distributions.Cauchy.entropy(self)
torch.distributions.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.Cauchy.icdf(self,value)
torch.distributions.Cauchy.log_prob(self,value)
torch.distributions.Cauchy.mean(self)
torch.distributions.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.Cauchy.variance(self)
torch.distributions.cauchy.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.__init__(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.cdf(self,value)
torch.distributions.cauchy.Cauchy.entropy(self)
torch.distributions.cauchy.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.cauchy.Cauchy.icdf(self,value)
torch.distributions.cauchy.Cauchy.log_prob(self,value)
torch.distributions.cauchy.Cauchy.mean(self)
torch.distributions.cauchy.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.cauchy.Cauchy.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/geometric.py----------------------------------------
A:torch.distributions.geometric.(self.probs,)->broadcast_all(probs)
A:torch.distributions.geometric.(self.logits,)->broadcast_all(logits)
A:torch.distributions.geometric.batch_shape->torch.Size(batch_shape)
A:torch.distributions.geometric.new->self._get_checked_instance(Geometric, _instance)
A:torch.distributions.geometric.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.geometric.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.geometric.shape->self._extended_shape(sample_shape)
A:torch.distributions.geometric.u->self.probs.new(shape).uniform_(tiny, 1)
A:torch.distributions.geometric.(value, probs)->broadcast_all(value, self.probs.clone(memory_format=torch.contiguous_format))
torch.distributions.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.Geometric.entropy(self)
torch.distributions.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.Geometric.log_prob(self,value)
torch.distributions.Geometric.logits(self)
torch.distributions.Geometric.mean(self)
torch.distributions.Geometric.probs(self)
torch.distributions.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.Geometric.variance(self)
torch.distributions.geometric.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.entropy(self)
torch.distributions.geometric.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.geometric.Geometric.log_prob(self,value)
torch.distributions.geometric.Geometric.logits(self)
torch.distributions.geometric.Geometric.mean(self)
torch.distributions.geometric.Geometric.probs(self)
torch.distributions.geometric.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.geometric.Geometric.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/half_normal.py----------------------------------------
A:torch.distributions.half_normal.base_dist->Normal(0, scale)
A:torch.distributions.half_normal.new->self._get_checked_instance(HalfNormal, _instance)
torch.distributions.HalfNormal(self,scale,validate_args=None)
torch.distributions.HalfNormal.cdf(self,value)
torch.distributions.HalfNormal.entropy(self)
torch.distributions.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.HalfNormal.icdf(self,prob)
torch.distributions.HalfNormal.log_prob(self,value)
torch.distributions.HalfNormal.mean(self)
torch.distributions.HalfNormal.scale(self)
torch.distributions.HalfNormal.variance(self)
torch.distributions.half_normal.HalfNormal(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.__init__(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.cdf(self,value)
torch.distributions.half_normal.HalfNormal.entropy(self)
torch.distributions.half_normal.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.half_normal.HalfNormal.icdf(self,prob)
torch.distributions.half_normal.HalfNormal.log_prob(self,value)
torch.distributions.half_normal.HalfNormal.mean(self)
torch.distributions.half_normal.HalfNormal.scale(self)
torch.distributions.half_normal.HalfNormal.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/one_hot_categorical.py----------------------------------------
A:torch.distributions.one_hot_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.one_hot_categorical.new->self._get_checked_instance(OneHotCategorical, _instance)
A:torch.distributions.one_hot_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.one_hot_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.one_hot_categorical.sample_shape->torch.Size(sample_shape)
A:torch.distributions.one_hot_categorical.indices->self._categorical.sample(sample_shape)
A:torch.distributions.one_hot_categorical.values->values.expand((n,) + self.batch_shape + (n,)).expand((n,) + self.batch_shape + (n,))
torch.distributions.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.OneHotCategorical._param(self)
torch.distributions.OneHotCategorical.entropy(self)
torch.distributions.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.OneHotCategorical.log_prob(self,value)
torch.distributions.OneHotCategorical.logits(self)
torch.distributions.OneHotCategorical.mean(self)
torch.distributions.OneHotCategorical.param_shape(self)
torch.distributions.OneHotCategorical.probs(self)
torch.distributions.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.OneHotCategorical.variance(self)
torch.distributions.one_hot_categorical.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.one_hot_categorical.OneHotCategorical._param(self)
torch.distributions.one_hot_categorical.OneHotCategorical.entropy(self)
torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.one_hot_categorical.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.one_hot_categorical.OneHotCategorical.log_prob(self,value)
torch.distributions.one_hot_categorical.OneHotCategorical.logits(self)
torch.distributions.one_hot_categorical.OneHotCategorical.mean(self)
torch.distributions.one_hot_categorical.OneHotCategorical.param_shape(self)
torch.distributions.one_hot_categorical.OneHotCategorical.probs(self)
torch.distributions.one_hot_categorical.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.one_hot_categorical.OneHotCategorical.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/logistic_normal.py----------------------------------------
A:torch.distributions.logistic_normal.base_dist->Normal(loc, scale)
A:torch.distributions.logistic_normal.self._event_shape->torch.Size([s + 1 for s in self._event_shape])
A:torch.distributions.logistic_normal.new->self._get_checked_instance(LogisticNormal, _instance)
torch.distributions.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogisticNormal.loc(self)
torch.distributions.LogisticNormal.scale(self)
torch.distributions.logistic_normal.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.logistic_normal.LogisticNormal.loc(self)
torch.distributions.logistic_normal.LogisticNormal.scale(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/utils.py----------------------------------------
A:torch.distributions.utils.options->dict(dtype=value.dtype, device=value.device)
A:torch.distributions.utils.ps_clamped->clamp_probs(probs)
A:torch.distributions.utils.value->self.wrapped(instance)
torch.distributions.utils._standard_normal(shape,dtype,device)
torch.distributions.utils._sum_rightmost(value,dim)
torch.distributions.utils.broadcast_all(*values)
torch.distributions.utils.clamp_probs(probs)
torch.distributions.utils.lazy_property(self,wrapped)
torch.distributions.utils.lazy_property.__get__(self,instance,obj_type=None)
torch.distributions.utils.lazy_property.__init__(self,wrapped)
torch.distributions.utils.logits_to_probs(logits,is_binary=False)
torch.distributions.utils.probs_to_logits(probs,is_binary=False)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/weibull.py----------------------------------------
A:torch.distributions.weibull.(self.scale, self.concentration)->broadcast_all(scale, concentration)
A:torch.distributions.weibull.self.concentration_reciprocal->self.concentration.reciprocal()
A:torch.distributions.weibull.base_dist->self.base_dist.expand(batch_shape)
A:torch.distributions.weibull.new->self._get_checked_instance(Weibull, _instance)
A:torch.distributions.weibull.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.weibull.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.weibull.new.concentration_reciprocal->self._get_checked_instance(Weibull, _instance).concentration.reciprocal()
torch.distributions.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.Weibull.entropy(self)
torch.distributions.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.Weibull.mean(self)
torch.distributions.Weibull.variance(self)
torch.distributions.weibull.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.__init__(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.entropy(self)
torch.distributions.weibull.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.weibull.Weibull.mean(self)
torch.distributions.weibull.Weibull.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/log_normal.py----------------------------------------
A:torch.distributions.log_normal.base_dist->Normal(loc, scale)
A:torch.distributions.log_normal.new->self._get_checked_instance(LogNormal, _instance)
torch.distributions.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.LogNormal.entropy(self)
torch.distributions.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogNormal.loc(self)
torch.distributions.LogNormal.mean(self)
torch.distributions.LogNormal.scale(self)
torch.distributions.LogNormal.variance(self)
torch.distributions.log_normal.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.entropy(self)
torch.distributions.log_normal.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.log_normal.LogNormal.loc(self)
torch.distributions.log_normal.LogNormal.mean(self)
torch.distributions.log_normal.LogNormal.scale(self)
torch.distributions.log_normal.LogNormal.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/lowrank_multivariate_normal.py----------------------------------------
A:torch.distributions.lowrank_multivariate_normal.m->W.size(-1)
A:torch.distributions.lowrank_multivariate_normal.K->torch.matmul(Dinvsqrt_W, Dinvsqrt_W.transpose(-1, -2)).contiguous()
A:torch.distributions.lowrank_multivariate_normal.Wt_Dinv_x->_batch_mv(Wt_Dinv, x)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term1->(x.pow(2) / D).sum(-1)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term2->_batch_mahalanobis(capacitance_tril, Wt_Dinv_x)
A:torch.distributions.lowrank_multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.cov_diag_->cov_diag.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.(loc_, self.cov_factor, cov_diag_)->torch.broadcast_tensors(loc_, cov_factor, cov_diag_)
A:torch.distributions.lowrank_multivariate_normal.self._capacitance_tril->_batch_capacitance_tril(cov_factor, cov_diag)
A:torch.distributions.lowrank_multivariate_normal.new->self._get_checked_instance(LowRankMultivariateNormal, _instance)
A:torch.distributions.lowrank_multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.lowrank_multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_diag->self.cov_diag.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_factor->self.cov_factor.expand(loc_shape + self.cov_factor.shape[-1:])
A:torch.distributions.lowrank_multivariate_normal.cov_diag_sqrt_unsqueeze->self._unbroadcasted_cov_diag.sqrt().unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.lowrank_multivariate_normal.eps_W->_standard_normal(W_shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.eps_D->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.M->_batch_lowrank_mahalanobis(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, diff, self._capacitance_tril)
A:torch.distributions.lowrank_multivariate_normal.log_det->_batch_lowrank_logdet(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, self._capacitance_tril)
torch.distributions.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.LowRankMultivariateNormal.entropy(self)
torch.distributions.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.LowRankMultivariateNormal.mean(self)
torch.distributions.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.__init__(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal._batch_capacitance_tril(W,D)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_logdet(W,D,capacitance_tril)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_mahalanobis(W,D,x,capacitance_tril)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/transformed_distribution.py----------------------------------------
A:torch.distributions.transformed_distribution.event_dim->len(self.event_shape)
A:torch.distributions.transformed_distribution.new->self._get_checked_instance(TransformedDistribution, _instance)
A:torch.distributions.transformed_distribution.batch_shape->torch.Size(batch_shape)
A:torch.distributions.transformed_distribution.new.base_dist->self.base_dist.expand(base_dist_batch_shape)
A:torch.distributions.transformed_distribution.x->transform.inv(y)
A:torch.distributions.transformed_distribution.value->transform(value)
torch.distributions.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.TransformedDistribution.cdf(self,value)
torch.distributions.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.TransformedDistribution.has_rsample(self)
torch.distributions.TransformedDistribution.icdf(self,value)
torch.distributions.TransformedDistribution.log_prob(self,value)
torch.distributions.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.support(self)
torch.distributions.transformed_distribution.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution.__init__(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.transformed_distribution.TransformedDistribution.has_rsample(self)
torch.distributions.transformed_distribution.TransformedDistribution.icdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.log_prob(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.support(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/exp_family.py----------------------------------------
A:torch.distributions.exp_family.lg_normal->self._log_normalizer(*nparams)
A:torch.distributions.exp_family.gradients->torch.autograd.grad(lg_normal.sum(), nparams, create_graph=True)
torch.distributions.ExponentialFamily(Distribution)
torch.distributions.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.ExponentialFamily._natural_params(self)
torch.distributions.ExponentialFamily.entropy(self)
torch.distributions.exp_family.ExponentialFamily(Distribution)
torch.distributions.exp_family.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.exp_family.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.exp_family.ExponentialFamily._natural_params(self)
torch.distributions.exp_family.ExponentialFamily.entropy(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/laplace.py----------------------------------------
A:torch.distributions.laplace.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.laplace.batch_shape->torch.Size(batch_shape)
A:torch.distributions.laplace.new->self._get_checked_instance(Laplace, _instance)
A:torch.distributions.laplace.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.laplace.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.laplace.shape->self._extended_shape(sample_shape)
A:torch.distributions.laplace.finfo->torch.finfo(self.loc.dtype)
A:torch.distributions.laplace.u->self.loc.new(shape).uniform_(finfo.eps - 1, 1)
torch.distributions.Laplace(self,loc,scale,validate_args=None)
torch.distributions.Laplace.cdf(self,value)
torch.distributions.Laplace.entropy(self)
torch.distributions.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.Laplace.icdf(self,value)
torch.distributions.Laplace.log_prob(self,value)
torch.distributions.Laplace.mean(self)
torch.distributions.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.Laplace.stddev(self)
torch.distributions.Laplace.variance(self)
torch.distributions.laplace.Laplace(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.__init__(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.cdf(self,value)
torch.distributions.laplace.Laplace.entropy(self)
torch.distributions.laplace.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.laplace.Laplace.icdf(self,value)
torch.distributions.laplace.Laplace.log_prob(self,value)
torch.distributions.laplace.Laplace.mean(self)
torch.distributions.laplace.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.laplace.Laplace.stddev(self)
torch.distributions.laplace.Laplace.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/multivariate_normal.py----------------------------------------
A:torch.distributions.multivariate_normal.n->bx.permute(permute_dims).size(-1)
A:torch.distributions.multivariate_normal.bx_batch_dims->len(bx_batch_shape)
A:torch.distributions.multivariate_normal.bx->bx.permute(permute_dims).permute(permute_dims)
A:torch.distributions.multivariate_normal.flat_L->bL.reshape(-1, n, n)
A:torch.distributions.multivariate_normal.flat_x->bx.permute(permute_dims).permute(permute_dims).reshape(-1, flat_L.size(0), n)
A:torch.distributions.multivariate_normal.flat_x_swap->bx.permute(permute_dims).permute(permute_dims).reshape(-1, flat_L.size(0), n).permute(1, 2, 0)
A:torch.distributions.multivariate_normal.M_swap->torch.triangular_solve(flat_x_swap, flat_L, upper=False)[0].pow(2).sum(-2)
A:torch.distributions.multivariate_normal.M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff)
A:torch.distributions.multivariate_normal.permuted_M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff).reshape(bx.shape[:-1])
A:torch.distributions.multivariate_normal.permute_inv_dims->list(range(outer_batch_dims))
A:torch.distributions.multivariate_normal.reshaped_M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff).reshape(bx.shape[:-1]).permute(permute_inv_dims)
A:torch.distributions.multivariate_normal.Lf->torch.cholesky(torch.flip(P, (-2, -1)))
A:torch.distributions.multivariate_normal.L_inv->torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)
A:torch.distributions.multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.multivariate_normal.(self.scale_tril, loc_)->torch.broadcast_tensors(scale_tril, loc_)
A:torch.distributions.multivariate_normal.(self.covariance_matrix, loc_)->torch.broadcast_tensors(covariance_matrix, loc_)
A:torch.distributions.multivariate_normal.(self.precision_matrix, loc_)->torch.broadcast_tensors(precision_matrix, loc_)
A:torch.distributions.multivariate_normal.self._unbroadcasted_scale_tril->_precision_to_scale_tril(precision_matrix)
A:torch.distributions.multivariate_normal.new->self._get_checked_instance(MultivariateNormal, _instance)
A:torch.distributions.multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.multivariate_normal.new.covariance_matrix->self.covariance_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.scale_tril->self.scale_tril.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.precision_matrix->self.precision_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.identity->torch.eye(self.loc.size(-1), device=self.loc.device, dtype=self.loc.dtype)
A:torch.distributions.multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.multivariate_normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.multivariate_normal.half_log_det->self._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)
torch.distributions.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.MultivariateNormal.covariance_matrix(self)
torch.distributions.MultivariateNormal.entropy(self)
torch.distributions.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.MultivariateNormal.log_prob(self,value)
torch.distributions.MultivariateNormal.mean(self)
torch.distributions.MultivariateNormal.precision_matrix(self)
torch.distributions.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.MultivariateNormal.scale_tril(self)
torch.distributions.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.__init__(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.entropy(self)
torch.distributions.multivariate_normal.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.multivariate_normal.MultivariateNormal.log_prob(self,value)
torch.distributions.multivariate_normal.MultivariateNormal.mean(self)
torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.multivariate_normal.MultivariateNormal.scale_tril(self)
torch.distributions.multivariate_normal.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal._batch_mahalanobis(bL,bx)
torch.distributions.multivariate_normal._batch_mv(bmat,bvec)
torch.distributions.multivariate_normal._precision_to_scale_tril(P)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/gamma.py----------------------------------------
A:torch.distributions.gamma.(self.concentration, self.rate)->broadcast_all(concentration, rate)
A:torch.distributions.gamma.batch_shape->torch.Size(batch_shape)
A:torch.distributions.gamma.new->self._get_checked_instance(Gamma, _instance)
A:torch.distributions.gamma.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.gamma.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.gamma.shape->self._extended_shape(sample_shape)
A:torch.distributions.gamma.value->torch.as_tensor(value, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.Gamma._log_normalizer(self,x,y)
torch.distributions.Gamma._natural_params(self)
torch.distributions.Gamma.entropy(self)
torch.distributions.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.Gamma.log_prob(self,value)
torch.distributions.Gamma.mean(self)
torch.distributions.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.Gamma.variance(self)
torch.distributions.gamma.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma.__init__(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma._log_normalizer(self,x,y)
torch.distributions.gamma.Gamma._natural_params(self)
torch.distributions.gamma.Gamma.entropy(self)
torch.distributions.gamma.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.gamma.Gamma.log_prob(self,value)
torch.distributions.gamma.Gamma.mean(self)
torch.distributions.gamma.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.gamma.Gamma.variance(self)
torch.distributions.gamma._standard_gamma(concentration)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/mixture_same_family.py----------------------------------------
A:torch.distributions.mixture_same_family.self._event_ndims->len(event_shape)
A:torch.distributions.mixture_same_family.batch_shape->torch.Size(batch_shape)
A:torch.distributions.mixture_same_family.new->self._get_checked_instance(MixtureSameFamily, _instance)
A:torch.distributions.mixture_same_family.new._component_distribution->self._component_distribution.expand(batch_shape_comp)
A:torch.distributions.mixture_same_family.new._mixture_distribution->self._mixture_distribution.expand(batch_shape)
A:torch.distributions.mixture_same_family.probs->self._pad_mixture_dimensions(self.mixture_distribution.probs)
A:torch.distributions.mixture_same_family.mean_cond_var->torch.sum(probs * self.component_distribution.variance, dim=-1 - self._event_ndims)
A:torch.distributions.mixture_same_family.var_cond_mean->torch.sum(probs * (self.component_distribution.mean - self._pad(self.mean)).pow(2.0), dim=-1 - self._event_ndims)
A:torch.distributions.mixture_same_family.x->x.reshape(xs[:-1] + torch.Size(pad_ndims * [1]) + xs[-1:] + torch.Size(self._event_ndims * [1])).reshape(xs[:-1] + torch.Size(pad_ndims * [1]) + xs[-1:] + torch.Size(self._event_ndims * [1]))
A:torch.distributions.mixture_same_family.cdf_x->self.component_distribution.cdf(x)
A:torch.distributions.mixture_same_family.log_prob_x->self.component_distribution.log_prob(x)
A:torch.distributions.mixture_same_family.log_mix_prob->torch.log_softmax(self.mixture_distribution.logits, dim=-1)
A:torch.distributions.mixture_same_family.sample_len->len(sample_shape)
A:torch.distributions.mixture_same_family.batch_len->len(self.batch_shape)
A:torch.distributions.mixture_same_family.mix_sample->self.mixture_distribution.sample(sample_shape)
A:torch.distributions.mixture_same_family.comp_samples->self.component_distribution.sample(sample_shape)
A:torch.distributions.mixture_same_family.mix_sample_r->mix_sample_r.repeat(torch.Size([1] * len(mix_shape)) + torch.Size([1]) + es).repeat(torch.Size([1] * len(mix_shape)) + torch.Size([1]) + es)
A:torch.distributions.mixture_same_family.samples->torch.gather(comp_samples, gather_dim, mix_sample_r)
A:torch.distributions.mixture_same_family.dist_batch_ndims->self.batch_shape.numel()
A:torch.distributions.mixture_same_family.cat_batch_ndims->self.mixture_distribution.batch_shape.numel()
A:torch.distributions.mixture_same_family.args_string->'\n  {},\n  {}'.format(self.mixture_distribution, self.component_distribution)
torch.distributions.MixtureSameFamily(self,mixture_distribution,component_distribution,validate_args=None)
torch.distributions.MixtureSameFamily.__repr__(self)
torch.distributions.MixtureSameFamily._pad(self,x)
torch.distributions.MixtureSameFamily._pad_mixture_dimensions(self,x)
torch.distributions.MixtureSameFamily.cdf(self,x)
torch.distributions.MixtureSameFamily.component_distribution(self)
torch.distributions.MixtureSameFamily.expand(self,batch_shape,_instance=None)
torch.distributions.MixtureSameFamily.log_prob(self,x)
torch.distributions.MixtureSameFamily.mean(self)
torch.distributions.MixtureSameFamily.mixture_distribution(self)
torch.distributions.MixtureSameFamily.sample(self,sample_shape=torch.Size())
torch.distributions.MixtureSameFamily.support(self)
torch.distributions.MixtureSameFamily.variance(self)
torch.distributions.mixture_same_family.MixtureSameFamily(self,mixture_distribution,component_distribution,validate_args=None)
torch.distributions.mixture_same_family.MixtureSameFamily.__init__(self,mixture_distribution,component_distribution,validate_args=None)
torch.distributions.mixture_same_family.MixtureSameFamily.__repr__(self)
torch.distributions.mixture_same_family.MixtureSameFamily._pad(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily._pad_mixture_dimensions(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily.cdf(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution(self)
torch.distributions.mixture_same_family.MixtureSameFamily.expand(self,batch_shape,_instance=None)
torch.distributions.mixture_same_family.MixtureSameFamily.log_prob(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily.mean(self)
torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution(self)
torch.distributions.mixture_same_family.MixtureSameFamily.sample(self,sample_shape=torch.Size())
torch.distributions.mixture_same_family.MixtureSameFamily.support(self)
torch.distributions.mixture_same_family.MixtureSameFamily.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/distribution.py----------------------------------------
A:torch.distributions.distribution.sample_shape->torch.Size(sample_shape)
A:torch.distributions.distribution.actual_shape->value.size()
A:torch.distributions.distribution.args_string->', '.join(['{}: {}'.format(p, self.__dict__[p] if self.__dict__[p].numel() == 1 else self.__dict__[p].size()) for p in param_names])
torch.distributions.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.Distribution.__repr__(self)
torch.distributions.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.Distribution._validate_sample(self,value)
torch.distributions.Distribution.arg_constraints(self)
torch.distributions.Distribution.batch_shape(self)
torch.distributions.Distribution.cdf(self,value)
torch.distributions.Distribution.entropy(self)
torch.distributions.Distribution.enumerate_support(self,expand=True)
torch.distributions.Distribution.event_shape(self)
torch.distributions.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.Distribution.icdf(self,value)
torch.distributions.Distribution.log_prob(self,value)
torch.distributions.Distribution.mean(self)
torch.distributions.Distribution.perplexity(self)
torch.distributions.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample_n(self,n)
torch.distributions.Distribution.set_default_validate_args(value)
torch.distributions.Distribution.stddev(self)
torch.distributions.Distribution.support(self)
torch.distributions.Distribution.variance(self)
torch.distributions.distribution.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__init__(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__repr__(self)
torch.distributions.distribution.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.distribution.Distribution._validate_sample(self,value)
torch.distributions.distribution.Distribution.arg_constraints(self)
torch.distributions.distribution.Distribution.batch_shape(self)
torch.distributions.distribution.Distribution.cdf(self,value)
torch.distributions.distribution.Distribution.entropy(self)
torch.distributions.distribution.Distribution.enumerate_support(self,expand=True)
torch.distributions.distribution.Distribution.event_shape(self)
torch.distributions.distribution.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.distribution.Distribution.icdf(self,value)
torch.distributions.distribution.Distribution.log_prob(self,value)
torch.distributions.distribution.Distribution.mean(self)
torch.distributions.distribution.Distribution.perplexity(self)
torch.distributions.distribution.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample_n(self,n)
torch.distributions.distribution.Distribution.set_default_validate_args(value)
torch.distributions.distribution.Distribution.stddev(self)
torch.distributions.distribution.Distribution.support(self)
torch.distributions.distribution.Distribution.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/transforms.py----------------------------------------
A:torch.distributions.transforms.inv->ComposeTransform([p.inv for p in reversed(self.parts)])
A:torch.distributions.transforms.self._inv->weakref.ref(inv)
A:torch.distributions.transforms.y->y.clamp(min=finfo.tiny, max=1.0 - finfo.eps).clamp(min=finfo.tiny, max=1.0 - finfo.eps)
A:torch.distributions.transforms.x->part(x)
A:torch.distributions.transforms.inv._inv->weakref.ref(self)
A:torch.distributions.transforms.y_tmp->part(x)
A:torch.distributions.transforms.identity_transform->ComposeTransform([])
A:torch.distributions.transforms.(self.exponent,)->broadcast_all(exponent)
A:torch.distributions.transforms.finfo->torch.finfo(y.dtype)
A:torch.distributions.transforms.codomain->torch.distributions.constraints.interval(-1.0, 1.0)
A:torch.distributions.transforms.result->result.view(result_size).sum(-1).view(result_size).sum(-1)
A:torch.distributions.transforms.probs->(logprobs - logprobs.max(-1, True)[0]).exp()
A:torch.distributions.transforms.z->_clipped_sigmoid(x - offset.log())
A:torch.distributions.transforms.z_cumprod->(1 - z).cumprod(-1)
A:torch.distributions.transforms.sf->torch.clamp(sf, min=torch.finfo(y.dtype).tiny)
A:torch.distributions.transforms.detJ->(-x + F.logsigmoid(x) + y[..., :-1].log()).sum(-1)
A:torch.distributions.transforms.self.transforms->list(tseq)
A:torch.distributions.transforms.self.lengths->list(lengths)
A:torch.distributions.transforms.xslice->part(x).narrow(self.dim, start, length)
A:torch.distributions.transforms.yslice->y.clamp(min=finfo.tiny, max=1.0 - finfo.eps).clamp(min=finfo.tiny, max=1.0 - finfo.eps).narrow(self.dim, start, length)
A:torch.distributions.transforms.yslices->self._slice(y)
A:torch.distributions.transforms.xslices->self._slice(x)
torch.distributions.AbsTransform(Transform)
torch.distributions.AbsTransform.__eq__(self,other)
torch.distributions.AbsTransform._call(self,x)
torch.distributions.AbsTransform._inverse(self,y)
torch.distributions.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.AffineTransform.__eq__(self,other)
torch.distributions.AffineTransform._call(self,x)
torch.distributions.AffineTransform._inverse(self,y)
torch.distributions.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.AffineTransform.sign(self)
torch.distributions.CatTransform(self,tseq,dim=0,lengths=None)
torch.distributions.CatTransform._call(self,x)
torch.distributions.CatTransform._inverse(self,y)
torch.distributions.CatTransform.bijective(self)
torch.distributions.CatTransform.codomain(self)
torch.distributions.CatTransform.domain(self)
torch.distributions.CatTransform.length(self)
torch.distributions.CatTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ComposeTransform(self,parts)
torch.distributions.ComposeTransform.__eq__(self,other)
torch.distributions.ComposeTransform.__repr__(self)
torch.distributions.ComposeTransform.bijective(self)
torch.distributions.ComposeTransform.codomain(self)
torch.distributions.ComposeTransform.domain(self)
torch.distributions.ComposeTransform.event_dim(self)
torch.distributions.ComposeTransform.inv(self)
torch.distributions.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ComposeTransform.sign(self)
torch.distributions.ExpTransform(Transform)
torch.distributions.ExpTransform.__eq__(self,other)
torch.distributions.ExpTransform._call(self,x)
torch.distributions.ExpTransform._inverse(self,y)
torch.distributions.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.LowerCholeskyTransform(Transform)
torch.distributions.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.LowerCholeskyTransform._call(self,x)
torch.distributions.LowerCholeskyTransform._inverse(self,y)
torch.distributions.PowerTransform(self,exponent,cache_size=0)
torch.distributions.PowerTransform.__eq__(self,other)
torch.distributions.PowerTransform._call(self,x)
torch.distributions.PowerTransform._inverse(self,y)
torch.distributions.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SigmoidTransform(Transform)
torch.distributions.SigmoidTransform.__eq__(self,other)
torch.distributions.SigmoidTransform._call(self,x)
torch.distributions.SigmoidTransform._inverse(self,y)
torch.distributions.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SoftmaxTransform(Transform)
torch.distributions.SoftmaxTransform.__eq__(self,other)
torch.distributions.SoftmaxTransform._call(self,x)
torch.distributions.SoftmaxTransform._inverse(self,y)
torch.distributions.StackTransform(self,tseq,dim=0)
torch.distributions.StackTransform._call(self,x)
torch.distributions.StackTransform._inverse(self,y)
torch.distributions.StackTransform._slice(self,z)
torch.distributions.StackTransform.bijective(self)
torch.distributions.StackTransform.codomain(self)
torch.distributions.StackTransform.domain(self)
torch.distributions.StackTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.StickBreakingTransform(Transform)
torch.distributions.StickBreakingTransform.__eq__(self,other)
torch.distributions.StickBreakingTransform._call(self,x)
torch.distributions.StickBreakingTransform._inverse(self,y)
torch.distributions.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.TanhTransform(Transform)
torch.distributions.TanhTransform.__eq__(self,other)
torch.distributions.TanhTransform._call(self,x)
torch.distributions.TanhTransform._inverse(self,y)
torch.distributions.TanhTransform.atanh(x)
torch.distributions.TanhTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform(self,cache_size=0)
torch.distributions.Transform.__eq__(self,other)
torch.distributions.Transform.__ne__(self,other)
torch.distributions.Transform.__repr__(self)
torch.distributions.Transform._call(self,x)
torch.distributions.Transform._inv_call(self,y)
torch.distributions.Transform._inverse(self,y)
torch.distributions.Transform.inv(self)
torch.distributions.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform.sign(self)
torch.distributions._InverseTransform(self,transform)
torch.distributions._InverseTransform.__eq__(self,other)
torch.distributions._InverseTransform.bijective(self)
torch.distributions._InverseTransform.codomain(self)
torch.distributions._InverseTransform.domain(self)
torch.distributions._InverseTransform.event_dim(self)
torch.distributions._InverseTransform.inv(self)
torch.distributions._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions._InverseTransform.sign(self)
torch.distributions._clipped_sigmoid(x)
torch.distributions.transforms.AbsTransform(Transform)
torch.distributions.transforms.AbsTransform.__eq__(self,other)
torch.distributions.transforms.AbsTransform._call(self,x)
torch.distributions.transforms.AbsTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform.__eq__(self,other)
torch.distributions.transforms.AffineTransform.__init__(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform._call(self,x)
torch.distributions.transforms.AffineTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.AffineTransform.sign(self)
torch.distributions.transforms.CatTransform(self,tseq,dim=0,lengths=None)
torch.distributions.transforms.CatTransform.__init__(self,tseq,dim=0,lengths=None)
torch.distributions.transforms.CatTransform._call(self,x)
torch.distributions.transforms.CatTransform._inverse(self,y)
torch.distributions.transforms.CatTransform.bijective(self)
torch.distributions.transforms.CatTransform.codomain(self)
torch.distributions.transforms.CatTransform.domain(self)
torch.distributions.transforms.CatTransform.length(self)
torch.distributions.transforms.CatTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ComposeTransform(self,parts)
torch.distributions.transforms.ComposeTransform.__eq__(self,other)
torch.distributions.transforms.ComposeTransform.__init__(self,parts)
torch.distributions.transforms.ComposeTransform.__repr__(self)
torch.distributions.transforms.ComposeTransform.bijective(self)
torch.distributions.transforms.ComposeTransform.codomain(self)
torch.distributions.transforms.ComposeTransform.domain(self)
torch.distributions.transforms.ComposeTransform.event_dim(self)
torch.distributions.transforms.ComposeTransform.inv(self)
torch.distributions.transforms.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ComposeTransform.sign(self)
torch.distributions.transforms.ExpTransform(Transform)
torch.distributions.transforms.ExpTransform.__eq__(self,other)
torch.distributions.transforms.ExpTransform._call(self,x)
torch.distributions.transforms.ExpTransform._inverse(self,y)
torch.distributions.transforms.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.LowerCholeskyTransform(Transform)
torch.distributions.transforms.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.transforms.LowerCholeskyTransform._call(self,x)
torch.distributions.transforms.LowerCholeskyTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform.__eq__(self,other)
torch.distributions.transforms.PowerTransform.__init__(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform._call(self,x)
torch.distributions.transforms.PowerTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SigmoidTransform(Transform)
torch.distributions.transforms.SigmoidTransform.__eq__(self,other)
torch.distributions.transforms.SigmoidTransform._call(self,x)
torch.distributions.transforms.SigmoidTransform._inverse(self,y)
torch.distributions.transforms.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SoftmaxTransform(Transform)
torch.distributions.transforms.SoftmaxTransform.__eq__(self,other)
torch.distributions.transforms.SoftmaxTransform._call(self,x)
torch.distributions.transforms.SoftmaxTransform._inverse(self,y)
torch.distributions.transforms.StackTransform(self,tseq,dim=0)
torch.distributions.transforms.StackTransform.__init__(self,tseq,dim=0)
torch.distributions.transforms.StackTransform._call(self,x)
torch.distributions.transforms.StackTransform._inverse(self,y)
torch.distributions.transforms.StackTransform._slice(self,z)
torch.distributions.transforms.StackTransform.bijective(self)
torch.distributions.transforms.StackTransform.codomain(self)
torch.distributions.transforms.StackTransform.domain(self)
torch.distributions.transforms.StackTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.StickBreakingTransform(Transform)
torch.distributions.transforms.StickBreakingTransform.__eq__(self,other)
torch.distributions.transforms.StickBreakingTransform._call(self,x)
torch.distributions.transforms.StickBreakingTransform._inverse(self,y)
torch.distributions.transforms.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.TanhTransform(Transform)
torch.distributions.transforms.TanhTransform.__eq__(self,other)
torch.distributions.transforms.TanhTransform._call(self,x)
torch.distributions.transforms.TanhTransform._inverse(self,y)
torch.distributions.transforms.TanhTransform.atanh(x)
torch.distributions.transforms.TanhTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform(self,cache_size=0)
torch.distributions.transforms.Transform.__eq__(self,other)
torch.distributions.transforms.Transform.__init__(self,cache_size=0)
torch.distributions.transforms.Transform.__ne__(self,other)
torch.distributions.transforms.Transform.__repr__(self)
torch.distributions.transforms.Transform._call(self,x)
torch.distributions.transforms.Transform._inv_call(self,y)
torch.distributions.transforms.Transform._inverse(self,y)
torch.distributions.transforms.Transform.inv(self)
torch.distributions.transforms.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform.sign(self)
torch.distributions.transforms._InverseTransform(self,transform)
torch.distributions.transforms._InverseTransform.__eq__(self,other)
torch.distributions.transforms._InverseTransform.__init__(self,transform)
torch.distributions.transforms._InverseTransform.bijective(self)
torch.distributions.transforms._InverseTransform.codomain(self)
torch.distributions.transforms._InverseTransform.domain(self)
torch.distributions.transforms._InverseTransform.event_dim(self)
torch.distributions.transforms._InverseTransform.inv(self)
torch.distributions.transforms._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms._InverseTransform.sign(self)
torch.distributions.transforms._clipped_sigmoid(x)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/constraint_registry.py----------------------------------------
A:torch.distributions.constraint_registry.constraint->type(constraint)
A:torch.distributions.constraint_registry.biject_to->ConstraintRegistry()
A:torch.distributions.constraint_registry.transform_to->ConstraintRegistry()
torch.distributions.constraint_registry.ConstraintRegistry(self)
torch.distributions.constraint_registry.ConstraintRegistry.__init__(self)
torch.distributions.constraint_registry.ConstraintRegistry.register(self,constraint,factory=None)
torch.distributions.constraint_registry._biject_to_cat(constraint)
torch.distributions.constraint_registry._biject_to_simplex(constraint)
torch.distributions.constraint_registry._biject_to_stack(constraint)
torch.distributions.constraint_registry._transform_to_cat(constraint)
torch.distributions.constraint_registry._transform_to_greater_than(constraint)
torch.distributions.constraint_registry._transform_to_interval(constraint)
torch.distributions.constraint_registry._transform_to_less_than(constraint)
torch.distributions.constraint_registry._transform_to_lower_cholesky(constraint)
torch.distributions.constraint_registry._transform_to_positive(constraint)
torch.distributions.constraint_registry._transform_to_real(constraint)
torch.distributions.constraint_registry._transform_to_simplex(constraint)
torch.distributions.constraint_registry._transform_to_stack(constraint)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/relaxed_categorical.py----------------------------------------
A:torch.distributions.relaxed_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.relaxed_categorical.new->self._get_checked_instance(RelaxedOneHotCategorical, _instance)
A:torch.distributions.relaxed_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.relaxed_categorical.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_categorical.uniforms->clamp_probs(torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device))
A:torch.distributions.relaxed_categorical.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_categorical.score->(score - score.logsumexp(dim=-1, keepdim=True)).sum(-1)
A:torch.distributions.relaxed_categorical.base_dist->ExpRelaxedCategorical(temperature, probs, logits)
torch.distributions.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedOneHotCategorical.logits(self)
torch.distributions.RelaxedOneHotCategorical.probs(self)
torch.distributions.RelaxedOneHotCategorical.temperature(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical._new(self,*args,**kwargs)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.log_prob(self,value)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.logits(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.param_shape(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.probs(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/von_mises.py----------------------------------------
A:torch.distributions.von_mises.coef->list(coef)
A:torch.distributions.von_mises.result->torch.where(x < 3.75, small, large)
A:torch.distributions.von_mises.small->small.log().log()
A:torch.distributions.von_mises.done->torch.zeros(x.shape, dtype=torch.bool, device=loc.device)
A:torch.distributions.von_mises.u->torch.rand((3,) + x.shape, dtype=loc.dtype, device=loc.device)
A:torch.distributions.von_mises.(u1, u2, u3)->torch.rand((3,) + x.shape, dtype=loc.dtype, device=loc.device).unbind()
A:torch.distributions.von_mises.z->torch.cos(math.pi * u1)
A:torch.distributions.von_mises.x->torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.von_mises.(self.loc, self.concentration)->broadcast_all(loc, concentration)
A:torch.distributions.von_mises.event_shape->torch.Size()
A:torch.distributions.von_mises.shape->self._extended_shape(sample_shape)
A:torch.distributions.von_mises.validate_args->self.__dict__.get('_validate_args')
A:torch.distributions.von_mises.loc->self.loc.expand(batch_shape)
A:torch.distributions.von_mises.concentration->self.concentration.expand(batch_shape)
torch.distributions.VonMises(self,loc,concentration,validate_args=None)
torch.distributions.VonMises.expand(self,batch_shape)
torch.distributions.VonMises.log_prob(self,value)
torch.distributions.VonMises.mean(self)
torch.distributions.VonMises.sample(self,sample_shape=torch.Size())
torch.distributions.VonMises.variance(self)
torch.distributions.von_mises.VonMises(self,loc,concentration,validate_args=None)
torch.distributions.von_mises.VonMises.__init__(self,loc,concentration,validate_args=None)
torch.distributions.von_mises.VonMises.expand(self,batch_shape)
torch.distributions.von_mises.VonMises.log_prob(self,value)
torch.distributions.von_mises.VonMises.mean(self)
torch.distributions.von_mises.VonMises.sample(self,sample_shape=torch.Size())
torch.distributions.von_mises.VonMises.variance(self)
torch.distributions.von_mises._eval_poly(y,coef)
torch.distributions.von_mises._log_modified_bessel_fn(x,order=0)
torch.distributions.von_mises._rejection_sample(loc,concentration,proposal_r,x)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/normal.py----------------------------------------
A:torch.distributions.normal.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.normal.new->self._get_checked_instance(Normal, _instance)
A:torch.distributions.normal.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.normal.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
torch.distributions.Normal(self,loc,scale,validate_args=None)
torch.distributions.Normal._log_normalizer(self,x,y)
torch.distributions.Normal._natural_params(self)
torch.distributions.Normal.cdf(self,value)
torch.distributions.Normal.entropy(self)
torch.distributions.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.Normal.icdf(self,value)
torch.distributions.Normal.log_prob(self,value)
torch.distributions.Normal.mean(self)
torch.distributions.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.Normal.stddev(self)
torch.distributions.Normal.variance(self)
torch.distributions.normal.Normal(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal.__init__(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal._log_normalizer(self,x,y)
torch.distributions.normal.Normal._natural_params(self)
torch.distributions.normal.Normal.cdf(self,value)
torch.distributions.normal.Normal.entropy(self)
torch.distributions.normal.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.normal.Normal.icdf(self,value)
torch.distributions.normal.Normal.log_prob(self,value)
torch.distributions.normal.Normal.mean(self)
torch.distributions.normal.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.stddev(self)
torch.distributions.normal.Normal.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/multinomial.py----------------------------------------
A:torch.distributions.multinomial.self._categorical->Categorical(probs=probs, logits=logits)
A:torch.distributions.multinomial.new->self._get_checked_instance(Multinomial, _instance)
A:torch.distributions.multinomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multinomial.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.multinomial.sample_shape->torch.Size(sample_shape)
A:torch.distributions.multinomial.samples->samples.permute(*shifted_idx).permute(*shifted_idx)
A:torch.distributions.multinomial.shifted_idx->list(range(samples.dim()))
A:torch.distributions.multinomial.counts->samples.permute(*shifted_idx).permute(*shifted_idx).new(self._extended_shape(sample_shape)).zero_()
A:torch.distributions.multinomial.(logits, value)->broadcast_all(self.logits.clone(memory_format=torch.contiguous_format), value)
A:torch.distributions.multinomial.log_factorial_n->torch.lgamma(value.sum(-1) + 1)
A:torch.distributions.multinomial.log_factorial_xs->torch.lgamma(value + 1).sum(-1)
A:torch.distributions.multinomial.log_powers->(logits * value).sum(-1)
torch.distributions.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Multinomial._new(self,*args,**kwargs)
torch.distributions.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.Multinomial.log_prob(self,value)
torch.distributions.Multinomial.logits(self)
torch.distributions.Multinomial.mean(self)
torch.distributions.Multinomial.param_shape(self)
torch.distributions.Multinomial.probs(self)
torch.distributions.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.Multinomial.support(self)
torch.distributions.Multinomial.variance(self)
torch.distributions.multinomial.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial._new(self,*args,**kwargs)
torch.distributions.multinomial.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.multinomial.Multinomial.log_prob(self,value)
torch.distributions.multinomial.Multinomial.logits(self)
torch.distributions.multinomial.Multinomial.mean(self)
torch.distributions.multinomial.Multinomial.param_shape(self)
torch.distributions.multinomial.Multinomial.probs(self)
torch.distributions.multinomial.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.multinomial.Multinomial.support(self)
torch.distributions.multinomial.Multinomial.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/negative_binomial.py----------------------------------------
A:torch.distributions.negative_binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.negative_binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.negative_binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.negative_binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.negative_binomial.new->self._get_checked_instance(NegativeBinomial, _instance)
A:torch.distributions.negative_binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.negative_binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.negative_binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.negative_binomial.rate->self._gamma.sample(sample_shape=sample_shape)
torch.distributions.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.NegativeBinomial._gamma(self)
torch.distributions.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.NegativeBinomial.log_prob(self,value)
torch.distributions.NegativeBinomial.logits(self)
torch.distributions.NegativeBinomial.mean(self)
torch.distributions.NegativeBinomial.param_shape(self)
torch.distributions.NegativeBinomial.probs(self)
torch.distributions.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.NegativeBinomial.variance(self)
torch.distributions.negative_binomial.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial.__init__(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial._gamma(self)
torch.distributions.negative_binomial.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.negative_binomial.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.negative_binomial.NegativeBinomial.log_prob(self,value)
torch.distributions.negative_binomial.NegativeBinomial.logits(self)
torch.distributions.negative_binomial.NegativeBinomial.mean(self)
torch.distributions.negative_binomial.NegativeBinomial.param_shape(self)
torch.distributions.negative_binomial.NegativeBinomial.probs(self)
torch.distributions.negative_binomial.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.negative_binomial.NegativeBinomial.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/chi2.py----------------------------------------
A:torch.distributions.chi2.new->self._get_checked_instance(Chi2, _instance)
torch.distributions.Chi2(self,df,validate_args=None)
torch.distributions.Chi2.df(self)
torch.distributions.Chi2.expand(self,batch_shape,_instance=None)
torch.distributions.chi2.Chi2(self,df,validate_args=None)
torch.distributions.chi2.Chi2.__init__(self,df,validate_args=None)
torch.distributions.chi2.Chi2.df(self)
torch.distributions.chi2.Chi2.expand(self,batch_shape,_instance=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/kl.py----------------------------------------
A:torch.distributions.kl.n->bmat.size(-1)
A:torch.distributions.kl.m->bmat.size(-2)
A:torch.distributions.kl.flat_trace->bmat.reshape(-1, m * n).pow(2).sum(-1)
A:torch.distributions.kl.fun->_dispatch_kl(type(p), type(q))
A:torch.distributions.kl.kl[inf_idxs]->_infinite_like(kl[inf_idxs])
A:torch.distributions.kl.sum_p_concentration->p.concentration.sum(-1)
A:torch.distributions.kl.sum_q_concentration->q.concentration.sum(-1)
A:torch.distributions.kl.t2->p.alpha.reciprocal()
A:torch.distributions.kl.lg_normal->p._log_normalizer(*p_nparams)
A:torch.distributions.kl.gradients->torch.autograd.grad(lg_normal.sum(), p_nparams, create_graph=True)
A:torch.distributions.kl.t3->((p.high + p.low - 2 * q.loc) / 2).pow(2)
A:torch.distributions.kl.loc_abs_diff->(p.loc - q.loc).abs()
A:torch.distributions.kl.term3->_batch_mahalanobis(q._unbroadcasted_scale_tril, q.loc - p.loc)
A:torch.distributions.kl.term21->_batch_trace_XXT(torch.triangular_solve(p_cov_factor, q_scale_tril, upper=False)[0])
A:torch.distributions.kl.term22->_batch_trace_XXT(torch.triangular_solve(p_cov_diag, q_scale_tril, upper=False)[0])
A:torch.distributions.kl.term23->_batch_trace_XXT(A * p._unbroadcasted_cov_diag.sqrt().unsqueeze(-2))
A:torch.distributions.kl.term24->_batch_trace_XXT(A.matmul(p._unbroadcasted_cov_factor))
A:torch.distributions.kl.combined_batch_shape->torch._C._infer_size(q._unbroadcasted_scale_tril.shape[:-2], p._unbroadcasted_scale_tril.shape[:-2])
A:torch.distributions.kl.q_scale_tril->q._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_cov_factor->p._unbroadcasted_cov_factor.expand(combined_batch_shape + (n, p.cov_factor.size(-1)))
A:torch.distributions.kl.p_cov_diag->torch.diag_embed(p._unbroadcasted_cov_diag.sqrt()).expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_scale_tril->p._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.term2->_batch_trace_XXT(torch.triangular_solve(p_scale_tril, q_scale_tril, upper=False)[0])
A:torch.distributions.kl.var_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t1->(q.alpha * q.scale.pow(q.alpha) * support_uniform).log()
A:torch.distributions.kl.extra_event_dim->len(p.event_shape)
A:torch.distributions.kl.base_kl_divergence->kl_divergence(p.base_dist, q.base_dist)
A:torch.distributions.kl.result->kl_divergence(p.base_dist, q.base_dist)
A:torch.distributions.kl.var_normal->q.scale.pow(2)
A:torch.distributions.kl.rate_sqr->p.rate.pow(2)
A:torch.distributions.kl.beta_sqr->p.rate.pow(2)
A:torch.distributions.kl.var_scale_sqr_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t4->(p.alpha * common_term - q.loc).pow(2)
torch.distributions.kl._Match(self,*types)
torch.distributions.kl._Match.__eq__(self,other)
torch.distributions.kl._Match.__init__(self,*types)
torch.distributions.kl._Match.__le__(self,other)
torch.distributions.kl._batch_trace_XXT(bmat)
torch.distributions.kl._dispatch_kl(type_p,type_q)
torch.distributions.kl._infinite_like(tensor)
torch.distributions.kl._kl_bernoulli_bernoulli(p,q)
torch.distributions.kl._kl_bernoulli_poisson(p,q)
torch.distributions.kl._kl_beta_beta(p,q)
torch.distributions.kl._kl_beta_continuous_bernoulli(p,q)
torch.distributions.kl._kl_beta_exponential(p,q)
torch.distributions.kl._kl_beta_gamma(p,q)
torch.distributions.kl._kl_beta_infinity(p,q)
torch.distributions.kl._kl_beta_normal(p,q)
torch.distributions.kl._kl_beta_uniform(p,q)
torch.distributions.kl._kl_binomial_binomial(p,q)
torch.distributions.kl._kl_categorical_categorical(p,q)
torch.distributions.kl._kl_continuous_bernoulli_continuous_bernoulli(p,q)
torch.distributions.kl._kl_continuous_bernoulli_exponential(p,q)
torch.distributions.kl._kl_continuous_bernoulli_infinity(p,q)
torch.distributions.kl._kl_continuous_bernoulli_normal(p,q)
torch.distributions.kl._kl_continuous_bernoulli_uniform(p,q)
torch.distributions.kl._kl_dirichlet_dirichlet(p,q)
torch.distributions.kl._kl_expfamily_expfamily(p,q)
torch.distributions.kl._kl_exponential_exponential(p,q)
torch.distributions.kl._kl_exponential_gamma(p,q)
torch.distributions.kl._kl_exponential_gumbel(p,q)
torch.distributions.kl._kl_exponential_infinity(p,q)
torch.distributions.kl._kl_exponential_normal(p,q)
torch.distributions.kl._kl_gamma_exponential(p,q)
torch.distributions.kl._kl_gamma_gamma(p,q)
torch.distributions.kl._kl_gamma_gumbel(p,q)
torch.distributions.kl._kl_gamma_infinity(p,q)
torch.distributions.kl._kl_gamma_normal(p,q)
torch.distributions.kl._kl_geometric_geometric(p,q)
torch.distributions.kl._kl_gumbel_gumbel(p,q)
torch.distributions.kl._kl_gumbel_infinity(p,q)
torch.distributions.kl._kl_gumbel_normal(p,q)
torch.distributions.kl._kl_halfnormal_halfnormal(p,q)
torch.distributions.kl._kl_independent_independent(p,q)
torch.distributions.kl._kl_laplace_infinity(p,q)
torch.distributions.kl._kl_laplace_laplace(p,q)
torch.distributions.kl._kl_laplace_normal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_normal_gumbel(p,q)
torch.distributions.kl._kl_normal_infinity(p,q)
torch.distributions.kl._kl_normal_normal(p,q)
torch.distributions.kl._kl_onehotcategorical_onehotcategorical(p,q)
torch.distributions.kl._kl_pareto_exponential(p,q)
torch.distributions.kl._kl_pareto_gamma(p,q)
torch.distributions.kl._kl_pareto_infinity(p,q)
torch.distributions.kl._kl_pareto_normal(p,q)
torch.distributions.kl._kl_pareto_pareto(p,q)
torch.distributions.kl._kl_poisson_infinity(p,q)
torch.distributions.kl._kl_poisson_poisson(p,q)
torch.distributions.kl._kl_transformed_transformed(p,q)
torch.distributions.kl._kl_uniform_beta(p,q)
torch.distributions.kl._kl_uniform_continuous_bernoulli(p,q)
torch.distributions.kl._kl_uniform_exponetial(p,q)
torch.distributions.kl._kl_uniform_gamma(p,q)
torch.distributions.kl._kl_uniform_gumbel(p,q)
torch.distributions.kl._kl_uniform_normal(p,q)
torch.distributions.kl._kl_uniform_pareto(p,q)
torch.distributions.kl._kl_uniform_uniform(p,q)
torch.distributions.kl._x_log_x(tensor)
torch.distributions.kl.kl_divergence(p,q)
torch.distributions.kl.register_kl(type_p,type_q)
torch.distributions.kl_divergence(p,q)
torch.distributions.register_kl(type_p,type_q)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/bernoulli.py----------------------------------------
A:torch.distributions.bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.bernoulli.new->self._get_checked_instance(Bernoulli, _instance)
A:torch.distributions.bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.bernoulli.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.Bernoulli._log_normalizer(self,x)
torch.distributions.Bernoulli._natural_params(self)
torch.distributions.Bernoulli._new(self,*args,**kwargs)
torch.distributions.Bernoulli.entropy(self)
torch.distributions.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.Bernoulli.log_prob(self,value)
torch.distributions.Bernoulli.logits(self)
torch.distributions.Bernoulli.mean(self)
torch.distributions.Bernoulli.param_shape(self)
torch.distributions.Bernoulli.probs(self)
torch.distributions.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.Bernoulli.variance(self)
torch.distributions.bernoulli.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli._log_normalizer(self,x)
torch.distributions.bernoulli.Bernoulli._natural_params(self)
torch.distributions.bernoulli.Bernoulli._new(self,*args,**kwargs)
torch.distributions.bernoulli.Bernoulli.entropy(self)
torch.distributions.bernoulli.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.bernoulli.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.bernoulli.Bernoulli.log_prob(self,value)
torch.distributions.bernoulli.Bernoulli.logits(self)
torch.distributions.bernoulli.Bernoulli.mean(self)
torch.distributions.bernoulli.Bernoulli.param_shape(self)
torch.distributions.bernoulli.Bernoulli.probs(self)
torch.distributions.bernoulli.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.bernoulli.Bernoulli.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/gumbel.py----------------------------------------
A:torch.distributions.gumbel.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.gumbel.finfo->torch.finfo(self.loc.dtype)
A:torch.distributions.gumbel.base_dist->Uniform(torch.full_like(self.loc, finfo.tiny), torch.full_like(self.loc, 1 - finfo.eps))
A:torch.distributions.gumbel.new->self._get_checked_instance(Gumbel, _instance)
A:torch.distributions.gumbel.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.gumbel.new.scale->self.scale.expand(batch_shape)
torch.distributions.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.Gumbel.entropy(self)
torch.distributions.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.Gumbel.log_prob(self,value)
torch.distributions.Gumbel.mean(self)
torch.distributions.Gumbel.stddev(self)
torch.distributions.Gumbel.variance(self)
torch.distributions.gumbel.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.__init__(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.entropy(self)
torch.distributions.gumbel.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.gumbel.Gumbel.log_prob(self,value)
torch.distributions.gumbel.Gumbel.mean(self)
torch.distributions.gumbel.Gumbel.stddev(self)
torch.distributions.gumbel.Gumbel.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/dirichlet.py----------------------------------------
A:torch.distributions.dirichlet.total->self.concentration.expand(shape).sum(-1, True).expand_as(concentration)
A:torch.distributions.dirichlet.grad->torch._dirichlet_grad(x, concentration, total)
A:torch.distributions.dirichlet.x->torch._sample_dirichlet(concentration)
A:torch.distributions.dirichlet.new->self._get_checked_instance(Dirichlet, _instance)
A:torch.distributions.dirichlet.batch_shape->torch.Size(batch_shape)
A:torch.distributions.dirichlet.new.concentration->self.concentration.expand(batch_shape + self.event_shape)
A:torch.distributions.dirichlet.shape->self._extended_shape(sample_shape)
A:torch.distributions.dirichlet.concentration->self.concentration.expand(shape)
A:torch.distributions.dirichlet.con0->self.concentration.sum(-1, True)
A:torch.distributions.dirichlet.k->self.concentration.size(-1)
A:torch.distributions.dirichlet.a0->self.concentration.sum(-1)
torch.distributions.Dirichlet(self,concentration,validate_args=None)
torch.distributions.Dirichlet._log_normalizer(self,x)
torch.distributions.Dirichlet._natural_params(self)
torch.distributions.Dirichlet.entropy(self)
torch.distributions.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.Dirichlet.log_prob(self,value)
torch.distributions.Dirichlet.mean(self)
torch.distributions.Dirichlet.rsample(self,sample_shape=())
torch.distributions.Dirichlet.variance(self)
torch.distributions.dirichlet.Dirichlet(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet.__init__(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet._log_normalizer(self,x)
torch.distributions.dirichlet.Dirichlet._natural_params(self)
torch.distributions.dirichlet.Dirichlet.entropy(self)
torch.distributions.dirichlet.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.dirichlet.Dirichlet.log_prob(self,value)
torch.distributions.dirichlet.Dirichlet.mean(self)
torch.distributions.dirichlet.Dirichlet.rsample(self,sample_shape=())
torch.distributions.dirichlet.Dirichlet.variance(self)
torch.distributions.dirichlet._Dirichlet(Function)
torch.distributions.dirichlet._Dirichlet.backward(ctx,grad_output)
torch.distributions.dirichlet._Dirichlet.forward(ctx,concentration)
torch.distributions.dirichlet._Dirichlet_backward(x,concentration,grad_output)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/relaxed_bernoulli.py----------------------------------------
A:torch.distributions.relaxed_bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.relaxed_bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.relaxed_bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.relaxed_bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_bernoulli.new->self._get_checked_instance(RelaxedBernoulli, _instance)
A:torch.distributions.relaxed_bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_bernoulli.probs->clamp_probs(self.probs.expand(shape))
A:torch.distributions.relaxed_bernoulli.uniforms->clamp_probs(torch.rand(shape, dtype=probs.dtype, device=probs.device))
A:torch.distributions.relaxed_bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_bernoulli.base_dist->LogitRelaxedBernoulli(temperature, probs, logits)
torch.distributions.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedBernoulli.logits(self)
torch.distributions.RelaxedBernoulli.probs(self)
torch.distributions.RelaxedBernoulli.temperature(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli._new(self,*args,**kwargs)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob(self,value)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_bernoulli.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/independent.py----------------------------------------
A:torch.distributions.independent.new->self._get_checked_instance(Independent, _instance)
A:torch.distributions.independent.batch_shape->torch.Size(batch_shape)
A:torch.distributions.independent.new.base_dist->self.base_dist.expand(batch_shape + self.event_shape[:self.reinterpreted_batch_ndims])
A:torch.distributions.independent.log_prob->self.base_dist.log_prob(value)
A:torch.distributions.independent.entropy->self.base_dist.entropy()
torch.distributions.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.Independent.__repr__(self)
torch.distributions.Independent.entropy(self)
torch.distributions.Independent.enumerate_support(self,expand=True)
torch.distributions.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.Independent.has_enumerate_support(self)
torch.distributions.Independent.has_rsample(self)
torch.distributions.Independent.log_prob(self,value)
torch.distributions.Independent.mean(self)
torch.distributions.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.Independent.support(self)
torch.distributions.Independent.variance(self)
torch.distributions.independent.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.__init__(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.__repr__(self)
torch.distributions.independent.Independent.entropy(self)
torch.distributions.independent.Independent.enumerate_support(self,expand=True)
torch.distributions.independent.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.independent.Independent.has_enumerate_support(self)
torch.distributions.independent.Independent.has_rsample(self)
torch.distributions.independent.Independent.log_prob(self,value)
torch.distributions.independent.Independent.mean(self)
torch.distributions.independent.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.support(self)
torch.distributions.independent.Independent.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/uniform.py----------------------------------------
A:torch.distributions.uniform.(self.low, self.high)->broadcast_all(low, high)
A:torch.distributions.uniform.batch_shape->torch.Size(batch_shape)
A:torch.distributions.uniform.new->self._get_checked_instance(Uniform, _instance)
A:torch.distributions.uniform.new.low->self.low.expand(batch_shape)
A:torch.distributions.uniform.new.high->self.high.expand(batch_shape)
A:torch.distributions.uniform.shape->self._extended_shape(sample_shape)
A:torch.distributions.uniform.rand->torch.rand(shape, dtype=self.low.dtype, device=self.low.device)
A:torch.distributions.uniform.lb->self.low.le(value).type_as(self.low)
A:torch.distributions.uniform.ub->self.high.gt(value).type_as(self.low)
torch.distributions.Uniform(self,low,high,validate_args=None)
torch.distributions.Uniform.cdf(self,value)
torch.distributions.Uniform.entropy(self)
torch.distributions.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.Uniform.icdf(self,value)
torch.distributions.Uniform.log_prob(self,value)
torch.distributions.Uniform.mean(self)
torch.distributions.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.Uniform.stddev(self)
torch.distributions.Uniform.support(self)
torch.distributions.Uniform.variance(self)
torch.distributions.uniform.Uniform(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.__init__(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.cdf(self,value)
torch.distributions.uniform.Uniform.entropy(self)
torch.distributions.uniform.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.uniform.Uniform.icdf(self,value)
torch.distributions.uniform.Uniform.log_prob(self,value)
torch.distributions.uniform.Uniform.mean(self)
torch.distributions.uniform.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.uniform.Uniform.stddev(self)
torch.distributions.uniform.Uniform.support(self)
torch.distributions.uniform.Uniform.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/categorical.py----------------------------------------
A:torch.distributions.categorical.new->self._get_checked_instance(Categorical, _instance)
A:torch.distributions.categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.categorical.new.probs->self.probs.expand(param_shape)
A:torch.distributions.categorical.new.logits->self.logits.expand(param_shape)
A:torch.distributions.categorical.sample_shape->torch.Size(sample_shape)
A:torch.distributions.categorical.probs_2d->self.probs.reshape(-1, self._num_events)
A:torch.distributions.categorical.value->value.long().unsqueeze(-1).long().unsqueeze(-1)
A:torch.distributions.categorical.(value, log_pmf)->torch.broadcast_tensors(value, self.logits)
A:torch.distributions.categorical.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.Categorical._new(self,*args,**kwargs)
torch.distributions.Categorical.entropy(self)
torch.distributions.Categorical.enumerate_support(self,expand=True)
torch.distributions.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.Categorical.log_prob(self,value)
torch.distributions.Categorical.logits(self)
torch.distributions.Categorical.mean(self)
torch.distributions.Categorical.param_shape(self)
torch.distributions.Categorical.probs(self)
torch.distributions.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.Categorical.support(self)
torch.distributions.Categorical.variance(self)
torch.distributions.categorical.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical._new(self,*args,**kwargs)
torch.distributions.categorical.Categorical.entropy(self)
torch.distributions.categorical.Categorical.enumerate_support(self,expand=True)
torch.distributions.categorical.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.categorical.Categorical.log_prob(self,value)
torch.distributions.categorical.Categorical.logits(self)
torch.distributions.categorical.Categorical.mean(self)
torch.distributions.categorical.Categorical.param_shape(self)
torch.distributions.categorical.Categorical.probs(self)
torch.distributions.categorical.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.categorical.Categorical.support(self)
torch.distributions.categorical.Categorical.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/continuous_bernoulli.py----------------------------------------
A:torch.distributions.continuous_bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.continuous_bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.continuous_bernoulli.self.probs->clamp_probs(self.probs)
A:torch.distributions.continuous_bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.continuous_bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.continuous_bernoulli.new->self._get_checked_instance(ContinuousBernoulli, _instance)
A:torch.distributions.continuous_bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.continuous_bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.continuous_bernoulli.cut_probs->self._cut_probs()
A:torch.distributions.continuous_bernoulli.cut_probs_below_half->torch.where(torch.le(cut_probs, 0.5), cut_probs, torch.zeros_like(cut_probs))
A:torch.distributions.continuous_bernoulli.cut_probs_above_half->torch.where(torch.ge(cut_probs, 0.5), cut_probs, torch.ones_like(cut_probs))
A:torch.distributions.continuous_bernoulli.x->torch.pow(self.probs - 0.5, 2)
A:torch.distributions.continuous_bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.continuous_bernoulli.u->torch.rand(shape, dtype=self.probs.dtype, device=self.probs.device)
A:torch.distributions.continuous_bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.continuous_bernoulli.unbounded_cdfs->torch.where(self._outside_unstable_region(), cdfs, value)
A:torch.distributions.continuous_bernoulli.log_probs0->torch.log1p(-self.probs)
A:torch.distributions.continuous_bernoulli.log_probs1->torch.log(self.probs)
A:torch.distributions.continuous_bernoulli.out_unst_reg->torch.max(torch.le(x, self._lims[0] - 0.5), torch.gt(x, self._lims[1] - 0.5))
A:torch.distributions.continuous_bernoulli.cut_nat_params->torch.where(out_unst_reg, x, (self._lims[0] - 0.5) * torch.ones_like(x))
torch.distributions.ContinuousBernoulli(self,probs=None,logits=None,lims=(0.499,0.501),validate_args=None)
torch.distributions.ContinuousBernoulli._cont_bern_log_norm(self)
torch.distributions.ContinuousBernoulli._cut_probs(self)
torch.distributions.ContinuousBernoulli._log_normalizer(self,x)
torch.distributions.ContinuousBernoulli._natural_params(self)
torch.distributions.ContinuousBernoulli._new(self,*args,**kwargs)
torch.distributions.ContinuousBernoulli._outside_unstable_region(self)
torch.distributions.ContinuousBernoulli.cdf(self,value)
torch.distributions.ContinuousBernoulli.entropy(self)
torch.distributions.ContinuousBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.ContinuousBernoulli.icdf(self,value)
torch.distributions.ContinuousBernoulli.log_prob(self,value)
torch.distributions.ContinuousBernoulli.logits(self)
torch.distributions.ContinuousBernoulli.mean(self)
torch.distributions.ContinuousBernoulli.param_shape(self)
torch.distributions.ContinuousBernoulli.probs(self)
torch.distributions.ContinuousBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.ContinuousBernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.ContinuousBernoulli.stddev(self)
torch.distributions.ContinuousBernoulli.variance(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli(self,probs=None,logits=None,lims=(0.499,0.501),validate_args=None)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.__init__(self,probs=None,logits=None,lims=(0.499,0.501),validate_args=None)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._cont_bern_log_norm(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._cut_probs(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._log_normalizer(self,x)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._natural_params(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._new(self,*args,**kwargs)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._outside_unstable_region(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf(self,value)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf(self,value)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob(self,value)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/studentT.py----------------------------------------
A:torch.distributions.studentT.m->self.df.clone(memory_format=torch.contiguous_format)
A:torch.distributions.studentT.(self.df, self.loc, self.scale)->broadcast_all(df, loc, scale)
A:torch.distributions.studentT.self._chi2->Chi2(self.df)
A:torch.distributions.studentT.batch_shape->torch.Size(batch_shape)
A:torch.distributions.studentT.new->self._get_checked_instance(StudentT, _instance)
A:torch.distributions.studentT.new.df->self.df.expand(batch_shape)
A:torch.distributions.studentT.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.studentT.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.studentT.new._chi2->self._chi2.expand(batch_shape)
A:torch.distributions.studentT.shape->self._extended_shape(sample_shape)
A:torch.distributions.studentT.X->_standard_normal(shape, dtype=self.df.dtype, device=self.df.device)
A:torch.distributions.studentT.Z->self._chi2.rsample(sample_shape)
torch.distributions.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.StudentT.entropy(self)
torch.distributions.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.StudentT.log_prob(self,value)
torch.distributions.StudentT.mean(self)
torch.distributions.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.StudentT.variance(self)
torch.distributions.studentT.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.__init__(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.entropy(self)
torch.distributions.studentT.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.studentT.StudentT.log_prob(self,value)
torch.distributions.studentT.StudentT.mean(self)
torch.distributions.studentT.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.studentT.StudentT.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/binomial.py----------------------------------------
A:torch.distributions.binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.binomial.is_scalar->isinstance(self.logits, Number)
A:torch.distributions.binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.binomial.new->self._get_checked_instance(Binomial, _instance)
A:torch.distributions.binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.binomial.max_count->max(int(self.total_count.max()), 1)
A:torch.distributions.binomial.bernoullis->torch.bernoulli(self.probs.unsqueeze(-1).expand(shape))
A:torch.distributions.binomial.arange->torch.arange(max_count, dtype=self._param.dtype, device=self._param.device)
A:torch.distributions.binomial.log_factorial_n->torch.lgamma(self.total_count + 1)
A:torch.distributions.binomial.log_factorial_k->torch.lgamma(value + 1)
A:torch.distributions.binomial.log_factorial_nmk->torch.lgamma(self.total_count - value + 1)
A:torch.distributions.binomial.total_count->int(self.total_count.max())
A:torch.distributions.binomial.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Binomial._new(self,*args,**kwargs)
torch.distributions.Binomial.enumerate_support(self,expand=True)
torch.distributions.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.Binomial.log_prob(self,value)
torch.distributions.Binomial.logits(self)
torch.distributions.Binomial.mean(self)
torch.distributions.Binomial.param_shape(self)
torch.distributions.Binomial.probs(self)
torch.distributions.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.Binomial.support(self)
torch.distributions.Binomial.variance(self)
torch.distributions.binomial.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial._new(self,*args,**kwargs)
torch.distributions.binomial.Binomial.enumerate_support(self,expand=True)
torch.distributions.binomial.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.binomial.Binomial.log_prob(self,value)
torch.distributions.binomial.Binomial.logits(self)
torch.distributions.binomial.Binomial.mean(self)
torch.distributions.binomial.Binomial.param_shape(self)
torch.distributions.binomial.Binomial.probs(self)
torch.distributions.binomial.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.binomial.Binomial.support(self)
torch.distributions.binomial.Binomial.variance(self)
torch.distributions.binomial._clamp_by_zero(x)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/half_cauchy.py----------------------------------------
A:torch.distributions.half_cauchy.base_dist->Cauchy(0, scale)
A:torch.distributions.half_cauchy.new->self._get_checked_instance(HalfCauchy, _instance)
A:torch.distributions.half_cauchy.value->torch.as_tensor(value, dtype=self.base_dist.scale.dtype, device=self.base_dist.scale.device)
torch.distributions.HalfCauchy(self,scale,validate_args=None)
torch.distributions.HalfCauchy.cdf(self,value)
torch.distributions.HalfCauchy.entropy(self)
torch.distributions.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.HalfCauchy.icdf(self,prob)
torch.distributions.HalfCauchy.log_prob(self,value)
torch.distributions.HalfCauchy.mean(self)
torch.distributions.HalfCauchy.scale(self)
torch.distributions.HalfCauchy.variance(self)
torch.distributions.half_cauchy.HalfCauchy(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.__init__(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.cdf(self,value)
torch.distributions.half_cauchy.HalfCauchy.entropy(self)
torch.distributions.half_cauchy.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.half_cauchy.HalfCauchy.icdf(self,prob)
torch.distributions.half_cauchy.HalfCauchy.log_prob(self,value)
torch.distributions.half_cauchy.HalfCauchy.mean(self)
torch.distributions.half_cauchy.HalfCauchy.scale(self)
torch.distributions.half_cauchy.HalfCauchy.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/exponential.py----------------------------------------
A:torch.distributions.exponential.(self.rate,)->broadcast_all(rate)
A:torch.distributions.exponential.new->self._get_checked_instance(Exponential, _instance)
A:torch.distributions.exponential.batch_shape->torch.Size(batch_shape)
A:torch.distributions.exponential.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.exponential.shape->self._extended_shape(sample_shape)
A:torch.distributions.exponential.u->torch.rand(shape, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Exponential(self,rate,validate_args=None)
torch.distributions.Exponential._log_normalizer(self,x)
torch.distributions.Exponential._natural_params(self)
torch.distributions.Exponential.cdf(self,value)
torch.distributions.Exponential.entropy(self)
torch.distributions.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.Exponential.icdf(self,value)
torch.distributions.Exponential.log_prob(self,value)
torch.distributions.Exponential.mean(self)
torch.distributions.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.Exponential.stddev(self)
torch.distributions.Exponential.variance(self)
torch.distributions.exponential.Exponential(self,rate,validate_args=None)
torch.distributions.exponential.Exponential.__init__(self,rate,validate_args=None)
torch.distributions.exponential.Exponential._log_normalizer(self,x)
torch.distributions.exponential.Exponential._natural_params(self)
torch.distributions.exponential.Exponential.cdf(self,value)
torch.distributions.exponential.Exponential.entropy(self)
torch.distributions.exponential.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.exponential.Exponential.icdf(self,value)
torch.distributions.exponential.Exponential.log_prob(self,value)
torch.distributions.exponential.Exponential.mean(self)
torch.distributions.exponential.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.exponential.Exponential.stddev(self)
torch.distributions.exponential.Exponential.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/beta.py----------------------------------------
A:torch.distributions.beta.concentration1_concentration0->torch.stack([concentration1, concentration0], -1)
A:torch.distributions.beta.(concentration1, concentration0)->broadcast_all(concentration1, concentration0)
A:torch.distributions.beta.self._dirichlet->Dirichlet(concentration1_concentration0)
A:torch.distributions.beta.new->self._get_checked_instance(Beta, _instance)
A:torch.distributions.beta.batch_shape->torch.Size(batch_shape)
A:torch.distributions.beta.new._dirichlet->self._dirichlet.expand(batch_shape)
A:torch.distributions.beta.heads_tails->torch.stack([value, 1.0 - value], -1)
torch.distributions.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.Beta._log_normalizer(self,x,y)
torch.distributions.Beta._natural_params(self)
torch.distributions.Beta.concentration0(self)
torch.distributions.Beta.concentration1(self)
torch.distributions.Beta.entropy(self)
torch.distributions.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.Beta.log_prob(self,value)
torch.distributions.Beta.mean(self)
torch.distributions.Beta.rsample(self,sample_shape=())
torch.distributions.Beta.variance(self)
torch.distributions.beta.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta.__init__(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta._log_normalizer(self,x,y)
torch.distributions.beta.Beta._natural_params(self)
torch.distributions.beta.Beta.concentration0(self)
torch.distributions.beta.Beta.concentration1(self)
torch.distributions.beta.Beta.entropy(self)
torch.distributions.beta.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.beta.Beta.log_prob(self,value)
torch.distributions.beta.Beta.mean(self)
torch.distributions.beta.Beta.rsample(self,sample_shape=())
torch.distributions.beta.Beta.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/constraints.py----------------------------------------
A:torch.distributions.constraints.value_tril->value.tril()
A:torch.distributions.constraints.flattened_value->value.reshape((-1,) + matrix_shape)
A:torch.distributions.constraints.self.cseq->list(cseq)
A:torch.distributions.constraints.self.lengths->list(lengths)
A:torch.distributions.constraints.v->value.narrow(self.dim, start, length)
A:torch.distributions.constraints.dependent->_Dependent()
A:torch.distributions.constraints.boolean->_Boolean()
A:torch.distributions.constraints.nonnegative_integer->_IntegerGreaterThan(0)
A:torch.distributions.constraints.positive_integer->_IntegerGreaterThan(1)
A:torch.distributions.constraints.real->_Real()
A:torch.distributions.constraints.real_vector->_RealVector()
A:torch.distributions.constraints.positive->_GreaterThan(0.0)
A:torch.distributions.constraints.unit_interval->_Interval(0.0, 1.0)
A:torch.distributions.constraints.simplex->_Simplex()
A:torch.distributions.constraints.lower_triangular->_LowerTriangular()
A:torch.distributions.constraints.lower_cholesky->_LowerCholesky()
A:torch.distributions.constraints.positive_definite->_PositiveDefinite()
torch.distributions.constraints.Constraint(object)
torch.distributions.constraints.Constraint.__repr__(self)
torch.distributions.constraints.Constraint.check(self,value)
torch.distributions.constraints._Boolean(Constraint)
torch.distributions.constraints._Boolean.check(self,value)
torch.distributions.constraints._Cat(self,cseq,dim=0,lengths=None)
torch.distributions.constraints._Cat.__init__(self,cseq,dim=0,lengths=None)
torch.distributions.constraints._Cat.check(self,value)
torch.distributions.constraints._Dependent(Constraint)
torch.distributions.constraints._Dependent.check(self,x)
torch.distributions.constraints._DependentProperty(property,_Dependent)
torch.distributions.constraints._GreaterThan(self,lower_bound)
torch.distributions.constraints._GreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThan.__repr__(self)
torch.distributions.constraints._GreaterThan.check(self,value)
torch.distributions.constraints._GreaterThanEq(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__repr__(self)
torch.distributions.constraints._GreaterThanEq.check(self,value)
torch.distributions.constraints._HalfOpenInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__repr__(self)
torch.distributions.constraints._HalfOpenInterval.check(self,value)
torch.distributions.constraints._IntegerGreaterThan(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__repr__(self)
torch.distributions.constraints._IntegerGreaterThan.check(self,value)
torch.distributions.constraints._IntegerInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__repr__(self)
torch.distributions.constraints._IntegerInterval.check(self,value)
torch.distributions.constraints._IntegerLessThan(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__init__(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__repr__(self)
torch.distributions.constraints._IntegerLessThan.check(self,value)
torch.distributions.constraints._Interval(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__repr__(self)
torch.distributions.constraints._Interval.check(self,value)
torch.distributions.constraints._LessThan(self,upper_bound)
torch.distributions.constraints._LessThan.__init__(self,upper_bound)
torch.distributions.constraints._LessThan.__repr__(self)
torch.distributions.constraints._LessThan.check(self,value)
torch.distributions.constraints._LowerCholesky(Constraint)
torch.distributions.constraints._LowerCholesky.check(self,value)
torch.distributions.constraints._LowerTriangular(Constraint)
torch.distributions.constraints._LowerTriangular.check(self,value)
torch.distributions.constraints._PositiveDefinite(Constraint)
torch.distributions.constraints._PositiveDefinite.check(self,value)
torch.distributions.constraints._Real(Constraint)
torch.distributions.constraints._Real.check(self,value)
torch.distributions.constraints._RealVector(Constraint)
torch.distributions.constraints._RealVector.check(self,value)
torch.distributions.constraints._Simplex(Constraint)
torch.distributions.constraints._Simplex.check(self,value)
torch.distributions.constraints._Stack(self,cseq,dim=0)
torch.distributions.constraints._Stack.__init__(self,cseq,dim=0)
torch.distributions.constraints._Stack.check(self,value)
torch.distributions.constraints.is_dependent(constraint)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/distributions/fishersnedecor.py----------------------------------------
A:torch.distributions.fishersnedecor.(self.df1, self.df2)->broadcast_all(df1, df2)
A:torch.distributions.fishersnedecor.self._gamma1->Gamma(self.df1 * 0.5, self.df1)
A:torch.distributions.fishersnedecor.self._gamma2->Gamma(self.df2 * 0.5, self.df2)
A:torch.distributions.fishersnedecor.batch_shape->torch.Size(batch_shape)
A:torch.distributions.fishersnedecor.new->self._get_checked_instance(FisherSnedecor, _instance)
A:torch.distributions.fishersnedecor.new.df1->self.df1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new.df2->self.df2.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma1->self._gamma1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma2->self._gamma2.expand(batch_shape)
A:torch.distributions.fishersnedecor.df2->self.df2.clone(memory_format=torch.contiguous_format)
A:torch.distributions.fishersnedecor.shape->self._extended_shape(sample_shape)
A:torch.distributions.fishersnedecor.X1->self._gamma1.rsample(sample_shape).view(shape)
A:torch.distributions.fishersnedecor.X2->self._gamma2.rsample(sample_shape).view(shape)
torch.distributions.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.FisherSnedecor.log_prob(self,value)
torch.distributions.FisherSnedecor.mean(self)
torch.distributions.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.FisherSnedecor.variance(self)
torch.distributions.fishersnedecor.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.__init__(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.fishersnedecor.FisherSnedecor.log_prob(self,value)
torch.distributions.fishersnedecor.FisherSnedecor.mean(self)
torch.distributions.fishersnedecor.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.fishersnedecor.FisherSnedecor.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/sgd.py----------------------------------------
A:torch.optim.sgd.defaults->dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov)
A:torch.optim.sgd.loss->closure()
A:torch.optim.sgd.d_p->d_p.add(buf, alpha=momentum).add(buf, alpha=momentum)
A:torch.optim.sgd.bufparam_state['momentum_buffer']->torch.clone(d_p).detach()
torch.optim.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.SGD.__setstate__(self,state)
torch.optim.SGD.step(self,closure=None)
torch.optim.sgd.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.sgd.SGD.__init__(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.sgd.SGD.__setstate__(self,state)
torch.optim.sgd.SGD.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/sgd.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/rprop.py----------------------------------------
A:torch.optim.rprop.defaults->dict(lr=lr, etas=etas, step_sizes=step_sizes)
A:torch.optim.rprop.loss->closure()
A:torch.optim.rprop.state['prev']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rprop.state['step_size']->grad.clone(memory_format=torch.preserve_format).new().resize_as_(grad).fill_(group['lr'])
A:torch.optim.rprop.sign->grad.clone(memory_format=torch.preserve_format).mul(state['prev']).sign()
A:torch.optim.rprop.grad->grad.clone(memory_format=torch.preserve_format).clone(memory_format=torch.preserve_format)
torch.optim.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.Rprop.step(self,closure=None)
torch.optim.rprop.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.__init__(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/rprop.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/lbfgs.py----------------------------------------
A:torch.optim.lbfgs.d2->d2_square.sqrt()
A:torch.optim.lbfgs.d_norm->flat_grad.neg().abs().max()
A:torch.optim.lbfgs.g->g.clone(memory_format=torch.contiguous_format).clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.(f_new, g_new)->obj_func(x, t, d)
A:torch.optim.lbfgs.gtd_new->g_new.dot(d)
A:torch.optim.lbfgs.t->state.get('t')
A:torch.optim.lbfgs.g_prev->g_new.clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.bracket_g[high_pos]->g_new.clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.bracket_g[low_pos]->g_new.clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.defaults->dict(lr=lr, max_iter=max_iter, max_eval=max_eval, tolerance_grad=tolerance_grad, tolerance_change=tolerance_change, history_size=history_size, line_search_fn=line_search_fn)
A:torch.optim.lbfgs.self._numel_cache->reduce(lambda total, p: total + p.numel(), self._params, 0)
A:torch.optim.lbfgs.view->p.grad.view(-1)
A:torch.optim.lbfgs.numel->p.numel()
A:torch.optim.lbfgs.loss->float(closure())
A:torch.optim.lbfgs.flat_grad->self._gather_flat_grad()
A:torch.optim.lbfgs.closure->torch.enable_grad()(closure)
A:torch.optim.lbfgs.orig_loss->closure()
A:torch.optim.lbfgs.d->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.old_dirs->state.get('old_dirs')
A:torch.optim.lbfgs.old_stps->state.get('old_stps')
A:torch.optim.lbfgs.ro->state.get('ro')
A:torch.optim.lbfgs.H_diag->state.get('H_diag')
A:torch.optim.lbfgs.prev_flat_grad->self._gather_flat_grad().clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.prev_loss->state.get('prev_loss')
A:torch.optim.lbfgs.y->self._gather_flat_grad().sub(prev_flat_grad)
A:torch.optim.lbfgs.s->self._gather_flat_grad().neg().mul(t)
A:torch.optim.lbfgs.ys->self._gather_flat_grad().sub(prev_flat_grad).dot(s)
A:torch.optim.lbfgs.num_old->len(old_dirs)
A:torch.optim.lbfgs.q->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.dr->torch.mul(q, H_diag)
A:torch.optim.lbfgs.gtd->self._gather_flat_grad().dot(d)
A:torch.optim.lbfgs.x_init->self._clone_param()
A:torch.optim.lbfgs.(loss, flat_grad, t, ls_func_evals)->_strong_wolfe(obj_func, x_init, t, d, loss, flat_grad, gtd)
torch.optim.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.LBFGS._add_grad(self,step_size,update)
torch.optim.LBFGS._clone_param(self)
torch.optim.LBFGS._directional_evaluate(self,closure,x,t,d)
torch.optim.LBFGS._gather_flat_grad(self)
torch.optim.LBFGS._numel(self)
torch.optim.LBFGS._set_param(self,params_data)
torch.optim.LBFGS.step(self,closure)
torch.optim.lbfgs.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS.__init__(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS._add_grad(self,step_size,update)
torch.optim.lbfgs.LBFGS._clone_param(self)
torch.optim.lbfgs.LBFGS._directional_evaluate(self,closure,x,t,d)
torch.optim.lbfgs.LBFGS._gather_flat_grad(self)
torch.optim.lbfgs.LBFGS._numel(self)
torch.optim.lbfgs.LBFGS._set_param(self,params_data)
torch.optim.lbfgs.LBFGS.step(self,closure)
torch.optim.lbfgs._cubic_interpolate(x1,f1,g1,x2,f2,g2,bounds=None)
torch.optim.lbfgs._strong_wolfe(obj_func,x,t,d,f,g,gtd,c1=0.0001,c2=0.9,tolerance_change=1e-09,max_ls=25)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/lbfgs.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adadelta.py----------------------------------------
A:torch.optim.adadelta.defaults->dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)
A:torch.optim.adadelta.loss->closure()
A:torch.optim.adadelta.state['square_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adadelta.state['acc_delta']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adadelta.grad->grad.add(p, alpha=group['weight_decay']).add(p, alpha=group['weight_decay'])
A:torch.optim.adadelta.std->square_avg.add(eps).sqrt_()
A:torch.optim.adadelta.delta->acc_delta.add(eps).sqrt_().div_(std).mul_(grad)
torch.optim.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.Adadelta.step(self,closure=None)
torch.optim.adadelta.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.__init__(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adadelta.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adamax.py----------------------------------------
A:torch.optim.adamax.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
A:torch.optim.adamax.loss->closure()
A:torch.optim.adamax.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamax.state['exp_inf']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamax.grad->grad.add(p, alpha=group['weight_decay']).add(p, alpha=group['weight_decay'])
A:torch.optim.adamax.norm_buf->torch.cat([exp_inf.mul_(beta2).unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)
torch.optim.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.Adamax.step(self,closure=None)
torch.optim.adamax.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adamax.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/rmsprop.py----------------------------------------
A:torch.optim.rmsprop.defaults->dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)
A:torch.optim.rmsprop.loss->closure()
A:torch.optim.rmsprop.state['square_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rmsprop.state['momentum_buffer']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rmsprop.state['grad_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rmsprop.grad->grad.add(p, alpha=group['weight_decay']).add(p, alpha=group['weight_decay'])
A:torch.optim.rmsprop.avg->square_avg.sqrt().add_(group['eps'])
torch.optim.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.RMSprop.__setstate__(self,state)
torch.optim.RMSprop.step(self,closure=None)
torch.optim.rmsprop.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__init__(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__setstate__(self,state)
torch.optim.rmsprop.RMSprop.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/rmsprop.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/asgd.py----------------------------------------
A:torch.optim.asgd.defaults->dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0, weight_decay=weight_decay)
A:torch.optim.asgd.loss->closure()
A:torch.optim.asgd.state['ax']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.asgd.grad->grad.add(p, alpha=group['weight_decay']).add(p, alpha=group['weight_decay'])
torch.optim.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.ASGD.step(self,closure=None)
torch.optim.asgd.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.__init__(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/asgd.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adagrad.py----------------------------------------
A:torch.optim.adagrad.defaults->dict(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay, initial_accumulator_value=initial_accumulator_value)
A:torch.optim.adagrad.state['sum']->torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format)
A:torch.optim.adagrad.loss->closure()
A:torch.optim.adagrad.grad->grad.coalesce().coalesce()
A:torch.optim.adagrad.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim.adagrad.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim.adagrad.size->grad.coalesce().coalesce().size()
A:torch.optim.adagrad.std->torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format).sqrt().add_(group['eps'])
A:torch.optim.adagrad.std_values->torch.full_like(p, initial_accumulator_value, memory_format=torch.preserve_format).sqrt().add_(group['eps'])._values().sqrt_().add_(group['eps'])
torch.optim.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.Adagrad.share_memory(self)
torch.optim.Adagrad.step(self,closure=None)
torch.optim.adagrad.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.adagrad.Adagrad.__init__(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.adagrad.Adagrad.share_memory(self)
torch.optim.adagrad.Adagrad.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adagrad.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adamw.py----------------------------------------
A:torch.optim.adamw.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)
A:torch.optim.adamw.loss->closure()
A:torch.optim.adamw.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamw.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamw.state['max_exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamw.denom->(exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])
torch.optim.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)
torch.optim.AdamW.__setstate__(self,state)
torch.optim.AdamW.step(self,closure=None)
torch.optim.adamw.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)
torch.optim.adamw.AdamW.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)
torch.optim.adamw.AdamW.__setstate__(self,state)
torch.optim.adamw.AdamW.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adamw.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/lr_scheduler.py----------------------------------------
A:torch.optim.lr_scheduler.self.base_lrs->list(map(lambda group: group['initial_lr'], optimizer.param_groups))
A:torch.optim.lr_scheduler.instance_ref->weakref.ref(method.__self__)
A:torch.optim.lr_scheduler.instance->instance_ref()
A:torch.optim.lr_scheduler.wrapped->func.__get__(instance, cls)
A:torch.optim.lr_scheduler.self.optimizer.step->with_counter(self.optimizer.step)
A:torch.optim.lr_scheduler.values->self.get_lr()
A:torch.optim.lr_scheduler.self.lr_lambdas->list(lr_lambda)
A:torch.optim.lr_scheduler.state_dict['lr_lambdas'][idx]->fn.__dict__.copy()
A:torch.optim.lr_scheduler.lr_lambdas->state_dict.pop('lr_lambdas')
A:torch.optim.lr_scheduler.self.milestones->Counter(milestones)
A:torch.optim.lr_scheduler.milestones->list(sorted(self.milestones.elements()))
A:torch.optim.lr_scheduler.self.min_lrs->list(min_lr)
A:torch.optim.lr_scheduler.current->float(metrics)
A:torch.optim.lr_scheduler.old_lr->float(param_group['lr'])
A:torch.optim.lr_scheduler.new_lr->max(old_lr * self.factor, self.min_lrs[i])
A:torch.optim.lr_scheduler.base_lrs->self._format_param('base_lr', optimizer, base_lr)
A:torch.optim.lr_scheduler.self.max_lrs->self._format_param('max_lr', optimizer, max_lr)
A:torch.optim.lr_scheduler.step_size_up->float(step_size_up)
A:torch.optim.lr_scheduler.base_momentums->self._format_param('base_momentum', optimizer, base_momentum)
A:torch.optim.lr_scheduler.self.base_momentums->list(map(lambda group: group['momentum'], optimizer.param_groups))
A:torch.optim.lr_scheduler.self.max_momentums->self._format_param('max_momentum', optimizer, max_momentum)
A:torch.optim.lr_scheduler.cycle->math.floor(1 + self.last_epoch / self.total_size)
A:torch.optim.lr_scheduler.n->int(math.log(epoch / self.T_0 * (self.T_mult - 1) + 1, self.T_mult))
A:torch.optim.lr_scheduler.self.last_epoch->math.floor(epoch)
A:torch.optim.lr_scheduler.max_lrs->self._format_param('max_lr', self.optimizer, max_lr)
A:torch.optim.lr_scheduler.max_momentums->self._format_param('max_momentum', optimizer, max_momentum)
A:torch.optim.lr_scheduler.computed_lr->self.anneal_func(group['max_lr'], group['min_lr'], down_step_num / self.step_size_down)
A:torch.optim.lr_scheduler.computed_momentum->self.anneal_func(group['base_momentum'], group['max_momentum'], down_step_num / self.step_size_down)
torch.optim.lr_scheduler.CosineAnnealingLR(self,optimizer,T_max,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingLR.__init__(self,optimizer,T_max,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.CosineAnnealingLR.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self,optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.__init__(self,optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step(self,epoch=None)
torch.optim.lr_scheduler.CyclicLR(self,optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1)
torch.optim.lr_scheduler.CyclicLR.__init__(self,optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1)
torch.optim.lr_scheduler.CyclicLR._exp_range_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR._format_param(self,name,optimizer,param)
torch.optim.lr_scheduler.CyclicLR._triangular2_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR._triangular_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR.get_lr(self)
torch.optim.lr_scheduler.ExponentialLR(self,optimizer,gamma,last_epoch=-1)
torch.optim.lr_scheduler.ExponentialLR.__init__(self,optimizer,gamma,last_epoch=-1)
torch.optim.lr_scheduler.ExponentialLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.ExponentialLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.LambdaLR.__init__(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.LambdaLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.LambdaLR.state_dict(self)
torch.optim.lr_scheduler.MultiStepLR(self,optimizer,milestones,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.MultiStepLR.__init__(self,optimizer,milestones,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.MultiStepLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.MultiStepLR.get_lr(self)
torch.optim.lr_scheduler.MultiplicativeLR(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.MultiplicativeLR.__init__(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.MultiplicativeLR.get_lr(self)
torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.MultiplicativeLR.state_dict(self)
torch.optim.lr_scheduler.OneCycleLR(self,optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,last_epoch=-1)
torch.optim.lr_scheduler.OneCycleLR.__init__(self,optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,last_epoch=-1)
torch.optim.lr_scheduler.OneCycleLR._annealing_cos(self,start,end,pct)
torch.optim.lr_scheduler.OneCycleLR._annealing_linear(self,start,end,pct)
torch.optim.lr_scheduler.OneCycleLR._format_param(self,name,optimizer,param)
torch.optim.lr_scheduler.OneCycleLR.get_lr(self)
torch.optim.lr_scheduler.ReduceLROnPlateau(self,optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)
torch.optim.lr_scheduler.ReduceLROnPlateau.__init__(self,optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)
torch.optim.lr_scheduler.ReduceLROnPlateau._init_is_better(self,mode,threshold,threshold_mode)
torch.optim.lr_scheduler.ReduceLROnPlateau._reduce_lr(self,epoch)
torch.optim.lr_scheduler.ReduceLROnPlateau._reset(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.in_cooldown(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.is_better(self,a,best)
torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.ReduceLROnPlateau.state_dict(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.step(self,metrics,epoch=None)
torch.optim.lr_scheduler.StepLR(self,optimizer,step_size,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.StepLR.__init__(self,optimizer,step_size,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.StepLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.StepLR.get_lr(self)
torch.optim.lr_scheduler._LRScheduler(self,optimizer,last_epoch=-1)
torch.optim.lr_scheduler._LRScheduler.__init__(self,optimizer,last_epoch=-1)
torch.optim.lr_scheduler._LRScheduler.get_last_lr(self)
torch.optim.lr_scheduler._LRScheduler.get_lr(self)
torch.optim.lr_scheduler._LRScheduler.load_state_dict(self,state_dict)
torch.optim.lr_scheduler._LRScheduler.state_dict(self)
torch.optim.lr_scheduler._LRScheduler.step(self,epoch=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/lr_scheduler.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/sparse_adam.py----------------------------------------
A:torch.optim.sparse_adam.defaults->dict(lr=lr, betas=betas, eps=eps)
A:torch.optim.sparse_adam.loss->closure()
A:torch.optim.sparse_adam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.sparse_adam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.sparse_adam.grad->grad.coalesce().coalesce()
A:torch.optim.sparse_adam.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim.sparse_adam.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim.sparse_adam.size->grad.coalesce().coalesce().size()
A:torch.optim.sparse_adam.old_exp_avg_values->exp_avg.sparse_mask(grad)._values()
A:torch.optim.sparse_adam.exp_avg_update_values->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1)
A:torch.optim.sparse_adam.old_exp_avg_sq_values->exp_avg_sq.sparse_mask(grad)._values()
A:torch.optim.sparse_adam.exp_avg_sq_update_values->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)
A:torch.optim.sparse_adam.numer->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1).add_(old_exp_avg_values)
A:torch.optim.sparse_adam.denom->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2).sqrt_().add_(group['eps'])
torch.optim.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.SparseAdam.step(self,closure=None)
torch.optim.sparse_adam.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/sparse_adam.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/optimizer.py----------------------------------------
A:torch.optim.optimizer.required->_RequiredParameter()
A:torch.optim.optimizer.self.state->defaultdict(dict)
A:torch.optim.optimizer.param_groups->list(params)
A:torch.optim.optimizer.state_dict->deepcopy(state_dict)
A:torch.optim.optimizer.value->value.to(param.device).to(param.device)
A:torch.optim.optimizer.state->defaultdict(dict)
A:torch.optim.optimizer.state[param]->cast(param, v)
A:torch.optim.optimizer.param_group['params']->list(params)
A:torch.optim.optimizer.param_set->set()
torch.optim.Optimizer(self,params,defaults)
torch.optim.Optimizer.__getstate__(self)
torch.optim.Optimizer.__repr__(self)
torch.optim.Optimizer.__setstate__(self,state)
torch.optim.Optimizer.add_param_group(self,param_group)
torch.optim.Optimizer.load_state_dict(self,state_dict)
torch.optim.Optimizer.state_dict(self)
torch.optim.Optimizer.step(self,closure)
torch.optim.Optimizer.zero_grad(self)
torch.optim.optimizer.Optimizer(self,params,defaults)
torch.optim.optimizer.Optimizer.__getstate__(self)
torch.optim.optimizer.Optimizer.__init__(self,params,defaults)
torch.optim.optimizer.Optimizer.__repr__(self)
torch.optim.optimizer.Optimizer.__setstate__(self,state)
torch.optim.optimizer.Optimizer.add_param_group(self,param_group)
torch.optim.optimizer.Optimizer.load_state_dict(self,state_dict)
torch.optim.optimizer.Optimizer.state_dict(self)
torch.optim.optimizer.Optimizer.step(self,closure)
torch.optim.optimizer.Optimizer.zero_grad(self)
torch.optim.optimizer._RequiredParameter(object)
torch.optim.optimizer._RequiredParameter.__repr__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/optimizer.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adam.py----------------------------------------
A:torch.optim.adam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)
A:torch.optim.adam.loss->closure()
A:torch.optim.adam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adam.state['max_exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adam.grad->grad.add(p, alpha=group['weight_decay']).add(p, alpha=group['weight_decay'])
A:torch.optim.adam.denom->(exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])
torch.optim.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.Adam.__setstate__(self,state)
torch.optim.Adam.step(self,closure=None)
torch.optim.adam.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.adam.Adam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.adam.Adam.__setstate__(self,state)
torch.optim.adam.Adam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.5.1/optim/adam.pyi----------------------------------------

