
----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/functional.py----------------------------------------
A:torch.functional.sz->LU_data.size(-1)
A:torch.functional.U->LU_data.triu()
A:torch.functional.L->LU_data.tril()
A:torch.functional.P->torch.eye(sz, device=LU_data.device, dtype=LU_data.dtype).expand_as(LU_data).clone()
A:torch.functional.final_order->list(range(sz))
A:torch.functional.P[idx]->P[idx].index_select(1, torch.as_tensor(final_order, device=LU_pivots.device)).index_select(1, torch.as_tensor(final_order, device=LU_pivots.device))
A:torch.functional.signal_dim->input.view(input.shape[-signal_dim:]).dim()
A:torch.functional.pad->int(n_fft // 2)
A:torch.functional.input->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:])
A:torch.functional.(output, inverse_indices)->torch._unique(input, sorted=sorted, return_inverse=return_inverse)
A:torch.functional.dims->dims.item().item()
A:torch.functional.dims_a->list(range(-dims, 0))
A:torch.functional.dims_b->list(range(dims))
A:torch.functional.ndim->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:]).dim()
A:torch.functional.dim->tuple(range(ndim))
torch.argmax(input,dim=None,keepdim=False)
torch.argmin(input,dim=None,keepdim=False)
torch.argsort(input,dim=None,descending=False)
torch.broadcast_tensors(*tensors)
torch.btrifact(A,info=None,pivot=True)
torch.btriunpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)
torch.chain_matmul(*matrices)
torch.einsum(equation,*operands)
torch.functional.argmax(input,dim=None,keepdim=False)
torch.functional.argmin(input,dim=None,keepdim=False)
torch.functional.argsort(input,dim=None,descending=False)
torch.functional.broadcast_tensors(*tensors)
torch.functional.btrifact(A,info=None,pivot=True)
torch.functional.btriunpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)
torch.functional.chain_matmul(*matrices)
torch.functional.einsum(equation,*operands)
torch.functional.isfinite(tensor)
torch.functional.isinf(tensor)
torch.functional.isnan(tensor)
torch.functional.meshgrid(*tensors,**kwargs)
torch.functional.norm(input,p='fro',dim=None,keepdim=False,out=None)
torch.functional.potrf(a,upper=True,out=None)
torch.functional.split(tensor,split_size_or_sections,dim=0)
torch.functional.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.functional.tensordot(a,b,dims=2)
torch.functional.unique(input,sorted=False,return_inverse=False,dim=None)
torch.isfinite(tensor)
torch.isinf(tensor)
torch.isnan(tensor)
torch.meshgrid(*tensors,**kwargs)
torch.norm(input,p='fro',dim=None,keepdim=False,out=None)
torch.potrf(a,upper=True,out=None)
torch.split(tensor,split_size_or_sections,dim=0)
torch.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.tensordot(a,b,dims=2)
torch.unique(input,sorted=False,return_inverse=False,dim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/__init__.py----------------------------------------
A:torch.__init__.NVTOOLEXT_HOME->torch._dl.getenv('NVTOOLSEXT_PATH', 'C:\\Program Files\\NVIDIA Corporation\\NvToolsExt')
A:torch.__init__.py_dll_path->torch._dl.path.join(_dl_flags.path.dirname(sys.executable), 'Library\\bin')
A:torch.__init__._dl_flags.environ['PATH']->';'.join(dll_paths)
A:torch.__init__.old_flags->sys.getdlopenflags()
A:torch.__init__.t->_import_dotted_name(t)
A:torch.__init__._tensor_classes->set()
A:torch.__init__.path->get_file_path('torch', 'lib', 'torch_shm_manager')
A:torch.__init__.globals()[name]->getattr(_C._VariableFunctions, name)
torch.__init__.ByteStorage(_C.ByteStorageBase,_StorageBase)
torch.__init__.CharStorage(_C.CharStorageBase,_StorageBase)
torch.__init__.DoubleStorage(_C.DoubleStorageBase,_StorageBase)
torch.__init__.FloatStorage(_C.FloatStorageBase,_StorageBase)
torch.__init__.HalfStorage(_C.HalfStorageBase,_StorageBase)
torch.__init__.IntStorage(_C.IntStorageBase,_StorageBase)
torch.__init__.LongStorage(_C.LongStorageBase,_StorageBase)
torch.__init__.ShortStorage(_C.ShortStorageBase,_StorageBase)
torch.__init__.compiled_with_cxx11_abi()
torch.__init__.is_storage(obj)
torch.__init__.is_tensor(obj)
torch.__init__.manager_path()
torch.__init__.set_default_dtype(d)
torch.__init__.set_default_tensor_type(t)
torch.__init__.typename(o)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/tensor.py----------------------------------------
A:torch.tensor.new_tensor->self.new()
A:torch.tensor.new_storage->self.storage().__deepcopy__(memo)
A:torch.tensor.self._backward_hooks->OrderedDict()
A:torch.tensor.handle->torch.utils.hooks.RemovableHandle(self._backward_hooks)
A:torch.tensor.detach->_add_docstr(_C._TensorBase.detach, '\n    Returns a new Tensor, detached from the current graph.\n\n    The result will never require gradient.\n\n    .. note::\n\n      Returned Tensor uses the same data tensor as the original one.\n      In-place modifications on either of them will be seen, and may trigger\n      errors in correctness checks.\n    ')
A:torch.tensor.detach_->_add_docstr(_C._TensorBase.detach_, '\n    Detaches the Tensor from the graph that created it, making it a leaf.\n    Views cannot be detached in-place.\n    ')
A:torch.tensor.weak_self->weakref.ref(self)
A:torch.tensor.var->weak_self()
A:torch.tensor.var._grad->grad.clone()
A:torch.tensor.storage->self.storage()
A:torch.tensor.(factorization, pivots, _info)->super(Tensor, self).btrifact_with_info(pivot=pivot)
A:torch.tensor.(output, inverse_indices)->torch._unique(self, sorted=sorted, return_inverse=return_inverse)
A:torch.tensor.result->result.trunc().trunc()
A:torch.tensor.tensor_methods->dir(self.__class__)
A:torch.tensor.attrs->list(self.__dict__.keys())
A:torch.tensor.array->array.astype('uint8').astype('uint8')
A:torch.tensor.itemsize->self.storage().element_size()
A:torch.tensor.strides->tuple((s * itemsize for s in self.stride()))
torch.Tensor(torch._C._TensorBase)
torch.Tensor.__array__(self,dtype=None)
torch.Tensor.__array_wrap__(self,array)
torch.Tensor.__cuda_array_interface__(self)
torch.Tensor.__deepcopy__(self,memo)
torch.Tensor.__dir__(self)
torch.Tensor.__floordiv__(self,other)
torch.Tensor.__format__(self,format_spec)
torch.Tensor.__hash__(self)
torch.Tensor.__ipow__(self,other)
torch.Tensor.__iter__(self)
torch.Tensor.__len__(self)
torch.Tensor.__rdiv__(self,other)
torch.Tensor.__reduce_ex__(self,proto)
torch.Tensor.__repr__(self)
torch.Tensor.__reversed__(self)
torch.Tensor.__rfloordiv__(self,other)
torch.Tensor.__rpow__(self,other)
torch.Tensor.__rsub__(self,other)
torch.Tensor.__setstate__(self,state)
torch.Tensor.argmax(self,dim=None,keepdim=False)
torch.Tensor.argmin(self,dim=None,keepdim=False)
torch.Tensor.argsort(self,dim=None,descending=False)
torch.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False)
torch.Tensor.btrifact(self,info=None,pivot=True)
torch.Tensor.index_add(self,dim,index,tensor)
torch.Tensor.index_copy(self,dim,index,tensor)
torch.Tensor.index_fill(self,dim,index,value)
torch.Tensor.is_pinned(self)
torch.Tensor.is_shared(self)
torch.Tensor.masked_fill(self,mask,value)
torch.Tensor.masked_scatter(self,mask,tensor)
torch.Tensor.norm(self,p='fro',dim=None,keepdim=False)
torch.Tensor.potrf(self,upper=True)
torch.Tensor.register_hook(self,hook)
torch.Tensor.reinforce(self,reward)
torch.Tensor.resize(self,*sizes)
torch.Tensor.resize_as(self,tensor)
torch.Tensor.retain_grad(self)
torch.Tensor.scatter(self,dim,index,source)
torch.Tensor.scatter_add(self,dim,index,source)
torch.Tensor.share_memory_(self)
torch.Tensor.split(self,split_size,dim=0)
torch.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.Tensor.unique(self,sorted=False,return_inverse=False,dim=None)
torch.tensor.Tensor(torch._C._TensorBase)
torch.tensor.Tensor.__array__(self,dtype=None)
torch.tensor.Tensor.__array_wrap__(self,array)
torch.tensor.Tensor.__cuda_array_interface__(self)
torch.tensor.Tensor.__deepcopy__(self,memo)
torch.tensor.Tensor.__dir__(self)
torch.tensor.Tensor.__floordiv__(self,other)
torch.tensor.Tensor.__format__(self,format_spec)
torch.tensor.Tensor.__hash__(self)
torch.tensor.Tensor.__ipow__(self,other)
torch.tensor.Tensor.__iter__(self)
torch.tensor.Tensor.__len__(self)
torch.tensor.Tensor.__rdiv__(self,other)
torch.tensor.Tensor.__reduce_ex__(self,proto)
torch.tensor.Tensor.__repr__(self)
torch.tensor.Tensor.__reversed__(self)
torch.tensor.Tensor.__rfloordiv__(self,other)
torch.tensor.Tensor.__rpow__(self,other)
torch.tensor.Tensor.__rsub__(self,other)
torch.tensor.Tensor.__setstate__(self,state)
torch.tensor.Tensor.argmax(self,dim=None,keepdim=False)
torch.tensor.Tensor.argmin(self,dim=None,keepdim=False)
torch.tensor.Tensor.argsort(self,dim=None,descending=False)
torch.tensor.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False)
torch.tensor.Tensor.btrifact(self,info=None,pivot=True)
torch.tensor.Tensor.index_add(self,dim,index,tensor)
torch.tensor.Tensor.index_copy(self,dim,index,tensor)
torch.tensor.Tensor.index_fill(self,dim,index,value)
torch.tensor.Tensor.is_pinned(self)
torch.tensor.Tensor.is_shared(self)
torch.tensor.Tensor.masked_fill(self,mask,value)
torch.tensor.Tensor.masked_scatter(self,mask,tensor)
torch.tensor.Tensor.norm(self,p='fro',dim=None,keepdim=False)
torch.tensor.Tensor.potrf(self,upper=True)
torch.tensor.Tensor.register_hook(self,hook)
torch.tensor.Tensor.reinforce(self,reward)
torch.tensor.Tensor.resize(self,*sizes)
torch.tensor.Tensor.resize_as(self,tensor)
torch.tensor.Tensor.retain_grad(self)
torch.tensor.Tensor.scatter(self,dim,index,source)
torch.tensor.Tensor.scatter_add(self,dim,index,source)
torch.tensor.Tensor.share_memory_(self)
torch.tensor.Tensor.split(self,split_size,dim=0)
torch.tensor.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.tensor.Tensor.unique(self,sorted=False,return_inverse=False,dim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_utils_internal.py----------------------------------------
A:torch._utils_internal.torch_parent->os.path.dirname(os.path.dirname(__file__))
torch._utils_internal.get_file_path(*path_components)
torch._utils_internal.get_file_path_2(*path_components)
torch._utils_internal.get_writable_path(path)
torch._utils_internal.prepare_multiprocessing_environment(path)
torch.get_file_path(*path_components)
torch.get_file_path_2(*path_components)
torch.prepare_multiprocessing_environment(path)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_utils.py----------------------------------------
A:torch._utils.non_blocking->_get_async_or_non_blocking('cuda', non_blocking, kwargs)
A:torch._utils.dtype->_import_dotted_name(dtype)
A:torch._utils.new_module_name->_import_dotted_name(dtype).__module__.replace('.sparse', '')
A:torch._utils.new_values->torch._values(self).type(new_values_type_name, non_blocking)
A:torch._utils.new_indices->torch._indices(self).type(new_indices_type_name, non_blocking)
A:torch._utils.device->torch.cuda.current_device()
A:torch._utils.new_type->getattr(torch.cuda, self.__class__.__name__)
A:torch._utils.indices->torch._indices(tensor)
A:torch._utils.values->torch._values(tensor)
A:torch._utils.argument->list(kwargs.keys()).pop()
A:torch._utils.class_name->storage.__class__.__name__.replace('Storage', 'Tensor')
A:torch._utils.module->importlib.import_module(storage.__module__)
A:torch._utils.tensor_class->getattr(module, class_name)
A:torch._utils.tensor->_rebuild_tensor(storage, storage_offset, size, stride)
A:torch._utils.param->torch.nn.Parameter(data, requires_grad)
A:torch._utils.components->name.split('.')
A:torch._utils.obj->getattr(obj, component)
A:torch._utils.it->iter(iterable)
A:torch._utils.total->fn(total, element)
A:torch._utils.flat->torch.cat([t.contiguous().view(-1) for t in tensors], dim=0)
A:torch._utils.flat_indices->_flatten_dense_tensors([torch._indices(t) for t in tensors])
A:torch._utils.flat_values->_flatten_dense_tensors([torch._values(t) for t in tensors])
A:torch._utils.numel->_rebuild_tensor(storage, storage_offset, size, stride).numel()
A:torch._utils.type_dict->defaultdict(list)
A:torch._utils.buf_dict->defaultdict(lambda : [[], 0])
A:torch._utils.t->_rebuild_tensor(storage, storage_offset, size, stride).type()
A:torch._utils.fun.__annotations__->dict(kwargs)
torch._import_dotted_name(name)
torch._utils._accumulate(iterable,fn=lambdax,y:x+y)
torch._utils._cuda(self,device=None,non_blocking=False,**kwargs)
torch._utils._flatten_dense_tensors(tensors)
torch._utils._flatten_sparse_tensors(tensors)
torch._utils._get_async_or_non_blocking(function_name,non_blocking,kwargs)
torch._utils._import_dotted_name(name)
torch._utils._rebuild_parameter(data,requires_grad,backward_hooks)
torch._utils._rebuild_tensor(storage,storage_offset,size,stride)
torch._utils._rebuild_tensor_v2(storage,storage_offset,size,stride,requires_grad,backward_hooks)
torch._utils._reorder_tensors_as(tensors,ordered_tensors)
torch._utils._take_tensors(tensors,size_limit)
torch._utils._type(self,dtype=None,non_blocking=False,**kwargs)
torch._utils._unflatten_dense_tensors(flat,tensors)
torch._utils._unflatten_sparse_tensors(flat,tensors)
torch._utils.annotate(ret,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_six.py----------------------------------------
A:torch._six.inf->float('inf')
A:torch._six.nan->float('nan')
A:torch._six.exec_->getattr(builtins, 'exec')
A:torch._six.frame->sys._getframe(1)
A:torch._six.method->getattr(cls, name, None)
torch._six.with_metaclass(meta,*bases)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/version.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/hub.py----------------------------------------
A:torch.hub.response->urlopen(url)
A:torch.hub.data->urlopen(url).read(READ_DATA_CHUNK)
A:torch.hub.m->importlib.import_module(module_name)
A:torch.hub.hub_dir->os.path.expanduser(hub_dir)
A:torch.hub.(repo_info, branch)->github.split(':')
A:torch.hub.(repo_owner, repo_name)->repo_info.split('/')
A:torch.hub.url->_git_archive_link(repo_info, branch)
A:torch.hub.cached_file->os.path.join(hub_dir, branch + '.zip')
A:torch.hub.repo_dir->os.path.join(hub_dir, repo_name + '_' + branch)
A:torch.hub.cached_zipfile->zipfile.ZipFile(cached_file)
A:torch.hub.extracted_repo->os.path.join(hub_dir, extraced_repo_name)
A:torch.hub.dependencies->_load_attr_from_module('hubconf', 'dependencies')
A:torch.hub.func->_load_attr_from_module('hubconf', model)
torch.hub._check_module_exists(name)
torch.hub._download_url_to_file(url,filename)
torch.hub._git_archive_link(repo,branch)
torch.hub._load_attr_from_module(module_name,func_name)
torch.hub._remove_if_exists(path)
torch.hub.load(github,model,force_reload=False,*args,**kwargs)
torch.hub.set_dir(d)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_jit_internal.py----------------------------------------
A:torch._jit_internal._compiled_weak_fns->weakref.WeakKeyDictionary()
A:torch._jit_internal._weak_script_methods->weakref.WeakKeyDictionary()
A:torch._jit_internal._weak_modules->weakref.WeakKeyDictionary()
A:torch._jit_internal._weak_types->weakref.WeakKeyDictionary()
A:torch._jit_internal._boolean_dispatched->weakref.WeakKeyDictionary()
A:torch._jit_internal.COMPILATION_PENDING->object()
A:torch._jit_internal.COMPILED->object()
A:torch._jit_internal.frame->inspect.currentframe()
A:torch._jit_internal.Tuple->TupleCls()
A:torch._jit_internal.List->ListCls()
A:torch._jit_internal.BroadcastingList1->BroadcastingListCls()
torch._jit_internal.BroadcastingListCls(object)
torch._jit_internal.BroadcastingListCls.__getitem__(self,types)
torch._jit_internal.boolean_dispatch(arg_name,arg_index,default,if_true,if_false)
torch._jit_internal.createResolutionCallback(frames_up=0)
torch._jit_internal.weak_module(cls)
torch._jit_internal.weak_script(fn,_frames_up=0)
torch._jit_internal.weak_script_method(fn)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_tensor_str.py----------------------------------------
A:torch._tensor_str.PRINT_OPTS->__PrinterOptions()
A:torch._tensor_str.tensor_view->tensor.reshape(-1)
A:torch._tensor_str.value_str->'{{:.{}f}}'.format(PRINT_OPTS.precision).format(value)
A:torch._tensor_str.self.max_width->max(self.max_width, len(value_str))
A:torch._tensor_str.nonzero_finite_vals->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
A:torch._tensor_str.nonzero_finite_abs->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double()
A:torch._tensor_str.nonzero_finite_min->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().min().double()
A:torch._tensor_str.nonzero_finite_max->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().max().double()
A:torch._tensor_str.ret->'{}'.format(value)
A:torch._tensor_str.elements_per_line->max(1, int(math.floor((PRINT_OPTS.linewidth - indent) / element_length)))
A:torch._tensor_str.dim->self.float().dim()
A:torch._tensor_str.tensor_str->_tensor_str(self, indent)
A:torch._tensor_str.self->self.float().float()
A:torch._tensor_str.formatter->_Formatter(get_summarized_data(self) if summarize else self)
A:torch._tensor_str.suffix_len->len(suffix)
A:torch._tensor_str.indent->len(prefix)
A:torch._tensor_str.indices->self.float().float()._indices().detach()
A:torch._tensor_str.indices_str->_tensor_str(indices, indent + len(indices_prefix))
A:torch._tensor_str.values->self.float().float()._values().detach()
A:torch._tensor_str.values_str->_tensor_str(values, indent + len(values_prefix))
torch._tensor_str._Formatter(self,tensor)
torch._tensor_str._Formatter.__init__(self,tensor)
torch._tensor_str._Formatter.format(self,value)
torch._tensor_str._Formatter.width(self)
torch._tensor_str.__PrinterOptions(object)
torch._tensor_str._add_suffixes(tensor_str,suffixes,indent,force_newline)
torch._tensor_str._scalar_str(self,formatter)
torch._tensor_str._str(self)
torch._tensor_str._tensor_str(self,indent)
torch._tensor_str._tensor_str_with_formatter(self,indent,formatter,summarize)
torch._tensor_str._vector_str(self,indent,formatter,summarize)
torch._tensor_str.get_summarized_data(self)
torch._tensor_str.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None)
torch.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/serialization.py----------------------------------------
A:torch.serialization.path->tempfile.mkdtemp()
A:torch.serialization.location->map_location.get(location, location)
A:torch.serialization.device->validate_cuda_device(location)
A:torch.serialization.result->pickle_module.Unpickler(f).load()
A:torch.serialization.storage_type->normalize_storage_type(type(obj))
A:torch.serialization.module->_import_dotted_name(storage_type.__module__)
A:torch.serialization.f->open(f, 'rb')
A:torch.serialization.source_file->inspect.getsourcefile(obj)
A:torch.serialization.source->inspect.getsource(obj)
A:torch.serialization.obj_key->str(obj._cdata)
A:torch.serialization.sys_info->dict(protocol_version=PROTOCOL_VERSION, little_endian=sys.byteorder == 'little', type_sizes=dict(short=SHORT_SIZE, int=INT_SIZE, long=LONG_SIZE))
A:torch.serialization.pickler->pickle_module.Pickler(f, protocol=pickle_protocol)
A:torch.serialization.serialized_storage_keys->sorted(serialized_storages.keys())
A:torch.serialization.current_source->inspect.getsource(container_type)
A:torch.serialization.diff->difflib.unified_diff(current_source.split('\n'), original_source.split('\n'), source_file, source_file, lineterm='')
A:torch.serialization.lines->'\n'.join(diff)
A:torch.serialization.file_size->open(f, 'rb').seek(0, 2)
A:torch.serialization.msg->"source code of class '{}' has changed. {}".format(torch.typename(container_type), msg)
A:torch.serialization.num_storages->pickle_module.load(f)
A:torch.serialization.args->pickle_module.load(f)
A:torch.serialization.obj->restore_location(obj, location)
A:torch.serialization.storage_views->pickle_module.load(f)
A:torch.serialization.num_tensors->pickle_module.load(f)
A:torch.serialization.tensor_type->storage_to_tensor_type(storage)
A:torch.serialization.(ndim,)->struct.unpack('<i', f.read(4))
A:torch.serialization.size->struct.unpack('<{}q'.format(ndim), f.read(8 * ndim))
A:torch.serialization.stride->struct.unpack('<{}q'.format(ndim), f.read(8 * ndim))
A:torch.serialization.(storage_offset,)->struct.unpack('<q', f.read(8))
A:torch.serialization.tensor->tensor_type().set_(storage, storage_offset, size, stride)
A:torch.serialization.pickle_file->tar.extractfile('pickle')
A:torch.serialization.unpickler->pickle_module.Unpickler(f)
A:torch.serialization.deserialized_objects[root_key]->restore_location(data_type(size), location)
A:torch.serialization.f_should_read_directly->_should_read_directly(f)
A:torch.serialization.magic_number->pickle_module.load(f)
A:torch.serialization.protocol_version->pickle_module.load(f)
A:torch.serialization._sys_info->pickle_module.load(f)
A:torch.serialization.deserialized_storage_keys->pickle_module.load(f)
torch.load(f,map_location=None,pickle_module=pickle)
torch.save(obj,f,pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL)
torch.serialization.SourceChangeWarning(Warning)
torch.serialization._check_seekable(f)
torch.serialization._cpu_deserialize(obj,location)
torch.serialization._cpu_tag(obj)
torch.serialization._cuda_deserialize(obj,location)
torch.serialization._cuda_tag(obj)
torch.serialization._is_compressed_file(f)
torch.serialization._load(f,map_location,pickle_module)
torch.serialization._save(obj,f,pickle_module,pickle_protocol)
torch.serialization._should_read_directly(f)
torch.serialization._with_file_like(f,mode,body)
torch.serialization.default_restore_location(storage,location)
torch.serialization.load(f,map_location=None,pickle_module=pickle)
torch.serialization.location_tag(storage)
torch.serialization.mkdtemp()
torch.serialization.normalize_storage_type(storage_type)
torch.serialization.register_package(priority,tagger,deserializer)
torch.serialization.save(obj,f,pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL)
torch.serialization.storage_to_tensor_type(storage)
torch.serialization.validate_cuda_device(location)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_torch_docs.py----------------------------------------
A:torch._torch_docs.regx->re.compile('\\n\\s{4}(?!\\s)')
A:torch._torch_docs.reduceops_common_args->parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n')
A:torch._torch_docs.factory_common_args->parse_kwargs('\n    out (Tensor, optional): the output tensor\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n')
A:torch._torch_docs.factory_like_common_args->parse_kwargs('\n    input (Tensor): the size of :attr:`input` will determine size of the output tensor\n    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n        Default: if ``None``, defaults to the layout of :attr:`input`.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n        Default: if ``None``, defaults to the dtype of :attr:`input`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, defaults to the device of :attr:`input`.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n')
A:torch._torch_docs.factory_data_common_args->parse_kwargs('\n    data (array_like): Initial data for the tensor. Can be a list, tuple,\n        NumPy ``ndarray``, scalar, and other types.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, infers data type from :attr:`data`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n')
torch._torch_docs.parse_kwargs(desc)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_storage_docs.py----------------------------------------
A:torch._storage_docs.cls->getattr(torch._C, cls_name)
torch._storage_docs.add_docstr_all(method,docstr)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/storage.py----------------------------------------
A:torch.storage.memo->memo.setdefault('torch', {}).setdefault('torch', {})
A:torch.storage.new_storage->self.clone()
A:torch.storage.b->io.BytesIO()
A:torch.storage.allocator->torch.cuda._host_allocator()
torch._StorageBase(object)
torch._StorageBase.__copy__(self)
torch._StorageBase.__deepcopy__(self,memo)
torch._StorageBase.__iter__(self)
torch._StorageBase.__reduce__(self)
torch._StorageBase.__repr__(self)
torch._StorageBase.__sizeof__(self)
torch._StorageBase.__str__(self)
torch._StorageBase._new_shared(cls,size)
torch._StorageBase.byte(self)
torch._StorageBase.char(self)
torch._StorageBase.clone(self)
torch._StorageBase.cpu(self)
torch._StorageBase.double(self)
torch._StorageBase.float(self)
torch._StorageBase.half(self)
torch._StorageBase.int(self)
torch._StorageBase.long(self)
torch._StorageBase.pin_memory(self)
torch._StorageBase.share_memory_(self)
torch._StorageBase.short(self)
torch._StorageBase.tolist(self)
torch.storage._StorageBase(object)
torch.storage._StorageBase.__copy__(self)
torch.storage._StorageBase.__deepcopy__(self,memo)
torch.storage._StorageBase.__iter__(self)
torch.storage._StorageBase.__reduce__(self)
torch.storage._StorageBase.__repr__(self)
torch.storage._StorageBase.__sizeof__(self)
torch.storage._StorageBase.__str__(self)
torch.storage._StorageBase._new_shared(cls,size)
torch.storage._StorageBase.byte(self)
torch.storage._StorageBase.char(self)
torch.storage._StorageBase.clone(self)
torch.storage._StorageBase.cpu(self)
torch.storage._StorageBase.double(self)
torch.storage._StorageBase.float(self)
torch.storage._StorageBase.half(self)
torch.storage._StorageBase.int(self)
torch.storage._StorageBase.long(self)
torch.storage._StorageBase.pin_memory(self)
torch.storage._StorageBase.share_memory_(self)
torch.storage._StorageBase.short(self)
torch.storage._StorageBase.tolist(self)
torch.storage._load_from_bytes(b)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/random.py----------------------------------------
A:torch.random.seed->int(seed)
A:torch.random.num_devices->torch.cuda.device_count()
A:torch.random.devices->list(devices)
A:torch.random.cpu_rng_state->torch.get_rng_state()
torch.get_rng_state()
torch.initial_seed()
torch.manual_seed(seed)
torch.random.fork_rng(devices=None,enabled=True,_caller='fork_rng',_devices_kw='devices')
torch.random.get_rng_state()
torch.random.initial_seed()
torch.random.manual_seed(seed)
torch.random.set_rng_state(new_state)
torch.set_rng_state(new_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_tensor_docs.py----------------------------------------
A:torch._tensor_docs.new_common_args->parse_kwargs('\n    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n        shape of the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n        Default: if None, same :class:`torch.dtype` as this tensor.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if None, same :class:`torch.device` as this tensor.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n')
torch._tensor_docs.add_docstr_all(method,docstr)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_ops.py----------------------------------------
A:torch._ops.old_flags->sys.getdlopenflags()
A:torch._ops.qualified_op_name->'{}::{}'.format(self.name, op_name)
A:torch._ops.op->torch._C._jit_get_operation(qualified_op_name)
A:torch._ops.self.loaded_libraries->set()
A:torch._ops.namespace->_OpNamespace(name)
A:torch._ops.path->os.path.realpath(path)
A:torch._ops.ops->_Ops()
torch._ops._OpNamespace(self,name)
torch._ops._OpNamespace.__getattr__(self,op_name)
torch._ops._OpNamespace.__init__(self,name)
torch._ops._Ops(self)
torch._ops._Ops.__getattr__(self,name)
torch._ops._Ops.__init__(self)
torch._ops._Ops.load_library(self,path)
torch._ops.dl_open_guard()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/backends/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/backends/mkl/__init__.py----------------------------------------
torch.backends.mkl.__init__.is_available()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/backends/cuda/__init__.py----------------------------------------
A:torch.backends.cuda.__init__.size->ContextProp(torch._cufft_get_plan_cache_size, 'cufft_plan_cache.size is a read-only property showing the current cache. To set the cache capacity, use cufft_plan_cache.max_size.')
A:torch.backends.cuda.__init__.max_size->ContextProp(torch._cufft_get_plan_cache_max_size, torch._cufft_set_plan_cache_max_size)
A:torch.backends.cuda.__init__.cufft_plan_cache->cuFFTPlanCache()
A:torch.backends.cuda.__init__.sys.modules[__name__]->CUDAModule(sys.modules[__name__])
torch.backends.cuda.__init__.CUDAModule(self,m)
torch.backends.cuda.__init__.CUDAModule.__init__(self,m)
torch.backends.cuda.__init__.ContextProp(self,getter,setter)
torch.backends.cuda.__init__.ContextProp.__get__(self,obj,objtype)
torch.backends.cuda.__init__.ContextProp.__init__(self,getter,setter)
torch.backends.cuda.__init__.ContextProp.__set__(self,obj,val)
torch.backends.cuda.__init__.cuFFTPlanCache(object)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py----------------------------------------
A:torch.backends.cudnn.rnn.dropout_state[dropout_desc_name]->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda')))
A:torch.backends.cudnn.rnn.dropout_ts->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda'))).get()
torch.backends.cudnn.rnn.Unserializable(self,inner)
torch.backends.cudnn.rnn.Unserializable.__getstate__(self)
torch.backends.cudnn.rnn.Unserializable.__init__(self,inner)
torch.backends.cudnn.rnn.Unserializable.__setstate__(self,state)
torch.backends.cudnn.rnn.Unserializable.get(self)
torch.backends.cudnn.rnn.get_cudnn_mode(mode)
torch.backends.cudnn.rnn.init_dropout_state(dropout,train,dropout_seed,dropout_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py----------------------------------------
A:torch.backends.cudnn.__init__.proc->Popen(['where', 'cudnn64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE)
A:torch.backends.cudnn.__init__.(out, err)->Popen(['where', 'cudnn64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE).communicate()
A:torch.backends.cudnn.__init__.out->out.decode().strip().decode().strip()
A:torch.backends.cudnn.__init__.cudnn_lib_name->os.path.basename(out)
A:torch.backends.cudnn.__init__.cudnn_lib->str(cudnn_lib)
A:torch.backends.cudnn.__init__.lib->ctypes.cdll.LoadLibrary(None)
A:torch.backends.cudnn.__init__.__cudnn_version->ctypes.cdll.LoadLibrary(None).cudnnGetVersion()
A:torch.backends.cudnn.__init__.compile_version->torch._C._cudnn_version()
A:torch.backends.cudnn.__init__.orig_flags->set_flags(enabled, benchmark, deterministic, verbose)
A:torch.backends.cudnn.__init__.ptr->ctypes.c_void_p()
A:torch.backends.cudnn.__init__.msg->'{}: {}'.format(status, get_error_string(status))
A:torch.backends.cudnn.__init__.self._type->tensor.view(padded_size).type()
A:torch.backends.cudnn.__init__.self._size->weight.size()
A:torch.backends.cudnn.__init__.self._stride->tensor.view(padded_size).stride()
A:torch.backends.cudnn.__init__.self.ptrs->(ctypes.c_void_p * N)()
A:torch.backends.cudnn.__init__._ndim->tensor.view(padded_size).dim()
A:torch.backends.cudnn.__init__._size->int_array(tensor.size() + dim_pad)
A:torch.backends.cudnn.__init__._stride->int_array(tensor.stride() + dim_pad)
A:torch.backends.cudnn.__init__.dropout_states_size->ctypes.c_long()
A:torch.backends.cudnn.__init__.self.state->torch.cuda.ByteTensor(dropout_states_size.value)
A:torch.backends.cudnn.__init__.state_ptr->self.state.data_ptr()
A:torch.backends.cudnn.__init__.state_size->self.state.size(0)
A:torch.backends.cudnn.__init__.current_device->torch.cuda.current_device()
A:torch.backends.cudnn.__init__.handle->CuDNNHandle()
A:torch.backends.cudnn.__init__.tensor->tensor.view(padded_size).view(padded_size)
A:torch.backends.cudnn.__init__.descriptor->TensorDescriptor()
A:torch.backends.cudnn.__init__.descriptors->TensorDescriptorArray(len(batch_sizes))
A:torch.backends.cudnn.__init__.enabled->ContextProp(torch._C._get_cudnn_enabled, torch._C._set_cudnn_enabled)
A:torch.backends.cudnn.__init__.deterministic->ContextProp(torch._C._get_cudnn_deterministic, torch._C._set_cudnn_deterministic)
A:torch.backends.cudnn.__init__.benchmark->ContextProp(torch._C._get_cudnn_benchmark, torch._C._set_cudnn_benchmark)
A:torch.backends.cudnn.__init__.sys.modules[__name__]->CudnnModule(sys.modules[__name__], __name__)
torch.backends.cudnn.__init__.ContextProp(self,getter,setter)
torch.backends.cudnn.__init__.ContextProp.__get__(self,obj,objtype)
torch.backends.cudnn.__init__.ContextProp.__init__(self,getter,setter)
torch.backends.cudnn.__init__.ContextProp.__set__(self,obj,val)
torch.backends.cudnn.__init__.CuDNNError(self,status)
torch.backends.cudnn.__init__.CuDNNError.__init__(self,status)
torch.backends.cudnn.__init__.CuDNNHandle(self)
torch.backends.cudnn.__init__.CuDNNHandle.__del__(self)
torch.backends.cudnn.__init__.CuDNNHandle.__init__(self)
torch.backends.cudnn.__init__.CudnnModule(self,m,name)
torch.backends.cudnn.__init__.CudnnModule.__getattr__(self,attr)
torch.backends.cudnn.__init__.CudnnModule.__init__(self,m,name)
torch.backends.cudnn.__init__.DropoutDescriptor(self,handle,dropout,seed)
torch.backends.cudnn.__init__.DropoutDescriptor.__del__(self)
torch.backends.cudnn.__init__.DropoutDescriptor.__init__(self,handle,dropout,seed)
torch.backends.cudnn.__init__.DropoutDescriptor._set(self,dropout,seed)
torch.backends.cudnn.__init__.DropoutDescriptor.set_dropout(self,dropout,seed)
torch.backends.cudnn.__init__.FilterDescriptor(self)
torch.backends.cudnn.__init__.FilterDescriptor.__del__(self)
torch.backends.cudnn.__init__.FilterDescriptor.__init__(self)
torch.backends.cudnn.__init__.FilterDescriptor.as_tuple(self)
torch.backends.cudnn.__init__.FilterDescriptor.set(self,weight)
torch.backends.cudnn.__init__.RNNDescriptor(self,handle,hidden_size,num_layers,dropout_desc,input_mode,bidirectional,mode,datatype)
torch.backends.cudnn.__init__.RNNDescriptor.__del__(self)
torch.backends.cudnn.__init__.RNNDescriptor.__init__(self,handle,hidden_size,num_layers,dropout_desc,input_mode,bidirectional,mode,datatype)
torch.backends.cudnn.__init__.TensorDescriptor(self)
torch.backends.cudnn.__init__.TensorDescriptor.__del__(self)
torch.backends.cudnn.__init__.TensorDescriptor.__init__(self)
torch.backends.cudnn.__init__.TensorDescriptor.as_tuple(self)
torch.backends.cudnn.__init__.TensorDescriptor.set(self,tensor)
torch.backends.cudnn.__init__.TensorDescriptorArray(self,N)
torch.backends.cudnn.__init__.TensorDescriptorArray.__del__(self)
torch.backends.cudnn.__init__.TensorDescriptorArray.__getitem__(self,key)
torch.backends.cudnn.__init__.TensorDescriptorArray.__init__(self,N)
torch.backends.cudnn.__init__.TensorDescriptorArray.set_all(self,tensor)
torch.backends.cudnn.__init__.TensorDescriptorArray.set_raw(self,i,_type,_ndim,_size,_stride)
torch.backends.cudnn.__init__.__allow_nonbracketed_mutation()
torch.backends.cudnn.__init__._libcudnn()
torch.backends.cudnn.__init__.add_tensor(*args)
torch.backends.cudnn.__init__.c_type(tensor)
torch.backends.cudnn.__init__.check_error(status)
torch.backends.cudnn.__init__.descriptor(tensor,N=None)
torch.backends.cudnn.__init__.descriptor_sequence(tensor,batch_sizes)
torch.backends.cudnn.__init__.disable_global_flags()
torch.backends.cudnn.__init__.find_cudnn_windows_lib()
torch.backends.cudnn.__init__.flags(enabled=False,benchmark=False,deterministic=False,verbose=False)
torch.backends.cudnn.__init__.flags_frozen()
torch.backends.cudnn.__init__.get_error_string(status)
torch.backends.cudnn.__init__.get_handle()
torch.backends.cudnn.__init__.int_array(itr)
torch.backends.cudnn.__init__.is_acceptable(tensor)
torch.backends.cudnn.__init__.is_available()
torch.backends.cudnn.__init__.set_flags(_enabled,_benchmark,_deterministic,_verbose)
torch.backends.cudnn.__init__.version()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/optimizer.py----------------------------------------
A:torch.optim.optimizer.required->_RequiredParameter()
A:torch.optim.optimizer.self.state->defaultdict(dict)
A:torch.optim.optimizer.param_groups->list(params)
A:torch.optim.optimizer.state_dict->deepcopy(state_dict)
A:torch.optim.optimizer.value->value.to(param.device).to(param.device)
A:torch.optim.optimizer.state->defaultdict(dict)
A:torch.optim.optimizer.state[param]->cast(param, v)
A:torch.optim.optimizer.param_group['params']->list(params)
A:torch.optim.optimizer.param_set->set()
torch.optim.Optimizer(self,params,defaults)
torch.optim.Optimizer.__getstate__(self)
torch.optim.Optimizer.__repr__(self)
torch.optim.Optimizer.__setstate__(self,state)
torch.optim.Optimizer.add_param_group(self,param_group)
torch.optim.Optimizer.load_state_dict(self,state_dict)
torch.optim.Optimizer.state_dict(self)
torch.optim.Optimizer.step(self,closure)
torch.optim.Optimizer.zero_grad(self)
torch.optim.optimizer.Optimizer(self,params,defaults)
torch.optim.optimizer.Optimizer.__getstate__(self)
torch.optim.optimizer.Optimizer.__init__(self,params,defaults)
torch.optim.optimizer.Optimizer.__repr__(self)
torch.optim.optimizer.Optimizer.__setstate__(self,state)
torch.optim.optimizer.Optimizer.add_param_group(self,param_group)
torch.optim.optimizer.Optimizer.load_state_dict(self,state_dict)
torch.optim.optimizer.Optimizer.state_dict(self)
torch.optim.optimizer.Optimizer.step(self,closure)
torch.optim.optimizer.Optimizer.zero_grad(self)
torch.optim.optimizer._RequiredParameter(object)
torch.optim.optimizer._RequiredParameter.__repr__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/rprop.py----------------------------------------
A:torch.optim.rprop.defaults->dict(lr=lr, etas=etas, step_sizes=step_sizes)
A:torch.optim.rprop.loss->closure()
A:torch.optim.rprop.state['prev']->torch.zeros_like(p.data)
A:torch.optim.rprop.state['step_size']->grad.clone().new().resize_as_(grad).fill_(group['lr'])
A:torch.optim.rprop.sign->grad.clone().mul(state['prev']).sign()
A:torch.optim.rprop.grad->grad.clone().clone()
torch.optim.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.Rprop.step(self,closure=None)
torch.optim.rprop.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.__init__(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/adagrad.py----------------------------------------
A:torch.optim.adagrad.defaults->dict(lr=lr, lr_decay=lr_decay, weight_decay=weight_decay, initial_accumulator_value=initial_accumulator_value)
A:torch.optim.adagrad.state['sum']->torch.full_like(p.data, initial_accumulator_value)
A:torch.optim.adagrad.loss->closure()
A:torch.optim.adagrad.grad->grad.coalesce().coalesce()
A:torch.optim.adagrad.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim.adagrad.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim.adagrad.size->grad.coalesce().coalesce().size()
A:torch.optim.adagrad.std->torch.full_like(p.data, initial_accumulator_value).sqrt().add_(1e-10)
A:torch.optim.adagrad.std_values->torch.full_like(p.data, initial_accumulator_value).sqrt().add_(1e-10)._values().sqrt_().add_(1e-10)
torch.optim.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0)
torch.optim.Adagrad.share_memory(self)
torch.optim.Adagrad.step(self,closure=None)
torch.optim.adagrad.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0)
torch.optim.adagrad.Adagrad.__init__(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0)
torch.optim.adagrad.Adagrad.share_memory(self)
torch.optim.adagrad.Adagrad.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/asgd.py----------------------------------------
A:torch.optim.asgd.defaults->dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0, weight_decay=weight_decay)
A:torch.optim.asgd.loss->closure()
A:torch.optim.asgd.state['ax']->torch.zeros_like(p.data)
A:torch.optim.asgd.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
torch.optim.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.ASGD.step(self,closure=None)
torch.optim.asgd.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.__init__(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/sgd.py----------------------------------------
A:torch.optim.sgd.defaults->dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov)
A:torch.optim.sgd.loss->closure()
A:torch.optim.sgd.bufparam_state['momentum_buffer']->torch.zeros_like(p.data)
A:torch.optim.sgd.d_p->d_p.add(momentum, buf).add(momentum, buf)
torch.optim.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.SGD.__setstate__(self,state)
torch.optim.SGD.step(self,closure=None)
torch.optim.sgd.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.sgd.SGD.__init__(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.sgd.SGD.__setstate__(self,state)
torch.optim.sgd.SGD.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/lr_scheduler.py----------------------------------------
A:torch.optim.lr_scheduler.self.base_lrs->list(map(lambda group: group['initial_lr'], optimizer.param_groups))
A:torch.optim.lr_scheduler.self.lr_lambdas->list(lr_lambda)
A:torch.optim.lr_scheduler.state_dict['lr_lambdas'][idx]->fn.__dict__.copy()
A:torch.optim.lr_scheduler.lr_lambdas->state_dict.pop('lr_lambdas')
A:torch.optim.lr_scheduler.self.min_lrs->list(min_lr)
A:torch.optim.lr_scheduler.old_lr->float(param_group['lr'])
A:torch.optim.lr_scheduler.new_lr->max(old_lr * self.factor, self.min_lrs[i])
A:torch.optim.lr_scheduler.self.is_better->partial(self._cmp, mode, threshold_mode, threshold)
torch.optim.lr_scheduler.CosineAnnealingLR(self,optimizer,T_max,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingLR.__init__(self,optimizer,T_max,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingLR.get_lr(self)
torch.optim.lr_scheduler.ExponentialLR(self,optimizer,gamma,last_epoch=-1)
torch.optim.lr_scheduler.ExponentialLR.__init__(self,optimizer,gamma,last_epoch=-1)
torch.optim.lr_scheduler.ExponentialLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.LambdaLR.__init__(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.LambdaLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.LambdaLR.state_dict(self)
torch.optim.lr_scheduler.MultiStepLR(self,optimizer,milestones,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.MultiStepLR.__init__(self,optimizer,milestones,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.MultiStepLR.get_lr(self)
torch.optim.lr_scheduler.ReduceLROnPlateau(self,optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)
torch.optim.lr_scheduler.ReduceLROnPlateau.__init__(self,optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)
torch.optim.lr_scheduler.ReduceLROnPlateau._cmp(self,mode,threshold_mode,threshold,a,best)
torch.optim.lr_scheduler.ReduceLROnPlateau._init_is_better(self,mode,threshold,threshold_mode)
torch.optim.lr_scheduler.ReduceLROnPlateau._reduce_lr(self,epoch)
torch.optim.lr_scheduler.ReduceLROnPlateau._reset(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.in_cooldown(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.ReduceLROnPlateau.state_dict(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.step(self,metrics,epoch=None)
torch.optim.lr_scheduler.StepLR(self,optimizer,step_size,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.StepLR.__init__(self,optimizer,step_size,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.StepLR.get_lr(self)
torch.optim.lr_scheduler._LRScheduler(self,optimizer,last_epoch=-1)
torch.optim.lr_scheduler._LRScheduler.__init__(self,optimizer,last_epoch=-1)
torch.optim.lr_scheduler._LRScheduler.get_lr(self)
torch.optim.lr_scheduler._LRScheduler.load_state_dict(self,state_dict)
torch.optim.lr_scheduler._LRScheduler.state_dict(self)
torch.optim.lr_scheduler._LRScheduler.step(self,epoch=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/lbfgs.py----------------------------------------
A:torch.optim.lbfgs.defaults->dict(lr=lr, max_iter=max_iter, max_eval=max_eval, tolerance_grad=tolerance_grad, tolerance_change=tolerance_change, history_size=history_size, line_search_fn=line_search_fn)
A:torch.optim.lbfgs.self._numel_cache->reduce(lambda total, p: total + p.numel(), self._params, 0)
A:torch.optim.lbfgs.view->p.grad.data.view(-1)
A:torch.optim.lbfgs.numel->p.numel()
A:torch.optim.lbfgs.orig_loss->closure()
A:torch.optim.lbfgs.loss->float(closure())
A:torch.optim.lbfgs.flat_grad->self._gather_flat_grad()
A:torch.optim.lbfgs.abs_grad_sum->self._gather_flat_grad().abs().sum()
A:torch.optim.lbfgs.d->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.t->state.get('t')
A:torch.optim.lbfgs.old_dirs->state.get('old_dirs')
A:torch.optim.lbfgs.old_stps->state.get('old_stps')
A:torch.optim.lbfgs.H_diag->state.get('H_diag')
A:torch.optim.lbfgs.prev_flat_grad->self._gather_flat_grad().clone()
A:torch.optim.lbfgs.prev_loss->state.get('prev_loss')
A:torch.optim.lbfgs.y->self._gather_flat_grad().sub(prev_flat_grad)
A:torch.optim.lbfgs.s->self._gather_flat_grad().neg().mul(t)
A:torch.optim.lbfgs.ys->self._gather_flat_grad().sub(prev_flat_grad).dot(s)
A:torch.optim.lbfgs.num_old->len(old_dirs)
A:torch.optim.lbfgs.q->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.dr->torch.mul(q, H_diag)
A:torch.optim.lbfgs.gtd->self._gather_flat_grad().dot(d)
torch.optim.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-05,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.LBFGS._add_grad(self,step_size,update)
torch.optim.LBFGS._gather_flat_grad(self)
torch.optim.LBFGS._numel(self)
torch.optim.LBFGS.step(self,closure)
torch.optim.lbfgs.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-05,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS.__init__(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-05,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS._add_grad(self,step_size,update)
torch.optim.lbfgs.LBFGS._gather_flat_grad(self)
torch.optim.lbfgs.LBFGS._numel(self)
torch.optim.lbfgs.LBFGS.step(self,closure)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/sparse_adam.py----------------------------------------
A:torch.optim.sparse_adam.defaults->dict(lr=lr, betas=betas, eps=eps)
A:torch.optim.sparse_adam.loss->closure()
A:torch.optim.sparse_adam.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.sparse_adam.state['exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.sparse_adam.grad->grad.coalesce().coalesce()
A:torch.optim.sparse_adam.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim.sparse_adam.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim.sparse_adam.size->grad.coalesce().coalesce().size()
A:torch.optim.sparse_adam.old_exp_avg_values->exp_avg.sparse_mask(grad)._values()
A:torch.optim.sparse_adam.exp_avg_update_values->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1)
A:torch.optim.sparse_adam.old_exp_avg_sq_values->exp_avg_sq.sparse_mask(grad)._values()
A:torch.optim.sparse_adam.exp_avg_sq_update_values->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)
A:torch.optim.sparse_adam.numer->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1).add_(old_exp_avg_values)
A:torch.optim.sparse_adam.denom->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2).sqrt_().add_(group['eps'])
torch.optim.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.SparseAdam.step(self,closure=None)
torch.optim.sparse_adam.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/adamax.py----------------------------------------
A:torch.optim.adamax.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
A:torch.optim.adamax.loss->closure()
A:torch.optim.adamax.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.adamax.state['exp_inf']->torch.zeros_like(p.data)
A:torch.optim.adamax.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
A:torch.optim.adamax.norm_buf->torch.cat([exp_inf.mul_(beta2).unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)
torch.optim.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.Adamax.step(self,closure=None)
torch.optim.adamax.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/rmsprop.py----------------------------------------
A:torch.optim.rmsprop.defaults->dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)
A:torch.optim.rmsprop.loss->closure()
A:torch.optim.rmsprop.state['square_avg']->torch.zeros_like(p.data)
A:torch.optim.rmsprop.state['momentum_buffer']->torch.zeros_like(p.data)
A:torch.optim.rmsprop.state['grad_avg']->torch.zeros_like(p.data)
A:torch.optim.rmsprop.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
A:torch.optim.rmsprop.avg->square_avg.sqrt().add_(group['eps'])
torch.optim.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.RMSprop.__setstate__(self,state)
torch.optim.RMSprop.step(self,closure=None)
torch.optim.rmsprop.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__init__(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__setstate__(self,state)
torch.optim.rmsprop.RMSprop.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/adadelta.py----------------------------------------
A:torch.optim.adadelta.defaults->dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)
A:torch.optim.adadelta.loss->closure()
A:torch.optim.adadelta.state['square_avg']->torch.zeros_like(p.data)
A:torch.optim.adadelta.state['acc_delta']->torch.zeros_like(p.data)
A:torch.optim.adadelta.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
A:torch.optim.adadelta.std->square_avg.add(eps).sqrt_()
A:torch.optim.adadelta.delta->acc_delta.add(eps).sqrt_().div_(std).mul_(grad)
torch.optim.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.Adadelta.step(self,closure=None)
torch.optim.adadelta.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.__init__(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/optim/adam.py----------------------------------------
A:torch.optim.adam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)
A:torch.optim.adam.loss->closure()
A:torch.optim.adam.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.adam.state['exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.adam.state['max_exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.adam.denom->exp_avg_sq.sqrt().add_(group['eps'])
torch.optim.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.Adam.__setstate__(self,state)
torch.optim.Adam.step(self,closure=None)
torch.optim.adam.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.adam.Adam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.adam.Adam.__setstate__(self,state)
torch.optim.adam.Adam.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_thnn/__init__.py----------------------------------------
A:torch._thnn.__init__.self.loading_lock->threading.Lock()
A:torch._thnn.__init__.lib->getattr(torch._C, self.lib_name)
A:torch._thnn.__init__.self.backend->load_backend(self.lib_prefix, lib, self.functions, self.mixins)
A:torch._thnn.__init__.type2backend->Backends()
A:torch._thnn.__init__._thnn_headers->parse_header(THNN_H_PATH)
A:torch._thnn.__init__._thcunn_headers->parse_header(THCUNN_H_PATH)
A:torch._thnn.__init__.backend->Backend('Cuda' + t, '_THCUNN', _thcunn_headers, (THNNCudaBackendStateMixin,))
torch._thnn.__init__.Backend(self,lib_prefix,lib_name,functions,mixins=tuple())
torch._thnn.__init__.Backend.__init__(self,lib_prefix,lib_name,functions,mixins=tuple())
torch._thnn.__init__.Backend.load(self)
torch._thnn.__init__.Backends(self)
torch._thnn.__init__.Backends.__getattr__(self,name)
torch._thnn.__init__.Backends.__getitem__(self,name)
torch._thnn.__init__.Backends.__init__(self)
torch._thnn.__init__.THNNCudaBackendStateMixin(object)
torch._thnn.__init__.THNNCudaBackendStateMixin.library_state(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/_thnn/utils.py----------------------------------------
A:torch._thnn.utils.THNN_H_PATH->get_file_path('torch', 'lib', 'THNN.h')
A:torch._thnn.utils.THCUNN_H_PATH->get_file_path('torch', 'lib', 'THCUNN.h')
A:torch._thnn.utils.method->self.methods.get(name, None)
A:torch._thnn.utils.lines->filter(lambda l: l[0], lines)
A:torch._thnn.utils.fn_name->l.lstrip('THC_API void THNN_')
A:torch._thnn.utils.(t, name)->l.split()
A:torch._thnn.utils.backend_name->'THNN{}Backend'.format(t)
A:torch._thnn.utils.backend->type(backend_name, mixins + (THNNBackendBase,), {})()
A:torch._thnn.utils.full_fn_name->'{}{}'.format(t, function.name)
A:torch._thnn.utils.fn->getattr(lib, full_fn_name)
torch._thnn.load_backend(t,lib,generic_functions,mixins=tuple())
torch._thnn.parse_header(path)
torch._thnn.utils.Argument(self,_type,name,is_optional)
torch._thnn.utils.Argument.__init__(self,_type,name,is_optional)
torch._thnn.utils.Argument.__repr__(self)
torch._thnn.utils.Function(self,name)
torch._thnn.utils.Function.__init__(self,name)
torch._thnn.utils.Function.__repr__(self)
torch._thnn.utils.Function.add_argument(self,arg)
torch._thnn.utils.THNNBackendBase(self)
torch._thnn.utils.THNNBackendBase.__getattr__(self,name)
torch._thnn.utils.THNNBackendBase.__init__(self)
torch._thnn.utils.THNNBackendBase.__reduce__(self)
torch._thnn.utils.THNNBackendBase.library_state(self)
torch._thnn.utils.THNNBackendBase.register_method(self,name,ctypes_fn)
torch._thnn.utils._unpickle_backend(backend_name)
torch._thnn.utils.load_backend(t,lib,generic_functions,mixins=tuple())
torch._thnn.utils.parse_header(path)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/sparse/__init__.py----------------------------------------
torch.sparse.__init__.addmm(mat,mat1,mat2,beta=1,alpha=1)
torch.sparse.__init__.mm(mat1,mat2)
torch.sparse.__init__.sum(input,dim=None,dtype=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/streams.py----------------------------------------
A:torch.cuda.streams.event->Event()
A:torch.cuda.streams.res->cudart().cudaEventQuery(self)
A:torch.cuda.streams.least_priority->ctypes.c_int()
A:torch.cuda.streams.greatest_priority->ctypes.c_int()
A:torch.cuda.streams.priority->ctypes.c_int()
A:torch.cuda.streams.ptr->ctypes.c_void_p()
A:torch.cuda.streams.self._cudart->cudart()
A:torch.cuda.streams.stream->torch.cuda.current_stream()
A:torch.cuda.streams.time_ms->ctypes.c_float()
A:torch.cuda.streams.handle->EventHandle()
torch.cuda.Event(self,enable_timing=False,blocking=False,interprocess=False,_handle=None)
torch.cuda.Event.__del__(self)
torch.cuda.Event.__repr__(self)
torch.cuda.Event.elapsed_time(self,end_event)
torch.cuda.Event.ipc_handle(self)
torch.cuda.Event.query(self)
torch.cuda.Event.record(self,stream=None)
torch.cuda.Event.synchronize(self)
torch.cuda.Event.wait(self,stream=None)
torch.cuda.EventHandle(ctypes.Structure)
torch.cuda.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.Stream.__eq__(self,o)
torch.cuda.Stream.__hash__(self)
torch.cuda.Stream.__repr__(self)
torch.cuda.Stream._as_parameter_(self)
torch.cuda.Stream.priority(self)
torch.cuda.Stream.priority_range()
torch.cuda.Stream.query(self)
torch.cuda.Stream.record_event(self,event=None)
torch.cuda.Stream.synchronize(self)
torch.cuda.Stream.wait_event(self,event)
torch.cuda.Stream.wait_stream(self,stream)
torch.cuda.streams.Event(self,enable_timing=False,blocking=False,interprocess=False,_handle=None)
torch.cuda.streams.Event.__del__(self)
torch.cuda.streams.Event.__init__(self,enable_timing=False,blocking=False,interprocess=False,_handle=None)
torch.cuda.streams.Event.__repr__(self)
torch.cuda.streams.Event.elapsed_time(self,end_event)
torch.cuda.streams.Event.ipc_handle(self)
torch.cuda.streams.Event.query(self)
torch.cuda.streams.Event.record(self,stream=None)
torch.cuda.streams.Event.synchronize(self)
torch.cuda.streams.Event.wait(self,stream=None)
torch.cuda.streams.EventHandle(ctypes.Structure)
torch.cuda.streams.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__eq__(self,o)
torch.cuda.streams.Stream.__hash__(self)
torch.cuda.streams.Stream.__new__(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__repr__(self)
torch.cuda.streams.Stream._as_parameter_(self)
torch.cuda.streams.Stream.priority(self)
torch.cuda.streams.Stream.priority_range()
torch.cuda.streams.Stream.query(self)
torch.cuda.streams.Stream.record_event(self,event=None)
torch.cuda.streams.Stream.synchronize(self)
torch.cuda.streams.Stream.wait_event(self,event)
torch.cuda.streams.Stream.wait_stream(self,stream)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/__init__.py----------------------------------------
A:torch.cuda.__init__.proc->Popen(['where', 'cudart64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE)
A:torch.cuda.__init__.(out, err)->Popen(['where', 'cudart64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE).communicate()
A:torch.cuda.__init__.out->out.decode().strip().decode().strip()
A:torch.cuda.__init__.cuda_lib_name->os.path.basename(out)
A:torch.cuda.__init__.cuda_lib->str(cuda_lib)
A:torch.cuda.__init__.lib->ctypes.cdll.LoadLibrary(None)
A:torch.cuda.__init__.CUDA_VERSION->torch._C._cuda_getCompiledVersion()
A:torch.cuda.__init__.capability->get_device_capability(d)
A:torch.cuda.__init__.name->get_device_name(d)
A:torch.cuda.__init__._cudart->_load_cudart()
A:torch.cuda.__init__._original_pid->os.getpid()
A:torch.cuda.__init__.msg->cudart().cudaGetErrorString(code).decode('utf-8')
A:torch.cuda.__init__.self.idx->_get_device_index(device, optional=True)
A:torch.cuda.__init__.self.prev_idx->torch._C._cuda_getDevice()
A:torch.cuda.__init__.device->_get_device_index(device, optional=True)
A:torch.cuda.__init__.prop->get_device_properties(device)
A:torch.cuda.__init__.prev_stream->current_stream()
A:torch.cuda.__init__.storage_name->'Cuda{0}StorageBase'.format(t)
A:torch.cuda.__init__.tensor_name->'Cuda{0}TensorBase'.format(t)
A:torch.cuda.__init__.torch._C.__dict__[storage_name]->_dummy_type(storage_name)
A:torch.cuda.__init__.torch._C.__dict__[tensor_name]->_dummy_type(tensor_name)
A:torch.cuda.__init__.torch._C.__dict__['_CudaStreamBase']->_dummy_type('CudaStreamBase')
torch.cuda.__init__.ByteStorage(_CudaBase,torch._C.CudaByteStorageBase,_StorageBase)
torch.cuda.__init__.CharStorage(_CudaBase,torch._C.CudaCharStorageBase,_StorageBase)
torch.cuda.__init__.CudaError(self,code)
torch.cuda.__init__.CudaError.__init__(self,code)
torch.cuda.__init__.DeferredCudaCallError(Exception)
torch.cuda.__init__.DoubleStorage(_CudaBase,torch._C.CudaDoubleStorageBase,_StorageBase)
torch.cuda.__init__.FloatStorage(_CudaBase,torch._C.CudaFloatStorageBase,_StorageBase)
torch.cuda.__init__.HalfStorage(_CudaBase,torch._C.CudaHalfStorageBase,_StorageBase)
torch.cuda.__init__.IntStorage(_CudaBase,torch._C.CudaIntStorageBase,_StorageBase)
torch.cuda.__init__.LongStorage(_CudaBase,torch._C.CudaLongStorageBase,_StorageBase)
torch.cuda.__init__.ShortStorage(_CudaBase,torch._C.CudaShortStorageBase,_StorageBase)
torch.cuda.__init__._CudaBase(object)
torch.cuda.__init__._CudaBase.type(self,*args,**kwargs)
torch.cuda.__init__._after_fork(arg)
torch.cuda.__init__._check_capability()
torch.cuda.__init__._check_driver()
torch.cuda.__init__._dummy_type(name)
torch.cuda.__init__._free_mutex()
torch.cuda.__init__._host_allocator()
torch.cuda.__init__._lazy_call(callable)
torch.cuda.__init__._lazy_init()
torch.cuda.__init__._lazy_new(cls,*args,**kwargs)
torch.cuda.__init__._load_cudart()
torch.cuda.__init__._sleep(cycles)
torch.cuda.__init__.check_error(res)
torch.cuda.__init__.cudaStatus(object)
torch.cuda.__init__.cudart()
torch.cuda.__init__.current_blas_handle()
torch.cuda.__init__.current_device()
torch.cuda.__init__.current_stream()
torch.cuda.__init__.device(self,device)
torch.cuda.__init__.device.__enter__(self)
torch.cuda.__init__.device.__exit__(self,*args)
torch.cuda.__init__.device.__init__(self,device)
torch.cuda.__init__.device_count()
torch.cuda.__init__.device_of(self,obj)
torch.cuda.__init__.device_of.__init__(self,obj)
torch.cuda.__init__.empty_cache()
torch.cuda.__init__.find_cuda_windows_lib()
torch.cuda.__init__.get_device_capability(device)
torch.cuda.__init__.get_device_name(device)
torch.cuda.__init__.get_device_properties(device)
torch.cuda.__init__.init()
torch.cuda.__init__.is_available()
torch.cuda.__init__.max_memory_allocated(device=None)
torch.cuda.__init__.max_memory_cached(device=None)
torch.cuda.__init__.memory_allocated(device=None)
torch.cuda.__init__.memory_cached(device=None)
torch.cuda.__init__.set_device(device)
torch.cuda.__init__.stream(stream)
torch.cuda.__init__.synchronize()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/sparse.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/nvtx.py----------------------------------------
A:torch.cuda.nvtx.lib_path->windows_nvToolsExt_path()
A:torch.cuda.nvtx.lib_name->os.path.basename(lib_path)
A:torch.cuda.nvtx.NVTOOLEXT_HOME->os.getenv('NVTOOLSEXT_PATH', WINDOWS_HOME)
A:torch.cuda.nvtx.lib_paths->glob.glob(NVTOOLEXT_HOME + '/bin/x64/nvToolsExt*.dll')
A:torch.cuda.nvtx.lib->windows_nvToolsExt_lib()
torch.cuda.nvtx._libnvToolsExt()
torch.cuda.nvtx.mark(msg)
torch.cuda.nvtx.range_pop()
torch.cuda.nvtx.range_push(msg)
torch.cuda.nvtx.windows_nvToolsExt_lib()
torch.cuda.nvtx.windows_nvToolsExt_path()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/_utils.py----------------------------------------
torch.cuda._get_device_index(device,optional=False)
torch.cuda._utils._get_device_index(device,optional=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/comm.py----------------------------------------
A:torch.cuda.comm.destination->torch.cuda.current_device()
A:torch.cuda.comm.input_size->inputs[0].size()
A:torch.cuda.comm.got->'x'.join((str(x) for x in inp.size()))
A:torch.cuda.comm.expected->'x'.join((str(x) for x in input_size))
A:torch.cuda.comm.result->reduce_add(tensor_at_gpus, destination)
A:torch.cuda.comm.input_correct_gpu->inp.cuda(result.get_device())
A:torch.cuda.comm.flat_result->reduce_add(flat_tensors, destination)
torch.cuda.comm.broadcast(tensor,devices)
torch.cuda.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)
torch.cuda.comm.gather(tensors,dim=0,destination=None)
torch.cuda.comm.reduce_add(inputs,destination=None)
torch.cuda.comm.reduce_add_coalesced(inputs,destination=None,buffer_size=10485760)
torch.cuda.comm.scatter(tensor,devices,chunk_sizes=None,dim=0,streams=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/nccl.py----------------------------------------
A:torch.cuda.nccl.devices->set()
A:torch.cuda.nccl.device->tensor.get_device()
torch.cuda.nccl.all_gather(inputs,outputs,streams=None,comms=None)
torch.cuda.nccl.all_reduce(inputs,outputs=None,op=SUM,streams=None,comms=None)
torch.cuda.nccl.broadcast(inputs,root=0,streams=None,comms=None)
torch.cuda.nccl.init_rank(num_ranks,uid,rank)
torch.cuda.nccl.is_available(tensors)
torch.cuda.nccl.reduce(inputs,outputs=None,root=0,op=SUM,streams=None,comms=None)
torch.cuda.nccl.reduce_scatter(inputs,outputs,op=SUM,streams=None,comms=None)
torch.cuda.nccl.unique_id()
torch.cuda.nccl.version()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/profiler.py----------------------------------------
A:torch.cuda.profiler.cudaKeyValuePair->ctypes.c_int(0)
A:torch.cuda.profiler.cudaCSV->ctypes.c_int(1)
A:torch.cuda.profiler.output_mode->cudaOutputMode.for_key(output_mode)
torch.cuda.profiler.cudaOutputMode(object)
torch.cuda.profiler.cudaOutputMode.for_key(key)
torch.cuda.profiler.init(output_file,flags=None,output_mode='key_value')
torch.cuda.profiler.profile()
torch.cuda.profiler.start()
torch.cuda.profiler.stop()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/random.py----------------------------------------
A:torch.cuda.random.new_state_copy->new_state.clone()
A:torch.cuda.random.seed->int(seed)
torch.cuda.get_rng_state(device=-1)
torch.cuda.get_rng_state_all()
torch.cuda.initial_seed()
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.cuda.random.get_rng_state(device=-1)
torch.cuda.random.get_rng_state_all()
torch.cuda.random.initial_seed()
torch.cuda.random.manual_seed(seed)
torch.cuda.random.manual_seed_all(seed)
torch.cuda.random.seed()
torch.cuda.random.seed_all()
torch.cuda.random.set_rng_state(new_state,device=-1)
torch.cuda.random.set_rng_state_all(new_states)
torch.cuda.seed()
torch.cuda.seed_all()
torch.cuda.set_rng_state(new_state,device=-1)
torch.cuda.set_rng_state_all(new_states)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/cuda/error.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/testing/__init__.py----------------------------------------
A:torch.testing.__init__.actual->torch.tensor(actual)
A:torch.testing.__init__.expected->expected.expand_as(actual).expand_as(actual)
A:torch.testing.__init__.(rtol, atol)->_get_default_tolerance(actual, expected)
A:torch.testing.__init__.close->torch.isclose(actual, expected, rtol, atol, equal_nan)
A:torch.testing.__init__.error->(expected - actual).abs()
A:torch.testing.__init__.(_, index)->delta.reshape(-1).max(0)
A:torch.testing.__init__.index->_unravel_index(index.item(), actual.shape)
A:torch.testing.__init__.count->(~close).long().sum()
A:torch.testing.__init__.osize->list(tensor.size())
A:torch.testing.__init__.dim->random.randint(0, len(osize) - 1)
A:torch.testing.__init__.add->random.randint(4, 15)
A:torch.testing.__init__.input->input.narrow(i, bounds, tensor.size(i)).narrow(i, bounds, tensor.size(i))
A:torch.testing.__init__.bounds->random.randint(1, input.size(i) - tensor.size(i))
A:torch.testing.__init__.a_tol->_get_default_tolerance(a)
A:torch.testing.__init__.b_tol->_get_default_tolerance(b)
torch.testing.__init__._get_default_tolerance(a,b=None)
torch.testing.__init__.assert_allclose(actual,expected,rtol=None,atol=None,equal_nan=True)
torch.testing.__init__.get_all_dtypes()
torch.testing.__init__.make_non_contiguous(tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/__init__.py----------------------------------------
A:torch.autograd.__init__.grad_tensors->_make_grads(tensors, grad_tensors)
A:torch.autograd.__init__.grad_outputs->_make_grads(outputs, grad_outputs)
torch.autograd.__init__._is_checkpoint_valid()
torch.autograd.__init__._make_grads(outputs,grads)
torch.autograd.__init__.backward(tensors,grad_tensors=None,retain_graph=None,create_graph=False,grad_variables=None)
torch.autograd.__init__.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)
torch.autograd.__init__.variable(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/variable.py----------------------------------------
A:torch.autograd.variable.Variable._execution_engine->ImperativeEngine()
torch.autograd.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.VariableMeta(type)
torch.autograd.VariableMeta.__instancecheck__(self,other)
torch.autograd.variable.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.variable.VariableMeta(type)
torch.autograd.variable.VariableMeta.__instancecheck__(self,other)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/anomaly_mode.py----------------------------------------
A:torch.autograd.anomaly_mode.self.prev->torch.is_anomaly_enabled()
torch.autograd.anomaly_mode.detect_anomaly(self)
torch.autograd.anomaly_mode.detect_anomaly.__enter__(self)
torch.autograd.anomaly_mode.detect_anomaly.__exit__(self,*args)
torch.autograd.anomaly_mode.detect_anomaly.__init__(self)
torch.autograd.anomaly_mode.set_detect_anomaly(self,mode)
torch.autograd.anomaly_mode.set_detect_anomaly.__enter__(self)
torch.autograd.anomaly_mode.set_detect_anomaly.__exit__(self,*args)
torch.autograd.anomaly_mode.set_detect_anomaly.__init__(self,mode)
torch.autograd.detect_anomaly(self)
torch.autograd.detect_anomaly.__enter__(self)
torch.autograd.detect_anomaly.__exit__(self,*args)
torch.autograd.set_detect_anomaly(self,mode)
torch.autograd.set_detect_anomaly.__enter__(self)
torch.autograd.set_detect_anomaly.__exit__(self,*args)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/grad_mode.py----------------------------------------
A:torch.autograd.grad_mode.self.prev->torch.is_grad_enabled()
torch.autograd.grad_mode.enable_grad(self,func)
torch.autograd.grad_mode.enable_grad.__call__(self,func)
torch.autograd.grad_mode.enable_grad.__enter__(self)
torch.autograd.grad_mode.enable_grad.__exit__(self,*args)
torch.autograd.grad_mode.no_grad(self,func)
torch.autograd.grad_mode.no_grad.__call__(self,func)
torch.autograd.grad_mode.no_grad.__enter__(self)
torch.autograd.grad_mode.no_grad.__exit__(self,*args)
torch.autograd.grad_mode.set_grad_enabled(self,mode)
torch.autograd.grad_mode.set_grad_enabled.__enter__(self)
torch.autograd.grad_mode.set_grad_enabled.__exit__(self,*args)
torch.autograd.grad_mode.set_grad_enabled.__init__(self,mode)
torch.enable_grad(self,func)
torch.enable_grad.__enter__(self)
torch.enable_grad.__exit__(self,*args)
torch.no_grad(self,func)
torch.no_grad.__enter__(self)
torch.no_grad.__exit__(self,*args)
torch.set_grad_enabled(self,mode)
torch.set_grad_enabled.__enter__(self)
torch.set_grad_enabled.__exit__(self,*args)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/profiler.py----------------------------------------
A:torch.autograd.profiler.stats->defaultdict(FunctionEventAvg)
A:torch.autograd.profiler.total_stat->FunctionEventAvg()
A:torch.autograd.profiler.records->torch.autograd._disable_profiler()
A:torch.autograd.profiler.self.function_events->EventList(parse_cpu_trace(records))
A:torch.autograd.profiler.cpu_time_str->attr_formatter('cpu_time')
A:torch.autograd.profiler.cuda_time_str->attr_formatter('cuda_time')
A:torch.autograd.profiler.cpu_time_total_str->attr_formatter('cpu_time_total')
A:torch.autograd.profiler.cuda_time_total_str->attr_formatter('cuda_time_total')
A:torch.autograd.profiler.self.cpu_interval->Interval(cpu_start, cpu_end)
A:torch.autograd.profiler.self[key]->torch._C._demangle(key)
A:torch.autograd.profiler.string_table->StringTable()
A:torch.autograd.profiler.(function_id, start)->record_stack.pop()
A:torch.autograd.profiler.fe->FunctionEvent(id=function_id, name=string_table[start.name()], thread=start.thread_id(), cpu_start=start_record.cpu_elapsed_us(start), cpu_end=start_record.cpu_elapsed_us(record))
A:torch.autograd.profiler.cuda_start->adjusted_time(start)
A:torch.autograd.profiler.cuda_end->adjusted_time(record)
A:torch.autograd.profiler.self.seen->set()
A:torch.autograd.profiler.conn->sqlite3.connect(path)
A:torch.autograd.profiler.strings[r['id']]->torch._C._demangle(r['value'])
A:torch.autograd.profiler.unique->EnforceUnique()
A:torch.autograd.profiler.evt->FunctionEvent(id=row['marker_id'], name=strings[row['name']], cpu_start=row['start_time'], cpu_end=row['end_time'], thread=0)
A:torch.autograd.profiler.events->sorted(events, key=lambda evt: getattr(evt, sort_by))
A:torch.autograd.profiler.max_name_length->max(name_lengths)
torch.autograd.profiler.EnforceUnique(self)
torch.autograd.profiler.EnforceUnique.__init__(self)
torch.autograd.profiler.EnforceUnique.see(self,*key)
torch.autograd.profiler.EventList(self,*args,**kwargs)
torch.autograd.profiler.EventList.__init__(self,*args,**kwargs)
torch.autograd.profiler.EventList.__str__(self)
torch.autograd.profiler.EventList.export_chrome_trace(self,path)
torch.autograd.profiler.EventList.key_averages(self)
torch.autograd.profiler.EventList.table(self,sort_by=None)
torch.autograd.profiler.EventList.total_average(self)
torch.autograd.profiler.FormattedTimesMixin(object)
torch.autograd.profiler.FormattedTimesMixin.cpu_time(self)
torch.autograd.profiler.FormattedTimesMixin.cuda_time(self)
torch.autograd.profiler.FunctionEvent(self,id,name,thread,cpu_start,cpu_end)
torch.autograd.profiler.FunctionEvent.__init__(self,id,name,thread,cpu_start,cpu_end)
torch.autograd.profiler.FunctionEvent.__repr__(self)
torch.autograd.profiler.FunctionEvent.append_kernel(self,name,device,start,end)
torch.autograd.profiler.FunctionEvent.cpu_time_total(self)
torch.autograd.profiler.FunctionEvent.cuda_time_total(self)
torch.autograd.profiler.FunctionEvent.key(self)
torch.autograd.profiler.FunctionEventAvg(self)
torch.autograd.profiler.FunctionEventAvg.__iadd__(self,other)
torch.autograd.profiler.FunctionEventAvg.__init__(self)
torch.autograd.profiler.FunctionEventAvg.__repr__(self)
torch.autograd.profiler.Interval(self,start,end)
torch.autograd.profiler.Interval.__init__(self,start,end)
torch.autograd.profiler.Interval.elapsed_us(self)
torch.autograd.profiler.Kernel(self,name,device,interval)
torch.autograd.profiler.Kernel.__init__(self,name,device,interval)
torch.autograd.profiler.StringTable(defaultdict)
torch.autograd.profiler.StringTable.__missing__(self,key)
torch.autograd.profiler.attr_formatter(name)
torch.autograd.profiler.build_table(events,sort_by=None,header=None)
torch.autograd.profiler.emit_nvtx(self,enabled=True)
torch.autograd.profiler.emit_nvtx.__enter__(self)
torch.autograd.profiler.emit_nvtx.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.emit_nvtx.__init__(self,enabled=True)
torch.autograd.profiler.format_time(time_us)
torch.autograd.profiler.load_nvprof(path)
torch.autograd.profiler.parse_cpu_trace(thread_records)
torch.autograd.profiler.parse_nvprof_trace(path)
torch.autograd.profiler.profile(self,enabled=True,use_cuda=False)
torch.autograd.profiler.profile.__enter__(self)
torch.autograd.profiler.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.profile.__init__(self,enabled=True,use_cuda=False)
torch.autograd.profiler.profile.__repr__(self)
torch.autograd.profiler.profile.__str__(self)
torch.autograd.profiler.profile._check_finish(self)
torch.autograd.profiler.profile.export_chrome_trace(self,path)
torch.autograd.profiler.profile.key_averages(self)
torch.autograd.profiler.profile.table(self,sort_by=None)
torch.autograd.profiler.profile.total_average(self)
torch.autograd.profiler.range(self,name)
torch.autograd.profiler.range.__enter__(self)
torch.autograd.profiler.range.__exit__(self,*args)
torch.autograd.profiler.range.__init__(self,name)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py----------------------------------------
A:torch.autograd.gradcheck.jacobians->list(filter(lambda x: x is not None, (make_jacobian(elem, num_out) for elem in input)))
A:torch.autograd.gradcheck.output_size->fn(input).numel()
A:torch.autograd.gradcheck.jacobian->make_jacobian(input, output.numel())
A:torch.autograd.gradcheck.orig->x_tensor[x_idx].item()
A:torch.autograd.gradcheck.outa->fn(input).clone()
A:torch.autograd.gradcheck.outb->fn(input).clone()
A:torch.autograd.gradcheck.d_tensor[d_idx]->r.detach().reshape(-1)
A:torch.autograd.gradcheck.diff_input_list->list(iter_tensors(tupled_inputs, True))
A:torch.autograd.gradcheck.jacobian_reentrant->make_jacobian(input, output.numel())
A:torch.autograd.gradcheck.grad_output->torch.zeros_like(output)
A:torch.autograd.gradcheck.flat_grad_output->torch.zeros_like(output).view(-1)
A:torch.autograd.gradcheck.grads_input->torch.autograd.grad(output, diff_input_list, [torch.zeros_like(o) for o in output], allow_unused=True)
A:torch.autograd.gradcheck.jacobian_x[:, i]->d_x_dense.contiguous().view(-1)
A:torch.autograd.gradcheck.tupled_inputs->_as_tuple(inputs)
A:torch.autograd.gradcheck.output->_differentiable_outputs(func(*tupled_inputs))
A:torch.autograd.gradcheck.(analytical, reentrant, correct_grad_sizes)->get_analytical_jacobian(tupled_inputs, o)
A:torch.autograd.gradcheck.numerical->get_numerical_jacobian(fn, tupled_inputs, eps=eps)
A:torch.autograd.gradcheck.y->torch.testing.make_non_contiguous(y)
A:torch.autograd.gradcheck.outputs->_differentiable_outputs(func(*input_args))
A:torch.autograd.gradcheck.tupled_grad_outputs->_as_tuple(grad_outputs)
A:torch.autograd.gradcheck.num_outputs->len(tupled_grad_outputs)
A:torch.autograd.gradcheck.input_args->tuple((x for x in input_args if isinstance(x, torch.Tensor) and x.requires_grad))
A:torch.autograd.gradcheck.grad_inputs->torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)
torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True)
torch.autograd.gradcheck._as_tuple(x)
torch.autograd.gradcheck._differentiable_outputs(x)
torch.autograd.gradcheck.get_analytical_jacobian(input,output)
torch.autograd.gradcheck.get_numerical_jacobian(fn,input,target=None,eps=0.001)
torch.autograd.gradcheck.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True)
torch.autograd.gradcheck.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True)
torch.autograd.gradcheck.iter_tensors(x,only_requiring_grad=False)
torch.autograd.gradcheck.make_jacobian(input,num_out)
torch.autograd.gradcheck.zero_gradients(x)
torch.autograd.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/function.py----------------------------------------
A:torch.autograd.function.backward_hooks->OrderedDict()
A:torch.autograd.function.handle->torch.utils.hooks.RemovableHandle(backward_hooks)
A:torch.autograd.function.forward->super_cls.__dict__.get('forward')
A:torch.autograd.function.backward_fn->type(name + 'Backward', (BackwardCFunction,), {'_forward_cls': cls})
A:torch.autograd.function.outputs->fn(ctx, *args)
A:torch.autograd.function.requires_grad->any((isinstance(arg, torch.Tensor) and arg.requires_grad for arg in args))
A:torch.autograd.function.err_fn->torch._C._functions.DelayedError(b'trying to differentiate twice a function that was markedwith @once_differentiable', len(outputs))
A:torch.autograd.function.var->var.detach().detach()
A:torch.autograd.function.obj->conversion(obj)
A:torch.autograd.function.(res_e, input)->unflatten_helper(input, e)
A:torch.autograd.function._iter_jit_values->_iter_filter(lambda o: o is None or isinstance(o, torch._C.Value), condition_msg="jit's Values or None")
A:torch.autograd.function._iter_tensors->_iter_filter(lambda x: isinstance(x, torch.Tensor), condition_msg='Tensors', conversion=_jit_unwrap_structured)
A:torch.autograd.function._iter_tensors_permissive->_iter_filter(lambda x: isinstance(x, torch.Tensor), allow_unknown=True, condition_msg='Tensors (permissive)')
A:torch.autograd.function._iter_None_tensors->_iter_filter(lambda o: o is None or isinstance(o, torch.Tensor), condition_msg='Tensors or None')
A:torch.autograd.function._map_tensor_data->_nested_map(lambda x: isinstance(x, torch.Tensor), lambda o: o.data, condition_msg='Tensors')
A:torch.autograd.function.flat_input->tuple(_iter_tensors(input))
A:torch.autograd.function.flat_output->super(NestedIOFunction, self)._do_forward(*flat_input)
A:torch.autograd.function.nested_tensors->_map_tensor_data(self._nested_input)
A:torch.autograd.function.result->self.forward_extended(*nested_tensors)
A:torch.autograd.function.nested_gradients->_unflatten(gradients, self._nested_output)
A:torch.autograd.function.self.to_save->tuple(_iter_tensors(args))
A:torch.autograd.function.self.dirty_tensors->tuple(_iter_tensors((args, kwargs)))
A:torch.autograd.function.self.non_differentiable->tuple(_iter_tensors((args, kwargs)))
torch.autograd.Function(with_metaclass(FunctionMeta,_C._FunctionBase,_ContextMethodMixin,_HookMixin))
torch.autograd.Function.backward(ctx,*grad_outputs)
torch.autograd.Function.forward(ctx,*args,**kwargs)
torch.autograd.FunctionMeta(cls,name,bases,attrs)
torch.autograd.NestedIOFunction(Function)
torch.autograd.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.NestedIOFunction._do_forward(self,*input)
torch.autograd.NestedIOFunction.backward(self,*gradients)
torch.autograd.NestedIOFunction.backward_extended(self,*grad_output)
torch.autograd.NestedIOFunction.forward(self,*args)
torch.autograd.NestedIOFunction.forward_extended(self,*input)
torch.autograd.NestedIOFunction.mark_dirty(self,*args,**kwargs)
torch.autograd.NestedIOFunction.mark_non_differentiable(self,*args,**kwargs)
torch.autograd.NestedIOFunction.save_for_backward(self,*args)
torch.autograd.NestedIOFunction.saved_tensors(self)
torch.autograd.function.BackwardCFunction(_C._FunctionBase,_ContextMethodMixin,_HookMixin)
torch.autograd.function.BackwardCFunction.apply(self,*args)
torch.autograd.function.Function(with_metaclass(FunctionMeta,_C._FunctionBase,_ContextMethodMixin,_HookMixin))
torch.autograd.function.Function.backward(ctx,*grad_outputs)
torch.autograd.function.Function.forward(ctx,*args,**kwargs)
torch.autograd.function.FunctionMeta(cls,name,bases,attrs)
torch.autograd.function.FunctionMeta.__init__(cls,name,bases,attrs)
torch.autograd.function.InplaceFunction(self,inplace=False)
torch.autograd.function.InplaceFunction.__init__(self,inplace=False)
torch.autograd.function.NestedIOFunction(Function)
torch.autograd.function.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.function.NestedIOFunction._do_forward(self,*input)
torch.autograd.function.NestedIOFunction.backward(self,*gradients)
torch.autograd.function.NestedIOFunction.backward_extended(self,*grad_output)
torch.autograd.function.NestedIOFunction.forward(self,*args)
torch.autograd.function.NestedIOFunction.forward_extended(self,*input)
torch.autograd.function.NestedIOFunction.mark_dirty(self,*args,**kwargs)
torch.autograd.function.NestedIOFunction.mark_non_differentiable(self,*args,**kwargs)
torch.autograd.function.NestedIOFunction.save_for_backward(self,*args)
torch.autograd.function.NestedIOFunction.saved_tensors(self)
torch.autograd.function._ContextMethodMixin(object)
torch.autograd.function._ContextMethodMixin.mark_dirty(self,*args)
torch.autograd.function._ContextMethodMixin.mark_non_differentiable(self,*args)
torch.autograd.function._ContextMethodMixin.mark_shared_storage(self,*pairs)
torch.autograd.function._ContextMethodMixin.save_for_backward(self,*tensors)
torch.autograd.function._HookMixin(object)
torch.autograd.function._HookMixin._register_hook(backward_hooks,hook)
torch.autograd.function._iter_filter(condition,allow_unknown=False,condition_msg=None,conversion=None)
torch.autograd.function._jit_unwrap_structured(obj)
torch.autograd.function._nested_map(condition,fn,condition_msg=None)
torch.autograd.function._unflatten(input,proto)
torch.autograd.function.once_differentiable(fn)
torch.autograd.function.traceable(fn_cls)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/_functions/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py----------------------------------------
A:torch.autograd._functions.tensor.ctx.input_type->type(i)
A:torch.autograd._functions.tensor.ctx.numel->reduce(lambda x, y: x * y, sizes, 1)
A:torch.autograd._functions.tensor.ctx.input_sizes->tensor.size()
A:torch.autograd._functions.tensor.result->tensor.new(tensor).contiguous().view(*sizes)
torch.autograd._functions.Resize(Function)
torch.autograd._functions.Resize.backward(ctx,grad_output)
torch.autograd._functions.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.Type(Function)
torch.autograd._functions.Type.backward(ctx,grad_output)
torch.autograd._functions.Type.forward(ctx,i,dest_type)
torch.autograd._functions.tensor.Resize(Function)
torch.autograd._functions.tensor.Resize.backward(ctx,grad_output)
torch.autograd._functions.tensor.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.tensor.Type(Function)
torch.autograd._functions.tensor.Type.backward(ctx,grad_output)
torch.autograd._functions.tensor.Type.forward(ctx,i,dest_type)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/autograd/_functions/utils.py----------------------------------------
A:torch.autograd._functions.utils.tensor->tensor.sum(dim, keepdim=True).sum(dim, keepdim=True)
A:torch.autograd._functions.utils.len1->len(dims1)
A:torch.autograd._functions.utils.len2->len(dims2)
A:torch.autograd._functions.utils.numel1->reduce(lambda x, y: x * y, dims1)
A:torch.autograd._functions.utils.numel2->reduce(lambda x, y: x * y, dims2)
torch.autograd._functions.utils.check_onnx_broadcast(dims1,dims2)
torch.autograd._functions.utils.maybe_unexpand(tensor,old_size,check_same_size=True)
torch.autograd._functions.utils.maybe_view(tensor,size,check_same_size=True)
torch.autograd._functions.utils.prepare_onnx_paddings(dim,pad)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/beta.py----------------------------------------
A:torch.distributions.beta.concentration1_concentration0->torch.stack([concentration1, concentration0], -1)
A:torch.distributions.beta.(concentration1, concentration0)->broadcast_all(concentration1, concentration0)
A:torch.distributions.beta.self._dirichlet->Dirichlet(concentration1_concentration0)
A:torch.distributions.beta.new->self._get_checked_instance(Beta, _instance)
A:torch.distributions.beta.batch_shape->torch.Size(batch_shape)
A:torch.distributions.beta.new._dirichlet->self._dirichlet.expand(batch_shape)
A:torch.distributions.beta.value->self._dirichlet.concentration.new_tensor(value)
A:torch.distributions.beta.heads_tails->torch.stack([value, 1.0 - value], -1)
torch.distributions.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.Beta._log_normalizer(self,x,y)
torch.distributions.Beta._natural_params(self)
torch.distributions.Beta.concentration0(self)
torch.distributions.Beta.concentration1(self)
torch.distributions.Beta.entropy(self)
torch.distributions.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.Beta.log_prob(self,value)
torch.distributions.Beta.mean(self)
torch.distributions.Beta.rsample(self,sample_shape=())
torch.distributions.Beta.variance(self)
torch.distributions.beta.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta.__init__(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta._log_normalizer(self,x,y)
torch.distributions.beta.Beta._natural_params(self)
torch.distributions.beta.Beta.concentration0(self)
torch.distributions.beta.Beta.concentration1(self)
torch.distributions.beta.Beta.entropy(self)
torch.distributions.beta.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.beta.Beta.log_prob(self,value)
torch.distributions.beta.Beta.mean(self)
torch.distributions.beta.Beta.rsample(self,sample_shape=())
torch.distributions.beta.Beta.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py----------------------------------------
A:torch.distributions.multivariate_normal.flat_b->bb.reshape((-1,) + bb.shape[-2:])
A:torch.distributions.multivariate_normal.flat_A->bA.reshape((-1,) + bA.shape[-2:])
A:torch.distributions.multivariate_normal.flat_X->torch.stack([torch.trtrs(b, A, upper=False)[0] for (b, A) in zip(flat_b, flat_A)])
A:torch.distributions.multivariate_normal.n->bx.size(-1)
A:torch.distributions.multivariate_normal.bL->bL.expand(bx.shape[bx.dim() - bL.dim() + 1:] + (n,)).expand(bx.shape[bx.dim() - bL.dim() + 1:] + (n,))
A:torch.distributions.multivariate_normal.flat_L->bL.expand(bx.shape[bx.dim() - bL.dim() + 1:] + (n,)).expand(bx.shape[bx.dim() - bL.dim() + 1:] + (n,)).reshape(-1, n, n)
A:torch.distributions.multivariate_normal.flat_x->bx.reshape(-1, flat_L.size(0), n)
A:torch.distributions.multivariate_normal.flat_x_swap->bx.reshape(-1, flat_L.size(0), n).permute(1, 2, 0)
A:torch.distributions.multivariate_normal.M_swap->_batch_trtrs_lower(flat_x_swap, flat_L).pow(2).sum(-2)
A:torch.distributions.multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.multivariate_normal.(self.scale_tril, loc_)->torch.broadcast_tensors(scale_tril, loc_)
A:torch.distributions.multivariate_normal.(self.covariance_matrix, loc_)->torch.broadcast_tensors(covariance_matrix, loc_)
A:torch.distributions.multivariate_normal.(self.precision_matrix, loc_)->torch.broadcast_tensors(precision_matrix, loc_)
A:torch.distributions.multivariate_normal.self.covariance_matrix->torch.inverse(precision_matrix).expand_as(loc_)
A:torch.distributions.multivariate_normal.self._unbroadcasted_scale_tril->torch.cholesky(self.covariance_matrix)
A:torch.distributions.multivariate_normal.new->self._get_checked_instance(MultivariateNormal, _instance)
A:torch.distributions.multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.multivariate_normal.new.covariance_matrix->self.covariance_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.scale_tril->self.scale_tril.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.precision_matrix->self.precision_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.scale_tril_inv->torch.inverse(self._unbroadcasted_scale_tril)
A:torch.distributions.multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.multivariate_normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.multivariate_normal.M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff)
A:torch.distributions.multivariate_normal.half_log_det->_batch_diag(self._unbroadcasted_scale_tril).log().sum(-1)
torch.distributions.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.MultivariateNormal.covariance_matrix(self)
torch.distributions.MultivariateNormal.entropy(self)
torch.distributions.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.MultivariateNormal.log_prob(self,value)
torch.distributions.MultivariateNormal.mean(self)
torch.distributions.MultivariateNormal.precision_matrix(self)
torch.distributions.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.MultivariateNormal.scale_tril(self)
torch.distributions.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.__init__(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.entropy(self)
torch.distributions.multivariate_normal.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.multivariate_normal.MultivariateNormal.log_prob(self,value)
torch.distributions.multivariate_normal.MultivariateNormal.mean(self)
torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.multivariate_normal.MultivariateNormal.scale_tril(self)
torch.distributions.multivariate_normal.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal._batch_diag(bmat)
torch.distributions.multivariate_normal._batch_mahalanobis(bL,bx)
torch.distributions.multivariate_normal._batch_mv(bmat,bvec)
torch.distributions.multivariate_normal._batch_trtrs_lower(bb,bA)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/chi2.py----------------------------------------
A:torch.distributions.chi2.new->self._get_checked_instance(Chi2, _instance)
torch.distributions.Chi2(self,df,validate_args=None)
torch.distributions.Chi2.df(self)
torch.distributions.Chi2.expand(self,batch_shape,_instance=None)
torch.distributions.chi2.Chi2(self,df,validate_args=None)
torch.distributions.chi2.Chi2.__init__(self,df,validate_args=None)
torch.distributions.chi2.Chi2.df(self)
torch.distributions.chi2.Chi2.expand(self,batch_shape,_instance=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/cauchy.py----------------------------------------
A:torch.distributions.cauchy.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.cauchy.batch_shape->torch.Size(batch_shape)
A:torch.distributions.cauchy.new->self._get_checked_instance(Cauchy, _instance)
A:torch.distributions.cauchy.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.cauchy.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.cauchy.shape->self._extended_shape(sample_shape)
A:torch.distributions.cauchy.eps->self.loc.new(shape).cauchy_()
torch.distributions.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.Cauchy.cdf(self,value)
torch.distributions.Cauchy.entropy(self)
torch.distributions.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.Cauchy.icdf(self,value)
torch.distributions.Cauchy.log_prob(self,value)
torch.distributions.Cauchy.mean(self)
torch.distributions.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.Cauchy.variance(self)
torch.distributions.cauchy.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.__init__(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.cdf(self,value)
torch.distributions.cauchy.Cauchy.entropy(self)
torch.distributions.cauchy.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.cauchy.Cauchy.icdf(self,value)
torch.distributions.cauchy.Cauchy.log_prob(self,value)
torch.distributions.cauchy.Cauchy.mean(self)
torch.distributions.cauchy.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.cauchy.Cauchy.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/bernoulli.py----------------------------------------
A:torch.distributions.bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.bernoulli.new->self._get_checked_instance(Bernoulli, _instance)
A:torch.distributions.bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.bernoulli.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.Bernoulli._log_normalizer(self,x)
torch.distributions.Bernoulli._natural_params(self)
torch.distributions.Bernoulli._new(self,*args,**kwargs)
torch.distributions.Bernoulli.entropy(self)
torch.distributions.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.Bernoulli.log_prob(self,value)
torch.distributions.Bernoulli.logits(self)
torch.distributions.Bernoulli.mean(self)
torch.distributions.Bernoulli.param_shape(self)
torch.distributions.Bernoulli.probs(self)
torch.distributions.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.Bernoulli.variance(self)
torch.distributions.bernoulli.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli._log_normalizer(self,x)
torch.distributions.bernoulli.Bernoulli._natural_params(self)
torch.distributions.bernoulli.Bernoulli._new(self,*args,**kwargs)
torch.distributions.bernoulli.Bernoulli.entropy(self)
torch.distributions.bernoulli.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.bernoulli.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.bernoulli.Bernoulli.log_prob(self,value)
torch.distributions.bernoulli.Bernoulli.logits(self)
torch.distributions.bernoulli.Bernoulli.mean(self)
torch.distributions.bernoulli.Bernoulli.param_shape(self)
torch.distributions.bernoulli.Bernoulli.probs(self)
torch.distributions.bernoulli.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.bernoulli.Bernoulli.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/categorical.py----------------------------------------
A:torch.distributions.categorical.new->self._get_checked_instance(Categorical, _instance)
A:torch.distributions.categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.categorical.new.probs->self.probs.expand(param_shape)
A:torch.distributions.categorical.new.logits->self.logits.expand(param_shape)
A:torch.distributions.categorical.sample_shape->self._extended_shape(sample_shape)
A:torch.distributions.categorical.probs->self.probs.expand(param_shape)
A:torch.distributions.categorical.probs_2d->self.probs.expand(param_shape).contiguous().view(-1, self._num_events)
A:torch.distributions.categorical.sample_2d->torch.multinomial(probs_2d, 1, True)
A:torch.distributions.categorical.value->value.long().unsqueeze(-1).long().unsqueeze(-1)
A:torch.distributions.categorical.(value, log_pmf)->torch.broadcast_tensors(value, self.logits)
A:torch.distributions.categorical.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.Categorical._new(self,*args,**kwargs)
torch.distributions.Categorical.entropy(self)
torch.distributions.Categorical.enumerate_support(self,expand=True)
torch.distributions.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.Categorical.log_prob(self,value)
torch.distributions.Categorical.logits(self)
torch.distributions.Categorical.mean(self)
torch.distributions.Categorical.param_shape(self)
torch.distributions.Categorical.probs(self)
torch.distributions.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.Categorical.support(self)
torch.distributions.Categorical.variance(self)
torch.distributions.categorical.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical._new(self,*args,**kwargs)
torch.distributions.categorical.Categorical.entropy(self)
torch.distributions.categorical.Categorical.enumerate_support(self,expand=True)
torch.distributions.categorical.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.categorical.Categorical.log_prob(self,value)
torch.distributions.categorical.Categorical.logits(self)
torch.distributions.categorical.Categorical.mean(self)
torch.distributions.categorical.Categorical.param_shape(self)
torch.distributions.categorical.Categorical.probs(self)
torch.distributions.categorical.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.categorical.Categorical.support(self)
torch.distributions.categorical.Categorical.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/weibull.py----------------------------------------
A:torch.distributions.weibull.(self.scale, self.concentration)->broadcast_all(scale, concentration)
A:torch.distributions.weibull.self.concentration_reciprocal->self.concentration.reciprocal()
A:torch.distributions.weibull.base_dist->self.base_dist.expand(batch_shape)
A:torch.distributions.weibull.new->self._get_checked_instance(Weibull, _instance)
A:torch.distributions.weibull.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.weibull.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.weibull.new.concentration_reciprocal->self._get_checked_instance(Weibull, _instance).concentration.reciprocal()
torch.distributions.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.Weibull.entropy(self)
torch.distributions.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.Weibull.mean(self)
torch.distributions.Weibull.variance(self)
torch.distributions.weibull.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.__init__(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.entropy(self)
torch.distributions.weibull.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.weibull.Weibull.mean(self)
torch.distributions.weibull.Weibull.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/geometric.py----------------------------------------
A:torch.distributions.geometric.(self.probs,)->broadcast_all(probs)
A:torch.distributions.geometric.(self.logits,)->broadcast_all(logits)
A:torch.distributions.geometric.batch_shape->torch.Size(batch_shape)
A:torch.distributions.geometric.new->self._get_checked_instance(Geometric, _instance)
A:torch.distributions.geometric.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.geometric.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.geometric.shape->self._extended_shape(sample_shape)
A:torch.distributions.geometric.u->self.probs.new(shape).uniform_(_finfo(self.probs).tiny, 1)
A:torch.distributions.geometric.(value, probs)->broadcast_all(value, self.probs.clone())
torch.distributions.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.Geometric.entropy(self)
torch.distributions.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.Geometric.log_prob(self,value)
torch.distributions.Geometric.logits(self)
torch.distributions.Geometric.mean(self)
torch.distributions.Geometric.probs(self)
torch.distributions.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.Geometric.variance(self)
torch.distributions.geometric.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.entropy(self)
torch.distributions.geometric.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.geometric.Geometric.log_prob(self,value)
torch.distributions.geometric.Geometric.logits(self)
torch.distributions.geometric.Geometric.mean(self)
torch.distributions.geometric.Geometric.probs(self)
torch.distributions.geometric.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.geometric.Geometric.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/one_hot_categorical.py----------------------------------------
A:torch.distributions.one_hot_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.one_hot_categorical.new->self._get_checked_instance(OneHotCategorical, _instance)
A:torch.distributions.one_hot_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.one_hot_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.one_hot_categorical.sample_shape->torch.Size(sample_shape)
A:torch.distributions.one_hot_categorical.indices->indices.unsqueeze(-1).unsqueeze(-1)
A:torch.distributions.one_hot_categorical.eye->torch.eye(self.event_shape[-1], dtype=self._param.dtype, device=self._param.device)
A:torch.distributions.one_hot_categorical.one_hot->probs.new_zeros(self._extended_shape(sample_shape))
A:torch.distributions.one_hot_categorical.values->values.expand((n,) + self.batch_shape + (n,)).expand((n,) + self.batch_shape + (n,))
torch.distributions.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.OneHotCategorical._param(self)
torch.distributions.OneHotCategorical.entropy(self)
torch.distributions.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.OneHotCategorical.log_prob(self,value)
torch.distributions.OneHotCategorical.logits(self)
torch.distributions.OneHotCategorical.mean(self)
torch.distributions.OneHotCategorical.param_shape(self)
torch.distributions.OneHotCategorical.probs(self)
torch.distributions.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.OneHotCategorical.variance(self)
torch.distributions.one_hot_categorical.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.one_hot_categorical.OneHotCategorical._param(self)
torch.distributions.one_hot_categorical.OneHotCategorical.entropy(self)
torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.one_hot_categorical.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.one_hot_categorical.OneHotCategorical.log_prob(self,value)
torch.distributions.one_hot_categorical.OneHotCategorical.logits(self)
torch.distributions.one_hot_categorical.OneHotCategorical.mean(self)
torch.distributions.one_hot_categorical.OneHotCategorical.param_shape(self)
torch.distributions.one_hot_categorical.OneHotCategorical.probs(self)
torch.distributions.one_hot_categorical.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.one_hot_categorical.OneHotCategorical.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/normal.py----------------------------------------
A:torch.distributions.normal.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.normal.new->self._get_checked_instance(Normal, _instance)
A:torch.distributions.normal.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.normal.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
torch.distributions.Normal(self,loc,scale,validate_args=None)
torch.distributions.Normal._log_normalizer(self,x,y)
torch.distributions.Normal._natural_params(self)
torch.distributions.Normal.cdf(self,value)
torch.distributions.Normal.entropy(self)
torch.distributions.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.Normal.icdf(self,value)
torch.distributions.Normal.log_prob(self,value)
torch.distributions.Normal.mean(self)
torch.distributions.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.Normal.stddev(self)
torch.distributions.Normal.variance(self)
torch.distributions.normal.Normal(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal.__init__(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal._log_normalizer(self,x,y)
torch.distributions.normal.Normal._natural_params(self)
torch.distributions.normal.Normal.cdf(self,value)
torch.distributions.normal.Normal.entropy(self)
torch.distributions.normal.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.normal.Normal.icdf(self,value)
torch.distributions.normal.Normal.log_prob(self,value)
torch.distributions.normal.Normal.mean(self)
torch.distributions.normal.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.stddev(self)
torch.distributions.normal.Normal.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/lowrank_multivariate_normal.py----------------------------------------
A:torch.distributions.lowrank_multivariate_normal.n->bvec.size(-1)
A:torch.distributions.lowrank_multivariate_normal.bmat->bvec.new_zeros(bvec.shape + (n,))
A:torch.distributions.lowrank_multivariate_normal.m->W.size(-1)
A:torch.distributions.lowrank_multivariate_normal.K->torch.matmul(Dinvsqrt_W, Dinvsqrt_W.transpose(-1, -2)).contiguous()
A:torch.distributions.lowrank_multivariate_normal.Wt_Dinv_x->_batch_mv(Wt_Dinv, x)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term1->(x.pow(2) / D).sum(-1)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term2->_batch_mahalanobis(capacitance_tril, Wt_Dinv_x)
A:torch.distributions.lowrank_multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.cov_diag_->cov_diag.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.(loc_, self.cov_factor, cov_diag_)->torch.broadcast_tensors(loc_, cov_factor, cov_diag_)
A:torch.distributions.lowrank_multivariate_normal.self._capacitance_tril->_batch_capacitance_tril(cov_factor, cov_diag)
A:torch.distributions.lowrank_multivariate_normal.new->self._get_checked_instance(LowRankMultivariateNormal, _instance)
A:torch.distributions.lowrank_multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.lowrank_multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_diag->self.cov_diag.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_factor->self.cov_factor.expand(loc_shape + self.cov_factor.shape[-1:])
A:torch.distributions.lowrank_multivariate_normal.cov_diag_sqrt_unsqueeze->self._unbroadcasted_cov_diag.sqrt().unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.A->_batch_trtrs_lower(Wt_Dinv, self._capacitance_tril)
A:torch.distributions.lowrank_multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.lowrank_multivariate_normal.eps_W->_standard_normal(W_shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.eps_D->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.M->_batch_lowrank_mahalanobis(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, diff, self._capacitance_tril)
A:torch.distributions.lowrank_multivariate_normal.log_det->_batch_lowrank_logdet(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, self._capacitance_tril)
torch.distributions.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.LowRankMultivariateNormal.entropy(self)
torch.distributions.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.LowRankMultivariateNormal.mean(self)
torch.distributions.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.__init__(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal._batch_capacitance_tril(W,D)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_logdet(W,D,capacitance_tril)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_mahalanobis(W,D,x,capacitance_tril)
torch.distributions.lowrank_multivariate_normal._batch_vector_diag(bvec)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/utils.py----------------------------------------
A:torch.distributions.utils._Finfo->namedtuple('_Finfo', ['eps', 'tiny'])
A:torch.distributions.utils.ps_clamped->clamp_probs(probs)
A:torch.distributions.utils.value->self.wrapped(instance)
torch.distributions.utils._default_promotion(v)
torch.distributions.utils._finfo(tensor)
torch.distributions.utils._standard_normal(shape,dtype,device)
torch.distributions.utils._sum_rightmost(value,dim)
torch.distributions.utils.broadcast_all(*values)
torch.distributions.utils.clamp_probs(probs)
torch.distributions.utils.lazy_property(self,wrapped)
torch.distributions.utils.lazy_property.__get__(self,instance,obj_type=None)
torch.distributions.utils.lazy_property.__init__(self,wrapped)
torch.distributions.utils.logits_to_probs(logits,is_binary=False)
torch.distributions.utils.probs_to_logits(probs,is_binary=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/relaxed_bernoulli.py----------------------------------------
A:torch.distributions.relaxed_bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.relaxed_bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.relaxed_bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.relaxed_bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_bernoulli.new->self._get_checked_instance(RelaxedBernoulli, _instance)
A:torch.distributions.relaxed_bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_bernoulli.probs->clamp_probs(self.probs.expand(shape))
A:torch.distributions.relaxed_bernoulli.uniforms->clamp_probs(torch.rand(shape, dtype=probs.dtype, device=probs.device))
A:torch.distributions.relaxed_bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_bernoulli.base_dist->LogitRelaxedBernoulli(temperature, probs, logits)
torch.distributions.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedBernoulli.logits(self)
torch.distributions.RelaxedBernoulli.probs(self)
torch.distributions.RelaxedBernoulli.temperature(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli._new(self,*args,**kwargs)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob(self,value)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_bernoulli.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/kl.py----------------------------------------
A:torch.distributions.kl.n->bmat.size(-1)
A:torch.distributions.kl.m->bmat.size(-2)
A:torch.distributions.kl.flat_trace->bmat.reshape(-1, m * n).pow(2).sum(-1)
A:torch.distributions.kl.fun->_dispatch_kl(type(p), type(q))
A:torch.distributions.kl.kl[inf_idxs]->_infinite_like(kl[inf_idxs])
A:torch.distributions.kl.sum_p_concentration->p.concentration.sum(-1)
A:torch.distributions.kl.sum_q_concentration->q.concentration.sum(-1)
A:torch.distributions.kl.t2->p.alpha.reciprocal()
A:torch.distributions.kl.lg_normal->p._log_normalizer(*p_nparams)
A:torch.distributions.kl.gradients->torch.autograd.grad(lg_normal.sum(), p_nparams, create_graph=True)
A:torch.distributions.kl.t3->((p.high + p.low - 2 * q.loc) / 2).pow(2)
A:torch.distributions.kl.loc_abs_diff->(p.loc - q.loc).abs()
A:torch.distributions.kl.term3->_batch_mahalanobis(q._unbroadcasted_scale_tril, q.loc - p.loc)
A:torch.distributions.kl.A->_batch_trtrs_lower(qWt_qDinv, q._capacitance_tril)
A:torch.distributions.kl.term21->_batch_trace_XXT(_batch_trtrs_lower(p_cov_factor, q_scale_tril))
A:torch.distributions.kl.term22->_batch_trace_XXT(_batch_trtrs_lower(p_cov_diag, q_scale_tril))
A:torch.distributions.kl.term23->_batch_trace_XXT(A * p._unbroadcasted_cov_diag.sqrt().unsqueeze(-2))
A:torch.distributions.kl.term24->_batch_trace_XXT(A.matmul(p._unbroadcasted_cov_factor))
A:torch.distributions.kl.combined_batch_shape->torch._C._infer_size(q._unbroadcasted_scale_tril.shape[:-2], p._unbroadcasted_scale_tril.shape[:-2])
A:torch.distributions.kl.q_scale_tril->q._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_cov_factor->p._unbroadcasted_cov_factor.expand(combined_batch_shape + (n, p.cov_factor.size(-1)))
A:torch.distributions.kl.p_cov_diag->_batch_vector_diag(p._unbroadcasted_cov_diag.sqrt()).expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_scale_tril->p._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.term2->_batch_trace_XXT(_batch_trtrs_lower(p_scale_tril, q_scale_tril))
A:torch.distributions.kl.var_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t1->(q.alpha * q.scale.pow(q.alpha) * support_uniform).log()
A:torch.distributions.kl.extra_event_dim->len(p.event_shape)
A:torch.distributions.kl.base_kl_divergence->kl_divergence(p.base_dist, q.base_dist)
A:torch.distributions.kl.result->((q.high - q.low) / (p.high - p.low)).log()
A:torch.distributions.kl.var_normal->q.scale.pow(2)
A:torch.distributions.kl.rate_sqr->p.rate.pow(2)
A:torch.distributions.kl.beta_sqr->p.rate.pow(2)
A:torch.distributions.kl.var_scale_sqr_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t4->(p.alpha * common_term - q.loc).pow(2)
torch.distributions.kl._Match(self,*types)
torch.distributions.kl._Match.__eq__(self,other)
torch.distributions.kl._Match.__init__(self,*types)
torch.distributions.kl._Match.__le__(self,other)
torch.distributions.kl._batch_trace_XXT(bmat)
torch.distributions.kl._dispatch_kl(type_p,type_q)
torch.distributions.kl._infinite_like(tensor)
torch.distributions.kl._kl_bernoulli_bernoulli(p,q)
torch.distributions.kl._kl_bernoulli_poisson(p,q)
torch.distributions.kl._kl_beta_beta(p,q)
torch.distributions.kl._kl_beta_exponential(p,q)
torch.distributions.kl._kl_beta_gamma(p,q)
torch.distributions.kl._kl_beta_infinity(p,q)
torch.distributions.kl._kl_beta_normal(p,q)
torch.distributions.kl._kl_beta_uniform(p,q)
torch.distributions.kl._kl_binomial_binomial(p,q)
torch.distributions.kl._kl_categorical_categorical(p,q)
torch.distributions.kl._kl_dirichlet_dirichlet(p,q)
torch.distributions.kl._kl_expfamily_expfamily(p,q)
torch.distributions.kl._kl_exponential_exponential(p,q)
torch.distributions.kl._kl_exponential_gamma(p,q)
torch.distributions.kl._kl_exponential_gumbel(p,q)
torch.distributions.kl._kl_exponential_infinity(p,q)
torch.distributions.kl._kl_exponential_normal(p,q)
torch.distributions.kl._kl_gamma_exponential(p,q)
torch.distributions.kl._kl_gamma_gamma(p,q)
torch.distributions.kl._kl_gamma_gumbel(p,q)
torch.distributions.kl._kl_gamma_infinity(p,q)
torch.distributions.kl._kl_gamma_normal(p,q)
torch.distributions.kl._kl_geometric_geometric(p,q)
torch.distributions.kl._kl_gumbel_gumbel(p,q)
torch.distributions.kl._kl_gumbel_infinity(p,q)
torch.distributions.kl._kl_gumbel_normal(p,q)
torch.distributions.kl._kl_halfnormal_halfnormal(p,q)
torch.distributions.kl._kl_laplace_infinity(p,q)
torch.distributions.kl._kl_laplace_laplace(p,q)
torch.distributions.kl._kl_laplace_normal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_normal_gumbel(p,q)
torch.distributions.kl._kl_normal_infinity(p,q)
torch.distributions.kl._kl_normal_normal(p,q)
torch.distributions.kl._kl_onehotcategorical_onehotcategorical(p,q)
torch.distributions.kl._kl_pareto_exponential(p,q)
torch.distributions.kl._kl_pareto_gamma(p,q)
torch.distributions.kl._kl_pareto_infinity(p,q)
torch.distributions.kl._kl_pareto_normal(p,q)
torch.distributions.kl._kl_pareto_pareto(p,q)
torch.distributions.kl._kl_poisson_infinity(p,q)
torch.distributions.kl._kl_poisson_poisson(p,q)
torch.distributions.kl._kl_transformed_transformed(p,q)
torch.distributions.kl._kl_uniform_beta(p,q)
torch.distributions.kl._kl_uniform_exponetial(p,q)
torch.distributions.kl._kl_uniform_gamma(p,q)
torch.distributions.kl._kl_uniform_gumbel(p,q)
torch.distributions.kl._kl_uniform_normal(p,q)
torch.distributions.kl._kl_uniform_pareto(p,q)
torch.distributions.kl._kl_uniform_uniform(p,q)
torch.distributions.kl._x_log_x(tensor)
torch.distributions.kl.kl_divergence(p,q)
torch.distributions.kl.register_kl(type_p,type_q)
torch.distributions.kl_divergence(p,q)
torch.distributions.register_kl(type_p,type_q)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/gumbel.py----------------------------------------
A:torch.distributions.gumbel.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.gumbel.finfo->_finfo(self.loc)
A:torch.distributions.gumbel.base_dist->Uniform(torch.full_like(self.loc, finfo.tiny), torch.full_like(self.loc, 1 - finfo.eps))
A:torch.distributions.gumbel.new->self._get_checked_instance(Gumbel, _instance)
A:torch.distributions.gumbel.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.gumbel.new.scale->self.scale.expand(batch_shape)
torch.distributions.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.Gumbel.entropy(self)
torch.distributions.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.Gumbel.log_prob(self,value)
torch.distributions.Gumbel.mean(self)
torch.distributions.Gumbel.stddev(self)
torch.distributions.Gumbel.variance(self)
torch.distributions.gumbel.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.__init__(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.entropy(self)
torch.distributions.gumbel.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.gumbel.Gumbel.log_prob(self,value)
torch.distributions.gumbel.Gumbel.mean(self)
torch.distributions.gumbel.Gumbel.stddev(self)
torch.distributions.gumbel.Gumbel.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/transformed_distribution.py----------------------------------------
A:torch.distributions.transformed_distribution.event_dim->len(self.event_shape)
A:torch.distributions.transformed_distribution.new->self._get_checked_instance(TransformedDistribution, _instance)
A:torch.distributions.transformed_distribution.batch_shape->torch.Size(batch_shape)
A:torch.distributions.transformed_distribution.new.base_dist->self.base_dist.expand(base_dist_batch_shape)
A:torch.distributions.transformed_distribution.x->transform.inv(y)
A:torch.distributions.transformed_distribution.value->transform(value)
torch.distributions.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.TransformedDistribution.cdf(self,value)
torch.distributions.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.TransformedDistribution.has_rsample(self)
torch.distributions.TransformedDistribution.icdf(self,value)
torch.distributions.TransformedDistribution.log_prob(self,value)
torch.distributions.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.support(self)
torch.distributions.transformed_distribution.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution.__init__(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.transformed_distribution.TransformedDistribution.has_rsample(self)
torch.distributions.transformed_distribution.TransformedDistribution.icdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.log_prob(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.support(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/exp_family.py----------------------------------------
A:torch.distributions.exp_family.lg_normal->self._log_normalizer(*nparams)
A:torch.distributions.exp_family.gradients->torch.autograd.grad(lg_normal.sum(), nparams, create_graph=True)
torch.distributions.ExponentialFamily(Distribution)
torch.distributions.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.ExponentialFamily._natural_params(self)
torch.distributions.ExponentialFamily.entropy(self)
torch.distributions.exp_family.ExponentialFamily(Distribution)
torch.distributions.exp_family.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.exp_family.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.exp_family.ExponentialFamily._natural_params(self)
torch.distributions.exp_family.ExponentialFamily.entropy(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/logistic_normal.py----------------------------------------
A:torch.distributions.logistic_normal.base_dist->Normal(loc, scale)
A:torch.distributions.logistic_normal.self._event_shape->torch.Size([s + 1 for s in self._event_shape])
A:torch.distributions.logistic_normal.new->self._get_checked_instance(LogisticNormal, _instance)
torch.distributions.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogisticNormal.loc(self)
torch.distributions.LogisticNormal.scale(self)
torch.distributions.logistic_normal.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.logistic_normal.LogisticNormal.loc(self)
torch.distributions.logistic_normal.LogisticNormal.scale(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/constraint_registry.py----------------------------------------
A:torch.distributions.constraint_registry.constraint->type(constraint)
A:torch.distributions.constraint_registry.biject_to->ConstraintRegistry()
A:torch.distributions.constraint_registry.transform_to->ConstraintRegistry()
torch.distributions.constraint_registry.ConstraintRegistry(self)
torch.distributions.constraint_registry.ConstraintRegistry.__init__(self)
torch.distributions.constraint_registry.ConstraintRegistry.register(self,constraint,factory=None)
torch.distributions.constraint_registry._biject_to_simplex(constraint)
torch.distributions.constraint_registry._transform_to_greater_than(constraint)
torch.distributions.constraint_registry._transform_to_interval(constraint)
torch.distributions.constraint_registry._transform_to_less_than(constraint)
torch.distributions.constraint_registry._transform_to_lower_cholesky(constraint)
torch.distributions.constraint_registry._transform_to_positive(constraint)
torch.distributions.constraint_registry._transform_to_real(constraint)
torch.distributions.constraint_registry._transform_to_simplex(constraint)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/poisson.py----------------------------------------
A:torch.distributions.poisson.(self.rate,)->broadcast_all(rate)
A:torch.distributions.poisson.batch_shape->torch.Size(batch_shape)
A:torch.distributions.poisson.new->self._get_checked_instance(Poisson, _instance)
A:torch.distributions.poisson.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.poisson.shape->self._extended_shape(sample_shape)
A:torch.distributions.poisson.(rate, value)->broadcast_all(self.rate, value)
torch.distributions.Poisson(self,rate,validate_args=None)
torch.distributions.Poisson._log_normalizer(self,x)
torch.distributions.Poisson._natural_params(self)
torch.distributions.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.Poisson.log_prob(self,value)
torch.distributions.Poisson.mean(self)
torch.distributions.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.Poisson.variance(self)
torch.distributions.poisson.Poisson(self,rate,validate_args=None)
torch.distributions.poisson.Poisson.__init__(self,rate,validate_args=None)
torch.distributions.poisson.Poisson._log_normalizer(self,x)
torch.distributions.poisson.Poisson._natural_params(self)
torch.distributions.poisson.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.poisson.Poisson.log_prob(self,value)
torch.distributions.poisson.Poisson.mean(self)
torch.distributions.poisson.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.poisson.Poisson.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/multinomial.py----------------------------------------
A:torch.distributions.multinomial.self._categorical->Categorical(probs=probs, logits=logits)
A:torch.distributions.multinomial.new->self._get_checked_instance(Multinomial, _instance)
A:torch.distributions.multinomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multinomial.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.multinomial.sample_shape->torch.Size(sample_shape)
A:torch.distributions.multinomial.samples->samples.permute(*shifted_idx).permute(*shifted_idx)
A:torch.distributions.multinomial.shifted_idx->list(range(samples.dim()))
A:torch.distributions.multinomial.counts->samples.permute(*shifted_idx).permute(*shifted_idx).new(self._extended_shape(sample_shape)).zero_()
A:torch.distributions.multinomial.(logits, value)->broadcast_all(self.logits.clone(), value)
A:torch.distributions.multinomial.log_factorial_n->torch.lgamma(value.sum(-1) + 1)
A:torch.distributions.multinomial.log_factorial_xs->torch.lgamma(value + 1).sum(-1)
A:torch.distributions.multinomial.log_powers->(logits * value).sum(-1)
torch.distributions.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Multinomial._new(self,*args,**kwargs)
torch.distributions.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.Multinomial.log_prob(self,value)
torch.distributions.Multinomial.logits(self)
torch.distributions.Multinomial.mean(self)
torch.distributions.Multinomial.param_shape(self)
torch.distributions.Multinomial.probs(self)
torch.distributions.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.Multinomial.support(self)
torch.distributions.Multinomial.variance(self)
torch.distributions.multinomial.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial._new(self,*args,**kwargs)
torch.distributions.multinomial.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.multinomial.Multinomial.log_prob(self,value)
torch.distributions.multinomial.Multinomial.logits(self)
torch.distributions.multinomial.Multinomial.mean(self)
torch.distributions.multinomial.Multinomial.param_shape(self)
torch.distributions.multinomial.Multinomial.probs(self)
torch.distributions.multinomial.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.multinomial.Multinomial.support(self)
torch.distributions.multinomial.Multinomial.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/transforms.py----------------------------------------
A:torch.distributions.transforms.inv->ComposeTransform([p.inv for p in reversed(self.parts)])
A:torch.distributions.transforms.self._inv->weakref.ref(inv)
A:torch.distributions.transforms.y->part(x)
A:torch.distributions.transforms.x->part(x)
A:torch.distributions.transforms.inv._inv->weakref.ref(self)
A:torch.distributions.transforms.identity_transform->ComposeTransform([])
A:torch.distributions.transforms.(self.exponent,)->broadcast_all(exponent)
A:torch.distributions.transforms.result->result.view(result_size).sum(-1).view(result_size).sum(-1)
A:torch.distributions.transforms.probs->(logprobs - logprobs.max(-1, True)[0]).exp()
A:torch.distributions.transforms.z->torch.sigmoid(x - offset.log())
A:torch.distributions.transforms.z_cumprod->(1 - z).cumprod(-1)
A:torch.distributions.transforms.detJ->((1 - z).log() + y[..., :-1].log()).sum(-1)
A:torch.distributions.transforms.flat_x->part(x).contiguous().view((-1,) + x.shape[-2:])
A:torch.distributions.transforms.flat_y->part(x).contiguous().view((-1,) + y.shape[-2:])
torch.distributions.AbsTransform(Transform)
torch.distributions.AbsTransform.__eq__(self,other)
torch.distributions.AbsTransform._call(self,x)
torch.distributions.AbsTransform._inverse(self,y)
torch.distributions.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.AffineTransform.__eq__(self,other)
torch.distributions.AffineTransform._call(self,x)
torch.distributions.AffineTransform._inverse(self,y)
torch.distributions.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.AffineTransform.sign(self)
torch.distributions.ComposeTransform(self,parts)
torch.distributions.ComposeTransform.__eq__(self,other)
torch.distributions.ComposeTransform.__repr__(self)
torch.distributions.ComposeTransform.bijective(self)
torch.distributions.ComposeTransform.codomain(self)
torch.distributions.ComposeTransform.domain(self)
torch.distributions.ComposeTransform.event_dim(self)
torch.distributions.ComposeTransform.inv(self)
torch.distributions.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ComposeTransform.sign(self)
torch.distributions.ExpTransform(Transform)
torch.distributions.ExpTransform.__eq__(self,other)
torch.distributions.ExpTransform._call(self,x)
torch.distributions.ExpTransform._inverse(self,y)
torch.distributions.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.LowerCholeskyTransform(Transform)
torch.distributions.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.LowerCholeskyTransform._call(self,x)
torch.distributions.LowerCholeskyTransform._call_on_event(self,x)
torch.distributions.LowerCholeskyTransform._inverse(self,y)
torch.distributions.LowerCholeskyTransform._inverse_on_event(self,y)
torch.distributions.PowerTransform(self,exponent,cache_size=0)
torch.distributions.PowerTransform.__eq__(self,other)
torch.distributions.PowerTransform._call(self,x)
torch.distributions.PowerTransform._inverse(self,y)
torch.distributions.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SigmoidTransform(Transform)
torch.distributions.SigmoidTransform.__eq__(self,other)
torch.distributions.SigmoidTransform._call(self,x)
torch.distributions.SigmoidTransform._inverse(self,y)
torch.distributions.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SoftmaxTransform(Transform)
torch.distributions.SoftmaxTransform.__eq__(self,other)
torch.distributions.SoftmaxTransform._call(self,x)
torch.distributions.SoftmaxTransform._inverse(self,y)
torch.distributions.StickBreakingTransform(Transform)
torch.distributions.StickBreakingTransform.__eq__(self,other)
torch.distributions.StickBreakingTransform._call(self,x)
torch.distributions.StickBreakingTransform._inverse(self,y)
torch.distributions.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform(self,cache_size=0)
torch.distributions.Transform.__eq__(self,other)
torch.distributions.Transform.__ne__(self,other)
torch.distributions.Transform.__repr__(self)
torch.distributions.Transform._call(self,x)
torch.distributions.Transform._inv_call(self,y)
torch.distributions.Transform._inverse(self,y)
torch.distributions.Transform.inv(self)
torch.distributions.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform.sign(self)
torch.distributions._InverseTransform(self,transform)
torch.distributions._InverseTransform.__eq__(self,other)
torch.distributions._InverseTransform.bijective(self)
torch.distributions._InverseTransform.codomain(self)
torch.distributions._InverseTransform.domain(self)
torch.distributions._InverseTransform.event_dim(self)
torch.distributions._InverseTransform.inv(self)
torch.distributions._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions._InverseTransform.sign(self)
torch.distributions.transforms.AbsTransform(Transform)
torch.distributions.transforms.AbsTransform.__eq__(self,other)
torch.distributions.transforms.AbsTransform._call(self,x)
torch.distributions.transforms.AbsTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform.__eq__(self,other)
torch.distributions.transforms.AffineTransform.__init__(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform._call(self,x)
torch.distributions.transforms.AffineTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.AffineTransform.sign(self)
torch.distributions.transforms.ComposeTransform(self,parts)
torch.distributions.transforms.ComposeTransform.__eq__(self,other)
torch.distributions.transforms.ComposeTransform.__init__(self,parts)
torch.distributions.transforms.ComposeTransform.__repr__(self)
torch.distributions.transforms.ComposeTransform.bijective(self)
torch.distributions.transforms.ComposeTransform.codomain(self)
torch.distributions.transforms.ComposeTransform.domain(self)
torch.distributions.transforms.ComposeTransform.event_dim(self)
torch.distributions.transforms.ComposeTransform.inv(self)
torch.distributions.transforms.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ComposeTransform.sign(self)
torch.distributions.transforms.ExpTransform(Transform)
torch.distributions.transforms.ExpTransform.__eq__(self,other)
torch.distributions.transforms.ExpTransform._call(self,x)
torch.distributions.transforms.ExpTransform._inverse(self,y)
torch.distributions.transforms.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.LowerCholeskyTransform(Transform)
torch.distributions.transforms.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.transforms.LowerCholeskyTransform._call(self,x)
torch.distributions.transforms.LowerCholeskyTransform._call_on_event(self,x)
torch.distributions.transforms.LowerCholeskyTransform._inverse(self,y)
torch.distributions.transforms.LowerCholeskyTransform._inverse_on_event(self,y)
torch.distributions.transforms.PowerTransform(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform.__eq__(self,other)
torch.distributions.transforms.PowerTransform.__init__(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform._call(self,x)
torch.distributions.transforms.PowerTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SigmoidTransform(Transform)
torch.distributions.transforms.SigmoidTransform.__eq__(self,other)
torch.distributions.transforms.SigmoidTransform._call(self,x)
torch.distributions.transforms.SigmoidTransform._inverse(self,y)
torch.distributions.transforms.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SoftmaxTransform(Transform)
torch.distributions.transforms.SoftmaxTransform.__eq__(self,other)
torch.distributions.transforms.SoftmaxTransform._call(self,x)
torch.distributions.transforms.SoftmaxTransform._inverse(self,y)
torch.distributions.transforms.StickBreakingTransform(Transform)
torch.distributions.transforms.StickBreakingTransform.__eq__(self,other)
torch.distributions.transforms.StickBreakingTransform._call(self,x)
torch.distributions.transforms.StickBreakingTransform._inverse(self,y)
torch.distributions.transforms.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform(self,cache_size=0)
torch.distributions.transforms.Transform.__eq__(self,other)
torch.distributions.transforms.Transform.__init__(self,cache_size=0)
torch.distributions.transforms.Transform.__ne__(self,other)
torch.distributions.transforms.Transform.__repr__(self)
torch.distributions.transforms.Transform._call(self,x)
torch.distributions.transforms.Transform._inv_call(self,y)
torch.distributions.transforms.Transform._inverse(self,y)
torch.distributions.transforms.Transform.inv(self)
torch.distributions.transforms.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform.sign(self)
torch.distributions.transforms._InverseTransform(self,transform)
torch.distributions.transforms._InverseTransform.__eq__(self,other)
torch.distributions.transforms._InverseTransform.__init__(self,transform)
torch.distributions.transforms._InverseTransform.bijective(self)
torch.distributions.transforms._InverseTransform.codomain(self)
torch.distributions.transforms._InverseTransform.domain(self)
torch.distributions.transforms._InverseTransform.event_dim(self)
torch.distributions.transforms._InverseTransform.inv(self)
torch.distributions.transforms._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms._InverseTransform.sign(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/laplace.py----------------------------------------
A:torch.distributions.laplace.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.laplace.batch_shape->torch.Size(batch_shape)
A:torch.distributions.laplace.new->self._get_checked_instance(Laplace, _instance)
A:torch.distributions.laplace.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.laplace.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.laplace.shape->self._extended_shape(sample_shape)
A:torch.distributions.laplace.u->self.loc.new(shape).uniform_(_finfo(self.loc).eps - 1, 1)
torch.distributions.Laplace(self,loc,scale,validate_args=None)
torch.distributions.Laplace.cdf(self,value)
torch.distributions.Laplace.entropy(self)
torch.distributions.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.Laplace.icdf(self,value)
torch.distributions.Laplace.log_prob(self,value)
torch.distributions.Laplace.mean(self)
torch.distributions.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.Laplace.stddev(self)
torch.distributions.Laplace.variance(self)
torch.distributions.laplace.Laplace(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.__init__(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.cdf(self,value)
torch.distributions.laplace.Laplace.entropy(self)
torch.distributions.laplace.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.laplace.Laplace.icdf(self,value)
torch.distributions.laplace.Laplace.log_prob(self,value)
torch.distributions.laplace.Laplace.mean(self)
torch.distributions.laplace.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.laplace.Laplace.stddev(self)
torch.distributions.laplace.Laplace.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/binomial.py----------------------------------------
A:torch.distributions.binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.binomial.is_scalar->isinstance(self.logits, Number)
A:torch.distributions.binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.binomial.new->self._get_checked_instance(Binomial, _instance)
A:torch.distributions.binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.binomial.max_count->max(int(self.total_count.max()), 1)
A:torch.distributions.binomial.bernoullis->torch.bernoulli(self.probs.unsqueeze(-1).expand(shape))
A:torch.distributions.binomial.arange->torch.arange(max_count, dtype=self._param.dtype, device=self._param.device)
A:torch.distributions.binomial.log_factorial_n->torch.lgamma(self.total_count + 1)
A:torch.distributions.binomial.log_factorial_k->torch.lgamma(value + 1)
A:torch.distributions.binomial.log_factorial_nmk->torch.lgamma(self.total_count - value + 1)
A:torch.distributions.binomial.max_val->(-self.logits).clamp(min=0.0)
A:torch.distributions.binomial.total_count->int(self.total_count.max())
A:torch.distributions.binomial.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Binomial._new(self,*args,**kwargs)
torch.distributions.Binomial.enumerate_support(self,expand=True)
torch.distributions.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.Binomial.log_prob(self,value)
torch.distributions.Binomial.logits(self)
torch.distributions.Binomial.mean(self)
torch.distributions.Binomial.param_shape(self)
torch.distributions.Binomial.probs(self)
torch.distributions.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.Binomial.support(self)
torch.distributions.Binomial.variance(self)
torch.distributions.binomial.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial._new(self,*args,**kwargs)
torch.distributions.binomial.Binomial.enumerate_support(self,expand=True)
torch.distributions.binomial.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.binomial.Binomial.log_prob(self,value)
torch.distributions.binomial.Binomial.logits(self)
torch.distributions.binomial.Binomial.mean(self)
torch.distributions.binomial.Binomial.param_shape(self)
torch.distributions.binomial.Binomial.probs(self)
torch.distributions.binomial.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.binomial.Binomial.support(self)
torch.distributions.binomial.Binomial.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/studentT.py----------------------------------------
A:torch.distributions.studentT.m->self.df.clone()
A:torch.distributions.studentT.(self.df, self.loc, self.scale)->broadcast_all(df, loc, scale)
A:torch.distributions.studentT.self._chi2->Chi2(self.df)
A:torch.distributions.studentT.batch_shape->torch.Size(batch_shape)
A:torch.distributions.studentT.new->self._get_checked_instance(StudentT, _instance)
A:torch.distributions.studentT.new.df->self.df.expand(batch_shape)
A:torch.distributions.studentT.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.studentT.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.studentT.new._chi2->self._chi2.expand(batch_shape)
A:torch.distributions.studentT.shape->self._extended_shape(sample_shape)
A:torch.distributions.studentT.X->_standard_normal(shape, dtype=self.df.dtype, device=self.df.device)
A:torch.distributions.studentT.Z->self._chi2.rsample(sample_shape)
torch.distributions.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.StudentT.entropy(self)
torch.distributions.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.StudentT.log_prob(self,value)
torch.distributions.StudentT.mean(self)
torch.distributions.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.StudentT.variance(self)
torch.distributions.studentT.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.__init__(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.entropy(self)
torch.distributions.studentT.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.studentT.StudentT.log_prob(self,value)
torch.distributions.studentT.StudentT.mean(self)
torch.distributions.studentT.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.studentT.StudentT.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/distribution.py----------------------------------------
A:torch.distributions.distribution.sample_shape->torch.Size(sample_shape)
A:torch.distributions.distribution.actual_shape->value.size()
A:torch.distributions.distribution.args_string->', '.join(['{}: {}'.format(p, self.__dict__[p] if self.__dict__[p].dim() == 0 else self.__dict__[p].size()) for p in param_names])
torch.distributions.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.Distribution.__repr__(self)
torch.distributions.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.Distribution._validate_sample(self,value)
torch.distributions.Distribution.arg_constraints(self)
torch.distributions.Distribution.batch_shape(self)
torch.distributions.Distribution.cdf(self,value)
torch.distributions.Distribution.entropy(self)
torch.distributions.Distribution.enumerate_support(self,expand=True)
torch.distributions.Distribution.event_shape(self)
torch.distributions.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.Distribution.icdf(self,value)
torch.distributions.Distribution.log_prob(self,value)
torch.distributions.Distribution.mean(self)
torch.distributions.Distribution.perplexity(self)
torch.distributions.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample_n(self,n)
torch.distributions.Distribution.set_default_validate_args(value)
torch.distributions.Distribution.stddev(self)
torch.distributions.Distribution.support(self)
torch.distributions.Distribution.variance(self)
torch.distributions.distribution.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__init__(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__repr__(self)
torch.distributions.distribution.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.distribution.Distribution._validate_sample(self,value)
torch.distributions.distribution.Distribution.arg_constraints(self)
torch.distributions.distribution.Distribution.batch_shape(self)
torch.distributions.distribution.Distribution.cdf(self,value)
torch.distributions.distribution.Distribution.entropy(self)
torch.distributions.distribution.Distribution.enumerate_support(self,expand=True)
torch.distributions.distribution.Distribution.event_shape(self)
torch.distributions.distribution.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.distribution.Distribution.icdf(self,value)
torch.distributions.distribution.Distribution.log_prob(self,value)
torch.distributions.distribution.Distribution.mean(self)
torch.distributions.distribution.Distribution.perplexity(self)
torch.distributions.distribution.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample_n(self,n)
torch.distributions.distribution.Distribution.set_default_validate_args(value)
torch.distributions.distribution.Distribution.stddev(self)
torch.distributions.distribution.Distribution.support(self)
torch.distributions.distribution.Distribution.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/negative_binomial.py----------------------------------------
A:torch.distributions.negative_binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.negative_binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.negative_binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.negative_binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.negative_binomial.new->self._get_checked_instance(NegativeBinomial, _instance)
A:torch.distributions.negative_binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.negative_binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.negative_binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.negative_binomial.rate->self._gamma.sample(sample_shape=sample_shape)
torch.distributions.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.NegativeBinomial._gamma(self)
torch.distributions.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.NegativeBinomial.log_prob(self,value)
torch.distributions.NegativeBinomial.logits(self)
torch.distributions.NegativeBinomial.mean(self)
torch.distributions.NegativeBinomial.param_shape(self)
torch.distributions.NegativeBinomial.probs(self)
torch.distributions.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.NegativeBinomial.variance(self)
torch.distributions.negative_binomial.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial.__init__(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial._gamma(self)
torch.distributions.negative_binomial.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.negative_binomial.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.negative_binomial.NegativeBinomial.log_prob(self,value)
torch.distributions.negative_binomial.NegativeBinomial.logits(self)
torch.distributions.negative_binomial.NegativeBinomial.mean(self)
torch.distributions.negative_binomial.NegativeBinomial.param_shape(self)
torch.distributions.negative_binomial.NegativeBinomial.probs(self)
torch.distributions.negative_binomial.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.negative_binomial.NegativeBinomial.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/fishersnedecor.py----------------------------------------
A:torch.distributions.fishersnedecor.(self.df1, self.df2)->broadcast_all(df1, df2)
A:torch.distributions.fishersnedecor.self._gamma1->Gamma(self.df1 * 0.5, self.df1)
A:torch.distributions.fishersnedecor.self._gamma2->Gamma(self.df2 * 0.5, self.df2)
A:torch.distributions.fishersnedecor.batch_shape->torch.Size(batch_shape)
A:torch.distributions.fishersnedecor.new->self._get_checked_instance(FisherSnedecor, _instance)
A:torch.distributions.fishersnedecor.new.df1->self.df1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new.df2->self.df2.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma1->self._gamma1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma2->self._gamma2.expand(batch_shape)
A:torch.distributions.fishersnedecor.df2->self.df2.clone()
A:torch.distributions.fishersnedecor.shape->self._extended_shape(sample_shape)
A:torch.distributions.fishersnedecor.X1->self._gamma1.rsample(sample_shape).view(shape)
A:torch.distributions.fishersnedecor.X2->self._gamma2.rsample(sample_shape).view(shape)
torch.distributions.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.FisherSnedecor.log_prob(self,value)
torch.distributions.FisherSnedecor.mean(self)
torch.distributions.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.FisherSnedecor.variance(self)
torch.distributions.fishersnedecor.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.__init__(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.fishersnedecor.FisherSnedecor.log_prob(self,value)
torch.distributions.fishersnedecor.FisherSnedecor.mean(self)
torch.distributions.fishersnedecor.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.fishersnedecor.FisherSnedecor.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/pareto.py----------------------------------------
A:torch.distributions.pareto.(self.scale, self.alpha)->broadcast_all(scale, alpha)
A:torch.distributions.pareto.base_dist->Exponential(self.alpha)
A:torch.distributions.pareto.new->self._get_checked_instance(Pareto, _instance)
A:torch.distributions.pareto.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.pareto.new.alpha->self.alpha.expand(batch_shape)
A:torch.distributions.pareto.a->self.alpha.clone().clamp(min=2)
torch.distributions.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.Pareto.entropy(self)
torch.distributions.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.Pareto.mean(self)
torch.distributions.Pareto.support(self)
torch.distributions.Pareto.variance(self)
torch.distributions.pareto.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.__init__(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.entropy(self)
torch.distributions.pareto.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.pareto.Pareto.mean(self)
torch.distributions.pareto.Pareto.support(self)
torch.distributions.pareto.Pareto.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/half_cauchy.py----------------------------------------
A:torch.distributions.half_cauchy.base_dist->Cauchy(0, scale)
A:torch.distributions.half_cauchy.new->self._get_checked_instance(HalfCauchy, _instance)
torch.distributions.HalfCauchy(self,scale,validate_args=None)
torch.distributions.HalfCauchy.cdf(self,value)
torch.distributions.HalfCauchy.entropy(self)
torch.distributions.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.HalfCauchy.icdf(self,prob)
torch.distributions.HalfCauchy.log_prob(self,value)
torch.distributions.HalfCauchy.mean(self)
torch.distributions.HalfCauchy.scale(self)
torch.distributions.HalfCauchy.variance(self)
torch.distributions.half_cauchy.HalfCauchy(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.__init__(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.cdf(self,value)
torch.distributions.half_cauchy.HalfCauchy.entropy(self)
torch.distributions.half_cauchy.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.half_cauchy.HalfCauchy.icdf(self,prob)
torch.distributions.half_cauchy.HalfCauchy.log_prob(self,value)
torch.distributions.half_cauchy.HalfCauchy.mean(self)
torch.distributions.half_cauchy.HalfCauchy.scale(self)
torch.distributions.half_cauchy.HalfCauchy.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/independent.py----------------------------------------
A:torch.distributions.independent.new->self._get_checked_instance(Independent, _instance)
A:torch.distributions.independent.batch_shape->torch.Size(batch_shape)
A:torch.distributions.independent.new.base_dist->self.base_dist.expand(batch_shape + self.event_shape[:self.reinterpreted_batch_ndims])
A:torch.distributions.independent.log_prob->self.base_dist.log_prob(value)
A:torch.distributions.independent.entropy->self.base_dist.entropy()
torch.distributions.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.Independent.entropy(self)
torch.distributions.Independent.enumerate_support(self,expand=True)
torch.distributions.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.Independent.has_enumerate_support(self)
torch.distributions.Independent.has_rsample(self)
torch.distributions.Independent.log_prob(self,value)
torch.distributions.Independent.mean(self)
torch.distributions.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.Independent.support(self)
torch.distributions.Independent.variance(self)
torch.distributions.independent.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.__init__(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.entropy(self)
torch.distributions.independent.Independent.enumerate_support(self,expand=True)
torch.distributions.independent.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.independent.Independent.has_enumerate_support(self)
torch.distributions.independent.Independent.has_rsample(self)
torch.distributions.independent.Independent.log_prob(self,value)
torch.distributions.independent.Independent.mean(self)
torch.distributions.independent.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.support(self)
torch.distributions.independent.Independent.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/exponential.py----------------------------------------
A:torch.distributions.exponential.(self.rate,)->broadcast_all(rate)
A:torch.distributions.exponential.new->self._get_checked_instance(Exponential, _instance)
A:torch.distributions.exponential.batch_shape->torch.Size(batch_shape)
A:torch.distributions.exponential.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.exponential.shape->self._extended_shape(sample_shape)
A:torch.distributions.exponential.u->torch.rand(shape, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Exponential(self,rate,validate_args=None)
torch.distributions.Exponential._log_normalizer(self,x)
torch.distributions.Exponential._natural_params(self)
torch.distributions.Exponential.cdf(self,value)
torch.distributions.Exponential.entropy(self)
torch.distributions.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.Exponential.icdf(self,value)
torch.distributions.Exponential.log_prob(self,value)
torch.distributions.Exponential.mean(self)
torch.distributions.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.Exponential.stddev(self)
torch.distributions.Exponential.variance(self)
torch.distributions.exponential.Exponential(self,rate,validate_args=None)
torch.distributions.exponential.Exponential.__init__(self,rate,validate_args=None)
torch.distributions.exponential.Exponential._log_normalizer(self,x)
torch.distributions.exponential.Exponential._natural_params(self)
torch.distributions.exponential.Exponential.cdf(self,value)
torch.distributions.exponential.Exponential.entropy(self)
torch.distributions.exponential.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.exponential.Exponential.icdf(self,value)
torch.distributions.exponential.Exponential.log_prob(self,value)
torch.distributions.exponential.Exponential.mean(self)
torch.distributions.exponential.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.exponential.Exponential.stddev(self)
torch.distributions.exponential.Exponential.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/dirichlet.py----------------------------------------
A:torch.distributions.dirichlet.probs->torch._standard_gamma(concentration)
A:torch.distributions.dirichlet.total->self.concentration.expand(shape).sum(-1, True).expand_as(concentration)
A:torch.distributions.dirichlet.grad->torch._dirichlet_grad(x, concentration, total)
A:torch.distributions.dirichlet.x->_dirichlet_sample_nograd(concentration)
A:torch.distributions.dirichlet.new->self._get_checked_instance(Dirichlet, _instance)
A:torch.distributions.dirichlet.batch_shape->torch.Size(batch_shape)
A:torch.distributions.dirichlet.new.concentration->self.concentration.expand(batch_shape + self.event_shape)
A:torch.distributions.dirichlet.shape->self._extended_shape(sample_shape)
A:torch.distributions.dirichlet.concentration->self.concentration.expand(shape)
A:torch.distributions.dirichlet.con0->self.concentration.sum(-1, True)
A:torch.distributions.dirichlet.k->self.concentration.size(-1)
A:torch.distributions.dirichlet.a0->self.concentration.sum(-1)
torch.distributions.Dirichlet(self,concentration,validate_args=None)
torch.distributions.Dirichlet._log_normalizer(self,x)
torch.distributions.Dirichlet._natural_params(self)
torch.distributions.Dirichlet.entropy(self)
torch.distributions.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.Dirichlet.log_prob(self,value)
torch.distributions.Dirichlet.mean(self)
torch.distributions.Dirichlet.rsample(self,sample_shape=())
torch.distributions.Dirichlet.variance(self)
torch.distributions.dirichlet.Dirichlet(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet.__init__(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet._log_normalizer(self,x)
torch.distributions.dirichlet.Dirichlet._natural_params(self)
torch.distributions.dirichlet.Dirichlet.entropy(self)
torch.distributions.dirichlet.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.dirichlet.Dirichlet.log_prob(self,value)
torch.distributions.dirichlet.Dirichlet.mean(self)
torch.distributions.dirichlet.Dirichlet.rsample(self,sample_shape=())
torch.distributions.dirichlet.Dirichlet.variance(self)
torch.distributions.dirichlet._Dirichlet(Function)
torch.distributions.dirichlet._Dirichlet.backward(ctx,grad_output)
torch.distributions.dirichlet._Dirichlet.forward(ctx,concentration)
torch.distributions.dirichlet._Dirichlet_backward(x,concentration,grad_output)
torch.distributions.dirichlet._dirichlet_sample_nograd(concentration)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/log_normal.py----------------------------------------
A:torch.distributions.log_normal.base_dist->Normal(loc, scale)
A:torch.distributions.log_normal.new->self._get_checked_instance(LogNormal, _instance)
torch.distributions.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.LogNormal.entropy(self)
torch.distributions.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogNormal.loc(self)
torch.distributions.LogNormal.mean(self)
torch.distributions.LogNormal.scale(self)
torch.distributions.LogNormal.variance(self)
torch.distributions.log_normal.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.entropy(self)
torch.distributions.log_normal.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.log_normal.LogNormal.loc(self)
torch.distributions.log_normal.LogNormal.mean(self)
torch.distributions.log_normal.LogNormal.scale(self)
torch.distributions.log_normal.LogNormal.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/constraints.py----------------------------------------
A:torch.distributions.constraints.value_tril->value.tril()
A:torch.distributions.constraints.flattened_value->value.reshape((-1,) + matrix_shape)
A:torch.distributions.constraints.dependent->_Dependent()
A:torch.distributions.constraints.boolean->_Boolean()
A:torch.distributions.constraints.nonnegative_integer->_IntegerGreaterThan(0)
A:torch.distributions.constraints.positive_integer->_IntegerGreaterThan(1)
A:torch.distributions.constraints.real->_Real()
A:torch.distributions.constraints.real_vector->_RealVector()
A:torch.distributions.constraints.positive->_GreaterThan(0.0)
A:torch.distributions.constraints.unit_interval->_Interval(0.0, 1.0)
A:torch.distributions.constraints.simplex->_Simplex()
A:torch.distributions.constraints.lower_triangular->_LowerTriangular()
A:torch.distributions.constraints.lower_cholesky->_LowerCholesky()
A:torch.distributions.constraints.positive_definite->_PositiveDefinite()
torch.distributions.constraints.Constraint(object)
torch.distributions.constraints.Constraint.__repr__(self)
torch.distributions.constraints.Constraint.check(self,value)
torch.distributions.constraints._Boolean(Constraint)
torch.distributions.constraints._Boolean.check(self,value)
torch.distributions.constraints._Dependent(Constraint)
torch.distributions.constraints._Dependent.check(self,x)
torch.distributions.constraints._DependentProperty(property,_Dependent)
torch.distributions.constraints._GreaterThan(self,lower_bound)
torch.distributions.constraints._GreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThan.__repr__(self)
torch.distributions.constraints._GreaterThan.check(self,value)
torch.distributions.constraints._GreaterThanEq(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__repr__(self)
torch.distributions.constraints._GreaterThanEq.check(self,value)
torch.distributions.constraints._HalfOpenInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__repr__(self)
torch.distributions.constraints._HalfOpenInterval.check(self,value)
torch.distributions.constraints._IntegerGreaterThan(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__repr__(self)
torch.distributions.constraints._IntegerGreaterThan.check(self,value)
torch.distributions.constraints._IntegerInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__repr__(self)
torch.distributions.constraints._IntegerInterval.check(self,value)
torch.distributions.constraints._IntegerLessThan(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__init__(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__repr__(self)
torch.distributions.constraints._IntegerLessThan.check(self,value)
torch.distributions.constraints._Interval(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__repr__(self)
torch.distributions.constraints._Interval.check(self,value)
torch.distributions.constraints._LessThan(self,upper_bound)
torch.distributions.constraints._LessThan.__init__(self,upper_bound)
torch.distributions.constraints._LessThan.__repr__(self)
torch.distributions.constraints._LessThan.check(self,value)
torch.distributions.constraints._LowerCholesky(Constraint)
torch.distributions.constraints._LowerCholesky.check(self,value)
torch.distributions.constraints._LowerTriangular(Constraint)
torch.distributions.constraints._LowerTriangular.check(self,value)
torch.distributions.constraints._PositiveDefinite(Constraint)
torch.distributions.constraints._PositiveDefinite.check(self,value)
torch.distributions.constraints._Real(Constraint)
torch.distributions.constraints._Real.check(self,value)
torch.distributions.constraints._RealVector(Constraint)
torch.distributions.constraints._RealVector.check(self,value)
torch.distributions.constraints._Simplex(Constraint)
torch.distributions.constraints._Simplex.check(self,value)
torch.distributions.constraints.is_dependent(constraint)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/half_normal.py----------------------------------------
A:torch.distributions.half_normal.base_dist->Normal(0, scale)
A:torch.distributions.half_normal.new->self._get_checked_instance(HalfNormal, _instance)
torch.distributions.HalfNormal(self,scale,validate_args=None)
torch.distributions.HalfNormal.cdf(self,value)
torch.distributions.HalfNormal.entropy(self)
torch.distributions.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.HalfNormal.icdf(self,prob)
torch.distributions.HalfNormal.log_prob(self,value)
torch.distributions.HalfNormal.mean(self)
torch.distributions.HalfNormal.scale(self)
torch.distributions.HalfNormal.variance(self)
torch.distributions.half_normal.HalfNormal(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.__init__(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.cdf(self,value)
torch.distributions.half_normal.HalfNormal.entropy(self)
torch.distributions.half_normal.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.half_normal.HalfNormal.icdf(self,prob)
torch.distributions.half_normal.HalfNormal.log_prob(self,value)
torch.distributions.half_normal.HalfNormal.mean(self)
torch.distributions.half_normal.HalfNormal.scale(self)
torch.distributions.half_normal.HalfNormal.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/gamma.py----------------------------------------
A:torch.distributions.gamma.(self.concentration, self.rate)->broadcast_all(concentration, rate)
A:torch.distributions.gamma.batch_shape->torch.Size(batch_shape)
A:torch.distributions.gamma.new->self._get_checked_instance(Gamma, _instance)
A:torch.distributions.gamma.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.gamma.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.gamma.shape->self._extended_shape(sample_shape)
torch.distributions.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.Gamma._log_normalizer(self,x,y)
torch.distributions.Gamma._natural_params(self)
torch.distributions.Gamma.entropy(self)
torch.distributions.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.Gamma.log_prob(self,value)
torch.distributions.Gamma.mean(self)
torch.distributions.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.Gamma.variance(self)
torch.distributions.gamma.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma.__init__(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma._log_normalizer(self,x,y)
torch.distributions.gamma.Gamma._natural_params(self)
torch.distributions.gamma.Gamma.entropy(self)
torch.distributions.gamma.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.gamma.Gamma.log_prob(self,value)
torch.distributions.gamma.Gamma.mean(self)
torch.distributions.gamma.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.gamma.Gamma.variance(self)
torch.distributions.gamma._standard_gamma(concentration)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/uniform.py----------------------------------------
A:torch.distributions.uniform.(self.low, self.high)->broadcast_all(low, high)
A:torch.distributions.uniform.batch_shape->torch.Size(batch_shape)
A:torch.distributions.uniform.new->self._get_checked_instance(Uniform, _instance)
A:torch.distributions.uniform.new.low->self.low.expand(batch_shape)
A:torch.distributions.uniform.new.high->self.high.expand(batch_shape)
A:torch.distributions.uniform.shape->self._extended_shape(sample_shape)
A:torch.distributions.uniform.rand->torch.rand(shape, dtype=self.low.dtype, device=self.low.device)
A:torch.distributions.uniform.lb->value.ge(self.low).type_as(self.low)
A:torch.distributions.uniform.ub->value.lt(self.high).type_as(self.low)
torch.distributions.Uniform(self,low,high,validate_args=None)
torch.distributions.Uniform.cdf(self,value)
torch.distributions.Uniform.entropy(self)
torch.distributions.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.Uniform.icdf(self,value)
torch.distributions.Uniform.log_prob(self,value)
torch.distributions.Uniform.mean(self)
torch.distributions.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.Uniform.stddev(self)
torch.distributions.Uniform.support(self)
torch.distributions.Uniform.variance(self)
torch.distributions.uniform.Uniform(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.__init__(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.cdf(self,value)
torch.distributions.uniform.Uniform.entropy(self)
torch.distributions.uniform.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.uniform.Uniform.icdf(self,value)
torch.distributions.uniform.Uniform.log_prob(self,value)
torch.distributions.uniform.Uniform.mean(self)
torch.distributions.uniform.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.uniform.Uniform.stddev(self)
torch.distributions.uniform.Uniform.support(self)
torch.distributions.uniform.Uniform.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributions/relaxed_categorical.py----------------------------------------
A:torch.distributions.relaxed_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.relaxed_categorical.new->self._get_checked_instance(RelaxedOneHotCategorical, _instance)
A:torch.distributions.relaxed_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.relaxed_categorical.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_categorical.uniforms->clamp_probs(torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device))
A:torch.distributions.relaxed_categorical.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_categorical.score->(score - score.logsumexp(dim=-1, keepdim=True)).sum(-1)
A:torch.distributions.relaxed_categorical.base_dist->ExpRelaxedCategorical(temperature, probs, logits)
torch.distributions.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedOneHotCategorical.logits(self)
torch.distributions.RelaxedOneHotCategorical.probs(self)
torch.distributions.RelaxedOneHotCategorical.temperature(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical._new(self,*args,**kwargs)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.log_prob(self,value)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.logits(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.param_shape(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.probs(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/jit/__init__.py----------------------------------------
A:torch.jit.__init__.value->_make_strong(value)
A:torch.jit.__init__._enabled->_parse_env('PYTORCH_JIT', True, '> Using PyTorch JIT', '> PyTorch JIT DISABLED')
A:torch.jit.__init__.tracing_state->torch._C._get_tracing_state()
A:torch.jit.__init__.m->ScriptModule()
A:torch.jit.__init__.curr->getattr(curr, name)
A:torch.jit.__init__.map_location->torch.device(map_location)
A:torch.jit.__init__.ret->ScriptModule().save_to_buffer()
A:torch.jit.__init__.state_dict->_make_strong(module).state_dict(keep_vars=keep_vars)
A:torch.jit.__init__.filtered_dict->type(state_dict)()
A:torch.jit.__init__.seen_ids->set()
A:torch.jit.__init__.frame->inspect.currentframe()
A:torch.jit.__init__.(in_vars, in_desc)->_flatten(args)
A:torch.jit.__init__.module_state->list(_unique_state_dict(self, keep_vars=True).values())
A:torch.jit.__init__.(trace, all_trace_inputs)->torch._C._tracer_enter(*in_vars + module_state)
A:torch.jit.__init__.trace_inputs->_unflatten(all_trace_inputs[:len(in_vars)], in_desc)
A:torch.jit.__init__.out->model(*args)
A:torch.jit.__init__.(out_vars, _)->_flatten(out)
A:torch.jit.__init__.v->getattr(mod, name)
A:torch.jit.__init__.v.grad->clone_input(v.grad)
A:torch.jit.__init__._JIT_DUMP->os.environ.get('PYTORCH_JIT_DUMP', False)
A:torch.jit.__init__._JIT_TIME->os.environ.get('PYTORCH_JIT_TIME', False)
A:torch.jit.__init__._JIT_DISABLE->os.environ.get('PYTORCH_JIT_DISABLE', False)
A:torch.jit.__init__._JIT_STATS->os.environ.get('PYTORCH_JIT_STATS', False)
A:torch.jit.__init__.filename->'{}_{}'.format(trace_name, pass_name)
A:torch.jit.__init__.stream->torch.cuda.current_stream()
A:torch.jit.__init__.start->torch.cuda.Event(enable_timing=True)
A:torch.jit.__init__.end->torch.cuda.Event(enable_timing=True)
A:torch.jit.__init__.is_module->isinstance(model, Module)
A:torch.jit.__init__.saved_args->_clone_inputs(args)
A:torch.jit.__init__.saved_state->copy.deepcopy(model.state_dict())
A:torch.jit.__init__.(in_vars, _)->_flatten((args, params))
A:torch.jit.__init__.loss->loss_fn(*out)
A:torch.jit.__init__.grads->torch.autograd.grad([loss], in_vars)
A:torch.jit.__init__.(uncompiled_outs, uncompiled_grads)->run_fwd_bwd(args, force_trace=True)
A:torch.jit.__init__.(compiled_outs, compiled_grads)->run_fwd_bwd(args, assert_compiled=True)
A:torch.jit.__init__.check_mod->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, _force_outplace=force_outplace, **executor_options)
A:torch.jit.__init__.mod_canonicalized->torch._C._jit_pass_canonicalize(module.graph)
A:torch.jit.__init__.check_canonicalized->torch._C._jit_pass_canonicalize(check_mod.graph)
A:torch.jit.__init__.graph_diff->difflib.ndiff(str(mod_canonicalized).splitlines(True), str(check_canonicalized).splitlines(True))
A:torch.jit.__init__.node_diff->difflib.ndiff(str(n_mod).splitlines(True), str(n_check).splitlines(True))
A:torch.jit.__init__.mod_stack->n_mod.getSourceLocation()
A:torch.jit.__init__.check_stack->n_check.getSourceLocation()
A:torch.jit.__init__.mod_tensor_val->n_mod.t('value')
A:torch.jit.__init__.check_tensor_val->n_check.t('value')
A:torch.jit.__init__.compare_stack->n_mod.getSourceLocation()
A:torch.jit.__init__.outs->wrap_retval(mod(*_clone_inputs(inputs)))
A:torch.jit.__init__.traced_outs->run_mod_and_filter_tensor_outputs(module, inputs, 'trace')
A:torch.jit.__init__.fn_outs->run_mod_and_filter_tensor_outputs(func, inputs, 'Python function')
A:torch.jit.__init__.check_outs->run_mod_and_filter_tensor_outputs(check_mod, inputs, 'repeated trace')
A:torch.jit.__init__.diag_info->graph_diagnostic_info()
A:torch.jit.__init__.example_inputs->tuple(example_inputs)
A:torch.jit.__init__.module->_make_strong(module)
A:torch.jit.__init__.var_lookup_fn->_create_interpreter_name_lookup_fn(0)
A:torch.jit.__init__.self.module->torch._C.ScriptModule()
A:torch.jit.__init__.rcb->createResolutionCallback(frames_up=1)
A:torch.jit.__init__.entry->_jit_internal._compiled_weak_fns.get(fn)
A:torch.jit.__init__.compiled_fn->torch.jit.script(fn, True, 0, entry['rcb'])
A:torch.jit.__init__._rcb->createResolutionCallback(frames_up=2)
A:torch.jit.__init__.ast->get_jit_ast(fn, is_method=True)
A:torch.jit.__init__.mod->script(fn, optimize, _frames_up)
A:torch.jit.__init__.ScriptMethodStub->namedtuple('ScriptMethodStub', ('resolution_callback', 'def_', 'original_method'))
A:torch.jit.__init__.res_graph->torch.to_batch_graph(mod.graph)
A:torch.jit.__init__.res_mod->ScriptModule()
A:torch.jit.__init__.arg->BatchTensor(arg, batch_size)
A:torch.jit.__init__.res->res_mod(*new_args)
A:torch.jit.__init__.self.module_ref->weakref.ref(module)
A:torch.jit.__init__.r->self._python_modules.items()
A:torch.jit.__init__.self._python_modules->OrderedDict()
A:torch.jit.__init__.constants->', '.join((typ.__name__ for typ in _constant_types))
A:torch.jit.__init__.original_init->getattr(cls, '__init__', lambda self: None)
A:torch.jit.__init__.super_constants->getattr(super(cls), '_constants_set', set())
A:torch.jit.__init__.cls._constants_set->set(getattr(cls, '__constants__', ())).union(super_constants)
A:torch.jit.__init__.self._parameters->OrderedParameterDict(self)
A:torch.jit.__init__.self._buffers->OrderedBufferDict(self)
A:torch.jit.__init__.self._modules->OrderedModuleDict(self)
A:torch.jit.__init__.script_method->self._get_method(attr)
A:torch.jit.__init__.self.__dict__['_original']->weakref.ref(original)
A:torch.jit.__init__.item->getattr(original, name)
A:torch.jit.__init__.self.__dict__['_constants_set']->set(getattr(original, '__constants__', []))
A:torch.jit.__init__.func->get_function_from_type(cls, name)
A:torch.jit.__init__.stub->script_method(entry['original_method'], entry['rcb'])
A:torch.jit.__init__.stubs->_get_weak_stubs(type(mod))
A:torch.jit.__init__.proxy->WeakScriptModuleProxy(mod, stubs)
A:torch.jit.__init__.id_set->set()
A:torch.jit.__init__.orig->torch.nn.Module()
A:torch.jit.__init__.self._modules[name]->TracedModule(submodule, id_set, optimize=optimize)
A:torch.jit.__init__.keys->super(_ConstModuleList, self).__dir__()
A:torch.jit.__init__.self.state->torch._C._get_tracing_state()
torch.jit.__init__.CompilationUnit(self,lang=None,optimize=True,_frames_up=0)
torch.jit.__init__.CompilationUnit.__getattr__(self,attr)
torch.jit.__init__.CompilationUnit.__init__(self,lang=None,optimize=True,_frames_up=0)
torch.jit.__init__.CompilationUnit.define(self,lang,rcb=None,_frames_up=0)
torch.jit.__init__.LegacyTracedModule(self,inner,force_outplace=False)
torch.jit.__init__.LegacyTracedModule.__init__(self,inner,force_outplace=False)
torch.jit.__init__.LegacyTracedModule.forward(self,*args)
torch.jit.__init__.OrderedBufferDict(self,module)
torch.jit.__init__.OrderedBufferDict.__contains__(self,k)
torch.jit.__init__.OrderedBufferDict.__getitem__(self,k)
torch.jit.__init__.OrderedBufferDict.__init__(self,module)
torch.jit.__init__.OrderedBufferDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedBufferDict.items(self)
torch.jit.__init__.OrderedDictWrapper(self,module)
torch.jit.__init__.OrderedDictWrapper.__contains__(self,k)
torch.jit.__init__.OrderedDictWrapper.__delitem__(self,k)
torch.jit.__init__.OrderedDictWrapper.__getitem__(self,k)
torch.jit.__init__.OrderedDictWrapper.__init__(self,module)
torch.jit.__init__.OrderedDictWrapper.__setitem__(self,k,v)
torch.jit.__init__.OrderedDictWrapper.items(self)
torch.jit.__init__.OrderedDictWrapper.keys(self)
torch.jit.__init__.OrderedDictWrapper.module(self)
torch.jit.__init__.OrderedDictWrapper.values(self)
torch.jit.__init__.OrderedModuleDict(self,module)
torch.jit.__init__.OrderedModuleDict.__contains__(self,k)
torch.jit.__init__.OrderedModuleDict.__getitem__(self,k)
torch.jit.__init__.OrderedModuleDict.__init__(self,module)
torch.jit.__init__.OrderedModuleDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedModuleDict.items(self)
torch.jit.__init__.OrderedParameterDict(self,module)
torch.jit.__init__.OrderedParameterDict.__contains__(self,k)
torch.jit.__init__.OrderedParameterDict.__getitem__(self,k)
torch.jit.__init__.OrderedParameterDict.__init__(self,module)
torch.jit.__init__.OrderedParameterDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedParameterDict.items(self)
torch.jit.__init__.ScriptMeta(cls,name,bases,attrs)
torch.jit.__init__.ScriptMeta.__init__(cls,name,bases,attrs)
torch.jit.__init__.TopLevelTracedModule(TracedModule)
torch.jit.__init__.TopLevelTracedModule.forward(self,*args,**kwargs)
torch.jit.__init__.TracedModule(self,orig,id_set=None,optimize=True)
torch.jit.__init__.TracedModule.__init__(self,orig,id_set=None,optimize=True)
torch.jit.__init__.TracedModule.__setattr__(self,attr,value)
torch.jit.__init__.TracedModule._freeze(self)
torch.jit.__init__.TracedModule._get_name(self)
torch.jit.__init__.TracedModule.forward(self,*args,**kwargs)
torch.jit.__init__.TracerWarning(Warning)
torch.jit.__init__.TracerWarning.ignore_lib_warnings()
torch.jit.__init__.TracingCheckError(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit.__init__.TracingCheckError.__init__(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit.__init__._ConstModuleList(self,modules)
torch.jit.__init__._ConstModuleList.__dir__(self)
torch.jit.__init__._ConstModuleList.__getitem__(self,idx)
torch.jit.__init__._ConstModuleList.__init__(self,modules)
torch.jit.__init__._ConstModuleList.__iter__(self)
torch.jit.__init__._ConstModuleList.__len__(self)
torch.jit.__init__._ConstSequential(self,mods)
torch.jit.__init__._ConstSequential.__init__(self,mods)
torch.jit.__init__._check_trace(check_inputs,func,executor_options,module,check_tolerance,force_outplace)
torch.jit.__init__._clone_inputs(args)
torch.jit.__init__._create_interpreter_name_lookup_fn(frames_up=1)
torch.jit.__init__._create_methods_from_stubs(self,stubs)
torch.jit.__init__._disable_tracing(object)
torch.jit.__init__._disable_tracing.__enter__(self)
torch.jit.__init__._disable_tracing.__exit__(self,*args)
torch.jit.__init__._dump_trace(trace_name,pass_name,input_key,trace)
torch.jit.__init__._find_builtin(fn)
torch.jit.__init__._get_builtin_table()
torch.jit.__init__._get_methods(cls)
torch.jit.__init__._get_valid_constant(attr,v)
torch.jit.__init__._get_weak_stubs(cls)
torch.jit.__init__._is_weak_type(cls)
torch.jit.__init__._make_fail(name)
torch.jit.__init__._make_strong(mod)
torch.jit.__init__._parse_env(name,default,true_message,false_message)
torch.jit.__init__._register_builtin(fn,op)
torch.jit.__init__._time(trace_name,name,time=True)
torch.jit.__init__._try_compile_weak_script(fn)
torch.jit.__init__._try_get_dispatched_fn(fn)
torch.jit.__init__._try_get_weak_module(mod)
torch.jit.__init__._unique_state_dict(module,keep_vars=False)
torch.jit.__init__._unwrap_optional(x)
torch.jit.__init__._verify_equal(xs,ys)
torch.jit.__init__.annotate(the_type,the_value)
torch.jit.__init__.batch(batch_size=1,optimize=True,_frames_up=0)
torch.jit.__init__.get_trace_graph(f,args=(),kwargs=None,_force_outplace=False)
torch.jit.__init__.indent(s)
torch.jit.__init__.load(f,map_location=None)
torch.jit.__init__.save(m,f)
torch.jit.__init__.scope(scope_name)
torch.jit.__init__.script(fn,optimize=True,_frames_up=0,_rcb=None)
torch.jit.__init__.script_method(fn,_rcb=None)
torch.jit.__init__.trace(func,example_inputs,optimize=True,check_trace=True,check_inputs=None,check_tolerance=1e-05,_force_outplace=False)
torch.jit.__init__.verify(model,args,loss_fn=torch.sum,devices=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/jit/supported_ops.py----------------------------------------
A:torch.jit.supported_ops.v->'\n{}{}'.format(' ' * indent, v)
A:torch.jit.supported_ops.qualified_name->'{}.{}'.format(mod, name)
A:torch.jit.supported_ops.schema->torch.jit._try_compile_weak_script(attr)._get_method('forward').schema()
A:torch.jit.supported_ops.builtin->torch.jit._find_builtin(getattr(mod, elem))
A:torch.jit.supported_ops.schemas->torch._C._jit_get_schemas_for_operator('aten::' + elem)
A:torch.jit.supported_ops.attr->getattr(mod, elem)
A:torch.jit.supported_ops.scripted->torch.jit._try_compile_weak_script(attr)
A:torch.jit.supported_ops.__doc__->_list_supported_ops()
torch.jit.supported_ops._list_supported_ops()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/jit/frontend.py----------------------------------------
A:torch.jit.frontend._identifier_chars->set(string.ascii_lowercase + string.ascii_uppercase + string.digits)
A:torch.jit.frontend.node_type->type(offending_node)
A:torch.jit.frontend.range_len->len(node_start_tokens.get(node_type, ' '))
A:torch.jit.frontend.source_range->SourceContext(source, _uses_true_division(fn)).make_range(offending_node.lineno, offending_node.col_offset, offending_node.col_offset + range_len)
A:torch.jit.frontend.feature_name->pretty_node_names.get(node_type, node_type.__name__)
A:torch.jit.frontend.msg->"{} aren't supported".format(feature_name)
A:torch.jit.frontend.source->dedent(inspect.getsource(fn))
A:torch.jit.frontend.py_ast->ast.parse(source)
A:torch.jit.frontend.type_line->torch.jit.annotations.get_type_line(source)
A:torch.jit.frontend.ctx->SourceContext(source, _uses_true_division(fn))
A:torch.jit.frontend.method->getattr(self, 'build_' + node.__class__.__name__, None)
A:torch.jit.frontend.r->SourceContext(source, _uses_true_division(fn)).make_range(expr.lineno, expr.col_offset, expr.col_offset + 1)
A:torch.jit.frontend.param_list->build_param_list(ctx, py_def.args)
A:torch.jit.frontend.return_type->build_expr(ctx, py_def.returns)
A:torch.jit.frontend.decl->torch._C.merge_type_from_type_comment(decl, type_comment_decl, is_method)
A:torch.jit.frontend.type_comment_decl->torch._C.parse_type_comment(type_line)
A:torch.jit.frontend.annotation_expr->Var(Ident(r, 'Tensor'))
A:torch.jit.frontend.argspec->inspect.getargspec(fn)
A:torch.jit.frontend.signature->inspect.signature(fn)
A:torch.jit.frontend.rhs->build_expr(ctx, expr.right)
A:torch.jit.frontend.start_point->SourceContext(source, _uses_true_division(fn)).make_range(stmt.lineno, stmt.col_offset, stmt.col_offset + 1)
A:torch.jit.frontend.lhs->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.expr->build_expr(ctx, stmt.exc)
A:torch.jit.frontend.test->build_expr(ctx, stmt.test)
A:torch.jit.frontend.op->type(op_)
A:torch.jit.frontend.value->str(expr.s)
A:torch.jit.frontend.name_range->SourceContext(source, _uses_true_division(fn)).make_raw_range(start_pos, pos)
A:torch.jit.frontend.func->build_expr(ctx, expr.func)
A:torch.jit.frontend.stararg_expr->build_expr(ctx, expr.starargs)
A:torch.jit.frontend.kw_expr->build_expr(ctx, kw.value)
A:torch.jit.frontend.op_token->ExprBuilder.cmpop_map.get(op)
A:torch.jit.frontend.err_range->SourceContext(source, _uses_true_division(fn)).make_raw_range(lhs.range().end, rhs.range().start)
A:torch.jit.frontend.sub_expr->build_expr(ctx, expr.operand)
A:torch.jit.frontend.cmp_expr->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.result->BinOp('and', result, cmp_expr)
A:torch.jit.frontend.step->build_expr(ctx, slice_expr.step)
A:torch.jit.frontend.sub_type->type(expr.slice)
A:torch.jit.frontend.base->build_expr(ctx, expr.value)
A:torch.jit.frontend.build_expr->ExprBuilder()
A:torch.jit.frontend.build_stmt->StmtBuilder()
A:torch.jit.frontend.new_pos->SourceContext(source, _uses_true_division(fn)).source[:pos].rindex(substr)
torch.jit.frontend.Builder(self,ctx,node)
torch.jit.frontend.Builder.__call__(self,ctx,node)
torch.jit.frontend.ExprBuilder(Builder)
torch.jit.frontend.ExprBuilder.build_Attribute(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BinOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BoolOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Call(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Compare(ctx,expr)
torch.jit.frontend.ExprBuilder.build_IfExp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_List(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Name(ctx,expr)
torch.jit.frontend.ExprBuilder.build_NameConstant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Num(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Starred(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Str(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Subscript(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Tuple(ctx,expr)
torch.jit.frontend.ExprBuilder.build_UnaryOp(ctx,expr)
torch.jit.frontend.FrontendError(self,source_range,msg)
torch.jit.frontend.FrontendError.__init__(self,source_range,msg)
torch.jit.frontend.FrontendError.__str__(self)
torch.jit.frontend.FrontendTypeError(FrontendError)
torch.jit.frontend.NotSupportedError(FrontendError)
torch.jit.frontend.SourceContext(self,source,uses_true_division=True)
torch.jit.frontend.SourceContext.__init__(self,source,uses_true_division=True)
torch.jit.frontend.StmtBuilder(Builder)
torch.jit.frontend.StmtBuilder.build_Assert(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_AugAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Expr(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_For(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_If(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Pass(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Print(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Raise(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Return(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_While(ctx,stmt)
torch.jit.frontend.UnsupportedNodeError(self,ctx,offending_node)
torch.jit.frontend.UnsupportedNodeError.__init__(self,ctx,offending_node)
torch.jit.frontend._uses_true_division(fn)
torch.jit.frontend.build_def(ctx,py_def,type_line,is_method)
torch.jit.frontend.build_param(ctx,py_arg)
torch.jit.frontend.build_param_list(ctx,py_args)
torch.jit.frontend.build_stmts(ctx,stmts)
torch.jit.frontend.find_after(ctx,pos,substr,offsets=(0,0))
torch.jit.frontend.find_before(ctx,pos,substr,offsets=(0,0))
torch.jit.frontend.get_default_args(fn)
torch.jit.frontend.get_jit_ast(fn,is_method)
torch.jit.frontend.is_reserved_name(name)
torch.jit.get_default_args(fn)
torch.jit.get_jit_ast(fn,is_method)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/jit/batchop.py----------------------------------------
A:torch.jit.batchop.data->data.unsqueeze(0).unsqueeze(0)
A:torch.jit.batchop.alpha->float(alpha_)
A:torch.jit.batchop.dims->torch.zeros([0], dtype=torch.uint8)
A:torch.jit.batchop.mask->torch.ones([1], dtype=torch.uint8)
A:torch.jit.batchop.data1->data1.unsqueeze(-2).unsqueeze(-2)
A:torch.jit.batchop.data2->data2.unsqueeze(-1).unsqueeze(-1)
A:torch.jit.batchop.dim->int(dim_)
A:torch.jit.batchop.index->int(index_)
A:torch.jit.batchop.other->float(other_)
A:torch.jit.batchop.res_data->data.unsqueeze(0).unsqueeze(0).view(data_sizes)
A:torch.jit.batchop.batch_size->data.unsqueeze(0).unsqueeze(0).size(0)
A:torch.jit.batchop.res_mask->torch.ones([1], dtype=torch.uint8).view(mask_sizes)
A:torch.jit.batchop.d->data[i].unsqueeze(0).softmax(dim)
A:torch.jit.batchop.m->mask[i].transpose(0, dim - 1)
A:torch.jit.batchop.cond_data->data.unsqueeze(0).unsqueeze(0).expand_as(data1)
A:torch.jit.batchop.cond_mask->data.unsqueeze(0).unsqueeze(0).expand_as(mask1)
A:torch.jit.batchop.res_dims->torch.cat([res_dims, torch.ones([1], dtype=torch.int) * cur_dim])
A:torch.jit.batchop.cond->torch.zeros([1], dtype=torch.uint8)
A:torch.jit.batchop.keepdim->bool(keepdim_)
A:torch.jit.batchop.valid_num->int(valid_num)
A:torch.jit.batchop.k->int(k_)
A:torch.jit.batchop.largest->bool(largest_)
A:torch.jit.batchop.sorted->bool(sorted_)
A:torch.jit.batchop.res_index->torch.cat([res_index, idx], 0)
A:torch.jit.batchop.(d, idx)->data[i].unsqueeze(0).softmax(dim).topk(k, dim, largest, sorted)
A:torch.jit.batchop.max_len->data.unsqueeze(0).unsqueeze(0).size(dim)
A:torch.jit.batchop.sizes->sizes.type_as(torch.ones([1], dtype=torch.int)).type_as(torch.ones([1], dtype=torch.int))
A:torch.jit.batchop.data_sizes_->torch.cat([torch.ones([1], dtype=torch.int) * batch_size, sizes.narrow(0, 1, sizes.size(0) - 1)], 0)
A:torch.jit.batchop.data_sizes->torch.cat([torch.ones([1], dtype=torch.int) * batch_size, sizes.narrow(0, 1, sizes.size(0) - 1)], 0)._tensor_to_list()
A:torch.jit.batchop.mask_sizes_->torch.cat([mask_sizes_, torch.ones([1], dtype=torch.int) * cur_size_])
A:torch.jit.batchop.cur_size_->torch.ones([1], dtype=torch.uint8).size(i)
A:torch.jit.batchop.mask_sizes->torch.cat([mask_sizes_, torch.ones([1], dtype=torch.int) * cur_size_])._tensor_to_list()
A:torch.jit.batchop.dimension->int(dimension_)
A:torch.jit.batchop.start->int(start_)
A:torch.jit.batchop.length->int(length_)
torch.jit.batchop.batch_add(data1,mask1,dims1,data2,mask2,dims2,alpha_)
torch.jit.batchop.batch_add_scalar(data,mask,dims,other,alpha_)
torch.jit.batchop.batch_any(data,mask,dims)
torch.jit.batchop.batch_argmax(data,mask,dims,dim_,keepdim_)
torch.jit.batchop.batch_cat2(data1,mask1,dims1,data2,mask2,dims2,dim_)
torch.jit.batchop.batch_cat3(data1,mask1,dims1,data2,mask2,dims2,data3,mask3,dims3,dim_)
torch.jit.batchop.batch_dim(data,mask,dims)
torch.jit.batchop.batch_div(data,mask,dims,other)
torch.jit.batchop.batch_eq(data,mask,dims,data1,mask1,dims1)
torch.jit.batchop.batch_fmod(data,mask,dims,other_)
torch.jit.batchop.batch_from_scalar_tensor(data)
torch.jit.batchop.batch_gt(data,mask,dims,data1,mask1,dims1)
torch.jit.batchop.batch_gt_one_scalar(data,mask,dims,other_)
torch.jit.batchop.batch_gt_scalar(data1,data2)
torch.jit.batchop.batch_index_select(data,mask,dims,dim_,index_data,index_mask,index_dims)
torch.jit.batchop.batch_lt(data,mask,dims,data1,mask1,dims1)
torch.jit.batchop.batch_matmul(data1,mask1,dims1,data2,mask2,dims2)
torch.jit.batchop.batch_mm(data1,mask1,dims1,data2,mask2,dims2)
torch.jit.batchop.batch_mul(data1,mask1,dims1,data2,mask2,dims2)
torch.jit.batchop.batch_mul_scalar(data1,data2)
torch.jit.batchop.batch_narrow(data,mask,dims,dimension_,start_,length_)
torch.jit.batchop.batch_neg(data,mask,dims)
torch.jit.batchop.batch_neg_scalar(data)
torch.jit.batchop.batch_relu(data,mask,dims)
torch.jit.batchop.batch_select(data,mask,dims,dim_,index_)
torch.jit.batchop.batch_sigmoid(data,mask,dims)
torch.jit.batchop.batch_size(data,mask,dims,dim_)
torch.jit.batchop.batch_softmax(data,mask,dims,dim_)
torch.jit.batchop.batch_squeeze(data,mask,dims,dim_)
torch.jit.batchop.batch_sub(data1,mask1,dims1,data2,mask2,dims2,alpha_)
torch.jit.batchop.batch_sub_scalar(data1,data2)
torch.jit.batchop.batch_sum(data,mask,dims)
torch.jit.batchop.batch_tanh(data,mask,dims)
torch.jit.batchop.batch_topk(data,mask,dims,k_,dim_,largest_,sorted_)
torch.jit.batchop.batch_type_as(data,mask,dims,data1,mask1,dims1)
torch.jit.batchop.batch_unsqueeze(data,mask,dims,dim_)
torch.jit.batchop.batch_update(batch_data,batch_mask,batch_dims,new_data,new_mask,new_dims)
torch.jit.batchop.batch_view(data,mask,dims,sizes)
torch.jit.batchop.batch_view_as(data,mask,dims,data1,mask1,dims1)
torch.jit.batchop.batch_where(data,mask,dims,data1,mask1,dims1,data2,mask2,dims2)
torch.jit.batchop.batch_where_scalar(cond,data1,mask1,dims1,data2,mask2,dims2)
torch.jit.batchop.batch_zeros_like(data,mask,dims)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/jit/annotations.py----------------------------------------
A:torch.jit.annotations.sig->inspect.signature(fn)
A:torch.jit.annotations.source->dedent(inspect.getsource(fn))
A:torch.jit.annotations.type_line->line.strip()
A:torch.jit.annotations.py_ast->ast.parse(source)
A:torch.jit.annotations.num_params->len(py_def.args.args)
A:torch.jit.annotations.(arg_ann_str, ret_ann_str)->split_type_line(type_line)
A:torch.jit.annotations.arg_ann->eval(arg_ann_str, _eval_env)
A:torch.jit.annotations.ret_ann->eval(ret_ann_str, _eval_env)
A:torch.jit.annotations.lines->dedent(inspect.getsource(fn)).split('\n')
A:torch.jit.annotations.start_offset->len('# type:')
A:torch.jit.annotations.arrow_pos->line.strip().index('->')
A:torch.jit.annotations.return_type->ann_to_type(as_ann(sig.return_annotation))
torch.jit.annotations.Module(self,name,members)
torch.jit.annotations.Module.__getattr__(self,name)
torch.jit.annotations.Module.__init__(self,name,members)
torch.jit.annotations.ann_to_type(ann)
torch.jit.annotations.get_num_params(fn)
torch.jit.annotations.get_signature(fn)
torch.jit.annotations.get_type_line(source)
torch.jit.annotations.parse_type_line(type_line)
torch.jit.annotations.split_type_line(type_line)
torch.jit.annotations.try_real_annotations(fn)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/functional.py----------------------------------------
A:torch.nn.functional.conv1d->_add_docstr(torch.conv1d, '\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} \\times \\frac{\\text{in\\_channels}}{\\text{groups}} \\times kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or\n      a one-element tuple `(sW,)`. Default: 1\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a one-element tuple `(padW,)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a one-element tuple `(dW,)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3)\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> F.conv1d(inputs, filters)\n')
A:torch.nn.functional.conv2d->_add_docstr(torch.conv2d, '\nconv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iH \\times iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} \\times \\frac{\\text{in\\_channels}}{\\text{groups}} \\times kH \\times kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8,4,3,3)\n    >>> inputs = torch.randn(1,4,5,5)\n    >>> F.conv2d(inputs, filters, padding=1)\n')
A:torch.nn.functional.conv3d->_add_docstr(torch.conv3d, '\nconv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 3D convolution over an input image composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv3d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iT \\times iH \\times iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} \\times \\frac{\\text{in\\_channels}}{\\text{groups}} \\times kT \\times kH \\times kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: 1\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3, 3, 3)\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> F.conv3d(inputs, filters)\n')
A:torch.nn.functional.conv_transpose1d->_add_docstr(torch.conv_transpose1d, '\nconv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called "deconvolution".\n\nSee :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} \\times \\frac{\\text{out\\_channels}}{\\text{groups}} \\times kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sW,)``. Default: 1\n    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padW,)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dW,)``. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> weights = torch.randn(16, 33, 5)\n    >>> F.conv_transpose1d(inputs, weights)\n')
A:torch.nn.functional.conv_transpose2d->_add_docstr(torch.conv_transpose2d, '\nconv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution".\n\nSee :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iH \\times iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} \\times \\frac{\\text{out\\_channels}}{\\text{groups}} \\times kH \\times kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sH, sW)``. Default: 1\n    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n      Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dH, dW)``. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> weights = torch.randn(4, 8, 3, 3)\n    >>> F.conv_transpose2d(inputs, weights, padding=1)\n')
A:torch.nn.functional.conv_transpose3d->_add_docstr(torch.conv_transpose3d, '\nconv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution"\n\nSee :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iT \\times iH \\times iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} \\times \\frac{\\text{out\\_channels}}{\\text{groups}} \\times kT \\times kH \\times kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sT, sH, sW)``. Default: 1\n    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padT, padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple\n      ``(out_padT, out_padH, out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> weights = torch.randn(16, 33, 3, 3, 3)\n    >>> F.conv_transpose3d(inputs, weights)\n')
A:torch.nn.functional.conv_tbc->_add_docstr(torch.conv_tbc, '\nApplies a 1-dimensional sequence convolution over an input sequence.\nInput and output dimensions are (Time, Batch, Channels) - hence TBC.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n    bias: bias of shape (:math:`\\text{out\\_channels}`)\n    pad: number of timesteps to pad. Default: 0\n')
A:torch.nn.functional.avg_pool1d->_add_docstr(torch.avg_pool1d, '\navg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple :math:`(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1,2,3,4,5,6,7]]])\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])\n\n')
A:torch.nn.functional.avg_pool2d->_add_docstr(torch._C._nn.avg_pool2d, '\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\n\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iH \\times iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple :math:`(kH \\times kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n')
A:torch.nn.functional.avg_pool3d->_add_docstr(torch._C._nn.avg_pool3d, '\navg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\nsize :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n\nSee :class:`~torch.nn.AvgPool3d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} \\times \\text{in\\_channels} \\times iT \\times iH \\times iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple :math:`(kT \\times kH \\times kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`, Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation\n')
A:torch.nn.functional._output_ratio->_pair(torch.jit._unwrap_optional(output_ratio))
A:torch.nn.functional._output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional._random_samples->torch.jit._unwrap_optional(_random_samples)
A:torch.nn.functional.fractional_max_pool2d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool2d_with_indices, if_false=_fractional_max_pool2d)
A:torch.nn.functional._stride->_triple(torch.jit._unwrap_optional(stride))
A:torch.nn.functional.max_pool1d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool1d_with_indices, if_false=_max_pool1d)
A:torch.nn.functional.max_pool2d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool2d_with_indices, if_false=_max_pool2d)
A:torch.nn.functional.max_pool3d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool3d_with_indices, if_false=_max_pool3d)
A:torch.nn.functional.input_size->input.contiguous().view(n, c, 1, -1).size()
A:torch.nn.functional.default_size->torch.jit.annotate(List[int], [])
A:torch.nn.functional.output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.kernel_size->_triple(kernel_size)
A:torch.nn.functional.padding->_triple(padding)
A:torch.nn.functional.(kw, kh)->modules.utils._pair(kernel_size)
A:torch.nn.functional.stride->torch.jit._unwrap_optional(stride)
A:torch.nn.functional.out->torch._C._nn.nll_loss2d(input, target, weight, reduction_enum, ignore_index)
A:torch.nn.functional.adaptive_max_pool1d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool1d_with_indices, if_false=_adaptive_max_pool1d)
A:torch.nn.functional.adaptive_max_pool2d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool2d_with_indices, if_false=_adaptive_max_pool2d)
A:torch.nn.functional.adaptive_max_pool3d->torch._jit_internal.boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool3d_with_indices, if_false=_adaptive_max_pool3d)
A:torch.nn.functional.adaptive_avg_pool1d->_add_docstr(torch.adaptive_avg_pool1d, '\nadaptive_avg_pool1d(input, output_size) -> Tensor\n\nApplies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\n\nSee :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n\nArgs:\n    output_size: the target output size (single integer)\n')
A:torch.nn.functional.result->torch.rrelu(input, lower, upper, training)
A:torch.nn.functional.threshold_->_add_docstr(_VF.threshold_, '\nthreshold_(input, threshold, value) -> Tensor\n\nIn-place version of :func:`~threshold`.\n')
A:torch.nn.functional.relu_->_add_docstr(torch.relu_, '\nrelu_(input) -> Tensor\n\nIn-place version of :func:`~relu`.\n')
A:torch.nn.functional.hardtanh_->_add_docstr(torch._C._nn.hardtanh_, '\nhardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n\nIn-place version of :func:`~hardtanh`.\n')
A:torch.nn.functional.elu_->_add_docstr(torch._C._nn.elu_, '\nelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~elu`.\n')
A:torch.nn.functional.selu_->_add_docstr(torch.selu_, '\nselu_(input) -> Tensor\n\nIn-place version of :func:`~selu`.\n')
A:torch.nn.functional.celu_->_add_docstr(torch.celu_, '\ncelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~celu`.\n')
A:torch.nn.functional.leaky_relu_->_add_docstr(torch._C._nn.leaky_relu_, '\nleaky_relu_(input, negative_slope=0.01) -> Tensor\n\nIn-place version of :func:`~leaky_relu`.\n')
A:torch.nn.functional.rrelu_->_add_docstr(torch.rrelu_, '\nrrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n\nIn-place version of :func:`~rrelu`.\n')
A:torch.nn.functional.logsigmoid->_add_docstr(torch._C._nn.log_sigmoid, '\nlogsigmoid(input) -> Tensor\n\nApplies element-wise :math:`\\text{LogSigmoid}(x) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n\nSee :class:`~torch.nn.LogSigmoid` for more details.\n')
A:torch.nn.functional.softplus->_add_docstr(torch._C._nn.softplus, '\nsoftplus(input, beta=1, threshold=20) -> Tensor\n')
A:torch.nn.functional.dim->input.contiguous().view(n, c, 1, -1).dim()
A:torch.nn.functional.ret->torch.div(input, denom, out=torch.jit._unwrap_optional(out))
A:torch.nn.functional.dtype->torch.jit._unwrap_optional(dtype)
A:torch.nn.functional.U->torch.jit._unwrap_optional(out).resize_(shape).uniform_()
A:torch.nn.functional.dims->logits.dim()
A:torch.nn.functional.gumbel_noise->_sample_gumbel(logits.size(), eps=eps, out=torch.empty_like(logits))
A:torch.nn.functional.shape->logits.size()
A:torch.nn.functional.y_soft->_gumbel_softmax_sample(logits, tau=tau, eps=eps)
A:torch.nn.functional.(_, k)->_gumbel_softmax_sample(logits, tau=tau, eps=eps).max(-1)
A:torch.nn.functional.y_hard->torch.zeros(shape, dtype=logits.dtype, device=logits.device).scatter_(-1, k.view(-1, 1), 1.0)
A:torch.nn.functional._dtype->torch.jit._unwrap_optional(dtype)
A:torch.nn.functional.softshrink->_add_docstr(torch._C._nn.softshrink, '\nsoftshrink(input, lambd=0.5) -> Tensor\n\nApplies the soft shrinkage function elementwise\n\nSee :class:`~torch.nn.Softshrink` for more details.\n')
A:torch.nn.functional.output->input.contiguous().view(n, c, 1, -1).matmul(weight.t())
A:torch.nn.functional.padding_idx->torch.jit._unwrap_optional(padding_idx)
A:torch.nn.functional.max_norm->torch.jit._unwrap_optional(max_norm)
A:torch.nn.functional.input->input.contiguous().view(n, c, 1, -1).contiguous().view(n, c, 1, -1)
A:torch.nn.functional.offsets->torch.jit._unwrap_optional(offsets)
A:torch.nn.functional.(ret, _, _, _)->torch.embedding_bag(weight, input, offsets, scale_grad_by_freq, mode_enum, sparse)
A:torch.nn.functional.size->input.contiguous().view(n, c, 1, -1).contiguous().view(n, c, 1, -1).size()
A:torch.nn.functional.div->div.mul(alpha).add(k).pow(beta).mul(alpha).add(k).pow(beta)
A:torch.nn.functional.sizes->input.contiguous().view(n, c, 1, -1).contiguous().view(n, c, 1, -1).size()
A:torch.nn.functional.reduction->_Reduction.legacy_get_string(size_average, reduce)
A:torch.nn.functional.n->input.contiguous().view(n, c, 1, -1).contiguous().view(n, c, 1, -1).size(0)
A:torch.nn.functional.c->input.contiguous().view(n, c, 1, -1).contiguous().view(n, c, 1, -1).size(1)
A:torch.nn.functional.target->target.contiguous().view(n, 1, -1).contiguous().view(n, 1, -1)
A:torch.nn.functional.reduction_enum->_Reduction.get_enum(reduction)
A:torch.nn.functional.reduced->torch.kl_div(input, target, reduction_enum)
A:torch.nn.functional.weight->torch.jit._unwrap_optional(weight)
A:torch.nn.functional.new_size->_infer_size(target.size(), weight.size())
A:torch.nn.functional.d->lambd(input, target)
A:torch.nn.functional.(expanded_input, expanded_target)->torch.broadcast_tensors(input, target)
A:torch.nn.functional.t->torch.abs(input - target)
A:torch.nn.functional.pixel_shuffle->_add_docstr(torch.pixel_shuffle, '\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\ntensor of shape :math:`(C, H \\times r, W \\times r)`.\n\nSee :class:`~torch.nn.PixelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    upscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n')
A:torch.nn.functional.scale_factors->_ntuple(dim)(scale_factor)
A:torch.nn.functional.pdist->_add_docstr(torch.pdist, "\npdist(input, p=2) -> Tensor\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape :math:`N \\times M` then the output will have shape\n:math:`\\frac{1}{2} N (N - 1)`.\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n'minkowski', p=p)` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\nequivalent to `scipy.spatial.distance.pdist(input, 'hamming') * M`.\nWhen :math:`p = \\infty`, the closest scipy function is\n`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.\n\nArgs:\n    input: input tensor of shape :math:`N \\times M`.\n    p: p value for the p-norm distance to calculate between each vector pair\n        :math:`\\in [0, \\infty]`.\n")
A:torch.nn.functional.cosine_similarity->_add_docstr(torch.cosine_similarity, '\ncosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between x1 and x2, computed along dim.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input (of size matching x1).\n    dim (int, optional): Dimension of vectors. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nShape:\n    - Input: :math:`(\\ast_1, D, \\ast_2)` where D is at position `dim`.\n    - Output: :math:`(\\ast_1, \\ast_2)` where 1 is at position `dim`.\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)\n')
A:torch.nn.functional.denom->input.contiguous().view(n, c, 1, -1).contiguous().view(n, c, 1, -1).norm(p, dim, True).clamp_(min=eps).expand_as(input)
torch.nn._adaptive_max_pool1d(input,output_size,return_indices=False)
torch.nn._adaptive_max_pool2d(input,output_size,return_indices=False)
torch.nn._adaptive_max_pool3d(input,output_size,return_indices=False)
torch.nn._fractional_max_pool2d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn._get_softmax_dim(name,ndim,stacklevel)
torch.nn._gumbel_softmax_sample(logits,tau=1,eps=1e-10)
torch.nn._max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._max_pool3d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._no_grad_embedding_renorm_(weight,input,max_norm,norm_type)
torch.nn._pointwise_loss(lambd,lambd_optimized,input,target,reduction='mean')
torch.nn._sample_gumbel(shape,eps=1e-10,out=None)
torch.nn._smooth_l1_loss(input,target)
torch.nn._unpool_output_size(input,kernel_size,stride,padding,output_size)
torch.nn.adaptive_avg_pool2d(input,output_size)
torch.nn.adaptive_avg_pool3d(input,output_size)
torch.nn.adaptive_max_pool1d_with_indices(input,output_size,return_indices=False)
torch.nn.adaptive_max_pool2d_with_indices(input,output_size,return_indices=False)
torch.nn.adaptive_max_pool3d_with_indices(input,output_size,return_indices=False)
torch.nn.affine_grid(theta,size)
torch.nn.alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.assert_int_or_pair(arg,arg_name,message)
torch.nn.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)
torch.nn.bilinear(input1,input2,weight,bias=None)
torch.nn.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.celu(input,alpha=1.0,inplace=False)
torch.nn.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean')
torch.nn.dropout(input,p=0.5,training=True,inplace=False)
torch.nn.dropout2d(input,p=0.5,training=True,inplace=False)
torch.nn.dropout3d(input,p=0.5,training=True,inplace=False)
torch.nn.elu(input,alpha=1.0,inplace=False)
torch.nn.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.feature_alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.fractional_max_pool2d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._adaptive_max_pool1d(input,output_size,return_indices=False)
torch.nn.functional._adaptive_max_pool2d(input,output_size,return_indices=False)
torch.nn.functional._adaptive_max_pool3d(input,output_size,return_indices=False)
torch.nn.functional._fractional_max_pool2d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._get_softmax_dim(name,ndim,stacklevel)
torch.nn.functional._gumbel_softmax_sample(logits,tau=1,eps=1e-10)
torch.nn.functional._max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._max_pool3d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._no_grad_embedding_renorm_(weight,input,max_norm,norm_type)
torch.nn.functional._pointwise_loss(lambd,lambd_optimized,input,target,reduction='mean')
torch.nn.functional._sample_gumbel(shape,eps=1e-10,out=None)
torch.nn.functional._smooth_l1_loss(input,target)
torch.nn.functional._unpool_output_size(input,kernel_size,stride,padding,output_size)
torch.nn.functional.adaptive_avg_pool2d(input,output_size)
torch.nn.functional.adaptive_avg_pool3d(input,output_size)
torch.nn.functional.adaptive_max_pool1d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.adaptive_max_pool2d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.adaptive_max_pool3d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.affine_grid(theta,size)
torch.nn.functional.alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.functional.assert_int_or_pair(arg,arg_name,message)
torch.nn.functional.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)
torch.nn.functional.bilinear(input1,input2,weight,bias=None)
torch.nn.functional.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.functional.celu(input,alpha=1.0,inplace=False)
torch.nn.functional.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.functional.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean')
torch.nn.functional.dropout(input,p=0.5,training=True,inplace=False)
torch.nn.functional.dropout2d(input,p=0.5,training=True,inplace=False)
torch.nn.functional.dropout3d(input,p=0.5,training=True,inplace=False)
torch.nn.functional.elu(input,alpha=1.0,inplace=False)
torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.functional.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.functional.feature_alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.functional.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.functional.fractional_max_pool2d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional.glu(input,dim=-1)
torch.nn.functional.grid_sample(input,grid,mode='bilinear',padding_mode='zeros')
torch.nn.functional.group_norm(input,num_groups,weight=None,bias=None,eps=1e-05)
torch.nn.functional.gumbel_softmax(logits,tau=1.0,hard=False,eps=1e-10)
torch.nn.functional.hardshrink(input,lambd=0.5)
torch.nn.functional.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.functional.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)
torch.nn.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)
torch.nn.functional.leaky_relu(input,negative_slope=0.01,inplace=False)
torch.nn.functional.linear(input,weight,bias=None)
torch.nn.functional.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.functional.log_softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.functional.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.functional.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.max_pool1d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_pool2d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_pool3d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multilabel_soft_margin_loss(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.functional.normalize(input,p=2,dim=1,eps=1e-12,out=None)
torch.nn.functional.pad(input,pad,mode='constant',value=0)
torch.nn.functional.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)
torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.functional.prelu(input,weight)
torch.nn.functional.relu(input,inplace=False)
torch.nn.functional.relu6(input,inplace=False)
torch.nn.functional.rrelu(input,lower=1.0/8,upper=1.0/3,training=False,inplace=False)
torch.nn.functional.selu(input,inplace=False)
torch.nn.functional.sigmoid(input)
torch.nn.functional.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.softmin(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.softsign(input)
torch.nn.functional.tanh(input)
torch.nn.functional.tanhshrink(input)
torch.nn.functional.threshold(input,threshold,value,inplace=False)
torch.nn.functional.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.unfold(input,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.functional.upsample_nearest(input,size=None,scale_factor=None)
torch.nn.glu(input,dim=-1)
torch.nn.grid_sample(input,grid,mode='bilinear',padding_mode='zeros')
torch.nn.group_norm(input,num_groups,weight=None,bias=None,eps=1e-05)
torch.nn.gumbel_softmax(logits,tau=1.0,hard=False,eps=1e-10)
torch.nn.hardshrink(input,lambd=0.5)
torch.nn.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)
torch.nn.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.kl_div(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)
torch.nn.leaky_relu(input,negative_slope=0.01,inplace=False)
torch.nn.linear(input,weight,bias=None)
torch.nn.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.log_softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.max_pool1d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_pool2d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_pool3d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.multilabel_soft_margin_loss(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.normalize(input,p=2,dim=1,eps=1e-12,out=None)
torch.nn.pad(input,pad,mode='constant',value=0)
torch.nn.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)
torch.nn.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.prelu(input,weight)
torch.nn.relu(input,inplace=False)
torch.nn.relu6(input,inplace=False)
torch.nn.rrelu(input,lower=1.0/8,upper=1.0/3,training=False,inplace=False)
torch.nn.selu(input,inplace=False)
torch.nn.sigmoid(input)
torch.nn.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.softmin(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.softsign(input)
torch.nn.tanh(input)
torch.nn.tanhshrink(input)
torch.nn.threshold(input,threshold,value,inplace=False)
torch.nn.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.unfold(input,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_reduction.py----------------------------------------
A:torch.nn._reduction.size_average->torch.jit._unwrap_optional(size_average)
A:torch.nn._reduction.reduce->torch.jit._unwrap_optional(reduce)
torch.nn._reduction.get_enum(reduction)
torch.nn._reduction.legacy_get_enum(size_average,reduce,emit_warning=True)
torch.nn._reduction.legacy_get_string(size_average,reduce,emit_warning=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/init.py----------------------------------------
A:torch.nn.init.dimensions->tensor.ndimension()
A:torch.nn.init.sizes->tensor.size()
A:torch.nn.init.min_dim->min(sizes[0], sizes[1])
A:torch.nn.init.fan_in->tensor.size(1)
A:torch.nn.init.fan_out->tensor.size(0)
A:torch.nn.init.num_input_fmaps->tensor.size(1)
A:torch.nn.init.num_output_fmaps->tensor.size(0)
A:torch.nn.init.receptive_field_size->tensor[0][0].numel()
A:torch.nn.init.(fan_in, fan_out)->_calculate_fan_in_and_fan_out(tensor)
A:torch.nn.init.mode->mode.lower().lower()
A:torch.nn.init.fan->_calculate_correct_fan(tensor, mode)
A:torch.nn.init.gain->calculate_gain(nonlinearity, a)
A:torch.nn.init.rows->tensor.size(0)
A:torch.nn.init.cols->tensor[0].numel()
A:torch.nn.init.flattened->tensor.new(rows, cols).normal_(0, 1)
A:torch.nn.init.(q, r)->torch.qr(flattened)
A:torch.nn.init.d->torch.diag(r, 0)
A:torch.nn.init.ph->torch.diag(r, 0).sign()
A:torch.nn.init.num_zeros->int(math.ceil(sparsity * rows))
A:torch.nn.init.row_indices->torch.randperm(rows)
A:torch.nn.init.deprecated_init.__doc__->'\n    {old_name}(...)\n\n    .. warning::\n        This method is now deprecated in favor of :func:`torch.nn.init.{new_name}`.\n\n    See :func:`~torch.nn.init.{new_name}` for details.'.format(old_name=old_name, new_name=new_name)
A:torch.nn.init.uniform->_make_deprecate(uniform_)
A:torch.nn.init.normal->_make_deprecate(normal_)
A:torch.nn.init.constant->_make_deprecate(constant_)
A:torch.nn.init.eye->_make_deprecate(eye_)
A:torch.nn.init.dirac->_make_deprecate(dirac_)
A:torch.nn.init.xavier_uniform->_make_deprecate(xavier_uniform_)
A:torch.nn.init.xavier_normal->_make_deprecate(xavier_normal_)
A:torch.nn.init.kaiming_uniform->_make_deprecate(kaiming_uniform_)
A:torch.nn.init.kaiming_normal->_make_deprecate(kaiming_normal_)
A:torch.nn.init.orthogonal->_make_deprecate(orthogonal_)
A:torch.nn.init.sparse->_make_deprecate(sparse_)
torch.nn.init._calculate_correct_fan(tensor,mode)
torch.nn.init._calculate_fan_in_and_fan_out(tensor)
torch.nn.init._make_deprecate(meth)
torch.nn.init.calculate_gain(nonlinearity,param=None)
torch.nn.init.constant_(tensor,val)
torch.nn.init.dirac_(tensor)
torch.nn.init.eye_(tensor)
torch.nn.init.kaiming_normal_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.kaiming_uniform_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.normal_(tensor,mean=0,std=1)
torch.nn.init.ones_(tensor)
torch.nn.init.orthogonal_(tensor,gain=1)
torch.nn.init.sparse_(tensor,sparsity,std=0.01)
torch.nn.init.uniform_(tensor,a=0,b=1)
torch.nn.init.xavier_normal_(tensor,gain=1)
torch.nn.init.xavier_uniform_(tensor,gain=1)
torch.nn.init.zeros_(tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_VF.py----------------------------------------
A:torch.nn._VF.sys.modules[__name__]->VFModule(__name__)
torch.nn._VF.VFModule(self,name)
torch.nn._VF.VFModule.__getattr__(self,attr)
torch.nn._VF.VFModule.__init__(self,name)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/grad.py----------------------------------------
A:torch.nn.grad.input_size->list(input_size)
A:torch.nn.grad.stride->_triple(stride)
A:torch.nn.grad.padding->_triple(padding)
A:torch.nn.grad.dilation->_triple(dilation)
A:torch.nn.grad.grad_input_padding->_grad_input_padding(grad_output, input_size, stride, padding, kernel_size)
A:torch.nn.grad.grad_output->grad_output.contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4]).contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4])
A:torch.nn.grad.input->input.contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4]).contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4])
A:torch.nn.grad.grad_weight->grad_weight.contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4]).contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4])
torch.nn.grad._grad_input_padding(grad_output,input_size,stride,padding,kernel_size)
torch.nn.grad.conv1d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv1d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parameter.py----------------------------------------
A:torch.nn.parameter.data->torch.Tensor()
A:torch.nn.parameter.result->type(self)(self.data.clone(), self.requires_grad)
torch.nn.Parameter(cls,data=None,requires_grad=True)
torch.nn.Parameter.__deepcopy__(self,memo)
torch.nn.Parameter.__reduce_ex__(self,proto)
torch.nn.Parameter.__repr__(self)
torch.nn.parameter.Parameter(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__deepcopy__(self,memo)
torch.nn.parameter.Parameter.__new__(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__reduce_ex__(self,proto)
torch.nn.parameter.Parameter.__repr__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/backends/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/backends/backend.py----------------------------------------
A:torch.nn.backends.backend.fn->self.function_classes.get(name)
torch.nn.backends.backend.FunctionBackend(self)
torch.nn.backends.backend.FunctionBackend.__getattr__(self,name)
torch.nn.backends.backend.FunctionBackend.__init__(self)
torch.nn.backends.backend.FunctionBackend.register_function(self,name,function_class)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/backends/thnn.py----------------------------------------
A:torch.nn.backends.thnn.backend->THNNFunctionBackend()
torch.nn.backends.thnn.THNNFunctionBackend(FunctionBackend)
torch.nn.backends.thnn.THNNFunctionBackend.__copy__(self)
torch.nn.backends.thnn.THNNFunctionBackend.__deepcopy__(self,memo)
torch.nn.backends.thnn.THNNFunctionBackend.__reduce__(self)
torch.nn.backends.thnn._get_thnn_function_backend()
torch.nn.backends.thnn._initialize_backend()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/_functions.py----------------------------------------
A:torch.nn.parallel._functions.target_gpus->list(map(lambda x: _get_device_index(x, True), target_gpus))
A:torch.nn.parallel._functions.ctx.num_inputs->len(inputs)
A:torch.nn.parallel._functions.ctx.input_device->inputs[0].get_device()
A:torch.nn.parallel._functions.outputs->torch.cuda.comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
A:torch.nn.parallel._functions.target_device->_get_device_index(target_device, True)
A:torch.nn.parallel._functions.ctx.input_gpus->tuple(map(lambda i: i.get_device(), inputs))
A:torch.nn.parallel._functions.inputs->tuple((t.view(1) for t in inputs))
A:torch.nn.parallel._functions.ctx.input_sizes->tuple(map(lambda i: i.size(ctx.dim), inputs))
A:torch.nn.parallel._functions.scattered_grads->tuple((g[0] for g in scattered_grads))
A:torch.nn.parallel._functions.main_stream->torch.cuda.current_stream()
A:torch.nn.parallel._functions._streams[device]->torch.cuda.Stream(device)
torch.nn.parallel._functions.Broadcast(Function)
torch.nn.parallel._functions.Broadcast.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.Broadcast.forward(ctx,target_gpus,*inputs)
torch.nn.parallel._functions.Gather(Function)
torch.nn.parallel._functions.Gather.backward(ctx,grad_output)
torch.nn.parallel._functions.Gather.forward(ctx,target_device,dim,*inputs)
torch.nn.parallel._functions.ReduceAddCoalesced(Function)
torch.nn.parallel._functions.ReduceAddCoalesced.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.ReduceAddCoalesced.forward(ctx,destination,num_inputs,*grads)
torch.nn.parallel._functions.Scatter(Function)
torch.nn.parallel._functions.Scatter.backward(ctx,*grad_output)
torch.nn.parallel._functions.Scatter.forward(ctx,target_gpus,chunk_sizes,dim,input)
torch.nn.parallel._functions._get_stream(device)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py----------------------------------------
A:torch.nn.parallel.parallel_apply.devices->list(map(lambda x: _get_device_index(x, True), devices))
A:torch.nn.parallel.parallel_apply.lock->threading.Lock()
A:torch.nn.parallel.parallel_apply.grad_enabled->torch.is_grad_enabled()
A:torch.nn.parallel.parallel_apply.device->get_a_var(input).get_device()
A:torch.nn.parallel.parallel_apply.output->module(*input, **kwargs)
torch.nn.parallel.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)
torch.nn.parallel.parallel_apply.get_a_var(obj)
torch.nn.parallel.parallel_apply.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/distributed.py----------------------------------------
A:torch.nn.parallel.distributed.device_ids->list(range(torch.cuda.device_count()))
A:torch.nn.parallel.distributed.self.process_group->_get_default_group()
A:torch.nn.parallel.distributed.self.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.distributed.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.distributed.module_states->list(self.module.state_dict().values())
A:torch.nn.parallel.distributed.self._module_copies->replicate(self.module, self.device_ids, detach=True)
A:torch.nn.parallel.distributed.self.ready_buckets_not_reduced->set()
A:torch.nn.parallel.distributed.attrs->copy.copy(self.__dict__)
A:torch.nn.parallel.distributed.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.distributed.outputs->self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
A:torch.nn.parallel.distributed.result->torch.distributed._queue_reduction(self.process_group, self.buckets[bucket_idx], self.device_ids)
A:torch.nn.parallel.distributed.p_tmp->p.expand_as(p)
A:torch.nn.parallel.distributed.sorted_todo->sorted(self.ready_buckets_not_reduced, reverse=True)
torch.nn.parallel.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,check_reduction=False)
torch.nn.parallel.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.DistributedDataParallel._check_previous_reduction(self)
torch.nn.parallel.DistributedDataParallel._ddp_init_helper(self)
torch.nn.parallel.DistributedDataParallel._dist_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.DistributedDataParallel._make_param_hook(self,param,device_idx)
torch.nn.parallel.DistributedDataParallel._queue_reduction(self,bucket_idx)
torch.nn.parallel.DistributedDataParallel._register_grad_hooks(self)
torch.nn.parallel.DistributedDataParallel._sync_params(self)
torch.nn.parallel.DistributedDataParallel._sync_reduction_works(self)
torch.nn.parallel.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.distributed.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,check_reduction=False)
torch.nn.parallel.distributed.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.distributed.DistributedDataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,check_reduction=False)
torch.nn.parallel.distributed.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.distributed.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.distributed.DistributedDataParallel._check_previous_reduction(self)
torch.nn.parallel.distributed.DistributedDataParallel._ddp_init_helper(self)
torch.nn.parallel.distributed.DistributedDataParallel._dist_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.distributed.DistributedDataParallel._make_param_hook(self,param,device_idx)
torch.nn.parallel.distributed.DistributedDataParallel._queue_reduction(self,bucket_idx)
torch.nn.parallel.distributed.DistributedDataParallel._register_grad_hooks(self)
torch.nn.parallel.distributed.DistributedDataParallel._sync_params(self)
torch.nn.parallel.distributed.DistributedDataParallel._sync_reduction_works(self)
torch.nn.parallel.distributed.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.distributed.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.distributed.DistributedDataParallel.train(self,mode=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/distributed_cpu.py----------------------------------------
A:torch.nn.parallel.distributed_cpu.buckets->defaultdict(list)
A:torch.nn.parallel.distributed_cpu.tp->type(param.data)
A:torch.nn.parallel.distributed_cpu.coalesced->_flatten_dense_tensors(grads)
torch.nn.parallel.DistributedDataParallelCPU(self,module)
torch.nn.parallel.DistributedDataParallelCPU.forward(self,*inputs,**kwargs)
torch.nn.parallel.DistributedDataParallelCPU.sync_parameters(self)
torch.nn.parallel.distributed_cpu.DistributedDataParallelCPU(self,module)
torch.nn.parallel.distributed_cpu.DistributedDataParallelCPU.__init__(self,module)
torch.nn.parallel.distributed_cpu.DistributedDataParallelCPU.forward(self,*inputs,**kwargs)
torch.nn.parallel.distributed_cpu.DistributedDataParallelCPU.sync_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/replicate.py----------------------------------------
A:torch.nn.parallel.replicate.devices->list(map(lambda x: _get_device_index(x, True), devices))
A:torch.nn.parallel.replicate.num_replicas->len(devices)
A:torch.nn.parallel.replicate.params->list(network.parameters())
A:torch.nn.parallel.replicate.param_copies->_functions.Broadcast.apply(devices, *params)
A:torch.nn.parallel.replicate.buffers->list(network.buffers())
A:torch.nn.parallel.replicate.buffer_copies->torch.cuda.comm.broadcast_coalesced(buffers, devices)
A:torch.nn.parallel.replicate.modules->list(network.modules())
A:torch.nn.parallel.replicate.replica->module.__new__(type(module))
A:torch.nn.parallel.replicate.replica.__dict__->module.__dict__.copy()
A:torch.nn.parallel.replicate.replica._parameters->module.__new__(type(module))._parameters.copy()
A:torch.nn.parallel.replicate.replica._buffers->module.__new__(type(module))._buffers.copy()
A:torch.nn.parallel.replicate.replica._modules->module.__new__(type(module))._modules.copy()
torch.nn.parallel.replicate(network,devices,detach=False)
torch.nn.parallel.replicate.replicate(network,devices,detach=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py----------------------------------------
A:torch.nn.parallel.data_parallel.device_ids->list(range(torch.cuda.device_count()))
A:torch.nn.parallel.data_parallel.(min_pos, min_val)->min(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.(max_pos, max_val)->max(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.self.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.data_parallel.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.data_parallel.replicas->replicate(module, used_device_ids)
A:torch.nn.parallel.data_parallel.outputs->parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
A:torch.nn.parallel.data_parallel.(inputs, module_kwargs)->scatter_kwargs(inputs, module_kwargs, device_ids, dim)
torch.nn.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.DataParallel.gather(self,outputs,output_device)
torch.nn.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.DataParallel.replicate(self,module,device_ids)
torch.nn.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)
torch.nn.parallel.data_parallel.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.data_parallel.DataParallel.gather(self,outputs,output_device)
torch.nn.parallel.data_parallel.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.data_parallel.DataParallel.replicate(self,module,device_ids)
torch.nn.parallel.data_parallel.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel._check_balance(device_ids)
torch.nn.parallel.data_parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py----------------------------------------
A:torch.nn.parallel.scatter_gather.inputs->tuple(inputs)
A:torch.nn.parallel.scatter_gather.kwargs->tuple(kwargs)
torch.nn.parallel.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter_gather.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)
torch.nn.parallel.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/deprecated/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/deprecated/distributed.py----------------------------------------
A:torch.nn.parallel.deprecated.distributed.device_ids->list(range(torch.cuda.device_count()))
A:torch.nn.parallel.deprecated.distributed.module_states->list(self.module.state_dict().values())
A:torch.nn.parallel.deprecated.distributed.self._module_copies->replicate(self.module, self.device_ids, detach=True)
A:torch.nn.parallel.deprecated.distributed.bucket_param_type->param_tuple[0].type()
A:torch.nn.parallel.deprecated.distributed.self.dispatch_lock->threading.Lock()
A:torch.nn.parallel.deprecated.distributed.attrs->copy.copy(self.__dict__)
A:torch.nn.parallel.deprecated.distributed.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.deprecated.distributed.outputs->self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
A:torch.nn.parallel.deprecated.distributed.flat_tensors->_flatten_dense_tensors(tensors)
A:torch.nn.parallel.deprecated.distributed.result->broadcast_coalesced(buffers, self.device_ids, self.broadcast_bucket_size)
A:torch.nn.parallel.deprecated.distributed.p_tmp->p.expand_as(p)
A:torch.nn.parallel.deprecated.distributed.self.nccl_reduction_group_id->torch.distributed.deprecated.new_group()
A:torch.nn.parallel.deprecated.distributed.dev_grads_buckets->_take_tensors(all_grads[dev_idx], self.nccl_reduce_bucket_size)
A:torch.nn.parallel.deprecated.distributed.dev_grads_batch_coalesced->_flatten_dense_tensors(dev_grads_batch)
A:torch.nn.parallel.deprecated.distributed.grads_batch_reduced->_unflatten_dense_tensors(grads_batch_coalesced[0], grads_batch[0])
A:torch.nn.parallel.deprecated.distributed.event->threading.Event()
A:torch.nn.parallel.deprecated.distributed.r_streams->zip(*self._reduction_streams)
A:torch.nn.parallel.deprecated.distributed.num_buckets->len(self.bucket_sizes)
A:torch.nn.parallel.deprecated.distributed.group_id->torch.distributed.deprecated.new_group()
A:torch.nn.parallel.deprecated.distributed.(dev_grad_batch, dev_events, job_event)->Queue.get()
A:torch.nn.parallel.deprecated.distributed.coalesced->_flatten_dense_tensors(grad_batch)
torch.nn.parallel.deprecated.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True)
torch.nn.parallel.deprecated.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.deprecated.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.deprecated.DistributedDataParallel._dist_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.deprecated.DistributedDataParallel._make_param_hook(self,param,device_idx)
torch.nn.parallel.deprecated.DistributedDataParallel._queue_reduction(self,bucket_idx)
torch.nn.parallel.deprecated.DistributedDataParallel._reduction_thread_fn(queue,group_id,device_ids,reduction_streams,nccl_streams)
torch.nn.parallel.deprecated.DistributedDataParallel._register_grad_hooks(self)
torch.nn.parallel.deprecated.DistributedDataParallel._register_nccl_grad_hook(self)
torch.nn.parallel.deprecated.DistributedDataParallel._start_reduction_threads(self)
torch.nn.parallel.deprecated.DistributedDataParallel._sync_params(self)
torch.nn.parallel.deprecated.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.deprecated.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.deprecated.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.deprecated.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.deprecated.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._dist_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._make_param_hook(self,param,device_idx)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._queue_reduction(self,bucket_idx)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._reduction_thread_fn(queue,group_id,device_ids,reduction_streams,nccl_streams)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._register_grad_hooks(self)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._register_nccl_grad_hook(self)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._start_reduction_threads(self)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel._sync_params(self)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.deprecated.distributed.DistributedDataParallel.train(self,mode=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/parallel/deprecated/distributed_cpu.py----------------------------------------
A:torch.nn.parallel.deprecated.distributed_cpu.buckets->defaultdict(list)
A:torch.nn.parallel.deprecated.distributed_cpu.tp->type(param.data)
A:torch.nn.parallel.deprecated.distributed_cpu.coalesced->_flatten_dense_tensors(grads)
torch.nn.parallel.deprecated.DistributedDataParallelCPU(self,module)
torch.nn.parallel.deprecated.DistributedDataParallelCPU.forward(self,*inputs,**kwargs)
torch.nn.parallel.deprecated.DistributedDataParallelCPU.sync_parameters(self)
torch.nn.parallel.deprecated.distributed_cpu.DistributedDataParallelCPU(self,module)
torch.nn.parallel.deprecated.distributed_cpu.DistributedDataParallelCPU.__init__(self,module)
torch.nn.parallel.deprecated.distributed_cpu.DistributedDataParallelCPU.forward(self,*inputs,**kwargs)
torch.nn.parallel.deprecated.distributed_cpu.DistributedDataParallelCPU.sync_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/vision.py----------------------------------------
A:torch.nn._functions.vision.ret->torch.affine_grid_generator(theta, size)
torch.nn._functions.vision.affine_grid_generator(theta,size)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py----------------------------------------
A:torch.nn._functions.thnn.sparse.ctx._weight_size->weight.size()
A:torch.nn._functions.thnn.sparse.ctx._offset2bag->ctx._offset2bag.cumsum(0)
A:torch.nn._functions.thnn.sparse.indices->indices.contiguous().view(-1).contiguous().view(-1)
A:torch.nn._functions.thnn.sparse.output->weight.new()
A:torch.nn._functions.thnn.sparse.ctx.bag_size->ctx.bag_size[:, None].expand_as(output)
A:torch.nn._functions.thnn.sparse.index_output->torch.index_select(weight, 0, indices)
A:torch.nn._functions.thnn.sparse.grad_output->grad_output.contiguous().contiguous()
A:torch.nn._functions.thnn.sparse._sorted->torch.cuda.LongTensor()
A:torch.nn._functions.thnn.sparse._indices->torch.cuda.LongTensor()
A:torch.nn._functions.thnn.sparse._count->torch.IntTensor()
A:torch.nn._functions.thnn.sparse.grad_weight->grad_output.contiguous().contiguous().new(ctx._weight_size).zero_()
A:torch.nn._functions.thnn.sparse.index_grad_output->grad_output.contiguous().contiguous().index_select(0, ctx._offset2bag)
torch.nn._functions.thnn.EmbeddingBag(Function)
torch.nn._functions.thnn.EmbeddingBag._renorm(ctx,indices,weight,max_norm,norm_type)
torch.nn._functions.thnn.EmbeddingBag.backward(ctx,grad_output)
torch.nn._functions.thnn.EmbeddingBag.forward(cls,ctx,weight,indices,offsets,max_norm,norm_type,scale_grad_by_freq,mode)
torch.nn._functions.thnn.sparse.EmbeddingBag(Function)
torch.nn._functions.thnn.sparse.EmbeddingBag._renorm(ctx,indices,weight,max_norm,norm_type)
torch.nn._functions.thnn.sparse.EmbeddingBag.backward(ctx,grad_output)
torch.nn._functions.thnn.sparse.EmbeddingBag.forward(cls,ctx,weight,indices,offsets,max_norm,norm_type,scale_grad_by_freq,mode)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto_double_backwards.py----------------------------------------
A:torch.nn._functions.thnn.auto_double_backwards.negative_mask->(input < target).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.non_negative_mask->(input >= 0).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.first_half->input.narrow(dim, 0, input_size)
A:torch.nn._functions.thnn.auto_double_backwards.second_half->input.narrow(dim, input_size, input_size)
A:torch.nn._functions.thnn.auto_double_backwards.sig_second_half->input.narrow(dim, input_size, input_size).sigmoid()
A:torch.nn._functions.thnn.auto_double_backwards.ggI_first_half->ggI.narrow(dim, 0, input_size)
A:torch.nn._functions.thnn.auto_double_backwards.ggI_second_half->ggI.narrow(dim, input_size, input_size)
A:torch.nn._functions.thnn.auto_double_backwards.gI->torch.zeros_like(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.mask->torch.zeros_like(ggI).scatter_(1, safe_target.unsqueeze(1), weights_to_scatter.unsqueeze(1))
A:torch.nn._functions.thnn.auto_double_backwards.input_lt_0->(input < 0).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.input_ge_0->(input >= 0).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.exp_input->input.exp()
A:torch.nn._functions.thnn.auto_double_backwards.above_threshold->torch.zeros_like(ggI).masked_fill_(input_beta > threshold, 1)
A:torch.nn._functions.thnn.auto_double_backwards.below_threshold->torch.zeros_like(ggI).masked_fill_(input_beta <= threshold, 1)
A:torch.nn._functions.thnn.auto_double_backwards.exp_output_beta->(output * beta).exp()
A:torch.nn._functions.thnn.auto_double_backwards.input_gt_threshold->(input > threshold).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.positive_mask->(input > target).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.ggO->(ggI * mask).sum(dim=1)
A:torch.nn._functions.thnn.auto_double_backwards.safe_target->target.clone()
A:torch.nn._functions.thnn.auto_double_backwards.weights_to_scatter->weights_maybe_resized.expand(weights.size()[0:1] + target.size()[1:]).gather(0, safe_target)
A:torch.nn._functions.thnn.auto_double_backwards.weights_maybe_resized->weights_maybe_resized.expand(weights.size()[0:1] + target.size()[1:]).expand(weights.size()[0:1] + target.size()[1:])
A:torch.nn._functions.thnn.auto_double_backwards.zeros->torch.zeros_like(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.large_error_pos_mask->((input_sub_target > 0) + large_error_mask == 2).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.large_error_neg_mask->((input_sub_target <= 0) + large_error_mask == 2).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.small_error_mask->small_error_mask.type_as(ggI).type_as(ggI)
A:torch.nn._functions.thnn.auto_double_backwards.t0->(1 + (-target * input).exp()).pow(-1)
torch.nn._functions.thnn.auto_double_backwards.elu_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.gatedlinear_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.hardshrink_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.hardtanh_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.klddivloss_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.l1loss_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.leakyrelu_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.logsigmoid_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.mseloss_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.nllloss_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.smoothl1loss_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.softmarginloss_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.softplus_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.softshrink_double_backwards(ctx,ggI)
torch.nn._functions.thnn.auto_double_backwards.threshold_double_backwards(ctx,ggI)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py----------------------------------------
A:torch.nn._functions.thnn.auto.a->symbolic_fn(*args, **kwargs)
A:torch.nn._functions.thnn.auto.ctx.additional_args->list(args)
A:torch.nn._functions.thnn.auto.ctx.forward_args_count->len(ctx.additional_args)
A:torch.nn._functions.thnn.auto.output->input.new()
A:torch.nn._functions.thnn.auto.grad_input->input.new(input.size())
A:torch.nn._functions.thnn.auto.grad_output_expanded->grad_output.view(*repeat(1, grad_input.dim()))
A:torch.nn._functions.thnn.auto.backward_cls->type(class_name + 'Backward', (base_class,), dict(forward=backward_cls_forward, backward=backward_cls_backward))
A:torch.nn._functions.thnn.auto.save_output->has_argument(update_grad_input, 'output')
A:torch.nn._functions.thnn.auto.buffers['update_output']->_find_buffers(update_output.arguments[3:], ignored_args)
A:torch.nn._functions.thnn.auto.buffers['update_grad_input']->_find_buffers(update_grad_input.arguments[4:], ignored_args)
A:torch.nn._functions.thnn.auto.buffers['acc_grad_parameters']->_find_buffers(acc_grad_parameters.arguments[3:], ignored_args)
A:torch.nn._functions.thnn.auto.tensor_params->tuple(tensor_param_list)
A:torch.nn._functions.thnn.auto.ctx.buffers->defaultdict(type(input))
A:torch.nn._functions.thnn.auto.additional_args->_initialize_buffers(ctx, 'acc_grad_parameters')
A:torch.nn._functions.thnn.auto.grad_params->tuple((p.new(p.size()).zero_() for p in params))
A:torch.nn._functions.thnn.auto.tmp_args->list(additional_args)
A:torch.nn._functions.thnn.auto.update_grad_input_fn->getattr(ctx._backend, update_grad_input.name)
A:torch.nn._functions.thnn.auto.acc_grad_parameters_fn->getattr(ctx._backend, acc_grad_parameters.name)
A:torch.nn._functions.thnn.auto.function_list->parse_header(THNN_H_PATH)
A:torch.nn._functions.thnn.auto.acc_grad_parameters->function_by_name.get(fn + '_accGradParameters')
A:torch.nn._functions.thnn.auto.class_name->name_remap.get(fn, fn)
A:torch.nn._functions.thnn.auto.double_backwards_fn->make_default_double_backwards_fn(class_name)
A:torch.nn._functions.thnn.auto.symbolic_fn->auto_symbolic.symbolic_fns.get(class_name)
A:torch.nn._functions.thnn.auto.(cls, backward_cls)->_make_function_class(class_name, update_output, update_grad_input, acc_grad_parameters, double_backwards_fn, symbolic_fn)
torch.nn._functions.thnn._find_buffers(args,ignored_args)
torch.nn._functions.thnn._generate_function_classes(scope_dict)
torch.nn._functions.thnn._make_function_class(class_name,update_output,update_grad_input,acc_grad_parameters,double_backwards_fn,symbolic_fn)
torch.nn._functions.thnn._make_function_class_criterion(class_name,update_output,update_grad_input,acc_grad_parameters,double_backwards_fn,symbolic_fn)
torch.nn._functions.thnn.auto._find_buffers(args,ignored_args)
torch.nn._functions.thnn.auto._generate_function_classes(scope_dict)
torch.nn._functions.thnn.auto._make_function_class(class_name,update_output,update_grad_input,acc_grad_parameters,double_backwards_fn,symbolic_fn)
torch.nn._functions.thnn.auto._make_function_class_criterion(class_name,update_output,update_grad_input,acc_grad_parameters,double_backwards_fn,symbolic_fn)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto_symbolic.py----------------------------------------
A:torch.nn._functions.thnn.auto_symbolic.paddings->prepare_onnx_paddings(len(input.type().sizes()), params)
torch.nn._functions.thnn.auto_symbolic.reflectionpad_symbolic(g,input,*params)
torch.nn._functions.thnn.auto_symbolic.replicationpad_symbolic(g,input,*params)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/normalization.py----------------------------------------
A:torch.nn._functions.thnn.normalization.output->input.new()
A:torch.nn._functions.thnn.normalization.batch_size->input.size(0)
A:torch.nn._functions.thnn.normalization.channels->input.size(1)
A:torch.nn._functions.thnn.normalization.input_height->input.size(2)
A:torch.nn._functions.thnn.normalization.input_width->input.size(3)
A:torch.nn._functions.thnn.normalization.pre_pad->int((self.size - 1) / 2 + 1)
A:torch.nn._functions.thnn.normalization.scale_first->self.scale.select(1, 0)
A:torch.nn._functions.thnn.normalization.scale_previous->self.scale.select(1, c - 1)
A:torch.nn._functions.thnn.normalization.scale_current->self.scale.select(1, c)
A:torch.nn._functions.thnn.normalization.square_next->input_square.select(1, c + pre_pad - 1)
A:torch.nn._functions.thnn.normalization.square_previous->input_square.select(1, c - pre_pad)
A:torch.nn._functions.thnn.normalization.grad_input->grad_output.new()
A:torch.nn._functions.thnn.normalization.paddded_ratio->input.new(channels + self.size - 1, input_height, input_width)
A:torch.nn._functions.thnn.normalization.accum_ratio->input.new(input_height, input_width)
A:torch.nn._functions.thnn.normalization.inversePrePad->int(self.size - (self.size - 1) / 2)
A:torch.nn._functions.thnn.normalization.padded_ratio_center->input.new(channels + self.size - 1, input_height, input_width).narrow(0, inversePrePad, channels)
torch.nn._functions.thnn.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn._functions.thnn.CrossMapLRN2d.backward(self,grad_output)
torch.nn._functions.thnn.CrossMapLRN2d.forward(self,input)
torch.nn._functions.thnn.normalization.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn._functions.thnn.normalization.CrossMapLRN2d.__init__(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn._functions.thnn.normalization.CrossMapLRN2d.backward(self,grad_output)
torch.nn._functions.thnn.normalization.CrossMapLRN2d.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/_functions/thnn/fold.py----------------------------------------
A:torch.nn._functions.thnn.fold.output->input.new()
A:torch.nn._functions.thnn.fold.grad_input->grad_output.new()
torch.nn._functions.thnn.Col2Im(Function)
torch.nn._functions.thnn.Col2Im.backward(ctx,grad_output)
torch.nn._functions.thnn.Col2Im.forward(ctx,input,output_size,kernel_size,dilation,padding,stride)
torch.nn._functions.thnn.Im2Col(Function)
torch.nn._functions.thnn.Im2Col.backward(ctx,grad_output)
torch.nn._functions.thnn.Im2Col.forward(ctx,input,kernel_size,dilation,padding,stride)
torch.nn._functions.thnn.fold.Col2Im(Function)
torch.nn._functions.thnn.fold.Col2Im.backward(ctx,grad_output)
torch.nn._functions.thnn.fold.Col2Im.forward(ctx,input,output_size,kernel_size,dilation,padding,stride)
torch.nn._functions.thnn.fold.Im2Col(Function)
torch.nn._functions.thnn.fold.Im2Col.backward(ctx,grad_output)
torch.nn._functions.thnn.fold.Im2Col.forward(ctx,input,kernel_size,dilation,padding,stride)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/utils/rnn.py----------------------------------------
A:torch.nn.utils.rnn.PackedSequence_->namedtuple('PackedSequence', ['data', 'batch_sizes'])
A:torch.nn.utils.rnn.data->self.data.to(*args, **kwargs)
A:torch.nn.utils.rnn.lengths->torch.as_tensor(lengths, dtype=torch.int64)
A:torch.nn.utils.rnn.max_seq_length->sequence.batch_sizes.size(0)
A:torch.nn.utils.rnn.max_size->sequences[0].size()
A:torch.nn.utils.rnn.max_len->max([s.size(0) for s in sequences])
A:torch.nn.utils.rnn.out_tensor->sequences[0].data.new(*out_dims).fill_(padding_value)
A:torch.nn.utils.rnn.length->tensor.size(0)
torch.nn.utils.rnn.PackedSequence(cls,data,batch_sizes=None)
torch.nn.utils.rnn.PackedSequence.__new__(cls,data,batch_sizes=None)
torch.nn.utils.rnn.PackedSequence.byte(self)
torch.nn.utils.rnn.PackedSequence.char(self)
torch.nn.utils.rnn.PackedSequence.cpu(self)
torch.nn.utils.rnn.PackedSequence.cuda(self,*args,**kwargs)
torch.nn.utils.rnn.PackedSequence.double(self)
torch.nn.utils.rnn.PackedSequence.float(self)
torch.nn.utils.rnn.PackedSequence.half(self)
torch.nn.utils.rnn.PackedSequence.int(self)
torch.nn.utils.rnn.PackedSequence.is_cuda(self)
torch.nn.utils.rnn.PackedSequence.long(self)
torch.nn.utils.rnn.PackedSequence.short(self)
torch.nn.utils.rnn.PackedSequence.to(self,*args,**kwargs)
torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False)
torch.nn.utils.rnn.pack_sequence(sequences)
torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)
torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/utils/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/utils/weight_norm.py----------------------------------------
A:torch.nn.utils.weight_norm.g->getattr(module, self.name + '_g')
A:torch.nn.utils.weight_norm.v->getattr(module, self.name + '_v')
A:torch.nn.utils.weight_norm.fn->WeightNorm(name, dim)
A:torch.nn.utils.weight_norm.weight->self.compute_weight(module)
torch.nn.utils.remove_weight_norm(module,name='weight')
torch.nn.utils.weight_norm(module,name='weight',dim=0)
torch.nn.utils.weight_norm.WeightNorm(self,name,dim)
torch.nn.utils.weight_norm.WeightNorm.__init__(self,name,dim)
torch.nn.utils.weight_norm.WeightNorm.apply(module,name,dim)
torch.nn.utils.weight_norm.WeightNorm.compute_weight(self,module)
torch.nn.utils.weight_norm.WeightNorm.remove(self,module)
torch.nn.utils.weight_norm.remove_weight_norm(module,name='weight')
torch.nn.utils.weight_norm.weight_norm(module,name='weight',dim=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/utils/spectral_norm.py----------------------------------------
A:torch.nn.utils.spectral_norm.weight_mat->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig)
A:torch.nn.utils.spectral_norm.height->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size(0)
A:torch.nn.utils.spectral_norm.weight->state_dict.pop(prefix + fn.name)
A:torch.nn.utils.spectral_norm.u->normalize(weight.new_empty(h).normal_(0, 1), dim=0, eps=fn.eps)
A:torch.nn.utils.spectral_norm.v->SpectralNorm(name, n_power_iterations, dim, eps)._solve_v_and_rescale(weight_mat, u, sigma)
A:torch.nn.utils.spectral_norm.sigma->(weight_orig / weight).mean()
A:torch.nn.utils.spectral_norm.fn->SpectralNorm(name, n_power_iterations, dim, eps)
A:torch.nn.utils.spectral_norm.(h, w)->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size()
A:torch.nn.utils.spectral_norm.version->local_metadata.get('spectral_norm', {}).get(fn.name + '.version', None)
torch.nn.utils.remove_spectral_norm(module,name='weight')
torch.nn.utils.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)
torch.nn.utils.spectral_norm.SpectralNorm(self,name='weight',n_power_iterations=1,dim=0,eps=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm.__init__(self,name='weight',n_power_iterations=1,dim=0,eps=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm._solve_v_and_rescale(self,weight_mat,u,target_sigma)
torch.nn.utils.spectral_norm.SpectralNorm.apply(module,name,n_power_iterations,dim,eps)
torch.nn.utils.spectral_norm.SpectralNorm.compute_weight(self,module,do_power_iteration)
torch.nn.utils.spectral_norm.SpectralNorm.remove(self,module)
torch.nn.utils.spectral_norm.SpectralNorm.reshape_weight_to_matrix(self,weight)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook.__init__(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook.__init__(self,fn)
torch.nn.utils.spectral_norm.remove_spectral_norm(module,name='weight')
torch.nn.utils.spectral_norm.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/utils/convert_parameters.py----------------------------------------
A:torch.nn.utils.convert_parameters.param_device->_check_param_device(param, param_device)
A:torch.nn.utils.convert_parameters.num_param->param.numel()
torch.nn.utils.convert_parameters._check_param_device(param,old_param_device)
torch.nn.utils.convert_parameters.parameters_to_vector(parameters)
torch.nn.utils.convert_parameters.vector_to_parameters(vec,parameters)
torch.nn.utils.parameters_to_vector(parameters)
torch.nn.utils.vector_to_parameters(vec,parameters)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py----------------------------------------
A:torch.nn.utils.clip_grad.parameters->list(filter(lambda p: p.grad is not None, parameters))
A:torch.nn.utils.clip_grad.max_norm->float(max_norm)
A:torch.nn.utils.clip_grad.norm_type->float(norm_type)
A:torch.nn.utils.clip_grad.total_norm->max((p.grad.data.abs().max() for p in parameters))
A:torch.nn.utils.clip_grad.param_norm->p.grad.data.norm(norm_type)
A:torch.nn.utils.clip_grad.clip_value->float(clip_value)
torch.nn.utils.clip_grad.clip_grad_norm(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad.clip_grad_norm_(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad.clip_grad_value_(parameters,clip_value)
torch.nn.utils.clip_grad_norm(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad_value_(parameters,clip_value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py----------------------------------------
A:torch.nn.modules.batchnorm.self.weight->Parameter(torch.Tensor(num_features))
A:torch.nn.modules.batchnorm.self.bias->Parameter(torch.Tensor(num_features))
A:torch.nn.modules.batchnorm.version->local_metadata.get('version', None)
A:torch.nn.modules.batchnorm.state_dict[num_batches_tracked_key]->torch.tensor(0, dtype=torch.long)
torch.nn.BatchNorm1d(_BatchNorm)
torch.nn.BatchNorm1d._check_input_dim(self,input)
torch.nn.BatchNorm2d(_BatchNorm)
torch.nn.BatchNorm2d._check_input_dim(self,input)
torch.nn.BatchNorm3d(_BatchNorm)
torch.nn.BatchNorm3d._check_input_dim(self,input)
torch.nn.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.batchnorm._BatchNorm._check_input_dim(self,input)
torch.nn.batchnorm._BatchNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.batchnorm._BatchNorm.extra_repr(self)
torch.nn.batchnorm._BatchNorm.forward(self,input)
torch.nn.batchnorm._BatchNorm.reset_parameters(self)
torch.nn.batchnorm._BatchNorm.reset_running_stats(self)
torch.nn.modules.batchnorm.BatchNorm1d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm1d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm2d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm2d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm3d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm3d._check_input_dim(self,input)
torch.nn.modules.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._BatchNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._BatchNorm._check_input_dim(self,input)
torch.nn.modules.batchnorm._BatchNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.batchnorm._BatchNorm.extra_repr(self)
torch.nn.modules.batchnorm._BatchNorm.forward(self,input)
torch.nn.modules.batchnorm._BatchNorm.reset_parameters(self)
torch.nn.modules.batchnorm._BatchNorm.reset_running_stats(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/container.py----------------------------------------
A:torch.nn.modules.container.size->len(self)
A:torch.nn.modules.container.idx->self._get_abs_string_index(idx)
A:torch.nn.modules.container.key->self._get_item_by_idx(self._modules.keys(), idx)
A:torch.nn.modules.container.keys->super(ParameterList, self).__dir__()
A:torch.nn.modules.container.input->module(input)
A:torch.nn.modules.container.self._modules->OrderedDict(list(zip(str_indices, self._modules.values())))
A:torch.nn.modules.container.offset->len(self)
A:torch.nn.modules.container.size_str->'x'.join((str(size) for size in p.size()))
A:torch.nn.modules.container.parastr->'Parameter containing: [{} of size {}{}]'.format(torch.typename(p.data), size_str, device_str)
A:torch.nn.modules.container.tmpstr->'\n'.join(child_lines)
torch.nn.Container(self,**kwargs)
torch.nn.ModuleDict(self,modules=None)
torch.nn.ModuleDict.__contains__(self,key)
torch.nn.ModuleDict.__delitem__(self,key)
torch.nn.ModuleDict.__getitem__(self,key)
torch.nn.ModuleDict.__iter__(self)
torch.nn.ModuleDict.__len__(self)
torch.nn.ModuleDict.__setitem__(self,key,module)
torch.nn.ModuleDict.clear(self)
torch.nn.ModuleDict.items(self)
torch.nn.ModuleDict.keys(self)
torch.nn.ModuleDict.pop(self,key)
torch.nn.ModuleDict.update(self,modules)
torch.nn.ModuleDict.values(self)
torch.nn.ModuleList(self,modules=None)
torch.nn.ModuleList.__delitem__(self,idx)
torch.nn.ModuleList.__dir__(self)
torch.nn.ModuleList.__getitem__(self,idx)
torch.nn.ModuleList.__iadd__(self,modules)
torch.nn.ModuleList.__iter__(self)
torch.nn.ModuleList.__len__(self)
torch.nn.ModuleList.__setitem__(self,idx,module)
torch.nn.ModuleList._get_abs_string_index(self,idx)
torch.nn.ModuleList.append(self,module)
torch.nn.ModuleList.extend(self,modules)
torch.nn.ModuleList.insert(self,index,module)
torch.nn.ParameterDict(self,parameters=None)
torch.nn.ParameterDict.__contains__(self,key)
torch.nn.ParameterDict.__delitem__(self,key)
torch.nn.ParameterDict.__getitem__(self,key)
torch.nn.ParameterDict.__iter__(self)
torch.nn.ParameterDict.__len__(self)
torch.nn.ParameterDict.__setitem__(self,key,parameter)
torch.nn.ParameterDict.clear(self)
torch.nn.ParameterDict.extra_repr(self)
torch.nn.ParameterDict.items(self)
torch.nn.ParameterDict.keys(self)
torch.nn.ParameterDict.pop(self,key)
torch.nn.ParameterDict.update(self,parameters)
torch.nn.ParameterDict.values(self)
torch.nn.ParameterList(self,parameters=None)
torch.nn.ParameterList.__dir__(self)
torch.nn.ParameterList.__getitem__(self,idx)
torch.nn.ParameterList.__iadd__(self,parameters)
torch.nn.ParameterList.__iter__(self)
torch.nn.ParameterList.__len__(self)
torch.nn.ParameterList.__setitem__(self,idx,param)
torch.nn.ParameterList._get_abs_string_index(self,idx)
torch.nn.ParameterList.append(self,parameter)
torch.nn.ParameterList.extend(self,parameters)
torch.nn.ParameterList.extra_repr(self)
torch.nn.Sequential(self,*args)
torch.nn.Sequential.__delitem__(self,idx)
torch.nn.Sequential.__dir__(self)
torch.nn.Sequential.__getitem__(self,idx)
torch.nn.Sequential.__len__(self)
torch.nn.Sequential.__setitem__(self,idx,module)
torch.nn.Sequential._get_item_by_idx(self,iterator,idx)
torch.nn.Sequential.forward(self,input)
torch.nn.modules.container.Container(self,**kwargs)
torch.nn.modules.container.Container.__init__(self,**kwargs)
torch.nn.modules.container.ModuleDict(self,modules=None)
torch.nn.modules.container.ModuleDict.__contains__(self,key)
torch.nn.modules.container.ModuleDict.__delitem__(self,key)
torch.nn.modules.container.ModuleDict.__getitem__(self,key)
torch.nn.modules.container.ModuleDict.__init__(self,modules=None)
torch.nn.modules.container.ModuleDict.__iter__(self)
torch.nn.modules.container.ModuleDict.__len__(self)
torch.nn.modules.container.ModuleDict.__setitem__(self,key,module)
torch.nn.modules.container.ModuleDict.clear(self)
torch.nn.modules.container.ModuleDict.items(self)
torch.nn.modules.container.ModuleDict.keys(self)
torch.nn.modules.container.ModuleDict.pop(self,key)
torch.nn.modules.container.ModuleDict.update(self,modules)
torch.nn.modules.container.ModuleDict.values(self)
torch.nn.modules.container.ModuleList(self,modules=None)
torch.nn.modules.container.ModuleList.__delitem__(self,idx)
torch.nn.modules.container.ModuleList.__dir__(self)
torch.nn.modules.container.ModuleList.__getitem__(self,idx)
torch.nn.modules.container.ModuleList.__iadd__(self,modules)
torch.nn.modules.container.ModuleList.__init__(self,modules=None)
torch.nn.modules.container.ModuleList.__iter__(self)
torch.nn.modules.container.ModuleList.__len__(self)
torch.nn.modules.container.ModuleList.__setitem__(self,idx,module)
torch.nn.modules.container.ModuleList._get_abs_string_index(self,idx)
torch.nn.modules.container.ModuleList.append(self,module)
torch.nn.modules.container.ModuleList.extend(self,modules)
torch.nn.modules.container.ModuleList.insert(self,index,module)
torch.nn.modules.container.ParameterDict(self,parameters=None)
torch.nn.modules.container.ParameterDict.__contains__(self,key)
torch.nn.modules.container.ParameterDict.__delitem__(self,key)
torch.nn.modules.container.ParameterDict.__getitem__(self,key)
torch.nn.modules.container.ParameterDict.__init__(self,parameters=None)
torch.nn.modules.container.ParameterDict.__iter__(self)
torch.nn.modules.container.ParameterDict.__len__(self)
torch.nn.modules.container.ParameterDict.__setitem__(self,key,parameter)
torch.nn.modules.container.ParameterDict.clear(self)
torch.nn.modules.container.ParameterDict.extra_repr(self)
torch.nn.modules.container.ParameterDict.items(self)
torch.nn.modules.container.ParameterDict.keys(self)
torch.nn.modules.container.ParameterDict.pop(self,key)
torch.nn.modules.container.ParameterDict.update(self,parameters)
torch.nn.modules.container.ParameterDict.values(self)
torch.nn.modules.container.ParameterList(self,parameters=None)
torch.nn.modules.container.ParameterList.__dir__(self)
torch.nn.modules.container.ParameterList.__getitem__(self,idx)
torch.nn.modules.container.ParameterList.__iadd__(self,parameters)
torch.nn.modules.container.ParameterList.__init__(self,parameters=None)
torch.nn.modules.container.ParameterList.__iter__(self)
torch.nn.modules.container.ParameterList.__len__(self)
torch.nn.modules.container.ParameterList.__setitem__(self,idx,param)
torch.nn.modules.container.ParameterList._get_abs_string_index(self,idx)
torch.nn.modules.container.ParameterList.append(self,parameter)
torch.nn.modules.container.ParameterList.extend(self,parameters)
torch.nn.modules.container.ParameterList.extra_repr(self)
torch.nn.modules.container.Sequential(self,*args)
torch.nn.modules.container.Sequential.__delitem__(self,idx)
torch.nn.modules.container.Sequential.__dir__(self)
torch.nn.modules.container.Sequential.__getitem__(self,idx)
torch.nn.modules.container.Sequential.__init__(self,*args)
torch.nn.modules.container.Sequential.__len__(self)
torch.nn.modules.container.Sequential.__setitem__(self,idx,module)
torch.nn.modules.container.Sequential._get_item_by_idx(self,iterator,idx)
torch.nn.modules.container.Sequential.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/rnn.py----------------------------------------
A:torch.nn.modules.rnn.w_ih->Parameter(torch.Tensor(gate_size, layer_input_size))
A:torch.nn.modules.rnn.w_hh->Parameter(torch.Tensor(gate_size, hidden_size))
A:torch.nn.modules.rnn.b_ih->Parameter(torch.Tensor(gate_size))
A:torch.nn.modules.rnn.b_hh->Parameter(torch.Tensor(gate_size))
A:torch.nn.modules.rnn.unique_data_ptrs->set((p.data_ptr() for p in all_weights))
A:torch.nn.modules.rnn.ret->super(RNNBase, self)._apply(fn)
A:torch.nn.modules.rnn.mini_batch->int(batch_sizes[0])
A:torch.nn.modules.rnn.is_packed->isinstance(input, PackedSequence)
A:torch.nn.modules.rnn.max_batch_size->int(batch_sizes[0])
A:torch.nn.modules.rnn.hx->input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
A:torch.nn.modules.rnn.result->_impl(input, batch_sizes, hx, self._flat_weights, self.bias, self.num_layers, self.dropout, self.training, self.bidirectional)
A:torch.nn.modules.rnn.output->PackedSequence(output, batch_sizes)
A:torch.nn.modules.rnn.self.weight_ih->Parameter(torch.Tensor(num_chunks * hidden_size, input_size))
A:torch.nn.modules.rnn.self.weight_hh->Parameter(torch.Tensor(num_chunks * hidden_size, hidden_size))
A:torch.nn.modules.rnn.self.bias_ih->Parameter(torch.Tensor(num_chunks * hidden_size))
A:torch.nn.modules.rnn.self.bias_hh->Parameter(torch.Tensor(num_chunks * hidden_size))
torch.nn.GRU(self,*args,**kwargs)
torch.nn.GRUCell(self,input_size,hidden_size,bias=True)
torch.nn.GRUCell.forward(self,input,hx=None)
torch.nn.LSTM(self,*args,**kwargs)
torch.nn.LSTMCell(self,input_size,hidden_size,bias=True)
torch.nn.LSTMCell.forward(self,input,hx=None)
torch.nn.RNN(self,*args,**kwargs)
torch.nn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0,bidirectional=False)
torch.nn.RNNBase.__setstate__(self,d)
torch.nn.RNNBase._apply(self,fn)
torch.nn.RNNBase._flat_weights(self)
torch.nn.RNNBase.all_weights(self)
torch.nn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.RNNBase.extra_repr(self)
torch.nn.RNNBase.flatten_parameters(self)
torch.nn.RNNBase.forward(self,input,hx=None)
torch.nn.RNNBase.reset_parameters(self)
torch.nn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.RNNCell.forward(self,input,hx=None)
torch.nn.RNNCellBase(self,input_size,hidden_size,bias,num_chunks)
torch.nn.RNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.nn.RNNCellBase.check_forward_input(self,input)
torch.nn.RNNCellBase.extra_repr(self)
torch.nn.RNNCellBase.reset_parameters(self)
torch.nn.modules.rnn.GRU(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.GRUCell(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.GRUCell.__init__(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.GRUCell.forward(self,input,hx=None)
torch.nn.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.LSTMCell(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.LSTMCell.__init__(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.LSTMCell.forward(self,input,hx=None)
torch.nn.modules.rnn.RNN(self,*args,**kwargs)
torch.nn.modules.rnn.RNN.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0,bidirectional=False)
torch.nn.modules.rnn.RNNBase.__init__(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0,bidirectional=False)
torch.nn.modules.rnn.RNNBase.__setstate__(self,d)
torch.nn.modules.rnn.RNNBase._apply(self,fn)
torch.nn.modules.rnn.RNNBase._flat_weights(self)
torch.nn.modules.rnn.RNNBase.all_weights(self)
torch.nn.modules.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.modules.rnn.RNNBase.extra_repr(self)
torch.nn.modules.rnn.RNNBase.flatten_parameters(self)
torch.nn.modules.rnn.RNNBase.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNBase.reset_parameters(self)
torch.nn.modules.rnn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.modules.rnn.RNNCell.__init__(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.modules.rnn.RNNCell.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNCellBase(self,input_size,hidden_size,bias,num_chunks)
torch.nn.modules.rnn.RNNCellBase.__init__(self,input_size,hidden_size,bias,num_chunks)
torch.nn.modules.rnn.RNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.nn.modules.rnn.RNNCellBase.check_forward_input(self,input)
torch.nn.modules.rnn.RNNCellBase.extra_repr(self)
torch.nn.modules.rnn.RNNCellBase.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/sparse.py----------------------------------------
A:torch.nn.modules.sparse.self.weight->Parameter(torch.Tensor(num_embeddings, embedding_dim))
A:torch.nn.modules.sparse.embedding->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, sparse=sparse)
torch.nn.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.Embedding.extra_repr(self)
torch.nn.Embedding.forward(self,input)
torch.nn.Embedding.from_pretrained(cls,embeddings,freeze=True,sparse=False)
torch.nn.Embedding.reset_parameters(self)
torch.nn.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.EmbeddingBag.extra_repr(self)
torch.nn.EmbeddingBag.forward(self,input,offsets=None)
torch.nn.EmbeddingBag.reset_parameters(self)
torch.nn.modules.sparse.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.modules.sparse.Embedding.__init__(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.modules.sparse.Embedding.extra_repr(self)
torch.nn.modules.sparse.Embedding.forward(self,input)
torch.nn.modules.sparse.Embedding.from_pretrained(cls,embeddings,freeze=True,sparse=False)
torch.nn.modules.sparse.Embedding.reset_parameters(self)
torch.nn.modules.sparse.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.modules.sparse.EmbeddingBag.__init__(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.modules.sparse.EmbeddingBag.extra_repr(self)
torch.nn.modules.sparse.EmbeddingBag.forward(self,input,offsets=None)
torch.nn.modules.sparse.EmbeddingBag.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/loss.py----------------------------------------
A:torch.nn.modules.loss.self.reduction->_Reduction.legacy_get_string(size_average, reduce)
torch.nn.BCELoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.BCELoss.forward(self,input,target)
torch.nn.BCEWithLogitsLoss(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.BCEWithLogitsLoss.forward(self,input,target)
torch.nn.CTCLoss(self,blank=0,reduction='mean')
torch.nn.CTCLoss.forward(self,log_probs,targets,input_lengths,target_lengths)
torch.nn.CosineEmbeddingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.CosineEmbeddingLoss.forward(self,input1,input2,target)
torch.nn.CrossEntropyLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.CrossEntropyLoss.forward(self,input,target)
torch.nn.HingeEmbeddingLoss(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.HingeEmbeddingLoss.forward(self,input,target)
torch.nn.KLDivLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.KLDivLoss.forward(self,input,target)
torch.nn.L1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.L1Loss.forward(self,input,target)
torch.nn.MSELoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.MSELoss.forward(self,input,target)
torch.nn.MarginRankingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.MarginRankingLoss.forward(self,input1,input2,target)
torch.nn.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiLabelMarginLoss.forward(self,input,target)
torch.nn.MultiLabelSoftMarginLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiLabelSoftMarginLoss.forward(self,input,target)
torch.nn.MultiMarginLoss(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiMarginLoss.forward(self,input,target)
torch.nn.NLLLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.NLLLoss.forward(self,input,target)
torch.nn.NLLLoss2d(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.PoissonNLLLoss(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.PoissonNLLLoss.forward(self,log_input,target)
torch.nn.SmoothL1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.SmoothL1Loss.forward(self,input,target)
torch.nn.SoftMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.SoftMarginLoss.forward(self,input,target)
torch.nn.TripletMarginLoss(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.TripletMarginLoss.forward(self,anchor,positive,negative)
torch.nn.loss._Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.loss._WeightedLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss.forward(self,input,target)
torch.nn.modules.loss.BCEWithLogitsLoss(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.modules.loss.BCEWithLogitsLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.modules.loss.BCEWithLogitsLoss.forward(self,input,target)
torch.nn.modules.loss.CTCLoss(self,blank=0,reduction='mean')
torch.nn.modules.loss.CTCLoss.__init__(self,blank=0,reduction='mean')
torch.nn.modules.loss.CTCLoss.forward(self,log_probs,targets,input_lengths,target_lengths)
torch.nn.modules.loss.CosineEmbeddingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.__init__(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.forward(self,input1,input2,target)
torch.nn.modules.loss.CrossEntropyLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.CrossEntropyLoss.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.CrossEntropyLoss.forward(self,input,target)
torch.nn.modules.loss.HingeEmbeddingLoss(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.__init__(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.forward(self,input,target)
torch.nn.modules.loss.KLDivLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.KLDivLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.KLDivLoss.forward(self,input,target)
torch.nn.modules.loss.L1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.L1Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.L1Loss.forward(self,input,target)
torch.nn.modules.loss.MSELoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MSELoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MSELoss.forward(self,input,target)
torch.nn.modules.loss.MarginRankingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MarginRankingLoss.__init__(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MarginRankingLoss.forward(self,input1,input2,target)
torch.nn.modules.loss.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.forward(self,input,target)
torch.nn.modules.loss.MultiLabelSoftMarginLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.forward(self,input,target)
torch.nn.modules.loss.MultiMarginLoss(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiMarginLoss.__init__(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiMarginLoss.forward(self,input,target)
torch.nn.modules.loss.NLLLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss.forward(self,input,target)
torch.nn.modules.loss.NLLLoss2d(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss2d.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss.__init__(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss.forward(self,log_input,target)
torch.nn.modules.loss.SmoothL1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SmoothL1Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SmoothL1Loss.forward(self,input,target)
torch.nn.modules.loss.SoftMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SoftMarginLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SoftMarginLoss.forward(self,input,target)
torch.nn.modules.loss.TripletMarginLoss(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.TripletMarginLoss.__init__(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.TripletMarginLoss.forward(self,anchor,positive,negative)
torch.nn.modules.loss._Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._WeightedLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._WeightedLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/distance.py----------------------------------------
torch.nn.CosineSimilarity(self,dim=1,eps=1e-08)
torch.nn.CosineSimilarity.forward(self,x1,x2)
torch.nn.PairwiseDistance(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.PairwiseDistance.forward(self,x1,x2)
torch.nn.modules.distance.CosineSimilarity(self,dim=1,eps=1e-08)
torch.nn.modules.distance.CosineSimilarity.__init__(self,dim=1,eps=1e-08)
torch.nn.modules.distance.CosineSimilarity.forward(self,x1,x2)
torch.nn.modules.distance.PairwiseDistance(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.modules.distance.PairwiseDistance.__init__(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.modules.distance.PairwiseDistance.forward(self,x1,x2)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/utils.py----------------------------------------
A:torch.nn.modules.utils._single->_ntuple(1)
A:torch.nn.modules.utils._pair->_ntuple(2)
A:torch.nn.modules.utils._triple->_ntuple(3)
A:torch.nn.modules.utils._quadruple->_ntuple(4)
torch.nn.modules.utils._list_with_default(out_size,defaults)
torch.nn.modules.utils._ntuple(n)
torch.nn.utils._list_with_default(out_size,defaults)
torch.nn.utils._ntuple(n)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/pooling.py----------------------------------------
A:torch.nn.modules.pooling.self.kernel_size->_pair(kernel_size)
A:torch.nn.modules.pooling.self.stride->_single(stride if stride is not None else kernel_size)
A:torch.nn.modules.pooling.self.padding->_single(padding)
torch.nn.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool1d.forward(self,input)
torch.nn.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool2d.forward(self,input)
torch.nn.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool3d.forward(self,input)
torch.nn.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool1d.forward(self,input)
torch.nn.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool2d.forward(self,input)
torch.nn.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool3d.forward(self,input)
torch.nn.AvgPool1d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.AvgPool1d.forward(self,input)
torch.nn.AvgPool2d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.AvgPool2d.forward(self,input)
torch.nn.AvgPool3d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.AvgPool3d.__setstate__(self,d)
torch.nn.AvgPool3d.forward(self,input)
torch.nn.FractionalMaxPool2d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.FractionalMaxPool2d.forward(self,input)
torch.nn.LPPool1d(_LPPoolNd)
torch.nn.LPPool1d.forward(self,input)
torch.nn.LPPool2d(_LPPoolNd)
torch.nn.LPPool2d.forward(self,input)
torch.nn.MaxPool1d(_MaxPoolNd)
torch.nn.MaxPool1d.extra_repr(self)
torch.nn.MaxPool1d.forward(self,input)
torch.nn.MaxPool2d(_MaxPoolNd)
torch.nn.MaxPool2d.forward(self,input)
torch.nn.MaxPool3d(_MaxPoolNd)
torch.nn.MaxPool3d.forward(self,input)
torch.nn.MaxUnpool1d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool1d.forward(self,input,indices,output_size=None)
torch.nn.MaxUnpool2d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool2d.forward(self,input,indices,output_size=None)
torch.nn.MaxUnpool3d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool3d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool1d.forward(self,input)
torch.nn.modules.pooling.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool2d.forward(self,input)
torch.nn.modules.pooling.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool3d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool1d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool2d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool3d.forward(self,input)
torch.nn.modules.pooling.AvgPool1d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool1d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool1d.forward(self,input)
torch.nn.modules.pooling.AvgPool2d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool2d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool2d.forward(self,input)
torch.nn.modules.pooling.AvgPool3d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool3d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool3d.__setstate__(self,d)
torch.nn.modules.pooling.AvgPool3d.forward(self,input)
torch.nn.modules.pooling.FractionalMaxPool2d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.__init__(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.forward(self,input)
torch.nn.modules.pooling.LPPool1d(_LPPoolNd)
torch.nn.modules.pooling.LPPool1d.forward(self,input)
torch.nn.modules.pooling.LPPool2d(_LPPoolNd)
torch.nn.modules.pooling.LPPool2d.forward(self,input)
torch.nn.modules.pooling.MaxPool1d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool1d.extra_repr(self)
torch.nn.modules.pooling.MaxPool1d.forward(self,input)
torch.nn.modules.pooling.MaxPool2d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool2d.forward(self,input)
torch.nn.modules.pooling.MaxPool3d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool3d.forward(self,input)
torch.nn.modules.pooling.MaxUnpool1d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool1d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool1d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.MaxUnpool2d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool2d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool2d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.MaxUnpool3d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool3d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool3d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling._AdaptiveAvgPoolNd(self,output_size)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.__init__(self,output_size)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.extra_repr(self)
torch.nn.modules.pooling._AdaptiveMaxPoolNd(self,output_size,return_indices=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.__init__(self,output_size,return_indices=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.extra_repr(self)
torch.nn.modules.pooling._AvgPoolNd(Module)
torch.nn.modules.pooling._AvgPoolNd.extra_repr(self)
torch.nn.modules.pooling._LPPoolNd(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.modules.pooling._LPPoolNd.__init__(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.modules.pooling._LPPoolNd.extra_repr(self)
torch.nn.modules.pooling._MaxPoolNd(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.modules.pooling._MaxPoolNd.__init__(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.modules.pooling._MaxPoolNd.extra_repr(self)
torch.nn.modules.pooling._MaxUnpoolNd(Module)
torch.nn.modules.pooling._MaxUnpoolNd.extra_repr(self)
torch.nn.pooling._AdaptiveAvgPoolNd(self,output_size)
torch.nn.pooling._AdaptiveAvgPoolNd.extra_repr(self)
torch.nn.pooling._AdaptiveMaxPoolNd(self,output_size,return_indices=False)
torch.nn.pooling._AdaptiveMaxPoolNd.extra_repr(self)
torch.nn.pooling._AvgPoolNd(Module)
torch.nn.pooling._AvgPoolNd.extra_repr(self)
torch.nn.pooling._LPPoolNd(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.pooling._LPPoolNd.extra_repr(self)
torch.nn.pooling._MaxPoolNd(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.pooling._MaxPoolNd.extra_repr(self)
torch.nn.pooling._MaxUnpoolNd(Module)
torch.nn.pooling._MaxUnpoolNd.extra_repr(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/adaptive.py----------------------------------------
A:torch.nn.modules.adaptive._ASMoutput->namedtuple('ASMoutput', ['output', 'loss'])
A:torch.nn.modules.adaptive.cutoffs->list(cutoffs)
A:torch.nn.modules.adaptive.self.head->Linear(self.in_features, self.head_size, bias=self.head_bias)
A:torch.nn.modules.adaptive.self.tail->ModuleList()
A:torch.nn.modules.adaptive.hsz->int(self.in_features // self.div_value ** (i + 1))
A:torch.nn.modules.adaptive.projection->Sequential(Linear(self.in_features, hsz, bias=False), Linear(hsz, osz, bias=False))
A:torch.nn.modules.adaptive.batch_size->target.size(0)
A:torch.nn.modules.adaptive.output->torch.argmax(head_output, dim=1)
A:torch.nn.modules.adaptive.gather_inds->target.new_empty(batch_size)
A:torch.nn.modules.adaptive.row_indices->target_mask.nonzero().squeeze()
A:torch.nn.modules.adaptive.input_subset->input.index_select(0, row_indices)
A:torch.nn.modules.adaptive.cluster_output->self.tail[i](input)
A:torch.nn.modules.adaptive.cluster_logprob->log_softmax(cluster_output, dim=1)
A:torch.nn.modules.adaptive.local_logprob->log_softmax(cluster_output, dim=1).gather(1, relative_target.unsqueeze(1))
A:torch.nn.modules.adaptive.head_output->self.head(input)
A:torch.nn.modules.adaptive.head_logprob->log_softmax(head_output, dim=1)
A:torch.nn.modules.adaptive.loss->(-output).mean()
A:torch.nn.modules.adaptive.out->input.new_empty((head_output.size(0), self.n_classes))
A:torch.nn.modules.adaptive.log_prob->self._get_full_log_prob(input[not_in_shortlist], head_output[not_in_shortlist])
A:torch.nn.modules.adaptive.output[not_in_shortlist]->torch.argmax(log_prob, dim=1)
torch.nn.AdaptiveLogSoftmaxWithLoss(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.AdaptiveLogSoftmaxWithLoss.forward(self,input,target)
torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob(self,input)
torch.nn.AdaptiveLogSoftmaxWithLoss.predict(self,input)
torch.nn.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.__init__(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.forward(self,input,target)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.log_prob(self,input)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.predict(self,input)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/dropout.py----------------------------------------
torch.nn.AlphaDropout(_DropoutNd)
torch.nn.AlphaDropout.forward(self,input)
torch.nn.Dropout(_DropoutNd)
torch.nn.Dropout.forward(self,input)
torch.nn.Dropout2d(_DropoutNd)
torch.nn.Dropout2d.forward(self,input)
torch.nn.Dropout3d(_DropoutNd)
torch.nn.Dropout3d.forward(self,input)
torch.nn.FeatureAlphaDropout(_DropoutNd)
torch.nn.FeatureAlphaDropout.forward(self,input)
torch.nn.dropout._DropoutNd(self,p=0.5,inplace=False)
torch.nn.dropout._DropoutNd.extra_repr(self)
torch.nn.modules.dropout.AlphaDropout(_DropoutNd)
torch.nn.modules.dropout.AlphaDropout.forward(self,input)
torch.nn.modules.dropout.Dropout(_DropoutNd)
torch.nn.modules.dropout.Dropout.forward(self,input)
torch.nn.modules.dropout.Dropout2d(_DropoutNd)
torch.nn.modules.dropout.Dropout2d.forward(self,input)
torch.nn.modules.dropout.Dropout3d(_DropoutNd)
torch.nn.modules.dropout.Dropout3d.forward(self,input)
torch.nn.modules.dropout.FeatureAlphaDropout(_DropoutNd)
torch.nn.modules.dropout.FeatureAlphaDropout.forward(self,input)
torch.nn.modules.dropout._DropoutNd(self,p=0.5,inplace=False)
torch.nn.modules.dropout._DropoutNd.__init__(self,p=0.5,inplace=False)
torch.nn.modules.dropout._DropoutNd.extra_repr(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/module.py----------------------------------------
A:torch.nn.modules.module.s->'\n'.join(s)
A:torch.nn.modules.module.first->'\n'.join(s).pop(0)
A:torch.nn.modules.module.self._parameters->OrderedDict()
A:torch.nn.modules.module.self._buffers->OrderedDict()
A:torch.nn.modules.module.self._backward_hooks->OrderedDict()
A:torch.nn.modules.module.self._forward_hooks->OrderedDict()
A:torch.nn.modules.module.self._forward_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._state_dict_hooks->OrderedDict()
A:torch.nn.modules.module.self._load_state_dict_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._modules->OrderedDict()
A:torch.nn.modules.module.param.data->fn(param.data)
A:torch.nn.modules.module.param._grad.data->fn(param._grad.data)
A:torch.nn.modules.module.self._buffers[key]->fn(buf)
A:torch.nn.modules.module.(device, dtype, non_blocking)->torch._C._nn._parse_to(*args, **kwargs)
A:torch.nn.modules.module.handle->torch.utils.hooks.RemovableHandle(self._load_state_dict_pre_hooks)
A:torch.nn.modules.module.input_vars->tuple(torch.autograd.function._iter_tensors(input))
A:torch.nn.modules.module.tracing_state->torch._C._get_tracing_state()
A:torch.nn.modules.module.name->self._tracing_name(tracing_state)
A:torch.nn.modules.module.result->self.forward(*input, **kwargs)
A:torch.nn.modules.module.hook_result->hook(self, destination, prefix, local_metadata)
A:torch.nn.modules.module.var->next((v for v in var.values() if isinstance(v, torch.Tensor)))
A:torch.nn.modules.module.wrapper->functools.partial(hook, self)
A:torch.nn.modules.module.params->self.__dict__.get('_parameters')
A:torch.nn.modules.module.modules->list(self._modules.keys())
A:torch.nn.modules.module.buffers->list(self._buffers.keys())
A:torch.nn.modules.module.destination->OrderedDict()
A:torch.nn.modules.module.destination._metadata->OrderedDict()
A:torch.nn.modules.module.destination._metadata[prefix[:-1]]local_metadata->dict(version=self._version)
A:torch.nn.modules.module.local_name_params->itertools.chain(self._parameters.items(), self._buffers.items())
A:torch.nn.modules.module.metadata->getattr(state_dict, '_metadata', None)
A:torch.nn.modules.module.state_dict->state_dict.copy().copy()
A:torch.nn.modules.module.memo->set()
A:torch.nn.modules.module.members->get_members_fn(module)
A:torch.nn.modules.module.gen->self._named_members(lambda module: module._buffers.items(), prefix=prefix, recurse=recurse)
A:torch.nn.modules.module.extra_repr->self.extra_repr()
A:torch.nn.modules.module.extra_lines->self.extra_repr().split('\n')
A:torch.nn.modules.module.mod_str->_addindent(mod_str, 2)
A:torch.nn.modules.module.module_attrs->dir(self.__class__)
A:torch.nn.modules.module.attrs->list(self.__dict__.keys())
A:torch.nn.modules.module.parameters->list(self._parameters.keys())
torch.nn.Module(self)
torch.nn.Module.__delattr__(self,name)
torch.nn.Module.__dir__(self)
torch.nn.Module.__getattr__(self,name)
torch.nn.Module.__repr__(self)
torch.nn.Module.__setattr__(self,name,value)
torch.nn.Module.__setstate__(self,state)
torch.nn.Module._apply(self,fn)
torch.nn.Module._get_name(self)
torch.nn.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.Module._register_load_state_dict_pre_hook(self,hook)
torch.nn.Module._register_state_dict_hook(self,hook)
torch.nn.Module._slow_forward(self,*input,**kwargs)
torch.nn.Module._tracing_name(self,tracing_state)
torch.nn.Module.add_module(self,name,module)
torch.nn.Module.apply(self,fn)
torch.nn.Module.buffers(self,recurse=True)
torch.nn.Module.children(self)
torch.nn.Module.cpu(self)
torch.nn.Module.cuda(self,device=None)
torch.nn.Module.double(self)
torch.nn.Module.eval(self)
torch.nn.Module.extra_repr(self)
torch.nn.Module.float(self)
torch.nn.Module.forward(self,*input)
torch.nn.Module.half(self)
torch.nn.Module.load_state_dict(self,state_dict,strict=True)
torch.nn.Module.modules(self)
torch.nn.Module.named_buffers(self,prefix='',recurse=True)
torch.nn.Module.named_children(self)
torch.nn.Module.named_modules(self,memo=None,prefix='')
torch.nn.Module.named_parameters(self,prefix='',recurse=True)
torch.nn.Module.parameters(self,recurse=True)
torch.nn.Module.register_backward_hook(self,hook)
torch.nn.Module.register_buffer(self,name,tensor)
torch.nn.Module.register_forward_hook(self,hook)
torch.nn.Module.register_forward_pre_hook(self,hook)
torch.nn.Module.register_parameter(self,name,param)
torch.nn.Module.share_memory(self)
torch.nn.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.Module.to(self,*args,**kwargs)
torch.nn.Module.train(self,mode=True)
torch.nn.Module.type(self,dst_type)
torch.nn.Module.zero_grad(self)
torch.nn.module._addindent(s_,numSpaces)
torch.nn.modules.module.Module(self)
torch.nn.modules.module.Module.__delattr__(self,name)
torch.nn.modules.module.Module.__dir__(self)
torch.nn.modules.module.Module.__getattr__(self,name)
torch.nn.modules.module.Module.__init__(self)
torch.nn.modules.module.Module.__repr__(self)
torch.nn.modules.module.Module.__setattr__(self,name,value)
torch.nn.modules.module.Module.__setstate__(self,state)
torch.nn.modules.module.Module._apply(self,fn)
torch.nn.modules.module.Module._get_name(self)
torch.nn.modules.module.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.module.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self,hook)
torch.nn.modules.module.Module._register_state_dict_hook(self,hook)
torch.nn.modules.module.Module._slow_forward(self,*input,**kwargs)
torch.nn.modules.module.Module._tracing_name(self,tracing_state)
torch.nn.modules.module.Module.add_module(self,name,module)
torch.nn.modules.module.Module.apply(self,fn)
torch.nn.modules.module.Module.buffers(self,recurse=True)
torch.nn.modules.module.Module.children(self)
torch.nn.modules.module.Module.cpu(self)
torch.nn.modules.module.Module.cuda(self,device=None)
torch.nn.modules.module.Module.double(self)
torch.nn.modules.module.Module.eval(self)
torch.nn.modules.module.Module.extra_repr(self)
torch.nn.modules.module.Module.float(self)
torch.nn.modules.module.Module.forward(self,*input)
torch.nn.modules.module.Module.half(self)
torch.nn.modules.module.Module.load_state_dict(self,state_dict,strict=True)
torch.nn.modules.module.Module.modules(self)
torch.nn.modules.module.Module.named_buffers(self,prefix='',recurse=True)
torch.nn.modules.module.Module.named_children(self)
torch.nn.modules.module.Module.named_modules(self,memo=None,prefix='')
torch.nn.modules.module.Module.named_parameters(self,prefix='',recurse=True)
torch.nn.modules.module.Module.parameters(self,recurse=True)
torch.nn.modules.module.Module.register_backward_hook(self,hook)
torch.nn.modules.module.Module.register_buffer(self,name,tensor)
torch.nn.modules.module.Module.register_forward_hook(self,hook)
torch.nn.modules.module.Module.register_forward_pre_hook(self,hook)
torch.nn.modules.module.Module.register_parameter(self,name,param)
torch.nn.modules.module.Module.share_memory(self)
torch.nn.modules.module.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.modules.module.Module.to(self,*args,**kwargs)
torch.nn.modules.module.Module.train(self,mode=True)
torch.nn.modules.module.Module.type(self,dst_type)
torch.nn.modules.module.Module.zero_grad(self)
torch.nn.modules.module._addindent(s_,numSpaces)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/instancenorm.py----------------------------------------
A:torch.nn.modules.instancenorm.version->local_metadata.get('version', None)
torch.nn.InstanceNorm1d(_InstanceNorm)
torch.nn.InstanceNorm1d._check_input_dim(self,input)
torch.nn.InstanceNorm2d(_InstanceNorm)
torch.nn.InstanceNorm2d._check_input_dim(self,input)
torch.nn.InstanceNorm3d(_InstanceNorm)
torch.nn.InstanceNorm3d._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.instancenorm._InstanceNorm.forward(self,input)
torch.nn.modules.instancenorm.InstanceNorm1d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm1d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm2d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm2d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm3d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm3d._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.modules.instancenorm._InstanceNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.modules.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.instancenorm._InstanceNorm.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/padding.py----------------------------------------
A:torch.nn.modules.padding.self.padding->_ntuple(6)(padding)
torch.nn.ConstantPad1d(self,padding,value)
torch.nn.ConstantPad2d(self,padding,value)
torch.nn.ConstantPad3d(self,padding,value)
torch.nn.ReflectionPad1d(self,padding)
torch.nn.ReflectionPad2d(self,padding)
torch.nn.ReplicationPad1d(self,padding)
torch.nn.ReplicationPad2d(self,padding)
torch.nn.ReplicationPad3d(self,padding)
torch.nn.ZeroPad2d(self,padding)
torch.nn.modules.padding.ConstantPad1d(self,padding,value)
torch.nn.modules.padding.ConstantPad1d.__init__(self,padding,value)
torch.nn.modules.padding.ConstantPad2d(self,padding,value)
torch.nn.modules.padding.ConstantPad2d.__init__(self,padding,value)
torch.nn.modules.padding.ConstantPad3d(self,padding,value)
torch.nn.modules.padding.ConstantPad3d.__init__(self,padding,value)
torch.nn.modules.padding.ReflectionPad1d(self,padding)
torch.nn.modules.padding.ReflectionPad1d.__init__(self,padding)
torch.nn.modules.padding.ReflectionPad2d(self,padding)
torch.nn.modules.padding.ReflectionPad2d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad1d(self,padding)
torch.nn.modules.padding.ReplicationPad1d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad2d(self,padding)
torch.nn.modules.padding.ReplicationPad2d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad3d(self,padding)
torch.nn.modules.padding.ReplicationPad3d.__init__(self,padding)
torch.nn.modules.padding.ZeroPad2d(self,padding)
torch.nn.modules.padding.ZeroPad2d.__init__(self,padding)
torch.nn.modules.padding._ConstantPadNd(self,value)
torch.nn.modules.padding._ConstantPadNd.__init__(self,value)
torch.nn.modules.padding._ConstantPadNd.extra_repr(self)
torch.nn.modules.padding._ConstantPadNd.forward(self,input)
torch.nn.modules.padding._ReflectionPadNd(Module)
torch.nn.modules.padding._ReflectionPadNd.extra_repr(self)
torch.nn.modules.padding._ReflectionPadNd.forward(self,input)
torch.nn.modules.padding._ReplicationPadNd(Module)
torch.nn.modules.padding._ReplicationPadNd.extra_repr(self)
torch.nn.modules.padding._ReplicationPadNd.forward(self,input)
torch.nn.padding._ConstantPadNd(self,value)
torch.nn.padding._ConstantPadNd.extra_repr(self)
torch.nn.padding._ConstantPadNd.forward(self,input)
torch.nn.padding._ReflectionPadNd(Module)
torch.nn.padding._ReflectionPadNd.extra_repr(self)
torch.nn.padding._ReflectionPadNd.forward(self,input)
torch.nn.padding._ReplicationPadNd(Module)
torch.nn.padding._ReplicationPadNd.extra_repr(self)
torch.nn.padding._ReplicationPadNd.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/normalization.py----------------------------------------
A:torch.nn.modules.normalization.self.normalized_shape->torch.Size(normalized_shape)
A:torch.nn.modules.normalization.self.weight->Parameter(torch.Tensor(num_channels))
A:torch.nn.modules.normalization.self.bias->Parameter(torch.Tensor(num_channels))
torch.nn.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.CrossMapLRN2d.extra_repr(self)
torch.nn.CrossMapLRN2d.forward(self,input)
torch.nn.GroupNorm(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.GroupNorm.extra_repr(self)
torch.nn.GroupNorm.forward(self,input)
torch.nn.GroupNorm.reset_parameters(self)
torch.nn.LayerNorm(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.LayerNorm.extra_repr(self)
torch.nn.LayerNorm.forward(self,input)
torch.nn.LayerNorm.reset_parameters(self)
torch.nn.LocalResponseNorm(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.LocalResponseNorm.extra_repr(self)
torch.nn.LocalResponseNorm.forward(self,input)
torch.nn.modules.normalization.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules.normalization.CrossMapLRN2d.__init__(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules.normalization.CrossMapLRN2d.extra_repr(self)
torch.nn.modules.normalization.CrossMapLRN2d.forward(self,input)
torch.nn.modules.normalization.GroupNorm(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.modules.normalization.GroupNorm.__init__(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.modules.normalization.GroupNorm.extra_repr(self)
torch.nn.modules.normalization.GroupNorm.forward(self,input)
torch.nn.modules.normalization.GroupNorm.reset_parameters(self)
torch.nn.modules.normalization.LayerNorm(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.modules.normalization.LayerNorm.__init__(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.modules.normalization.LayerNorm.extra_repr(self)
torch.nn.modules.normalization.LayerNorm.forward(self,input)
torch.nn.modules.normalization.LayerNorm.reset_parameters(self)
torch.nn.modules.normalization.LocalResponseNorm(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.modules.normalization.LocalResponseNorm.__init__(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.modules.normalization.LocalResponseNorm.extra_repr(self)
torch.nn.modules.normalization.LocalResponseNorm.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/linear.py----------------------------------------
A:torch.nn.modules.linear.self.weight->Parameter(torch.Tensor(out_features, in1_features, in2_features))
A:torch.nn.modules.linear.self.bias->Parameter(torch.Tensor(out_features))
A:torch.nn.modules.linear.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
torch.nn.Bilinear(self,in1_features,in2_features,out_features,bias=True)
torch.nn.Bilinear.extra_repr(self)
torch.nn.Bilinear.forward(self,input1,input2)
torch.nn.Bilinear.reset_parameters(self)
torch.nn.Linear(self,in_features,out_features,bias=True)
torch.nn.Linear.extra_repr(self)
torch.nn.Linear.forward(self,input)
torch.nn.Linear.reset_parameters(self)
torch.nn.modules.linear.Bilinear(self,in1_features,in2_features,out_features,bias=True)
torch.nn.modules.linear.Bilinear.__init__(self,in1_features,in2_features,out_features,bias=True)
torch.nn.modules.linear.Bilinear.extra_repr(self)
torch.nn.modules.linear.Bilinear.forward(self,input1,input2)
torch.nn.modules.linear.Bilinear.reset_parameters(self)
torch.nn.modules.linear.Linear(self,in_features,out_features,bias=True)
torch.nn.modules.linear.Linear.__init__(self,in_features,out_features,bias=True)
torch.nn.modules.linear.Linear.extra_repr(self)
torch.nn.modules.linear.Linear.forward(self,input)
torch.nn.modules.linear.Linear.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/conv.py----------------------------------------
A:torch.nn.modules.conv.self.weight->Parameter(torch.Tensor(out_channels, in_channels // groups, *kernel_size))
A:torch.nn.modules.conv.self.bias->Parameter(torch.Tensor(out_channels))
A:torch.nn.modules.conv.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
A:torch.nn.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.modules.conv.stride->_triple(stride)
A:torch.nn.modules.conv.padding->_triple(padding)
A:torch.nn.modules.conv.dilation->_triple(dilation)
A:torch.nn.modules.conv.output_padding->self._output_padding(input, output_size, self.stride, self.padding, self.kernel_size)
A:torch.nn.modules.conv.func->self._backend.ConvNd(self.stride, self.padding, self.dilation, self.transposed, output_padding, self.groups)
A:torch.nn.modules.conv.ret->_single(self.output_padding)
A:torch.nn.modules.conv.output_size->torch.jit._unwrap_optional(output_size)
A:torch.nn.modules.conv.min_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.max_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.res->torch.jit.annotate(List[int], [])
torch.nn.Conv1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.Conv1d.forward(self,input)
torch.nn.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.Conv2d.forward(self,input)
torch.nn.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.Conv3d.forward(self,input)
torch.nn.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.ConvTranspose1d.forward(self,input,output_size=None)
torch.nn.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.ConvTranspose2d.forward(self,input,output_size=None)
torch.nn.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.ConvTranspose3d.forward(self,input,output_size=None)
torch.nn.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias)
torch.nn.conv._ConvNd.extra_repr(self)
torch.nn.conv._ConvNd.reset_parameters(self)
torch.nn.conv._ConvTransposeMixin(object)
torch.nn.conv._ConvTransposeMixin._output_padding(self,input,output_size,stride,padding,kernel_size)
torch.nn.conv._ConvTransposeMixin.forward(self,input,output_size=None)
torch.nn.modules.conv.Conv1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.modules.conv.Conv1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.modules.conv.Conv1d.forward(self,input)
torch.nn.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.modules.conv.Conv2d.forward(self,input)
torch.nn.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True)
torch.nn.modules.conv.Conv3d.forward(self,input)
torch.nn.modules.conv.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.modules.conv.ConvTranspose1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.modules.conv.ConvTranspose1d.forward(self,input,output_size=None)
torch.nn.modules.conv.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.modules.conv.ConvTranspose2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.modules.conv.ConvTranspose2d.forward(self,input,output_size=None)
torch.nn.modules.conv.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.modules.conv.ConvTranspose3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1)
torch.nn.modules.conv.ConvTranspose3d.forward(self,input,output_size=None)
torch.nn.modules.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias)
torch.nn.modules.conv._ConvNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias)
torch.nn.modules.conv._ConvNd.extra_repr(self)
torch.nn.modules.conv._ConvNd.reset_parameters(self)
torch.nn.modules.conv._ConvTransposeMixin(object)
torch.nn.modules.conv._ConvTransposeMixin._output_padding(self,input,output_size,stride,padding,kernel_size)
torch.nn.modules.conv._ConvTransposeMixin.forward(self,input,output_size=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/activation.py----------------------------------------
A:torch.nn.modules.activation.self.weight->Parameter(torch.Tensor(num_parameters).fill_(init))
torch.nn.CELU(self,alpha=1.0,inplace=False)
torch.nn.CELU.extra_repr(self)
torch.nn.CELU.forward(self,input)
torch.nn.ELU(self,alpha=1.0,inplace=False)
torch.nn.ELU.extra_repr(self)
torch.nn.ELU.forward(self,input)
torch.nn.GLU(self,dim=-1)
torch.nn.GLU.extra_repr(self)
torch.nn.GLU.forward(self,input)
torch.nn.Hardshrink(self,lambd=0.5)
torch.nn.Hardshrink.extra_repr(self)
torch.nn.Hardshrink.forward(self,input)
torch.nn.Hardtanh(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.Hardtanh.extra_repr(self)
torch.nn.Hardtanh.forward(self,input)
torch.nn.LeakyReLU(self,negative_slope=0.01,inplace=False)
torch.nn.LeakyReLU.extra_repr(self)
torch.nn.LeakyReLU.forward(self,input)
torch.nn.LogSigmoid(Module)
torch.nn.LogSigmoid.forward(self,input)
torch.nn.LogSoftmax(self,dim=None)
torch.nn.LogSoftmax.__setstate__(self,state)
torch.nn.LogSoftmax.forward(self,input)
torch.nn.PReLU(self,num_parameters=1,init=0.25)
torch.nn.PReLU.extra_repr(self)
torch.nn.PReLU.forward(self,input)
torch.nn.RReLU(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.RReLU.extra_repr(self)
torch.nn.RReLU.forward(self,input)
torch.nn.ReLU(self,inplace=False)
torch.nn.ReLU.extra_repr(self)
torch.nn.ReLU6(self,inplace=False)
torch.nn.ReLU6.extra_repr(self)
torch.nn.SELU(self,inplace=False)
torch.nn.SELU.extra_repr(self)
torch.nn.SELU.forward(self,input)
torch.nn.Sigmoid(Module)
torch.nn.Sigmoid.forward(self,input)
torch.nn.Softmax(self,dim=None)
torch.nn.Softmax.__setstate__(self,state)
torch.nn.Softmax.forward(self,input)
torch.nn.Softmax2d(Module)
torch.nn.Softmax2d.forward(self,input)
torch.nn.Softmin(self,dim=None)
torch.nn.Softmin.forward(self,input)
torch.nn.Softplus(self,beta=1,threshold=20)
torch.nn.Softplus.extra_repr(self)
torch.nn.Softplus.forward(self,input)
torch.nn.Softshrink(self,lambd=0.5)
torch.nn.Softshrink.extra_repr(self)
torch.nn.Softshrink.forward(self,input)
torch.nn.Softsign(Module)
torch.nn.Softsign.forward(self,input)
torch.nn.Tanh(Module)
torch.nn.Tanh.forward(self,input)
torch.nn.Tanhshrink(Module)
torch.nn.Tanhshrink.forward(self,input)
torch.nn.Threshold(self,threshold,value,inplace=False)
torch.nn.Threshold.extra_repr(self)
torch.nn.Threshold.forward(self,input)
torch.nn.modules.activation.CELU(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.CELU.__init__(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.CELU.extra_repr(self)
torch.nn.modules.activation.CELU.forward(self,input)
torch.nn.modules.activation.ELU(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.ELU.__init__(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.ELU.extra_repr(self)
torch.nn.modules.activation.ELU.forward(self,input)
torch.nn.modules.activation.GLU(self,dim=-1)
torch.nn.modules.activation.GLU.__init__(self,dim=-1)
torch.nn.modules.activation.GLU.extra_repr(self)
torch.nn.modules.activation.GLU.forward(self,input)
torch.nn.modules.activation.Hardshrink(self,lambd=0.5)
torch.nn.modules.activation.Hardshrink.__init__(self,lambd=0.5)
torch.nn.modules.activation.Hardshrink.extra_repr(self)
torch.nn.modules.activation.Hardshrink.forward(self,input)
torch.nn.modules.activation.Hardtanh(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.modules.activation.Hardtanh.__init__(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.modules.activation.Hardtanh.extra_repr(self)
torch.nn.modules.activation.Hardtanh.forward(self,input)
torch.nn.modules.activation.LeakyReLU(self,negative_slope=0.01,inplace=False)
torch.nn.modules.activation.LeakyReLU.__init__(self,negative_slope=0.01,inplace=False)
torch.nn.modules.activation.LeakyReLU.extra_repr(self)
torch.nn.modules.activation.LeakyReLU.forward(self,input)
torch.nn.modules.activation.LogSigmoid(Module)
torch.nn.modules.activation.LogSigmoid.forward(self,input)
torch.nn.modules.activation.LogSoftmax(self,dim=None)
torch.nn.modules.activation.LogSoftmax.__init__(self,dim=None)
torch.nn.modules.activation.LogSoftmax.__setstate__(self,state)
torch.nn.modules.activation.LogSoftmax.forward(self,input)
torch.nn.modules.activation.PReLU(self,num_parameters=1,init=0.25)
torch.nn.modules.activation.PReLU.__init__(self,num_parameters=1,init=0.25)
torch.nn.modules.activation.PReLU.extra_repr(self)
torch.nn.modules.activation.PReLU.forward(self,input)
torch.nn.modules.activation.RReLU(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.modules.activation.RReLU.__init__(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.modules.activation.RReLU.extra_repr(self)
torch.nn.modules.activation.RReLU.forward(self,input)
torch.nn.modules.activation.ReLU(self,inplace=False)
torch.nn.modules.activation.ReLU.__init__(self,inplace=False)
torch.nn.modules.activation.ReLU.extra_repr(self)
torch.nn.modules.activation.ReLU6(self,inplace=False)
torch.nn.modules.activation.ReLU6.__init__(self,inplace=False)
torch.nn.modules.activation.ReLU6.extra_repr(self)
torch.nn.modules.activation.SELU(self,inplace=False)
torch.nn.modules.activation.SELU.__init__(self,inplace=False)
torch.nn.modules.activation.SELU.extra_repr(self)
torch.nn.modules.activation.SELU.forward(self,input)
torch.nn.modules.activation.Sigmoid(Module)
torch.nn.modules.activation.Sigmoid.forward(self,input)
torch.nn.modules.activation.Softmax(self,dim=None)
torch.nn.modules.activation.Softmax.__init__(self,dim=None)
torch.nn.modules.activation.Softmax.__setstate__(self,state)
torch.nn.modules.activation.Softmax.forward(self,input)
torch.nn.modules.activation.Softmax2d(Module)
torch.nn.modules.activation.Softmax2d.forward(self,input)
torch.nn.modules.activation.Softmin(self,dim=None)
torch.nn.modules.activation.Softmin.__init__(self,dim=None)
torch.nn.modules.activation.Softmin.forward(self,input)
torch.nn.modules.activation.Softplus(self,beta=1,threshold=20)
torch.nn.modules.activation.Softplus.__init__(self,beta=1,threshold=20)
torch.nn.modules.activation.Softplus.extra_repr(self)
torch.nn.modules.activation.Softplus.forward(self,input)
torch.nn.modules.activation.Softshrink(self,lambd=0.5)
torch.nn.modules.activation.Softshrink.__init__(self,lambd=0.5)
torch.nn.modules.activation.Softshrink.extra_repr(self)
torch.nn.modules.activation.Softshrink.forward(self,input)
torch.nn.modules.activation.Softsign(Module)
torch.nn.modules.activation.Softsign.forward(self,input)
torch.nn.modules.activation.Tanh(Module)
torch.nn.modules.activation.Tanh.forward(self,input)
torch.nn.modules.activation.Tanhshrink(Module)
torch.nn.modules.activation.Tanhshrink.forward(self,input)
torch.nn.modules.activation.Threshold(self,threshold,value,inplace=False)
torch.nn.modules.activation.Threshold.__init__(self,threshold,value,inplace=False)
torch.nn.modules.activation.Threshold.extra_repr(self)
torch.nn.modules.activation.Threshold.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/fold.py----------------------------------------
torch.nn.Fold(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.Fold.extra_repr(self)
torch.nn.Fold.forward(self,input)
torch.nn.Unfold(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.Unfold.extra_repr(self)
torch.nn.Unfold.forward(self,input)
torch.nn.modules.fold.Fold(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Fold.__init__(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Fold.extra_repr(self)
torch.nn.modules.fold.Fold.forward(self,input)
torch.nn.modules.fold.Unfold(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Unfold.__init__(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Unfold.extra_repr(self)
torch.nn.modules.fold.Unfold.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/pixelshuffle.py----------------------------------------
torch.nn.PixelShuffle(self,upscale_factor)
torch.nn.PixelShuffle.extra_repr(self)
torch.nn.PixelShuffle.forward(self,input)
torch.nn.modules.pixelshuffle.PixelShuffle(self,upscale_factor)
torch.nn.modules.pixelshuffle.PixelShuffle.__init__(self,upscale_factor)
torch.nn.modules.pixelshuffle.PixelShuffle.extra_repr(self)
torch.nn.modules.pixelshuffle.PixelShuffle.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/nn/modules/upsampling.py----------------------------------------
torch.nn.Upsample(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.Upsample.extra_repr(self)
torch.nn.Upsample.forward(self,input)
torch.nn.UpsamplingBilinear2d(self,size=None,scale_factor=None)
torch.nn.UpsamplingNearest2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.Upsample(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.modules.upsampling.Upsample.__init__(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.modules.upsampling.Upsample.extra_repr(self)
torch.nn.modules.upsampling.Upsample.forward(self,input)
torch.nn.modules.upsampling.UpsamplingBilinear2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingBilinear2d.__init__(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingNearest2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingNearest2d.__init__(self,size=None,scale_factor=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/multiprocessing/__init__.py----------------------------------------
torch.multiprocessing.__init__.get_all_sharing_strategies()
torch.multiprocessing.__init__.get_sharing_strategy()
torch.multiprocessing.__init__.set_sharing_strategy(new_strategy)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/multiprocessing/queue.py----------------------------------------
A:torch.multiprocessing.queue.buf->self.recv_bytes()
A:torch.multiprocessing.queue.self._reader->ConnectionWrapper(self._reader)
A:torch.multiprocessing.queue.self._writer->ConnectionWrapper(self._writer)
torch.multiprocessing.Queue(self,*args,**kwargs)
torch.multiprocessing.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.SimpleQueue._make_methods(self)
torch.multiprocessing.queue.ConnectionWrapper(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.__getattr__(self,name)
torch.multiprocessing.queue.ConnectionWrapper.__init__(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.recv(self)
torch.multiprocessing.queue.ConnectionWrapper.send(self,obj)
torch.multiprocessing.queue.Queue(self,*args,**kwargs)
torch.multiprocessing.queue.Queue.__init__(self,*args,**kwargs)
torch.multiprocessing.queue.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.queue.SimpleQueue._make_methods(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/multiprocessing/spawn.py----------------------------------------
A:torch.multiprocessing.spawn.ready->multiprocessing.connection.wait(self.sentinels.keys(), timeout=timeout)
A:torch.multiprocessing.spawn.index->self.sentinels.pop(sentinel)
A:torch.multiprocessing.spawn.original_trace->self.error_queues[error_index].get()
A:torch.multiprocessing.spawn.mp->multiprocessing.get_context('spawn')
A:torch.multiprocessing.spawn.error_queue->multiprocessing.get_context('spawn').SimpleQueue()
A:torch.multiprocessing.spawn.process->multiprocessing.get_context('spawn').Process(target=_wrap, args=(fn, i, args, error_queue), daemon=daemon)
A:torch.multiprocessing.spawn.spawn_context->SpawnContext(processes, error_queues)
torch.multiprocessing.SpawnContext(self,processes,error_queues)
torch.multiprocessing.SpawnContext.join(self,timeout=None)
torch.multiprocessing.SpawnContext.pids(self)
torch.multiprocessing.spawn(fn,args=(),nprocs=1,join=True,daemon=False)
torch.multiprocessing.spawn.SpawnContext(self,processes,error_queues)
torch.multiprocessing.spawn.SpawnContext.__init__(self,processes,error_queues)
torch.multiprocessing.spawn.SpawnContext.join(self,timeout=None)
torch.multiprocessing.spawn.SpawnContext.pids(self)
torch.multiprocessing.spawn._python_version_check()
torch.multiprocessing.spawn._wrap(fn,i,args,error_queue)
torch.multiprocessing.spawn.spawn(fn,args=(),nprocs=1,join=True,daemon=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/multiprocessing/pool.py----------------------------------------
A:torch.multiprocessing.pool.self._inqueue->SimpleQueue()
A:torch.multiprocessing.pool.self._outqueue->SimpleQueue()
A:torch.multiprocessing.pool.w->self.Process(target=clean_worker, args=args)
A:torch.multiprocessing.pool.w.name->self.Process(target=clean_worker, args=args).name.replace('Process', 'PoolWorker')
torch.multiprocessing.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.Pool._repopulate_pool(self)
torch.multiprocessing.Pool._setup_queues(self)
torch.multiprocessing.pool.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.pool.Pool._repopulate_pool(self)
torch.multiprocessing.pool.Pool._setup_queues(self)
torch.multiprocessing.pool.clean_worker(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/multiprocessing/reductions.py----------------------------------------
A:torch.multiprocessing.reductions.self.cdata->cls._new_shared_filename(manager, handle, size)._weak_ref()
A:torch.multiprocessing.reductions.self.lock->threading.Lock()
A:torch.multiprocessing.reductions.self.limit->max(128, live * 2)
A:torch.multiprocessing.reductions.shared_cache->SharedCache()
A:torch.multiprocessing.reductions.t->torch.nn.parameter.Parameter(t)
A:torch.multiprocessing.reductions.storage->cls._new_shared_filename(manager, handle, size)
A:torch.multiprocessing.reductions.shared_cache[storage_handle, storage_offset_bytes]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.(device, handle, storage_size_bytes, storage_offset_bytes)->cls._new_shared_filename(manager, handle, size)._share_cuda_()
A:torch.multiprocessing.reductions.tensor_offset->tensor.storage_offset()
A:torch.multiprocessing.reductions.shared_cache[handle]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.stat->os.fstat(fd)
A:torch.multiprocessing.reductions.storage_ref->SharedCache().get(key)
A:torch.multiprocessing.reductions.fd->multiprocessing.reduction.DupFd(fd).detach()
A:torch.multiprocessing.reductions.shared_cache[fd_id(fd)]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.metadata->cls._new_shared_filename(manager, handle, size)._share_filename_()
A:torch.multiprocessing.reductions.(fd, size)->cls._new_shared_filename(manager, handle, size)._share_fd_()
A:torch.multiprocessing.reductions.df->multiprocessing.reduction.DupFd(fd)
A:torch.multiprocessing.reductions.cache_key->fd_id(fd)
A:torch.multiprocessing.reductions.shared_cache[cache_key]->StorageWeakRef(storage)
torch.multiprocessing.init_reductions()
torch.multiprocessing.reductions.SharedCache(self)
torch.multiprocessing.reductions.SharedCache.__init__(self)
torch.multiprocessing.reductions.SharedCache.__setitem__(self,key,storage_ref)
torch.multiprocessing.reductions.SharedCache.free_dead_references(self)
torch.multiprocessing.reductions.StorageWeakRef(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.__del__(self)
torch.multiprocessing.reductions.StorageWeakRef.__init__(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.expired(self)
torch.multiprocessing.reductions.fd_id(fd)
torch.multiprocessing.reductions.init_reductions()
torch.multiprocessing.reductions.rebuild_cuda_tensor(tensor_cls,tensor_size,tensor_stride,tensor_offset,storage_cls,storage_device,storage_handle,storage_size_bytes,storage_offset_bytes,requires_grad)
torch.multiprocessing.reductions.rebuild_event(handle)
torch.multiprocessing.reductions.rebuild_storage_empty(cls)
torch.multiprocessing.reductions.rebuild_storage_fd(cls,df,size)
torch.multiprocessing.reductions.rebuild_storage_filename(cls,manager,handle,size)
torch.multiprocessing.reductions.rebuild_tensor(cls,storage,metadata)
torch.multiprocessing.reductions.reduce_event(event)
torch.multiprocessing.reductions.reduce_storage(storage)
torch.multiprocessing.reductions.reduce_tensor(tensor)
torch.multiprocessing.reductions.storage_from_cache(cls,key)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/for_onnx/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/checkpoint.py----------------------------------------
A:torch.utils.checkpoint.x->inp.detach()
A:torch.utils.checkpoint.ctx.fwd_cpu_rng_state->torch.get_rng_state()
A:torch.utils.checkpoint.ctx.fwd_cuda_rng_state->torch.cuda.get_rng_state()
A:torch.utils.checkpoint.outputs->ctx.run_function(*detached_inputs)
A:torch.utils.checkpoint.detached_inputs->detach_variable(inputs)
A:torch.utils.checkpoint.inputs->checkpoint(run_function(start, end, functions), *inputs)
A:torch.utils.checkpoint.functions->list(functions.children())
torch.utils.checkpoint.CheckpointFunction(torch.autograd.Function)
torch.utils.checkpoint.CheckpointFunction.backward(ctx,*args)
torch.utils.checkpoint.CheckpointFunction.forward(ctx,run_function,*args)
torch.utils.checkpoint.check_backward_validity(inputs)
torch.utils.checkpoint.checkpoint(function,*args)
torch.utils.checkpoint.checkpoint_sequential(functions,segments,*inputs)
torch.utils.checkpoint.detach_variable(inputs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/file_baton.py----------------------------------------
A:torch.utils.file_baton.self.fd->os.open(self.lock_file_path, os.O_CREAT | os.O_EXCL)
torch.utils.file_baton.FileBaton(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.__init__(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.release(self)
torch.utils.file_baton.FileBaton.try_acquire(self)
torch.utils.file_baton.FileBaton.wait(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/collect_env.py----------------------------------------
A:torch.utils.collect_env.SystemEnv->namedtuple('SystemEnv', ['torch_version', 'is_debug_build', 'cuda_compiled_version', 'gcc_version', 'cmake_version', 'os', 'python_version', 'is_cuda_available', 'cuda_runtime_version', 'nvidia_driver_version', 'nvidia_gpu_models', 'cudnn_version', 'pip_version', 'pip_packages', 'conda_packages'])
A:torch.utils.collect_env.p->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
A:torch.utils.collect_env.(output, err)->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()
A:torch.utils.collect_env.output->get_pretty_env_info()
A:torch.utils.collect_env.err->err.decode('ascii').decode('ascii')
A:torch.utils.collect_env.(rc, out, _)->run_lambda(cudnn_cmd)
A:torch.utils.collect_env.match->re.search(regex, out)
A:torch.utils.collect_env.out->run_and_read_all(run_lambda, 'conda list | ' + grep_cmd)
A:torch.utils.collect_env.comment_regex->re.compile('^#.*\\n')
A:torch.utils.collect_env.smi->get_nvidia_smi()
A:torch.utils.collect_env.uuid_regex->re.compile(' \\(UUID: .+?\\)')
A:torch.utils.collect_env.result->'\n'.join(sorted(out.split('\n')))
A:torch.utils.collect_env.platform->get_platform()
A:torch.utils.collect_env.version->get_mac_version(run_lambda)
A:torch.utils.collect_env.desc->check_release_file(run_lambda)
A:torch.utils.collect_env.out2->run_with_pip('pip')
A:torch.utils.collect_env.out3->run_with_pip('pip3')
A:torch.utils.collect_env.num_pips->len([x for x in [out2, out3] if x is not None])
A:torch.utils.collect_env.(pip_version, pip_list_output)->get_pip_packages(run_lambda)
A:torch.utils.collect_env.env_info_fmt->'\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\n\nOS: {os}\nGCC version: {gcc_version}\nCMake version: {cmake_version}\n\nPython version: {python_version}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n'.strip()
A:torch.utils.collect_env.lines->text.split('\n')
A:torch.utils.collect_env.mutable_dict->replace_nones(mutable_dict)
A:torch.utils.collect_env.mutable_dict['nvidia_gpu_models']->maybe_start_on_next_line(envinfo.nvidia_gpu_models)
A:torch.utils.collect_env.all_dynamic_cuda_fields_missing->all((mutable_dict[field] is None for field in dynamic_cuda_fields))
A:torch.utils.collect_env.mutable_dict['pip_packages']->prepend(mutable_dict['pip_packages'], '[{}] '.format(envinfo.pip_version))
A:torch.utils.collect_env.mutable_dict['conda_packages']->prepend(mutable_dict['conda_packages'], '[conda] ')
torch.utils.collect_env.check_release_file(run_lambda)
torch.utils.collect_env.get_cmake_version(run_lambda)
torch.utils.collect_env.get_conda_packages(run_lambda)
torch.utils.collect_env.get_cudnn_version(run_lambda)
torch.utils.collect_env.get_env_info()
torch.utils.collect_env.get_gcc_version(run_lambda)
torch.utils.collect_env.get_gpu_info(run_lambda)
torch.utils.collect_env.get_lsb_version(run_lambda)
torch.utils.collect_env.get_mac_version(run_lambda)
torch.utils.collect_env.get_nvidia_driver_version(run_lambda)
torch.utils.collect_env.get_nvidia_smi()
torch.utils.collect_env.get_os(run_lambda)
torch.utils.collect_env.get_pip_packages(run_lambda)
torch.utils.collect_env.get_platform()
torch.utils.collect_env.get_pretty_env_info()
torch.utils.collect_env.get_running_cuda_version(run_lambda)
torch.utils.collect_env.get_windows_version(run_lambda)
torch.utils.collect_env.main()
torch.utils.collect_env.pretty_str(envinfo)
torch.utils.collect_env.run(command)
torch.utils.collect_env.run_and_parse_first_match(run_lambda,command,regex)
torch.utils.collect_env.run_and_read_all(run_lambda,command)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/dlpack.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/_cpp_extension_versioner.py----------------------------------------
A:torch.utils._cpp_extension_versioner.Entry->collections.namedtuple('Entry', 'version, hash')
A:torch.utils._cpp_extension_versioner.hash_value->update_hash(hash_value, with_cuda)
A:torch.utils._cpp_extension_versioner.entry->self.entries.get(name)
A:torch.utils._cpp_extension_versioner.self.entries[name]entry->Entry(entry.version + 1, hash_value)
torch.utils._cpp_extension_versioner.ExtensionVersioner(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.__init__(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.bump_version_if_changed(self,name,source_files,build_arguments,build_directory,with_cuda)
torch.utils._cpp_extension_versioner.ExtensionVersioner.get_version(self,name)
torch.utils._cpp_extension_versioner.hash_build_arguments(hash_value,build_arguments)
torch.utils._cpp_extension_versioner.hash_source_files(hash_value,source_files)
torch.utils._cpp_extension_versioner.update_hash(seed,value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/cpp_extension.py----------------------------------------
A:torch.utils.cpp_extension.cuda_homes->glob.glob('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')
A:torch.utils.cpp_extension.nvcc->_join_cuda_home('bin', 'nvcc')
A:torch.utils.cpp_extension.cuda_home->os.path.dirname(os.path.dirname(nvcc))
A:torch.utils.cpp_extension.CUDA_HOME->_find_cuda_home()
A:torch.utils.cpp_extension.BUILT_FROM_SOURCE_VERSION_PATTERN->re.compile('\\d+\\.\\d+\\.\\d+\\w+\\+\\w+')
A:torch.utils.cpp_extension.JIT_EXTENSION_VERSIONER->ExtensionVersioner()
A:torch.utils.cpp_extension.which->subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.compiler_path->os.path.realpath(which.decode().strip())
A:torch.utils.cpp_extension.version->ExtensionVersioner().bump_version_if_changed(name, sources, build_arguments=[extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths], build_directory=build_directory, with_cuda=with_cuda)
A:torch.utils.cpp_extension.compiler_info->subprocess.check_output(compiler, stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.match->re.search('(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip())
A:torch.utils.cpp_extension.(_, error, _)->sys.exc_info()
A:torch.utils.cpp_extension.compiler->os.environ.get('CXX', 'c++')
A:torch.utils.cpp_extension.kwargs->kwargs.copy().copy()
A:torch.utils.cpp_extension.self.no_python_abi_suffix->kwargs.copy().copy().get('no_python_abi_suffix', False)
A:torch.utils.cpp_extension.cflags->_nt_quote_args(cflags)
A:torch.utils.cpp_extension.self.cflags->copy.deepcopy(extra_postargs)
A:torch.utils.cpp_extension.src_regex->re.compile('/T(p|c)(.*)')
A:torch.utils.cpp_extension.obj_regex->re.compile('/Fo(.*)')
A:torch.utils.cpp_extension.include_regex->re.compile('((\\-|\\/)I.*)')
A:torch.utils.cpp_extension.ext_filename->'.'.join(without_abi)
A:torch.utils.cpp_extension.ext_filename_parts->'.'.join(without_abi).split('.')
A:torch.utils.cpp_extension.extension.extra_compile_args->copy.copy(extension.extra_compile_args)
A:torch.utils.cpp_extension.names->extension.name.split('.')
A:torch.utils.cpp_extension.define->'-DTORCH_EXTENSION_NAME={}'.format(name)
A:torch.utils.cpp_extension.include_dirs->kwargs.copy().copy().get('include_dirs', [])
A:torch.utils.cpp_extension.library_dirs->kwargs.copy().copy().get('library_dirs', [])
A:torch.utils.cpp_extension.libraries->kwargs.copy().copy().get('libraries', [])
A:torch.utils.cpp_extension.here->os.path.abspath(__file__)
A:torch.utils.cpp_extension.torch_path->os.path.dirname(os.path.dirname(here))
A:torch.utils.cpp_extension.lib_include->os.path.join(torch_path, 'lib', 'include')
A:torch.utils.cpp_extension.lib_path->os.path.join(torch_path, 'lib')
A:torch.utils.cpp_extension.functions->dict(((f, f) for f in functions))
A:torch.utils.cpp_extension.cpp_source_path->os.path.join(build_directory, 'main.cpp')
A:torch.utils.cpp_extension.cuda_source_path->os.path.join(build_directory, 'cuda.cu')
A:torch.utils.cpp_extension.old_version->ExtensionVersioner().get_version(name)
A:torch.utils.cpp_extension.name->'{}_v{}'.format(name, version)
A:torch.utils.cpp_extension.baton->FileBaton(os.path.join(build_directory, 'lock'))
A:torch.utils.cpp_extension.with_cuda->any(map(_is_cuda_file, sources))
A:torch.utils.cpp_extension.extra_ldflags->_prepare_ldflags(extra_ldflags or [], with_cuda, verbose)
A:torch.utils.cpp_extension.build_file_path->os.path.join(build_directory, 'build.ninja')
A:torch.utils.cpp_extension.python_path->os.path.dirname(sys.executable)
A:torch.utils.cpp_extension.python_lib_path->os.path.join(python_path, 'libs')
A:torch.utils.cpp_extension.root_extensions_directory->get_default_build_root()
A:torch.utils.cpp_extension.build_directory->os.path.join(root_extensions_directory, name)
A:torch.utils.cpp_extension.message->"Error building extension '{}'".format(name)
A:torch.utils.cpp_extension.(file, path, description)->imp.find_module(module_name, [path])
A:torch.utils.cpp_extension.system_includes->include_paths(with_cuda)
A:torch.utils.cpp_extension.cuda_flags->_nt_quote_args(cuda_flags)
A:torch.utils.cpp_extension.ldflags->_nt_quote_args(ldflags)
A:torch.utils.cpp_extension.cl_paths->subprocess.check_output(['where', 'cl']).decode().split('\r\n')
A:torch.utils.cpp_extension.cl_path->os.path.dirname(cl_paths[0]).replace(':', '$:')
A:torch.utils.cpp_extension.target->'{}.o'.format(file_name)
A:torch.utils.cpp_extension.source_file->source_file.replace(' ', '$ ').replace(' ', '$ ')
A:torch.utils.cpp_extension.library_target->'{}.{}'.format(name, ext)
A:torch.utils.cpp_extension.lines->'\n'.join(block)
torch.utils.cpp_extension.BuildExtension(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension.__init__(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension._add_compile_flag(self,extension,flag)
torch.utils.cpp_extension.BuildExtension._add_gnu_abi_flag_if_binary(self,extension)
torch.utils.cpp_extension.BuildExtension._check_abi(self)
torch.utils.cpp_extension.BuildExtension._define_torch_extension_name(self,extension)
torch.utils.cpp_extension.BuildExtension.build_extensions(self)
torch.utils.cpp_extension.BuildExtension.get_ext_filename(self,ext_name)
torch.utils.cpp_extension.BuildExtension.with_options(cls,**options)
torch.utils.cpp_extension.CUDAExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension.CppExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension._accepted_compilers_for_platform()
torch.utils.cpp_extension._build_extension_module(name,build_directory,verbose)
torch.utils.cpp_extension._find_cuda_home()
torch.utils.cpp_extension._get_build_directory(name,verbose)
torch.utils.cpp_extension._import_module_from_library(module_name,path,is_python_module)
torch.utils.cpp_extension._is_binary_build()
torch.utils.cpp_extension._is_cuda_file(path)
torch.utils.cpp_extension._jit_compile(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory,verbose,with_cuda,is_python_module)
torch.utils.cpp_extension._join_cuda_home(*paths)
torch.utils.cpp_extension._prepare_ldflags(extra_ldflags,with_cuda,verbose)
torch.utils.cpp_extension._write_ninja_file(path,name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,with_cuda)
torch.utils.cpp_extension._write_ninja_file_and_build(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory,verbose,with_cuda)
torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)
torch.utils.cpp_extension.check_compiler_ok_for_platform(compiler)
torch.utils.cpp_extension.get_default_build_root()
torch.utils.cpp_extension.include_paths(cuda=False)
torch.utils.cpp_extension.library_paths(cuda=False)
torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)
torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)
torch.utils.cpp_extension.verify_ninja_availability()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/model_zoo.py----------------------------------------
A:torch.utils.model_zoo.HASH_REGEX->re.compile('-([a-f0-9]*)\\.')
A:torch.utils.model_zoo.torch_home->os.path.expanduser(os.getenv('TORCH_HOME', '~/.torch'))
A:torch.utils.model_zoo.model_dir->os.getenv('TORCH_MODEL_ZOO', os.path.join(torch_home, 'models'))
A:torch.utils.model_zoo.parts->urlparse(url)
A:torch.utils.model_zoo.filename->os.path.basename(parts.path)
A:torch.utils.model_zoo.cached_file->os.path.join(model_dir, filename)
A:torch.utils.model_zoo.hash_prefix->re.compile('-([a-f0-9]*)\\.').search(filename).group(1)
A:torch.utils.model_zoo.u->urlopen(url)
A:torch.utils.model_zoo.file_size->int(content_length[0])
A:torch.utils.model_zoo.meta->urlopen(url).info()
A:torch.utils.model_zoo.content_length->urlopen(url).info().get_all('Content-Length')
A:torch.utils.model_zoo.f->tempfile.NamedTemporaryFile(delete=False)
A:torch.utils.model_zoo.sha256->hashlib.sha256()
A:torch.utils.model_zoo.buffer->urlopen(url).read(8192)
A:torch.utils.model_zoo.digest->hashlib.sha256().hexdigest()
torch.utils.model_zoo._download_url_to_file(url,dst,hash_prefix,progress)
torch.utils.model_zoo.load_url(url,model_dir=None,map_location=None,progress=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/hooks.py----------------------------------------
A:torch.utils.hooks.self.hooks_dict_ref->weakref.ref(state[0])
A:torch.utils.hooks.hooks_dict->self.hooks_dict_ref()
A:torch.utils.hooks.RemovableHandle.next_id->max(RemovableHandle.next_id, self.id + 1)
torch.utils.hooks.RemovableHandle(self,hooks_dict)
torch.utils.hooks.RemovableHandle.__enter__(self)
torch.utils.hooks.RemovableHandle.__exit__(self,type,value,tb)
torch.utils.hooks.RemovableHandle.__getstate__(self)
torch.utils.hooks.RemovableHandle.__init__(self,hooks_dict)
torch.utils.hooks.RemovableHandle.__setstate__(self,state)
torch.utils.hooks.RemovableHandle.remove(self)
torch.utils.hooks.unserializable_hook(f)
torch.utils.hooks.warn_if_has_hooks(tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/ffi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/backcompat/__init__.py----------------------------------------
A:torch.utils.backcompat.__init__.enabled->property(get_enabled, set_enabled)
A:torch.utils.backcompat.__init__.broadcast_warning->Warning(_set_backcompat_broadcast_warn, _get_backcompat_broadcast_warn)
A:torch.utils.backcompat.__init__.keepdim_warning->Warning(_set_backcompat_keepdim_warn, _get_backcompat_keepdim_warn)
torch.utils.backcompat.__init__.Warning(self,setter,getter)
torch.utils.backcompat.__init__.Warning.__init__(self,setter,getter)
torch.utils.backcompat.__init__.Warning.get_enabled(self)
torch.utils.backcompat.__init__.Warning.set_enabled(self,value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py----------------------------------------
A:torch.utils.bottleneck.__main__.env_summary->run_env_analysis()
A:torch.utils.bottleneck.__main__.info->get_env_info()
A:torch.utils.bottleneck.__main__.prof->cProfile.Profile()
A:torch.utils.bottleneck.__main__.cprof_summary->'\n--------------------------------------------------------------------------------\n  cProfile output\n--------------------------------------------------------------------------------\n'.strip()
A:torch.utils.bottleneck.__main__.cprofile_stats->pstats.Stats(prof).sort_stats(sortby)
A:torch.utils.bottleneck.__main__.autograd_prof_summary->'\n--------------------------------------------------------------------------------\n  autograd profiler output ({mode} mode)\n--------------------------------------------------------------------------------\n        {description}\n{cuda_warning}\n{output}\n'.strip()
A:torch.utils.bottleneck.__main__.sorted_events->sorted(prof.function_events, key=lambda x: getattr(x, sortby), reverse=True)
A:torch.utils.bottleneck.__main__.descript->"\n`bottleneck` is a tool that can be used as an initial step for debugging\nbottlenecks in your program.\n\nIt summarizes runs of your script with the Python profiler and PyTorch's\nautograd profiler. Because your script will be profiled, please ensure that it\nexits in a finite amount of time.\n\nFor more complicated uses of the profilers, please see\nhttps://docs.python.org/3/library/profile.html and\nhttps://pytorch.org/docs/master/autograd.html#profiler for more information.\n".strip()
A:torch.utils.bottleneck.__main__.parser->argparse.ArgumentParser(description=descript)
A:torch.utils.bottleneck.__main__.args->parse_args()
A:torch.utils.bottleneck.__main__.code->compile(stream.read(), scriptfile, 'exec')
A:torch.utils.bottleneck.__main__.cprofile_prof->run_cprofile(code, globs)
A:torch.utils.bottleneck.__main__.(autograd_prof_cpu, autograd_prof_cuda)->run_autograd_prof(code, globs)
A:torch.utils.bottleneck.__main__.cuda_prof_exec_time->cpu_time_total(autograd_prof_cuda)
A:torch.utils.bottleneck.__main__.cpu_prof_exec_time->cpu_time_total(autograd_prof_cpu)
torch.utils.bottleneck.__main__.compiled_with_cuda(sysinfo)
torch.utils.bottleneck.__main__.cpu_time_total(autograd_prof)
torch.utils.bottleneck.__main__.main()
torch.utils.bottleneck.__main__.parse_args()
torch.utils.bottleneck.__main__.print_autograd_prof_summary(prof,mode,sortby='cpu_time',topk=15)
torch.utils.bottleneck.__main__.print_cprofile_summary(prof,sortby='tottime',topk=15)
torch.utils.bottleneck.__main__.redirect_argv(new_argv)
torch.utils.bottleneck.__main__.run_autograd_prof(code,globs)
torch.utils.bottleneck.__main__.run_cprofile(code,globs,launch_blocking=False)
torch.utils.bottleneck.__main__.run_env_analysis()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/bottleneck/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/data/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/data/sampler.py----------------------------------------
A:torch.utils.data.sampler.self.num_samples->len(self.data_source)
A:torch.utils.data.sampler.n->len(self.data_source)
A:torch.utils.data.sampler.self.weights->torch.tensor(weights, dtype=torch.double)
torch.utils.data.BatchSampler(self,sampler,batch_size,drop_last)
torch.utils.data.BatchSampler.__iter__(self)
torch.utils.data.BatchSampler.__len__(self)
torch.utils.data.RandomSampler(self,data_source,replacement=False,num_samples=None)
torch.utils.data.RandomSampler.__iter__(self)
torch.utils.data.RandomSampler.__len__(self)
torch.utils.data.Sampler(self,data_source)
torch.utils.data.Sampler.__iter__(self)
torch.utils.data.Sampler.__len__(self)
torch.utils.data.SequentialSampler(self,data_source)
torch.utils.data.SequentialSampler.__iter__(self)
torch.utils.data.SequentialSampler.__len__(self)
torch.utils.data.SubsetRandomSampler(self,indices)
torch.utils.data.SubsetRandomSampler.__iter__(self)
torch.utils.data.SubsetRandomSampler.__len__(self)
torch.utils.data.WeightedRandomSampler(self,weights,num_samples,replacement=True)
torch.utils.data.WeightedRandomSampler.__iter__(self)
torch.utils.data.WeightedRandomSampler.__len__(self)
torch.utils.data.sampler.BatchSampler(self,sampler,batch_size,drop_last)
torch.utils.data.sampler.BatchSampler.__init__(self,sampler,batch_size,drop_last)
torch.utils.data.sampler.BatchSampler.__iter__(self)
torch.utils.data.sampler.BatchSampler.__len__(self)
torch.utils.data.sampler.RandomSampler(self,data_source,replacement=False,num_samples=None)
torch.utils.data.sampler.RandomSampler.__init__(self,data_source,replacement=False,num_samples=None)
torch.utils.data.sampler.RandomSampler.__iter__(self)
torch.utils.data.sampler.RandomSampler.__len__(self)
torch.utils.data.sampler.Sampler(self,data_source)
torch.utils.data.sampler.Sampler.__init__(self,data_source)
torch.utils.data.sampler.Sampler.__iter__(self)
torch.utils.data.sampler.Sampler.__len__(self)
torch.utils.data.sampler.SequentialSampler(self,data_source)
torch.utils.data.sampler.SequentialSampler.__init__(self,data_source)
torch.utils.data.sampler.SequentialSampler.__iter__(self)
torch.utils.data.sampler.SequentialSampler.__len__(self)
torch.utils.data.sampler.SubsetRandomSampler(self,indices)
torch.utils.data.sampler.SubsetRandomSampler.__init__(self,indices)
torch.utils.data.sampler.SubsetRandomSampler.__iter__(self)
torch.utils.data.sampler.SubsetRandomSampler.__len__(self)
torch.utils.data.sampler.WeightedRandomSampler(self,weights,num_samples,replacement=True)
torch.utils.data.sampler.WeightedRandomSampler.__init__(self,weights,num_samples,replacement=True)
torch.utils.data.sampler.WeightedRandomSampler.__iter__(self)
torch.utils.data.sampler.WeightedRandomSampler.__len__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/data/dataset.py----------------------------------------
A:torch.utils.data.dataset.l->len(e)
A:torch.utils.data.dataset.self.datasets->list(datasets)
A:torch.utils.data.dataset.self.cumulative_sizes->self.cumsum(self.datasets)
A:torch.utils.data.dataset.dataset_idx->bisect.bisect_right(self.cumulative_sizes, idx)
A:torch.utils.data.dataset.indices->randperm(sum(lengths))
torch.utils.data.ConcatDataset(self,datasets)
torch.utils.data.ConcatDataset.__getitem__(self,idx)
torch.utils.data.ConcatDataset.__len__(self)
torch.utils.data.ConcatDataset.cummulative_sizes(self)
torch.utils.data.ConcatDataset.cumsum(sequence)
torch.utils.data.Dataset(object)
torch.utils.data.Dataset.__add__(self,other)
torch.utils.data.Dataset.__getitem__(self,index)
torch.utils.data.Dataset.__len__(self)
torch.utils.data.Subset(self,dataset,indices)
torch.utils.data.Subset.__getitem__(self,idx)
torch.utils.data.Subset.__len__(self)
torch.utils.data.TensorDataset(self,*tensors)
torch.utils.data.TensorDataset.__getitem__(self,index)
torch.utils.data.TensorDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset(self,datasets)
torch.utils.data.dataset.ConcatDataset.__getitem__(self,idx)
torch.utils.data.dataset.ConcatDataset.__init__(self,datasets)
torch.utils.data.dataset.ConcatDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset.cummulative_sizes(self)
torch.utils.data.dataset.ConcatDataset.cumsum(sequence)
torch.utils.data.dataset.Dataset(object)
torch.utils.data.dataset.Dataset.__add__(self,other)
torch.utils.data.dataset.Dataset.__getitem__(self,index)
torch.utils.data.dataset.Dataset.__len__(self)
torch.utils.data.dataset.Subset(self,dataset,indices)
torch.utils.data.dataset.Subset.__getitem__(self,idx)
torch.utils.data.dataset.Subset.__init__(self,dataset,indices)
torch.utils.data.dataset.Subset.__len__(self)
torch.utils.data.dataset.TensorDataset(self,*tensors)
torch.utils.data.dataset.TensorDataset.__getitem__(self,index)
torch.utils.data.dataset.TensorDataset.__init__(self,*tensors)
torch.utils.data.dataset.TensorDataset.__len__(self)
torch.utils.data.dataset.random_split(dataset,lengths)
torch.utils.data.random_split(dataset,lengths)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/data/dataloader.py----------------------------------------
A:torch.utils.data.dataloader.self.exc_msg->''.join(traceback.format_exception(*exc_info))
A:torch.utils.data.dataloader.self.manager_pid->os.getppid()
A:torch.utils.data.dataloader.self.kernel32->ctypes.WinDLL('kernel32', use_last_error=True)
A:torch.utils.data.dataloader.self.manager_handle->self.kernel32.OpenProcess(SYNCHRONIZE, 0, self.manager_pid)
A:torch.utils.data.dataloader.watchdog->ManagerWatchdog()
A:torch.utils.data.dataloader.r->in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data.dataloader.samples->collate_fn([dataset[i] for i in batch_indices])
A:torch.utils.data.dataloader.batch->self.reorder_dict.pop(self.rcvd_idx)
A:torch.utils.data.dataloader.elem_type->type(batch[0])
A:torch.utils.data.dataloader.numel->sum([x.numel() for x in batch])
A:torch.utils.data.dataloader.storage->batch[0].storage()._new_shared(numel)
A:torch.utils.data.dataloader.out->batch[0].new(storage)
A:torch.utils.data.dataloader.transposed->zip(*batch)
A:torch.utils.data.dataloader.previous_handler->signal.getsignal(signal.SIGCHLD)
A:torch.utils.data.dataloader.self.sample_iter->iter(self.batch_sampler)
A:torch.utils.data.dataloader.base_seed->torch.LongTensor(1).random_().item()
A:torch.utils.data.dataloader.self.worker_result_queue->torch.multiprocessing.Queue()
A:torch.utils.data.dataloader.self.done_event->torch.multiprocessing.Event()
A:torch.utils.data.dataloader.index_queue->torch.multiprocessing.Queue()
A:torch.utils.data.dataloader.w->torch.multiprocessing.Process(target=_worker_loop, args=(self.dataset, index_queue, self.worker_result_queue, self.done_event, self.collate_fn, base_seed + i, self.worker_init_fn, i))
A:torch.utils.data.dataloader.self.data_queue->queue.Queue()
A:torch.utils.data.dataloader.pin_memory_thread->threading.Thread(target=_pin_memory_loop, args=(self.worker_result_queue, self.data_queue, torch.cuda.current_device(), self.done_event))
A:torch.utils.data.dataloader.indices->next(self.sample_iter, None)
A:torch.utils.data.dataloader.(idx, batch)->self._get_batch()
A:torch.utils.data.dataloader.sampler->SequentialSampler(dataset)
A:torch.utils.data.dataloader.batch_sampler->BatchSampler(sampler, batch_size, drop_last)
torch.utils.data.DataLoader(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None)
torch.utils.data.DataLoader.__iter__(self)
torch.utils.data.DataLoader.__len__(self)
torch.utils.data.DataLoader.__setattr__(self,attr,val)
torch.utils.data.dataloader.DataLoader(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None)
torch.utils.data.dataloader.DataLoader.__init__(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None)
torch.utils.data.dataloader.DataLoader.__iter__(self)
torch.utils.data.dataloader.DataLoader.__len__(self)
torch.utils.data.dataloader.DataLoader.__setattr__(self,attr,val)
torch.utils.data.dataloader.ExceptionWrapper(self,exc_info)
torch.utils.data.dataloader.ExceptionWrapper.__init__(self,exc_info)
torch.utils.data.dataloader._DataLoaderIter(self,loader)
torch.utils.data.dataloader._DataLoaderIter.__del__(self)
torch.utils.data.dataloader._DataLoaderIter.__getstate__(self)
torch.utils.data.dataloader._DataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._DataLoaderIter.__iter__(self)
torch.utils.data.dataloader._DataLoaderIter.__len__(self)
torch.utils.data.dataloader._DataLoaderIter.__next__(self)
torch.utils.data.dataloader._DataLoaderIter._get_batch(self)
torch.utils.data.dataloader._DataLoaderIter._process_next_batch(self,batch)
torch.utils.data.dataloader._DataLoaderIter._put_indices(self)
torch.utils.data.dataloader._DataLoaderIter._shutdown_workers(self)
torch.utils.data.dataloader._pin_memory_loop(in_queue,out_queue,device_id,done_event)
torch.utils.data.dataloader._set_SIGCHLD_handler()
torch.utils.data.dataloader._set_python_exit_flag()
torch.utils.data.dataloader._worker_loop(dataset,index_queue,data_queue,done_event,collate_fn,seed,init_fn,worker_id)
torch.utils.data.dataloader.default_collate(batch)
torch.utils.data.dataloader.pin_memory_batch(batch)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/utils/data/distributed.py----------------------------------------
A:torch.utils.data.distributed.num_replicas->torch.distributed.get_world_size()
A:torch.utils.data.distributed.rank->torch.distributed.get_rank()
A:torch.utils.data.distributed.self.num_samples->int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
A:torch.utils.data.distributed.g->torch.Generator()
A:torch.utils.data.distributed.indices->torch.randperm(len(self.dataset), generator=g).tolist()
torch.utils.data.DistributedSampler(self,dataset,num_replicas=None,rank=None)
torch.utils.data.DistributedSampler.__iter__(self)
torch.utils.data.DistributedSampler.__len__(self)
torch.utils.data.DistributedSampler.set_epoch(self,epoch)
torch.utils.data.distributed.DistributedSampler(self,dataset,num_replicas=None,rank=None)
torch.utils.data.distributed.DistributedSampler.__init__(self,dataset,num_replicas=None,rank=None)
torch.utils.data.distributed.DistributedSampler.__iter__(self)
torch.utils.data.distributed.DistributedSampler.__len__(self)
torch.utils.data.distributed.DistributedSampler.set_epoch(self,epoch)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/contrib/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/contrib/_tensorboard_vis.py----------------------------------------
A:torch.contrib._tensorboard_vis.pb_graph->visualize(graph_executor)
A:torch.contrib._tensorboard_vis.evt->tensorflow.core.util.event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
A:torch.contrib._tensorboard_vis.input_node->visualize(graph_executor).node.add(op='input', name=name_prefix + 'input')
A:torch.contrib._tensorboard_vis.return_node->visualize(graph_executor).node.add(op='output', name=name_prefix + 'output')
A:torch.contrib._tensorboard_vis.state->state.get_debug_state().get_debug_state()
A:torch.contrib._tensorboard_vis.input_kinds->visualize(graph_executor).node.add(op='INPUT_KIND', name=subgraph_name)
A:torch.contrib._tensorboard_vis.input_kinds.attr['inputs'].s->repr(arg_spec).encode('ascii')
A:torch.contrib._tensorboard_vis.op_id_counter->defaultdict(int)
A:torch.contrib._tensorboard_vis.(op, name)->name_for(node)
A:torch.contrib._tensorboard_vis.ge->next(executors_it)
A:torch.contrib._tensorboard_vis.pb_node->visualize(graph_executor).node.add(op=op, name=name)
torch.contrib._tensorboard_vis.dump_tensorboard_summary(graph_executor,logdir)
torch.contrib._tensorboard_vis.visualize(graph,name_prefix='',pb_graph=None,executors_it=None)
torch.contrib._tensorboard_vis.visualize_graph_executor(state,name_prefix,pb_graph,inline_graph)
torch.contrib._tensorboard_vis.visualize_rec(graph,value_map,name_prefix,pb_graph,executors_it=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/contrib/_graph_vis.py----------------------------------------
A:torch.contrib._graph_vis._vis_template->string.Template('\n<!doctype html>\n<html>\n<head>\n  <title>$name</title>\n\n  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.20.1/vis.min.js"></script>\n  <link href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.20.1/vis.min.css" rel="stylesheet" type="text/css" />\n\n  <style type="text/css">\n    #mynetwork {\n      height: 100vh;\n    }\n  </style>\n</head>\n<body>\n\n<div id="mynetwork"></div>\n\n<script type="text/javascript">\n  // create an array with nodes\n  var nodes = new vis.DataSet(\n    $nodes\n  );\n\n  // create an array with edges\n  var edges = new vis.DataSet(\n    $edges\n  );\n\n  // create a network\n  var container = document.getElementById(\'mynetwork\');\n  var data = {\n    nodes: nodes,\n    edges: edges\n  };\n  var options = $options;\n  var network = new vis.Network(container, data, options);\n</script>\n</body>\n</html>\n')
A:torch.contrib._graph_vis.existing->set()
A:torch.contrib._graph_vis.ident->counts.get(n.kind(), 0)
A:torch.contrib._graph_vis.result->string.Template('\n<!doctype html>\n<html>\n<head>\n  <title>$name</title>\n\n  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.20.1/vis.min.js"></script>\n  <link href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.20.1/vis.min.css" rel="stylesheet" type="text/css" />\n\n  <style type="text/css">\n    #mynetwork {\n      height: 100vh;\n    }\n  </style>\n</head>\n<body>\n\n<div id="mynetwork"></div>\n\n<script type="text/javascript">\n  // create an array with nodes\n  var nodes = new vis.DataSet(\n    $nodes\n  );\n\n  // create an array with edges\n  var edges = new vis.DataSet(\n    $edges\n  );\n\n  // create a network\n  var container = document.getElementById(\'mynetwork\');\n  var data = {\n    nodes: nodes,\n    edges: edges\n  };\n  var options = $options;\n  var network = new vis.Network(container, data, options);\n</script>\n</body>\n</html>\n').substitute(nodes=json.dumps(nodes), edges=json.dumps(edges), options=json.dumps(options), name=filename)
torch.contrib._graph_vis.write(self,filename)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/onnx/__init__.py----------------------------------------
torch.onnx.__init__.ExportTypes
torch.onnx.__init__._export(*args,**kwargs)
torch.onnx.__init__._export_to_pretty_string(*args,**kwargs)
torch.onnx.__init__._optimize_trace(trace,operator_export_type)
torch.onnx.__init__._run_symbolic_function(*args,**kwargs)
torch.onnx.__init__._run_symbolic_method(*args,**kwargs)
torch.onnx.__init__.export(*args,**kwargs)
torch.onnx.__init__.export_to_pretty_string(*args,**kwargs)
torch.onnx.__init__.set_training(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/onnx/symbolic.py----------------------------------------
A:torch.onnx.symbolic.value_t->_maybe_get_const(value, 't')
A:torch.onnx.symbolic.list_node->list_value.node()
A:torch.onnx.symbolic.wrapper->wraps(fn)(wrapper)
A:torch.onnx.symbolic.ty->_try_get_scalar_type(self, other).lower()
A:torch.onnx.symbolic.other->_maybe_get_scalar(other)
A:torch.onnx.symbolic.tensors->_unpack_list(tensor_list)
A:torch.onnx.symbolic.C->g.constant(0, [1], ty)
A:torch.onnx.symbolic.mean->_reduce_op_symbolic('ReduceMean')
A:torch.onnx.symbolic.sum->_reduce_op_symbolic('ReduceSum')
A:torch.onnx.symbolic.prod->_reduce_op_symbolic('ReduceProd')
A:torch.onnx.symbolic.shape->_maybe_get_const(shapes_list[0], 'is')
A:torch.onnx.symbolic.full_shape->g.op('Shape', self)
A:torch.onnx.symbolic.axes->list(range(len(self.type().sizes())))
A:torch.onnx.symbolic.size->_maybe_get_const(size, 'is')
A:torch.onnx.symbolic.self_sizes->view(g, self, [1] * diff_dims + sizes).type().sizes()
A:torch.onnx.symbolic.index_val->_parse_arg(index, 'i')
A:torch.onnx.symbolic.slice_node->g.op('Slice', self, axes_i=[dim], starts_i=[index_val], ends_i=[index_val + 1])
A:torch.onnx.symbolic.negative_slope->_get_const(negative_slope, 't', 'negative_slope')
A:torch.onnx.symbolic.(first, second)->g.op('Split', input, axis_i=dim, outputs=2)
A:torch.onnx.symbolic.r->g.op('MaxPool', input, kernel_shape_i=_triple(kernel_size), pads_i=_triple(padding) * 2, strides_i=_triple(stride))
A:torch.onnx.symbolic.padding->tuple(tuple_fn(padding))
A:torch.onnx.symbolic.input->g.op('Transpose', input, perm_i=[1, 0, 2])
A:torch.onnx.symbolic.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic.paddings->prepare_onnx_paddings(len(input.type().sizes()), padding)
A:torch.onnx.symbolic.scales->g.op('Constant', value_t=torch.tensor([1.0, 1.0, height_scale, width_scale]))
A:torch.onnx.symbolic.weight_size->_unpack_list(weight_v).type().sizes()
A:torch.onnx.symbolic.n->g.op('ConvTranspose' if transposed else 'Conv', *args, **kwargs)
A:torch.onnx.symbolic.input_sizes->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic.weight_value->torch.tensor([1.0] * input_sizes[1]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic.weight->_unpack_list(weight_v)
A:torch.onnx.symbolic.bias_value->torch.tensor([0.0] * input_sizes[1]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic.bias->g.op('Constant', value_t=bias_value)
A:torch.onnx.symbolic.out->g.op('Squeeze', out, axes_i=[2])
A:torch.onnx.symbolic.res->g.op('Squeeze', res, axes_i=[2])
A:torch.onnx.symbolic.indices_list->_unpack_list(indices_list_value)
A:torch.onnx.symbolic.other_type_name->_maybe_get_scalar(other).type().scalarType()
A:torch.onnx.symbolic.exponent->_maybe_get_scalar(exponent)
A:torch.onnx.symbolic.min->_parse_arg(min, 'f')
A:torch.onnx.symbolic.max->_parse_arg(max, 'f')
A:torch.onnx.symbolic.dim->_parse_arg(dim, 'i')
A:torch.onnx.symbolic.keepdim->_get_const(keepdim, 'i', 'keepdim')
A:torch.onnx.symbolic.(r, _)->g.op('Dropout', input, ratio_f=p, outputs=2)
A:torch.onnx.symbolic.feature_dropout->_unsupported_dropout('feature_dropout')
A:torch.onnx.symbolic.alpha_dropout->_unsupported_dropout('alpha_dropout')
A:torch.onnx.symbolic.feature_alpha_dropout->_unsupported_dropout('feature_alpha_dropout')
A:torch.onnx.symbolic.f->_reduce_op_symbolic('ReduceL2')
A:torch.onnx.symbolic.name->'_cast_{}'.format(k)
A:torch.onnx.symbolic.globals()[name]->parse_args('v', 'i')(partial(_cast_func_template, v))
A:torch.onnx.symbolic.const_value->_maybe_get_const(value, 't')
A:torch.onnx.symbolic.tmp->zeros(sizes, dtype, layout, device)
A:torch.onnx.symbolic.dtype->_get_const(args[0], 'i', 'dtype')
A:torch.onnx.symbolic.start_unsqueezed->g.op('Unsqueeze', start, axes_i=[0])
A:torch.onnx.symbolic.end_unsqueezed->g.op('Unsqueeze', end, axes_i=[0])
A:torch.onnx.symbolic.dim_unsqueezed->g.op('Unsqueeze', dim, axes_i=[0])
A:torch.onnx.symbolic.start->_parse_arg(start, 'i')
A:torch.onnx.symbolic.end->_parse_arg(end, 'i')
A:torch.onnx.symbolic.repeats->g.op('Constant', value_t=torch.LongTensor(repeats))
A:torch.onnx.symbolic.const_repeats->_maybe_get_const(repeats, 'is')
A:torch.onnx.symbolic.sizes->view(g, self, [1] * diff_dims + sizes).type().sizes()
A:torch.onnx.symbolic.self->view(g, self, [1] * diff_dims + sizes)
A:torch.onnx.symbolic.dims->view(g, self, [1] * diff_dims + sizes).type().sizes()
A:torch.onnx.symbolic.after_view->view(g, self, [-1, upscale_factor, upscale_factor, output_channel, dims[2], dims[3]])
A:torch.onnx.symbolic.after_transpose->g.op('Transpose', after_view, perm_i=[0, 1, 4, 2, 5, 3])
A:torch.onnx.symbolic.nonlinearity->variant[4:].lower()
A:torch.onnx.symbolic.bias_concat->g.op('Concat', bias_f, bias_b, axis_i=0)
A:torch.onnx.symbolic.(weight_ih, weight_hh, bias_concat)->transform_weights(i)
A:torch.onnx.symbolic.(weight_ih_f, weight_hh_f, bias_f)->transform_weights(2 * i)
A:torch.onnx.symbolic.(weight_ih_b, weight_hh_b, bias_b)->transform_weights(2 * i + 1)
A:torch.onnx.symbolic.weight_ih->g.op('Concat', weight_ih_f, weight_ih_b, axis_i=0)
A:torch.onnx.symbolic.weight_hh->g.op('Concat', weight_hh_f, weight_hh_b, axis_i=0)
A:torch.onnx.symbolic.(prev_output, h_out)->g.op('GRU', *inputs, outputs=2, hidden_size_i=hidden_size, linear_before_reset_i=1, **extra_kwargs)
A:torch.onnx.symbolic.(prev_output, h_out, c_out)->g.op('LSTM', *inputs, outputs=3, hidden_size_i=hidden_size, **extra_kwargs)
A:torch.onnx.symbolic.prev_output->g.op('Squeeze', prev_output, axes_i=[1])
A:torch.onnx.symbolic.gru->_one_hidden_rnn('GRU')
A:torch.onnx.symbolic.rnn_tanh->_one_hidden_rnn('RNN_TANH')
A:torch.onnx.symbolic.rnn_relu->_one_hidden_rnn('RNN_RELU')
A:torch.onnx.symbolic.lengths->_cast_Int(g, lengths, False)
A:torch.onnx.symbolic.(data, lengths)->g.op('prim::PadPacked', data, batch_sizes, outputs=2)
A:torch.onnx.symbolic.data->g.op('Transpose', data, perm_i=[1, 0, 2])
A:torch.onnx.symbolic.shapes_list->list(shapes)
A:torch.onnx.symbolic.p->g.op('RandomUniformLike', input, high_f=upper, low_f=lower)
torch.onnx.symbolic._avg_pool(name,tuple_fn)
torch.onnx.symbolic._cast_func_template(to_i,g,input,non_blocking)
torch.onnx.symbolic._convolution(g,input,weight,bias,stride,padding,dilation,transposed,output_padding,groups,benchmark,deterministic,cudnn_enabled)
torch.onnx.symbolic._dim_arange(g,like,dim)
torch.onnx.symbolic._generic_rnn(g,variant,input,initial_states,all_weights,has_biases,num_layers,dropout,train,bidirectional,batch_first=None,batch_sizes=None)
torch.onnx.symbolic._get_const(value,desc,arg_name)
torch.onnx.symbolic._if_scalar_type_as(g,self,tensor)
torch.onnx.symbolic._is_tensor_list(x)
torch.onnx.symbolic._is_value(x)
torch.onnx.symbolic._lstm_full(g,input,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional,batch_first)
torch.onnx.symbolic._lstm_packed(g,input,batch_sizes,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional)
torch.onnx.symbolic._maybe_get_const(value,desc)
torch.onnx.symbolic._maybe_get_scalar(value)
torch.onnx.symbolic._one_hidden_rnn(kind)
torch.onnx.symbolic._pack_padded_sequence(g,input,lengths,batch_first)
torch.onnx.symbolic._pad_packed_sequence(g,data,batch_sizes,batch_first,padding_value,total_length)
torch.onnx.symbolic._parse_arg(value,desc)
torch.onnx.symbolic._reduce_op_symbolic(onnx_op_name)
torch.onnx.symbolic._reshape_from_tensor(g,input,shape)
torch.onnx.symbolic._scalar(x)
torch.onnx.symbolic._shape_as_tensor(g,input)
torch.onnx.symbolic._try_get_scalar_type(*args)
torch.onnx.symbolic._unimplemented(op,msg)
torch.onnx.symbolic._unique(g,input,sorted,return_inverse)
torch.onnx.symbolic._unpack_list(list_value)
torch.onnx.symbolic._unsupported_dropout(name)
torch.onnx.symbolic._weight_norm(graph,v,g,dim)
torch.onnx.symbolic.abs(g,self)
torch.onnx.symbolic.acos(g,self)
torch.onnx.symbolic.adaptive_avg_pool2d(g,input,output_size)
torch.onnx.symbolic.adaptive_max_pool2d(g,input,output_size)
torch.onnx.symbolic.add(g,self,other,alpha=None)
torch.onnx.symbolic.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic.alias(g,self)
torch.onnx.symbolic.asin(g,self)
torch.onnx.symbolic.atan(g,self)
torch.onnx.symbolic.batch_norm(g,input,weight,bias,running_mean,running_var,training,momentum,eps,cudnn_enabled)
torch.onnx.symbolic.bmm(g,self,other)
torch.onnx.symbolic.cat(g,tensor_list,dim)
torch.onnx.symbolic.clamp(g,self,min,max)
torch.onnx.symbolic.clamp_max(g,self,max)
torch.onnx.symbolic.clamp_min(g,self,min)
torch.onnx.symbolic.clone(g,input)
torch.onnx.symbolic.constant_pad_nd(g,input,padding,value)
torch.onnx.symbolic.contiguous(g,input)
torch.onnx.symbolic.conv_tbc(g,input,weight,bias,pad)
torch.onnx.symbolic.cos(g,self)
torch.onnx.symbolic.cumsum(g,input,dim)
torch.onnx.symbolic.detach(g,input)
torch.onnx.symbolic.div(g,self,other)
torch.onnx.symbolic.dropout(g,input,p,train)
torch.onnx.symbolic.elu(g,input,alpha,scale,input_scale)
torch.onnx.symbolic.embedding(g,weight,indices,padding_idx,scale_grad_by_freq,sparse)
torch.onnx.symbolic.embedding_bag(g,embedding_matrix,indices,offsets,scale_grad_by_freq,mode,sparse)
torch.onnx.symbolic.eq(g,self,other)
torch.onnx.symbolic.exp(g,self)
torch.onnx.symbolic.expand(g,self,size,implicit)
torch.onnx.symbolic.expand_as(g,self,other)
torch.onnx.symbolic.full(g,sizes,value,dtype,layout,device)
torch.onnx.symbolic.full_like(g,input,fill_value)
torch.onnx.symbolic.ge(g,input,other)
torch.onnx.symbolic.glu(g,input,dim)
torch.onnx.symbolic.gt(g,input,other)
torch.onnx.symbolic.hardtanh(g,self,min_val,max_val)
torch.onnx.symbolic.index_put(g,self,indices_list_value,values,accumulate)
torch.onnx.symbolic.index_select(g,self,dim,index)
torch.onnx.symbolic.instance_norm(g,input,weight,bias,running_mean,running_var,use_input_stats,momentum,eps,cudnn_enabled)
torch.onnx.symbolic.layer_norm(g,self,normalized_shape,weight,bias,eps,cudnn_enable)
torch.onnx.symbolic.le(g,input,other)
torch.onnx.symbolic.leaky_relu(g,input,negative_slope,inplace=False)
torch.onnx.symbolic.log(g,self)
torch.onnx.symbolic.log_softmax(g,input,dim=None)
torch.onnx.symbolic.lstm(g,*args)
torch.onnx.symbolic.lt(g,input,other)
torch.onnx.symbolic.matmul(g,self,other)
torch.onnx.symbolic.max(g,self,dim_or_y,keepdim=None)
torch.onnx.symbolic.max_pool1d_with_indices(g,input,kernel_size,stride,padding,dilation,ceil_mode)
torch.onnx.symbolic.max_pool2d_with_indices(g,input,kernel_size,stride,padding,dilation,ceil_mode)
torch.onnx.symbolic.max_pool3d_with_indices(g,input,kernel_size,stride,padding,dilation,ceil_mode)
torch.onnx.symbolic.min(g,self,dim_or_y,keepdim=None)
torch.onnx.symbolic.mm(g,self,other)
torch.onnx.symbolic.mul(g,self,other)
torch.onnx.symbolic.neg(g,self)
torch.onnx.symbolic.norm(g,self,p,dim,keepdim)
torch.onnx.symbolic.ones(g,sizes,dtype,layout,device)
torch.onnx.symbolic.parse_args(*arg_descriptors)
torch.onnx.symbolic.permute(g,self,dims)
torch.onnx.symbolic.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic.pow(g,self,exponent)
torch.onnx.symbolic.prelu(g,self,weight)
torch.onnx.symbolic.prim_ConstantChunk(g,self,chunks,dim)
torch.onnx.symbolic.prim_ConstantSplit(g,self,split_size,dim)
torch.onnx.symbolic.randn(g,*shapes)
torch.onnx.symbolic.reciprocal(g,self)
torch.onnx.symbolic.reflection_pad(g,input,padding)
torch.onnx.symbolic.relu(g,input)
torch.onnx.symbolic.repeat(g,self,repeats)
torch.onnx.symbolic.replication_pad(g,input,padding)
torch.onnx.symbolic.rrelu(g,input,lower,upper,training,generator)
torch.onnx.symbolic.rsub(g,self,other,alpha=None)
torch.onnx.symbolic.select(g,self,dim,index)
torch.onnx.symbolic.selu(g,input)
torch.onnx.symbolic.sigmoid(g,self)
torch.onnx.symbolic.sin(g,self)
torch.onnx.symbolic.size(g,self,dim)
torch.onnx.symbolic.slice(g,self,dim,start,end,step)
torch.onnx.symbolic.softmax(g,input,dim)
torch.onnx.symbolic.softplus(g,self,beta,threshold)
torch.onnx.symbolic.sqrt(g,self)
torch.onnx.symbolic.squeeze(g,self,dim=None)
torch.onnx.symbolic.stack(g,tensor_list,dim)
torch.onnx.symbolic.sub(g,self,other,alpha=None)
torch.onnx.symbolic.t(g,self)
torch.onnx.symbolic.tan(g,self)
torch.onnx.symbolic.tanh(g,self)
torch.onnx.symbolic.threshold(g,self,threshold,value)
torch.onnx.symbolic.to(g,self,*args)
torch.onnx.symbolic.topk(g,self,k,dim,largest,sorted,out=None)
torch.onnx.symbolic.transpose(g,self,dim0,dim1)
torch.onnx.symbolic.type_as(g,self,other)
torch.onnx.symbolic.unfold(g,input,dimension,size,step)
torch.onnx.symbolic.unsqueeze(g,self,dim)
torch.onnx.symbolic.unused(g)
torch.onnx.symbolic.upsample_bilinear2d(g,input,output_size,align_corners)
torch.onnx.symbolic.upsample_nearest2d(g,input,output_size)
torch.onnx.symbolic.view(g,self,size)
torch.onnx.symbolic.where(g,condition,self,other)
torch.onnx.symbolic.zeros(g,sizes,dtype,layout,device)
torch.onnx.symbolic.zeros_like(g,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/onnx/operators.py----------------------------------------
torch.onnx.operators.reshape_from_tensor_shape(x,shape)
torch.onnx.operators.shape_as_tensor(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/onnx/utils.py----------------------------------------
A:torch.onnx.utils.output_type->node.output().type()
A:torch.onnx.utils.lc->g.create('prim::ListConstruct', inputs).insertBefore(node).output().setType(ListType.ofTensors())
A:torch.onnx.utils.graph->_optimize_graph(graph, operator_export_type)
A:torch.onnx.utils.(trace, torch_out)->torch.jit.get_trace_graph(model, args, _force_outplace=True)
A:torch.onnx.utils.orig_state_dict_keys->_unique_state_dict(model).keys()
A:torch.onnx.utils.method->model.__getattr__('forward')
A:torch.onnx.utils.params->list(_unique_state_dict(model).values())
A:torch.onnx.utils.(graph, torch_out)->_trace_and_get_graph_from_model(model, args, training)
A:torch.onnx.utils.(output_tensors, _)->torch._C._jit_flatten(torch_out)
A:torch.onnx.utils.(graph, params, torch_out)->_model_to_graph(model, args, f, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate)
A:torch.onnx.utils.(proto, export_map)->_optimize_graph(graph, operator_export_type)._export_onnx([], _onnx_opset_version, False, operator_export_type)
A:torch.onnx.utils.model_proto_file->os.path.join(f, ONNX_ARCHIVE_MODEL_PROTO_NAME)
A:torch.onnx.utils.weight_proto_file->os.path.join(f, k)
A:torch.onnx.utils.attr_pattern->re.compile('^(.+)_([ifstgz])$')
A:torch.onnx.utils.m->re.compile('^(.+)_([ifstgz])$').match(key)
A:torch.onnx.utils.value->_scalar(value)
A:torch.onnx.utils.aten->dict(((k, v) for (k, v) in kwargs.items() if v is not None)).pop('aten', False)
A:torch.onnx.utils.n->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs))
A:torch.onnx.utils.outputs->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).outputsSize()
A:torch.onnx.utils.kwargs->dict(((k, v) for (k, v) in kwargs.items() if v is not None))
A:torch.onnx.utils.args->list((const_if_tensor(arg) for arg in raw_args))
A:torch.onnx.utils.ns_op_name->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).kind()
A:torch.onnx.utils.(ns, op_name)->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).kind().split('::')
A:torch.onnx.utils.is_exportable_aten_op->hasattr(torch.onnx.symbolic, op_name)
A:torch.onnx.utils.fn->getattr(torch.onnx.symbolic, op_name)
A:torch.onnx.utils.new_op_outputs->g.op(op_name, *inputs, outputs=n.outputsSize())
A:torch.onnx.utils.new_block->new_node.addBlock()
A:torch.onnx.utils.symbolic_fn->getattr(torch.onnx.symbolic, symbolic_name, None)
A:torch.onnx.utils.type->type.lower().lower()
A:torch.onnx.utils.tensor->torch.DoubleTensor(*dims)
A:torch.onnx.utils.sel->self.kindOf(k)
torch.onnx.utils._add_attribute(node,key,value,aten)
torch.onnx.utils._export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False)
torch.onnx.utils._export_to_pretty_string(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,google_printer=False)
torch.onnx.utils._graph_at(g,opname,*args,**kwargs)
torch.onnx.utils._graph_constant(g,value,dims,type,*args,**kwargs)
torch.onnx.utils._graph_op(g,opname,*raw_args,**kwargs)
torch.onnx.utils._is_onnx_list(value)
torch.onnx.utils._model_to_graph(model,args,f,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,example_outputs=None,propagate=False)
torch.onnx.utils._newNode(g,opname,outputs,*args,**kwargs)
torch.onnx.utils._node_getitem(self,k)
torch.onnx.utils._optimize_graph(graph,operator_export_type)
torch.onnx.utils._run_symbolic_function(g,n,inputs,env,operator_export_type=OperatorExportTypes.ONNX)
torch.onnx.utils._run_symbolic_method(op_name,symbolic_fn,args)
torch.onnx.utils._scalar(x)
torch.onnx.utils._set_input_and_output_names(graph,input_names,output_names)
torch.onnx.utils._split_tensor_list_constants(g,block)
torch.onnx.utils._trace(func,args,operator_export_type,return_outs=False)
torch.onnx.utils._trace_and_get_graph_from_model(model,args,training)
torch.onnx.utils.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None)
torch.onnx.utils.export_to_pretty_string(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,google_printer=False)
torch.onnx.utils.set_training(model,mode)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py----------------------------------------
A:torch.distributed.distributed_c10d.value->getattr(Backend, name.upper(), Backend.UNDEFINED)
A:torch.distributed.distributed_c10d.reduce_op->reduce_op()
A:torch.distributed.distributed_c10d.WORLD->object()
A:torch.distributed.distributed_c10d.NON_GROUP_MEMBER->object()
A:torch.distributed.distributed_c10d._default_pg_timeout->timedelta(minutes=30)
A:torch.distributed.distributed_c10d.world_size->kwargs.pop('world_size', -1)
A:torch.distributed.distributed_c10d.group_name->str(_group_count)
A:torch.distributed.distributed_c10d.rank->kwargs.pop('rank', -1)
A:torch.distributed.distributed_c10d.backend->Backend(backend)
A:torch.distributed.distributed_c10d._default_pg->ProcessGroupNCCL(store, rank, world_size)
A:torch.distributed.distributed_c10d.(store, rank, world_size)->next(rendezvous(url))
A:torch.distributed.distributed_c10d.pg->_new_process_group_helper(group_world_size, group_rank, input_ranks, True, group_name, timeout=timeout)
A:torch.distributed.distributed_c10d.store->PrefixStore(group_name, default_store)
A:torch.distributed.distributed_c10d.group_dst_rank->_get_group_rank(group, dst)
A:torch.distributed.distributed_c10d.group_src_rank->_get_group_rank(group, src)
A:torch.distributed.distributed_c10d.work->group.barrier()
A:torch.distributed.distributed_c10d.src_rank->group.barrier().source_rank()
A:torch.distributed.distributed_c10d.opts->ScatterOptions()
A:torch.distributed.distributed_c10d.my_rank->get_rank()
A:torch.distributed.distributed_c10d.global_rank->ProcessGroupNCCL(store, rank, world_size).rank()
A:torch.distributed.distributed_c10d.global_world_size->ProcessGroupNCCL(store, rank, world_size).size()
A:torch.distributed.distributed_c10d.input_ranks->list(ranks)
A:torch.distributed.distributed_c10d.group_world_size->len(ranks)
A:torch.distributed.distributed_c10d.group_rank->list(range(global_world_size)).index(global_rank)
A:torch.distributed.distributed_c10d.ranks->list(range(global_world_size))
A:torch.distributed.distributed_c10d._pg_group_ranks[pg]->_new_process_group_helper(group_world_size, group_rank, input_ranks, True, group_name, timeout=timeout).group_ranks()
A:torch.distributed.distributed_c10d._pg_group_ranks[pg][rank]->list(range(global_world_size)).index(rank)
torch.distributed.Backend(cls,name)
torch.distributed.GroupMember(object)
torch.distributed._check_default_pg()
torch.distributed._check_single_tensor(param,param_name)
torch.distributed._check_tensor_list(param,param_name)
torch.distributed._get_default_group()
torch.distributed._get_global_rank(group,group_rank)
torch.distributed._get_group_rank(group,rank)
torch.distributed._get_group_size(group)
torch.distributed._new_process_group_helper(world_size,rank,group_ranks,in_group,group_name,timeout=_default_pg_timeout)
torch.distributed._rank_not_in_group(group)
torch.distributed.all_gather(tensor_list,tensor,group=group.WORLD,async_op=False)
torch.distributed.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.barrier(group=group.WORLD,async_op=False)
torch.distributed.broadcast(tensor,src,group=group.WORLD,async_op=False)
torch.distributed.broadcast_multigpu(tensor_list,src,group=group.WORLD,async_op=False,src_tensor=0)
torch.distributed.destroy_process_group(group=group.WORLD)
torch.distributed.distributed_c10d.Backend(cls,name)
torch.distributed.distributed_c10d.Backend.__new__(cls,name)
torch.distributed.distributed_c10d.GroupMember(object)
torch.distributed.distributed_c10d._check_default_pg()
torch.distributed.distributed_c10d._check_single_tensor(param,param_name)
torch.distributed.distributed_c10d._check_tensor_list(param,param_name)
torch.distributed.distributed_c10d._get_default_group()
torch.distributed.distributed_c10d._get_global_rank(group,group_rank)
torch.distributed.distributed_c10d._get_group_rank(group,rank)
torch.distributed.distributed_c10d._get_group_size(group)
torch.distributed.distributed_c10d._new_process_group_helper(world_size,rank,group_ranks,in_group,group_name,timeout=_default_pg_timeout)
torch.distributed.distributed_c10d._rank_not_in_group(group)
torch.distributed.distributed_c10d.all_gather(tensor_list,tensor,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce(tensor,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.barrier(group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.broadcast(tensor,src,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.broadcast_multigpu(tensor_list,src,group=group.WORLD,async_op=False,src_tensor=0)
torch.distributed.distributed_c10d.destroy_process_group(group=group.WORLD)
torch.distributed.distributed_c10d.gather(tensor,gather_list,dst,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.get_backend(group=group.WORLD)
torch.distributed.distributed_c10d.get_rank(group=group.WORLD)
torch.distributed.distributed_c10d.get_world_size(group=group.WORLD)
torch.distributed.distributed_c10d.group(object)
torch.distributed.distributed_c10d.init_process_group(backend,init_method='env://',timeout=_default_pg_timeout,**kwargs)
torch.distributed.distributed_c10d.irecv(tensor,src,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.is_initialized()
torch.distributed.distributed_c10d.is_mpi_available()
torch.distributed.distributed_c10d.is_nccl_available()
torch.distributed.distributed_c10d.isend(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.new_group(ranks=None,timeout=_default_pg_timeout)
torch.distributed.distributed_c10d.recv(tensor,src=None,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.reduce(tensor,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False,dst_tensor=0)
torch.distributed.distributed_c10d.reduce_op(self)
torch.distributed.distributed_c10d.reduce_op.__getattribute__(self,key)
torch.distributed.distributed_c10d.reduce_op.__init__(self)
torch.distributed.distributed_c10d.scatter(tensor,scatter_list,src,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.send(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.gather(tensor,gather_list,dst,group=group.WORLD,async_op=False)
torch.distributed.get_backend(group=group.WORLD)
torch.distributed.get_rank(group=group.WORLD)
torch.distributed.get_world_size(group=group.WORLD)
torch.distributed.group(object)
torch.distributed.init_process_group(backend,init_method='env://',timeout=_default_pg_timeout,**kwargs)
torch.distributed.irecv(tensor,src,group=group.WORLD,tag=0)
torch.distributed.is_initialized()
torch.distributed.is_mpi_available()
torch.distributed.is_nccl_available()
torch.distributed.isend(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.new_group(ranks=None,timeout=_default_pg_timeout)
torch.distributed.recv(tensor,src=None,group=group.WORLD,tag=0)
torch.distributed.reduce(tensor,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False,dst_tensor=0)
torch.distributed.reduce_op(self)
torch.distributed.reduce_op.__getattribute__(self,key)
torch.distributed.scatter(tensor,scatter_list,src,group=group.WORLD,async_op=False)
torch.distributed.send(tensor,dst,group=group.WORLD,tag=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributed/__init__.py----------------------------------------
torch.distributed.__init__.is_available()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributed/rendezvous.py----------------------------------------
A:torch.distributed.rendezvous.result->urlparse(url)
A:torch.distributed.rendezvous.query->dict((pair.split('=') for pair in filter(None, result.query.split('&'))))
A:torch.distributed.rendezvous.rank->int(rank)
A:torch.distributed.rendezvous.world_size->int(world_size)
A:torch.distributed.rendezvous.store->TCPStore(master_addr, master_port, world_size, start_daemon)
A:torch.distributed.rendezvous.master_addr->os.environ.get('MASTER_ADDR', None)
A:torch.distributed.rendezvous.master_port->int(master_port)
torch.distributed.rendezvous._env_rendezvous_handler(url)
torch.distributed.rendezvous._file_rendezvous_handler(url)
torch.distributed.rendezvous._rendezvous_error(msg)
torch.distributed.rendezvous._tcp_rendezvous_handler(url)
torch.distributed.rendezvous.register_rendezvous_handler(scheme,handler)
torch.distributed.rendezvous.rendezvous(url,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributed/launch.py----------------------------------------
A:torch.distributed.launch.parser->ArgumentParser(description='PyTorch distributed training launch helper utilty that will spawn up multiple distributed processes')
A:torch.distributed.launch.args->parse_args()
A:torch.distributed.launch.current_env->os.environ.copy()
A:torch.distributed.launch.current_env['MASTER_PORT']->str(args.master_port)
A:torch.distributed.launch.current_env['WORLD_SIZE']->str(dist_world_size)
A:torch.distributed.launch.current_env['RANK']->str(dist_rank)
A:torch.distributed.launch.process->subprocess.Popen(cmd, env=current_env)
torch.distributed.launch.main()
torch.distributed.launch.parse_args()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributed/deprecated/__init__.py----------------------------------------
A:torch.distributed.deprecated.__init__._scope->locals()
A:torch.distributed.deprecated.__init__.world_size->kwargs.pop('world_size', -1)
A:torch.distributed.deprecated.__init__.group_name->kwargs.pop('group_name', '')
A:torch.distributed.deprecated.__init__.rank->kwargs.pop('rank', -1)
A:torch.distributed.deprecated.__init__.backend->backend.lower().lower()
A:torch.distributed.deprecated.__init__.SUM->object()
A:torch.distributed.deprecated.__init__.PRODUCT->object()
A:torch.distributed.deprecated.__init__.MAX->object()
A:torch.distributed.deprecated.__init__.MIN->object()
A:torch.distributed.deprecated.__init__.WORLD->object()
A:torch.distributed.deprecated.__init__.ret->torch._C._dist_all_gather_multigpu(flatten_tensor_list, input_tensor_list, group)
A:torch.distributed.deprecated.__init__.my_rank->get_rank()
A:torch.distributed.deprecated.__init__.dst->kwargs.pop('dst', my_rank)
A:torch.distributed.deprecated.__init__.gather_list->kwargs.pop('gather_list', None)
A:torch.distributed.deprecated.__init__._group->kwargs.pop('group', group.WORLD)
A:torch.distributed.deprecated.__init__.src->kwargs.pop('src', my_rank)
A:torch.distributed.deprecated.__init__.scatter_list->kwargs.pop('scatter_list', None)
A:torch.distributed.deprecated.__init__.ranks->list(range(get_world_size()))
torch.distributed.deprecated.__init__._DistributedRequest(self,request)
torch.distributed.deprecated.__init__._DistributedRequest.__init__(self,request)
torch.distributed.deprecated.__init__._DistributedRequest.is_completed(self)
torch.distributed.deprecated.__init__._DistributedRequest.wait(self)
torch.distributed.deprecated.__init__._clear_group_cache(group=group.WORLD)
torch.distributed.deprecated.__init__._extend_scope(module)
torch.distributed.deprecated.__init__._register_stream(stream)
torch.distributed.deprecated.__init__.all_gather(tensor_list,tensor,group=group.WORLD)
torch.distributed.deprecated.__init__.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD)
torch.distributed.deprecated.__init__.all_reduce(tensor,op=reduce_op.SUM,group=group.WORLD)
torch.distributed.deprecated.__init__.all_reduce_multigpu(tensor_list,op=reduce_op.SUM,group=group.WORLD)
torch.distributed.deprecated.__init__.barrier(group=group.WORLD)
torch.distributed.deprecated.__init__.broadcast(tensor,src,group=group.WORLD)
torch.distributed.deprecated.__init__.broadcast_multigpu(tensor_list,src,group=group.WORLD)
torch.distributed.deprecated.__init__.destroy_process_group()
torch.distributed.deprecated.__init__.dist_backend
torch.distributed.deprecated.__init__.gather(tensor,**kwargs)
torch.distributed.deprecated.__init__.get_rank()
torch.distributed.deprecated.__init__.get_world_size()
torch.distributed.deprecated.__init__.group(object)
torch.distributed.deprecated.__init__.init_master_worker(backend,init_method='env://',**kwargs)
torch.distributed.deprecated.__init__.init_process_group(backend,init_method='env://',**kwargs)
torch.distributed.deprecated.__init__.irecv(tensor,src)
torch.distributed.deprecated.__init__.is_available()
torch.distributed.deprecated.__init__.is_initialized()
torch.distributed.deprecated.__init__.isend(tensor,dst)
torch.distributed.deprecated.__init__.new_group(ranks=None)
torch.distributed.deprecated.__init__.recv(tensor,src=None)
torch.distributed.deprecated.__init__.reduce(tensor,dst,op=reduce_op.SUM,group=group.WORLD)
torch.distributed.deprecated.__init__.reduce_multigpu(tensor_list,dst,op=reduce_op.SUM,group=group.WORLD)
torch.distributed.deprecated.__init__.reduce_op(object)
torch.distributed.deprecated.__init__.scatter(tensor,**kwargs)
torch.distributed.deprecated.__init__.send(tensor,dst)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.0.1/lib/python3.6/site-packages/torch/distributed/deprecated/remote_types.py----------------------------------------
A:torch.distributed.deprecated.remote_types._locals->locals()
torch.distributed.deprecated.remote_types.ByteStorage(_DistributedBase,torch._C.DistributedByteStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.CharStorage(_DistributedBase,torch._C.DistributedCharStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.DoubleStorage(_DistributedBase,torch._C.DistributedDoubleStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.FloatStorage(_DistributedBase,torch._C.DistributedFloatStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.HalfStorage(_DistributedBase,torch._C.DistributedHalfStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.IntStorage(_DistributedBase,torch._C.DistributedIntStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.LongStorage(_DistributedBase,torch._C.DistributedLongStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types.ShortStorage(_DistributedBase,torch._C.DistributedShortStorageBase,_StorageBase)
torch.distributed.deprecated.remote_types._DistributedBase(object)

