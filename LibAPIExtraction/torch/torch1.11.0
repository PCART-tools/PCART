
----------------------------------------/home/zhang/Packages/torch/torch1.11.0/serialization.py----------------------------------------
A:torch.serialization.path->tempfile.mkdtemp()
A:torch.serialization.start->f.tell()
A:torch.serialization.byte->f.read(1)
A:torch.serialization.version_strs->_import_dotted_name(storage_type.__module__).__version__.split('.')
A:torch.serialization.module_version->tuple((type(req_field)(version_strs[idx]) for (idx, req_field) in enumerate(req_version_tuple)))
A:torch.serialization.device->validate_cuda_device(location)
A:torch.serialization.device_count->torch.cuda.device_count()
A:torch.serialization.storage_type->normalize_storage_type(type(obj))
A:torch.serialization.location->map_location.get(location, location)
A:torch.serialization.result->UnpicklerWrapper(data_file, **pickle_load_args).load()
A:torch.serialization.module->_import_dotted_name(storage_type.__module__)
A:torch.serialization.(source_lines, _, source_file)->get_source_lines_and_file(obj)
A:torch.serialization.source->''.join(source_lines)
A:torch.serialization.storage_type_str->cast(Storage, torch._UntypedStorage(nbytes)).pickle_storage_type()
A:torch.serialization.storage_numel->zip_file.get_storage_from_record(name, numel, torch._UntypedStorage).storage()._untyped().nbytes()
A:torch.serialization.storage->zip_file.get_storage_from_record(name, numel, torch._UntypedStorage).storage()._untyped()
A:torch.serialization.storage_key->id_map.setdefault(storage._cdata, str(len(id_map)))
A:torch.serialization.sys_info->dict(protocol_version=PROTOCOL_VERSION, little_endian=sys.byteorder == 'little', type_sizes=dict(short=SHORT_SIZE, int=INT_SIZE, long=LONG_SIZE))
A:torch.serialization.pickler->pickle_module.Pickler(data_buf, protocol=pickle_protocol)
A:torch.serialization.serialized_storage_keys->sorted(serialized_storages.keys())
A:torch.serialization.data_buf->io.BytesIO()
A:torch.serialization.data_value->io.BytesIO().getvalue()
A:torch.serialization.num_bytes->zip_file.get_storage_from_record(name, numel, torch._UntypedStorage).storage()._untyped().nbytes()
A:torch.serialization.orig_position->opened_file.tell()
A:torch.serialization.restore_location->_get_restore_location(map_location)
A:torch.serialization.current_source->''.join(get_source_lines_and_file(container_type)[0])
A:torch.serialization.diff->difflib.unified_diff(current_source.split('\n'), original_source.split('\n'), source_file, source_file, lineterm='')
A:torch.serialization.lines->'\n'.join(diff)
A:torch.serialization.file_size->f.seek(0, 2)
A:torch.serialization.num_storages->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.args->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.obj->cast(Storage, torch._UntypedStorage(nbytes))
A:torch.serialization.deserialized_objects[key]->torch.storage._TypedStorage(wrap_storage=obj, dtype=dtype)
A:torch.serialization.storage_views->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.element_size->torch._utils._element_size(root.dtype)
A:torch.serialization.deserialized_objects[target_cdata]->torch.storage._TypedStorage(wrap_storage=root._storage[offset_bytes:offset_bytes + numel * element_size], dtype=root.dtype)
A:torch.serialization.num_tensors->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.(ndim,)->struct.unpack('<i', f.read(4))
A:torch.serialization.numel->struct.unpack(f'<{ndim}q', f.read(8 * ndim))
A:torch.serialization.stride->struct.unpack(f'<{ndim}q', f.read(8 * ndim))
A:torch.serialization.(storage_offset,)->struct.unpack('<q', f.read(8))
A:torch.serialization.tensor->torch.tensor([], dtype=storage.dtype).set_(storage._storage, storage_offset, numel, stride)
A:torch.serialization.pickle_file->tar.extractfile('pickle')
A:torch.serialization.unpickler->UnpicklerWrapper(data_file, **pickle_load_args)
A:torch.serialization.typename->_maybe_decode_ascii(saved_id[0])
A:torch.serialization.deserialized_objects[root_key]->torch.storage._TypedStorage(wrap_storage=restore_location(obj, location), dtype=dtype)
A:torch.serialization.deserialized_objects[view_key]->torch.storage._TypedStorage(wrap_storage=typed_storage._storage[offset_bytes:offset_bytes + view_size_bytes], dtype=dtype)
A:torch.serialization.f_should_read_directly->_should_read_directly(f)
A:torch.serialization.magic_number->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.protocol_version->pickle_module.load(f, **pickle_load_args)
A:torch.serialization._sys_info->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.deserialized_storage_keys->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.offset->f.tell()
A:torch.serialization.self.dtype->_get_dtype_from_pickle_storage_type(name)
A:torch.serialization.loaded_storages[key]->torch.storage._TypedStorage(wrap_storage=restore_location(storage, location), dtype=dtype)
A:torch.serialization.mod_name->load_module_mapping.get(mod_name, mod_name)
A:torch.serialization.data_file->io.BytesIO(zip_file.get_record(pickle_file))
torch.load(f,map_location=None,pickle_module=pickle,**pickle_load_args)
torch.save(obj,f:Union[str,os.PathLike,BinaryIO,IO[bytes]],pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL,_use_new_zipfile_serialization=True)->None
torch.serialization.SourceChangeWarning(Warning)
torch.serialization.StorageType(self,name)
torch.serialization.StorageType.__init__(self,name)
torch.serialization.StorageType.__str__(self)
torch.serialization._check_dill_version(pickle_module)->None
torch.serialization._check_seekable(f)->bool
torch.serialization._cpu_deserialize(obj,location)
torch.serialization._cpu_tag(obj)
torch.serialization._cuda_deserialize(obj,location)
torch.serialization._cuda_tag(obj)
torch.serialization._get_layout(name)
torch.serialization._get_restore_location(map_location)
torch.serialization._is_compressed_file(f)->bool
torch.serialization._is_path(name_or_buffer)
torch.serialization._is_torchscript_zip(zip_file)
torch.serialization._is_zipfile(f)->bool
torch.serialization._legacy_load(f,map_location,pickle_module,**pickle_load_args)
torch.serialization._legacy_save(obj,f,pickle_module,pickle_protocol)->None
torch.serialization._load(zip_file,map_location,pickle_module,pickle_file='data.pkl',**pickle_load_args)
torch.serialization._maybe_decode_ascii(bytes_str:Union[bytes,str])->str
torch.serialization._open_buffer_reader(self,buffer)
torch.serialization._open_buffer_reader.__init__(self,buffer)
torch.serialization._open_buffer_writer(_opener)
torch.serialization._open_buffer_writer.__exit__(self,*args)
torch.serialization._open_file(self,name,mode)
torch.serialization._open_file.__exit__(self,*args)
torch.serialization._open_file.__init__(self,name,mode)
torch.serialization._open_file_like(name_or_buffer,mode)
torch.serialization._open_zipfile_reader(self,name_or_buffer)
torch.serialization._open_zipfile_reader.__init__(self,name_or_buffer)
torch.serialization._open_zipfile_writer(name_or_buffer)
torch.serialization._open_zipfile_writer_buffer(self,buffer)
torch.serialization._open_zipfile_writer_buffer.__exit__(self,*args)->None
torch.serialization._open_zipfile_writer_buffer.__init__(self,buffer)
torch.serialization._open_zipfile_writer_file(self,name)
torch.serialization._open_zipfile_writer_file.__exit__(self,*args)->None
torch.serialization._open_zipfile_writer_file.__init__(self,name)
torch.serialization._opener(self,file_like)
torch.serialization._opener.__enter__(self)
torch.serialization._opener.__exit__(self,*args)
torch.serialization._opener.__init__(self,file_like)
torch.serialization._save(obj,zip_file,pickle_module,pickle_protocol)
torch.serialization._should_read_directly(f)
torch.serialization.check_module_version_greater_or_equal(module,req_version_tuple,error_if_malformed=True)
torch.serialization.default_restore_location(storage,location)
torch.serialization.load(f,map_location=None,pickle_module=pickle,**pickle_load_args)
torch.serialization.location_tag(storage:Union[Storage,torch.storage._TypedStorage])
torch.serialization.mkdtemp()
torch.serialization.normalize_storage_type(storage_type)
torch.serialization.register_package(priority,tagger,deserializer)
torch.serialization.save(obj,f:Union[str,os.PathLike,BinaryIO,IO[bytes]],pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL,_use_new_zipfile_serialization=True)->None
torch.serialization.storage_to_tensor_type(storage)
torch.serialization.validate_cuda_device(location)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_utils.py----------------------------------------
A:torch._utils.non_blocking->_get_async_or_non_blocking('cuda', non_blocking, kwargs)
A:torch._utils.dtype->_import_dotted_name(dtype)
A:torch._utils.new_module_name->_import_dotted_name(dtype).__module__.replace('.sparse', '')
A:torch._utils.new_values->torch.Tensor._values(self).type(new_values_type_name, non_blocking)
A:torch._utils.new_indices->torch.Tensor._indices(self).type(new_indices_type_name, non_blocking)
A:torch._utils.device->torch.device(device)
A:torch._utils.new_type->getattr(torch.cuda, self.__class__.__name__)
A:torch._utils.indices->torch.Tensor._indices(tensor)
A:torch._utils.values->torch.Tensor._values(tensor)
A:torch._utils.argument->list(kwargs.keys()).pop()
A:torch._utils.t->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype).type()
A:torch._utils.tensor->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype)
A:torch._utils.result->torch._sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size)
A:torch._utils.scales->torch.tensor(scales, dtype=torch.float)
A:torch._utils.zero_points->torch.tensor(zero_points, dtype=torch.float)
A:torch._utils.param->torch.nn.Parameter(data, requires_grad)
A:torch._utils.components->name.split('.')
A:torch._utils.obj->getattr(obj, component)
A:torch._utils.it->iter(iterable)
A:torch._utils.total->fn(total, element)
A:torch._utils.flat_indices->torch._C._nn.flatten_dense_tensors([torch.Tensor._indices(t) for t in tensors])
A:torch._utils.flat_values->torch._C._nn.flatten_dense_tensors([torch.Tensor._values(t) for t in tensors])
A:torch._utils.type_dict->defaultdict(list)
A:torch._utils.fun.__annotations__->dict(kwargs)
A:torch._utils.exc_info->sys.exc_info()
A:torch._utils.self.exc_msg->''.join(traceback.format_exception(*exc_info))
A:torch._utils.msg->KeyErrorMessage(msg)
A:torch._utils.exception->self.exc_type(msg)
A:torch._utils.device_type->_get_available_device_type()
A:torch._utils.device_idx->_get_current_device_index()
A:torch._utils.owner->type(instance)
A:torch._utils.func->classmethod(func)
torch._utils.ExceptionWrapper(self,exc_info=None,where='inbackground')
torch._utils.ExceptionWrapper.__init__(self,exc_info=None,where='inbackground')
torch._utils.ExceptionWrapper.reraise(self)
torch._utils.KeyErrorMessage(str)
torch._utils.KeyErrorMessage.__repr__(self)
torch._utils._ClassPropertyDescriptor(self,fget,fset=None)
torch._utils._ClassPropertyDescriptor.__get__(self,instance,owner=None)
torch._utils._ClassPropertyDescriptor.__init__(self,fget,fset=None)
torch._utils._accumulate(iterable,fn=lambdax,y:x+y)
torch._utils._cuda(self,device=None,non_blocking=False,**kwargs)
torch._utils._element_size(dtype)
torch._utils._flatten_dense_tensors(tensors)
torch._utils._flatten_sparse_tensors(tensors)
torch._utils._get_all_device_indices()
torch._utils._get_async_or_non_blocking(function_name,non_blocking,kwargs)
torch._utils._get_available_device_type()
torch._utils._get_current_device_index()
torch._utils._get_device_attr(get_member)
torch._utils._get_device_index(device:Any,optional:bool=False,allow_cpu:bool=False)->int
torch._utils._get_devices_properties(device_ids)
torch._utils._handle_complex(tensor)
torch._utils._import_dotted_name(name)
torch._utils._rebuild_device_tensor_from_numpy(data,dtype,device,requires_grad)
torch._utils._rebuild_meta_tensor_no_storage(dtype,size,stride,requires_grad)
torch._utils._rebuild_parameter(data,requires_grad,backward_hooks)
torch._utils._rebuild_qtensor(storage,storage_offset,size,stride,quantizer_params,requires_grad,backward_hooks)
torch._utils._rebuild_sparse_csr_tensor(layout,data)
torch._utils._rebuild_sparse_tensor(layout,data)
torch._utils._rebuild_tensor(storage,storage_offset,size,stride)
torch._utils._rebuild_tensor_v2(storage,storage_offset,size,stride,requires_grad,backward_hooks)
torch._utils._reorder_tensors_as(tensors,ordered_tensors)
torch._utils._take_tensors(tensors,size_limit)
torch._utils._type(self,dtype=None,non_blocking=False,**kwargs)
torch._utils._unflatten_dense_tensors(flat,tensors)
torch._utils._unflatten_sparse_tensors(flat,tensors)
torch._utils._validate_loaded_sparse_tensors()
torch._utils.annotate(ret,**kwargs)
torch._utils.classproperty(func)
torch._utils.get_current_device_index()->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/storage.py----------------------------------------
A:torch.storage.T->TypeVar('T', bound='Union[_StorageBase, _TypedStorage]')
A:torch.storage.memo->memo.setdefault('torch', {}).setdefault('torch', {})
A:torch.storage.new_storage->self.clone()
A:torch.storage.b->io.BytesIO()
A:torch.storage.storage->cls(wrap_storage=untyped_storage)
A:torch.storage.allocator->torch.cuda._host_allocator()
A:torch.storage.self._storage->torch.tensor([], dtype=self.dtype, device=self.device).set_(self).storage()._untyped()
A:torch.storage.tmp_tensor->torch.tensor([], dtype=self.dtype, device=self.device).set_(self)
A:torch.storage.module->eval(cls.__module__)
A:torch.storage.idx_wrapped->self._maybe_wrap_index(idx)
A:torch.storage.cuda_storage->self._storage.cuda(device, non_blocking, **kwargs)
A:torch.storage.untyped_storage->eval(cls.__module__)._UntypedStorage.from_file(filename, shared, size * torch._utils._element_size(cls.dtype))
A:torch.storage.(manager_handle, storage_handle, size)->self._storage._share_filename_(*args, **kwargs)
A:torch.storage.(fd, size)->self._storage._share_fd_(*args, **kwargs)
torch._StorageBase(self,*args,**kwargs)
torch._StorageBase.__copy__(self)
torch._StorageBase.__deepcopy__(self,memo)
torch._StorageBase.__getitem__(self,idx)
torch._StorageBase.__iter__(self)
torch._StorageBase.__len__(self)->int
torch._StorageBase.__reduce__(self)
torch._StorageBase.__repr__(self)
torch._StorageBase.__sizeof__(self)
torch._StorageBase.__str__(self)
torch._StorageBase._new_shared(cls,size)
torch._StorageBase._new_using_fd(cls:Type[T],size:int)->T
torch._StorageBase._new_using_filename(cls:Type[T],size:int)->T
torch._StorageBase._share_fd_(self)
torch._StorageBase._share_filename_(self)
torch._StorageBase._to(self,dtype)
torch._StorageBase._untyped(self)
torch._StorageBase.bfloat16(self)
torch._StorageBase.bool(self)
torch._StorageBase.byte(self)
torch._StorageBase.char(self)
torch._StorageBase.clone(self)
torch._StorageBase.complex_double(self)
torch._StorageBase.complex_float(self)
torch._StorageBase.copy_(self,source:T)->T
torch._StorageBase.data_ptr(self)->int
torch._StorageBase.double(self)
torch._StorageBase.element_size(self)->int
torch._StorageBase.float(self)
torch._StorageBase.get_device(self)->int
torch._StorageBase.half(self)
torch._StorageBase.int(self)
torch._StorageBase.long(self)
torch._StorageBase.nbytes(self)->int
torch._StorageBase.pin_memory(self)
torch._StorageBase.share_memory_(self)
torch._StorageBase.short(self)
torch._StorageBase.size(self)->int
torch._StorageBase.tolist(self)
torch._StorageBase.type(self,dtype:str=None,non_blocking:bool=False)->T
torch._TypedStorage(self,*args,**kwargs)
torch._TypedStorage.__copy__(self)
torch._TypedStorage.__deepcopy__(self,memo)
torch._TypedStorage.__getitem__(self,idx)
torch._TypedStorage.__iter__(self)
torch._TypedStorage.__len__(self)
torch._TypedStorage.__reduce__(self)
torch._TypedStorage.__repr__(self)
torch._TypedStorage.__setitem__(self,idx,value)
torch._TypedStorage.__sizeof__(self)
torch._TypedStorage.__str__(self)
torch._TypedStorage._cdata(self)
torch._TypedStorage._expired(cls,*args,**kwargs)
torch._TypedStorage._free_weak_ref(cls,*args,**kwargs)
torch._TypedStorage._maybe_wrap_index(self,idx,is_stop=False)
torch._TypedStorage._new_shared(cls,size)
torch._TypedStorage._new_shared_filename(cls,manager,obj,size)
torch._TypedStorage._new_with_weak_ptr(cls,*args,**kwargs)
torch._TypedStorage._new_wrapped_storage(self,untyped_storage)
torch._TypedStorage._release_ipc_counter(cls,*args,**kwargs)
torch._TypedStorage._set_cdata(self,*args,**kwargs)
torch._TypedStorage._set_from_file(self,*args,**kwargs)
torch._TypedStorage._share_fd_(self,*args,**kwargs)
torch._TypedStorage._share_filename_(self,*args,**kwargs)
torch._TypedStorage._shared_decref(self)
torch._TypedStorage._shared_incref(self,*args,**kwargs)
torch._TypedStorage._to(self,dtype)
torch._TypedStorage._untyped(self)
torch._TypedStorage._weak_ref(self,*args,**kwargs)
torch._TypedStorage._write_file(self,*args,**kwargs)
torch._TypedStorage.bfloat16(self)
torch._TypedStorage.bool(self)
torch._TypedStorage.byte(self)
torch._TypedStorage.char(self)
torch._TypedStorage.clone(self)
torch._TypedStorage.complex_double(self)
torch._TypedStorage.complex_float(self)
torch._TypedStorage.copy_(self,source:T,non_blocking=None)
torch._TypedStorage.data_ptr(self)
torch._TypedStorage.device(self)
torch._TypedStorage.double(self)
torch._TypedStorage.element_size(self)
torch._TypedStorage.fill_(self,value)
torch._TypedStorage.float(self)
torch._TypedStorage.from_buffer(cls,*args,**kwargs)
torch._TypedStorage.from_file(cls,filename,shared,size)
torch._TypedStorage.get_device(self)->int
torch._TypedStorage.half(self)
torch._TypedStorage.int(self)
torch._TypedStorage.is_shared(self)
torch._TypedStorage.long(self)
torch._TypedStorage.nbytes(self)
torch._TypedStorage.pickle_storage_type(self)
torch._TypedStorage.pin_memory(self)
torch._TypedStorage.resize_(self,size)
torch._TypedStorage.share_memory_(self)
torch._TypedStorage.short(self)
torch._TypedStorage.size(self)
torch._TypedStorage.tolist(self)
torch._TypedStorage.type(self,dtype:str=None,non_blocking:bool=False)->Union[T, str]
torch.storage._StorageBase(self,*args,**kwargs)
torch.storage._StorageBase.__copy__(self)
torch.storage._StorageBase.__deepcopy__(self,memo)
torch.storage._StorageBase.__getitem__(self,idx)
torch.storage._StorageBase.__init__(self,*args,**kwargs)
torch.storage._StorageBase.__iter__(self)
torch.storage._StorageBase.__len__(self)->int
torch.storage._StorageBase.__reduce__(self)
torch.storage._StorageBase.__repr__(self)
torch.storage._StorageBase.__sizeof__(self)
torch.storage._StorageBase.__str__(self)
torch.storage._StorageBase._new_shared(cls,size)
torch.storage._StorageBase._new_using_fd(cls:Type[T],size:int)->T
torch.storage._StorageBase._new_using_filename(cls:Type[T],size:int)->T
torch.storage._StorageBase._share_fd_(self)
torch.storage._StorageBase._share_filename_(self)
torch.storage._StorageBase._to(self,dtype)
torch.storage._StorageBase._untyped(self)
torch.storage._StorageBase.bfloat16(self)
torch.storage._StorageBase.bool(self)
torch.storage._StorageBase.byte(self)
torch.storage._StorageBase.char(self)
torch.storage._StorageBase.clone(self)
torch.storage._StorageBase.complex_double(self)
torch.storage._StorageBase.complex_float(self)
torch.storage._StorageBase.copy_(self,source:T)->T
torch.storage._StorageBase.cpu(self)
torch.storage._StorageBase.cuda(self,device=None,non_blocking=False,**kwargs)->T
torch.storage._StorageBase.data_ptr(self)->int
torch.storage._StorageBase.double(self)
torch.storage._StorageBase.element_size(self)->int
torch.storage._StorageBase.float(self)
torch.storage._StorageBase.get_device(self)->int
torch.storage._StorageBase.half(self)
torch.storage._StorageBase.int(self)
torch.storage._StorageBase.long(self)
torch.storage._StorageBase.nbytes(self)->int
torch.storage._StorageBase.pin_memory(self)
torch.storage._StorageBase.share_memory_(self)
torch.storage._StorageBase.short(self)
torch.storage._StorageBase.size(self)->int
torch.storage._StorageBase.tolist(self)
torch.storage._StorageBase.type(self,dtype:str=None,non_blocking:bool=False)->T
torch.storage._TypedStorage(self,*args,**kwargs)
torch.storage._TypedStorage.__copy__(self)
torch.storage._TypedStorage.__deepcopy__(self,memo)
torch.storage._TypedStorage.__getitem__(self,idx)
torch.storage._TypedStorage.__init__(self,*args,**kwargs)
torch.storage._TypedStorage.__iter__(self)
torch.storage._TypedStorage.__len__(self)
torch.storage._TypedStorage.__reduce__(self)
torch.storage._TypedStorage.__repr__(self)
torch.storage._TypedStorage.__setitem__(self,idx,value)
torch.storage._TypedStorage.__sizeof__(self)
torch.storage._TypedStorage.__str__(self)
torch.storage._TypedStorage._cdata(self)
torch.storage._TypedStorage._expired(cls,*args,**kwargs)
torch.storage._TypedStorage._free_weak_ref(cls,*args,**kwargs)
torch.storage._TypedStorage._maybe_wrap_index(self,idx,is_stop=False)
torch.storage._TypedStorage._new_shared(cls,size)
torch.storage._TypedStorage._new_shared_cuda(cls,*args,**kwargs)
torch.storage._TypedStorage._new_shared_filename(cls,manager,obj,size)
torch.storage._TypedStorage._new_with_weak_ptr(cls,*args,**kwargs)
torch.storage._TypedStorage._new_wrapped_storage(self,untyped_storage)
torch.storage._TypedStorage._release_ipc_counter(cls,*args,**kwargs)
torch.storage._TypedStorage._set_cdata(self,*args,**kwargs)
torch.storage._TypedStorage._set_from_file(self,*args,**kwargs)
torch.storage._TypedStorage._share_cuda_(self,*args,**kwargs)
torch.storage._TypedStorage._share_fd_(self,*args,**kwargs)
torch.storage._TypedStorage._share_filename_(self,*args,**kwargs)
torch.storage._TypedStorage._shared_decref(self)
torch.storage._TypedStorage._shared_incref(self,*args,**kwargs)
torch.storage._TypedStorage._to(self,dtype)
torch.storage._TypedStorage._untyped(self)
torch.storage._TypedStorage._weak_ref(self,*args,**kwargs)
torch.storage._TypedStorage._write_file(self,*args,**kwargs)
torch.storage._TypedStorage.bfloat16(self)
torch.storage._TypedStorage.bool(self)
torch.storage._TypedStorage.byte(self)
torch.storage._TypedStorage.char(self)
torch.storage._TypedStorage.clone(self)
torch.storage._TypedStorage.complex_double(self)
torch.storage._TypedStorage.complex_float(self)
torch.storage._TypedStorage.copy_(self,source:T,non_blocking=None)
torch.storage._TypedStorage.cpu(self)
torch.storage._TypedStorage.cuda(self,device=None,non_blocking=False,**kwargs)->T
torch.storage._TypedStorage.data_ptr(self)
torch.storage._TypedStorage.device(self)
torch.storage._TypedStorage.double(self)
torch.storage._TypedStorage.element_size(self)
torch.storage._TypedStorage.fill_(self,value)
torch.storage._TypedStorage.float(self)
torch.storage._TypedStorage.from_buffer(cls,*args,**kwargs)
torch.storage._TypedStorage.from_file(cls,filename,shared,size)
torch.storage._TypedStorage.get_device(self)->int
torch.storage._TypedStorage.half(self)
torch.storage._TypedStorage.int(self)
torch.storage._TypedStorage.is_cuda(self)
torch.storage._TypedStorage.is_pinned(self)
torch.storage._TypedStorage.is_shared(self)
torch.storage._TypedStorage.long(self)
torch.storage._TypedStorage.nbytes(self)
torch.storage._TypedStorage.pickle_storage_type(self)
torch.storage._TypedStorage.pin_memory(self)
torch.storage._TypedStorage.resize_(self,size)
torch.storage._TypedStorage.share_memory_(self)
torch.storage._TypedStorage.short(self)
torch.storage._TypedStorage.size(self)
torch.storage._TypedStorage.tolist(self)
torch.storage._TypedStorage.type(self,dtype:str=None,non_blocking:bool=False)->Union[T, str]
torch.storage._dtype_to_storage_type_map()
torch.storage._get_dtype_from_pickle_storage_type(pickle_storage_type:str)
torch.storage._load_from_bytes(b)
torch.storage._storage_type_to_dtype_map()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/types.py----------------------------------------
torch.types.Storage(object)
torch.types.Storage.__deepcopy__(self,memo)->'Storage'
torch.types.Storage._new_shared(self,int)->'Storage'
torch.types.Storage._new_with_file(self,f:Any,element_size:int)->'Storage'
torch.types.Storage._write_file(self,f:Any,is_real_file:_bool,save_size:_bool,element_size:int)->None
torch.types.Storage.cpu(self)->'Storage'
torch.types.Storage.data_ptr(self)->int
torch.types.Storage.element_size(self)->int
torch.types.Storage.from_file(self,filename:str,shared:bool=False,nbytes:int=0)->'Storage'
torch.types.Storage.is_shared(self)->bool
torch.types.Storage.nbytes(self)->int
torch.types.Storage.share_memory_(self)->'Storage'


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/__config__.py----------------------------------------
torch.__config__._cxx_flags()
torch.__config__.parallel_info()
torch.__config__.show()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_ops.py----------------------------------------
A:torch._ops.old_flags->sys.getdlopenflags()
A:torch._ops.op_->torch._C._get_operation_overload(self._qualified_op_name, use_key)
A:torch._ops.schema->torch._C._get_schema(self._qualified_op_name, use_key)
A:torch._ops.overload->OpOverload(self, op_, schema)
A:torch._ops.out->getattr(self._op, key)
A:torch._ops.qualified_op_name->'{}::{}'.format(namespace_name, op_name)
A:torch._ops.op->torch._C._jit_get_operation(qualified_op_name)
A:torch._ops.self.loaded_libraries->set()
A:torch._ops.namespace->_OpNamespace(name)
A:torch._ops.path->torch._utils_internal.resolve_library_path(path)
A:torch._ops.ops->_Ops()
torch._ops.OpOverload(self,overloadpacket,op,schema)
torch._ops.OpOverload.__deepcopy__(self,memo=None)
torch._ops.OpOverload.__getattr__(self,key)
torch._ops.OpOverload.__init__(self,overloadpacket,op,schema)
torch._ops.OpOverload.__str__(self)
torch._ops.OpOverload.name(self)
torch._ops.OpOverload.op(self)
torch._ops.OpOverload.overload_name(self)
torch._ops.OpOverload.overload_packet(self)
torch._ops.OpOverloadPacket(self,qualified_op_name,op_name,op)
torch._ops.OpOverloadPacket.__deepcopy__(self,memo=None)
torch._ops.OpOverloadPacket.__getattr__(self,key)
torch._ops.OpOverloadPacket.__init__(self,qualified_op_name,op_name,op)
torch._ops.OpOverloadPacket.__str__(self)
torch._ops.OpOverloadPacket.op(self)
torch._ops.OpOverloadPacket.op_name(self)
torch._ops.OpOverloadPacket.qualified_op_name(self)
torch._ops._OpNamespace(self,name)
torch._ops._OpNamespace.__getattr__(self,op_name)
torch._ops._OpNamespace.__init__(self,name)
torch._ops._Ops(self)
torch._ops._Ops.__getattr__(self,name)
torch._ops._Ops.__init__(self)
torch._ops._Ops.load_library(self,path)
torch._ops.dl_open_guard()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_namedtensor_internals.py----------------------------------------
A:torch._namedtensor_internals.namedshape->namedshape.items().items()
A:torch._namedtensor_internals.globbed_names->expand_single_ellipsis(ellipsis_idx, len(names) - ellipsis_idx - 1, tensor_names)
A:torch._namedtensor_internals.ellipsis_idx->single_ellipsis_index(names, fn_name)
A:torch._namedtensor_internals.dim_map->build_dim_map(tensor)
A:torch._namedtensor_internals.has_rename_pairs->bool(rename_map)
torch._namedtensor_internals.build_dim_map(tensor)
torch._namedtensor_internals.check_serializing_named_tensor(tensor)
torch._namedtensor_internals.expand_single_ellipsis(numel_pre_glob,numel_post_glob,names)
torch._namedtensor_internals.is_ellipsis(item)
torch._namedtensor_internals.namer_api_name(inplace)
torch._namedtensor_internals.replace_ellipsis_by_position(ellipsis_idx,names,tensor_names)
torch._namedtensor_internals.resolve_ellipsis(names,tensor_names,fn_name)
torch._namedtensor_internals.single_ellipsis_index(names,fn_name)
torch._namedtensor_internals.unzip_namedshape(namedshape)
torch._namedtensor_internals.update_names(tensor,names,rename_map,inplace)
torch._namedtensor_internals.update_names_with_list(tensor,names,inplace)
torch._namedtensor_internals.update_names_with_mapping(tensor,rename_map,inplace)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_linalg_utils.py----------------------------------------
A:torch._linalg_utils.ndim->len(A.shape)
A:torch._linalg_utils.Q->torch.orgqr(*torch.geqrf(A))
A:torch._linalg_utils.(E, Z)->torch.linalg.eigh(A, UPLO='U')
A:torch._linalg_utils.E->torch.flip(E, dims=(-1,))
A:torch._linalg_utils.Z->torch.flip(Z, dims=(-1,))
torch._linalg_utils.basis(A)
torch._linalg_utils.bform(X:Tensor,A:Optional[Tensor],Y:Tensor)->Tensor
torch._linalg_utils.conjugate(A)
torch._linalg_utils.get_floating_dtype(A)
torch._linalg_utils.is_sparse(A)
torch._linalg_utils.matmul(A:Optional[Tensor],B:Tensor)->Tensor
torch._linalg_utils.qform(A:Optional[Tensor],S:Tensor)
torch._linalg_utils.symeig(A:Tensor,largest:Optional[bool]=False)->Tuple[Tensor, Tensor]
torch._linalg_utils.transjugate(A)
torch._linalg_utils.transpose(A)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autocast_mode.py----------------------------------------
A:torch.autocast_mode.self.fast_dtype->torch.get_autocast_cpu_dtype()
A:torch.autocast_mode.self._cache_enabled->torch.is_autocast_cache_enabled()
A:torch.autocast_mode.self.prev_cache_enabled->torch.is_autocast_cache_enabled()
A:torch.autocast_mode.self.prev->torch.is_autocast_enabled()
A:torch.autocast_mode.self.prev_fastdtype->torch.get_autocast_gpu_dtype()
torch.autocast(self,device_type:str,dtype:Optional[_dtype]=None,enabled:bool=True,cache_enabled:Optional[bool]=None)
torch.autocast.__enter__(self)
torch.autocast.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)
torch.autocast_decorator(autocast_instance,func)
torch.autocast_mode.autocast(self,device_type:str,dtype:Optional[_dtype]=None,enabled:bool=True,cache_enabled:Optional[bool]=None)
torch.autocast_mode.autocast.__enter__(self)
torch.autocast_mode.autocast.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)
torch.autocast_mode.autocast.__init__(self,device_type:str,dtype:Optional[_dtype]=None,enabled:bool=True,cache_enabled:Optional[bool]=None)
torch.autocast_mode.autocast_decorator(autocast_instance,func)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_python_dispatcher.py----------------------------------------
A:torch._python_dispatcher.self.ref->torch._C._dispatch_library('FRAGMENT', self.namespace, '')
A:torch._python_dispatcher.output->self._format_header('Computed Dispatch Table')
A:torch._python_dispatcher.state->self.rawRegistrations()
A:torch._python_dispatcher.state_entries->self.rawRegistrations().split('\n')
A:torch._python_dispatcher.table->self.rawDispatchTable()
A:torch._python_dispatcher.table_entries->self.rawDispatchTable().split('\n')
A:torch._python_dispatcher.regex->re.compile('registered at .*FallbackKernel\\.cpp.*(\\[)')
A:torch._python_dispatcher.entry->re.compile('registered at .*FallbackKernel\\.cpp.*(\\[)').sub('[', line)
torch._python_dispatcher.PythonDispatcher(self)
torch._python_dispatcher.PythonDispatcher.__init__(self)
torch._python_dispatcher.PythonDispatcher._format_header(self,header)
torch._python_dispatcher.PythonDispatcher._format_line(self,key,kernel)
torch._python_dispatcher.PythonDispatcher.dispatchTable(self)
torch._python_dispatcher.PythonDispatcher.keys(self)
torch._python_dispatcher.PythonDispatcher.rawDispatchTable(self)
torch._python_dispatcher.PythonDispatcher.rawRegistrations(self)
torch._python_dispatcher.PythonDispatcher.register(self,dispatchKeys)
torch._python_dispatcher.PythonDispatcher.registrations(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_sources.py----------------------------------------
A:torch._sources.filename->inspect.getsourcefile(obj)
A:torch._sources.(sourcelines, file_lineno)->inspect.getsourcelines(obj)
A:torch._sources.(sourcelines, file_lineno, filename)->get_source_lines_and_file(fn, ErrorReport.call_stack())
A:torch._sources.sourcelines->normalize_source_lines(sourcelines)
A:torch._sources.source->''.join(sourcelines)
A:torch._sources.dedent_src->dedent(source)
A:torch._sources.py_ast->ast.parse(dedent_src)
A:torch._sources.ctx->make_source_context(source, filename, file_lineno, leading_whitespace_len, True, fn.__name__)
torch._sources.ParsedDef(NamedTuple)
torch._sources.SourceContext(self,source,filename,file_lineno,leading_whitespace_len,uses_true_division=True,funcname=None)
torch._sources.SourceContext.__init__(self,source,filename,file_lineno,leading_whitespace_len,uses_true_division=True,funcname=None)
torch._sources.fake_range()
torch._sources.get_source_lines_and_file(obj:Any,error_msg:Optional[str]=None)->Tuple[List[str], int, Optional[str]]
torch._sources.make_source_context(*args)
torch._sources.normalize_source_lines(sourcelines:List[str])->List[str]
torch._sources.parse_def(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/torch_version.py----------------------------------------
A:torch.torch_version.Version->_LazyImport('Version')
A:torch.torch_version.InvalidVersion->_LazyImport('InvalidVersion')
A:torch.torch_version.__version__->TorchVersion(internal_version)
torch.torch_version.TorchVersion(str)
torch.torch_version.TorchVersion._cmp_wrapper(self,cmp:Any,method:str)->bool
torch.torch_version.TorchVersion._convert_to_version(self,inp:Any)->Any
torch.torch_version._LazyImport(self,cls_name:str)
torch.torch_version._LazyImport.__init__(self,cls_name:str)
torch.torch_version._LazyImport.__instancecheck__(self,obj)
torch.torch_version._LazyImport.get_cls(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/overrides.py----------------------------------------
A:torch.overrides.ignored->get_ignored_functions()
A:torch.overrides.func->getattr(namespace, func_name)
A:torch.overrides.relevant_args->dispatcher(*args, **kwargs)
A:torch.overrides.arg_type->type(arg)
A:torch.overrides.index->len(overloaded_args)
A:torch.overrides.overloaded_args->_get_overloaded_args(relevant_args)
A:torch.overrides.types->tuple(map(type, overloaded_args))
A:torch.overrides.result->torch_func_method(public_api, types, args, kwargs)
A:torch.overrides.func_name->'{}.{}'.format(public_api.__module__, public_api.__name__)
A:torch.overrides.has_torch_function->_add_docstr(_has_torch_function, 'Check for __torch_function__ implementations in the elements of an iterable.\n    Considers exact ``Tensor`` s and ``Parameter`` s non-dispatchable.\n    Arguments\n    ---------\n    relevant_args : iterable\n        Iterable or aguments to check for __torch_function__ methods.\n    Returns\n    -------\n    bool\n        True if any of the elements of relevant_args have __torch_function__\n        implementations, False otherwise.\n    See Also\n    ________\n    torch.is_tensor_like\n        Checks if something is a Tensor-like, including an exact ``Tensor``.\n    ')
A:torch.overrides.has_torch_function_unary->_add_docstr(_has_torch_function_unary, 'Special case of `has_torch_function` for single inputs.\n    Instead of:\n      `has_torch_function((t,))`\n    call:\n      `has_torch_function_unary(t)`\n    which skips unnecessary packing and unpacking work.\n    ')
A:torch.overrides.has_torch_function_variadic->_add_docstr(_has_torch_function_variadic, 'Special case of `has_torch_function` that skips tuple creation.\n\n    This uses the METH_FASTCALL protocol introduced in Python 3.7; for 3.6\n    and before it has roughly equivilent performance compared to\n    `has_torch_function`.\n\n    Instead of:\n      `has_torch_function((a, b))`\n    call:\n      `has_torch_function_variadic(a, b)`\n    which skips unnecessary packing and unpacking work.\n    ')
A:torch.overrides.overridable_funcs->get_overridable_functions()
A:torch.overrides.methods->set(overridable_funcs[torch.Tensor])
torch.handle_torch_function(public_api:Callable,relevant_args:Iterable[Any],*args,**kwargs)->Any
torch.overrides._get_overloaded_args(relevant_args:Iterable[Any])->List[Any]
torch.overrides._get_tensor_methods()->Set[Callable]
torch.overrides.get_default_nowrap_functions()->Set[Callable]
torch.overrides.get_ignored_functions()->Set[Callable]
torch.overrides.get_overridable_functions()->Dict[Any, List[Callable]]
torch.overrides.get_testing_overrides()->Dict[Callable, Callable]
torch.overrides.handle_torch_function(public_api:Callable,relevant_args:Iterable[Any],*args,**kwargs)->Any
torch.overrides.is_tensor_like(inp)
torch.overrides.is_tensor_method_or_property(func:Callable)->bool
torch.overrides.wrap_torch_function(dispatcher:Callable)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/return_types.py----------------------------------------
A:torch.return_types.globals()[name]->getattr(return_types, name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_lobpcg.py----------------------------------------
A:torch._lobpcg.Ut->self._get_svqb(U, False, tau_replace).mT.contiguous()
A:torch._lobpcg.res->_symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)
A:torch._lobpcg.poly_coeffs_shape->list(roots.shape)
A:torch._lobpcg.poly_coeffs->roots.new_zeros(poly_coeffs_shape)
A:torch._lobpcg.out->poly_coeffs_new.narrow(-1, poly_order - i, i + 1)
A:torch._lobpcg.zero_power->x.new_ones(1).expand(x.shape)
A:torch._lobpcg.gen->torch.Generator(A.device)
A:torch._lobpcg.U_ortho->proj_U_ortho.matmul(torch.randn((*A.shape[:-1], A.size(-1) - D.size(-1)), dtype=A.dtype, device=A.device, generator=gen))
A:torch._lobpcg.U_ortho_t->proj_U_ortho.matmul(torch.randn((*A.shape[:-1], A.size(-1) - D.size(-1)), dtype=A.dtype, device=A.device, generator=gen)).mT.contiguous()
A:torch._lobpcg.chr_poly_D->_polynomial_coefficients_given_roots(D)
A:torch._lobpcg.series_acc->A.matmul(U_grad_projected).new_zeros(U_grad_projected.shape)
A:torch._lobpcg.poly_D->_vector_polynomial_value(chr_poly_D[..., k:], D)
A:torch._lobpcg.U_grad_projected->A.matmul(U_grad_projected)
A:torch._lobpcg.chr_poly_D_at_A->_matrix_polynomial_value(chr_poly_D, A)
A:torch._lobpcg.chr_poly_D_at_A_to_U_ortho->torch.matmul(U_ortho_t, torch.matmul(chr_poly_D_at_A, U_ortho))
A:torch._lobpcg.chr_poly_D_at_A_to_U_ortho_L->torch.linalg.cholesky(chr_poly_D_at_A_to_U_ortho_sign * chr_poly_D_at_A_to_U_ortho)
A:torch._lobpcg.(D, U)->_lobpcg(A, k, B, X, n, iK, niter, tol, largest, method, tracker, ortho_iparams, ortho_fparams, ortho_bparams)
A:torch._lobpcg.A_grad->_symeig_backward(D_grad, U_grad, A, D, U, largest)
A:torch._lobpcg.dtype->_utils.get_floating_dtype(A)
A:torch._lobpcg.iparams['ortho_i_max']->iparams.get('ortho_i_max', 3)
A:torch._lobpcg.iparams['ortho_j_max']->iparams.get('ortho_j_max', 3)
A:torch._lobpcg.fparams['ortho_tol']->fparams.get('ortho_tol', tol)
A:torch._lobpcg.fparams['ortho_tol_drop']->fparams.get('ortho_tol_drop', tol)
A:torch._lobpcg.fparams['ortho_tol_replace']->fparams.get('ortho_tol_replace', tol)
A:torch._lobpcg.bparams['ortho_use_drop']->bparams.get('ortho_use_drop', False)
A:torch._lobpcg.N->int(torch.prod(torch.tensor(A.shape[:-2])))
A:torch._lobpcg.bA->A.reshape((N,) + A.shape[-2:])
A:torch._lobpcg.bE->torch.empty((N, k), dtype=dtype, device=device)
A:torch._lobpcg.bXret->torch.empty((N, m, k), dtype=dtype, device=device)
A:torch._lobpcg.worker->LOBPCG(A, B, X, iK, iparams, fparams, bparams, method, tracker)
A:torch._lobpcg.self.E->torch.zeros((n,), dtype=X.dtype, device=X.device)
A:torch._lobpcg.self.R->torch.zeros((m, n), dtype=X.dtype, device=X.device)
A:torch._lobpcg.self.S->torch.zeros((m, 3 * n), dtype=X.dtype, device=X.device)
A:torch._lobpcg.X_norm->float(torch.norm(self.X))
A:torch._lobpcg.Ri->self._get_rayleigh_ritz_transform(self.X)
A:torch._lobpcg.M->_utils.qform(_utils.qform(self.A, self.X), Ri)
A:torch._lobpcg.(E, Z)->_utils.symeig(DUBUD)
A:torch._lobpcg.self.X[:]->mm(self.X, mm(Ri, Z))
A:torch._lobpcg.nc->self.update_converged_count()
A:torch._lobpcg.W->self._get_ortho(self.R[:, nc:], self.S[:, :n + np])
A:torch._lobpcg.(E_, Z)->_utils.symeig(_utils.qform(self.A, S_), largest)
A:torch._lobpcg.self.X[:, nc:]->mm(S_, Z[:, :n - nc])
A:torch._lobpcg.P->mm(S_, mm(Z[:, n - nc:], _utils.basis(_utils.transpose(Z[:n - nc, n - nc:]))))
A:torch._lobpcg.self.X->mm(self.X, mm(Ri, Z))
A:torch._lobpcg.SBS->_utils.qform(B, S)
A:torch._lobpcg.d_col->(d ** (-0.5)).reshape(d.shape[0], 1)
A:torch._lobpcg.R->torch.linalg.cholesky(SBS * d_row * d_col, upper=True)
A:torch._lobpcg.Id->torch.eye(R.size(-1), dtype=R.dtype, device=R.device)
A:torch._lobpcg.UBU->mm(_utils.transpose(U), BU)
A:torch._lobpcg.d->mm(_utils.transpose(U), BU).diagonal(0, -2, -1)
A:torch._lobpcg.nz->torch.where(abs(d) != 0.0)
A:torch._lobpcg.keep->torch.where(E > t)
A:torch._lobpcg.BV_norm->torch.norm(mm_B(self.B, V))
A:torch._lobpcg.BU->mm_B(self.B, U)
A:torch._lobpcg.VBU->mm(_utils.transpose(V), BU)
A:torch._lobpcg.U->self._get_svqb(U, False, tau_replace)
A:torch._lobpcg.U_norm->torch.norm(U)
A:torch._lobpcg.BU_norm->torch.norm(BU)
A:torch._lobpcg.R_norm->torch.norm(R)
A:torch._lobpcg.vkey->'ortho_VBU_rerr[{}]'.format(i)
A:torch._lobpcg.VBU_norm->torch.norm(VBU)
torch._lobpcg.LOBPCG(self,A,B,X,iK,iparams,fparams,bparams,method,tracker)
torch._lobpcg.LOBPCG.__init__(self,A,B,X,iK,iparams,fparams,bparams,method,tracker)
torch._lobpcg.LOBPCG.__str__(self)
torch._lobpcg.LOBPCG._get_ortho(self,U,V)
torch._lobpcg.LOBPCG._get_rayleigh_ritz_transform(self,S)
torch._lobpcg.LOBPCG._get_svqb(self,U,drop,tau)
torch._lobpcg.LOBPCG._update_basic(self)
torch._lobpcg.LOBPCG._update_ortho(self)
torch._lobpcg.LOBPCG.call_tracker(self)
torch._lobpcg.LOBPCG.run(self)
torch._lobpcg.LOBPCG.stop_iteration(self)
torch._lobpcg.LOBPCG.update(self)
torch._lobpcg.LOBPCG.update_converged_count(self)
torch._lobpcg.LOBPCG.update_residual(self)
torch._lobpcg.LOBPCGAutogradFunction(torch.autograd.Function)
torch._lobpcg.LOBPCGAutogradFunction.backward(ctx,D_grad,U_grad)
torch._lobpcg.LOBPCGAutogradFunction.forward(ctx,A:Tensor,k:Optional[int]=None,B:Optional[Tensor]=None,X:Optional[Tensor]=None,n:Optional[int]=None,iK:Optional[Tensor]=None,niter:Optional[int]=None,tol:Optional[float]=None,largest:Optional[bool]=None,method:Optional[str]=None,tracker:None=None,ortho_iparams:Optional[Dict[str,int]]=None,ortho_fparams:Optional[Dict[str,float]]=None,ortho_bparams:Optional[Dict[str,bool]]=None)->Tuple[Tensor, Tensor]
torch._lobpcg.LOBPCG_call_tracker(self)
torch._lobpcg._lobpcg(A:Tensor,k:Optional[int]=None,B:Optional[Tensor]=None,X:Optional[Tensor]=None,n:Optional[int]=None,iK:Optional[Tensor]=None,niter:Optional[int]=None,tol:Optional[float]=None,largest:Optional[bool]=None,method:Optional[str]=None,tracker:None=None,ortho_iparams:Optional[Dict[str,int]]=None,ortho_fparams:Optional[Dict[str,float]]=None,ortho_bparams:Optional[Dict[str,bool]]=None)->Tuple[Tensor, Tensor]
torch._lobpcg._matrix_polynomial_value(poly,x,zero_power=None)
torch._lobpcg._polynomial_coefficients_given_roots(roots)
torch._lobpcg._polynomial_value(poly,x,zero_power,transition)
torch._lobpcg._symeig_backward(D_grad,U_grad,A,D,U,largest)
torch._lobpcg._symeig_backward_complete_eigenspace(D_grad,U_grad,A,D,U)
torch._lobpcg._symeig_backward_partial_eigenspace(D_grad,U_grad,A,D,U,largest)
torch._lobpcg._vector_polynomial_value(poly,x,zero_power=None)
torch._lobpcg.lobpcg(A:Tensor,k:Optional[int]=None,B:Optional[Tensor]=None,X:Optional[Tensor]=None,n:Optional[int]=None,iK:Optional[Tensor]=None,niter:Optional[int]=None,tol:Optional[float]=None,largest:Optional[bool]=None,method:Optional[str]=None,tracker:None=None,ortho_iparams:Optional[Dict[str,int]]=None,ortho_fparams:Optional[Dict[str,float]]=None,ortho_bparams:Optional[Dict[str,bool]]=None)->Tuple[Tensor, Tensor]
torch.lobpcg(A:Tensor,k:Optional[int]=None,B:Optional[Tensor]=None,X:Optional[Tensor]=None,n:Optional[int]=None,iK:Optional[Tensor]=None,niter:Optional[int]=None,tol:Optional[float]=None,largest:Optional[bool]=None,method:Optional[str]=None,tracker:None=None,ortho_iparams:Optional[Dict[str,int]]=None,ortho_fparams:Optional[Dict[str,float]]=None,ortho_bparams:Optional[Dict[str,bool]]=None)->Tuple[Tensor, Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_six.py----------------------------------------
torch._six.with_metaclass(meta:type,*bases)->type


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_torch_docs.py----------------------------------------
A:torch._torch_docs.regx->re.compile('\\n\\s{4}(?!\\s)')
A:torch._torch_docs.common_args->parse_kwargs('\n    input (Tensor): the input tensor.\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned tensor. Default: ``torch.preserve_format``.\n')
A:torch._torch_docs.reduceops_common_args->merge_dicts(common_args, parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n'))
A:torch._torch_docs.multi_dim_common->merge_dicts(reduceops_common_args, parse_kwargs('\n    dim (int or tuple of ints): the dimension or dimensions to reduce.\n'), {'keepdim_details': '\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n'})
A:torch._torch_docs.single_dim_common->merge_dicts(reduceops_common_args, parse_kwargs('\n    dim (int): the dimension to reduce.\n'), {'keepdim_details': 'If :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\nthe output tensor having 1 fewer dimension than :attr:`input`.'})
A:torch._torch_docs.factory_common_args->merge_dicts(common_args, parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.contiguous_format``.\n'))
A:torch._torch_docs.factory_like_common_args->parse_kwargs('\n    input (Tensor): the size of :attr:`input` will determine size of the output tensor.\n    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n        Default: if ``None``, defaults to the layout of :attr:`input`.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n        Default: if ``None``, defaults to the dtype of :attr:`input`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, defaults to the device of :attr:`input`.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.\n')
A:torch._torch_docs.factory_data_common_args->parse_kwargs('\n    data (array_like): Initial data for the tensor. Can be a list, tuple,\n        NumPy ``ndarray``, scalar, and other types.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, infers data type from :attr:`data`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
torch._torch_docs.merge_dicts(*dicts)
torch._torch_docs.parse_kwargs(desc)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/functional.py----------------------------------------
A:torch.functional.scalar->torch.zeros((), device='cpu')
A:torch.functional.tensors->broadcast_tensors(*tensors)
A:torch.functional.equation->','.join((''.join((parse_subscript(s) for s in l)) for l in args[1::2]))
A:torch.functional.bins->list(itertools.repeat(bins, input.size()[-1]))
A:torch.functional.bin_edges->torch._VF._histogramdd_bin_edges(input, bins, range=range, weight=weight, density=density)
A:torch.functional.hist->torch._VF._histogramdd_from_bin_tensors(input, bin_edges, weight=weight, density=density)
A:torch.functional.histogramdd_return_type->namedtuple('histogramdd_return_type', 'hist bin_edges')
A:torch.functional.signal_dim->input.view(input.shape[-signal_dim:]).dim()
A:torch.functional.pad->int(n_fft // 2)
A:torch.functional.input->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:])
A:torch.functional.(output, inverse_indices, counts)->torch._VF.unique_consecutive(input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
A:torch.functional.(output, _, counts)->_unique_consecutive_impl(input, return_inverse, return_counts, dim)
A:torch.functional.(output, _, _)->_unique_consecutive_impl(input, return_inverse, return_counts, dim)
A:torch.functional.(output, inverse_indices, _)->_unique_consecutive_impl(input, return_inverse, return_counts, dim)
A:torch.functional._return_inverse_false->boolean_dispatch(arg_name='return_counts', arg_index=3, default=False, if_true=_return_counts, if_false=_return_output, module_name=__name__, func_name='unique')
A:torch.functional._return_inverse_true->boolean_dispatch(arg_name='return_counts', arg_index=3, default=False, if_true=_unique_impl, if_false=_return_inverse, module_name=__name__, func_name='unique')
A:torch.functional.unique->boolean_dispatch(arg_name='return_inverse', arg_index=2, default=False, if_true=_return_inverse_true, if_false=_return_inverse_false, module_name=__name__, func_name='unique')
A:torch.functional._consecutive_return_inverse_false->boolean_dispatch(arg_name='return_counts', arg_index=1, default=False, if_true=_consecutive_return_counts, if_false=_consecutive_return_output, module_name=__name__, func_name='unique_consecutive')
A:torch.functional._consecutive_return_inverse_true->boolean_dispatch(arg_name='return_counts', arg_index=1, default=False, if_true=_unique_consecutive_impl, if_false=_consecutive_return_inverse, module_name=__name__, func_name='unique_consecutive')
A:torch.functional.unique_consecutive->boolean_dispatch(arg_name='return_inverse', arg_index=2, default=False, if_true=_consecutive_return_inverse_true, if_false=_consecutive_return_inverse_false, module_name=__name__, func_name='unique_consecutive')
A:torch.functional.num_elements->dims.numel()
A:torch.functional.dims_a->list(range(-dims, 0))
A:torch.functional.dims_b->list(range(dims))
A:torch.functional.dims_val->int(dims.item())
A:torch.functional.ndim->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:]).dim()
A:torch.functional._dim->list(range(ndim))
A:torch.functional.result->_lu_impl(A, pivot, get_infos, out)
A:torch.functional.lu->boolean_dispatch(arg_name='get_infos', arg_index=2, default=False, if_true=_lu_with_infos, if_false=_lu_no_infos, module_name=__name__, func_name='lu')
torch._lu_impl(A,pivot=True,get_infos=False,out=None)
torch.block_diag(*tensors)
torch.cdist(x1,x2,p=2.0,compute_mode='use_mm_for_euclid_dist_if_necessary')
torch.chain_matmul(*matrices,out=None)
torch.functional._check_list_size(out_len:int,get_infos:bool,out:_ListOrSeq)->None
torch.functional._consecutive_return_counts(input,return_inverse=False,return_counts=False,dim=None)
torch.functional._consecutive_return_inverse(input,return_inverse=False,return_counts=False,dim=None)
torch.functional._consecutive_return_output(input,return_inverse=False,return_counts=False,dim=None)
torch.functional._lu_impl(A,pivot=True,get_infos=False,out=None)
torch.functional._lu_no_infos(A,pivot=True,get_infos=False,out=None)
torch.functional._lu_with_infos(A,pivot=True,get_infos=False,out=None)
torch.functional._meshgrid(*tensors,indexing:Optional[str])
torch.functional._return_counts(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional._return_inverse(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional._return_output(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional._unique_consecutive_impl(input:Tensor,return_inverse:bool=False,return_counts:bool=False,dim:Optional[int]=None)->_unique_impl_out
torch.functional._unique_impl(input:Tensor,sorted:bool=True,return_inverse:bool=False,return_counts:bool=False,dim:Optional[int]=None)->_unique_impl_out
torch.functional.align_tensors(*tensors)
torch.functional.atleast_1d(*tensors)
torch.functional.atleast_2d(*tensors)
torch.functional.atleast_3d(*tensors)
torch.functional.block_diag(*tensors)
torch.functional.broadcast_shapes(*shapes)
torch.functional.broadcast_tensors(*tensors)
torch.functional.cartesian_prod(*tensors)
torch.functional.cdist(x1,x2,p=2.0,compute_mode='use_mm_for_euclid_dist_if_necessary')
torch.functional.chain_matmul(*matrices,out=None)
torch.functional.einsum(*args)
torch.functional.istft(input:Tensor,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:Optional[Tensor]=None,center:bool=True,normalized:bool=False,onesided:Optional[bool]=None,length:Optional[int]=None,return_complex:bool=False)->Tensor
torch.functional.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)
torch.functional.split(tensor,split_size_or_sections,dim=0)
torch.functional.stft(input:Tensor,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:Optional[Tensor]=None,center:bool=True,pad_mode:str='reflect',normalized:bool=False,onesided:Optional[bool]=None,return_complex:Optional[bool]=None)->Tensor
torch.functional.tensordot(a,b,dims=2,out:Optional[torch.Tensor]=None)
torch.istft(input:Tensor,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:Optional[Tensor]=None,center:bool=True,normalized:bool=False,onesided:Optional[bool]=None,length:Optional[int]=None,return_complex:bool=False)->Tensor
torch.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)
torch.split(tensor,split_size_or_sections,dim=0)
torch.stft(input:Tensor,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:Optional[Tensor]=None,center:bool=True,pad_mode:str='reflect',normalized:bool=False,onesided:Optional[bool]=None,return_complex:Optional[bool]=None)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_vmap_internals.py----------------------------------------
A:torch._vmap_internals.(flat_args, args_spec)->tree_flatten(args)
A:torch._vmap_internals.flat_in_dims->_broadcast_to_and_flatten(in_dims, args_spec)
A:torch._vmap_internals.batch_size->_validate_and_get_batch_size(flat_in_dims, flat_args)
A:torch._vmap_internals.num_outputs->_num_outputs(batched_outputs)
A:torch._vmap_internals.out_dims_as_tuple->_as_tuple(out_dims, num_outputs, lambda : f'vmap({_get_name(func)}, ..., out_dims={out_dims}): `out_dims` must have one dim per output (got {num_outputs} outputs) of {_get_name(func)}.')
A:torch._vmap_internals.vmap_level->torch._C._vmapmode_increment_nesting()
A:torch._vmap_internals.(batched_inputs, batch_size)->_create_batched_inputs(in_dims, args, vmap_level, func)
A:torch._vmap_internals.batched_outputs->func(*batched_inputs)
torch._vmap_internals._as_tuple(value:Any,num_elements:int,error_message_lambda:Callable[[],str])->Tuple
torch._vmap_internals._check_out_dims_is_int_or_int_tuple(out_dims:out_dims_t,func:Callable)->None
torch._vmap_internals._create_batched_inputs(in_dims:in_dims_t,args:Tuple,vmap_level:int,func:Callable)->Tuple[Tuple, int]
torch._vmap_internals._get_name(func:Callable)
torch._vmap_internals._num_outputs(batched_outputs:Union[Tensor,Tuple[Tensor,...]])->int
torch._vmap_internals._unwrap_batched(batched_outputs:Union[Tensor,Tuple[Tensor,...]],out_dims:out_dims_t,vmap_level:int,batch_size:int,func:Callable,allow_none_pass_through:bool=False)->Tuple
torch._vmap_internals._validate_and_get_batch_size(flat_in_dims:List[Optional[int]],flat_args:List)->int
torch._vmap_internals._validate_outputs(outputs:Any,func:Callable)->None
torch._vmap_internals._vmap(func:Callable,in_dims:in_dims_t=0,out_dims:out_dims_t=0,allow_none_pass_through:bool=False)->Callable
torch._vmap_internals.vmap(func:Callable,in_dims:in_dims_t=0,out_dims:out_dims_t=0)->Callable


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_tensor.py----------------------------------------
A:torch._tensor.ret->type(ret)((_convert(r, cls) for r in ret))
A:torch._tensor.new_tensor->new_tensor.as_subclass(type(self)).as_subclass(type(self))
A:torch._tensor.new_storage->self.storage().__deepcopy__(memo)
A:torch._tensor.new_tensor.grad->self.grad.__deepcopy__(memo)
A:torch._tensor.slots_to_save->copyreg._slotnames(self.__class__)
A:torch._tensor.new_tensor.__dict__->deepcopy(self.__dict__, memo)
A:torch._tensor.(func, args)->self._reduce_ex_internal(proto)
A:torch._tensor.getstate_fn->getattr(self, '__getstate__', None)
A:torch._tensor.state->getstate_fn()
A:torch._tensor.storage->storage_class(wrap_storage=storage)
A:torch._tensor.storage_class->eval(type(storage).__module__ + '.' + storage_name)
A:torch._tensor.self._backward_hooks->OrderedDict()
A:torch._tensor.handle->torch.utils.hooks.RemovableHandle(self._backward_hooks)
A:torch._tensor.detach->torch._C._add_docstr(_C._TensorBase.detach, '\n    Returns a new Tensor, detached from the current graph.\n\n    The result will never require gradient.\n\n    This method also affects forward mode AD gradients and the result will never\n    have forward mode AD gradients.\n\n    .. note::\n\n      Returned Tensor shares the same storage with the original one.\n      In-place modifications on either of them will be seen, and may trigger\n      errors in correctness checks.\n      IMPORTANT NOTE: Previously, in-place size / stride / storage changes\n      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor\n      also update the original tensor. Now, these in-place changes will not update the\n      original tensor anymore, and will instead trigger an error.\n      For sparse tensors:\n      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the\n      returned tensor will not update the original tensor anymore, and will instead\n      trigger an error.\n    ')
A:torch._tensor.detach_->torch._C._add_docstr(_C._TensorBase.detach_, '\n    Detaches the Tensor from the graph that created it, making it a leaf.\n    Views cannot be detached in-place.\n\n    This method also affects forward mode AD gradients and the result will never\n    have forward mode AD gradients.\n    ')
A:torch._tensor.(LU, pivots, infos)->torch._lu_with_info(self, pivot=pivot, check_errors=not get_infos)
A:torch._tensor.split_size->int(split_size)
A:torch._tensor.__pow__->_wrap_type_error_to_not_implemented(_C._TensorBase.pow)
A:torch._tensor.dtype->torch.result_type(other, self)
A:torch._tensor.tensor_methods->dir(self.__class__)
A:torch._tensor.attrs->list(self.__dict__.keys())
A:torch._tensor.array->array.astype('uint8').astype('uint8')
A:torch._tensor.itemsize->self.storage().element_size()
A:torch._tensor.shape->self.size()
A:torch._tensor.strides->tuple((s * itemsize for s in self.stride()))
A:torch._tensor.names->resolve_ellipsis(names, self.names, 'refine_names')
A:torch._tensor.ellipsis_idx->single_ellipsis_index(names, 'align_to')
A:torch._tensor.(names, sizes)->unzip_namedshape(sizes)
A:torch._tensor.crow_indices->torch._convert_indices_from_coo_to_csr(row_indices, self.shape[0], out_int32=row_indices.dtype == torch.int32)
A:torch._tensor.col_indices->self.col_indices()
A:torch._tensor.indices->torch._convert_indices_from_csr_to_coo(crow_indices, col_indices, out_int32=crow_indices.dtype == torch.int32)
A:torch._tensor.coalesced_self->self.coalesce()
A:torch._tensor.stream->torch.cuda.ExternalStream(stream)
A:torch._tensor.event->torch.cuda.Event()
torch.Tensor(torch._C._TensorBase)
torch.Tensor.__array__(self,dtype=None)
torch.Tensor.__array_wrap__(self,array)
torch.Tensor.__contains__(self,element)
torch.Tensor.__deepcopy__(self,memo)
torch.Tensor.__dir__(self)
torch.Tensor.__dlpack__(self,stream=None)
torch.Tensor.__dlpack_device__(self)->Tuple[enum.IntEnum, int]
torch.Tensor.__floordiv__(self,other)
torch.Tensor.__format__(self,format_spec)
torch.Tensor.__hash__(self)
torch.Tensor.__ipow__(self,other)
torch.Tensor.__iter__(self)
torch.Tensor.__len__(self)
torch.Tensor.__rdiv__(self,other)
torch.Tensor.__reduce_ex__(self,proto)
torch.Tensor.__repr__(self)
torch.Tensor.__reversed__(self)
torch.Tensor.__rfloordiv__(self,other)
torch.Tensor.__rlshift__(self,other)
torch.Tensor.__rmatmul__(self,other)
torch.Tensor.__rmod__(self,other)
torch.Tensor.__rpow__(self,other)
torch.Tensor.__rrshift__(self,other)
torch.Tensor.__rsub__(self,other)
torch.Tensor.__setstate__(self,state)
torch.Tensor.__torch_function__(cls,func,types,args=(),kwargs=None)
torch.Tensor._reduce_ex_internal(self,proto)
torch.Tensor._update_names(self,names,inplace)
torch.Tensor.align_to(self,*names)
torch.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False,inputs=None)
torch.Tensor.grad(self)
torch.Tensor.grad(self)
torch.Tensor.grad(self,new_grad)
torch.Tensor.is_shared(self)
torch.Tensor.istft(self,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:'Optional[Tensor]'=None,center:bool=True,normalized:bool=False,onesided:Optional[bool]=None,length:Optional[int]=None,return_complex:bool=False)
torch.Tensor.lu(self,pivot=True,get_infos=False)
torch.Tensor.norm(self,p='fro',dim=None,keepdim=False,dtype=None)
torch.Tensor.refine_names(self,*names)
torch.Tensor.register_hook(self,hook)
torch.Tensor.reinforce(self,reward)
torch.Tensor.rename(self,*names,**rename_map)
torch.Tensor.rename_(self,*names,**rename_map)
torch.Tensor.resize(self,*sizes)
torch.Tensor.resize_as(self,tensor)
torch.Tensor.share_memory_(self)
torch.Tensor.split(self,split_size,dim=0)
torch.Tensor.stft(self,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:'Optional[Tensor]'=None,center:bool=True,pad_mode:str='reflect',normalized:bool=False,onesided:Optional[bool]=None,return_complex:Optional[bool]=None)
torch.Tensor.storage(self)
torch.Tensor.storage_type(self)
torch.Tensor.unflatten(self,dim,sizes)
torch.Tensor.unique(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.Tensor.unique_consecutive(self,return_inverse=False,return_counts=False,dim=None)
torch._tensor.Tensor(torch._C._TensorBase)
torch._tensor.Tensor.__array__(self,dtype=None)
torch._tensor.Tensor.__array_wrap__(self,array)
torch._tensor.Tensor.__contains__(self,element)
torch._tensor.Tensor.__cuda_array_interface__(self)
torch._tensor.Tensor.__deepcopy__(self,memo)
torch._tensor.Tensor.__dir__(self)
torch._tensor.Tensor.__dlpack__(self,stream=None)
torch._tensor.Tensor.__dlpack_device__(self)->Tuple[enum.IntEnum, int]
torch._tensor.Tensor.__floordiv__(self,other)
torch._tensor.Tensor.__format__(self,format_spec)
torch._tensor.Tensor.__hash__(self)
torch._tensor.Tensor.__ipow__(self,other)
torch._tensor.Tensor.__iter__(self)
torch._tensor.Tensor.__len__(self)
torch._tensor.Tensor.__rdiv__(self,other)
torch._tensor.Tensor.__reduce_ex__(self,proto)
torch._tensor.Tensor.__repr__(self)
torch._tensor.Tensor.__reversed__(self)
torch._tensor.Tensor.__rfloordiv__(self,other)
torch._tensor.Tensor.__rlshift__(self,other)
torch._tensor.Tensor.__rmatmul__(self,other)
torch._tensor.Tensor.__rmod__(self,other)
torch._tensor.Tensor.__rpow__(self,other)
torch._tensor.Tensor.__rrshift__(self,other)
torch._tensor.Tensor.__rsub__(self,other)
torch._tensor.Tensor.__setstate__(self,state)
torch._tensor.Tensor.__torch_function__(cls,func,types,args=(),kwargs=None)
torch._tensor.Tensor._reduce_ex_internal(self,proto)
torch._tensor.Tensor._update_names(self,names,inplace)
torch._tensor.Tensor.align_to(self,*names)
torch._tensor.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False,inputs=None)
torch._tensor.Tensor.grad(self)
torch._tensor.Tensor.grad(self)
torch._tensor.Tensor.grad(self,new_grad)
torch._tensor.Tensor.is_shared(self)
torch._tensor.Tensor.istft(self,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:'Optional[Tensor]'=None,center:bool=True,normalized:bool=False,onesided:Optional[bool]=None,length:Optional[int]=None,return_complex:bool=False)
torch._tensor.Tensor.lu(self,pivot=True,get_infos=False)
torch._tensor.Tensor.norm(self,p='fro',dim=None,keepdim=False,dtype=None)
torch._tensor.Tensor.refine_names(self,*names)
torch._tensor.Tensor.register_hook(self,hook)
torch._tensor.Tensor.reinforce(self,reward)
torch._tensor.Tensor.rename(self,*names,**rename_map)
torch._tensor.Tensor.rename_(self,*names,**rename_map)
torch._tensor.Tensor.resize(self,*sizes)
torch._tensor.Tensor.resize_as(self,tensor)
torch._tensor.Tensor.share_memory_(self)
torch._tensor.Tensor.split(self,split_size,dim=0)
torch._tensor.Tensor.stft(self,n_fft:int,hop_length:Optional[int]=None,win_length:Optional[int]=None,window:'Optional[Tensor]'=None,center:bool=True,pad_mode:str='reflect',normalized:bool=False,onesided:Optional[bool]=None,return_complex:Optional[bool]=None)
torch._tensor.Tensor.storage(self)
torch._tensor.Tensor.storage_type(self)
torch._tensor.Tensor.to_sparse_coo(self)
torch._tensor.Tensor.to_sparse_csr(self)
torch._tensor.Tensor.unflatten(self,dim,sizes)
torch._tensor.Tensor.unique(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch._tensor.Tensor.unique_consecutive(self,return_inverse=False,return_counts=False,dim=None)
torch._tensor._convert(ret,cls)
torch._tensor._rebuild_from_type(func,type,args,dict)
torch._tensor._rebuild_from_type_v2(func,new_type,args,state)
torch._tensor._wrap_type_error_to_not_implemented(f)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_lowrank.py----------------------------------------
A:torch._lowrank.dtype->_utils.get_floating_dtype(A)
A:torch._lowrank.R->torch.randn(n, q, dtype=dtype, device=A.device)
A:torch._lowrank.A_H->_utils.transjugate(A)
A:torch._lowrank.M_H->_utils.transjugate(M)
A:torch._lowrank.M_t->_utils.transpose(M)
A:torch._lowrank.A_t->_utils.transpose(A)
A:torch._lowrank.Q->get_approximate_basis(A, q, niter=niter, M=M)
A:torch._lowrank.Q_c->_utils.conjugate(Q)
A:torch._lowrank.B_t->_utils.transpose(B)
A:torch._lowrank.(U, S, Vh)->torch.linalg.svd(B_t, full_matrices=False)
A:torch._lowrank.V->get_approximate_basis(A, q, niter=niter, M=M).matmul(V)
A:torch._lowrank.B->matmul(A_t, Q_c)
A:torch._lowrank.U->get_approximate_basis(A, q, niter=niter, M=M).matmul(U)
A:torch._lowrank.q->min(6, m, n)
A:torch._lowrank.indices->torch.zeros(2, len(column_indices), dtype=column_indices.dtype, device=column_indices.device)
A:torch._lowrank.C_t->torch.sparse_coo_tensor(indices, c.values(), (n, 1), dtype=dtype, device=A.device)
A:torch._lowrank.ones_m1_t->torch.ones(A.shape[:-2] + (1, m), dtype=dtype, device=A.device)
A:torch._lowrank.M->_utils.transpose(torch.sparse.mm(C_t, ones_m1_t))
A:torch._lowrank.C->A.mean(dim=(-2,), keepdim=True)
torch._lowrank._svd_lowrank(A:Tensor,q:Optional[int]=6,niter:Optional[int]=2,M:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor]
torch._lowrank.get_approximate_basis(A:Tensor,q:int,niter:Optional[int]=2,M:Optional[Tensor]=None)->Tensor
torch._lowrank.pca_lowrank(A:Tensor,q:Optional[int]=None,center:bool=True,niter:int=2)->Tuple[Tensor, Tensor, Tensor]
torch._lowrank.svd_lowrank(A:Tensor,q:Optional[int]=6,niter:Optional[int]=2,M:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_utils_internal.py----------------------------------------
A:torch._utils_internal.torch_parent->os.path.dirname(os.path.dirname(__file__))
torch._utils_internal.get_file_path(*path_components:str)->str
torch._utils_internal.get_file_path_2(*path_components:str)->str
torch._utils_internal.get_writable_path(path:str)->str
torch._utils_internal.prepare_multiprocessing_environment(path:str)->None
torch._utils_internal.resolve_library_path(path:str)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/version.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_storage_docs.py----------------------------------------
A:torch._storage_docs.cls->getattr(torch._C, cls_name)
torch._storage_docs.add_docstr_all(method,docstr)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/__future__.py----------------------------------------
torch.__future__.get_overwrite_module_params_on_conversion()
torch.__future__.set_overwrite_module_params_on_conversion(value)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_appdirs.py----------------------------------------
A:torch._appdirs.__version_info__->tuple((int(segment) for segment in __version__.split('.')))
A:torch._appdirs.path->os.path.join(path, version)
A:torch._appdirs.appname->os.path.join(appname, version)
A:torch._appdirs.key->winreg.OpenKey(_winreg.HKEY_CURRENT_USER, 'Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders')
A:torch._appdirs.(dir, type)->winreg.QueryValueEx(key, shell_folder_name)
A:torch._appdirs.dir->com.sun.jna.Native.toString(buf.tostring()).rstrip('\x00')
A:torch._appdirs.buf->array.zeros('c', buf_size)
A:torch._appdirs.buf2->ctypes.create_unicode_buffer(1024)
A:torch._appdirs.dirs->AppDirs(appname, appauthor=False)
torch._appdirs.AppDirs(self,appname=None,appauthor=None,version=None,roaming=False,multipath=False)
torch._appdirs.AppDirs.__init__(self,appname=None,appauthor=None,version=None,roaming=False,multipath=False)
torch._appdirs.AppDirs.site_config_dir(self)
torch._appdirs.AppDirs.site_data_dir(self)
torch._appdirs.AppDirs.user_cache_dir(self)
torch._appdirs.AppDirs.user_config_dir(self)
torch._appdirs.AppDirs.user_data_dir(self)
torch._appdirs.AppDirs.user_log_dir(self)
torch._appdirs.AppDirs.user_state_dir(self)
torch._appdirs._get_win_folder_from_registry(csidl_name)
torch._appdirs._get_win_folder_with_ctypes(csidl_name)
torch._appdirs._get_win_folder_with_jna(csidl_name)
torch._appdirs._get_win_folder_with_pywin32(csidl_name)
torch._appdirs.site_config_dir(appname=None,appauthor=None,version=None,multipath=False)
torch._appdirs.site_data_dir(appname=None,appauthor=None,version=None,multipath=False)
torch._appdirs.user_cache_dir(appname=None,appauthor=None,version=None,opinion=True)
torch._appdirs.user_config_dir(appname=None,appauthor=None,version=None,roaming=False)
torch._appdirs.user_data_dir(appname=None,appauthor=None,version=None,roaming=False)
torch._appdirs.user_log_dir(appname=None,appauthor=None,version=None,opinion=True)
torch._appdirs.user_state_dir(appname=None,appauthor=None,version=None,roaming=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quasirandom.py----------------------------------------
A:torch.quasirandom.cpu->torch.device('cpu')
A:torch.quasirandom.self.sobolstate->torch.zeros(dimension, self.MAXBIT, device=cpu, dtype=torch.long)
A:torch.quasirandom.self.shift->torch.mv(shift_ints, torch.pow(2, torch.arange(0, self.MAXBIT, device=cpu)))
A:torch.quasirandom.self.quasi->self.shift.clone(memory_format=torch.contiguous_format)
A:torch.quasirandom.self._first_point->(self.quasi / 2 ** self.MAXBIT).reshape(1, -1)
A:torch.quasirandom.result->torch.cat((self._first_point, result), dim=-2)
A:torch.quasirandom.(result, self.quasi)->torch._sobol_engine_draw(self.quasi, n, self.sobolstate, self.dimension, self.num_generated - 1, dtype=dtype)
A:torch.quasirandom.g->torch.Generator()
A:torch.quasirandom.shift_ints->torch.randint(2, (self.dimension, self.MAXBIT), device=cpu, generator=g)
A:torch.quasirandom.ltm->torch.randint(2, ltm_dims, device=cpu, generator=g).tril()
torch.quasirandom.SobolEngine(self,dimension,scramble=False,seed=None)
torch.quasirandom.SobolEngine.__init__(self,dimension,scramble=False,seed=None)
torch.quasirandom.SobolEngine.__repr__(self)
torch.quasirandom.SobolEngine._scramble(self)
torch.quasirandom.SobolEngine.draw(self,n:int=1,out:Optional[torch.Tensor]=None,dtype:torch.dtype=torch.float32)->torch.Tensor
torch.quasirandom.SobolEngine.draw_base2(self,m:int,out:Optional[torch.Tensor]=None,dtype:torch.dtype=torch.float32)->torch.Tensor
torch.quasirandom.SobolEngine.fast_forward(self,n)
torch.quasirandom.SobolEngine.reset(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/random.py----------------------------------------
A:torch.random.seed->torch._C.default_generator.seed()
A:torch.random.num_devices->torch.cuda.device_count()
A:torch.random.devices->list(devices)
A:torch.random.cpu_rng_state->torch.get_rng_state()
torch.random.fork_rng(devices=None,enabled=True,_caller='fork_rng',_devices_kw='devices')->Generator
torch.random.get_rng_state()->torch.Tensor
torch.random.initial_seed()->int
torch.random.manual_seed(seed)->torch._C.Generator
torch.random.seed()->int
torch.random.set_rng_state(new_state:torch.Tensor)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/__init__.py----------------------------------------
A:torch.__init__.pfiles_path->os.getenv('ProgramFiles', 'C:\\Program Files')
A:torch.__init__.py_dll_path->os.path.join(sys.exec_prefix, 'Library', 'bin')
A:torch.__init__.th_dll_path->os.path.join(os.path.dirname(__file__), 'lib')
A:torch.__init__.base_py_dll_path->os.path.join(sys.base_exec_prefix, 'Library', 'bin')
A:torch.__init__.dll_paths->list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))
A:torch.__init__.nvtoolsext_dll_path->os.path.join(os.getenv('NVTOOLSEXT_PATH', os.path.join(pfiles_path, 'NVIDIA Corporation', 'NvToolsExt')), 'bin', 'x64')
A:torch.__init__.cuda_version_1->version.cuda.replace('.', '_')
A:torch.__init__.default_path->os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)
A:torch.__init__.cuda_path->os.path.join(os.getenv(cuda_path_var, default_path), 'bin')
A:torch.__init__.kernel32->ctypes.WinDLL('kernel32.dll', use_last_error=True)
A:torch.__init__.with_load_library_flags->hasattr(kernel32, 'AddDllDirectory')
A:torch.__init__.prev_error_mode->ctypes.WinDLL('kernel32.dll', use_last_error=True).SetErrorMode(1)
A:torch.__init__.res->ctypes.WinDLL('kernel32.dll', use_last_error=True).LoadLibraryW(dll)
A:torch.__init__.err->ctypes.WinError(ctypes.get_last_error())
A:torch.__init__.dlls->glob.glob(os.path.join(th_dll_path, '*.dll'))
A:torch.__init__.last_error->ctypes.get_last_error()
A:torch.__init__.os.environ['PATH']->';'.join(dll_paths + [os.environ['PATH']])
A:torch.__init__.here->os.path.abspath(__file__)
A:torch.__init__.lib_path->os.path.join(os.path.dirname(here), 'lib', lib_name)
A:torch.__init__.old_flags->sys.getdlopenflags()
A:torch.__init__.candidate->getattr(_C, attr)
A:torch.__init__.t->_import_dotted_name(t)
A:torch.__init__.path->get_file_path('torch', 'bin', 'torch_shm_manager')
A:torch.__init__.globals()[name]->getattr(_C._VariableFunctions, name)
torch.__init__.BFloat16Storage(_TypedStorage)
torch.__init__.BFloat16Storage.dtype(self)
torch.__init__.BoolStorage(_TypedStorage)
torch.__init__.BoolStorage.dtype(self)
torch.__init__.ByteStorage(_TypedStorage)
torch.__init__.ByteStorage.dtype(self)
torch.__init__.CharStorage(_TypedStorage)
torch.__init__.CharStorage.dtype(self)
torch.__init__.ComplexDoubleStorage(_TypedStorage)
torch.__init__.ComplexDoubleStorage.dtype(self)
torch.__init__.ComplexFloatStorage(_TypedStorage)
torch.__init__.ComplexFloatStorage.dtype(self)
torch.__init__.DoubleStorage(_TypedStorage)
torch.__init__.DoubleStorage.dtype(self)
torch.__init__.FloatStorage(_TypedStorage)
torch.__init__.FloatStorage.dtype(self)
torch.__init__.HalfStorage(_TypedStorage)
torch.__init__.HalfStorage.dtype(self)
torch.__init__.IntStorage(_TypedStorage)
torch.__init__.IntStorage.dtype(self)
torch.__init__.LongStorage(_TypedStorage)
torch.__init__.LongStorage.dtype(self)
torch.__init__.QInt32Storage(_TypedStorage)
torch.__init__.QInt32Storage.dtype(self)
torch.__init__.QInt8Storage(_TypedStorage)
torch.__init__.QInt8Storage.dtype(self)
torch.__init__.QUInt2x4Storage(_TypedStorage)
torch.__init__.QUInt2x4Storage.dtype(self)
torch.__init__.QUInt4x2Storage(_TypedStorage)
torch.__init__.QUInt4x2Storage.dtype(self)
torch.__init__.QUInt8Storage(_TypedStorage)
torch.__init__.QUInt8Storage.dtype(self)
torch.__init__.ShortStorage(_TypedStorage)
torch.__init__.ShortStorage.dtype(self)
torch.__init__._UntypedStorage(_C.ByteStorageBase,_StorageBase)
torch.__init__._assert(condition,message)
torch.__init__._load_global_deps()
torch.__init__._register_device_module(device_type,module)
torch.__init__.are_deterministic_algorithms_enabled()
torch.__init__.compiled_with_cxx11_abi()
torch.__init__.get_deterministic_debug_mode()->builtins.int
torch.__init__.is_deterministic_algorithms_warn_only_enabled()
torch.__init__.is_storage(obj)
torch.__init__.is_tensor(obj)
torch.__init__.is_warn_always_enabled()
torch.__init__.manager_path()
torch.__init__.set_default_dtype(d)
torch.__init__.set_default_tensor_type(t)
torch.__init__.set_deterministic_debug_mode(debug_mode:Union[builtins.int,str])->None
torch.__init__.set_warn_always(b)
torch.__init__.typename(o)
torch.__init__.use_deterministic_algorithms(mode,*,warn_only=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_tensor_docs.py----------------------------------------
A:torch._tensor_docs.common_args->parse_kwargs('\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.\n')
A:torch._tensor_docs.new_common_args->parse_kwargs('\n    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n        shape of the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n        Default: if None, same :class:`torch.dtype` as this tensor.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if None, same :class:`torch.device` as this tensor.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
torch._tensor_docs.add_docstr_all(method,docstr)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_classes.py----------------------------------------
A:torch._classes.proxy->torch._C._get_custom_class_python_wrapper(self.name, attr)
A:torch._classes.namespace->_ClassNamespace(name)
A:torch._classes.classes->_Classes()
torch._classes._ClassNamespace(self,name)
torch._classes._ClassNamespace.__getattr__(self,attr)
torch._classes._ClassNamespace.__init__(self,name)
torch._classes._Classes(self)
torch._classes._Classes.__getattr__(self,name)
torch._classes._Classes.__init__(self)
torch._classes._Classes.load_library(self,path)
torch._classes._Classes.loaded_libraries(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/hub.py----------------------------------------
A:torch.hub.HASH_REGEX->re.compile('-([a-f0-9]*)\\.')
A:torch.hub.spec->importlib.util.spec_from_file_location(name, path)
A:torch.hub.module->importlib.util.module_from_spec(spec)
A:torch.hub.torch_home->os.path.expanduser(os.getenv(ENV_TORCH_HOME, os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), 'torch')))
A:torch.hub.(repo_info, branch)->github.split(':')
A:torch.hub.(repo_owner, repo_name)->repo_info.split('/')
A:torch.hub.token->os.environ.get(ENV_GITHUB_TOKEN)
A:torch.hub.response->json.loads(_read_url(Request(url, headers=headers)))
A:torch.hub.hub_dir->get_dir()
A:torch.hub.(repo_owner, repo_name, branch)->_parse_repo_info(github)
A:torch.hub.normalized_br->branch.replace('/', '_')
A:torch.hub.repo_dir->_get_cache_or_reload(github, force_reload, verbose=True, skip_validation=skip_validation)
A:torch.hub.cached_file->os.path.join(model_dir, filename)
A:torch.hub.url->_git_archive_link(repo_owner, repo_name, branch)
A:torch.hub.extracted_repo->os.path.join(hub_dir, extraced_repo_name)
A:torch.hub.dependencies->_load_attr_from_module(m, VAR_DEPENDENCY)
A:torch.hub.func->_load_attr_from_module(m, model)
A:torch.hub.hubconf_path->os.path.join(hubconf_dir, MODULE_HUBCONF)
A:torch.hub.hub_module->_import_module(MODULE_HUBCONF, hubconf_path)
A:torch.hub.entry->_load_entry_from_hubconf(hub_module, model)
A:torch.hub.source->source.lower().lower()
A:torch.hub.repo_or_dir->_get_cache_or_reload(repo_or_dir, force_reload, verbose, skip_validation)
A:torch.hub.model->entry(*args, **kwargs)
A:torch.hub.req->Request(url, headers={'User-Agent': 'torch.hub'})
A:torch.hub.u->urlopen(req)
A:torch.hub.meta->urlopen(req).info()
A:torch.hub.content_length->urlopen(req).info().get_all('Content-Length')
A:torch.hub.file_size->int(content_length[0])
A:torch.hub.dst->os.path.expanduser(dst)
A:torch.hub.dst_dir->os.path.dirname(dst)
A:torch.hub.f->tempfile.NamedTemporaryFile(delete=False, dir=dst_dir)
A:torch.hub.sha256->hashlib.sha256()
A:torch.hub.buffer->urlopen(req).read(8192)
A:torch.hub.digest->hashlib.sha256().hexdigest()
A:torch.hub.infolist->zipfile.ZipFile(filename).infolist()
A:torch.hub.members->tempfile.NamedTemporaryFile(delete=False, dir=dst_dir).infolist()
A:torch.hub.extracted_file->os.path.join(model_dir, extraced_name)
A:torch.hub.model_dir->os.path.join(hub_dir, 'checkpoints')
A:torch.hub.parts->urlparse(url)
A:torch.hub.filename->os.path.basename(parts.path)
A:torch.hub.r->re.compile('-([a-f0-9]*)\\.').search(filename)
torch.hub._check_dependencies(m)
torch.hub._check_module_exists(name)
torch.hub._download_url_to_file(url,dst,hash_prefix=None,progress=True)
torch.hub._get_cache_or_reload(github,force_reload,verbose=True,skip_validation=False)
torch.hub._get_torch_home()
torch.hub._git_archive_link(repo_owner,repo_name,branch)
torch.hub._import_module(name,path)
torch.hub._is_legacy_zip_format(filename)
torch.hub._legacy_zip_load(filename,model_dir,map_location)
torch.hub._load_attr_from_module(module,func_name)
torch.hub._load_entry_from_hubconf(m,model)
torch.hub._load_local(hubconf_dir,model,*args,**kwargs)
torch.hub._parse_repo_info(github)
torch.hub._read_url(url)
torch.hub._remove_if_exists(path)
torch.hub._validate_not_a_forked_repo(repo_owner,repo_name,branch)
torch.hub.download_url_to_file(url,dst,hash_prefix=None,progress=True)
torch.hub.get_dir()
torch.hub.help(github,model,force_reload=False,skip_validation=False)
torch.hub.import_module(name,path)
torch.hub.list(github,force_reload=False,skip_validation=False)
torch.hub.load(repo_or_dir,model,*args,source='github',force_reload=False,verbose=True,skip_validation=False,**kwargs)
torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False,file_name=None)
torch.hub.set_dir(d)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_VF.py----------------------------------------
A:torch._VF.sys.modules[__name__]->VFModule(__name__)
torch._VF.VFModule(self,name)
torch._VF.VFModule.__getattr__(self,attr)
torch._VF.VFModule.__init__(self,name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_deploy.py----------------------------------------
A:torch._deploy.importers->OrderedImporter(importer, sys_importer)
A:torch._deploy.data_buf->io.BytesIO()
A:torch._deploy.pickler->create_pickler(data_buf, importers)
A:torch._deploy.data_value->io.BytesIO().getvalue()
A:torch._deploy.typename->_maybe_decode_ascii(saved_id[0])
A:torch._deploy._loaded_reduces[reduce_id]->func(_raw_packages[zip_reader], *args)
A:torch._deploy.importer->OrderedImporter(_get_package(zip_reader), sys_importer)
A:torch._deploy.unpickler->PackageUnpickler(importer, io.BytesIO(obj_bytes))
A:torch._deploy.result_deploy_objects[id]->PackageUnpickler(importer, io.BytesIO(obj_bytes)).load()
A:torch._deploy._raw_packages[zip_reader]->PackageImporter(zip_reader)
torch._deploy._get_package(zip_reader)
torch._deploy._load_storages(id,zip_reader,obj_bytes,serialized_storages,serialized_dtypes)
torch._deploy._save_storages(importer,obj)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_jit_internal.py----------------------------------------
A:torch._jit_internal.parts->qualified_name.split('.')
A:torch._jit_internal.remaining_pieces->'.'.join(parts[1:])
A:torch._jit_internal.module_value->getattr(module, base)
A:torch._jit_internal.base->lookupInModule(expr[:i].strip(), module)
A:torch._jit_internal.(part, part_len)->parseNestedExpr(expr[i:], module)
A:torch._jit_internal.(value, len_parsed)->parseNestedExpr(expr, module)
A:torch._jit_internal.frame->inspect.currentframe()
A:torch._jit_internal.closure->get_closure(fn)
A:torch._jit_internal.callable_signature->inspect.signature(fn)
A:torch._jit_internal.signature->inspect.signature(fn)
A:torch._jit_internal.src->inspect.getsource(fn)
A:torch._jit_internal.a->ast.parse(dedent(src))
A:torch._jit_internal.literal_return_annotation->get_annotation_str(f.returns)
A:torch._jit_internal.drop_on_export->kwargs.pop('drop_on_export', None)
A:torch._jit_internal.item->getattr(mod, name)
A:torch._jit_internal.attr->get_torchscript_modifier(orig)
A:torch._jit_internal.mod->get_torchscript_modifier(fn)
A:torch._jit_internal.(sourcelines, file_lineno, filename)->get_source_lines_and_file(obj)
A:torch._jit_internal.parsed_def->parse_def(func)
A:torch._jit_internal.qual_name->_qualified_name(method)
A:torch._jit_internal.fn_overload_list->_overloaded_fns.get(qual_name)
A:torch._jit_internal.current_frame->inspect.currentframe()
A:torch._jit_internal.class_name_map->_overloaded_methods.get(qual_name, None)
A:torch._jit_internal.(class_name, line_no)->get_class_name_lineno(func)
A:torch._jit_internal.method_overloads->_overloaded_methods.get(qual_name, None).get(class_name, None)
A:torch._jit_internal.overloads->_overloaded_methods.get(qual_name, None).get(mod_class.__name__, None)
A:torch._jit_internal.BroadcastingList1->BroadcastingListCls()
A:torch._jit_internal.module_name->module_name.replace('>', '_').replace('>', '_')
A:torch._jit_internal.has_annotations->hasattr(obj, '__annotations__')
A:torch._jit_internal.the_type->torch.jit.annotations.ann_to_type(obj.__annotations__[field], fake_range())
A:torch._jit_internal.TupleType->collections.namedtuple(unqual_name, field_names, defaults=defaults)
A:torch._jit_internal.hooks->torch._C._jit_get_emit_hooks()
A:torch._jit_internal.self.hooks->torch._C._jit_get_emit_hooks()
A:torch._jit_internal.origin_type->get_origin(target_type)
A:torch._jit_internal.arg_origin->get_origin(arg_type)
A:torch._jit_internal.val_origin->get_origin(val_type)
A:torch._jit_internal.arg_types->get_args(target_type)
A:torch._jit_internal.el_origin->get_origin(el_type)
A:torch._jit_internal.inner_types->get_args(target_type)
A:torch._jit_internal.t_origin->get_origin(t)
A:torch._jit_internal.extractor->_TensorExtractor(io.BytesIO(), protocol=-1, tensors=tensors)
torch._jit_internal.BroadcastingListCls(object)
torch._jit_internal.BroadcastingListCls.__getitem__(self,types)
torch._jit_internal.FunctionModifiers(object)
torch._jit_internal._IgnoreContextManager(self,**kwargs)
torch._jit_internal._IgnoreContextManager.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch._jit_internal._IgnoreContextManager.__init__(self,**kwargs)
torch._jit_internal._TensorExtractor(self,*args,tensors:List[torch.Tensor],**kwargs)
torch._jit_internal._TensorExtractor.__init__(self,*args,tensors:List[torch.Tensor],**kwargs)
torch._jit_internal._TensorExtractor.persistent_id(self,obj)
torch._jit_internal._check_overload_body(func)
torch._jit_internal._clear_fn_overloads(qual_name)->None
torch._jit_internal._copy_to_script_wrapper(fn)
torch._jit_internal._create_named_tuple(t,unqual_name:str,field_names:List[str],defaults:Tuple[Any,...])
torch._jit_internal._disable_emit_hooks()
torch._jit_internal._disable_emit_hooks_decorator(_DecoratorContextManager)->None
torch._jit_internal._extract_tensors(obj)
torch._jit_internal._get_fn_overloads(qual_name)
torch._jit_internal._get_named_tuple_properties(obj)
torch._jit_internal._get_overloaded_methods(method,mod_class)
torch._jit_internal._is_exception(obj)->bool
torch._jit_internal._isinstance(obj,target_type)->bool
torch._jit_internal._overload(func)
torch._jit_internal._overload_method(func)
torch._jit_internal._qualified_name(obj)->str
torch._jit_internal._try_get_dispatched_fn(fn)
torch._jit_internal.boolean_dispatch(arg_name,arg_index,default,if_true,if_false,module_name,func_name)
torch._jit_internal.can_compile_class(cls)->bool
torch._jit_internal.check_args_exist(target_type)->None
torch._jit_internal.check_empty_containers(obj)->None
torch._jit_internal.container_checker(obj,target_type)->bool
torch._jit_internal.copy_torchscript_modifier(orig,new)->None
torch._jit_internal.createResolutionCallbackForClassMethods(cls)
torch._jit_internal.createResolutionCallbackFromClosure(fn)
torch._jit_internal.createResolutionCallbackFromEnv(lookup_base)
torch._jit_internal.createResolutionCallbackFromFrame(frames_up:int=0)
torch._jit_internal.export(fn)
torch._jit_internal.get_annotation_str(annotation)
torch._jit_internal.get_args(target_type)
torch._jit_internal.get_callable_argument_names(fn)->List[str]
torch._jit_internal.get_class_name_lineno(method)->Tuple[str, int]
torch._jit_internal.get_closure(fn)
torch._jit_internal.get_origin(target_type)
torch._jit_internal.get_overload_no_implementation_error_message(kind,obj)
torch._jit_internal.get_static_fn(cls,fn)
torch._jit_internal.get_torchscript_modifier(fn)
torch._jit_internal.get_type_hint_captures(fn)
torch._jit_internal.ignore(drop=False,**kwargs)
torch._jit_internal.is_dict(ann)->bool
torch._jit_internal.is_final(ann)->bool
torch._jit_internal.is_future(ann)->bool
torch._jit_internal.is_ignored_fn(fn)->bool
torch._jit_internal.is_list(ann)->bool
torch._jit_internal.is_optional(ann)
torch._jit_internal.is_scripting()->bool
torch._jit_internal.is_static_fn(cls,fn)->bool
torch._jit_internal.is_tuple(ann)->bool
torch._jit_internal.is_union(ann)
torch._jit_internal.module_has_exports(mod)
torch._jit_internal.raise_error_container_parameter_missing(target_type)->None
torch._jit_internal.should_drop(fn)->bool
torch._jit_internal.unused(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_tensor_str.py----------------------------------------
A:torch._tensor_str.PRINT_OPTS->__PrinterOptions()
A:torch._tensor_str.tensor_view->tensor.reshape(-1)
A:torch._tensor_str.value_str->'{{:.{}f}}'.format(PRINT_OPTS.precision).format(value)
A:torch._tensor_str.self.max_width->max(self.max_width, len(value_str))
A:torch._tensor_str.nonzero_finite_vals->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
A:torch._tensor_str.nonzero_finite_abs->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double()
A:torch._tensor_str.nonzero_finite_min->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().min().double()
A:torch._tensor_str.nonzero_finite_max->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().max().double()
A:torch._tensor_str.ret->'{}'.format(value)
A:torch._tensor_str.real_str->formatter1.format(val.real)
A:torch._tensor_str.imag_str->(formatter2.format(val.imag) + 'j').lstrip()
A:torch._tensor_str.elements_per_line->max(1, int(math.floor((PRINT_OPTS.linewidth - indent) / element_length)))
A:torch._tensor_str.dim->self.to('cpu').dim()
A:torch._tensor_str.tensor_str->_tensor_str(self, indent)
A:torch._tensor_str.self->self.to('cpu').to('cpu')
A:torch._tensor_str.real_formatter->_Formatter(get_summarized_data(self.real) if summarize else self.real)
A:torch._tensor_str.imag_formatter->_Formatter(get_summarized_data(self.imag) if summarize else self.imag)
A:torch._tensor_str.formatter->_Formatter(get_summarized_data(self) if summarize else self)
A:torch._tensor_str.suffix_len->len(suffix)
A:torch._tensor_str.indent->len(prefix)
A:torch._tensor_str.(self, tangent)->torch.autograd.forward_ad.unpack_dual(inp)
A:torch._tensor_str.indices->self.to('cpu').to('cpu')._indices().detach()
A:torch._tensor_str.indices_str->_tensor_str(indices, indent + len(indices_prefix))
A:torch._tensor_str.values->self.to('cpu').to('cpu').values().detach()
A:torch._tensor_str.values_str->_tensor_str(values, indent + len(values_prefix))
A:torch._tensor_str.crow_indices->self.to('cpu').to('cpu').crow_indices().detach()
A:torch._tensor_str.crow_indices_str->_tensor_str(crow_indices, indent + len(crow_indices_prefix))
A:torch._tensor_str.col_indices->self.to('cpu').to('cpu').col_indices().detach()
A:torch._tensor_str.col_indices_str->_tensor_str(col_indices, indent + len(col_indices_prefix))
torch._tensor_str._Formatter(self,tensor)
torch._tensor_str._Formatter.__init__(self,tensor)
torch._tensor_str._Formatter.format(self,value)
torch._tensor_str._Formatter.width(self)
torch._tensor_str.__PrinterOptions(object)
torch._tensor_str._add_suffixes(tensor_str,suffixes,indent,force_newline)
torch._tensor_str._scalar_str(self,formatter1,formatter2=None)
torch._tensor_str._str(self)
torch._tensor_str._str_intern(inp)
torch._tensor_str._tensor_str(self,indent)
torch._tensor_str._tensor_str_with_formatter(self,indent,summarize,formatter1,formatter2=None)
torch._tensor_str._vector_str(self,indent,summarize,formatter1,formatter2=None)
torch._tensor_str.get_summarized_data(self)
torch._tensor_str.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)
torch.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/_utils.py----------------------------------------
A:torch.cuda._utils.device->torch.device(device)
torch.cuda._dummy_type(name:str)->type
torch.cuda._get_device_index(device:Any,optional:bool=False,allow_cpu:bool=False)->int
torch.cuda._utils._dummy_type(name:str)->type
torch.cuda._utils._get_device_index(device:Any,optional:bool=False,allow_cpu:bool=False)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/nvtx.py----------------------------------------
A:torch.cuda.nvtx._nvtx->_NVTXStub()
torch.cuda.nvtx.mark(msg)
torch.cuda.nvtx.range(msg,*args,**kwargs)
torch.cuda.nvtx.range_pop()
torch.cuda.nvtx.range_push(msg)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/profiler.py----------------------------------------
A:torch.cuda.profiler.rt->cudart()
torch.cuda.profiler.init(output_file,flags=None,output_mode='key_value')
torch.cuda.profiler.profile()
torch.cuda.profiler.start()
torch.cuda.profiler.stop()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/sparse.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/graphs.py----------------------------------------
A:torch.cuda.graphs.torch._C.__dict__['_CUDAGraph']->_dummy_type('_CUDAGraph')
A:torch.cuda.graphs.torch._C.__dict__['_graph_pool_handle']->_dummy_type('_graph_pool_handle')
A:torch.cuda.graphs.self.__class__.default_capture_stream->torch.cuda.Stream()
A:torch.cuda.graphs.self.stream_ctx->torch.cuda.stream(self.capture_stream)
A:torch.cuda.graphs.mempool->graph_pool_handle()
A:torch.cuda.graphs.outputs->func(*args)
A:torch.cuda.graphs.grad_inputs->torch.autograd.grad(outputs=static_outputs, inputs=tuple((i for i in static_input_surface if i.requires_grad)), grad_outputs=static_grad_outputs, only_inputs=True, allow_unused=False)
A:torch.cuda.graphs.static_grad_outputs->tuple((torch.empty_like(o) for o in static_outputs))
A:torch.cuda.graphs.static_grad_inputs->tuple(static_grad_inputs)
A:torch.cuda.graphs.per_callable_static_grad_outputs->list(reversed(per_callable_static_grad_outputs))
A:torch.cuda.graphs.per_callable_static_grad_inputs->list(reversed(per_callable_static_grad_inputs))
A:torch.cuda.graphs.out->Graphed.apply(*user_args + module_params)
A:torch.cuda.graphs.graphed->make_graphed_autograd_function(fwd_graphs[i], bwd_graphs[i], per_callable_module_params[i], per_callable_len_user_args[i], per_callable_output_was_tensor[i], per_callable_static_input_surfaces[i], per_callable_static_outputs[i], per_callable_static_grad_outputs[i], per_callable_static_grad_inputs[i])
A:torch.cuda.graphs.func.forward->make_graphed_forward(func, func.training, graphed, func.forward)
torch.cuda.CUDAGraph(self)
torch.cuda.CUDAGraph.capture_begin(self,pool=None)
torch.cuda.CUDAGraph.capture_end(self)
torch.cuda.CUDAGraph.pool(self)
torch.cuda.CUDAGraph.replay(self)
torch.cuda.CUDAGraph.reset(self)
torch.cuda.graph(self,cuda_graph,pool=None,stream=None)
torch.cuda.graph.__enter__(self)
torch.cuda.graph.__exit__(self,exc_type,exc_value,traceback)
torch.cuda.graph_pool_handle()
torch.cuda.graphs.CUDAGraph(self)
torch.cuda.graphs.CUDAGraph.__init__(self)
torch.cuda.graphs.CUDAGraph.capture_begin(self,pool=None)
torch.cuda.graphs.CUDAGraph.capture_end(self)
torch.cuda.graphs.CUDAGraph.pool(self)
torch.cuda.graphs.CUDAGraph.replay(self)
torch.cuda.graphs.CUDAGraph.reset(self)
torch.cuda.graphs.graph(self,cuda_graph,pool=None,stream=None)
torch.cuda.graphs.graph.__enter__(self)
torch.cuda.graphs.graph.__exit__(self,exc_type,exc_value,traceback)
torch.cuda.graphs.graph.__init__(self,cuda_graph,pool=None,stream=None)
torch.cuda.graphs.graph_pool_handle()
torch.cuda.graphs.make_graphed_callables(callables,sample_args)
torch.cuda.make_graphed_callables(callables,sample_args)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/nccl.py----------------------------------------
A:torch.cuda.nccl.devices->set()
A:torch.cuda.nccl.device->tensor.get_device()
A:torch.cuda.nccl.ver->torch._C._nccl_version()
torch.cuda.nccl._check_sequence_type(inputs:Union[torch.Tensor,Sequence[torch.Tensor]])->None
torch.cuda.nccl.all_gather(inputs:Sequence[torch.Tensor],outputs:Sequence[torch.Tensor],streams=None,comms=None)->None
torch.cuda.nccl.all_reduce(inputs,outputs=None,op=SUM,streams=None,comms=None)
torch.cuda.nccl.broadcast(inputs:Sequence[torch.Tensor],root:int=0,streams=None,comms=None)->None
torch.cuda.nccl.init_rank(num_ranks,uid,rank)
torch.cuda.nccl.is_available(tensors)
torch.cuda.nccl.reduce(inputs:Sequence[torch.Tensor],output:Optional[Union[torch.Tensor,Sequence[torch.Tensor]]]=None,root:int=0,op:int=SUM,streams:Optional[Sequence[torch.cuda.Stream]]=None,comms=None,*,outputs:Optional[Sequence[torch.Tensor]]=None)->None
torch.cuda.nccl.reduce_scatter(inputs:Sequence[torch.Tensor],outputs:Sequence[torch.Tensor],op:int=SUM,streams=None,comms=None)->None
torch.cuda.nccl.unique_id()
torch.cuda.nccl.version()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/memory.py----------------------------------------
A:torch.cuda.memory.device->_get_device_index(device)
A:torch.cuda.memory.stream->torch.cuda.current_stream(device)
A:torch.cuda.memory.stats->memory_stats(device=device)
A:torch.cuda.memory.handle->pynvml.nvmlDeviceGetHandleByIndex(device)
A:torch.cuda.memory.procs->pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
torch.cuda._free_mutex()
torch.cuda._host_allocator()
torch.cuda.caching_allocator_alloc(size,device:Union[Device,int]=None,stream=None)
torch.cuda.caching_allocator_delete(mem_ptr)
torch.cuda.empty_cache()->None
torch.cuda.list_gpu_processes(device:Union[Device,int]=None)->str
torch.cuda.max_memory_allocated(device:Union[Device,int]=None)->int
torch.cuda.max_memory_cached(device:Union[Device,int]=None)->int
torch.cuda.max_memory_reserved(device:Union[Device,int]=None)->int
torch.cuda.mem_get_info(device:Union[Device,int]=None)->int
torch.cuda.memory._free_mutex()
torch.cuda.memory._host_allocator()
torch.cuda.memory.caching_allocator_alloc(size,device:Union[Device,int]=None,stream=None)
torch.cuda.memory.caching_allocator_delete(mem_ptr)
torch.cuda.memory.empty_cache()->None
torch.cuda.memory.list_gpu_processes(device:Union[Device,int]=None)->str
torch.cuda.memory.max_memory_allocated(device:Union[Device,int]=None)->int
torch.cuda.memory.max_memory_cached(device:Union[Device,int]=None)->int
torch.cuda.memory.max_memory_reserved(device:Union[Device,int]=None)->int
torch.cuda.memory.mem_get_info(device:Union[Device,int]=None)->int
torch.cuda.memory.memory_allocated(device:Union[Device,int]=None)->int
torch.cuda.memory.memory_cached(device:Union[Device,int]=None)->int
torch.cuda.memory.memory_reserved(device:Union[Device,int]=None)->int
torch.cuda.memory.memory_snapshot()
torch.cuda.memory.memory_stats(device:Union[Device,int]=None)->Dict[str, Any]
torch.cuda.memory.memory_stats_as_nested_dict(device:Union[Device,int]=None)->Dict[str, Any]
torch.cuda.memory.memory_summary(device:Union[Device,int]=None,abbreviated:bool=False)->str
torch.cuda.memory.reset_accumulated_memory_stats(device:Union[Device,int]=None)->None
torch.cuda.memory.reset_max_memory_allocated(device:Union[Device,int]=None)->None
torch.cuda.memory.reset_max_memory_cached(device:Union[Device,int]=None)->None
torch.cuda.memory.reset_peak_memory_stats(device:Union[Device,int]=None)->None
torch.cuda.memory.set_per_process_memory_fraction(fraction,device:Union[Device,int]=None)->None
torch.cuda.memory_allocated(device:Union[Device,int]=None)->int
torch.cuda.memory_cached(device:Union[Device,int]=None)->int
torch.cuda.memory_reserved(device:Union[Device,int]=None)->int
torch.cuda.memory_snapshot()
torch.cuda.memory_stats(device:Union[Device,int]=None)->Dict[str, Any]
torch.cuda.memory_stats_as_nested_dict(device:Union[Device,int]=None)->Dict[str, Any]
torch.cuda.memory_summary(device:Union[Device,int]=None,abbreviated:bool=False)->str
torch.cuda.reset_accumulated_memory_stats(device:Union[Device,int]=None)->None
torch.cuda.reset_max_memory_allocated(device:Union[Device,int]=None)->None
torch.cuda.reset_max_memory_cached(device:Union[Device,int]=None)->None
torch.cuda.reset_peak_memory_stats(device:Union[Device,int]=None)->None
torch.cuda.set_per_process_memory_fraction(fraction,device:Union[Device,int]=None)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/comm.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/streams.py----------------------------------------
A:torch.cuda.streams.torch._C.__dict__['_CudaStreamBase']->_dummy_type('_CudaStreamBase')
A:torch.cuda.streams.torch._C.__dict__['_CudaEventBase']->_dummy_type('_CudaEventBase')
A:torch.cuda.streams.event->Event()
A:torch.cuda.streams.stream->torch.cuda.current_stream()
torch.cuda.Event(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.Event.__repr__(self)
torch.cuda.Event._as_parameter_(self)
torch.cuda.Event.elapsed_time(self,end_event)
torch.cuda.Event.from_ipc_handle(cls,device,handle)
torch.cuda.Event.ipc_handle(self)
torch.cuda.Event.query(self)
torch.cuda.Event.record(self,stream=None)
torch.cuda.Event.synchronize(self)
torch.cuda.Event.wait(self,stream=None)
torch.cuda.ExternalStream(cls,stream_ptr,device=None,**kwargs)
torch.cuda.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.Stream.__eq__(self,o)
torch.cuda.Stream.__hash__(self)
torch.cuda.Stream.__repr__(self)
torch.cuda.Stream._as_parameter_(self)
torch.cuda.Stream.query(self)
torch.cuda.Stream.record_event(self,event=None)
torch.cuda.Stream.synchronize(self)
torch.cuda.Stream.wait_event(self,event)
torch.cuda.Stream.wait_stream(self,stream)
torch.cuda.streams.Event(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.streams.Event.__new__(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.streams.Event.__repr__(self)
torch.cuda.streams.Event._as_parameter_(self)
torch.cuda.streams.Event.elapsed_time(self,end_event)
torch.cuda.streams.Event.from_ipc_handle(cls,device,handle)
torch.cuda.streams.Event.ipc_handle(self)
torch.cuda.streams.Event.query(self)
torch.cuda.streams.Event.record(self,stream=None)
torch.cuda.streams.Event.synchronize(self)
torch.cuda.streams.Event.wait(self,stream=None)
torch.cuda.streams.ExternalStream(cls,stream_ptr,device=None,**kwargs)
torch.cuda.streams.ExternalStream.__new__(cls,stream_ptr,device=None,**kwargs)
torch.cuda.streams.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__eq__(self,o)
torch.cuda.streams.Stream.__hash__(self)
torch.cuda.streams.Stream.__new__(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__repr__(self)
torch.cuda.streams.Stream._as_parameter_(self)
torch.cuda.streams.Stream.query(self)
torch.cuda.streams.Stream.record_event(self,event=None)
torch.cuda.streams.Stream.synchronize(self)
torch.cuda.streams.Stream.wait_event(self,event)
torch.cuda.streams.Stream.wait_stream(self,stream)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/error.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/random.py----------------------------------------
A:torch.cuda.random.device->torch.device('cuda', device)
A:torch.cuda.random.idx->current_device()
A:torch.cuda.random.new_state_copy->new_state.clone(memory_format=torch.contiguous_format)
A:torch.cuda.random.seed->int(seed)
A:torch.cuda.random.random_seed->default_generator.initial_seed()
torch.cuda.get_rng_state(device:Union[int,str,torch.device]='cuda')->Tensor
torch.cuda.get_rng_state_all()->List[Tensor]
torch.cuda.initial_seed()->int
torch.cuda.manual_seed(seed:int)->None
torch.cuda.manual_seed_all(seed:int)->None
torch.cuda.random.get_rng_state(device:Union[int,str,torch.device]='cuda')->Tensor
torch.cuda.random.get_rng_state_all()->List[Tensor]
torch.cuda.random.initial_seed()->int
torch.cuda.random.manual_seed(seed:int)->None
torch.cuda.random.manual_seed_all(seed:int)->None
torch.cuda.random.seed()->None
torch.cuda.random.seed_all()->None
torch.cuda.random.set_rng_state(new_state:Tensor,device:Union[int,str,torch.device]='cuda')->None
torch.cuda.random.set_rng_state_all(new_states:Iterable[Tensor])->None
torch.cuda.seed()->None
torch.cuda.seed_all()->None
torch.cuda.set_rng_state(new_state:Tensor,device:Union[int,str,torch.device]='cuda')->None
torch.cuda.set_rng_state_all(new_states:Iterable[Tensor])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/__init__.py----------------------------------------
A:torch.cuda.__init__._tls->threading.local()
A:torch.cuda.__init__._initialization_lock->threading.Lock()
A:torch.cuda.__init__._is_in_bad_fork->getattr(torch._C, '_cuda_isInBadFork', lambda : False)
A:torch.cuda.__init__._lazy_seed_tracker->_LazySeedTracker()
A:torch.cuda.__init__._CudaDeviceProperties->_dummy_type('_CudaDeviceProperties')
A:torch.cuda.__init__.CUDA_VERSION->torch._C._cuda_getCompiledVersion()
A:torch.cuda.__init__.capability->get_device_capability(d)
A:torch.cuda.__init__.name->get_device_name(d)
A:torch.cuda.__init__.min_arch->min((int(arch.split('_')[1]) for arch in torch.cuda.get_arch_list()), default=35)
A:torch.cuda.__init__.arch_list->get_arch_list()
A:torch.cuda.__init__.(cap_major, cap_minor)->get_device_capability(idx)
A:torch.cuda.__init__.supported->any([sm // 10 == cap_major for sm in supported_sm])
A:torch.cuda.__init__.device_name->get_device_name(idx)
A:torch.cuda.__init__.msg->torch._C._cudart.cudaGetErrorString(_cudart.cudaError(code))
A:torch.cuda.__init__.self.idx->_get_device_index(None, True)
A:torch.cuda.__init__.self.prev_idx->torch.cuda.current_device()
A:torch.cuda.__init__.device->_get_device_index(device, optional=True)
A:torch.cuda.__init__.prop->get_device_properties(device)
A:torch.cuda.__init__.peer_device->_get_device_index(peer_device)
A:torch.cuda.__init__.self.src_prev_stream->torch.cuda.current_stream(None)
A:torch.cuda.__init__.self.dst_prev_stream->torch.cuda.current_stream(cur_stream.device)
A:torch.cuda.__init__.arch_flags->torch._C._cuda_getArchFlags()
A:torch.cuda.__init__.handle->pynvml.nvmlDeviceGetHandleByIndex(device)
A:torch.cuda.__init__.tensor_name->'Cuda{0}TensorBase'.format(t)
A:torch.cuda.__init__.torch._C.__dict__[tensor_name]->_dummy_type(tensor_name)
A:torch.cuda.__init__.torch._C.__dict__[storage_name]->_dummy_type(storage_name)
A:torch.cuda.__init__.torch._C.__dict__['_CudaStreamBase']->_dummy_type('CudaStreamBase')
A:torch.cuda.__init__.torch._C.__dict__['_CudaEventBase']->_dummy_type('CudaEventBase')
torch.cuda.__init__.BFloat16Storage(_TypedStorage)
torch.cuda.__init__.BFloat16Storage.dtype(self)
torch.cuda.__init__.BoolStorage(_TypedStorage)
torch.cuda.__init__.BoolStorage.dtype(self)
torch.cuda.__init__.ByteStorage(_TypedStorage)
torch.cuda.__init__.ByteStorage.dtype(self)
torch.cuda.__init__.CharStorage(_TypedStorage)
torch.cuda.__init__.CharStorage.dtype(self)
torch.cuda.__init__.ComplexDoubleStorage(_TypedStorage)
torch.cuda.__init__.ComplexDoubleStorage.dtype(self)
torch.cuda.__init__.ComplexFloatStorage(_TypedStorage)
torch.cuda.__init__.ComplexFloatStorage.dtype(self)
torch.cuda.__init__.CudaError(self,code:int)
torch.cuda.__init__.CudaError.__init__(self,code:int)
torch.cuda.__init__.DeferredCudaCallError(Exception)
torch.cuda.__init__.DoubleStorage(_TypedStorage)
torch.cuda.__init__.DoubleStorage.dtype(self)
torch.cuda.__init__.FloatStorage(_TypedStorage)
torch.cuda.__init__.FloatStorage.dtype(self)
torch.cuda.__init__.HalfStorage(_TypedStorage)
torch.cuda.__init__.HalfStorage.dtype(self)
torch.cuda.__init__.IntStorage(_TypedStorage)
torch.cuda.__init__.IntStorage.dtype(self)
torch.cuda.__init__.LongStorage(_TypedStorage)
torch.cuda.__init__.LongStorage.dtype(self)
torch.cuda.__init__.ShortStorage(_TypedStorage)
torch.cuda.__init__.ShortStorage.dtype(self)
torch.cuda.__init__.StreamContext(self,stream:Optional['torch.cuda.Stream'])
torch.cuda.__init__.StreamContext.__enter__(self)
torch.cuda.__init__.StreamContext.__exit__(self,type:Any,value:Any,traceback:Any)
torch.cuda.__init__.StreamContext.__init__(self,stream:Optional['torch.cuda.Stream'])
torch.cuda.__init__._CudaBase(object)
torch.cuda.__init__._CudaBase.type(self,*args,**kwargs)
torch.cuda.__init__._LazySeedTracker(self)
torch.cuda.__init__._LazySeedTracker.__init__(self)
torch.cuda.__init__._LazySeedTracker.get_calls(self)->List
torch.cuda.__init__._LazySeedTracker.queue_seed(self,cb,traceback)
torch.cuda.__init__._LazySeedTracker.queue_seed_all(self,cb,traceback)
torch.cuda.__init__._UntypedStorage(_CudaBase,torch._C.CudaByteStorageBase,_StorageBase)
torch.cuda.__init__._check_capability()
torch.cuda.__init__._check_cubins()
torch.cuda.__init__._lazy_call(callable,**kwargs)
torch.cuda.__init__._lazy_init()
torch.cuda.__init__._lazy_new(cls,*args,**kwargs)
torch.cuda.__init__._sleep(cycles)
torch.cuda.__init__.can_device_access_peer(device:_device_t,peer_device:_device_t)->bool
torch.cuda.__init__.check_error(res:int)->None
torch.cuda.__init__.cudaStatus(object)
torch.cuda.__init__.cudart()
torch.cuda.__init__.current_blas_handle()
torch.cuda.__init__.current_device()->int
torch.cuda.__init__.current_stream(device:Optional[_device_t]=None)->Stream
torch.cuda.__init__.default_stream(device:Optional[_device_t]=None)->Stream
torch.cuda.__init__.device(self,device:Any)
torch.cuda.__init__.device.__enter__(self)
torch.cuda.__init__.device.__exit__(self,type:Any,value:Any,traceback:Any)
torch.cuda.__init__.device.__init__(self,device:Any)
torch.cuda.__init__.device_count()->int
torch.cuda.__init__.device_of(self,obj)
torch.cuda.__init__.device_of.__init__(self,obj)
torch.cuda.__init__.get_arch_list()->List[str]
torch.cuda.__init__.get_device_capability(device:Optional[_device_t]=None)->Tuple[int, int]
torch.cuda.__init__.get_device_name(device:Optional[_device_t]=None)->str
torch.cuda.__init__.get_device_properties(device:_device_t)->_CudaDeviceProperties
torch.cuda.__init__.get_gencode_flags()->str
torch.cuda.__init__.get_sync_debug_mode()->int
torch.cuda.__init__.init()
torch.cuda.__init__.ipc_collect()
torch.cuda.__init__.is_available()->bool
torch.cuda.__init__.is_bf16_supported()
torch.cuda.__init__.is_initialized()
torch.cuda.__init__.memory_usage(device:Optional[Union[Device,int]]=None)->int
torch.cuda.__init__.set_device(device:_device_t)->None
torch.cuda.__init__.set_stream(stream:Stream)
torch.cuda.__init__.set_sync_debug_mode(debug_mode:Union[int,str])->None
torch.cuda.__init__.stream(stream:Optional['torch.cuda.Stream'])->StreamContext
torch.cuda.__init__.synchronize(device:_device_t=None)->None
torch.cuda.__init__.utilization(device:Optional[Union[Device,int]]=None)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/amp/autocast_mode.py----------------------------------------
A:torch.cuda.amp.autocast_mode.iterable->map(lambda v: _cast(v, dtype), value)
A:torch.cuda.amp.autocast_mode.args[0]._fwd_used_autocast->torch.is_autocast_enabled()
A:torch.cuda.amp.autocast_mode.autocast_context->torch.is_autocast_enabled()
torch.cuda.amp.autocast(self,enabled:bool=True,dtype:torch.dtype=torch.float16,cache_enabled:bool=True)
torch.cuda.amp.autocast.__enter__(self)
torch.cuda.amp.autocast.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)
torch.cuda.amp.autocast_mode._cast(value,dtype)
torch.cuda.amp.autocast_mode.autocast(self,enabled:bool=True,dtype:torch.dtype=torch.float16,cache_enabled:bool=True)
torch.cuda.amp.autocast_mode.autocast.__enter__(self)
torch.cuda.amp.autocast_mode.autocast.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)
torch.cuda.amp.autocast_mode.autocast.__init__(self,enabled:bool=True,dtype:torch.dtype=torch.float16,cache_enabled:bool=True)
torch.cuda.amp.autocast_mode.custom_bwd(bwd)
torch.cuda.amp.autocast_mode.custom_fwd(fwd=None,**kwargs)
torch.cuda.amp.custom_bwd(bwd)
torch.cuda.amp.custom_fwd(fwd=None,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/amp/common.py----------------------------------------
torch.cuda.amp.common.amp_definitely_not_available()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/amp/grad_scaler.py----------------------------------------
A:torch.cuda.amp.grad_scaler.retval->self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
A:torch.cuda.amp.grad_scaler.self._per_optimizer_states->defaultdict(_refresh_per_optimizer_state)
A:torch.cuda.amp.grad_scaler.self._scale->torch.full((1,), self._init_scale, dtype=torch.float32, device=dev)
A:torch.cuda.amp.grad_scaler.self._growth_tracker->torch.full((1,), self._init_growth_tracker, dtype=torch.int32, device=dev)
A:torch.cuda.amp.grad_scaler.iterable->map(apply_scale, val)
A:torch.cuda.amp.grad_scaler.per_device_inv_scale->_MultiDeviceReplicator(inv_scale)
A:torch.cuda.amp.grad_scaler.per_device_found_inf->_MultiDeviceReplicator(found_inf)
A:torch.cuda.amp.grad_scaler.per_device_and_dtype_grads->defaultdict(lambda : defaultdict(list))
A:torch.cuda.amp.grad_scaler.param.grad->param.grad.coalesce()
A:torch.cuda.amp.grad_scaler.to_unscale->param.grad._values()
A:torch.cuda.amp.grad_scaler.inv_scale->self._scale.double().reciprocal().float()
A:torch.cuda.amp.grad_scaler.found_inf->torch.full((1,), 0.0, dtype=torch.float32, device=_scale.device)
A:torch.cuda.amp.grad_scaler.optimizer_state['found_inf_per_device']->self._unscale_grads_(optimizer, inv_scale, found_inf, False)
A:torch.cuda.amp.grad_scaler.(_scale, _growth_tracker)->self._check_scale_growth_tracker('update')
A:torch.cuda.amp.grad_scaler.state->self.__dict__.copy()
A:torch.cuda.amp.grad_scaler.state['_init_scale']->self.get_scale()
A:torch.cuda.amp.grad_scaler.state['_init_growth_tracker']->self._get_growth_tracker()
A:torch.cuda.amp.grad_scaler.(_scale, _)->self._check_scale_growth_tracker('_check_inf_per_device')
A:torch.cuda.amp.grad_scaler.dummy_inv_scale->torch.full((1,), 1.0, dtype=torch.float32, device=_scale.device)
A:torch.cuda.amp.grad_scaler.self._per_optimizer_states[id(optimizer)]['found_inf_per_device']->self._unscale_grads_(optimizer, dummy_inv_scale, found_inf, True)
torch.cuda.amp.GradScaler(self,init_scale=2.0**16,growth_factor=2.0,backoff_factor=0.5,growth_interval=2000,enabled=True)
torch.cuda.amp.GradScaler.__getstate__(self)
torch.cuda.amp.GradScaler.__setstate__(self,state)
torch.cuda.amp.GradScaler._check_inf_per_device(self,optimizer)
torch.cuda.amp.GradScaler._check_scale_growth_tracker(self,funcname)->Tuple[torch.Tensor, torch.Tensor]
torch.cuda.amp.GradScaler._found_inf_per_device(self,optimizer)
torch.cuda.amp.GradScaler._get_growth_tracker(self)
torch.cuda.amp.GradScaler._get_scale_async(self)
torch.cuda.amp.GradScaler._lazy_init_scale_growth_tracker(self,dev)
torch.cuda.amp.GradScaler._maybe_opt_step(self,optimizer,optimizer_state,*args,**kwargs)
torch.cuda.amp.GradScaler._unscale_grads_(self,optimizer,inv_scale,found_inf,allow_fp16)
torch.cuda.amp.GradScaler.get_backoff_factor(self)
torch.cuda.amp.GradScaler.get_growth_factor(self)
torch.cuda.amp.GradScaler.get_growth_interval(self)
torch.cuda.amp.GradScaler.get_scale(self)
torch.cuda.amp.GradScaler.is_enabled(self)
torch.cuda.amp.GradScaler.load_state_dict(self,state_dict)
torch.cuda.amp.GradScaler.scale(self,outputs)
torch.cuda.amp.GradScaler.set_backoff_factor(self,new_factor)
torch.cuda.amp.GradScaler.set_growth_factor(self,new_factor)
torch.cuda.amp.GradScaler.set_growth_interval(self,new_interval)
torch.cuda.amp.GradScaler.state_dict(self)
torch.cuda.amp.GradScaler.step(self,optimizer,*args,**kwargs)
torch.cuda.amp.GradScaler.unscale_(self,optimizer)
torch.cuda.amp.GradScaler.update(self,new_scale=None)
torch.cuda.amp.grad_scaler.GradScaler(self,init_scale=2.0**16,growth_factor=2.0,backoff_factor=0.5,growth_interval=2000,enabled=True)
torch.cuda.amp.grad_scaler.GradScaler.__getstate__(self)
torch.cuda.amp.grad_scaler.GradScaler.__init__(self,init_scale=2.0**16,growth_factor=2.0,backoff_factor=0.5,growth_interval=2000,enabled=True)
torch.cuda.amp.grad_scaler.GradScaler.__setstate__(self,state)
torch.cuda.amp.grad_scaler.GradScaler._check_inf_per_device(self,optimizer)
torch.cuda.amp.grad_scaler.GradScaler._check_scale_growth_tracker(self,funcname)->Tuple[torch.Tensor, torch.Tensor]
torch.cuda.amp.grad_scaler.GradScaler._found_inf_per_device(self,optimizer)
torch.cuda.amp.grad_scaler.GradScaler._get_growth_tracker(self)
torch.cuda.amp.grad_scaler.GradScaler._get_scale_async(self)
torch.cuda.amp.grad_scaler.GradScaler._lazy_init_scale_growth_tracker(self,dev)
torch.cuda.amp.grad_scaler.GradScaler._maybe_opt_step(self,optimizer,optimizer_state,*args,**kwargs)
torch.cuda.amp.grad_scaler.GradScaler._unscale_grads_(self,optimizer,inv_scale,found_inf,allow_fp16)
torch.cuda.amp.grad_scaler.GradScaler.get_backoff_factor(self)
torch.cuda.amp.grad_scaler.GradScaler.get_growth_factor(self)
torch.cuda.amp.grad_scaler.GradScaler.get_growth_interval(self)
torch.cuda.amp.grad_scaler.GradScaler.get_scale(self)
torch.cuda.amp.grad_scaler.GradScaler.is_enabled(self)
torch.cuda.amp.grad_scaler.GradScaler.load_state_dict(self,state_dict)
torch.cuda.amp.grad_scaler.GradScaler.scale(self,outputs)
torch.cuda.amp.grad_scaler.GradScaler.set_backoff_factor(self,new_factor)
torch.cuda.amp.grad_scaler.GradScaler.set_growth_factor(self,new_factor)
torch.cuda.amp.grad_scaler.GradScaler.set_growth_interval(self,new_interval)
torch.cuda.amp.grad_scaler.GradScaler.state_dict(self)
torch.cuda.amp.grad_scaler.GradScaler.step(self,optimizer,*args,**kwargs)
torch.cuda.amp.grad_scaler.GradScaler.unscale_(self,optimizer)
torch.cuda.amp.grad_scaler.GradScaler.update(self,new_scale=None)
torch.cuda.amp.grad_scaler.OptState(Enum)
torch.cuda.amp.grad_scaler._MultiDeviceReplicator(self,master_tensor:torch.Tensor)
torch.cuda.amp.grad_scaler._MultiDeviceReplicator.__init__(self,master_tensor:torch.Tensor)
torch.cuda.amp.grad_scaler._MultiDeviceReplicator.get(self,device)->torch.Tensor
torch.cuda.amp.grad_scaler._refresh_per_optimizer_state()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cuda/amp/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/monitor/__init__.py----------------------------------------
torch.monitor.__init__.TensorboardEventHandler(self,writer:'SummaryWriter')
torch.monitor.__init__.TensorboardEventHandler.__init__(self,writer:'SummaryWriter')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/bundled_inputs.py----------------------------------------
A:torch.utils.bundled_inputs.T->TypeVar('T')
A:torch.utils.bundled_inputs.(ignored_methods, ignored_attrs)->_get_bundled_inputs_attributes_and_methods(model)
A:torch.utils.bundled_inputs.clone->torch._C._hack_do_not_use_clone_module_with_class(model._c, ignored_methods, ignored_attrs)
A:torch.utils.bundled_inputs.cloned_module->wrap_cpp_module(clone)
A:torch.utils.bundled_inputs.inflate_helper_fn_name->_get_inflate_helper_fn_name(arg_idx, inp_idx, function_name)
A:torch.utils.bundled_inputs.(deflated, inflater, helper_definition)->_inflate_expr(arg, f'deflated[{inp_idx}][{arg_idx}]', inflate_helper_fn_name, skip_size_check=skip_size_check)
A:torch.utils.bundled_inputs.expr->'\n'.join(parts)
A:torch.utils.bundled_inputs.definition->textwrap.dedent('\n                def _generate_bundled_inputs_for_{name}(self):\n                    deflated = self._bundled_inputs_deflated_{name}\n                    return [\n                {expr}\n                    ]\n                ').format(expr=expr, name=function_name)
A:torch.utils.bundled_inputs.helper_definition->arg.fmt_fn.format(inflate_helper_fn_name)
A:torch.utils.bundled_inputs.all_info->script_module.get_bundled_inputs_functions_and_info()
A:torch.utils.bundled_inputs.bundled_inputs_fn->getattr(script_module, f'get_all_bundled_inputs_for_{function_name}')
A:torch.utils.bundled_inputs.func->getattr(script_module, function_name, None)
A:torch.utils.bundled_inputs.helper_fn_name->_get_inflate_helper_fn_name(arg_idx=arg_idx, input_idx=input_idx, function_name=function_name)
A:torch.utils.bundled_inputs.stub->torch.zeros(1, dtype=dtype).expand(*size)
torch.utils.bundled_inputs.InflatableArg(NamedTuple)
torch.utils.bundled_inputs._get_bundled_inputs_attributes_and_methods(script_module:torch.jit.ScriptModule)->Tuple[List[str], List[str]]
torch.utils.bundled_inputs._get_inflate_helper_fn_name(arg_idx:int,input_idx:int,function_name:str)->str
torch.utils.bundled_inputs._inflate_expr(arg:T,ref:str,inflate_helper_fn_name:str,skip_size_check:bool=False)->Tuple[Union[T, torch.Tensor], str, Optional[str]]
torch.utils.bundled_inputs.augment_many_model_functions_with_bundled_inputs(model:torch.jit.ScriptModule,inputs:Dict[Callable,Optional[Sequence[Tuple[Any,...]]]],_receive_inflate_expr:Optional[List[str]]=None,info:Optional[Dict[Callable,List[str]]]=None,skip_size_check=False)->None
torch.utils.bundled_inputs.augment_model_with_bundled_inputs(model:torch.jit.ScriptModule,inputs:Optional[Sequence[Tuple[Any,...]]]=None,_receive_inflate_expr:Optional[List[str]]=None,info:Optional[List[str]]=None,skip_size_check=False)->None
torch.utils.bundled_inputs.bundle_inputs(model:torch.jit.ScriptModule,inputs:Union[Optional[Sequence[Tuple[Any,...]]],Dict[Callable,Optional[Sequence[Tuple[Any,...]]]]],info:Optional[Union[List[str],Dict[Callable,List[str]]]]=None,*,_receive_inflate_expr:Optional[List[str]]=None)->torch.jit.ScriptModule
torch.utils.bundled_inputs.bundle_large_tensor(t)
torch.utils.bundled_inputs.bundle_randn(*size,dtype=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/_zip.py----------------------------------------
A:torch.utils._zip.stripped_file_path->remove_prefix(file_path, strip_file_dir + '/')
A:torch.utils._zip.path->Path(stripped_file_path)
A:torch.utils._zip.parser->argparse.ArgumentParser(description='Zip py source')
A:torch.utils._zip.args->argparse.ArgumentParser(description='Zip py source').parse_args()
A:torch.utils._zip.zf->ZipFile(zip_file_name, mode='w')
A:torch.utils._zip.files->glob.glob(p + '/**/*.py', recursive=True)
torch.utils._zip.remove_prefix(text,prefix)
torch.utils._zip.write_to_zip(file_path,strip_file_path,zf)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/collect_env.py----------------------------------------
A:torch.utils.collect_env.SystemEnv->namedtuple('SystemEnv', ['torch_version', 'is_debug_build', 'cuda_compiled_version', 'gcc_version', 'clang_version', 'cmake_version', 'os', 'libc_version', 'python_version', 'python_platform', 'is_cuda_available', 'cuda_runtime_version', 'nvidia_driver_version', 'nvidia_gpu_models', 'cudnn_version', 'pip_version', 'pip_packages', 'conda_packages', 'hip_compiled_version', 'hip_runtime_version', 'miopen_runtime_version', 'caching_allocator_config'])
A:torch.utils.collect_env.p->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
A:torch.utils.collect_env.(raw_output, raw_err)->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()
A:torch.utils.collect_env.enc->locale.getpreferredencoding()
A:torch.utils.collect_env.output->get_pretty_env_info()
A:torch.utils.collect_env.err->raw_err.decode(enc)
A:torch.utils.collect_env.(rc, out, _)->run_lambda(cudnn_cmd)
A:torch.utils.collect_env.match->re.search(regex, out)
A:torch.utils.collect_env.system_root->os.environ.get('SYSTEMROOT', 'C:\\Windows')
A:torch.utils.collect_env.findstr_cmd->os.path.join(system_root, 'System32', 'findstr')
A:torch.utils.collect_env.grep_cmd->'{} /R "numpy torch mypy"'.format(findstr_cmd)
A:torch.utils.collect_env.conda->os.environ.get('CONDA_EXE', 'conda')
A:torch.utils.collect_env.out->run_with_pip(sys.executable + ' -mpip')
A:torch.utils.collect_env.comment_regex->re.compile('^#.*\\n')
A:torch.utils.collect_env.smi->'"{}"'.format(candidate_smi)
A:torch.utils.collect_env.uuid_regex->re.compile(' \\(UUID: .+?\\)')
A:torch.utils.collect_env.cuda_path->os.environ.get('CUDA_PATH', '%CUDA_PATH%')
A:torch.utils.collect_env.where_cmd->os.path.join(system_root, 'System32', 'where')
A:torch.utils.collect_env.cudnn_cmd->'{} /R "{}\\bin" cudnn*.dll'.format(where_cmd, cuda_path)
A:torch.utils.collect_env.l->os.environ.get('CUDNN_LIBRARY')
A:torch.utils.collect_env.files_set->set()
A:torch.utils.collect_env.fn->os.path.realpath(fn)
A:torch.utils.collect_env.files->list(sorted(files_set))
A:torch.utils.collect_env.result->'\n'.join(files)
A:torch.utils.collect_env.program_files_root->os.environ.get('PROGRAMFILES', 'C:\\Program Files')
A:torch.utils.collect_env.legacy_path->os.path.join(program_files_root, 'NVIDIA Corporation', 'NVSMI', smi)
A:torch.utils.collect_env.new_path->os.path.join(system_root, 'System32', smi)
A:torch.utils.collect_env.wmic_cmd->os.path.join(system_root, 'System32', 'Wbem', 'wmic')
A:torch.utils.collect_env.platform->get_platform()
A:torch.utils.collect_env.version->get_mac_version(run_lambda)
A:torch.utils.collect_env.desc->check_release_file(run_lambda)
A:torch.utils.collect_env.ca_config->os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')
A:torch.utils.collect_env.(pip_version, pip_list_output)->get_pip_packages(run_lambda)
A:torch.utils.collect_env.debug_mode_str->str(torch.version.debug)
A:torch.utils.collect_env.cuda_available_str->str(torch.cuda.is_available())
A:torch.utils.collect_env.cfg->torch._C._show_config().split('\n')
A:torch.utils.collect_env.sys_version->sys.version.replace('\n', ' ')
A:torch.utils.collect_env.env_info_fmt->'\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\nROCM used to build PyTorch: {hip_compiled_version}\n\nOS: {os}\nGCC version: {gcc_version}\nClang version: {clang_version}\nCMake version: {cmake_version}\nLibc version: {libc_version}\n\nPython version: {python_version}\nPython platform: {python_platform}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\nHIP runtime version: {hip_runtime_version}\nMIOpen runtime version: {miopen_runtime_version}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n'.strip()
A:torch.utils.collect_env.lines->text.split('\n')
A:torch.utils.collect_env.mutable_dict->replace_nones(mutable_dict)
A:torch.utils.collect_env.mutable_dict['nvidia_gpu_models']->maybe_start_on_next_line(envinfo.nvidia_gpu_models)
A:torch.utils.collect_env.all_dynamic_cuda_fields_missing->all((mutable_dict[field] is None for field in dynamic_cuda_fields))
A:torch.utils.collect_env.mutable_dict['pip_packages']->prepend(mutable_dict['pip_packages'], '[{}] '.format(envinfo.pip_version))
A:torch.utils.collect_env.mutable_dict['conda_packages']->prepend(mutable_dict['conda_packages'], '[conda] ')
A:torch.utils.collect_env.latest->max(dumps, key=os.path.getctime)
A:torch.utils.collect_env.ctime->os.path.getctime(latest)
A:torch.utils.collect_env.creation_time->datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')
torch.utils.collect_env.check_release_file(run_lambda)
torch.utils.collect_env.get_cachingallocator_config()
torch.utils.collect_env.get_clang_version(run_lambda)
torch.utils.collect_env.get_cmake_version(run_lambda)
torch.utils.collect_env.get_conda_packages(run_lambda)
torch.utils.collect_env.get_cudnn_version(run_lambda)
torch.utils.collect_env.get_env_info()
torch.utils.collect_env.get_gcc_version(run_lambda)
torch.utils.collect_env.get_gpu_info(run_lambda)
torch.utils.collect_env.get_libc_version()
torch.utils.collect_env.get_lsb_version(run_lambda)
torch.utils.collect_env.get_mac_version(run_lambda)
torch.utils.collect_env.get_nvidia_driver_version(run_lambda)
torch.utils.collect_env.get_nvidia_smi()
torch.utils.collect_env.get_os(run_lambda)
torch.utils.collect_env.get_pip_packages(run_lambda)
torch.utils.collect_env.get_platform()
torch.utils.collect_env.get_pretty_env_info()
torch.utils.collect_env.get_python_platform()
torch.utils.collect_env.get_running_cuda_version(run_lambda)
torch.utils.collect_env.get_windows_version(run_lambda)
torch.utils.collect_env.main()
torch.utils.collect_env.pretty_str(envinfo)
torch.utils.collect_env.run(command)
torch.utils.collect_env.run_and_parse_first_match(run_lambda,command,regex)
torch.utils.collect_env.run_and_read_all(run_lambda,command)
torch.utils.collect_env.run_and_return_first_line(run_lambda,command)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/cpp_extension.py----------------------------------------
A:torch.utils.cpp_extension.IS_MACOS->sys.platform.startswith('darwin')
A:torch.utils.cpp_extension.IS_LINUX->sys.platform.startswith('linux')
A:torch.utils.cpp_extension._HERE->os.path.abspath(__file__)
A:torch.utils.cpp_extension._TORCH_PATH->os.path.dirname(os.path.dirname(_HERE))
A:torch.utils.cpp_extension.TORCH_LIB_PATH->os.path.join(_TORCH_PATH, 'lib')
A:torch.utils.cpp_extension.nvcc->_join_cuda_home('bin', 'nvcc')
A:torch.utils.cpp_extension.cuda_home->os.path.dirname(os.path.dirname(nvcc))
A:torch.utils.cpp_extension.cuda_homes->glob.glob('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')
A:torch.utils.cpp_extension.pipe_hipcc->subprocess.Popen(['which hipcc | xargs readlink -f'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
A:torch.utils.cpp_extension.(hipcc, _)->subprocess.Popen(['which hipcc | xargs readlink -f'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()
A:torch.utils.cpp_extension.rocm_home->os.path.dirname(rocm_home)
A:torch.utils.cpp_extension.ROCM_HOME->_find_rocm_home()
A:torch.utils.cpp_extension.ROCM_VERSION->tuple((int(v) for v in torch.version.hip.split('.')[:2]))
A:torch.utils.cpp_extension.CUDA_HOME->_find_cuda_home()
A:torch.utils.cpp_extension.BUILT_FROM_SOURCE_VERSION_PATTERN->re.compile('\\d+\\.\\d+\\.\\d+\\w+\\+\\w+')
A:torch.utils.cpp_extension.JIT_EXTENSION_VERSIONER->ExtensionVersioner()
A:torch.utils.cpp_extension.which->subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.compiler_path->os.path.realpath(results[0].strip())
A:torch.utils.cpp_extension.version_string->subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT).decode(*SUBPROCESS_DECODE_ARGS)
A:torch.utils.cpp_extension.pattern->re.compile('^COLLECT_GCC=(.*)$', re.MULTILINE)
A:torch.utils.cpp_extension.results->re.findall(pattern, version_string)
A:torch.utils.cpp_extension.versionstr->subprocess.check_output([compiler, '-dumpfullversion', '-dumpversion'])
A:torch.utils.cpp_extension.version->ExtensionVersioner().bump_version_if_changed(name, sources, build_arguments=[extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths], build_directory=build_directory, with_cuda=with_cuda, is_python_module=is_python_module, is_standalone=is_standalone)
A:torch.utils.cpp_extension.compiler_info->subprocess.check_output(compiler, stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.match->re.search('(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode(*SUBPROCESS_DECODE_ARGS).strip())
A:torch.utils.cpp_extension.(_, error, _)->sys.exc_info()
A:torch.utils.cpp_extension.self.no_python_abi_suffix->kwargs.get('no_python_abi_suffix', False)
A:torch.utils.cpp_extension.self.use_ninja->kwargs.get('use_ninja', True)
A:torch.utils.cpp_extension.extension_iter->iter(self.extensions)
A:torch.utils.cpp_extension.extension->next(extension_iter, None)
A:torch.utils.cpp_extension.(_, ext)->os.path.splitext(source)
A:torch.utils.cpp_extension.val->getattr(torch._C, f'_PYBIND11_{name}')
A:torch.utils.cpp_extension.cpp_flag_prefix->cpp_format_prefix.format('std')
A:torch.utils.cpp_extension._ccbin->os.getenv('CC')
A:torch.utils.cpp_extension.paths[i]->os.path.abspath(paths[i])
A:torch.utils.cpp_extension.cflags->sanitize_flags(cflags)
A:torch.utils.cpp_extension.output_dir->os.path.abspath(output_dir)
A:torch.utils.cpp_extension.(_, objects, extra_postargs, pp_opts, _)->self.compiler._setup_compile(output_dir, macros, include_dirs, sources, depends, extra_postargs)
A:torch.utils.cpp_extension.common_cflags->self.compiler._get_cc_args(pp_opts, debug, extra_preargs)
A:torch.utils.cpp_extension.with_cuda->any(map(_is_cuda_file, sources))
A:torch.utils.cpp_extension.post_cflags->sanitize_flags(post_cflags)
A:torch.utils.cpp_extension.cuda_post_cflags->sanitize_flags(cuda_post_cflags)
A:torch.utils.cpp_extension.self.cflags->copy.deepcopy(extra_postargs)
A:torch.utils.cpp_extension.src_regex->re.compile('/T(p|c)(.*)')
A:torch.utils.cpp_extension.obj_regex->re.compile('/Fo(.*)')
A:torch.utils.cpp_extension.include_regex->re.compile('((\\-|\\/)I.*)')
A:torch.utils.cpp_extension.cuda_cflags->sanitize_flags(cuda_cflags)
A:torch.utils.cpp_extension.ext_filename->'.'.join(without_abi)
A:torch.utils.cpp_extension.ext_filename_parts->'.'.join(without_abi).split('.')
A:torch.utils.cpp_extension.compiler->os.environ.get('CXX', 'c++')
A:torch.utils.cpp_extension.cuda_version_str->subprocess.check_output([nvcc, '--version']).strip().decode(*SUBPROCESS_DECODE_ARGS)
A:torch.utils.cpp_extension.cuda_version->re.search('release (\\d+[.]\\d+)', cuda_version_str)
A:torch.utils.cpp_extension.cuda_str_version->re.search('release (\\d+[.]\\d+)', cuda_version_str).group(1)
A:torch.utils.cpp_extension.cuda_ver->pkg_resources.packaging.version.parse(cuda_str_version)
A:torch.utils.cpp_extension.torch_cuda_version->pkg_resources.packaging.version.parse(torch.version.cuda)
A:torch.utils.cpp_extension.extension.extra_compile_args->copy.deepcopy(extension.extra_compile_args)
A:torch.utils.cpp_extension.names->next(extension_iter, None).name.split('.')
A:torch.utils.cpp_extension.include_dirs->kwargs.get('include_dirs', [])
A:torch.utils.cpp_extension.library_dirs->kwargs.get('library_dirs', [])
A:torch.utils.cpp_extension.libraries->kwargs.get('libraries', [])
A:torch.utils.cpp_extension.build_dir->os.getcwd()
A:torch.utils.cpp_extension.hipify_result->hipify.hipify_python.hipify(project_directory=build_dir, output_directory=build_dir, includes=[os.path.join(os.path.relpath(include_dir, build_dir), '*') for include_dir in include_dirs] if include_dirs else ['*'], extra_files=[os.path.abspath(s) for s in sources], show_detailed=True, is_pytorch_extension=True)
A:torch.utils.cpp_extension.hipified_sources->set()
A:torch.utils.cpp_extension.s_abs->os.path.abspath(source)
A:torch.utils.cpp_extension.sources->list(hipified_sources)
A:torch.utils.cpp_extension.lib_include->os.path.join(_TORCH_PATH, 'include')
A:torch.utils.cpp_extension.cuda_home_include->_join_cuda_home('include')
A:torch.utils.cpp_extension.functions->dict(((f, f) for f in functions))
A:torch.utils.cpp_extension.cpp_source_path->os.path.join(build_directory, 'main.cpp')
A:torch.utils.cpp_extension.cuda_source_path->os.path.join(build_directory, 'cuda.cu')
A:torch.utils.cpp_extension.with_cudnn->any(['cudnn' in f for f in extra_ldflags or []])
A:torch.utils.cpp_extension.old_version->ExtensionVersioner().get_version(name)
A:torch.utils.cpp_extension.baton->FileBaton(os.path.join(build_directory, 'lock'))
A:torch.utils.cpp_extension.build_file_path->os.path.join(build_directory, 'build.ninja')
A:torch.utils.cpp_extension.extra_ldflags->_prepare_ldflags(extra_ldflags or [], with_cuda, verbose, is_standalone)
A:torch.utils.cpp_extension.python_path->os.path.dirname(sys.executable)
A:torch.utils.cpp_extension.python_lib_path->os.path.join(python_path, 'libs')
A:torch.utils.cpp_extension.named_arches->collections.OrderedDict([('Kepler+Tesla', '3.7'), ('Kepler', '3.5+PTX'), ('Maxwell+Tegra', '5.3'), ('Maxwell', '5.0;5.2+PTX'), ('Pascal', '6.0;6.1+PTX'), ('Volta', '7.0+PTX'), ('Turing', '7.5+PTX'), ('Ampere', '8.0;8.6+PTX')])
A:torch.utils.cpp_extension._arch_list->_arch_list.replace(named_arch, archval).replace(named_arch, archval)
A:torch.utils.cpp_extension.capability->min(max_supported_sm, capability)
A:torch.utils.cpp_extension.max_supported_sm->max(((sm // 10, sm % 10) for sm in supported_sm))
A:torch.utils.cpp_extension.arch_list->_arch_list.replace(named_arch, archval).replace(named_arch, archval).split(';')
A:torch.utils.cpp_extension._archs->os.environ.get('PYTORCH_ROCM_ARCH', None)
A:torch.utils.cpp_extension.archs->os.environ.get('PYTORCH_ROCM_ARCH', None).replace(' ', ';').split(';')
A:torch.utils.cpp_extension.root_extensions_directory->os.path.join(root_extensions_directory, build_folder)
A:torch.utils.cpp_extension.build_directory->os.path.join(root_extensions_directory, name)
A:torch.utils.cpp_extension.max_jobs->os.environ.get('MAX_JOBS')
A:torch.utils.cpp_extension.num_workers->_get_num_workers(verbose)
A:torch.utils.cpp_extension.env->os.environ.copy()
A:torch.utils.cpp_extension.plat_name->setuptools.distutils.util.get_platform()
A:torch.utils.cpp_extension.vc_env->setuptools.distutils._msvccompiler._get_vc_env(plat_spec)
A:torch.utils.cpp_extension.uk->k.upper()
A:torch.utils.cpp_extension.torch_lib_in_path->any((os.path.exists(p) and os.path.samefile(p, TORCH_LIB_PATH) for p in os.getenv('PATH', '').split(';')))
A:torch.utils.cpp_extension.filepath->os.path.join(path, f'{module_name}{LIB_EXT}')
A:torch.utils.cpp_extension.spec->importlib.util.spec_from_file_location(module_name, filepath)
A:torch.utils.cpp_extension.module->importlib.util.module_from_spec(spec)
A:torch.utils.cpp_extension.system_includes->include_paths(with_cuda)
A:torch.utils.cpp_extension.python_include_path->sysconfig.get_path('include', scheme='nt' if IS_WINDOWS else 'posix_prefix')
A:torch.utils.cpp_extension.pval->getattr(torch._C, f'_PYBIND11_{pname}')
A:torch.utils.cpp_extension.cuda_flags->_nt_quote_args(cuda_flags)
A:torch.utils.cpp_extension.ldflags->sanitize_flags(ldflags)
A:torch.utils.cpp_extension.required_cuda_version->pkg_resources.packaging.version.parse('10.2')
A:torch.utils.cpp_extension.source_file->source_file.replace(' ', '$ ').replace(' ', '$ ')
A:torch.utils.cpp_extension.object_file->object_file.replace(' ', '$ ').replace(' ', '$ ')
A:torch.utils.cpp_extension.cl_paths->subprocess.check_output(['where', 'cl']).decode(*SUBPROCESS_DECODE_ARGS).split('\r\n')
A:torch.utils.cpp_extension.cl_path->os.path.dirname(cl_paths[0]).replace(':', '$:')
A:torch.utils.cpp_extension.lines->'\n'.join(block)
torch.utils.cpp_extension.BuildExtension(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension.__init__(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension._add_compile_flag(self,extension,flag)
torch.utils.cpp_extension.BuildExtension._add_gnu_cpp_abi_flag(self,extension)
torch.utils.cpp_extension.BuildExtension._check_abi(self)
torch.utils.cpp_extension.BuildExtension._check_cuda_version(self)
torch.utils.cpp_extension.BuildExtension._define_torch_extension_name(self,extension)
torch.utils.cpp_extension.BuildExtension.build_extensions(self)->None
torch.utils.cpp_extension.BuildExtension.finalize_options(self)->None
torch.utils.cpp_extension.BuildExtension.get_ext_filename(self,ext_name)
torch.utils.cpp_extension.BuildExtension.with_options(cls,**options)
torch.utils.cpp_extension.CUDAExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension.CppExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension._accepted_compilers_for_platform()->List[str]
torch.utils.cpp_extension._find_cuda_home()->Optional[str]
torch.utils.cpp_extension._find_rocm_home()->Optional[str]
torch.utils.cpp_extension._get_build_directory(name:str,verbose:bool)->str
torch.utils.cpp_extension._get_cuda_arch_flags(cflags:Optional[List[str]]=None)->List[str]
torch.utils.cpp_extension._get_exec_path(module_name,path)
torch.utils.cpp_extension._get_num_workers(verbose:bool)->Optional[int]
torch.utils.cpp_extension._get_rocm_arch_flags(cflags:Optional[List[str]]=None)->List[str]
torch.utils.cpp_extension._import_module_from_library(module_name,path,is_python_module)
torch.utils.cpp_extension._is_binary_build()->bool
torch.utils.cpp_extension._is_cuda_file(path:str)->bool
torch.utils.cpp_extension._jit_compile(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory:str,verbose:bool,with_cuda:Optional[bool],is_python_module,is_standalone,keep_intermediates=True)->None
torch.utils.cpp_extension._join_cuda_home(*paths)->str
torch.utils.cpp_extension._join_rocm_home(*paths)->str
torch.utils.cpp_extension._nt_quote_args(args:Optional[List[str]])->List[str]
torch.utils.cpp_extension._prepare_ldflags(extra_ldflags,with_cuda,verbose,is_standalone)
torch.utils.cpp_extension._run_ninja_build(build_directory:str,verbose:bool,error_prefix:str)->None
torch.utils.cpp_extension._write_ninja_file(path,cflags,post_cflags,cuda_cflags,cuda_post_cflags,sources,objects,ldflags,library_target,with_cuda)->None
torch.utils.cpp_extension._write_ninja_file_and_build_library(name,sources:List[str],extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory:str,verbose:bool,with_cuda:Optional[bool],is_standalone:bool=False)->None
torch.utils.cpp_extension._write_ninja_file_and_compile_objects(sources:List[str],objects,cflags,post_cflags,cuda_cflags,cuda_post_cflags,build_directory:str,verbose:bool,with_cuda:Optional[bool])->None
torch.utils.cpp_extension._write_ninja_file_to_build_library(path,name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,with_cuda,is_standalone)->None
torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)->bool
torch.utils.cpp_extension.check_compiler_ok_for_platform(compiler:str)->bool
torch.utils.cpp_extension.get_default_build_root()->str
torch.utils.cpp_extension.include_paths(cuda:bool=False)->List[str]
torch.utils.cpp_extension.is_ninja_available()
torch.utils.cpp_extension.library_paths(cuda:bool=False)->List[str]
torch.utils.cpp_extension.load(name,sources:Union[str,List[str]],extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda:Optional[bool]=None,is_python_module=True,is_standalone=False,keep_intermediates=True)
torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True,with_pytorch_error_handling=True,keep_intermediates=True)
torch.utils.cpp_extension.verify_ninja_availability()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/_pytree.py----------------------------------------
A:torch.utils._pytree.SUPPORTED_NODES[typ]->NodeDef(flatten_fn, unflatten_fn)
A:torch.utils._pytree.typ->type(pytree)
A:torch.utils._pytree.fields->getattr(typ, '_fields', None)
A:torch.utils._pytree.node_type->_get_node_type(pytree)
A:torch.utils._pytree.(child_pytrees, context)->flatten_fn(pytree)
A:torch.utils._pytree.(flat, child_spec)->tree_flatten(child)
A:torch.utils._pytree.(flat_args, spec)->tree_flatten(pytree)
A:torch.utils._pytree.(child_pytrees, ctx)->flatten_fn(pytree)
A:torch.utils._pytree.flat->_broadcast_to_and_flatten(child, child_spec)
torch.utils._pytree.LeafSpec(self)
torch.utils._pytree.LeafSpec.__init__(self)
torch.utils._pytree.LeafSpec.__repr__(self)->str
torch.utils._pytree.NodeDef(NamedTuple)
torch.utils._pytree.TreeSpec(self,typ:Any,context:Context,children_specs:List['TreeSpec'])
torch.utils._pytree.TreeSpec.__eq__(self,other:Any)->bool
torch.utils._pytree.TreeSpec.__init__(self,typ:Any,context:Context,children_specs:List['TreeSpec'])
torch.utils._pytree.TreeSpec.__ne__(self,other:Any)->bool
torch.utils._pytree.TreeSpec.__repr__(self)->str
torch.utils._pytree._broadcast_to_and_flatten(pytree:PyTree,spec:TreeSpec)->Optional[List[Any]]
torch.utils._pytree._dict_flatten(d:Dict[Any,Any])->Tuple[List[Any], Context]
torch.utils._pytree._dict_unflatten(values:List[Any],context:Context)->Dict[Any, Any]
torch.utils._pytree._get_node_type(pytree:Any)->Any
torch.utils._pytree._is_leaf(pytree:PyTree)->bool
torch.utils._pytree._is_namedtuple_instance(pytree:Any)->bool
torch.utils._pytree._list_flatten(d:List[Any])->Tuple[List[Any], Context]
torch.utils._pytree._list_unflatten(values:List[Any],context:Context)->List[Any]
torch.utils._pytree._namedtuple_flatten(d:NamedTuple)->Tuple[List[Any], Context]
torch.utils._pytree._namedtuple_unflatten(values:List[Any],context:Context)->NamedTuple
torch.utils._pytree._register_pytree_node(typ:Any,flatten_fn:FlattenFunc,unflatten_fn:UnflattenFunc)->None
torch.utils._pytree._tuple_flatten(d:Tuple[Any,...])->Tuple[List[Any], Context]
torch.utils._pytree._tuple_unflatten(values:List[Any],context:Context)->Tuple[Any, ...]
torch.utils._pytree.tree_flatten(pytree:PyTree)->Tuple[List[Any], TreeSpec]
torch.utils._pytree.tree_map(fn:Any,pytree:PyTree)->PyTree
torch.utils._pytree.tree_unflatten(values:List[Any],spec:TreeSpec)->PyTree


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/hooks.py----------------------------------------
A:torch.utils.hooks.self.hooks_dict_ref->weakref.ref(state[0])
A:torch.utils.hooks.hooks_dict->self.hooks_dict_ref()
A:torch.utils.hooks.RemovableHandle.next_id->max(RemovableHandle.next_id, self.id + 1)
A:torch.utils.hooks.grad_input->self._pack_with_none(self.input_tensors_index, grad_input, self.n_inputs)
A:torch.utils.hooks.res->user_hook(self.module, grad_inputs, self.grad_outputs)
A:torch.utils.hooks.new_tensors->torch.nn.modules._functions.BackwardHookFunction.apply(*tensors)
A:torch.utils.hooks.arg_list->list(args)
A:torch.utils.hooks.(res, input_idx)->self._apply_on_tensors(fn, args)
A:torch.utils.hooks.self.n_inputs->len(args)
A:torch.utils.hooks.self.grad_outputs->self._pack_with_none(self.output_tensors_index, grad_output, self.n_outputs)
A:torch.utils.hooks.grad_inputs->self._pack_with_none([], [], self.n_inputs)
A:torch.utils.hooks.(res, output_idx)->self._apply_on_tensors(fn, args)
A:torch.utils.hooks.self.n_outputs->len(args)
torch.utils.hooks.BackwardHook(self,module,user_hooks)
torch.utils.hooks.BackwardHook.__init__(self,module,user_hooks)
torch.utils.hooks.BackwardHook._apply_on_tensors(self,fn,args)
torch.utils.hooks.BackwardHook._pack_with_none(self,indices,values,size)
torch.utils.hooks.BackwardHook._set_user_hook(self,grad_fn,user_hook)
torch.utils.hooks.BackwardHook._unpack_none(self,indices,values)
torch.utils.hooks.BackwardHook.setup_input_hook(self,args)
torch.utils.hooks.BackwardHook.setup_output_hook(self,args)
torch.utils.hooks.RemovableHandle(self,hooks_dict:Any)
torch.utils.hooks.RemovableHandle.__enter__(self)->'RemovableHandle'
torch.utils.hooks.RemovableHandle.__exit__(self,type:Any,value:Any,tb:Any)->None
torch.utils.hooks.RemovableHandle.__getstate__(self)
torch.utils.hooks.RemovableHandle.__init__(self,hooks_dict:Any)
torch.utils.hooks.RemovableHandle.__setstate__(self,state)->None
torch.utils.hooks.RemovableHandle.remove(self)->None
torch.utils.hooks.unserializable_hook(f)
torch.utils.hooks.warn_if_has_hooks(tensor)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/checkpoint.py----------------------------------------
A:torch.utils.checkpoint.x->inp.detach()
A:torch.utils.checkpoint.fwd_gpu_devices->list(set((arg.get_device() for arg in args if isinstance(arg, torch.Tensor) and arg.is_cuda)))
A:torch.utils.checkpoint.ctx.fwd_cpu_state->torch.get_rng_state()
A:torch.utils.checkpoint.(ctx.fwd_gpu_devices, ctx.fwd_gpu_states)->get_device_states(*args)
A:torch.utils.checkpoint.outputs->ctx.run_function(*detached_inputs)
A:torch.utils.checkpoint.inputs->list(ctx.inputs)
A:torch.utils.checkpoint.detached_inputs->detach_variable(tuple(inputs))
A:torch.utils.checkpoint.grads->tuple((inp.grad if isinstance(inp, torch.Tensor) else None for inp in detached_inputs))
A:torch.utils.checkpoint.preserve->kwargs.pop('preserve_rng_state', True)
A:torch.utils.checkpoint.input->checkpoint(run_function(start, end, functions), input, preserve_rng_state=preserve)
A:torch.utils.checkpoint.functions->list(functions.children())
A:torch.utils.checkpoint.had_autocast_in_fwd->torch.is_autocast_enabled()
A:torch.utils.checkpoint.fwd_cpu_state->torch.get_rng_state()
A:torch.utils.checkpoint.(fwd_gpu_devices, fwd_gpu_states)->get_device_states(*args)
A:torch.utils.checkpoint._unused->function(*args)
A:torch.utils.checkpoint.output->function(*args)
torch.utils.checkpoint.CheckpointFunction(torch.autograd.Function)
torch.utils.checkpoint.CheckpointFunction.backward(ctx,*args)
torch.utils.checkpoint.CheckpointFunction.forward(ctx,run_function,preserve_rng_state,*args)
torch.utils.checkpoint._checkpoint_without_reentrant(function,preserve_rng_state=True,*args)
torch.utils.checkpoint.check_backward_validity(inputs:Iterable[Any])->None
torch.utils.checkpoint.checkpoint(function,*args,use_reentrant:bool=True,**kwargs)
torch.utils.checkpoint.checkpoint_sequential(functions,segments,input,**kwargs)
torch.utils.checkpoint.detach_variable(inputs:Tuple[Any,...])->Tuple[torch.Tensor, ...]
torch.utils.checkpoint.get_device_states(*args)->Tuple[List[int], List[torch.Tensor]]
torch.utils.checkpoint.set_device_states(devices,states)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/throughput_benchmark.py----------------------------------------
A:torch.utils.throughput_benchmark.self._benchmark->torch._C.ThroughputBenchmark(module)
A:torch.utils.throughput_benchmark.config->torch._C.BenchmarkConfig()
A:torch.utils.throughput_benchmark.c_stats->self._benchmark.benchmark(config)
torch.utils.ThroughputBenchmark(self,module)
torch.utils.ThroughputBenchmark.add_input(self,*args,**kwargs)
torch.utils.ThroughputBenchmark.benchmark(self,num_calling_threads=1,num_warmup_iters=10,num_iters=100,profiler_output_path='')
torch.utils.ThroughputBenchmark.run_once(self,*args,**kwargs)
torch.utils.throughput_benchmark.ExecutionStats(self,c_stats,benchmark_config)
torch.utils.throughput_benchmark.ExecutionStats.__init__(self,c_stats,benchmark_config)
torch.utils.throughput_benchmark.ExecutionStats.__str__(self)
torch.utils.throughput_benchmark.ExecutionStats.iters_per_second(self)
torch.utils.throughput_benchmark.ExecutionStats.latency_avg_ms(self)
torch.utils.throughput_benchmark.ExecutionStats.num_iters(self)
torch.utils.throughput_benchmark.ExecutionStats.total_time_seconds(self)
torch.utils.throughput_benchmark.ThroughputBenchmark(self,module)
torch.utils.throughput_benchmark.ThroughputBenchmark.__init__(self,module)
torch.utils.throughput_benchmark.ThroughputBenchmark.add_input(self,*args,**kwargs)
torch.utils.throughput_benchmark.ThroughputBenchmark.benchmark(self,num_calling_threads=1,num_warmup_iters=10,num_iters=100,profiler_output_path='')
torch.utils.throughput_benchmark.ThroughputBenchmark.run_once(self,*args,**kwargs)
torch.utils.throughput_benchmark.format_time(time_us=None,time_ms=None,time_s=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/show_pickle.py----------------------------------------
A:torch.utils.show_pickle.dispatch->dict(pickle._Unpickler.dispatch)
A:torch.utils.show_pickle.(strlen,)->struct.unpack('<I', self.read(4))
A:torch.utils.show_pickle.str_bytes->self.read(strlen)
A:torch.utils.show_pickle.obj->FakeObject('builtin', 'UnicodeDecodeError', (str(exn),))
A:torch.utils.show_pickle.value->cls(in_stream).load()
A:torch.utils.show_pickle.(zfname, mname)->fname.split('@', 1)
torch.utils.show_pickle.DumpUnpickler(self,file,*,catch_invalid_utf8=False,**kwargs)
torch.utils.show_pickle.DumpUnpickler.__init__(self,file,*,catch_invalid_utf8=False,**kwargs)
torch.utils.show_pickle.DumpUnpickler.dump(cls,in_stream,out_stream)
torch.utils.show_pickle.DumpUnpickler.find_class(self,module,name)
torch.utils.show_pickle.DumpUnpickler.load_binunicode(self)
torch.utils.show_pickle.DumpUnpickler.persistent_load(self,pid)
torch.utils.show_pickle.FakeClass(self,module,name)
torch.utils.show_pickle.FakeClass.__init__(self,module,name)
torch.utils.show_pickle.FakeClass.__repr__(self)
torch.utils.show_pickle.FakeClass.fake_new(self,*args)
torch.utils.show_pickle.FakeObject(self,module,name,args)
torch.utils.show_pickle.FakeObject.__init__(self,module,name,args)
torch.utils.show_pickle.FakeObject.__repr__(self)
torch.utils.show_pickle.FakeObject.__setstate__(self,state)
torch.utils.show_pickle.FakeObject.pp_format(printer,obj,stream,indent,allowance,context,level)
torch.utils.show_pickle.main(argv,output_stream=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/file_baton.py----------------------------------------
A:torch.utils.file_baton.self.fd->os.open(self.lock_file_path, os.O_CREAT | os.O_EXCL)
torch.utils.file_baton.FileBaton(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.__init__(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.release(self)
torch.utils.file_baton.FileBaton.try_acquire(self)
torch.utils.file_baton.FileBaton.wait(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/_python_dispatch.py----------------------------------------
torch.utils._python_dispatch.enable_python_mode(cls)->Iterator[None]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/dlpack.py----------------------------------------
A:torch.utils.dlpack.device->ext_tensor.__dlpack_device__()
A:torch.utils.dlpack.stream->torch.cuda.current_stream('cuda:{}'.format(device[1]))
A:torch.utils.dlpack.dlpack->ext_tensor.__dlpack__()
torch.from_dlpack(ext_tensor:Any)->torch.Tensor
torch.utils.dlpack.DLDeviceType(enum.IntEnum)
torch.utils.dlpack.from_dlpack(ext_tensor:Any)->torch.Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/_cpp_extension_versioner.py----------------------------------------
A:torch.utils._cpp_extension_versioner.Entry->collections.namedtuple('Entry', 'version, hash')
A:torch.utils._cpp_extension_versioner.hash_value->update_hash(hash_value, is_standalone)
A:torch.utils._cpp_extension_versioner.entry->self.entries.get(name)
A:torch.utils._cpp_extension_versioner.self.entries[name]entry->Entry(entry.version + 1, hash_value)
torch.utils._cpp_extension_versioner.ExtensionVersioner(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.__init__(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.bump_version_if_changed(self,name,source_files,build_arguments,build_directory,with_cuda,is_python_module,is_standalone)
torch.utils._cpp_extension_versioner.ExtensionVersioner.get_version(self,name)
torch.utils._cpp_extension_versioner.hash_build_arguments(hash_value,build_arguments)
torch.utils._cpp_extension_versioner.hash_source_files(hash_value,source_files)
torch.utils._cpp_extension_versioner.update_hash(seed,value)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/_freeze.py----------------------------------------
A:torch.utils._freeze.FAKE_PREFIX->MAIN_PREFIX_TEMPLATE.format('_PyImport_FrozenModules')
A:torch.utils._freeze.ret->fn(*args, **kwargs)
A:torch.utils._freeze.it->itertools.cycle(bytecode_files)
A:torch.utils._freeze.is_package_dir->any([child.name == '__init__.py' for child in path.iterdir()])
A:torch.utils._freeze.normalized_path->file_path.relative_to(top_package_path.parent)
A:torch.utils._freeze.module_qualname->self.get_module_qualname(path, top_package_path)
A:torch.utils._freeze.module_mangled_name->'__'.join(module_qualname)
A:torch.utils._freeze.co->self.compile_string(src_file.read())
A:torch.utils._freeze.bytecode->marshal.dumps(co)
A:torch.utils._freeze.size->len(bytecode)
A:torch.utils._freeze.parser->argparse.ArgumentParser(description='Compile py source')
A:torch.utils._freeze.args->argparse.ArgumentParser(description='Compile py source').parse_args()
A:torch.utils._freeze.f->Freezer(args.verbose)
A:torch.utils._freeze.path->Path(p)
torch.utils._freeze.Freezer(self,verbose:bool)
torch.utils._freeze.Freezer.__init__(self,verbose:bool)
torch.utils._freeze.Freezer.compile_file(self,path:Path,top_package_path:Path)
torch.utils._freeze.Freezer.compile_package(self,path:Path,top_package_path:Path)
torch.utils._freeze.Freezer.compile_path(self,path:Path,top_package_path:Path)
torch.utils._freeze.Freezer.compile_string(self,file_content:str)->types.CodeType
torch.utils._freeze.Freezer.get_module_qualname(self,file_path:Path,top_package_path:Path)->List[str]
torch.utils._freeze.Freezer.msg(self,path:Path,code:str)
torch.utils._freeze.Freezer.write_bytecode(self,install_root)
torch.utils._freeze.Freezer.write_frozen(self,m:FrozenModule,outfp)
torch.utils._freeze.Freezer.write_main(self,install_root,oss,symbol_name)
torch.utils._freeze.FrozenModule
torch.utils._freeze.indent_msg(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/__init__.py----------------------------------------
A:torch.utils.__init__.cmake_prefix_path->os.path.join(_osp.dirname(_osp.dirname(__file__)), 'share', 'cmake')
torch.utils.__init__.set_module(obj,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/model_zoo.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/mkldnn.py----------------------------------------
A:torch.utils.mkldnn.self.weight->state[0].to_mkldnn()
A:torch.utils.mkldnn.self.bias->state[1].to_mkldnn()
A:torch.utils.mkldnn.y_mkldnn->torch._C._nn.mkldnn_linear(x_mkldnn, self.weight, self.bias)
A:torch.utils.mkldnn.weight->self.weight.to_dense()
A:torch.utils.mkldnn.bias->self.bias.to_dense()
A:torch.utils.mkldnn.running_mean->self.running_mean.to_dense()
A:torch.utils.mkldnn.running_var->self.running_var.to_dense()
A:torch.utils.mkldnn.self.running_mean->state[2].to_mkldnn()
A:torch.utils.mkldnn.self.running_var->state[3].to_mkldnn()
A:torch.utils.mkldnn.new_m->m_fn(m, d)
torch.utils.mkldnn.MkldnnBatchNorm(self,dense_module)
torch.utils.mkldnn.MkldnnBatchNorm.__getstate__(self)
torch.utils.mkldnn.MkldnnBatchNorm.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnBatchNorm.__setstate__(self,state)
torch.utils.mkldnn.MkldnnBatchNorm.forward(self,x)
torch.utils.mkldnn.MkldnnConv1d(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnConv1d.__init__(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnConv1d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnConv2d(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnConv2d.__init__(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnConv2d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnConv3d(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnConv3d.__init__(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnConv3d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnLinear(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnLinear.__getstate__(self)
torch.utils.mkldnn.MkldnnLinear.__init__(self,dense_module,dtype)
torch.utils.mkldnn.MkldnnLinear.__setstate__(self,state)
torch.utils.mkldnn.MkldnnLinear.forward(self,x)
torch.utils.mkldnn._MkldnnConvNd(self,dense_module)
torch.utils.mkldnn._MkldnnConvNd.__getstate__(self)
torch.utils.mkldnn._MkldnnConvNd.__init__(self,dense_module)
torch.utils.mkldnn._MkldnnConvNd.forward(self,x)
torch.utils.mkldnn.to_mkldnn(module,dtype=torch.float)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/mobile_optimizer.py----------------------------------------
A:torch.utils.mobile_optimizer.optimization_blocklist->set()
A:torch.utils.mobile_optimizer.bundled_inputs_attributes->_get_bundled_inputs_preserved_attributes(script_module, preserved_methods_str)
A:torch.utils.mobile_optimizer.preserved_methods_str->list(set(preserved_methods_str + bundled_inputs_attributes))
A:torch.utils.mobile_optimizer.backend->backend.lower().lower()
A:torch.utils.mobile_optimizer.optimized_cpp_module->torch._C._jit_pass_metal_optimize_for_mobile(script_module._c, preserved_methods_str)
A:torch.utils.mobile_optimizer.op_names->torch.jit.export_opnames(script_module)
A:torch.utils.mobile_optimizer.all_info->script_module.get_bundled_inputs_functions_and_info()
torch.utils.mobile_optimizer.LintCode(Enum)
torch.utils.mobile_optimizer._get_bundled_inputs_preserved_attributes(script_module:torch.jit.ScriptModule,preserved_methods:List[str])->List[str]
torch.utils.mobile_optimizer.generate_mobile_module_lints(script_module:torch.jit.ScriptModule)
torch.utils.mobile_optimizer.optimize_for_mobile(script_module:torch.jit.ScriptModule,optimization_blocklist:Optional[Set[MobileOptimizerType]]=None,preserved_methods:Optional[List[AnyStr]]=None,backend:str='CPU')->torch.jit.RecursiveScriptModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/_crash_handler.py----------------------------------------
A:torch.utils._crash_handler.DEFAULT_MINIDUMP_DIR->str(pathlib.Path.home() / 'AppData' / 'pytorch_crashes')
torch.utils._crash_handler.disable_minidumps()
torch.utils._crash_handler.enable_minidumps(directory=DEFAULT_MINIDUMP_DIR)
torch.utils._crash_handler.enable_minidumps_on_exceptions()
torch.utils.disable_minidumps()
torch.utils.enable_minidumps(directory=DEFAULT_MINIDUMP_DIR)
torch.utils.enable_minidumps_on_exceptions()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/hipify/cuda_to_hip_mappings.py----------------------------------------
A:torch.utils.hipify.cuda_to_hip_mappings.MATH_TRANSPILATIONS->collections.OrderedDict([('std::max', '::max'), ('std::min', '::min'), ('std::ceil', '::ceil'), ('std::floor', '::floor'), ('std::exp', '::exp'), ('std::log', '::log'), ('std::pow', '::pow'), ('std::fabs', '::fabs'), ('std::fmod', '::fmod'), ('std::remainder', '::remainder'), ('std::frexp', '::frexp')])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_TYPE_NAME_MAP->collections.OrderedDict([('CUresult', ('hipError_t', CONV_TYPE, API_DRIVER)), ('cudaError_t', ('hipError_t', CONV_TYPE, API_RUNTIME)), ('CUDA_ARRAY3D_DESCRIPTOR', ('HIP_ARRAY3D_DESCRIPTOR', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY_DESCRIPTOR', ('HIP_ARRAY_DESCRIPTOR', CONV_TYPE, API_DRIVER)), ('CUDA_MEMCPY2D', ('hip_Memcpy2D', CONV_TYPE, API_DRIVER)), ('CUDA_MEMCPY3D', ('HIP_MEMCPY3D', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_MEMCPY3D_PEER', ('HIP_MEMCPY3D_PEER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_POINTER_ATTRIBUTE_P2P_TOKENS', ('HIP_POINTER_ATTRIBUTE_P2P_TOKENS', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_RESOURCE_DESC', ('HIP_RESOURCE_DESC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_RESOURCE_VIEW_DESC', ('HIP_RESOURCE_VIEW_DESC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUipcEventHandle', ('hipIpcEventHandle', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUipcMemHandle', ('hipIpcMemHandle', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUaddress_mode', ('hipAddress_mode', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUarray_cubemap_face', ('hipArray_cubemap_face', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUarray_format', ('hipArray_format', CONV_TYPE, API_DRIVER)), ('CUcomputemode', ('hipComputemode', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmem_advise', ('hipMemAdvise', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmem_range_attribute', ('hipMemRangeAttribute', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUctx_flags', ('hipCctx_flags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUdevice', ('hipDevice_t', CONV_TYPE, API_DRIVER)), ('CUdevice_attribute_enum', ('hipDeviceAttribute_t', CONV_TYPE, API_DRIVER)), ('CUdevice_attribute', ('hipDeviceAttribute_t', CONV_TYPE, API_DRIVER)), ('CUdeviceptr', ('hipDeviceptr_t', CONV_TYPE, API_DRIVER)), ('CUarray_st', ('hipArray', CONV_TYPE, API_DRIVER)), ('CUarray', ('hipArray *', CONV_TYPE, API_DRIVER)), ('CUdevprop_st', ('hipDeviceProp_t', CONV_TYPE, API_DRIVER)), ('CUdevprop', ('hipDeviceProp_t', CONV_TYPE, API_DRIVER)), ('CUfunction', ('hipFunction_t', CONV_TYPE, API_DRIVER)), ('CUgraphicsResource', ('hipGraphicsResource_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmipmappedArray', ('hipMipmappedArray_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUfunction_attribute', ('hipFuncAttribute_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUfunction_attribute_enum', ('hipFuncAttribute_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsMapResourceFlags', ('hipGraphicsMapFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsMapResourceFlags_enum', ('hipGraphicsMapFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsRegisterFlags', ('hipGraphicsRegisterFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUgraphicsRegisterFlags_enum', ('hipGraphicsRegisterFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUoccupancy_flags', ('hipOccupancyFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUoccupancy_flags_enum', ('hipOccupancyFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUfunc_cache_enum', ('hipFuncCache', CONV_TYPE, API_DRIVER)), ('CUfunc_cache', ('hipFuncCache', CONV_TYPE, API_DRIVER)), ('CUipcMem_flags', ('hipIpcMemFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUipcMem_flags_enum', ('hipIpcMemFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_cacheMode', ('hipJitCacheMode', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_cacheMode_enum', ('hipJitCacheMode', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_fallback', ('hipJitFallback', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_fallback_enum', ('hipJitFallback', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_option', ('hipJitOption', CONV_JIT, API_DRIVER)), ('CUjit_option_enum', ('hipJitOption', CONV_JIT, API_DRIVER)), ('CUjit_target', ('hipJitTarget', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjit_target_enum', ('hipJitTarget', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjitInputType', ('hipJitInputType', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUjitInputType_enum', ('hipJitInputType', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CUlimit', ('hipLimit_t', CONV_TYPE, API_DRIVER)), ('CUlimit_enum', ('hipLimit_t', CONV_TYPE, API_DRIVER)), ('CUmemAttach_flags', ('hipMemAttachFlags_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmemAttach_flags_enum', ('hipMemAttachFlags_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmemorytype', ('hipMemType_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUmemorytype_enum', ('hipMemType_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUresourcetype', ('hipResourceType', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CUresourcetype_enum', ('hipResourceType', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CUresourceViewFormat', ('hipResourceViewFormat', CONV_TEX, API_DRIVER)), ('CUresourceViewFormat_enum', ('hipResourceViewFormat', CONV_TEX, API_DRIVER)), ('CUsharedconfig', ('hipSharedMemConfig', CONV_TYPE, API_DRIVER)), ('CUsharedconfig_enum', ('hipSharedMemConfig', CONV_TYPE, API_DRIVER)), ('CUcontext', ('hipCtx_t', CONV_TYPE, API_DRIVER)), ('CUmodule', ('hipModule_t', CONV_TYPE, API_DRIVER)), ('CUstream', ('hipStream_t', CONV_TYPE, API_DRIVER)), ('CUstream_st', ('ihipStream_t', CONV_TYPE, API_DRIVER)), ('CUstreamCallback', ('hipStreamCallback_t', CONV_TYPE, API_DRIVER)), ('CUsurfObject', ('hipSurfaceObject', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUsurfref', ('hipSurfaceReference_t', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUtexObject', ('hipTextureObject_t', CONV_TYPE, API_DRIVER)), ('CUtexref', ('textureReference', CONV_TYPE, API_DRIVER)), ('CUstream_flags', ('hipStreamFlags', CONV_TYPE, API_DRIVER)), ('CUstreamWaitValue_flags', ('hipStreamWaitValueFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUstreamWriteValue_flags', ('hipStreamWriteValueFlags', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUstreamBatchMemOpType', ('hipStreamBatchMemOpType', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUdevice_P2PAttribute', ('hipDeviceP2PAttribute', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUevent', ('hipEvent_t', CONV_TYPE, API_DRIVER)), ('CUevent_st', ('ihipEvent_t', CONV_TYPE, API_DRIVER)), ('CUevent_flags', ('hipEventFlags', CONV_EVENT, API_DRIVER, HIP_UNSUPPORTED)), ('CUfilter_mode', ('hipTextureFilterMode', CONV_TEX, API_DRIVER)), ('CUGLDeviceList', ('hipGLDeviceList', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CUGLmap_flags', ('hipGLMapFlags', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d9DeviceList', ('hipD3D9DeviceList', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d9map_flags', ('hipD3D9MapFlags', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d9register_flags', ('hipD3D9RegisterFlags', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d10DeviceList', ('hipd3d10DeviceList', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d10map_flags', ('hipD3D10MapFlags', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d10register_flags', ('hipD3D10RegisterFlags', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CUd3d11DeviceList', ('hipd3d11DeviceList', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('CUeglStreamConnection_st', ('hipEglStreamConnection', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('CUeglStreamConnection', ('hipEglStreamConnection', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('libraryPropertyType_t', ('hipLibraryPropertyType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('libraryPropertyType', ('hipLibraryPropertyType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamCallback_t', ('hipStreamCallback_t', CONV_TYPE, API_RUNTIME)), ('cudaArray', ('hipArray', CONV_MEM, API_RUNTIME)), ('cudaArray_t', ('hipArray_t', CONV_MEM, API_RUNTIME)), ('cudaArray_const_t', ('hipArray_const_t', CONV_MEM, API_RUNTIME)), ('cudaMipmappedArray_t', ('hipMipmappedArray_t', CONV_MEM, API_RUNTIME)), ('cudaMipmappedArray_const_t', ('hipMipmappedArray_const_t', CONV_MEM, API_RUNTIME)), ('cudaArrayDefault', ('hipArrayDefault', CONV_MEM, API_RUNTIME)), ('cudaArrayLayered', ('hipArrayLayered', CONV_MEM, API_RUNTIME)), ('cudaArraySurfaceLoadStore', ('hipArraySurfaceLoadStore', CONV_MEM, API_RUNTIME)), ('cudaArrayCubemap', ('hipArrayCubemap', CONV_MEM, API_RUNTIME)), ('cudaArrayTextureGather', ('hipArrayTextureGather', CONV_MEM, API_RUNTIME)), ('cudaMemoryAdvise', ('hipMemAdvise', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttribute', ('hipMemRangeAttribute', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyKind', ('hipMemcpyKind', CONV_MEM, API_RUNTIME)), ('cudaMemoryType', ('hipMemoryType', CONV_MEM, API_RUNTIME)), ('cudaExtent', ('hipExtent', CONV_MEM, API_RUNTIME)), ('cudaPitchedPtr', ('hipPitchedPtr', CONV_MEM, API_RUNTIME)), ('cudaPos', ('hipPos', CONV_MEM, API_RUNTIME)), ('cudaEvent_t', ('hipEvent_t', CONV_TYPE, API_RUNTIME)), ('cudaStream_t', ('hipStream_t', CONV_TYPE, API_RUNTIME)), ('cudaPointerAttributes', ('hipPointerAttribute_t', CONV_TYPE, API_RUNTIME)), ('cudaDeviceAttr', ('hipDeviceAttribute_t', CONV_TYPE, API_RUNTIME)), ('cudaDeviceProp', ('hipDeviceProp_t', CONV_TYPE, API_RUNTIME)), ('cudaDeviceP2PAttr', ('hipDeviceP2PAttribute', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeMode', ('hipComputeMode', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFuncCache', ('hipFuncCache_t', CONV_CACHE, API_RUNTIME)), ('cudaFuncAttributes', ('hipFuncAttributes', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSharedMemConfig', ('hipSharedMemConfig', CONV_TYPE, API_RUNTIME)), ('cudaLimit', ('hipLimit_t', CONV_TYPE, API_RUNTIME)), ('cudaOutputMode', ('hipOutputMode', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaTextureReadMode', ('hipTextureReadMode', CONV_TEX, API_RUNTIME)), ('cudaTextureFilterMode', ('hipTextureFilterMode', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKind', ('hipChannelFormatKind', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatDesc', ('hipChannelFormatDesc', CONV_TEX, API_RUNTIME)), ('cudaResourceDesc', ('hipResourceDesc', CONV_TEX, API_RUNTIME)), ('cudaResourceViewDesc', ('hipResourceViewDesc', CONV_TEX, API_RUNTIME)), ('cudaTextureDesc', ('hipTextureDesc', CONV_TEX, API_RUNTIME)), ('surfaceReference', ('hipSurfaceReference', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaTextureObject_t', ('hipTextureObject_t', CONV_TEX, API_RUNTIME)), ('cudaResourceType', ('hipResourceType', CONV_TEX, API_RUNTIME)), ('cudaResourceViewFormat', ('hipResourceViewFormat', CONV_TEX, API_RUNTIME)), ('cudaTextureAddressMode', ('hipTextureAddressMode', CONV_TEX, API_RUNTIME)), ('cudaSurfaceBoundaryMode', ('hipSurfaceBoundaryMode', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSurfaceFormatMode', ('hipSurfaceFormatMode', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaTextureType1D', ('hipTextureType1D', CONV_TEX, API_RUNTIME)), ('cudaTextureType2D', ('hipTextureType2D', CONV_TEX, API_RUNTIME)), ('cudaTextureType3D', ('hipTextureType3D', CONV_TEX, API_RUNTIME)), ('cudaTextureTypeCubemap', ('hipTextureTypeCubemap', CONV_TEX, API_RUNTIME)), ('cudaTextureType1DLayered', ('hipTextureType1DLayered', CONV_TEX, API_RUNTIME)), ('cudaTextureType2DLayered', ('hipTextureType2DLayered', CONV_TEX, API_RUNTIME)), ('cudaTextureTypeCubemapLayered', ('hipTextureTypeCubemapLayered', CONV_TEX, API_RUNTIME)), ('cudaIpcEventHandle_t', ('hipIpcEventHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaIpcEventHandle_st', ('hipIpcEventHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaIpcMemHandle_t', ('hipIpcMemHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaIpcMemHandle_st', ('hipIpcMemHandle_t', CONV_TYPE, API_RUNTIME)), ('cudaGraphicsCubeFace', ('hipGraphicsCubeFace', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlags', ('hipGraphicsMapFlags', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlags', ('hipGraphicsRegisterFlags', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceList', ('hipGLDeviceList', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlags', ('hipGLMapFlags', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceList', ('hipD3D9DeviceList', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlags', ('hipD3D9MapFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterFlags', ('hipD3D9RegisterFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceList', ('hipd3d10DeviceList', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlags', ('hipD3D10MapFlags', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterFlags', ('hipD3D10RegisterFlags', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceList', ('hipd3d11DeviceList', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEglStreamConnection', ('hipEglStreamConnection', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cublasHandle_t', ('rocblas_handle', CONV_TYPE, API_BLAS)), ('cublasOperation_t', ('rocblas_operation', CONV_TYPE, API_BLAS)), ('cublasStatus_t', ('rocblas_status', CONV_TYPE, API_BLAS)), ('cublasFillMode_t', ('rocblas_fill', CONV_TYPE, API_BLAS)), ('cublasDiagType_t', ('rocblas_diagonal', CONV_TYPE, API_BLAS)), ('cublasSideMode_t', ('rocblas_side', CONV_TYPE, API_BLAS)), ('cublasPointerMode_t', ('rocblas_pointer_mode', CONV_TYPE, API_BLAS)), ('cublasAtomicsMode_t', ('rocblas_atomics_mode', CONV_TYPE, API_BLAS, HIP_UNSUPPORTED)), ('cublasDataType_t', ('rocblas_data_type', CONV_TYPE, API_BLAS, HIP_UNSUPPORTED)), ('curandStatus', ('hiprandStatus_t', CONV_TYPE, API_RAND)), ('curandStatus_t', ('hiprandStatus_t', CONV_TYPE, API_RAND)), ('curandRngType', ('hiprandRngType_t', CONV_TYPE, API_RAND)), ('curandRngType_t', ('hiprandRngType_t', CONV_TYPE, API_RAND)), ('curandGenerator_st', ('hiprandGenerator_st', CONV_TYPE, API_RAND)), ('curandGenerator_t', ('hiprandGenerator_t', CONV_TYPE, API_RAND)), ('curandDirectionVectorSet', ('hiprandDirectionVectorSet_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDirectionVectorSet_t', ('hiprandDirectionVectorSet_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandOrdering', ('hiprandOrdering_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandOrdering_t', ('hiprandOrdering_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistribution_st', ('hiprandDistribution_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2V_st', ('hiprandDistribution_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistribution_t', ('hiprandDistribution_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2V_t', ('hiprandDistribution_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionShift_st', ('hiprandDistributionShift_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionShift_t', ('hiprandDistributionShift_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionM2Shift_st', ('hiprandDistributionM2Shift_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDistributionM2Shift_t', ('hiprandDistributionM2Shift_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2_st', ('hiprandHistogramM2_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2_t', ('hiprandHistogramM2_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2K_st', ('hiprandHistogramM2K_st', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandHistogramM2K_t', ('hiprandHistogramM2K_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDiscreteDistribution_st', ('hiprandDiscreteDistribution_st', CONV_TYPE, API_RAND)), ('curandDiscreteDistribution_t', ('hiprandDiscreteDistribution_t', CONV_TYPE, API_RAND)), ('curandMethod', ('hiprandMethod_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandMethod_t', ('hiprandMethod_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandDirectionVectors32_t', ('hiprandDirectionVectors32_t', CONV_TYPE, API_RAND)), ('curandDirectionVectors64_t', ('hiprandDirectionVectors64_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateMtgp32_t', ('hiprandStateMtgp32_t', CONV_TYPE, API_RAND)), ('curandStateMtgp32', ('hiprandStateMtgp32_t', CONV_TYPE, API_RAND)), ('curandStateScrambledSobol64_t', ('hiprandStateScrambledSobol64_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateSobol64_t', ('hiprandStateSobol64_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateScrambledSobol32_t', ('hiprandStateScrambledSobol32_t', CONV_TYPE, API_RAND, HIP_UNSUPPORTED)), ('curandStateSobol32_t', ('hiprandStateSobol32_t', CONV_TYPE, API_RAND)), ('curandStateMRG32k3a_t', ('hiprandStateMRG32k3a_t', CONV_TYPE, API_RAND)), ('curandStatePhilox4_32_10_t', ('hiprandStatePhilox4_32_10_t', CONV_TYPE, API_RAND)), ('curandStateXORWOW_t', ('hiprandStateXORWOW_t', CONV_TYPE, API_RAND)), ('curandState_t', ('hiprandState_t', CONV_TYPE, API_RAND)), ('curandState', ('hiprandState_t', CONV_TYPE, API_RAND))])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_INCLUDE_MAP->collections.OrderedDict([('include <cuda.h', ('include <hip/hip_runtime.h', CONV_INCLUDE_CUDA_MAIN_H, API_DRIVER)), ('include "cuda.h', ('include "hip/hip_runtime.h', CONV_INCLUDE_CUDA_MAIN_H, API_DRIVER)), ('cuda_runtime.h', ('hip/hip_runtime.h', CONV_INCLUDE_CUDA_MAIN_H, API_RUNTIME)), ('cuda_runtime_api.h', ('hip/hip_runtime_api.h', CONV_INCLUDE, API_RUNTIME)), ('channel_descriptor.h', ('hip/channel_descriptor.h', CONV_INCLUDE, API_RUNTIME)), ('device_functions.h', ('hip/device_functions.h', CONV_INCLUDE, API_RUNTIME)), ('driver_types.h', ('hip/driver_types.h', CONV_INCLUDE, API_RUNTIME)), ('library_types.h', ('hip/library_types.h', CONV_INCLUDE, API_RUNTIME)), ('cuComplex.h', ('hip/hip_complex.h', CONV_INCLUDE, API_RUNTIME)), ('cuda_fp16.h', ('hip/hip_fp16.h', CONV_INCLUDE, API_RUNTIME)), ('cuda_texture_types.h', ('hip/hip_texture_types.h', CONV_INCLUDE, API_RUNTIME)), ('vector_types.h', ('hip/hip_vector_types.h', CONV_INCLUDE, API_RUNTIME)), ('cublas.h', ('rocblas.h', CONV_INCLUDE_CUDA_MAIN_H, API_BLAS)), ('cublas_v2.h', ('rocblas.h', CONV_INCLUDE_CUDA_MAIN_H, API_BLAS)), ('curand.h', ('hiprand/hiprand.h', CONV_INCLUDE_CUDA_MAIN_H, API_RAND)), ('curand_kernel.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_discrete.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_discrete2.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_globals.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_lognormal.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mrg32k3a.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32_host.h', ('hiprand/hiprand_mtgp32_host.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32_kernel.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_mtgp32dc_p_11213.h', ('rocrand/rocrand_mtgp32_11213.h', CONV_INCLUDE, API_RAND)), ('curand_normal.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_normal_static.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_philox4x32_x.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_poisson.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_precalc.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('curand_uniform.h', ('hiprand/hiprand_kernel.h', CONV_INCLUDE, API_RAND)), ('cusparse.h', ('hipsparse.h', CONV_INCLUDE, API_RAND)), ('cufft.h', ('hipfft.h', CONV_INCLUDE, API_BLAS)), ('cufftXt.h', ('hipfft.h', CONV_INCLUDE, API_BLAS)), ('<nccl.h>', ('<rccl.h>', CONV_INCLUDE, API_RUNTIME)), ('nvrtc.h', ('hip/hiprtc.h', CONV_INCLUDE, API_RTC)), ('thrust/system/cuda', ('thrust/system/hip', CONV_INCLUDE, API_BLAS)), ('cub/util_allocator.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/block/block_reduce.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/cub.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/block/block_load.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/device/device_radix_sort.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/device/device_reduce.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('cub/device/device_scan.cuh', ('hipcub/hipcub.hpp', CONV_INCLUDE, API_BLAS)), ('nvToolsExt.h', ('roctracer/roctx.h', CONV_INCLUDE, API_ROCTX))])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_IDENTIFIER_MAP->collections.OrderedDict([('__CUDACC__', ('__HIPCC__', CONV_DEF, API_RUNTIME)), ('CUDA_ERROR_INVALID_CONTEXT', ('hipErrorInvalidContext', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_CONTEXT_ALREADY_CURRENT', ('hipErrorContextAlreadyCurrent', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_ARRAY_IS_MAPPED', ('hipErrorArrayIsMapped', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_ALREADY_MAPPED', ('hipErrorAlreadyMapped', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_ALREADY_ACQUIRED', ('hipErrorAlreadyAcquired', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_MAPPED', ('hipErrorNotMapped', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_MAPPED_AS_ARRAY', ('hipErrorNotMappedAsArray', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_MAPPED_AS_POINTER', ('hipErrorNotMappedAsPointer', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_CONTEXT_ALREADY_IN_USE', ('hipErrorContextAlreadyInUse', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_INVALID_SOURCE', ('hipErrorInvalidSource', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_FILE_NOT_FOUND', ('hipErrorFileNotFound', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_NOT_FOUND', ('hipErrorNotFound', CONV_TYPE, API_DRIVER)), ('CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING', ('hipErrorLaunchIncompatibleTexturing', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE', ('hipErrorPrimaryContextActive', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_CONTEXT_IS_DESTROYED', ('hipErrorContextIsDestroyed', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_NOT_PERMITTED', ('hipErrorNotPermitted', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ERROR_NOT_SUPPORTED', ('hipErrorNotSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorMissingConfiguration', ('hipErrorMissingConfiguration', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorPriorLaunchFailure', ('hipErrorPriorLaunchFailure', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidDeviceFunction', ('hipErrorInvalidDeviceFunction', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidConfiguration', ('hipErrorInvalidConfiguration', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidPitchValue', ('hipErrorInvalidPitchValue', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidSymbol', ('hipErrorInvalidSymbol', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidHostPointer', ('hipErrorInvalidHostPointer', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidDevicePointer', ('hipErrorInvalidDevicePointer', CONV_TYPE, API_RUNTIME)), ('cudaErrorInvalidTexture', ('hipErrorInvalidTexture', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidTextureBinding', ('hipErrorInvalidTextureBinding', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidChannelDescriptor', ('hipErrorInvalidChannelDescriptor', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidMemcpyDirection', ('hipErrorInvalidMemcpyDirection', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorAddressOfConstant', ('hipErrorAddressOfConstant', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorTextureFetchFailed', ('hipErrorTextureFetchFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorTextureNotBound', ('hipErrorTextureNotBound', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorSynchronizationError', ('hipErrorSynchronizationError', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidFilterSetting', ('hipErrorInvalidFilterSetting', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidNormSetting', ('hipErrorInvalidNormSetting', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorMixedDeviceExecution', ('hipErrorMixedDeviceExecution', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorNotYetImplemented', ('hipErrorNotYetImplemented', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorMemoryValueTooLarge', ('hipErrorMemoryValueTooLarge', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInsufficientDriver', ('hipErrorInsufficientDriver', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorSetOnActiveProcess', ('hipErrorSetOnActiveProcess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorInvalidSurface', ('hipErrorInvalidSurface', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDuplicateVariableName', ('hipErrorDuplicateVariableName', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDuplicateTextureName', ('hipErrorDuplicateTextureName', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDuplicateSurfaceName', ('hipErrorDuplicateSurfaceName', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDevicesUnavailable', ('hipErrorDevicesUnavailable', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorIncompatibleDriverContext', ('hipErrorIncompatibleDriverContext', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorDeviceAlreadyInUse', ('hipErrorDeviceAlreadyInUse', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchMaxDepthExceeded', ('hipErrorLaunchMaxDepthExceeded', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchFileScopedTex', ('hipErrorLaunchFileScopedTex', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchFileScopedSurf', ('hipErrorLaunchFileScopedSurf', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorSyncDepthExceeded', ('hipErrorSyncDepthExceeded', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorLaunchPendingCountExceeded', ('hipErrorLaunchPendingCountExceeded', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorNotPermitted', ('hipErrorNotPermitted', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorNotSupported', ('hipErrorNotSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorStartupFailure', ('hipErrorStartupFailure', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaErrorApiFailureBase', ('hipErrorApiFailureBase', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_SUCCESS', ('hipSuccess', CONV_TYPE, API_DRIVER)), ('cudaSuccess', ('hipSuccess', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_INVALID_VALUE', ('hipErrorInvalidValue', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidValue', ('hipErrorInvalidValue', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_OUT_OF_MEMORY', ('hipErrorMemoryAllocation', CONV_TYPE, API_DRIVER)), ('cudaErrorMemoryAllocation', ('hipErrorMemoryAllocation', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_NOT_INITIALIZED', ('hipErrorNotInitialized', CONV_TYPE, API_DRIVER)), ('cudaErrorInitializationError', ('hipErrorInitializationError', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_DEINITIALIZED', ('hipErrorDeinitialized', CONV_TYPE, API_DRIVER)), ('cudaErrorCudartUnloading', ('hipErrorDeinitialized', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_DISABLED', ('hipErrorProfilerDisabled', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerDisabled', ('hipErrorProfilerDisabled', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_NOT_INITIALIZED', ('hipErrorProfilerNotInitialized', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerNotInitialized', ('hipErrorProfilerNotInitialized', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_ALREADY_STARTED', ('hipErrorProfilerAlreadyStarted', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerAlreadyStarted', ('hipErrorProfilerAlreadyStarted', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PROFILER_ALREADY_STOPPED', ('hipErrorProfilerAlreadyStopped', CONV_TYPE, API_DRIVER)), ('cudaErrorProfilerAlreadyStopped', ('hipErrorProfilerAlreadyStopped', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_NO_DEVICE', ('hipErrorNoDevice', CONV_TYPE, API_DRIVER)), ('cudaErrorNoDevice', ('hipErrorNoDevice', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_INVALID_DEVICE', ('hipErrorInvalidDevice', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidDevice', ('hipErrorInvalidDevice', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_INVALID_IMAGE', ('hipErrorInvalidImage', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidKernelImage', ('hipErrorInvalidImage', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_MAP_FAILED', ('hipErrorMapFailed', CONV_TYPE, API_DRIVER)), ('cudaErrorMapBufferObjectFailed', ('hipErrorMapFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_UNMAP_FAILED', ('hipErrorUnmapFailed', CONV_TYPE, API_DRIVER)), ('cudaErrorUnmapBufferObjectFailed', ('hipErrorUnmapFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_NO_BINARY_FOR_GPU', ('hipErrorNoBinaryForGpu', CONV_TYPE, API_DRIVER)), ('cudaErrorNoKernelImageForDevice', ('hipErrorNoBinaryForGpu', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_ECC_UNCORRECTABLE', ('hipErrorECCNotCorrectable', CONV_TYPE, API_DRIVER)), ('cudaErrorECCUncorrectable', ('hipErrorECCNotCorrectable', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_UNSUPPORTED_LIMIT', ('hipErrorUnsupportedLimit', CONV_TYPE, API_DRIVER)), ('cudaErrorUnsupportedLimit', ('hipErrorUnsupportedLimit', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PEER_ACCESS_UNSUPPORTED', ('hipErrorPeerAccessUnsupported', CONV_TYPE, API_DRIVER)), ('cudaErrorPeerAccessUnsupported', ('hipErrorPeerAccessUnsupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_PTX', ('hipErrorInvalidKernelFile', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidPtx', ('hipErrorInvalidKernelFile', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_GRAPHICS_CONTEXT', ('hipErrorInvalidGraphicsContext', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidGraphicsContext', ('hipErrorInvalidGraphicsContext', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_NVLINK_UNCORRECTABLE', ('hipErrorNvlinkUncorrectable', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorNvlinkUncorrectable', ('hipErrorNvlinkUncorrectable', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND', ('hipErrorSharedObjectSymbolNotFound', CONV_TYPE, API_DRIVER)), ('cudaErrorSharedObjectSymbolNotFound', ('hipErrorSharedObjectSymbolNotFound', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_SHARED_OBJECT_INIT_FAILED', ('hipErrorSharedObjectInitFailed', CONV_TYPE, API_DRIVER)), ('cudaErrorSharedObjectInitFailed', ('hipErrorSharedObjectInitFailed', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_OPERATING_SYSTEM', ('hipErrorOperatingSystem', CONV_TYPE, API_DRIVER)), ('cudaErrorOperatingSystem', ('hipErrorOperatingSystem', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_HANDLE', ('hipErrorInvalidResourceHandle', CONV_TYPE, API_DRIVER)), ('cudaErrorInvalidResourceHandle', ('hipErrorInvalidResourceHandle', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_NOT_READY', ('hipErrorNotReady', CONV_TYPE, API_DRIVER)), ('cudaErrorNotReady', ('hipErrorNotReady', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_ILLEGAL_ADDRESS', ('hipErrorIllegalAddress', CONV_TYPE, API_DRIVER)), ('cudaErrorIllegalAddress', ('hipErrorIllegalAddress', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES', ('hipErrorLaunchOutOfResources', CONV_TYPE, API_DRIVER)), ('cudaErrorLaunchOutOfResources', ('hipErrorLaunchOutOfResources', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_LAUNCH_TIMEOUT', ('hipErrorLaunchTimeOut', CONV_TYPE, API_DRIVER)), ('cudaErrorLaunchTimeout', ('hipErrorLaunchTimeOut', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED', ('hipErrorPeerAccessAlreadyEnabled', CONV_TYPE, API_DRIVER)), ('cudaErrorPeerAccessAlreadyEnabled', ('hipErrorPeerAccessAlreadyEnabled', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_PEER_ACCESS_NOT_ENABLED', ('hipErrorPeerAccessNotEnabled', CONV_TYPE, API_DRIVER)), ('cudaErrorPeerAccessNotEnabled', ('hipErrorPeerAccessNotEnabled', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_ASSERT', ('hipErrorAssert', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorAssert', ('hipErrorAssert', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_TOO_MANY_PEERS', ('hipErrorTooManyPeers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorTooManyPeers', ('hipErrorTooManyPeers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED', ('hipErrorHostMemoryAlreadyRegistered', CONV_TYPE, API_DRIVER)), ('cudaErrorHostMemoryAlreadyRegistered', ('hipErrorHostMemoryAlreadyRegistered', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED', ('hipErrorHostMemoryNotRegistered', CONV_TYPE, API_DRIVER)), ('cudaErrorHostMemoryNotRegistered', ('hipErrorHostMemoryNotRegistered', CONV_TYPE, API_RUNTIME)), ('CUDA_ERROR_HARDWARE_STACK_ERROR', ('hipErrorHardwareStackError', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorHardwareStackError', ('hipErrorHardwareStackError', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_ILLEGAL_INSTRUCTION', ('hipErrorIllegalInstruction', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorIllegalInstruction', ('hipErrorIllegalInstruction', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_MISALIGNED_ADDRESS', ('hipErrorMisalignedAddress', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorMisalignedAddress', ('hipErrorMisalignedAddress', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_ADDRESS_SPACE', ('hipErrorInvalidAddressSpace', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorInvalidAddressSpace', ('hipErrorInvalidAddressSpace', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_INVALID_PC', ('hipErrorInvalidPc', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorInvalidPc', ('hipErrorInvalidPc', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_LAUNCH_FAILED', ('hipErrorLaunchFailure', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorLaunchFailure', ('hipErrorLaunchFailure', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_ERROR_UNKNOWN', ('hipErrorUnknown', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cudaErrorUnknown', ('hipErrorUnknown', CONV_TYPE, API_RUNTIME)), ('CU_TR_ADDRESS_MODE_WRAP', ('HIP_TR_ADDRESS_MODE_WRAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_ADDRESS_MODE_CLAMP', ('HIP_TR_ADDRESS_MODE_CLAMP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_ADDRESS_MODE_MIRROR', ('HIP_TR_ADDRESS_MODE_MIRROR', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_ADDRESS_MODE_BORDER', ('HIP_TR_ADDRESS_MODE_BORDER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_POSITIVE_X', ('HIP_CUBEMAP_FACE_POSITIVE_X', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_NEGATIVE_X', ('HIP_CUBEMAP_FACE_NEGATIVE_X', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_POSITIVE_Y', ('HIP_CUBEMAP_FACE_POSITIVE_Y', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_NEGATIVE_Y', ('HIP_CUBEMAP_FACE_NEGATIVE_Y', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_POSITIVE_Z', ('HIP_CUBEMAP_FACE_POSITIVE_Z', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CUBEMAP_FACE_NEGATIVE_Z', ('HIP_CUBEMAP_FACE_NEGATIVE_Z', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_AD_FORMAT_UNSIGNED_INT8', ('HIP_AD_FORMAT_UNSIGNED_INT8', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_UNSIGNED_INT16', ('HIP_AD_FORMAT_UNSIGNED_INT16', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_UNSIGNED_INT32', ('HIP_AD_FORMAT_UNSIGNED_INT32', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_SIGNED_INT8', ('HIP_AD_FORMAT_SIGNED_INT8', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_SIGNED_INT16', ('HIP_AD_FORMAT_SIGNED_INT16', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_SIGNED_INT32', ('HIP_AD_FORMAT_SIGNED_INT32', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_HALF', ('HIP_AD_FORMAT_HALF', CONV_TYPE, API_DRIVER)), ('CU_AD_FORMAT_FLOAT', ('HIP_AD_FORMAT_FLOAT', CONV_TYPE, API_DRIVER)), ('CU_COMPUTEMODE_DEFAULT', ('hipComputeModeDefault', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_COMPUTEMODE_EXCLUSIVE', ('hipComputeModeExclusive', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_COMPUTEMODE_PROHIBITED', ('hipComputeModeProhibited', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_COMPUTEMODE_EXCLUSIVE_PROCESS', ('hipComputeModeExclusiveProcess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_SET_READ_MOSTLY', ('hipMemAdviseSetReadMostly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_UNSET_READ_MOSTLY', ('hipMemAdviseUnsetReadMostly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_SET_PREFERRED_LOCATION', ('hipMemAdviseSetPreferredLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION', ('hipMemAdviseUnsetPreferredLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_SET_ACCESSED_BY', ('hipMemAdviseSetAccessedBy', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ADVISE_UNSET_ACCESSED_BY', ('hipMemAdviseUnsetAccessedBy', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY', ('hipMemRangeAttributeReadMostly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION', ('hipMemRangeAttributePreferredLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY', ('hipMemRangeAttributeAccessedBy', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION', ('hipMemRangeAttributeLastPrefetchLocation', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_AUTO', ('HIP_CTX_SCHED_AUTO', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_SPIN', ('HIP_CTX_SCHED_SPIN', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_YIELD', ('HIP_CTX_SCHED_YIELD', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_BLOCKING_SYNC', ('HIP_CTX_SCHED_BLOCKING_SYNC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_BLOCKING_SYNC', ('HIP_CTX_BLOCKING_SYNC', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_SCHED_MASK', ('HIP_CTX_SCHED_MASK', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_MAP_HOST', ('HIP_CTX_MAP_HOST', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_LMEM_RESIZE_TO_MAX', ('HIP_CTX_LMEM_RESIZE_TO_MAX', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_CTX_FLAGS_MASK', ('HIP_CTX_FLAGS_MASK', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LAUNCH_PARAM_BUFFER_POINTER', ('HIP_LAUNCH_PARAM_BUFFER_POINTER', CONV_TYPE, API_DRIVER)), ('CU_LAUNCH_PARAM_BUFFER_SIZE', ('HIP_LAUNCH_PARAM_BUFFER_SIZE', CONV_TYPE, API_DRIVER)), ('CU_LAUNCH_PARAM_END', ('HIP_LAUNCH_PARAM_END', CONV_TYPE, API_DRIVER)), ('CU_IPC_HANDLE_SIZE', ('HIP_LAUNCH_PARAM_END', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTALLOC_DEVICEMAP', ('HIP_MEMHOSTALLOC_DEVICEMAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTALLOC_PORTABLE', ('HIP_MEMHOSTALLOC_PORTABLE', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTALLOC_WRITECOMBINED', ('HIP_MEMHOSTALLOC_WRITECOMBINED', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTREGISTER_DEVICEMAP', ('HIP_MEMHOSTREGISTER_DEVICEMAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTREGISTER_IOMEMORY', ('HIP_MEMHOSTREGISTER_IOMEMORY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMHOSTREGISTER_PORTABLE', ('HIP_MEMHOSTREGISTER_PORTABLE', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_PARAM_TR_DEFAULT', ('HIP_PARAM_TR_DEFAULT', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_LEGACY', ('HIP_STREAM_LEGACY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_PER_THREAD', ('HIP_STREAM_PER_THREAD', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSA_OVERRIDE_FORMAT', ('HIP_TRSA_OVERRIDE_FORMAT', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSF_NORMALIZED_COORDINATES', ('HIP_TRSF_NORMALIZED_COORDINATES', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSF_READ_AS_INTEGER', ('HIP_TRSF_READ_AS_INTEGER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TRSF_SRGB', ('HIP_TRSF_SRGB', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_2DARRAY', ('HIP_ARRAY3D_LAYERED', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_CUBEMAP', ('HIP_ARRAY3D_CUBEMAP', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_DEPTH_TEXTURE', ('HIP_ARRAY3D_DEPTH_TEXTURE', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_LAYERED', ('HIP_ARRAY3D_LAYERED', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_SURFACE_LDST', ('HIP_ARRAY3D_SURFACE_LDST', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_ARRAY3D_TEXTURE_GATHER', ('HIP_ARRAY3D_TEXTURE_GATHER', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK', ('hipDeviceAttributeMaxThreadsPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X', ('hipDeviceAttributeMaxBlockDimX', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y', ('hipDeviceAttributeMaxBlockDimY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z', ('hipDeviceAttributeMaxBlockDimZ', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X', ('hipDeviceAttributeMaxGridDimX', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y', ('hipDeviceAttributeMaxGridDimY', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z', ('hipDeviceAttributeMaxGridDimZ', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK', ('hipDeviceAttributeMaxSharedMemoryPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_SHARED_MEMORY_PER_BLOCK', ('hipDeviceAttributeMaxSharedMemoryPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY', ('hipDeviceAttributeTotalConstantMemory', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_WARP_SIZE', ('hipDeviceAttributeWarpSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_PITCH', ('hipDeviceAttributeMaxPitch', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK', ('hipDeviceAttributeMaxRegistersPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_REGISTERS_PER_BLOCK', ('hipDeviceAttributeMaxRegistersPerBlock', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CLOCK_RATE', ('hipDeviceAttributeClockRate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT', ('hipDeviceAttributeTextureAlignment', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_GPU_OVERLAP', ('hipDeviceAttributeAsyncEngineCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT', ('hipDeviceAttributeMultiprocessorCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT', ('hipDeviceAttributeKernelExecTimeout', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_INTEGRATED', ('hipDeviceAttributeIntegrated', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY', ('hipDeviceAttributeCanMapHostMemory', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_MODE', ('hipDeviceAttributeComputeMode', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH', ('hipDeviceAttributeMaxTexture1DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_WIDTH', ('hipDeviceAttributeMaxTexture2DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_HEIGHT', ('hipDeviceAttributeMaxTexture2DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH', ('hipDeviceAttributeMaxTexture3DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT', ('hipDeviceAttributeMaxTexture3DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH', ('hipDeviceAttributeMaxTexture3DDepth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH', ('hipDeviceAttributeMaxTexture2DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT', ('hipDeviceAttributeMaxTexture2DLayeredHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS', ('hipDeviceAttributeMaxTexture2DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_WIDTH', ('hipDeviceAttributeMaxTexture2DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_HEIGHT', ('hipDeviceAttributeMaxTexture2DLayeredHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES', ('hipDeviceAttributeMaxTexture2DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_SURFACE_ALIGNMENT', ('hipDeviceAttributeSurfaceAlignment', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS', ('hipDeviceAttributeConcurrentKernels', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_ECC_ENABLED', ('hipDeviceAttributeEccEnabled', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_PCI_BUS_ID', ('hipDeviceAttributePciBusId', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID', ('hipDeviceAttributePciDeviceId', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_TCC_DRIVER', ('hipDeviceAttributeTccDriver', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE', ('hipDeviceAttributeMemoryClockRate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH', ('hipDeviceAttributeMemoryBusWidth', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE', ('hipDeviceAttributeL2CacheSize', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR', ('hipDeviceAttributeMaxThreadsPerMultiProcessor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT', ('hipDeviceAttributeAsyncEngineCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING', ('hipDeviceAttributeUnifiedAddressing', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH', ('hipDeviceAttributeMaxTexture1DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS', ('hipDeviceAttributeMaxTexture1DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CAN_TEX2D_GATHER', ('hipDeviceAttributeCanTex2DGather', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_WIDTH', ('hipDeviceAttributeMaxTexture2DGatherWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_HEIGHT', ('hipDeviceAttributeMaxTexture2DGatherHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE', ('hipDeviceAttributeMaxTexture3DWidthAlternate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE', ('hipDeviceAttributeMaxTexture3DHeightAlternate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE', ('hipDeviceAttributeMaxTexture3DDepthAlternate', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID', ('hipDeviceAttributePciDomainId', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT', ('hipDeviceAttributeTexturePitchAlignment', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_WIDTH', ('hipDeviceAttributeMaxTextureCubemapWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH', ('hipDeviceAttributeMaxTextureCubemapLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS', ('hipDeviceAttributeMaxTextureCubemapLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_WIDTH', ('hipDeviceAttributeMaxSurface1DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_WIDTH', ('hipDeviceAttributeMaxSurface2DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_HEIGHT', ('hipDeviceAttributeMaxSurface2DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_WIDTH', ('hipDeviceAttributeMaxSurface3DWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_HEIGHT', ('hipDeviceAttributeMaxSurface3DHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_DEPTH', ('hipDeviceAttributeMaxSurface3DDepth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_WIDTH', ('hipDeviceAttributeMaxSurface1DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_LAYERS', ('hipDeviceAttributeMaxSurface1DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_WIDTH', ('hipDeviceAttributeMaxSurface2DLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_HEIGHT', ('hipDeviceAttributeMaxSurface2DLayeredHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_LAYERS', ('hipDeviceAttributeMaxSurface2DLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_WIDTH', ('hipDeviceAttributeMaxSurfaceCubemapWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH', ('hipDeviceAttributeMaxSurfaceCubemapLayeredWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS', ('hipDeviceAttributeMaxSurfaceCubemapLayeredLayers', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH', ('hipDeviceAttributeMaxTexture1DLinearWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH', ('hipDeviceAttributeMaxTexture2DLinearWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT', ('hipDeviceAttributeMaxTexture2DLinearHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH', ('hipDeviceAttributeMaxTexture2DLinearPitch', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH', ('hipDeviceAttributeMaxTexture2DMipmappedWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT', ('hipDeviceAttributeMaxTexture2DMipmappedHeight', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR', ('hipDeviceAttributeComputeCapabilityMajor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR', ('hipDeviceAttributeComputeCapabilityMinor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH', ('hipDeviceAttributeMaxTexture1DMipmappedWidth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_STREAM_PRIORITIES_SUPPORTED', ('hipDeviceAttributeStreamPrioritiesSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_GLOBAL_L1_CACHE_SUPPORTED', ('hipDeviceAttributeGlobalL1CacheSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_LOCAL_L1_CACHE_SUPPORTED', ('hipDeviceAttributeLocalL1CacheSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR', ('hipDeviceAttributeMaxSharedMemoryPerMultiprocessor', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR', ('hipDeviceAttributeMaxRegistersPerMultiprocessor', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY', ('hipDeviceAttributeManagedMemory', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD', ('hipDeviceAttributeIsMultiGpuBoard', CONV_TYPE, API_DRIVER)), ('CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD_GROUP_ID', ('hipDeviceAttributeMultiGpuBoardGroupId', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_HOST_NATIVE_ATOMIC_SUPPORTED', ('hipDeviceAttributeHostNativeAtomicSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO', ('hipDeviceAttributeSingleToDoublePrecisionPerfRatio', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS', ('hipDeviceAttributePageableMemoryAccess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS', ('hipDeviceAttributeConcurrentManagedAccess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_COMPUTE_PREEMPTION_SUPPORTED', ('hipDeviceAttributeComputePreemptionSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM', ('hipDeviceAttributeCanUseHostPointerForRegisteredMem', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_ATTRIBUTE_MAX', ('hipDeviceAttributeMax', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_CONTEXT', ('hipPointerAttributeContext', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_MEMORY_TYPE', ('hipPointerAttributeMemoryType', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_DEVICE_POINTER', ('hipPointerAttributeDevicePointer', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_HOST_POINTER', ('hipPointerAttributeHostPointer', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_P2P_TOKENS', ('hipPointerAttributeP2pTokens', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_SYNC_MEMOPS', ('hipPointerAttributeSyncMemops', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_BUFFER_ID', ('hipPointerAttributeBufferId', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_POINTER_ATTRIBUTE_IS_MANAGED', ('hipPointerAttributeIsManaged', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK', ('hipFuncAttributeMaxThreadsPerBlocks', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES', ('hipFuncAttributeSharedSizeBytes', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_CONST_SIZE_BYTES', ('hipFuncAttributeConstSizeBytes', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES', ('hipFuncAttributeLocalSizeBytes', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_NUM_REGS', ('hipFuncAttributeNumRegs', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_PTX_VERSION', ('hipFuncAttributePtxVersion', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_BINARY_VERSION', ('hipFuncAttributeBinaryVersion', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_CACHE_MODE_CA', ('hipFuncAttributeCacheModeCA', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_ATTRIBUTE_MAX', ('hipFuncAttributeMax', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_MAP_RESOURCE_FLAGS_NONE', ('hipGraphicsMapFlagsNone', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_MAP_RESOURCE_FLAGS_READ_ONLY', ('hipGraphicsMapFlagsReadOnly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_MAP_RESOURCE_FLAGS_WRITE_DISCARD', ('hipGraphicsMapFlagsWriteDiscard', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_NONE', ('hipGraphicsRegisterFlagsNone', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_READ_ONLY', ('hipGraphicsRegisterFlagsReadOnly', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_WRITE_DISCARD', ('hipGraphicsRegisterFlagsWriteDiscard', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_SURFACE_LDST', ('hipGraphicsRegisterFlagsSurfaceLoadStore', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GRAPHICS_REGISTER_FLAGS_TEXTURE_GATHER', ('hipGraphicsRegisterFlagsTextureGather', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_OCCUPANCY_DEFAULT', ('hipOccupancyDefault', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE', ('hipOccupancyDisableCachingOverride', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_FUNC_CACHE_PREFER_NONE', ('hipFuncCachePreferNone', CONV_CACHE, API_DRIVER)), ('CU_FUNC_CACHE_PREFER_SHARED', ('hipFuncCachePreferShared', CONV_CACHE, API_DRIVER)), ('CU_FUNC_CACHE_PREFER_L1', ('hipFuncCachePreferL1', CONV_CACHE, API_DRIVER)), ('CU_FUNC_CACHE_PREFER_EQUAL', ('hipFuncCachePreferEqual', CONV_CACHE, API_DRIVER)), ('CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS', ('hipIpcMemLazyEnablePeerAccess', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CUDA_IPC_HANDLE_SIZE', ('HIP_IPC_HANDLE_SIZE', CONV_TYPE, API_DRIVER)), ('CU_JIT_CACHE_OPTION_NONE', ('hipJitCacheModeOptionNone', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_CACHE_OPTION_CG', ('hipJitCacheModeOptionCG', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_CACHE_OPTION_CA', ('hipJitCacheModeOptionCA', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_PREFER_PTX', ('hipJitFallbackPreferPtx', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_PREFER_BINARY', ('hipJitFallbackPreferBinary', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_MAX_REGISTERS', ('hipJitOptionMaxRegisters', CONV_JIT, API_DRIVER)), ('CU_JIT_THREADS_PER_BLOCK', ('hipJitOptionThreadsPerBlock', CONV_JIT, API_DRIVER)), ('CU_JIT_WALL_TIME', ('hipJitOptionWallTime', CONV_JIT, API_DRIVER)), ('CU_JIT_INFO_LOG_BUFFER', ('hipJitOptionInfoLogBuffer', CONV_JIT, API_DRIVER)), ('CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES', ('hipJitOptionInfoLogBufferSizeBytes', CONV_JIT, API_DRIVER)), ('CU_JIT_ERROR_LOG_BUFFER', ('hipJitOptionErrorLogBuffer', CONV_JIT, API_DRIVER)), ('CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES', ('hipJitOptionErrorLogBufferSizeBytes', CONV_JIT, API_DRIVER)), ('CU_JIT_OPTIMIZATION_LEVEL', ('hipJitOptionOptimizationLevel', CONV_JIT, API_DRIVER)), ('CU_JIT_TARGET_FROM_CUCONTEXT', ('hipJitOptionTargetFromContext', CONV_JIT, API_DRIVER)), ('CU_JIT_TARGET', ('hipJitOptionTarget', CONV_JIT, API_DRIVER)), ('CU_JIT_FALLBACK_STRATEGY', ('hipJitOptionFallbackStrategy', CONV_JIT, API_DRIVER)), ('CU_JIT_GENERATE_DEBUG_INFO', ('hipJitOptionGenerateDebugInfo', CONV_JIT, API_DRIVER)), ('CU_JIT_LOG_VERBOSE', ('hipJitOptionLogVerbose', CONV_JIT, API_DRIVER)), ('CU_JIT_GENERATE_LINE_INFO', ('hipJitOptionGenerateLineInfo', CONV_JIT, API_DRIVER)), ('CU_JIT_CACHE_MODE', ('hipJitOptionCacheMode', CONV_JIT, API_DRIVER)), ('CU_JIT_NEW_SM3X_OPT', ('hipJitOptionSm3xOpt', CONV_JIT, API_DRIVER)), ('CU_JIT_FAST_COMPILE', ('hipJitOptionFastCompile', CONV_JIT, API_DRIVER)), ('CU_JIT_NUM_OPTIONS', ('hipJitOptionNumOptions', CONV_JIT, API_DRIVER)), ('CU_TARGET_COMPUTE_10', ('hipJitTargetCompute10', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_11', ('hipJitTargetCompute11', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_12', ('hipJitTargetCompute12', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_13', ('hipJitTargetCompute13', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_20', ('hipJitTargetCompute20', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_21', ('hipJitTargetCompute21', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_30', ('hipJitTargetCompute30', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_32', ('hipJitTargetCompute32', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_35', ('hipJitTargetCompute35', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_37', ('hipJitTargetCompute37', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_50', ('hipJitTargetCompute50', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_52', ('hipJitTargetCompute52', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_53', ('hipJitTargetCompute53', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_60', ('hipJitTargetCompute60', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_61', ('hipJitTargetCompute61', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TARGET_COMPUTE_62', ('hipJitTargetCompute62', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_CUBIN', ('hipJitInputTypeBin', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_PTX', ('hipJitInputTypePtx', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_FATBINARY', ('hipJitInputTypeFatBinary', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_OBJECT', ('hipJitInputTypeObject', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_INPUT_LIBRARY', ('hipJitInputTypeLibrary', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_JIT_NUM_INPUT_TYPES', ('hipJitInputTypeNumInputTypes', CONV_JIT, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_STACK_SIZE', ('hipLimitStackSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_PRINTF_FIFO_SIZE', ('hipLimitPrintfFifoSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_MALLOC_HEAP_SIZE', ('hipLimitMallocHeapSize', CONV_TYPE, API_DRIVER)), ('CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH', ('hipLimitDevRuntimeSyncDepth', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_DEV_RUNTIME_PENDING_LAUNCH_COUNT', ('hipLimitDevRuntimePendingLaunchCount', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_LIMIT_STACK_SIZE', ('hipLimitStackSize', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ATTACH_GLOBAL', ('hipMemAttachGlobal', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ATTACH_HOST', ('hipMemAttachHost', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEM_ATTACH_SINGLE', ('hipMemAttachSingle', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_HOST', ('hipMemTypeHost', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_DEVICE', ('hipMemTypeDevice', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_ARRAY', ('hipMemTypeArray', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_MEMORYTYPE_UNIFIED', ('hipMemTypeUnified', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_ARRAY', ('hipResourceTypeArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_MIPMAPPED_ARRAY', ('hipResourceTypeMipmappedArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_LINEAR', ('hipResourceTypeLinear', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RESOURCE_TYPE_PITCH2D', ('hipResourceTypePitch2D', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('CU_RES_VIEW_FORMAT_NONE', ('hipResViewFormatNone', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_1X8', ('hipResViewFormatUnsignedChar1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_2X8', ('hipResViewFormatUnsignedChar2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_4X8', ('hipResViewFormatUnsignedChar4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_1X8', ('hipResViewFormatSignedChar1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_2X8', ('hipResViewFormatSignedChar2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_4X8', ('hipResViewFormatSignedChar4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_1X16', ('hipResViewFormatUnsignedShort1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_2X16', ('hipResViewFormatUnsignedShort2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_4X16', ('hipResViewFormatUnsignedShort4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_1X16', ('hipResViewFormatSignedShort1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_2X16', ('hipResViewFormatSignedShort2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_4X16', ('hipResViewFormatSignedShort4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_1X32', ('hipResViewFormatUnsignedInt1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_2X32', ('hipResViewFormatUnsignedInt2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UINT_4X32', ('hipResViewFormatUnsignedInt4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_1X32', ('hipResViewFormatSignedInt1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_2X32', ('hipResViewFormatSignedInt2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SINT_4X32', ('hipResViewFormatSignedInt4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_1X16', ('hipResViewFormatHalf1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_2X16', ('hipResViewFormatHalf2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_4X16', ('hipResViewFormatHalf4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_1X32', ('hipResViewFormatFloat1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_2X32', ('hipResViewFormatFloat2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_FLOAT_4X32', ('hipResViewFormatFloat4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC1', ('hipResViewFormatUnsignedBlockCompressed1', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC2', ('hipResViewFormatUnsignedBlockCompressed2', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC3', ('hipResViewFormatUnsignedBlockCompressed3', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC4', ('hipResViewFormatUnsignedBlockCompressed4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SIGNED_BC4', ('hipResViewFormatSignedBlockCompressed4', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC5', ('hipResViewFormatUnsignedBlockCompressed5', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SIGNED_BC5', ('hipResViewFormatSignedBlockCompressed5', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC6H', ('hipResViewFormatUnsignedBlockCompressed6H', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_SIGNED_BC6H', ('hipResViewFormatSignedBlockCompressed6H', CONV_TEX, API_DRIVER)), ('CU_RES_VIEW_FORMAT_UNSIGNED_BC7', ('hipResViewFormatUnsignedBlockCompressed7', CONV_TEX, API_DRIVER)), ('CU_SHARED_MEM_CONFIG_DEFAULT_BANK_SIZE', ('hipSharedMemBankSizeDefault', CONV_TYPE, API_DRIVER)), ('CU_SHARED_MEM_CONFIG_FOUR_BYTE_BANK_SIZE', ('hipSharedMemBankSizeFourByte', CONV_TYPE, API_DRIVER)), ('CU_SHARED_MEM_CONFIG_EIGHT_BYTE_BANK_SIZE', ('hipSharedMemBankSizeEightByte', CONV_TYPE, API_DRIVER)), ('CU_STREAM_DEFAULT', ('hipStreamDefault', CONV_TYPE, API_DRIVER)), ('CU_STREAM_NON_BLOCKING', ('hipStreamNonBlocking', CONV_TYPE, API_DRIVER)), ('CU_STREAM_WAIT_VALUE_GEQ', ('hipStreamWaitValueGeq', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WAIT_VALUE_EQ', ('hipStreamWaitValueEq', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WAIT_VALUE_AND', ('hipStreamWaitValueAnd', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WAIT_VALUE_FLUSH', ('hipStreamWaitValueFlush', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WRITE_VALUE_DEFAULT', ('hipStreamWriteValueDefault', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_WRITE_VALUE_NO_MEMORY_BARRIER', ('hipStreamWriteValueNoMemoryBarrier', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_MEM_OP_WAIT_VALUE_32', ('hipStreamBatchMemOpWaitValue32', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_MEM_OP_WRITE_VALUE_32', ('hipStreamBatchMemOpWriteValue32', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_STREAM_MEM_OP_FLUSH_REMOTE_WRITES', ('hipStreamBatchMemOpFlushRemoteWrites', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('cuGetErrorName', ('hipGetErrorName___', CONV_ERROR, API_DRIVER, HIP_UNSUPPORTED)), ('cuGetErrorString', ('hipGetErrorString___', CONV_ERROR, API_DRIVER, HIP_UNSUPPORTED)), ('cuInit', ('hipInit', CONV_INIT, API_DRIVER)), ('cuDriverGetVersion', ('hipDriverGetVersion', CONV_VERSION, API_DRIVER)), ('cuCtxCreate_v2', ('hipCtxCreate', CONV_CONTEXT, API_DRIVER)), ('cuCtxDestroy_v2', ('hipCtxDestroy', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetApiVersion', ('hipCtxGetApiVersion', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetCacheConfig', ('hipCtxGetCacheConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetCurrent', ('hipCtxGetCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetDevice', ('hipCtxGetDevice', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetFlags', ('hipCtxGetFlags', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetLimit', ('hipCtxGetLimit', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxGetSharedMemConfig', ('hipCtxGetSharedMemConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxGetStreamPriorityRange', ('hipCtxGetStreamPriorityRange', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxPopCurrent_v2', ('hipCtxPopCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxPushCurrent_v2', ('hipCtxPushCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxSetCacheConfig', ('hipCtxSetCacheConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxSetCurrent', ('hipCtxSetCurrent', CONV_CONTEXT, API_DRIVER)), ('cuCtxSetLimit', ('hipCtxSetLimit', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxSetSharedMemConfig', ('hipCtxSetSharedMemConfig', CONV_CONTEXT, API_DRIVER)), ('cuCtxSynchronize', ('hipCtxSynchronize', CONV_CONTEXT, API_DRIVER)), ('cuCtxAttach', ('hipCtxAttach', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxDetach', ('hipCtxDetach', CONV_CONTEXT, API_DRIVER, HIP_UNSUPPORTED)), ('cuCtxEnablePeerAccess', ('hipCtxEnablePeerAccess', CONV_PEER, API_DRIVER)), ('cuCtxDisablePeerAccess', ('hipCtxDisablePeerAccess', CONV_PEER, API_DRIVER)), ('cuDeviceCanAccessPeer', ('hipDeviceCanAccessPeer', CONV_PEER, API_DRIVER)), ('cuDeviceGetP2PAttribute', ('hipDeviceGetP2PAttribute', CONV_PEER, API_DRIVER, HIP_UNSUPPORTED)), ('cuDevicePrimaryCtxGetState', ('hipDevicePrimaryCtxGetState', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxRelease', ('hipDevicePrimaryCtxRelease', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxReset', ('hipDevicePrimaryCtxReset', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxRetain', ('hipDevicePrimaryCtxRetain', CONV_CONTEXT, API_DRIVER)), ('cuDevicePrimaryCtxSetFlags', ('hipDevicePrimaryCtxSetFlags', CONV_CONTEXT, API_DRIVER)), ('cuDeviceGet', ('hipGetDevice', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetName', ('hipDeviceGetName', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetCount', ('hipGetDeviceCount', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetAttribute', ('hipDeviceGetAttribute', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetPCIBusId', ('hipDeviceGetPCIBusId', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetByPCIBusId', ('hipDeviceGetByPCIBusId', CONV_DEVICE, API_DRIVER)), ('cuDeviceTotalMem_v2', ('hipDeviceTotalMem', CONV_DEVICE, API_DRIVER)), ('cuDeviceComputeCapability', ('hipDeviceComputeCapability', CONV_DEVICE, API_DRIVER)), ('cuDeviceGetProperties', ('hipGetDeviceProperties', CONV_DEVICE, API_DRIVER)), ('cuLinkAddData', ('hipLinkAddData', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkAddFile', ('hipLinkAddFile', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkComplete', ('hipLinkComplete', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkCreate', ('hipLinkCreate', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLinkDestroy', ('hipLinkDestroy', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuModuleGetFunction', ('hipModuleGetFunction', CONV_MODULE, API_DRIVER)), ('cuModuleGetGlobal_v2', ('hipModuleGetGlobal', CONV_MODULE, API_DRIVER)), ('cuModuleGetSurfRef', ('hipModuleGetSurfRef', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuModuleGetTexRef', ('hipModuleGetTexRef', CONV_MODULE, API_DRIVER)), ('cuModuleLoad', ('hipModuleLoad', CONV_MODULE, API_DRIVER)), ('cuModuleLoadData', ('hipModuleLoadData', CONV_MODULE, API_DRIVER)), ('cuModuleLoadDataEx', ('hipModuleLoadDataEx', CONV_MODULE, API_DRIVER)), ('cuModuleLoadFatBinary', ('hipModuleLoadFatBinary', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuModuleUnload', ('hipModuleUnload', CONV_MODULE, API_DRIVER)), ('CU_DEVICE_P2P_ATTRIBUTE_PERFORMANCE_RANK', ('hipDeviceP2PAttributePerformanceRank', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_P2P_ATTRIBUTE_ACCESS_SUPPORTED', ('hipDeviceP2PAttributeAccessSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_DEVICE_P2P_ATTRIBUTE_NATIVE_ATOMIC_SUPPORTED', ('hipDeviceP2PAttributeNativeAtomicSupported', CONV_TYPE, API_DRIVER, HIP_UNSUPPORTED)), ('CU_EVENT_DEFAULT', ('hipEventDefault', CONV_EVENT, API_DRIVER)), ('CU_EVENT_BLOCKING_SYNC', ('hipEventBlockingSync', CONV_EVENT, API_DRIVER)), ('CU_EVENT_DISABLE_TIMING', ('hipEventDisableTiming', CONV_EVENT, API_DRIVER)), ('CU_EVENT_INTERPROCESS', ('hipEventInterprocess', CONV_EVENT, API_DRIVER)), ('cuEventCreate', ('hipEventCreate', CONV_EVENT, API_DRIVER)), ('cuEventDestroy_v2', ('hipEventDestroy', CONV_EVENT, API_DRIVER)), ('cuEventElapsedTime', ('hipEventElapsedTime', CONV_EVENT, API_DRIVER)), ('cuEventQuery', ('hipEventQuery', CONV_EVENT, API_DRIVER)), ('cuEventRecord', ('hipEventRecord', CONV_EVENT, API_DRIVER)), ('cuEventSynchronize', ('hipEventSynchronize', CONV_EVENT, API_DRIVER)), ('cuFuncGetAttribute', ('hipFuncGetAttribute', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuFuncSetCacheConfig', ('hipFuncSetCacheConfig', CONV_MODULE, API_DRIVER)), ('cuFuncSetSharedMemConfig', ('hipFuncSetSharedMemConfig', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunchKernel', ('hipModuleLaunchKernel', CONV_MODULE, API_DRIVER)), ('cuFuncSetBlockShape', ('hipFuncSetBlockShape', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuFuncSetSharedSize', ('hipFuncSetSharedSize', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunch', ('hipLaunch', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunchGrid', ('hipLaunchGrid', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuLaunchGridAsync', ('hipLaunchGridAsync', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetf', ('hipParamSetf', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSeti', ('hipParamSeti', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetSize', ('hipParamSetSize', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetSize', ('hipParamSetSize', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuParamSetv', ('hipParamSetv', CONV_MODULE, API_DRIVER, HIP_UNSUPPORTED)), ('cuOccupancyMaxActiveBlocksPerMultiprocessor', ('hipModuleOccupancyMaxActiveBlocksPerMultiprocessor', CONV_OCCUPANCY, API_DRIVER)), ('cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', ('hipModuleOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', CONV_OCCUPANCY, API_DRIVER, HIP_UNSUPPORTED)), ('cuOccupancyMaxPotentialBlockSize', ('hipModuleOccupancyMaxPotentialBlockSize', CONV_OCCUPANCY, API_DRIVER)), ('cuOccupancyMaxPotentialBlockSizeWithFlags', ('hipModuleOccupancyMaxPotentialBlockSizeWithFlags', CONV_OCCUPANCY, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamAddCallback', ('hipStreamAddCallback', CONV_STREAM, API_DRIVER)), ('cuStreamAttachMemAsync', ('hipStreamAttachMemAsync', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamCreate', ('hipStreamCreate__', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamCreateWithPriority', ('hipStreamCreateWithPriority', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamDestroy_v2', ('hipStreamDestroy', CONV_STREAM, API_DRIVER)), ('cuStreamGetFlags', ('hipStreamGetFlags', CONV_STREAM, API_DRIVER)), ('cuStreamGetPriority', ('hipStreamGetPriority', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamQuery', ('hipStreamQuery', CONV_STREAM, API_DRIVER)), ('cuStreamSynchronize', ('hipStreamSynchronize', CONV_STREAM, API_DRIVER)), ('cuStreamWaitEvent', ('hipStreamWaitEvent', CONV_STREAM, API_DRIVER)), ('cuStreamWaitValue32', ('hipStreamWaitValue32', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamWriteValue32', ('hipStreamWriteValue32', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuStreamBatchMemOp', ('hipStreamBatchMemOp', CONV_STREAM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArray3DCreate', ('hipArray3DCreate', CONV_MEM, API_DRIVER)), ('cuArray3DGetDescriptor', ('hipArray3DGetDescriptor', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArrayCreate', ('hipArrayCreate', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArrayDestroy', ('hipArrayDestroy', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuArrayGetDescriptor', ('hipArrayGetDescriptor', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcCloseMemHandle', ('hipIpcCloseMemHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcGetEventHandle', ('hipIpcGetEventHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcGetMemHandle', ('hipIpcGetMemHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcOpenEventHandle', ('hipIpcOpenEventHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuIpcOpenMemHandle', ('hipIpcOpenMemHandle', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAlloc_v2', ('hipMalloc', CONV_MEM, API_DRIVER)), ('cuMemAllocHost', ('hipMemAllocHost', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAllocManaged', ('hipMemAllocManaged', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAllocPitch', ('hipMemAllocPitch__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy', ('hipMemcpy__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy2D', ('hipMemcpy2D__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy2DAsync', ('hipMemcpy2DAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy2DUnaligned', ('hipMemcpy2DUnaligned', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3D', ('hipMemcpy3D__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3DAsync', ('hipMemcpy3DAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3DPeer', ('hipMemcpy3DPeer__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpy3DPeerAsync', ('hipMemcpy3DPeerAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAsync', ('hipMemcpyAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoA', ('hipMemcpyAtoA', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoD', ('hipMemcpyAtoD', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoH', ('hipMemcpyAtoH', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyAtoHAsync', ('hipMemcpyAtoHAsync', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyDtoA', ('hipMemcpyDtoA', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyDtoD_v2', ('hipMemcpyDtoD', CONV_MEM, API_DRIVER)), ('cuMemcpyDtoDAsync_v2', ('hipMemcpyDtoDAsync', CONV_MEM, API_DRIVER)), ('cuMemcpyDtoH_v2', ('hipMemcpyDtoH', CONV_MEM, API_DRIVER)), ('cuMemcpyDtoHAsync_v2', ('hipMemcpyDtoHAsync', CONV_MEM, API_DRIVER)), ('cuMemcpyHtoA', ('hipMemcpyHtoA', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyHtoAAsync', ('hipMemcpyHtoAAsync', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyHtoD_v2', ('hipMemcpyHtoD', CONV_MEM, API_DRIVER)), ('cuMemcpyHtoDAsync_v2', ('hipMemcpyHtoDAsync', CONV_MEM, API_DRIVER)), ('cuMemcpyPeerAsync', ('hipMemcpyPeerAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemcpyPeer', ('hipMemcpyPeer__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemFree_v2', ('hipFree', CONV_MEM, API_DRIVER)), ('cuMemFreeHost', ('hipHostFree', CONV_MEM, API_DRIVER)), ('cuMemGetAddressRange', ('hipMemGetAddressRange', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemGetInfo_v2', ('hipMemGetInfo', CONV_MEM, API_DRIVER)), ('cuMemHostAlloc', ('hipHostMalloc', CONV_MEM, API_DRIVER)), ('cuMemHostGetDevicePointer', ('hipMemHostGetDevicePointer', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemHostGetFlags', ('hipMemHostGetFlags', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemHostRegister_v2', ('hipHostRegister', CONV_MEM, API_DRIVER)), ('cuMemHostUnregister', ('hipHostUnregister', CONV_MEM, API_DRIVER)), ('cuMemsetD16_v2', ('hipMemsetD16', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD16Async', ('hipMemsetD16Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D16_v2', ('hipMemsetD2D16', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D16Async', ('hipMemsetD2D16Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D32_v2', ('hipMemsetD2D32', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D32Async', ('hipMemsetD2D32Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D8_v2', ('hipMemsetD2D8', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD2D8Async', ('hipMemsetD2D8Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD32_v2', ('hipMemset', CONV_MEM, API_DRIVER)), ('cuMemsetD32Async', ('hipMemsetAsync', CONV_MEM, API_DRIVER)), ('cuMemsetD8_v2', ('hipMemsetD8', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemsetD8Async', ('hipMemsetD8Async', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMipmappedArrayCreate', ('hipMipmappedArrayCreate', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMipmappedArrayDestroy', ('hipMipmappedArrayDestroy', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMipmappedArrayGetLevel', ('hipMipmappedArrayGetLevel', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemPrefetchAsync', ('hipMemPrefetchAsync__', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemAdvise', ('hipMemAdvise', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemRangeGetAttribute', ('hipMemRangeGetAttribute', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuMemRangeGetAttributes', ('hipMemRangeGetAttributes', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuPointerGetAttribute', ('hipPointerGetAttribute', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuPointerGetAttributes', ('hipPointerGetAttributes', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('cuPointerSetAttribute', ('hipPointerSetAttribute', CONV_MEM, API_DRIVER, HIP_UNSUPPORTED)), ('CU_TR_FILTER_MODE_POINT', ('hipFilterModePoint', CONV_TEX, API_DRIVER)), ('CU_TR_FILTER_MODE_LINEAR', ('hipFilterModeLinear', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetAddress', ('hipTexRefGetAddress', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetAddressMode', ('hipTexRefGetAddressMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetArray', ('hipTexRefGetArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetBorderColor', ('hipTexRefGetBorderColor', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetFilterMode', ('hipTexRefGetFilterMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetFlags', ('hipTexRefGetFlags', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetFormat', ('hipTexRefGetFormat', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMaxAnisotropy', ('hipTexRefGetMaxAnisotropy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmapFilterMode', ('hipTexRefGetMipmapFilterMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmapLevelBias', ('hipTexRefGetMipmapLevelBias', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmapLevelClamp', ('hipTexRefGetMipmapLevelClamp', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefGetMipmappedArray', ('hipTexRefGetMipmappedArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetAddress', ('hipTexRefSetAddress', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetAddress2D', ('hipTexRefSetAddress2D', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetAddressMode', ('hipTexRefSetAddressMode', CONV_TEX, API_DRIVER)), ('cuTexRefSetArray', ('hipTexRefSetArray', CONV_TEX, API_DRIVER)), ('cuTexRefSetBorderColor', ('hipTexRefSetBorderColor', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetFilterMode', ('hipTexRefSetFilterMode', CONV_TEX, API_DRIVER)), ('cuTexRefSetFlags', ('hipTexRefSetFlags', CONV_TEX, API_DRIVER)), ('cuTexRefSetFormat', ('hipTexRefSetFormat', CONV_TEX, API_DRIVER)), ('cuTexRefSetMaxAnisotropy', ('hipTexRefSetMaxAnisotropy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmapFilterMode', ('hipTexRefSetMipmapFilterMode', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmapLevelBias', ('hipTexRefSetMipmapLevelBias', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmapLevelClamp', ('hipTexRefSetMipmapLevelClamp', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefSetMipmappedArray', ('hipTexRefSetMipmappedArray', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefCreate', ('hipTexRefCreate', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexRefDestroy', ('hipTexRefDestroy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfRefGetArray', ('hipSurfRefGetArray', CONV_SURFACE, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfRefSetArray', ('hipSurfRefSetArray', CONV_SURFACE, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectCreate', ('hipTexObjectCreate', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectDestroy', ('hipTexObjectDestroy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectGetResourceDesc', ('hipTexObjectGetResourceDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectGetResourceViewDesc', ('hipTexObjectGetResourceViewDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuTexObjectGetTextureDesc', ('hipTexObjectGetTextureDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfObjectCreate', ('hipSurfObjectCreate', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfObjectDestroy', ('hipSurfObjectDestroy', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuSurfObjectGetResourceDesc', ('hipSurfObjectGetResourceDesc', CONV_TEX, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsMapResources', ('hipGraphicsMapResources', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceGetMappedMipmappedArray', ('hipGraphicsResourceGetMappedMipmappedArray', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceGetMappedPointer', ('hipGraphicsResourceGetMappedPointer', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceSetMapFlags', ('hipGraphicsResourceSetMapFlags', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsSubResourceGetMappedArray', ('hipGraphicsSubResourceGetMappedArray', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsUnmapResources', ('hipGraphicsUnmapResources', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsUnregisterResource', ('hipGraphicsUnregisterResource', CONV_GRAPHICS, API_DRIVER, HIP_UNSUPPORTED)), ('cuProfilerInitialize', ('hipProfilerInitialize', CONV_OTHER, API_DRIVER, HIP_UNSUPPORTED)), ('cuProfilerStart', ('hipProfilerStart', CONV_OTHER, API_DRIVER)), ('cuProfilerStop', ('hipProfilerStop', CONV_OTHER, API_DRIVER)), ('CU_GL_DEVICE_LIST_ALL', ('HIP_GL_DEVICE_LIST_ALL', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_DEVICE_LIST_CURRENT_FRAME', ('HIP_GL_DEVICE_LIST_CURRENT_FRAME', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_DEVICE_LIST_NEXT_FRAME', ('HIP_GL_DEVICE_LIST_NEXT_FRAME', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLGetDevices', ('hipGLGetDevices', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsGLRegisterBuffer', ('hipGraphicsGLRegisterBuffer', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsGLRegisterImage', ('hipGraphicsGLRegisterImage', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuWGLGetDevice', ('hipWGLGetDevice', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_MAP_RESOURCE_FLAGS_NONE', ('HIP_GL_MAP_RESOURCE_FLAGS_NONE', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_MAP_RESOURCE_FLAGS_READ_ONLY', ('HIP_GL_MAP_RESOURCE_FLAGS_READ_ONLY', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD', ('HIP_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLCtxCreate', ('hipGLCtxCreate', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLInit', ('hipGLInit', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLMapBufferObject', ('hipGLMapBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLMapBufferObjectAsync', ('hipGLMapBufferObjectAsync', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLRegisterBufferObject', ('hipGLRegisterBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLSetBufferObjectMapFlags', ('hipGLSetBufferObjectMapFlags', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLUnmapBufferObject', ('hipGLUnmapBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLUnmapBufferObjectAsync', ('hipGLUnmapBufferObjectAsync', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGLUnregisterBufferObject', ('hipGLUnregisterBufferObject', CONV_GL, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_DEVICE_LIST_ALL', ('HIP_D3D9_DEVICE_LIST_ALL', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_DEVICE_LIST_CURRENT_FRAME', ('HIP_D3D9_DEVICE_LIST_CURRENT_FRAME', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_DEVICE_LIST_NEXT_FRAME', ('HIP_D3D9_DEVICE_LIST_NEXT_FRAME', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9CtxCreate', ('hipD3D9CtxCreate', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9CtxCreateOnDevice', ('hipD3D9CtxCreateOnDevice', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9GetDevice', ('hipD3D9GetDevice', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9GetDevices', ('hipD3D9GetDevices', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9GetDirect3DDevice', ('hipD3D9GetDirect3DDevice', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsD3D9RegisterResource', ('hipGraphicsD3D9RegisterResource', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_MAPRESOURCE_FLAGS_NONE', ('HIP_D3D9_MAPRESOURCE_FLAGS_NONE', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_MAPRESOURCE_FLAGS_READONLY', ('HIP_D3D9_MAPRESOURCE_FLAGS_READONLY', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD', ('HIP_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_REGISTER_FLAGS_NONE', ('HIP_D3D9_REGISTER_FLAGS_NONE', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D9_REGISTER_FLAGS_ARRAY', ('HIP_D3D9_REGISTER_FLAGS_ARRAY', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9MapResources', ('hipD3D9MapResources', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9RegisterResource', ('hipD3D9RegisterResource', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedArray', ('hipD3D9ResourceGetMappedArray', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedPitch', ('hipD3D9ResourceGetMappedPitch', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedPointer', ('hipD3D9ResourceGetMappedPointer', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetMappedSize', ('hipD3D9ResourceGetMappedSize', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceGetSurfaceDimensions', ('hipD3D9ResourceGetSurfaceDimensions', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9ResourceSetMapFlags', ('hipD3D9ResourceSetMapFlags', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9UnmapResources', ('hipD3D9UnmapResources', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D9UnregisterResource', ('hipD3D9UnregisterResource', CONV_D3D9, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_DEVICE_LIST_ALL', ('HIP_D3D10_DEVICE_LIST_ALL', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_DEVICE_LIST_CURRENT_FRAME', ('HIP_D3D10_DEVICE_LIST_CURRENT_FRAME', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_DEVICE_LIST_NEXT_FRAME', ('HIP_D3D10_DEVICE_LIST_NEXT_FRAME', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10GetDevice', ('hipD3D10GetDevice', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10GetDevices', ('hipD3D10GetDevices', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsD3D10RegisterResource', ('hipGraphicsD3D10RegisterResource', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_MAPRESOURCE_FLAGS_NONE', ('HIP_D3D10_MAPRESOURCE_FLAGS_NONE', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_MAPRESOURCE_FLAGS_READONLY', ('HIP_D3D10_MAPRESOURCE_FLAGS_READONLY', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD', ('HIP_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_REGISTER_FLAGS_NONE', ('HIP_D3D10_REGISTER_FLAGS_NONE', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D10_REGISTER_FLAGS_ARRAY', ('HIP_D3D10_REGISTER_FLAGS_ARRAY', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10CtxCreate', ('hipD3D10CtxCreate', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10CtxCreateOnDevice', ('hipD3D10CtxCreateOnDevice', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10GetDirect3DDevice', ('hipD3D10GetDirect3DDevice', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10MapResources', ('hipD3D10MapResources', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10RegisterResource', ('hipD3D10RegisterResource', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedArray', ('hipD3D10ResourceGetMappedArray', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedPitch', ('hipD3D10ResourceGetMappedPitch', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedPointer', ('hipD3D10ResourceGetMappedPointer', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetMappedSize', ('hipD3D10ResourceGetMappedSize', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10ResourceGetSurfaceDimensions', ('hipD3D10ResourceGetSurfaceDimensions', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD310ResourceSetMapFlags', ('hipD3D10ResourceSetMapFlags', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10UnmapResources', ('hipD3D10UnmapResources', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D10UnregisterResource', ('hipD3D10UnregisterResource', CONV_D3D10, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D11_DEVICE_LIST_ALL', ('HIP_D3D11_DEVICE_LIST_ALL', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D11_DEVICE_LIST_CURRENT_FRAME', ('HIP_D3D11_DEVICE_LIST_CURRENT_FRAME', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('CU_D3D11_DEVICE_LIST_NEXT_FRAME', ('HIP_D3D11_DEVICE_LIST_NEXT_FRAME', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11GetDevice', ('hipD3D11GetDevice', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11GetDevices', ('hipD3D11GetDevices', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsD3D11RegisterResource', ('hipGraphicsD3D11RegisterResource', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11CtxCreate', ('hipD3D11CtxCreate', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11CtxCreateOnDevice', ('hipD3D11CtxCreateOnDevice', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuD3D11GetDirect3DDevice', ('hipD3D11GetDirect3DDevice', CONV_D3D11, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsVDPAURegisterOutputSurface', ('hipGraphicsVDPAURegisterOutputSurface', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsVDPAURegisterVideoSurface', ('hipGraphicsVDPAURegisterVideoSurface', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuVDPAUGetDevice', ('hipVDPAUGetDevice', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuVDPAUCtxCreate', ('hipVDPAUCtxCreate', CONV_VDPAU, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerAcquireFrame', ('hipEGLStreamConsumerAcquireFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerConnect', ('hipEGLStreamConsumerConnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerConnectWithFlags', ('hipEGLStreamConsumerConnectWithFlags', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerDisconnect', ('hipEGLStreamConsumerDisconnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamConsumerReleaseFrame', ('hipEGLStreamConsumerReleaseFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerConnect', ('hipEGLStreamProducerConnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerDisconnect', ('hipEGLStreamProducerDisconnect', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerPresentFrame', ('hipEGLStreamProducerPresentFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuEGLStreamProducerReturnFrame', ('hipEGLStreamProducerReturnFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsEGLRegisterImage', ('hipGraphicsEGLRegisterImage', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cuGraphicsResourceGetMappedEglFrame', ('hipGraphicsResourceGetMappedEglFrame', CONV_EGL, API_DRIVER, HIP_UNSUPPORTED)), ('cudaDataType_t', ('hipDataType_t', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDataType', ('hipDataType', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_16F', ('HIP_R_16F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_16F', ('HIP_C_16F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_32F', ('HIP_R_32F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_32F', ('HIP_C_32F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_64F', ('HIP_R_64F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_64F', ('HIP_C_64F', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_8I', ('HIP_R_8I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_8I', ('HIP_C_8I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_8U', ('HIP_R_8U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_8U', ('HIP_C_8U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_32I', ('HIP_R_32I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_32I', ('HIP_C_32I', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_R_32U', ('HIP_R_32U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('CUDA_C_32U', ('HIP_C_32U', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('MAJOR_VERSION', ('hipLibraryMajorVersion', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('MINOR_VERSION', ('hipLibraryMinorVersion', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('PATCH_LEVEL', ('hipLibraryPatchVersion', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAttachGlobal', ('hipMemAttachGlobal', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAttachHost', ('hipMemAttachHost', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAttachSingle', ('hipMemAttachSingle', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyDefault', ('hipOccupancyDefault', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyDisableCachingOverride', ('hipOccupancyDisableCachingOverride', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetLastError', ('hipGetLastError', CONV_ERROR, API_RUNTIME)), ('cudaPeekAtLastError', ('hipPeekAtLastError', CONV_ERROR, API_RUNTIME)), ('cudaGetErrorName', ('hipGetErrorName', CONV_ERROR, API_RUNTIME)), ('cudaGetErrorString', ('hipGetErrorString', CONV_ERROR, API_RUNTIME)), ('cudaMemcpy3DParms', ('hipMemcpy3DParms', CONV_MEM, API_RUNTIME)), ('cudaMemcpy3DPeerParms', ('hipMemcpy3DPeerParms', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy', ('hipMemcpy', CONV_MEM, API_RUNTIME)), ('cudaMemcpyToArray', ('hipMemcpyToArray', CONV_MEM, API_RUNTIME)), ('cudaMemcpyToSymbol', ('hipMemcpyToSymbol', CONV_MEM, API_RUNTIME)), ('cudaMemcpyToSymbolAsync', ('hipMemcpyToSymbolAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpyAsync', ('hipMemcpyAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2D', ('hipMemcpy2D', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2DAsync', ('hipMemcpy2DAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2DToArray', ('hipMemcpy2DToArray', CONV_MEM, API_RUNTIME)), ('cudaMemcpy2DArrayToArray', ('hipMemcpy2DArrayToArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy2DFromArray', ('hipMemcpy2DFromArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy2DFromArrayAsync', ('hipMemcpy2DFromArrayAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy2DToArrayAsync', ('hipMemcpy2DToArrayAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy3D', ('hipMemcpy3D', CONV_MEM, API_RUNTIME)), ('cudaMemcpy3DAsync', ('hipMemcpy3DAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy3DPeer', ('hipMemcpy3DPeer', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpy3DPeerAsync', ('hipMemcpy3DPeerAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyArrayToArray', ('hipMemcpyArrayToArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyFromArrayAsync', ('hipMemcpyFromArrayAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyFromSymbol', ('hipMemcpyFromSymbol', CONV_MEM, API_RUNTIME)), ('cudaMemcpyFromSymbolAsync', ('hipMemcpyFromSymbolAsync', CONV_MEM, API_RUNTIME)), ('cudaMemAdvise', ('hipMemAdvise', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeGetAttribute', ('hipMemRangeGetAttribute', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeGetAttributes', ('hipMemRangeGetAttributes', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseSetReadMostly', ('hipMemAdviseSetReadMostly', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseUnsetReadMostly', ('hipMemAdviseUnsetReadMostly', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseSetPreferredLocation', ('hipMemAdviseSetPreferredLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseUnsetPreferredLocation', ('hipMemAdviseUnsetPreferredLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseSetAccessedBy', ('hipMemAdviseSetAccessedBy', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemAdviseUnsetAccessedBy', ('hipMemAdviseUnsetAccessedBy', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributeReadMostly', ('hipMemRangeAttributeReadMostly', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributePreferredLocation', ('hipMemRangeAttributePreferredLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributeAccessedBy', ('hipMemRangeAttributeAccessedBy', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemRangeAttributeLastPrefetchLocation', ('hipMemRangeAttributeLastPrefetchLocation', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemcpyHostToHost', ('hipMemcpyHostToHost', CONV_MEM, API_RUNTIME)), ('cudaMemcpyHostToDevice', ('hipMemcpyHostToDevice', CONV_MEM, API_RUNTIME)), ('cudaMemcpyDeviceToHost', ('hipMemcpyDeviceToHost', CONV_MEM, API_RUNTIME)), ('cudaMemcpyDeviceToDevice', ('hipMemcpyDeviceToDevice', CONV_MEM, API_RUNTIME)), ('cudaMemcpyDefault', ('hipMemcpyDefault', CONV_MEM, API_RUNTIME)), ('cudaMemset', ('hipMemset', CONV_MEM, API_RUNTIME)), ('cudaMemsetAsync', ('hipMemsetAsync', CONV_MEM, API_RUNTIME)), ('cudaMemset2D', ('hipMemset2D', CONV_MEM, API_RUNTIME)), ('cudaMemset2DAsync', ('hipMemset2DAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemset3D', ('hipMemset3D', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemset3DAsync', ('hipMemset3DAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemGetInfo', ('hipMemGetInfo', CONV_MEM, API_RUNTIME)), ('cudaArrayGetInfo', ('hipArrayGetInfo', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFreeMipmappedArray', ('hipFreeMipmappedArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetMipmappedArrayLevel', ('hipGetMipmappedArrayLevel', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSymbolAddress', ('hipGetSymbolAddress', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSymbolSize', ('hipGetSymbolSize', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMemPrefetchAsync', ('hipMemPrefetchAsync', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMallocHost', ('hipHostMalloc', CONV_MEM, API_RUNTIME)), ('cudaMallocArray', ('hipMallocArray', CONV_MEM, API_RUNTIME)), ('cudaMalloc', ('hipMalloc', CONV_MEM, API_RUNTIME)), ('cudaMalloc3D', ('hipMalloc3D', CONV_MEM, API_RUNTIME)), ('cudaMalloc3DArray', ('hipMalloc3DArray', CONV_MEM, API_RUNTIME)), ('cudaMallocManaged', ('hipMallocManaged', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMallocMipmappedArray', ('hipMallocMipmappedArray', CONV_MEM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaMallocPitch', ('hipMallocPitch', CONV_MEM, API_RUNTIME)), ('cudaFreeHost', ('hipHostFree', CONV_MEM, API_RUNTIME)), ('cudaFreeArray', ('hipFreeArray', CONV_MEM, API_RUNTIME)), ('cudaFree', ('hipFree', CONV_MEM, API_RUNTIME)), ('cudaHostRegister', ('hipHostRegister', CONV_MEM, API_RUNTIME)), ('cudaHostUnregister', ('hipHostUnregister', CONV_MEM, API_RUNTIME)), ('cudaHostAlloc', ('hipHostMalloc', CONV_MEM, API_RUNTIME)), ('cudaMemoryTypeHost', ('hipMemoryTypeHost', CONV_MEM, API_RUNTIME)), ('cudaMemoryTypeDevice', ('hipMemoryTypeDevice', CONV_MEM, API_RUNTIME)), ('make_cudaExtent', ('make_hipExtent', CONV_MEM, API_RUNTIME)), ('make_cudaPitchedPtr', ('make_hipPitchedPtr', CONV_MEM, API_RUNTIME)), ('make_cudaPos', ('make_hipPos', CONV_MEM, API_RUNTIME)), ('cudaHostAllocDefault', ('hipHostMallocDefault', CONV_MEM, API_RUNTIME)), ('cudaHostAllocPortable', ('hipHostMallocPortable', CONV_MEM, API_RUNTIME)), ('cudaHostAllocMapped', ('hipHostMallocMapped', CONV_MEM, API_RUNTIME)), ('cudaHostAllocWriteCombined', ('hipHostMallocWriteCombined', CONV_MEM, API_RUNTIME)), ('cudaHostGetFlags', ('hipHostGetFlags', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterDefault', ('hipHostRegisterDefault', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterPortable', ('hipHostRegisterPortable', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterMapped', ('hipHostRegisterMapped', CONV_MEM, API_RUNTIME)), ('cudaHostRegisterIoMemory', ('hipHostRegisterIoMemory', CONV_MEM, API_RUNTIME)), ('cudaEventCreate', ('hipEventCreate', CONV_EVENT, API_RUNTIME)), ('cudaEventCreateWithFlags', ('hipEventCreateWithFlags', CONV_EVENT, API_RUNTIME)), ('cudaEventDestroy', ('hipEventDestroy', CONV_EVENT, API_RUNTIME)), ('cudaEventRecord', ('hipEventRecord', CONV_EVENT, API_RUNTIME)), ('cudaEventElapsedTime', ('hipEventElapsedTime', CONV_EVENT, API_RUNTIME)), ('cudaEventSynchronize', ('hipEventSynchronize', CONV_EVENT, API_RUNTIME)), ('cudaEventQuery', ('hipEventQuery', CONV_EVENT, API_RUNTIME)), ('cudaEventDefault', ('hipEventDefault', CONV_EVENT, API_RUNTIME)), ('cudaEventBlockingSync', ('hipEventBlockingSync', CONV_EVENT, API_RUNTIME)), ('cudaEventDisableTiming', ('hipEventDisableTiming', CONV_EVENT, API_RUNTIME)), ('cudaEventInterprocess', ('hipEventInterprocess', CONV_EVENT, API_RUNTIME)), ('cudaStreamCreate', ('hipStreamCreate', CONV_STREAM, API_RUNTIME)), ('cudaStreamCreateWithFlags', ('hipStreamCreateWithFlags', CONV_STREAM, API_RUNTIME)), ('cudaStreamCreateWithPriority', ('hipStreamCreateWithPriority', CONV_STREAM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamDestroy', ('hipStreamDestroy', CONV_STREAM, API_RUNTIME)), ('cudaStreamWaitEvent', ('hipStreamWaitEvent', CONV_STREAM, API_RUNTIME)), ('cudaStreamSynchronize', ('hipStreamSynchronize', CONV_STREAM, API_RUNTIME)), ('cudaStreamGetFlags', ('hipStreamGetFlags', CONV_STREAM, API_RUNTIME)), ('cudaStreamQuery', ('hipStreamQuery', CONV_STREAM, API_RUNTIME)), ('cudaStreamAddCallback', ('hipStreamAddCallback', CONV_STREAM, API_RUNTIME)), ('cudaStreamAttachMemAsync', ('hipStreamAttachMemAsync', CONV_STREAM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamGetPriority', ('hipStreamGetPriority', CONV_STREAM, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaStreamDefault', ('hipStreamDefault', CONV_TYPE, API_RUNTIME)), ('cudaStreamNonBlocking', ('hipStreamNonBlocking', CONV_TYPE, API_RUNTIME)), ('cudaDeviceSynchronize', ('hipDeviceSynchronize', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceReset', ('hipDeviceReset', CONV_DEVICE, API_RUNTIME)), ('cudaSetDevice', ('hipSetDevice', CONV_DEVICE, API_RUNTIME)), ('cudaGetDevice', ('hipGetDevice', CONV_DEVICE, API_RUNTIME)), ('cudaGetDeviceCount', ('hipGetDeviceCount', CONV_DEVICE, API_RUNTIME)), ('cudaChooseDevice', ('hipChooseDevice', CONV_DEVICE, API_RUNTIME)), ('cudaThreadExit', ('hipDeviceReset', CONV_THREAD, API_RUNTIME)), ('cudaThreadGetCacheConfig', ('hipDeviceGetCacheConfig', CONV_THREAD, API_RUNTIME)), ('cudaThreadGetLimit', ('hipThreadGetLimit', CONV_THREAD, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaThreadSetCacheConfig', ('hipDeviceSetCacheConfig', CONV_THREAD, API_RUNTIME)), ('cudaThreadSetLimit', ('hipThreadSetLimit', CONV_THREAD, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaThreadSynchronize', ('hipDeviceSynchronize', CONV_THREAD, API_RUNTIME)), ('cudaDeviceGetAttribute', ('hipDeviceGetAttribute', CONV_DEVICE, API_RUNTIME)), ('cudaDevAttrMaxThreadsPerBlock', ('hipDeviceAttributeMaxThreadsPerBlock', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxBlockDimX', ('hipDeviceAttributeMaxBlockDimX', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxBlockDimY', ('hipDeviceAttributeMaxBlockDimY', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxBlockDimZ', ('hipDeviceAttributeMaxBlockDimZ', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxGridDimX', ('hipDeviceAttributeMaxGridDimX', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxGridDimY', ('hipDeviceAttributeMaxGridDimY', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxGridDimZ', ('hipDeviceAttributeMaxGridDimZ', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxSharedMemoryPerBlock', ('hipDeviceAttributeMaxSharedMemoryPerBlock', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrTotalConstantMemory', ('hipDeviceAttributeTotalConstantMemory', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrWarpSize', ('hipDeviceAttributeWarpSize', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxPitch', ('hipDeviceAttributeMaxPitch', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxRegistersPerBlock', ('hipDeviceAttributeMaxRegistersPerBlock', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrClockRate', ('hipDeviceAttributeClockRate', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrTextureAlignment', ('hipDeviceAttributeTextureAlignment', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrGpuOverlap', ('hipDeviceAttributeGpuOverlap', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMultiProcessorCount', ('hipDeviceAttributeMultiprocessorCount', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrKernelExecTimeout', ('hipDeviceAttributeKernelExecTimeout', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrIntegrated', ('hipDeviceAttributeIntegrated', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrCanMapHostMemory', ('hipDeviceAttributeCanMapHostMemory', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrComputeMode', ('hipDeviceAttributeComputeMode', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxTexture1DWidth', ('hipDeviceAttributeMaxTexture1DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DWidth', ('hipDeviceAttributeMaxTexture2DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DHeight', ('hipDeviceAttributeMaxTexture2DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DWidth', ('hipDeviceAttributeMaxTexture3DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DHeight', ('hipDeviceAttributeMaxTexture3DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DDepth', ('hipDeviceAttributeMaxTexture3DDepth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLayeredWidth', ('hipDeviceAttributeMaxTexture2DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLayeredHeight', ('hipDeviceAttributeMaxTexture2DLayeredHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLayeredLayers', ('hipDeviceAttributeMaxTexture2DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrSurfaceAlignment', ('hipDeviceAttributeSurfaceAlignment', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrConcurrentKernels', ('hipDeviceAttributeConcurrentKernels', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrEccEnabled', ('hipDeviceAttributeEccEnabled', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrPciBusId', ('hipDeviceAttributePciBusId', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrPciDeviceId', ('hipDeviceAttributePciDeviceId', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrTccDriver', ('hipDeviceAttributeTccDriver', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMemoryClockRate', ('hipDeviceAttributeMemoryClockRate', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrGlobalMemoryBusWidth', ('hipDeviceAttributeMemoryBusWidth', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrL2CacheSize', ('hipDeviceAttributeL2CacheSize', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxThreadsPerMultiProcessor', ('hipDeviceAttributeMaxThreadsPerMultiProcessor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrAsyncEngineCount', ('hipDeviceAttributeAsyncEngineCount', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrUnifiedAddressing', ('hipDeviceAttributeUnifiedAddressing', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture1DLayeredWidth', ('hipDeviceAttributeMaxTexture1DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture1DLayeredLayers', ('hipDeviceAttributeMaxTexture1DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DGatherWidth', ('hipDeviceAttributeMaxTexture2DGatherWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DGatherHeight', ('hipDeviceAttributeMaxTexture2DGatherHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DWidthAlt', ('hipDeviceAttributeMaxTexture3DWidthAlternate', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DHeightAlt', ('hipDeviceAttributeMaxTexture3DHeightAlternate', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture3DDepthAlt', ('hipDeviceAttributeMaxTexture3DDepthAlternate', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrPciDomainId', ('hipDeviceAttributePciDomainId', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrTexturePitchAlignment', ('hipDeviceAttributeTexturePitchAlignment', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTextureCubemapWidth', ('hipDeviceAttributeMaxTextureCubemapWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTextureCubemapLayeredWidth', ('hipDeviceAttributeMaxTextureCubemapLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTextureCubemapLayeredLayers', ('hipDeviceAttributeMaxTextureCubemapLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface1DWidth', ('hipDeviceAttributeMaxSurface1DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DWidth', ('hipDeviceAttributeMaxSurface2DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DHeight', ('hipDeviceAttributeMaxSurface2DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface3DWidth', ('hipDeviceAttributeMaxSurface3DWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface3DHeight', ('hipDeviceAttributeMaxSurface3DHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface3DDepth', ('hipDeviceAttributeMaxSurface3DDepth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface1DLayeredWidth', ('hipDeviceAttributeMaxSurface1DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface1DLayeredLayers', ('hipDeviceAttributeMaxSurface1DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DLayeredWidth', ('hipDeviceAttributeMaxSurface2DLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DLayeredHeight', ('hipDeviceAttributeMaxSurface2DLayeredHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurface2DLayeredLayers', ('hipDeviceAttributeMaxSurface2DLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurfaceCubemapWidth', ('hipDeviceAttributeMaxSurfaceCubemapWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurfaceCubemapLayeredWidth', ('hipDeviceAttributeMaxSurfaceCubemapLayeredWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSurfaceCubemapLayeredLayers', ('hipDeviceAttributeMaxSurfaceCubemapLayeredLayers', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture1DLinearWidth', ('hipDeviceAttributeMaxTexture1DLinearWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLinearWidth', ('hipDeviceAttributeMaxTexture2DLinearWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLinearHeight', ('hipDeviceAttributeMaxTexture2DLinearHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DLinearPitch', ('hipDeviceAttributeMaxTexture2DLinearPitch', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DMipmappedWidth', ('hipDeviceAttributeMaxTexture2DMipmappedWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxTexture2DMipmappedHeight', ('hipDeviceAttributeMaxTexture2DMipmappedHeight', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrComputeCapabilityMajor', ('hipDeviceAttributeComputeCapabilityMajor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrComputeCapabilityMinor', ('hipDeviceAttributeComputeCapabilityMinor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxTexture1DMipmappedWidth', ('hipDeviceAttributeMaxTexture1DMipmappedWidth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrStreamPrioritiesSupported', ('hipDeviceAttributeStreamPrioritiesSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrGlobalL1CacheSupported', ('hipDeviceAttributeGlobalL1CacheSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrLocalL1CacheSupported', ('hipDeviceAttributeLocalL1CacheSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrMaxSharedMemoryPerMultiprocessor', ('hipDeviceAttributeMaxSharedMemoryPerMultiprocessor', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMaxRegistersPerMultiprocessor', ('hipDeviceAttributeMaxRegistersPerMultiprocessor', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrManagedMemory', ('hipDeviceAttributeManagedMemory', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrIsMultiGpuBoard', ('hipDeviceAttributeIsMultiGpuBoard', CONV_TYPE, API_RUNTIME)), ('cudaDevAttrMultiGpuBoardGroupID', ('hipDeviceAttributeMultiGpuBoardGroupID', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrHostNativeAtomicSupported', ('hipDeviceAttributeHostNativeAtomicSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrSingleToDoublePrecisionPerfRatio', ('hipDeviceAttributeSingleToDoublePrecisionPerfRatio', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrPageableMemoryAccess', ('hipDeviceAttributePageableMemoryAccess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrConcurrentManagedAccess', ('hipDeviceAttributeConcurrentManagedAccess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrComputePreemptionSupported', ('hipDeviceAttributeComputePreemptionSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevAttrCanUseHostPointerForRegisteredMem', ('hipDeviceAttributeCanUseHostPointerForRegisteredMem', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaPointerGetAttributes', ('hipPointerGetAttributes', CONV_MEM, API_RUNTIME)), ('cudaHostGetDevicePointer', ('hipHostGetDevicePointer', CONV_MEM, API_RUNTIME)), ('cudaGetDeviceProperties', ('hipGetDeviceProperties', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetPCIBusId', ('hipDeviceGetPCIBusId', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetByPCIBusId', ('hipDeviceGetByPCIBusId', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetStreamPriorityRange', ('hipDeviceGetStreamPriorityRange', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetValidDevices', ('hipSetValidDevices', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevP2PAttrPerformanceRank', ('hipDeviceP2PAttributePerformanceRank', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevP2PAttrAccessSupported', ('hipDeviceP2PAttributeAccessSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDevP2PAttrNativeAtomicSupported', ('hipDeviceP2PAttributeNativeAtomicSupported', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceGetP2PAttribute', ('hipDeviceGetP2PAttribute', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeDefault', ('hipComputeModeDefault', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeExclusive', ('hipComputeModeExclusive', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeProhibited', ('hipComputeModeProhibited', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaComputeModeExclusiveProcess', ('hipComputeModeExclusiveProcess', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetDeviceFlags', ('hipGetDeviceFlags', CONV_DEVICE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetDeviceFlags', ('hipSetDeviceFlags', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceScheduleAuto', ('hipDeviceScheduleAuto', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleSpin', ('hipDeviceScheduleSpin', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleYield', ('hipDeviceScheduleYield', CONV_TYPE, API_RUNTIME)), ('cudaDeviceBlockingSync', ('hipDeviceScheduleBlockingSync', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleBlockingSync', ('hipDeviceScheduleBlockingSync', CONV_TYPE, API_RUNTIME)), ('cudaDeviceScheduleMask', ('hipDeviceScheduleMask', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceMapHost', ('hipDeviceMapHost', CONV_TYPE, API_RUNTIME)), ('cudaDeviceLmemResizeToMax', ('hipDeviceLmemResizeToMax', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceMask', ('hipDeviceMask', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceSetCacheConfig', ('hipDeviceSetCacheConfig', CONV_CACHE, API_RUNTIME)), ('cudaDeviceGetCacheConfig', ('hipDeviceGetCacheConfig', CONV_CACHE, API_RUNTIME)), ('cudaFuncSetCacheConfig', ('hipFuncSetCacheConfig', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferNone', ('hipFuncCachePreferNone', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferShared', ('hipFuncCachePreferShared', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferL1', ('hipFuncCachePreferL1', CONV_CACHE, API_RUNTIME)), ('cudaFuncCachePreferEqual', ('hipFuncCachePreferEqual', CONV_CACHE, API_RUNTIME)), ('cudaFuncGetAttributes', ('hipFuncGetAttributes', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFuncSetSharedMemConfig', ('hipFuncSetSharedMemConfig', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetParameterBuffer', ('hipGetParameterBuffer', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetDoubleForDevice', ('hipSetDoubleForDevice', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetDoubleForHost', ('hipSetDoubleForHost', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaConfigureCall', ('hipConfigureCall', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLaunch', ('hipLaunch', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaSetupArgument', ('hipSetupArgument', CONV_EXEC, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDriverGetVersion', ('hipDriverGetVersion', CONV_VERSION, API_RUNTIME)), ('cudaRuntimeGetVersion', ('hipRuntimeGetVersion', CONV_VERSION, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxPotentialBlockSize', ('hipOccupancyMaxPotentialBlockSize', CONV_OCCUPANCY, API_RUNTIME)), ('cudaOccupancyMaxPotentialBlockSizeWithFlags', ('hipOccupancyMaxPotentialBlockSizeWithFlags', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxActiveBlocksPerMultiprocessor', ('hipOccupancyMaxActiveBlocksPerMultiprocessor', CONV_OCCUPANCY, API_RUNTIME)), ('cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', ('hipOccupancyMaxActiveBlocksPerMultiprocessorWithFlags', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxPotentialBlockSizeVariableSMem', ('hipOccupancyMaxPotentialBlockSizeVariableSMem', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags', ('hipOccupancyMaxPotentialBlockSizeVariableSMemWithFlags', CONV_OCCUPANCY, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceCanAccessPeer', ('hipDeviceCanAccessPeer', CONV_PEER, API_RUNTIME)), ('cudaDeviceDisablePeerAccess', ('hipDeviceDisablePeerAccess', CONV_PEER, API_RUNTIME)), ('cudaDeviceEnablePeerAccess', ('hipDeviceEnablePeerAccess', CONV_PEER, API_RUNTIME)), ('cudaMemcpyPeerAsync', ('hipMemcpyPeerAsync', CONV_MEM, API_RUNTIME)), ('cudaMemcpyPeer', ('hipMemcpyPeer', CONV_MEM, API_RUNTIME)), ('cudaIpcMemLazyEnablePeerAccess', ('hipIpcMemLazyEnablePeerAccess', CONV_TYPE, API_RUNTIME)), ('cudaDeviceSetSharedMemConfig', ('hipDeviceSetSharedMemConfig', CONV_DEVICE, API_RUNTIME)), ('cudaDeviceGetSharedMemConfig', ('hipDeviceGetSharedMemConfig', CONV_DEVICE, API_RUNTIME)), ('cudaSharedMemBankSizeDefault', ('hipSharedMemBankSizeDefault', CONV_TYPE, API_RUNTIME)), ('cudaSharedMemBankSizeFourByte', ('hipSharedMemBankSizeFourByte', CONV_TYPE, API_RUNTIME)), ('cudaSharedMemBankSizeEightByte', ('hipSharedMemBankSizeEightByte', CONV_TYPE, API_RUNTIME)), ('cudaLimitStackSize', ('hipLimitStackSize', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLimitPrintfFifoSize', ('hipLimitPrintfFifoSize', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLimitMallocHeapSize', ('hipLimitMallocHeapSize', CONV_TYPE, API_RUNTIME)), ('cudaLimitDevRuntimeSyncDepth', ('hipLimitDevRuntimeSyncDepth', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaLimitDevRuntimePendingLaunchCount', ('hipLimitDevRuntimePendingLaunchCount', CONV_TYPE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDeviceGetLimit', ('hipDeviceGetLimit', CONV_DEVICE, API_RUNTIME)), ('cudaProfilerInitialize', ('hipProfilerInitialize', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaProfilerStart', ('hipProfilerStart', CONV_OTHER, API_RUNTIME)), ('cudaProfilerStop', ('hipProfilerStop', CONV_OTHER, API_RUNTIME)), ('cudaKeyValuePair', ('hipKeyValuePair', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaCSV', ('hipCSV', CONV_OTHER, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaReadModeElementType', ('hipReadModeElementType', CONV_TEX, API_RUNTIME)), ('cudaReadModeNormalizedFloat', ('hipReadModeNormalizedFloat', CONV_TEX, API_RUNTIME)), ('cudaFilterModePoint', ('hipFilterModePoint', CONV_TEX, API_RUNTIME)), ('cudaFilterModeLinear', ('hipFilterModeLinear', CONV_TEX, API_RUNTIME)), ('cudaBindTexture', ('hipBindTexture', CONV_TEX, API_RUNTIME)), ('cudaUnbindTexture', ('hipUnbindTexture', CONV_TEX, API_RUNTIME)), ('cudaBindTexture2D', ('hipBindTexture2D', CONV_TEX, API_RUNTIME)), ('cudaBindTextureToArray', ('hipBindTextureToArray', CONV_TEX, API_RUNTIME)), ('cudaBindTextureToMipmappedArray', ('hipBindTextureToMipmappedArray', CONV_TEX, API_RUNTIME)), ('cudaGetTextureAlignmentOffset', ('hipGetTextureAlignmentOffset', CONV_TEX, API_RUNTIME)), ('cudaGetTextureReference', ('hipGetTextureReference', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindSigned', ('hipChannelFormatKindSigned', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindUnsigned', ('hipChannelFormatKindUnsigned', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindFloat', ('hipChannelFormatKindFloat', CONV_TEX, API_RUNTIME)), ('cudaChannelFormatKindNone', ('hipChannelFormatKindNone', CONV_TEX, API_RUNTIME)), ('cudaCreateChannelDesc', ('hipCreateChannelDesc', CONV_TEX, API_RUNTIME)), ('cudaGetChannelDesc', ('hipGetChannelDesc', CONV_TEX, API_RUNTIME)), ('cudaResourceTypeArray', ('hipResourceTypeArray', CONV_TEX, API_RUNTIME)), ('cudaResourceTypeMipmappedArray', ('hipResourceTypeMipmappedArray', CONV_TEX, API_RUNTIME)), ('cudaResourceTypeLinear', ('hipResourceTypeLinear', CONV_TEX, API_RUNTIME)), ('cudaResourceTypePitch2D', ('hipResourceTypePitch2D', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatNone', ('hipResViewFormatNone', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedChar1', ('hipResViewFormatUnsignedChar1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedChar2', ('hipResViewFormatUnsignedChar2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedChar4', ('hipResViewFormatUnsignedChar4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedChar1', ('hipResViewFormatSignedChar1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedChar2', ('hipResViewFormatSignedChar2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedChar4', ('hipResViewFormatSignedChar4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedShort1', ('hipResViewFormatUnsignedShort1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedShort2', ('hipResViewFormatUnsignedShort2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedShort4', ('hipResViewFormatUnsignedShort4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedShort1', ('hipResViewFormatSignedShort1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedShort2', ('hipResViewFormatSignedShort2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedShort4', ('hipResViewFormatSignedShort4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedInt1', ('hipResViewFormatUnsignedInt1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedInt2', ('hipResViewFormatUnsignedInt2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedInt4', ('hipResViewFormatUnsignedInt4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedInt1', ('hipResViewFormatSignedInt1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedInt2', ('hipResViewFormatSignedInt2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedInt4', ('hipResViewFormatSignedInt4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatHalf1', ('hipResViewFormatHalf1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatHalf2', ('hipResViewFormatHalf2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatHalf4', ('hipResViewFormatHalf4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatFloat1', ('hipResViewFormatFloat1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatFloat2', ('hipResViewFormatFloat2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatFloat4', ('hipResViewFormatFloat4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed1', ('hipResViewFormatUnsignedBlockCompressed1', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed2', ('hipResViewFormatUnsignedBlockCompressed2', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed3', ('hipResViewFormatUnsignedBlockCompressed3', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed4', ('hipResViewFormatUnsignedBlockCompressed4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedBlockCompressed4', ('hipResViewFormatSignedBlockCompressed4', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed5', ('hipResViewFormatUnsignedBlockCompressed5', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedBlockCompressed5', ('hipResViewFormatSignedBlockCompressed5', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed6H', ('hipResViewFormatUnsignedBlockCompressed6H', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatSignedBlockCompressed6H', ('hipResViewFormatSignedBlockCompressed6H', CONV_TEX, API_RUNTIME)), ('cudaResViewFormatUnsignedBlockCompressed7', ('hipResViewFormatUnsignedBlockCompressed7', CONV_TEX, API_RUNTIME)), ('cudaAddressModeWrap', ('hipAddressModeWrap', CONV_TEX, API_RUNTIME)), ('cudaAddressModeClamp', ('hipAddressModeClamp', CONV_TEX, API_RUNTIME)), ('cudaAddressModeMirror', ('hipAddressModeMirror', CONV_TEX, API_RUNTIME)), ('cudaAddressModeBorder', ('hipAddressModeBorder', CONV_TEX, API_RUNTIME)), ('cudaCreateTextureObject', ('hipCreateTextureObject', CONV_TEX, API_RUNTIME)), ('cudaDestroyTextureObject', ('hipDestroyTextureObject', CONV_TEX, API_RUNTIME)), ('cudaGetTextureObjectResourceDesc', ('hipGetTextureObjectResourceDesc', CONV_TEX, API_RUNTIME)), ('cudaGetTextureObjectResourceViewDesc', ('hipGetTextureObjectResourceViewDesc', CONV_TEX, API_RUNTIME)), ('cudaGetTextureObjectTextureDesc', ('hipGetTextureObjectTextureDesc', CONV_TEX, API_RUNTIME)), ('cudaBindSurfaceToArray', ('hipBindSurfaceToArray', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSurfaceReference', ('hipGetSurfaceReference', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaBoundaryModeZero', ('hipBoundaryModeZero', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaBoundaryModeClamp', ('hipBoundaryModeClamp', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaBoundaryModeTrap', ('hipBoundaryModeTrap', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFormatModeForced', ('hipFormatModeForced', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaFormatModeAuto', ('hipFormatModeAuto', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaCreateSurfaceObject', ('hipCreateSurfaceObject', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaDestroySurfaceObject', ('hipDestroySurfaceObject', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGetSurfaceObjectResourceDesc', ('hipGetSurfaceObjectResourceDesc', CONV_SURFACE, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaIpcCloseMemHandle', ('hipIpcCloseMemHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcGetEventHandle', ('hipIpcGetEventHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcGetMemHandle', ('hipIpcGetMemHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcOpenEventHandle', ('hipIpcOpenEventHandle', CONV_DEVICE, API_RUNTIME)), ('cudaIpcOpenMemHandle', ('hipIpcOpenMemHandle', CONV_DEVICE, API_RUNTIME)), ('cudaGLGetDevices', ('hipGLGetDevices', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterBuffer', ('hipGraphicsGLRegisterBuffer', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterImage', ('hipGraphicsGLRegisterImage', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaWGLGetDevice', ('hipWGLGetDevice', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapResources', ('hipGraphicsMapResources', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceGetMappedMipmappedArray', ('hipGraphicsResourceGetMappedMipmappedArray', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceGetMappedPointer', ('hipGraphicsResourceGetMappedPointer', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceSetMapFlags', ('hipGraphicsResourceSetMapFlags', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsSubResourceGetMappedArray', ('hipGraphicsSubResourceGetMappedArray', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsUnmapResources', ('hipGraphicsUnmapResources', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsUnregisterResource', ('hipGraphicsUnregisterResource', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFacePositiveX', ('hipGraphicsCubeFacePositiveX', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFaceNegativeX', ('hipGraphicsCubeFaceNegativeX', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFacePositiveY', ('hipGraphicsCubeFacePositiveY', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFaceNegativeY', ('hipGraphicsCubeFaceNegativeY', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFacePositiveZ', ('hipGraphicsCubeFacePositiveZ', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsCubeFaceNegativeZ', ('hipGraphicsCubeFaceNegativeZ', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlagsNone', ('hipGraphicsMapFlagsNone', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlagsReadOnly', ('hipGraphicsMapFlagsReadOnly', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsMapFlagsWriteDiscard', ('hipGraphicsMapFlagsWriteDiscard', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsNone', ('hipGraphicsRegisterFlagsNone', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsReadOnly', ('hipGraphicsRegisterFlagsReadOnly', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsWriteDiscard', ('hipGraphicsRegisterFlagsWriteDiscard', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsSurfaceLoadStore', ('hipGraphicsRegisterFlagsSurfaceLoadStore', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsRegisterFlagsTextureGather', ('hipGraphicsRegisterFlagsTextureGather', CONV_GRAPHICS, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceListAll', ('HIP_GL_DEVICE_LIST_ALL', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceListCurrentFrame', ('HIP_GL_DEVICE_LIST_CURRENT_FRAME', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLDeviceListNextFrame', ('HIP_GL_DEVICE_LIST_NEXT_FRAME', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLGetDevices', ('hipGLGetDevices', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterBuffer', ('hipGraphicsGLRegisterBuffer', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsGLRegisterImage', ('hipGraphicsGLRegisterImage', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaWGLGetDevice', ('hipWGLGetDevice', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlagsNone', ('HIP_GL_MAP_RESOURCE_FLAGS_NONE', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlagsReadOnly', ('HIP_GL_MAP_RESOURCE_FLAGS_READ_ONLY', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapFlagsWriteDiscard', ('HIP_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapBufferObject', ('hipGLMapBufferObject__', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLMapBufferObjectAsync', ('hipGLMapBufferObjectAsync__', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLRegisterBufferObject', ('hipGLRegisterBufferObject', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLSetBufferObjectMapFlags', ('hipGLSetBufferObjectMapFlags', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLSetGLDevice', ('hipGLSetGLDevice', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLUnmapBufferObject', ('hipGLUnmapBufferObject', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLUnmapBufferObjectAsync', ('hipGLUnmapBufferObjectAsync', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGLUnregisterBufferObject', ('hipGLUnregisterBufferObject', CONV_GL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceListAll', ('HIP_D3D9_DEVICE_LIST_ALL', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceListCurrentFrame', ('HIP_D3D9_DEVICE_LIST_CURRENT_FRAME', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9DeviceListNextFrame', ('HIP_D3D9_DEVICE_LIST_NEXT_FRAME', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9GetDevice', ('hipD3D9GetDevice', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9GetDevices', ('hipD3D9GetDevices', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9GetDirect3DDevice', ('hipD3D9GetDirect3DDevice', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9SetDirect3DDevice', ('hipD3D9SetDirect3DDevice', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D9RegisterResource', ('hipGraphicsD3D9RegisterResource', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlags', ('hipD3D9MapFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlagsNone', ('HIP_D3D9_MAPRESOURCE_FLAGS_NONE', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlagsReadOnly', ('HIP_D3D9_MAPRESOURCE_FLAGS_READONLY', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapFlagsWriteDiscard', ('HIP_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterFlagsNone', ('HIP_D3D9_REGISTER_FLAGS_NONE', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterFlagsArray', ('HIP_D3D9_REGISTER_FLAGS_ARRAY', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9MapResources', ('hipD3D9MapResources', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9RegisterResource', ('hipD3D9RegisterResource', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedArray', ('hipD3D9ResourceGetMappedArray', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedPitch', ('hipD3D9ResourceGetMappedPitch', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedPointer', ('hipD3D9ResourceGetMappedPointer', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetMappedSize', ('hipD3D9ResourceGetMappedSize', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceGetSurfaceDimensions', ('hipD3D9ResourceGetSurfaceDimensions', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9ResourceSetMapFlags', ('hipD3D9ResourceSetMapFlags', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9UnmapResources', ('hipD3D9UnmapResources', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D9UnregisterResource', ('hipD3D9UnregisterResource', CONV_D3D9, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceListAll', ('HIP_D3D10_DEVICE_LIST_ALL', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceListCurrentFrame', ('HIP_D3D10_DEVICE_LIST_CURRENT_FRAME', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10DeviceListNextFrame', ('HIP_D3D10_DEVICE_LIST_NEXT_FRAME', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10GetDevice', ('hipD3D10GetDevice', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10GetDevices', ('hipD3D10GetDevices', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D10RegisterResource', ('hipGraphicsD3D10RegisterResource', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlagsNone', ('HIP_D3D10_MAPRESOURCE_FLAGS_NONE', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlagsReadOnly', ('HIP_D3D10_MAPRESOURCE_FLAGS_READONLY', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapFlagsWriteDiscard', ('HIP_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterFlagsNone', ('HIP_D3D10_REGISTER_FLAGS_NONE', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterFlagsArray', ('HIP_D3D10_REGISTER_FLAGS_ARRAY', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10GetDirect3DDevice', ('hipD3D10GetDirect3DDevice', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10MapResources', ('hipD3D10MapResources', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10RegisterResource', ('hipD3D10RegisterResource', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedArray', ('hipD3D10ResourceGetMappedArray', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedPitch', ('hipD3D10ResourceGetMappedPitch', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedPointer', ('hipD3D10ResourceGetMappedPointer', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetMappedSize', ('hipD3D10ResourceGetMappedSize', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceGetSurfaceDimensions', ('hipD3D10ResourceGetSurfaceDimensions', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10ResourceSetMapFlags', ('hipD3D10ResourceSetMapFlags', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10SetDirect3DDevice', ('hipD3D10SetDirect3DDevice', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10UnmapResources', ('hipD3D10UnmapResources', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D10UnregisterResource', ('hipD3D10UnregisterResource', CONV_D3D10, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceListAll', ('HIP_D3D11_DEVICE_LIST_ALL', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceListCurrentFrame', ('HIP_D3D11_DEVICE_LIST_CURRENT_FRAME', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11DeviceListNextFrame', ('HIP_D3D11_DEVICE_LIST_NEXT_FRAME', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevice', ('hipD3D11GetDevice', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevices', ('hipD3D11GetDevices', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D11RegisterResource', ('hipGraphicsD3D11RegisterResource', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevice', ('hipD3D11GetDevice', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaD3D11GetDevices', ('hipD3D11GetDevices', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsD3D11RegisterResource', ('hipGraphicsD3D11RegisterResource', CONV_D3D11, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsVDPAURegisterOutputSurface', ('hipGraphicsVDPAURegisterOutputSurface', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsVDPAURegisterVideoSurface', ('hipGraphicsVDPAURegisterVideoSurface', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaVDPAUGetDevice', ('hipVDPAUGetDevice', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaVDPAUSetVDPAUDevice', ('hipVDPAUSetDevice', CONV_VDPAU, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerAcquireFrame', ('hipEGLStreamConsumerAcquireFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerConnect', ('hipEGLStreamConsumerConnect', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerConnectWithFlags', ('hipEGLStreamConsumerConnectWithFlags', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamConsumerReleaseFrame', ('hipEGLStreamConsumerReleaseFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerConnect', ('hipEGLStreamProducerConnect', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerDisconnect', ('hipEGLStreamProducerDisconnect', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerPresentFrame', ('hipEGLStreamProducerPresentFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaEGLStreamProducerReturnFrame', ('hipEGLStreamProducerReturnFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsEGLRegisterImage', ('hipGraphicsEGLRegisterImage', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cudaGraphicsResourceGetMappedEglFrame', ('hipGraphicsResourceGetMappedEglFrame', CONV_EGL, API_RUNTIME, HIP_UNSUPPORTED)), ('cublasInit', ('rocblas_init', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasShutdown', ('rocblas_shutdown', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetVersion', ('rocblas_get_version', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetError', ('rocblas_get_error', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasAlloc', ('rocblas_alloc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasFree', ('rocblas_free', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetKernelStream', ('rocblas_set_kernel_stream', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetAtomicsMode', ('rocblas_get_atomics_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetAtomicsMode', ('rocblas_set_atomics_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetMathMode', ('rocblas_get_math_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetMathMode', ('rocblas_set_math_mode', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_OP_N', ('rocblas_operation_none', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_OP_T', ('rocblas_operation_transpose', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_OP_C', ('rocblas_operation_conjugate_transpose', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_SUCCESS', ('rocblas_status_success', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_NOT_INITIALIZED', ('rocblas_status_invalid_handle', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_ALLOC_FAILED', ('rocblas_status_memory_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_INVALID_VALUE', ('rocblas_status_invalid_pointer', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_MAPPING_ERROR', ('rocblas_status_internal_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_EXECUTION_FAILED', ('rocblas_status_internal_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_INTERNAL_ERROR', ('rocblas_status_internal_error', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_NOT_SUPPORTED', ('rocblas_status_not_implemented', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_STATUS_ARCH_MISMATCH', ('rocblas_status_not_implemented', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_FILL_MODE_LOWER', ('rocblas_fill_lower', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_FILL_MODE_UPPER', ('rocblas_fill_upper', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_DIAG_NON_UNIT', ('rocblas_diagonal_non_unit', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_DIAG_UNIT', ('rocblas_diagonal_unit', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_SIDE_LEFT', ('rocblas_side_left', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_SIDE_RIGHT', ('rocblas_side_right', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_POINTER_MODE_HOST', ('rocblas_pointer_mode_host', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_POINTER_MODE_DEVICE', ('rocblas_pointer_mode_device', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUBLAS_ATOMICS_NOT_ALLOWED', ('rocblas_atomics_not_allowed', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_ATOMICS_ALLOWED', ('rocblas_atomics_allowed', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_FLOAT', ('rocblas_precision_float', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_DOUBLE', ('rocblas_precision_double', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_HALF', ('rocblas_precision_half', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('CUBLAS_DATA_INT8', ('rocblas_precision_int8', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('cublasCreate', ('rocblas_create_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasDestroy', ('rocblas_destroy_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasSetVector', ('rocblas_set_vector', CONV_MATH_FUNC, API_BLAS)), ('cublasGetVector', ('rocblas_get_vector', CONV_MATH_FUNC, API_BLAS)), ('cublasSetVectorAsync', ('rocblas_set_vector_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGetVectorAsync', ('rocblas_get_vector_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetMatrix', ('rocblas_set_matrix', CONV_MATH_FUNC, API_BLAS)), ('cublasGetMatrix', ('rocblas_get_matrix', CONV_MATH_FUNC, API_BLAS)), ('cublasGetMatrixAsync', ('rocblas_get_matrix_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetMatrixAsync', ('rocblas_set_matrix_async', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasXerbla', ('rocblas_xerbla', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSnrm2', ('rocblas_snrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasDnrm2', ('rocblas_dnrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasScnrm2', ('rocblas_scnrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDznrm2', ('rocblas_dznrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasNrm2Ex', ('rocblas_nrm2_ex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSdot', ('rocblas_sdot', CONV_MATH_FUNC, API_BLAS)), ('cublasSdotBatched', ('rocblas_sdot_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDdot', ('rocblas_ddot', CONV_MATH_FUNC, API_BLAS)), ('cublasDdotBatched', ('rocblas_ddot_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdotu', ('rocblas_cdotu', CONV_MATH_FUNC, API_BLAS)), ('cublasCdotc', ('rocblas_cdotc', CONV_MATH_FUNC, API_BLAS)), ('cublasZdotu', ('rocblas_zdotu', CONV_MATH_FUNC, API_BLAS)), ('cublasZdotc', ('rocblas_zdotc', CONV_MATH_FUNC, API_BLAS)), ('cublasSscal', ('rocblas_sscal', CONV_MATH_FUNC, API_BLAS)), ('cublasSscalBatched', ('rocblas_sscal_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDscal', ('rocblas_dscal', CONV_MATH_FUNC, API_BLAS)), ('cublasDscalBatched', ('rocblas_dscal_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCscal', ('rocblas_cscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsscal', ('rocblas_csscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZscal', ('rocblas_zscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdscal', ('rocblas_zdscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSaxpy', ('rocblas_saxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasSaxpyBatched', ('rocblas_saxpy_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDaxpy', ('rocblas_daxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasCaxpy', ('rocblas_caxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZaxpy', ('rocblas_zaxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScopy', ('rocblas_scopy', CONV_MATH_FUNC, API_BLAS)), ('cublasScopyBatched', ('rocblas_scopy_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDcopy', ('rocblas_dcopy', CONV_MATH_FUNC, API_BLAS)), ('cublasDcopyBatched', ('rocblas_dcopy_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCcopy', ('rocblas_ccopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZcopy', ('rocblas_zcopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSswap', ('rocblas_sswap', CONV_MATH_FUNC, API_BLAS)), ('cublasDswap', ('rocblas_dswap', CONV_MATH_FUNC, API_BLAS)), ('cublasCswap', ('rocblas_cswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZswap', ('rocblas_zswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamax', ('rocblas_isamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamax', ('rocblas_idamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamax', ('rocblas_icamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamax', ('rocblas_izamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamin', ('rocblas_isamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamin', ('rocblas_idamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamin', ('rocblas_icamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamin', ('rocblas_izamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSasum', ('rocblas_sasum', CONV_MATH_FUNC, API_BLAS)), ('cublasSasumBatched', ('rocblas_sasum_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDasum', ('rocblas_dasum', CONV_MATH_FUNC, API_BLAS)), ('cublasDasumBatched', ('rocblas_dasum_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScasum', ('rocblas_scasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDzasum', ('rocblas_dzasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrot', ('rocblas_srot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrot', ('rocblas_drot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrot', ('rocblas_crot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsrot', ('rocblas_csrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrot', ('rocblas_zrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdrot', ('rocblas_zdrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotg', ('rocblas_srotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotg', ('rocblas_drotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrotg', ('rocblas_crotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrotg', ('rocblas_zrotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotm', ('rocblas_srotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotm', ('rocblas_drotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotmg', ('rocblas_srotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotmg', ('rocblas_drotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemv', ('rocblas_sgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasSgemvBatched', ('rocblas_sgemv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgemv', ('rocblas_dgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemv', ('rocblas_cgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemv', ('rocblas_zgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgbmv', ('rocblas_sgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgbmv', ('rocblas_dgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgbmv', ('rocblas_cgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgbmv', ('rocblas_zgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmv', ('rocblas_strmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmv', ('rocblas_dtrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmv', ('rocblas_ctrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmv', ('rocblas_ztrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbmv', ('rocblas_stbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbmv', ('rocblas_dtbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbmv', ('rocblas_ctbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbmv', ('rocblas_ztbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpmv', ('rocblas_stpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpmv', ('rocblas_dtpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpmv', ('rocblas_ctpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpmv', ('rocblas_ztpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsv', ('rocblas_strsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsv', ('rocblas_dtrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsv', ('rocblas_ctrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsv', ('rocblas_ztrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpsv', ('rocblas_stpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpsv', ('rocblas_dtpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpsv', ('rocblas_ctpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpsv', ('rocblas_ztpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbsv', ('rocblas_stbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbsv', ('rocblas_dtbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbsv', ('rocblas_ctbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbsv', ('rocblas_ztbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymv', ('rocblas_ssymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymv', ('rocblas_dsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymv', ('rocblas_csymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymv', ('rocblas_zsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemv', ('rocblas_chemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemv', ('rocblas_zhemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsbmv', ('rocblas_ssbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsbmv', ('rocblas_dsbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChbmv', ('rocblas_chbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhbmv', ('rocblas_zhbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspmv', ('rocblas_sspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspmv', ('rocblas_dspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpmv', ('rocblas_chpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpmv', ('rocblas_zhpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSger', ('rocblas_sger', CONV_MATH_FUNC, API_BLAS)), ('cublasDger', ('rocblas_dger', CONV_MATH_FUNC, API_BLAS)), ('cublasCgeru', ('rocblas_cgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgerc', ('rocblas_cgerc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeru', ('rocblas_zgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgerc', ('rocblas_zgerc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr', ('rocblas_ssyr', CONV_MATH_FUNC, API_BLAS)), ('cublasDsyr', ('rocblas_dsyr', CONV_MATH_FUNC, API_BLAS)), ('cublasCher', ('rocblas_cher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher', ('rocblas_zher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr', ('rocblas_sspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr', ('rocblas_dspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr', ('rocblas_chpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr', ('rocblas_zhpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2', ('rocblas_ssyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2', ('rocblas_dsyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2', ('rocblas_cher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2', ('rocblas_zher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr2', ('rocblas_sspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr2', ('rocblas_dspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr2', ('rocblas_chpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr2', ('rocblas_zhpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemmBatched', ('rocblas_sgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgemmBatched', ('rocblas_dgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasHgemmBatched', ('rocblas_hgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemmStridedBatched', ('rocblas_sgemm_strided_batched', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemmStridedBatched', ('rocblas_dgemm_strided_batched', CONV_MATH_FUNC, API_BLAS)), ('cublasHgemmStridedBatched', ('rocblas_hgemm_strided_batched', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemmBatched', ('rocblas_cgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3mBatched', ('rocblas_cgemm_3m_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemmBatched', ('rocblas_zgemm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemmStridedBatched', ('rocblas_cgemm_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3mStridedBatched', ('rocblas_cgemm_3m_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemmStridedBatched', ('rocblas_zgemm_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasHgemmStridedBatched', ('rocblas_hgemm_strided_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemm', ('rocblas_sgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemm', ('rocblas_dgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemm', ('rocblas_cgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasZgemm', ('rocblas_zgemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasHgemm', ('rocblas_hgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasSsyrk', ('rocblas_ssyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyrk', ('rocblas_dsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrk', ('rocblas_csyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyrk', ('rocblas_zsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherk', ('rocblas_cherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZherk', ('rocblas_zherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2k', ('rocblas_ssyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2k', ('rocblas_dsyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr2k', ('rocblas_csyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr2k', ('rocblas_zyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyrkx', ('rocblas_ssyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyrkx', ('rocblas_dsyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrkx', ('rocblas_csyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyrkx', ('rocblas_zsyrkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2k', ('rocblas_cher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2k', ('rocblas_zher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherkx', ('rocblas_cherkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZherkx', ('rocblas_zherkx', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymm', ('rocblas_ssymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymm', ('rocblas_dsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymm', ('rocblas_csymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymm', ('rocblas_zsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemm', ('rocblas_chemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemm', ('rocblas_zhemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsm', ('rocblas_strsm', CONV_MATH_FUNC, API_BLAS)), ('cublasDtrsm', ('rocblas_dtrsm', CONV_MATH_FUNC, API_BLAS)), ('cublasCtrsm', ('rocblas_ctrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsm', ('rocblas_ztrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsmBatched', ('rocblas_strsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsmBatched', ('rocblas_dtrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsmBatched', ('rocblas_ctrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsmBatched', ('rocblas_ztrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmm', ('rocblas_strmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmm', ('rocblas_dtrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmm', ('rocblas_ctrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmm', ('rocblas_ztrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgeam', ('rocblas_sgeam', CONV_MATH_FUNC, API_BLAS)), ('cublasDgeam', ('rocblas_dgeam', CONV_MATH_FUNC, API_BLAS)), ('cublasCgeam', ('rocblas_cgeam', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeam', ('rocblas_zgeam', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgetrfBatched', ('rocblas_sgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgetrfBatched', ('rocblas_dgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgetrfBatched', ('rocblas_cgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgetrfBatched', ('rocblas_zgetrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgetriBatched', ('rocblas_sgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgetriBatched', ('rocblas_dgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgetriBatched', ('rocblas_cgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgetriBatched', ('rocblas_zgetri_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgetrsBatched', ('rocblas_sgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgetrsBatched', ('rocblas_dgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgetrsBatched', ('rocblas_cgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgetrsBatched', ('rocblas_zgetrs_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsmBatched', ('rocblas_strsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsmBatched', ('rocblas_dtrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsmBatched', ('rocblas_ctrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsmBatched', ('rocblas_ztrsm_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSmatinvBatched', ('rocblas_smatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDmatinvBatched', ('rocblas_dmatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCmatinvBatched', ('rocblas_cmatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZmatinvBatched', ('rocblas_zmatinv_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgeqrfBatched', ('rocblas_sgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgeqrfBatched', ('rocblas_dgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgeqrfBatched', ('rocblas_cgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeqrfBatched', ('rocblas_zgeqrf_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgelsBatched', ('rocblas_sgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgelsBatched', ('rocblas_dgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgelsBatched', ('rocblas_cgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgelsBatched', ('rocblas_zgels_batched', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSdgmm', ('rocblas_sdgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDdgmm', ('rocblas_ddgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdgmm', ('rocblas_cdgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdgmm', ('rocblas_zdgmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpttr', ('rocblas_stpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpttr', ('rocblas_dtpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpttr', ('rocblas_ctpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpttr', ('rocblas_ztpttr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrttp', ('rocblas_strttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrttp', ('rocblas_dtrttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrttp', ('rocblas_ctrttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrttp', ('rocblas_ztrttp', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCreate_v2', ('rocblas_create_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasDestroy_v2', ('rocblas_destroy_handle', CONV_MATH_FUNC, API_BLAS)), ('cublasGetVersion_v2', ('rocblas_get_version', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSetStream', ('rocblas_set_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasGetStream', ('rocblas_get_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasSetStream_v2', ('rocblas_set_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasGetStream_v2', ('rocblas_get_stream', CONV_MATH_FUNC, API_BLAS)), ('cublasGetPointerMode', ('rocblas_get_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasSetPointerMode', ('rocblas_set_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasGetPointerMode_v2', ('rocblas_get_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasSetPointerMode_v2', ('rocblas_set_pointer_mode', CONV_MATH_FUNC, API_BLAS)), ('cublasSgemv_v2', ('rocblas_sgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemv_v2', ('rocblas_dgemv', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemv_v2', ('rocblas_cgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemv_v2', ('rocblas_zgemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgbmv_v2', ('rocblas_sgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDgbmv_v2', ('rocblas_dgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgbmv_v2', ('rocblas_cgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgbmv_v2', ('rocblas_zgbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmv_v2', ('rocblas_strmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmv_v2', ('rocblas_dtrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmv_v2', ('rocblas_ctrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmv_v2', ('rocblas_ztrmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbmv_v2', ('rocblas_stbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbmv_v2', ('rocblas_dtbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbmv_v2', ('rocblas_ctbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbmv_v2', ('rocblas_ztbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpmv_v2', ('rocblas_stpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpmv_v2', ('rocblas_dtpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpmv_v2', ('rocblas_ctpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpmv_v2', ('rocblas_ztpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsv_v2', ('rocblas_strsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsv_v2', ('rocblas_dtrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsv_v2', ('rocblas_ctrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsv_v2', ('rocblas_ztrsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStpsv_v2', ('rocblas_stpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtpsv_v2', ('rocblas_dtpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtpsv_v2', ('rocblas_ctpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtpsv_v2', ('rocblas_ztpsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStbsv_v2', ('rocblas_stbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtbsv_v2', ('rocblas_dtbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtbsv_v2', ('rocblas_ctbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtbsv_v2', ('rocblas_ztbsv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymv_v2', ('rocblas_ssymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymv_v2', ('rocblas_dsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymv_v2', ('rocblas_csymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymv_v2', ('rocblas_zsymv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemv_v2', ('rocblas_chemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemv_v2', ('rocblas_zhemv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsbmv_v2', ('rocblas_ssbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsbmv_v2', ('rocblas_dsbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChbmv_v2', ('rocblas_chbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhbmv_v2', ('rocblas_zhbmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspmv_v2', ('rocblas_sspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspmv_v2', ('rocblas_dspmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpmv_v2', ('rocblas_chpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpmv_v2', ('rocblas_zhpmv', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSger_v2', ('rocblas_sger', CONV_MATH_FUNC, API_BLAS)), ('cublasDger_v2', ('rocblas_dger', CONV_MATH_FUNC, API_BLAS)), ('cublasCgeru_v2', ('rocblas_cgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgerc_v2', ('rocblas_cergc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgeru_v2', ('rocblas_zgeru', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgerc_v2', ('rocblas_zgerc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr_v2', ('rocblas_ssyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr_v2', ('rocblas_dsyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr_v2', ('rocblas_csyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr_v2', ('rocblas_zsyr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher_v2', ('rocblas_cher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher_v2', ('rocblas_zher', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr_v2', ('rocblas_sspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr_v2', ('rocblas_dspr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr_v2', ('rocblas_chpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr_v2', ('rocblas_zhpr', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2_v2', ('rocblas_ssyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2_v2', ('rocblas_dsyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr2_v2', ('rocblas_csyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr2_v2', ('rocblas_zsyr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2_v2', ('rocblas_cher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2_v2', ('rocblas_zher2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSspr2_v2', ('rocblas_sspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDspr2_v2', ('rocblas_dspr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChpr2_v2', ('rocblas_chpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhpr2_v2', ('rocblas_zhpr2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemm_v2', ('rocblas_sgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasDgemm_v2', ('rocblas_dgemm', CONV_MATH_FUNC, API_BLAS)), ('cublasCgemm_v2', ('rocblas_cgemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3m', ('rocblas_cgemm_3m', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemm3mEx', ('rocblas_cgemm_3mex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemm_v2', ('rocblas_zgemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZgemm3m', ('rocblas_zgemm_3m', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSgemmEx', ('rocblas_sgemmex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasGemmEx', ('rocblas_gemmex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCgemmEx', ('rocblas_cgemmex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasUint8gemmBias', ('rocblas_uint8gemmbias', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyrk_v2', ('rocblas_ssyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyrk_v2', ('rocblas_dsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrk_v2', ('rocblas_csyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyrk_v2', ('rocblas_zsyrk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrkEx', ('rocblas_csyrkex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyrk3mEx', ('rocblas_csyrk3mex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherk_v2', ('rocblas_cherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherkEx', ('rocblas_cherkex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCherk3mEx', ('rocblas_cherk3mex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZherk_v2', ('rocblas_zherk', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsyr2k_v2', ('rocblas_ssyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsyr2k_v2', ('rocblas_dsyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsyr2k_v2', ('rocblas_csyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsyr2k_v2', ('rocblas_zsyr2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCher2k_v2', ('rocblas_cher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZher2k_v2', ('rocblas_zher2k', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSsymm_v2', ('rocblas_ssymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDsymm_v2', ('rocblas_dsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsymm_v2', ('rocblas_csymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZsymm_v2', ('rocblas_zsymm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasChemm_v2', ('rocblas_chemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZhemm_v2', ('rocblas_zhemm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrsm_v2', ('rocblas_strsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrsm_v2', ('rocblas_dtrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrsm_v2', ('rocblas_ctrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrsm_v2', ('rocblas_ztrsm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasStrmm_v2', ('rocblas_strmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDtrmm_v2', ('rocblas_dtrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCtrmm_v2', ('rocblas_ctrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZtrmm_v2', ('rocblas_ztrmm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSnrm2_v2', ('rocblas_snrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasDnrm2_v2', ('rocblas_dnrm2', CONV_MATH_FUNC, API_BLAS)), ('cublasScnrm2_v2', ('rocblas_scnrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDznrm2_v2', ('rocblas_dznrm2', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDotEx', ('rocblas_dotex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDotcEx', ('rocblas_dotcex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSdot_v2', ('rocblas_sdot', CONV_MATH_FUNC, API_BLAS)), ('cublasDdot_v2', ('rocblas_ddot', CONV_MATH_FUNC, API_BLAS)), ('cublasCdotu_v2', ('rocblas_cdotu', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCdotc_v2', ('rocblas_cdotc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdotu_v2', ('rocblas_zdotu', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdotc_v2', ('rocblas_zdotc', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScalEx', ('rocblas_scalex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSscal_v2', ('rocblas_sscal', CONV_MATH_FUNC, API_BLAS)), ('cublasDscal_v2', ('rocblas_dscal', CONV_MATH_FUNC, API_BLAS)), ('cublasCscal_v2', ('rocblas_cscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsscal_v2', ('rocblas_csscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZscal_v2', ('rocblas_zcsal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdscal_v2', ('rocblas_zdscal', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasAxpyEx', ('rocblas_axpyex', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSaxpy_v2', ('rocblas_saxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasDaxpy_v2', ('rocblas_daxpy', CONV_MATH_FUNC, API_BLAS)), ('cublasCaxpy_v2', ('rocblas_caxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZaxpy_v2', ('rocblas_zaxpy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasScopy_v2', ('rocblas_scopy', CONV_MATH_FUNC, API_BLAS)), ('cublasDcopy_v2', ('rocblas_dcopy', CONV_MATH_FUNC, API_BLAS)), ('cublasCcopy_v2', ('rocblas_ccopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZcopy_v2', ('rocblas_zcopy', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSswap_v2', ('rocblas_sswap', CONV_MATH_FUNC, API_BLAS)), ('cublasDswap_v2', ('rocblas_dswap', CONV_MATH_FUNC, API_BLAS)), ('cublasCswap_v2', ('rocblas_cswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZswap_v2', ('rocblas_zswap', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamax_v2', ('rocblas_isamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamax_v2', ('rocblas_idamax', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamax_v2', ('rocblas_icamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamax_v2', ('rocblas_izamax', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIsamin_v2', ('rocblas_isamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIdamin_v2', ('rocblas_idamin', CONV_MATH_FUNC, API_BLAS)), ('cublasIcamin_v2', ('rocblas_icamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasIzamin_v2', ('rocblas_izamin', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSasum_v2', ('rocblas_sasum', CONV_MATH_FUNC, API_BLAS)), ('cublasDasum_v2', ('rocblas_dasum', CONV_MATH_FUNC, API_BLAS)), ('cublasScasum_v2', ('rocblas_scasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDzasum_v2', ('rocblas_dzasum', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrot_v2', ('rocblas_srot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrot_v2', ('rocblas_drot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrot_v2', ('rocblas_crot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCsrot_v2', ('rocblas_csrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrot_v2', ('rocblas_zrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZdrot_v2', ('rocblas_zdrot', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotg_v2', ('rocblas_srotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotg_v2', ('rocblas_drotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasCrotg_v2', ('rocblas_crotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasZrotg_v2', ('rocblas_zrotg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotm_v2', ('rocblas_srotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotm_v2', ('rocblas_drotm', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasSrotmg_v2', ('rocblas_srotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('cublasDrotmg_v2', ('rocblas_drotmg', CONV_MATH_FUNC, API_BLAS, HIP_UNSUPPORTED)), ('CURAND_STATUS_SUCCESS', ('HIPRAND_STATUS_SUCCESS', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_VERSION_MISMATCH', ('HIPRAND_STATUS_VERSION_MISMATCH', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_NOT_INITIALIZED', ('HIPRAND_STATUS_NOT_INITIALIZED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_ALLOCATION_FAILED', ('HIPRAND_STATUS_ALLOCATION_FAILED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_TYPE_ERROR', ('HIPRAND_STATUS_TYPE_ERROR', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_OUT_OF_RANGE', ('HIPRAND_STATUS_OUT_OF_RANGE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_LENGTH_NOT_MULTIPLE', ('HIPRAND_STATUS_LENGTH_NOT_MULTIPLE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_DOUBLE_PRECISION_REQUIRED', ('HIPRAND_STATUS_DOUBLE_PRECISION_REQUIRED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_LAUNCH_FAILURE', ('HIPRAND_STATUS_LAUNCH_FAILURE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_PREEXISTING_FAILURE', ('HIPRAND_STATUS_PREEXISTING_FAILURE', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_INITIALIZATION_FAILED', ('HIPRAND_STATUS_INITIALIZATION_FAILED', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_ARCH_MISMATCH', ('HIPRAND_STATUS_ARCH_MISMATCH', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_STATUS_INTERNAL_ERROR', ('HIPRAND_STATUS_INTERNAL_ERROR', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_TEST', ('HIPRAND_RNG_TEST', CONV_NUMERIC_LITERAL, API_RAND)), ('mtgp32dc_params_fast_11213', ('mtgp32dc_params_fast_11213', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_DEFAULT', ('HIPRAND_RNG_PSEUDO_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_XORWOW', ('HIPRAND_RNG_PSEUDO_XORWOW', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_MRG32K3A', ('HIPRAND_RNG_PSEUDO_MRG32K3A', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_MTGP32', ('HIPRAND_RNG_PSEUDO_MTGP32', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_MT19937', ('HIPRAND_RNG_PSEUDO_MT19937', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_PSEUDO_PHILOX4_32_10', ('HIPRAND_RNG_PSEUDO_PHILOX4_32_10', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_DEFAULT', ('HIPRAND_RNG_QUASI_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SOBOL32', ('HIPRAND_RNG_QUASI_SOBOL32', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SCRAMBLED_SOBOL32', ('HIPRAND_RNG_QUASI_SCRAMBLED_SOBOL32', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SOBOL64', ('HIPRAND_RNG_QUASI_SOBOL64', CONV_NUMERIC_LITERAL, API_RAND)), ('CURAND_RNG_QUASI_SCRAMBLED_SOBOL64', ('HIPRAND_RNG_QUASI_SCRAMBLED_SOBOL64', CONV_NUMERIC_LITERAL, API_RAND)), ('curand_ORDERING_PSEUDO_BEST', ('HIPRAND_ORDERING_PSEUDO_BEST', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ORDERING_PSEUDO_DEFAULT', ('HIPRAND_ORDERING_PSEUDO_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ORDERING_PSEUDO_SEEDED', ('HIPRAND_ORDERING_PSEUDO_SEEDED', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ORDERING_QUASI_DEFAULT', ('HIPRAND_ORDERING_QUASI_DEFAULT', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DIRECTION_VECTORS_32_JOEKUO6', ('HIPRAND_DIRECTION_VECTORS_32_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_SCRAMBLED_DIRECTION_VECTORS_32_JOEKUO6', ('HIPRAND_SCRAMBLED_DIRECTION_VECTORS_32_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DIRECTION_VECTORS_64_JOEKUO6', ('HIPRAND_DIRECTION_VECTORS_64_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_SCRAMBLED_DIRECTION_VECTORS_64_JOEKUO6', ('HIPRAND_SCRAMBLED_DIRECTION_VECTORS_64_JOEKUO6', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_CHOOSE_BEST', ('HIPRAND_CHOOSE_BEST', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_ITR', ('HIPRAND_ITR', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_KNUTH', ('HIPRAND_KNUTH', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_HITR', ('HIPRAND_HITR', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_M1', ('HIPRAND_M1', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_M2', ('HIPRAND_M2', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_BINARY_SEARCH', ('HIPRAND_BINARY_SEARCH', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DISCRETE_GAUSS', ('HIPRAND_DISCRETE_GAUSS', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_REJECTION', ('HIPRAND_REJECTION', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DEVICE_API', ('HIPRAND_DEVICE_API', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_FAST_REJECTION', ('HIPRAND_FAST_REJECTION', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_3RD', ('HIPRAND_3RD', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_DEFINITION', ('HIPRAND_DEFINITION', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curand_POISSON', ('HIPRAND_POISSON', CONV_NUMERIC_LITERAL, API_RAND, HIP_UNSUPPORTED)), ('curandCreateGenerator', ('hiprandCreateGenerator', CONV_MATH_FUNC, API_RAND)), ('curandCreateGeneratorHost', ('hiprandCreateGeneratorHost', CONV_MATH_FUNC, API_RAND)), ('curandCreatePoissonDistribution', ('hiprandCreatePoissonDistribution', CONV_MATH_FUNC, API_RAND)), ('curandDestroyDistribution', ('hiprandDestroyDistribution', CONV_MATH_FUNC, API_RAND)), ('curandDestroyGenerator', ('hiprandDestroyGenerator', CONV_MATH_FUNC, API_RAND)), ('curandGenerate', ('hiprandGenerate', CONV_MATH_FUNC, API_RAND)), ('curandGenerateLogNormal', ('hiprandGenerateLogNormal', CONV_MATH_FUNC, API_RAND)), ('curandGenerateLogNormalDouble', ('hiprandGenerateLogNormalDouble', CONV_MATH_FUNC, API_RAND)), ('curandGenerateLongLong', ('hiprandGenerateLongLong', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGenerateNormal', ('hiprandGenerateNormal', CONV_MATH_FUNC, API_RAND)), ('curandGenerateNormalDouble', ('hiprandGenerateNormalDouble', CONV_MATH_FUNC, API_RAND)), ('curandGeneratePoisson', ('hiprandGeneratePoisson', CONV_MATH_FUNC, API_RAND)), ('curandGenerateSeeds', ('hiprandGenerateSeeds', CONV_MATH_FUNC, API_RAND)), ('curandGenerateUniform', ('hiprandGenerateUniform', CONV_MATH_FUNC, API_RAND)), ('curandGenerateUniformDouble', ('hiprandGenerateUniformDouble', CONV_MATH_FUNC, API_RAND)), ('curandGetDirectionVectors32', ('hiprandGetDirectionVectors32', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetDirectionVectors64', ('hiprandGetDirectionVectors64', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetProperty', ('hiprandGetProperty', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetScrambleConstants32', ('hiprandGetScrambleConstants32', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetScrambleConstants64', ('hiprandGetScrambleConstants64', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandGetVersion', ('hiprandGetVersion', CONV_MATH_FUNC, API_RAND)), ('curandSetGeneratorOffset', ('hiprandSetGeneratorOffset', CONV_MATH_FUNC, API_RAND)), ('curandSetGeneratorOrdering', ('hiprandSetGeneratorOrdering', CONV_MATH_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curandSetPseudoRandomGeneratorSeed', ('hiprandSetPseudoRandomGeneratorSeed', CONV_MATH_FUNC, API_RAND)), ('curandSetQuasiRandomGeneratorDimensions', ('hiprandSetQuasiRandomGeneratorDimensions', CONV_MATH_FUNC, API_RAND)), ('curandSetStream', ('hiprandSetStream', CONV_MATH_FUNC, API_RAND)), ('curand', ('hiprand', CONV_DEVICE_FUNC, API_RAND)), ('curand4', ('hiprand4', CONV_DEVICE_FUNC, API_RAND)), ('curand_init', ('hiprand_init', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal', ('hiprand_log_normal', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal_double', ('hiprand_log_normal_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal2', ('hiprand_log_normal2', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal2_double', ('hiprand_log_normal2_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal4', ('hiprand_log_normal4', CONV_DEVICE_FUNC, API_RAND)), ('curand_log_normal4_double', ('hiprand_log_normal4_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_mtgp32_single', ('hiprand_mtgp32_single', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curand_mtgp32_single_specific', ('hiprand_mtgp32_single_specific', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curand_mtgp32_specific', ('hiprand_mtgp32_specific', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('curand_normal', ('hiprand_normal', CONV_DEVICE_FUNC, API_RAND)), ('curandMakeMTGP32Constants', ('hiprandMakeMTGP32Constants', CONV_DEVICE_FUNC, API_RAND)), ('curandMakeMTGP32KernelState', ('hiprandMakeMTGP32KernelState', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal_double', ('hiprand_normal_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal2', ('hiprand_normal2', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal2_double', ('hiprand_normal2_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal4', ('hiprand_normal4', CONV_DEVICE_FUNC, API_RAND)), ('curand_normal4_double', ('hiprand_normal4_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform', ('hiprand_uniform', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform_double', ('hiprand_uniform_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform2_double', ('hiprand_uniform2_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform4', ('hiprand_uniform4', CONV_DEVICE_FUNC, API_RAND)), ('curand_uniform4_double', ('hiprand_uniform4_double', CONV_DEVICE_FUNC, API_RAND)), ('curand_discrete', ('hiprand_discrete', CONV_DEVICE_FUNC, API_RAND)), ('curand_discrete4', ('hiprand_discrete4', CONV_DEVICE_FUNC, API_RAND)), ('curand_poisson', ('hiprand_poisson', CONV_DEVICE_FUNC, API_RAND)), ('curand_poisson4', ('hiprand_poisson4', CONV_DEVICE_FUNC, API_RAND)), ('curand_Philox4x32_10', ('hiprand_Philox4x32_10', CONV_DEVICE_FUNC, API_RAND, HIP_UNSUPPORTED)), ('mtgp32_kernel_params', ('mtgp32_kernel_params_t', CONV_MATH_FUNC, API_RAND)), ('CUFFT_FORWARD', ('HIPFFT_FORWARD', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUFFT_INVERSE', ('HIPFFT_BACKWARD', CONV_NUMERIC_LITERAL, API_BLAS)), ('CUFFT_COMPATIBILITY_DEFAULT', ('HIPFFT_COMPATIBILITY_DEFAULT', CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED)), ('cuComplex', ('rocblas_float_complex', CONV_TYPE, API_BLAS)), ('cuDoubleComplex', ('rocblas_double_complex', CONV_TYPE, API_BLAS)), ('cufftResult_t', ('hipfftResult_t', CONV_TYPE, API_FFT)), ('cufftResult', ('hipfftResult', CONV_TYPE, API_FFT)), ('CUFFT_SUCCESS', ('HIPFFT_SUCCESS', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_PLAN', ('HIPFFT_INVALID_PLAN', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_ALLOC_FAILED', ('HIPFFT_ALLOC_FAILED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_TYPE', ('HIPFFT_INVALID_TYPE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_VALUE', ('HIPFFT_INVALID_VALUE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INTERNAL_ERROR', ('HIPFFT_INTERNAL_ERROR', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_EXEC_FAILED', ('HIPFFT_EXEC_FAILED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_SETUP_FAILED', ('HIPFFT_SETUP_FAILED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_SIZE', ('HIPFFT_INVALID_SIZE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_UNALIGNED_DATA', ('HIPFFT_UNALIGNED_DATA', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INCOMPLETE_PARAMETER_LIST', ('HIPFFT_INCOMPLETE_PARAMETER_LIST', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_INVALID_DEVICE', ('HIPFFT_INVALID_DEVICE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_PARSE_ERROR', ('HIPFFT_PARSE_ERROR', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_NO_WORKSPACE', ('HIPFFT_NO_WORKSPACE', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_NOT_IMPLEMENTED', ('HIPFFT_NOT_IMPLEMENTED', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_LICENSE_ERROR', ('HIPFFT_LICENSE_ERROR', CONV_NUMERIC_LITERAL, API_FFT, HIP_UNSUPPORTED)), ('CUFFT_NOT_SUPPORTED', ('HIPFFT_NOT_SUPPORTED', CONV_NUMERIC_LITERAL, API_FFT)), ('cufftType_t', ('hipfftType_t', CONV_TYPE, API_FFT)), ('cufftType', ('hipfftType', CONV_TYPE, API_FFT)), ('CUFFT_R2C', ('HIPFFT_R2C', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_C2R', ('HIPFFT_C2R', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_C2C', ('HIPFFT_C2C', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_D2Z', ('HIPFFT_D2Z', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_Z2D', ('HIPFFT_Z2D', CONV_NUMERIC_LITERAL, API_FFT)), ('CUFFT_Z2Z', ('HIPFFT_Z2Z', CONV_NUMERIC_LITERAL, API_FFT)), ('cufftCompatibility_t', ('hipfftCompatibility_t', CONV_TYPE, API_FFT, HIP_UNSUPPORTED)), ('cufftCompatibility', ('hipfftCompatibility', CONV_TYPE, API_FFT, HIP_UNSUPPORTED)), ('CUFFT_COMPATIBILITY_FFTW_PADDING', ('HIPFFT_COMPATIBILITY_FFTW_PADDING', CONV_NUMERIC_LITERAL, API_FFT, HIP_UNSUPPORTED)), ('cufftReal', ('hipfftReal', CONV_TYPE, API_FFT)), ('cufftDoubleReal', ('hipfftDoubleReal', CONV_TYPE, API_FFT)), ('cufftComplex', ('hipfftComplex', CONV_TYPE, API_FFT)), ('cufftDoubleComplex', ('hipfftDoubleComplex', CONV_TYPE, API_FFT)), ('cufftHandle', ('hipfftHandle', CONV_TYPE, API_FFT)), ('cufftPlan1d', ('hipfftPlan1d', CONV_MATH_FUNC, API_FFT)), ('cufftPlan2d', ('hipfftPlan2d', CONV_MATH_FUNC, API_FFT)), ('cufftPlan3d', ('hipfftPlan3d', CONV_MATH_FUNC, API_FFT)), ('cufftPlanMany', ('hipfftPlanMany', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlan1d', ('hipfftMakePlan1d', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlan2d', ('hipfftMakePlan2d', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlan3d', ('hipfftMakePlan3d', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlanMany', ('hipfftMakePlanMany', CONV_MATH_FUNC, API_FFT)), ('cufftMakePlanMany64', ('hipfftMakePlanMany64', CONV_MATH_FUNC, API_FFT)), ('cufftGetSizeMany64', ('hipfftGetSizeMany64', CONV_MATH_FUNC, API_FFT)), ('cufftEstimate1d', ('hipfftEstimate1d', CONV_MATH_FUNC, API_FFT)), ('cufftEstimate2d', ('hipfftEstimate2d', CONV_MATH_FUNC, API_FFT)), ('cufftEstimate3d', ('hipfftEstimate3d', CONV_MATH_FUNC, API_FFT)), ('cufftEstimateMany', ('hipfftEstimateMany', CONV_MATH_FUNC, API_FFT)), ('cufftCreate', ('hipfftCreate', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize1d', ('hipfftGetSize1d', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize2d', ('hipfftGetSize2d', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize3d', ('hipfftGetSize3d', CONV_MATH_FUNC, API_FFT)), ('cufftGetSizeMany', ('hipfftGetSizeMany', CONV_MATH_FUNC, API_FFT)), ('cufftGetSize', ('hipfftGetSize', CONV_MATH_FUNC, API_FFT)), ('cufftSetWorkArea', ('hipfftSetWorkArea', CONV_MATH_FUNC, API_FFT)), ('cufftSetAutoAllocation', ('hipfftSetAutoAllocation', CONV_MATH_FUNC, API_FFT)), ('cufftExecC2C', ('hipfftExecC2C', CONV_MATH_FUNC, API_FFT)), ('cufftExecR2C', ('hipfftExecR2C', CONV_MATH_FUNC, API_FFT)), ('cufftExecC2R', ('hipfftExecC2R', CONV_MATH_FUNC, API_FFT)), ('cufftExecZ2Z', ('hipfftExecZ2Z', CONV_MATH_FUNC, API_FFT)), ('cufftExecD2Z', ('hipfftExecD2Z', CONV_MATH_FUNC, API_FFT)), ('cufftExecZ2D', ('hipfftExecZ2D', CONV_MATH_FUNC, API_FFT)), ('cufftSetStream', ('hipfftSetStream', CONV_MATH_FUNC, API_FFT)), ('cufftDestroy', ('hipfftDestroy', CONV_MATH_FUNC, API_FFT)), ('cufftGetVersion', ('hipfftGetVersion', CONV_MATH_FUNC, API_FFT)), ('cufftGetProperty', ('hipfftGetProperty', CONV_MATH_FUNC, API_FFT, HIP_UNSUPPORTED)), ('nvrtcResult', ('hiprtcResult', CONV_TYPE, API_RTC)), ('NVRTC_SUCCESS', ('HIPRTC_SUCCESS', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_OUT_OF_MEMORY', ('HIPRTC_ERROR_OUT_OF_MEMORY', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_PROGRAM_CREATION_FAILURE', ('HIPRTC_ERROR_PROGRAM_CREATION_FAILURE', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_INVALID_INPUT', ('HIPRTC_ERROR_INVALID_INPUT', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_INVALID_PROGRAM', ('HIPRTC_ERROR_INVALID_PROGRAM', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_COMPILATION', ('HIPRTC_ERROR_COMPILATION', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_BUILTIN_OPERATION_FAILURE', ('HIPRTC_ERROR_BUILTIN_OPERATION_FAILURE', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION', ('HIPRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID', ('HIPRTC_ERROR_NAME_EXPRESSION_NOT_VALID', CONV_TYPE, API_RTC)), ('NVRTC_ERROR_INTERNAL_ERROR', ('HIPRTC_ERROR_INTERNAL_ERROR', CONV_TYPE, API_RTC)), ('nvrtcGetErrorString', ('hiprtcGetErrorString', CONV_JIT, API_RTC)), ('nvrtcVersion', ('hiprtcVersion', CONV_JIT, API_RTC)), ('nvrtcProgram', ('hiprtcProgram', CONV_TYPE, API_RTC)), ('nvrtcAddNameExpression', ('hiprtcAddNameExpression', CONV_JIT, API_RTC)), ('nvrtcCompileProgram', ('hiprtcCompileProgram', CONV_JIT, API_RTC)), ('nvrtcCreateProgram', ('hiprtcCreateProgram', CONV_JIT, API_RTC)), ('nvrtcDestroyProgram', ('hiprtcDestroyProgram', CONV_JIT, API_RTC)), ('nvrtcGetLoweredName', ('hiprtcGetLoweredName', CONV_JIT, API_RTC)), ('nvrtcGetProgramLog', ('hiprtcGetProgramLog', CONV_JIT, API_RTC)), ('nvrtcGetProgramLogSize', ('hiprtcGetProgramLogSize', CONV_JIT, API_RTC)), ('nvrtcGetPTX', ('hiprtcGetCode', CONV_JIT, API_RTC)), ('nvrtcGetPTXSize', ('hiprtcGetCodeSize', CONV_JIT, API_RTC)), ('thrust::cuda', ('thrust::hip', CONV_MATH_FUNC, API_BLAS)), ('cub::', ('hipcub::', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::ArgMax', ('hipcub::ArgMax', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::ArgMin', ('hipcub::ArgMin', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::BLOCK_REDUCE_WARP_REDUCTIONS', ('hipcub::BLOCK_REDUCE_WARP_REDUCTIONS', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::BlockReduce', ('hipcub::BlockReduce', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::BlockScan', ('hipcub::BlockScan', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::CachingDeviceAllocator', ('hipcub::CachingDeviceAllocator', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::CountingInputIterator', ('hipcub::CountingInputIterator', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceRadixSort', ('hipcub::DeviceRadixSort', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceReduce', ('hipcub::DeviceReduce', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceRunLengthEncode', ('hipcub::DeviceRunLengthEncode', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceScan', ('hipcub::DeviceScan', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceSegmentedRadixSort', ('hipcub::DeviceSegmentedRadixSort', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceSegmentedReduce', ('hipcub::DeviceSegmentedReduce', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::DeviceSelect', ('hipcub::DeviceSelect', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::KeyValuePair', ('hipcub::KeyValuePair', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::Max', ('hipcub::Max', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::Min', ('hipcub::Min', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::Sum', ('hipcub::Sum', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::ArgIndexInputIterator', ('hipcub::ArgIndexInputIterator', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::TransformInputIterator', ('hipcub::TransformInputIterator', CONV_SPECIAL_FUNC, API_RUNTIME)), ('cub::WarpReduce', ('hipcub::WarpReduce', CONV_SPECIAL_FUNC, API_RUNTIME)), ('nvtxMark', ('roctxMark', CONV_OTHER, API_ROCTX)), ('nvtxMarkA', ('roctxMarkA', CONV_OTHER, API_ROCTX)), ('nvtxRangePushA', ('roctxRangePushA', CONV_OTHER, API_ROCTX)), ('nvtxRangePop', ('roctxRangePop', CONV_OTHER, API_ROCTX))])
A:torch.utils.hipify.cuda_to_hip_mappings.CUDA_SPARSE_MAP->collections.OrderedDict([('cusparseStatus_t', ('hipsparseStatus_t', CONV_MATH_FUNC, API_SPARSE)), ('cusparseHandle_t', ('hipsparseHandle_t', CONV_MATH_FUNC, API_SPARSE)), ('cuComplex', ('hipComplex', CONV_TYPE, API_SPARSE)), ('cuDoubleComplex', ('hipDoubleComplex', CONV_TYPE, API_SPARSE)), ('CUSPARSE_POINTER_MODE_HOST', ('HIPSPARSE_POINTER_MODE_HOST', CONV_NUMERIC_LITERAL, API_SPARSE)), ('cusparseOperation_t', ('hipsparseOperation_t', CONV_TYPE, API_SPARSE)), ('cusparseCreateMatDescr', ('hipsparseCreateMatDescr', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCreate', ('hipsparseCreate', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroyMatDescr', ('hipsparseDestroyMatDescr', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroy', ('hipsparseDestroy', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcoo2csr', ('hipsparseXcoo2csr', CONV_MATH_FUNC, API_SPARSE)), ('cusparseMatDescr_t', ('hipsparseMatDescr_t', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDiagType_t', ('hipsparseDiagType_t', CONV_TYPE, API_SPARSE)), ('CUSPARSE_DIAG_TYPE_UNIT', ('HIPSPARSE_DIAG_TYPE_UNIT', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_DIAG_TYPE_NON_UNIT', ('HIPSPARSE_DIAG_TYPE_NON_UNIT', CONV_NUMERIC_LITERAL, API_SPARSE)), ('cusparseSetMatDiagType', ('hipsparseSetMatDiagType', CONV_MATH_FUNC, API_SPARSE)), ('cusparseFillMode_t', ('hipsparseFillMode_t', CONV_TYPE, API_SPARSE)), ('CUSPARSE_FILL_MODE_UPPER', ('HIPSPARSE_FILL_MODE_UPPER', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_FILL_MODE_LOWER', ('HIPSPARSE_FILL_MODE_LOWER', CONV_NUMERIC_LITERAL, API_SPARSE)), ('cusparseSetMatFillMode', ('hipsparseSetMatFillMode', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDirection_t', ('hipsparseDirection_t', CONV_TYPE, API_SPARSE)), ('CUSPARSE_DIRECTION_ROW', ('HIPSPARSE_DIRECTION_ROW', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_DIRECTION_COLUMN', ('HIPSPARSE_DIRECTION_COLUMN', CONV_NUMERIC_LITERAL, API_SPARSE)), ('cusparseSolvePolicy_t', ('hipsparseSolvePolicy_t', CONV_TYPE, API_SPARSE)), ('CUSPARSE_SOLVE_POLICY_NO_LEVEL', ('HIPSPARSE_SOLVE_POLICY_NO_LEVEL', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_SOLVE_POLICY_USE_LEVEL', ('HIPSPARSE_SOLVE_POLICY_USE_LEVEL', CONV_NUMERIC_LITERAL, API_SPARSE)), ('cusparseCreateBsrsv2Info', ('hipsparseCreateBsrsv2Info', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCreateBsrsm2Info', ('hipsparseCreateBsrsm2Info', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroyBsrsv2Info', ('hipsparseDestroyBsrsv2Info', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroyBsrsm2Info', ('hipsparseDestroyBsrsm2Info', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrmm', ('hipsparseSbsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrmm', ('hipsparseDbsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrmm', ('hipsparseCbsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrmm', ('hipsparseZbsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrmv', ('hipsparseSbsrmv', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrmv', ('hipsparseDbsrmv', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrmv', ('hipsparseCbsrmv', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrmv', ('hipsparseZbsrmv', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrsv2_bufferSize', ('hipsparseSbsrsv2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrsv2_bufferSize', ('hipsparseDbsrsv2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrsv2_bufferSize', ('hipsparseCbsrsv2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrsv2_bufferSize', ('hipsparseZbsrsv2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrsv2_analysis', ('hipsparseSbsrsv2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrsv2_analysis', ('hipsparseDbsrsv2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrsv2_analysis', ('hipsparseCbsrsv2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrsv2_analysis', ('hipsparseZbsrsv2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrsv2_solve', ('hipsparseSbsrsv2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrsv2_solve', ('hipsparseDbsrsv2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrsv2_solve', ('hipsparseCbsrsv2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrsv2_solve', ('hipsparseZbsrsv2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrsm2_bufferSize', ('hipsparseSbsrsm2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrsm2_bufferSize', ('hipsparseDbsrsm2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrsm2_bufferSize', ('hipsparseCbsrsm2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrsm2_bufferSize', ('hipsparseZbsrsm2_bufferSize', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrsm2_analysis', ('hipsparseSbsrsm2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrsm2_analysis', ('hipsparseDbsrsm2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrsm2_analysis', ('hipsparseCbsrsm2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrsm2_analysis', ('hipsparseZbsrsm2_analysis', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSbsrsm2_solve', ('hipsparseSbsrsm2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDbsrsm2_solve', ('hipsparseDbsrsm2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCbsrsm2_solve', ('hipsparseCbsrsm2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZbsrsm2_solve', ('hipsparseZbsrsm2_solve', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrmm2', ('hipsparseScsrmm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrmm2', ('hipsparseDcsrmm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCcsrmm2', ('hipsparseCcsrmm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZcsrmm2', ('hipsparseZcsrmm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrmm', ('hipsparseScsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrmm', ('hipsparseDcsrmm', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcsrsort_bufferSizeExt', ('hipsparseXcsrsort_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCreateCsrgemm2Info', ('hipsparseCreateCsrgemm2Info', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDestroyCsrgemm2Info', ('hipsparseDestroyCsrgemm2Info', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcsrgemm2Nnz', ('hipsparseXcsrgemm2Nnz', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrgemm2_bufferSizeExt', ('hipsparseDcsrgemm2_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrgemm2_bufferSizeExt', ('hipsparseScsrgemm2_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrgemm2', ('hipsparseDcsrgemm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrgemm2', ('hipsparseScsrgemm2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetPointerMode', ('hipsparseSetPointerMode', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcsrgeam2Nnz', ('hipsparseXcsrgeam2Nnz', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrgeam2_bufferSizeExt', ('hipsparseScsrgeam2_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrgeam2_bufferSizeExt', ('hipsparseDcsrgeam2_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCcsrgeam2_bufferSizeExt', ('hipsparseCcsrgeam2_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZcsrgeam2_bufferSizeExt', ('hipsparseZcsrgeam2_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseScsrgeam2', ('hipsparseScsrgeam2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseDcsrgeam2', ('hipsparseDcsrgeam2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCcsrgeam2', ('hipsparseCcsrgeam2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseZcsrgeam2', ('hipsparseZcsrgeam2', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcsrsort', ('hipsparseXcsrsort', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcoosort_bufferSizeExt', ('hipsparseXcoosort_bufferSizeExt', CONV_MATH_FUNC, API_SPARSE)), ('cusparseXcoosortByRow', ('hipsparseXcoosortByRow', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetStream', ('hipsparseSetStream', CONV_MATH_FUNC, API_SPARSE)), ('cusparseCreateIdentityPermutation', ('hipsparseCreateIdentityPermutation', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetMatIndexBase', ('hipsparseSetMatIndexBase', CONV_MATH_FUNC, API_SPARSE)), ('cusparseSetMatType', ('hipsparseSetMatType', CONV_MATH_FUNC, API_SPARSE)), ('CUSPARSE_STATUS_SUCCESS', ('HIPSPARSE_STATUS_SUCCESS', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_NOT_INITIALIZED', ('HIPSPARSE_STATUS_NOT_INITIALIZED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_ALLOC_FAILED', ('HIPSPARSE_STATUS_ALLOC_FAILED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_INVALID_VALUE', ('HIPSPARSE_STATUS_INVALID_VALUE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_MAPPING_ERROR', ('HIPSPARSE_STATUS_MAPPING_ERROR', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_EXECUTION_FAILED', ('HIPSPARSE_STATUS_EXECUTION_FAILED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_INTERNAL_ERROR', ('HIPSPARSE_STATUS_INTERNAL_ERROR', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED', ('HIPSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_ARCH_MISMATCH', ('HIPSPARSE_STATUS_ARCH_MISMATCH', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_STATUS_ZERO_PIVOT', ('HIPSPARSE_STATUS_ZERO_PIVOT', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_OPERATION_TRANSPOSE', ('HIPSPARSE_OPERATION_TRANSPOSE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_OPERATION_NON_TRANSPOSE', ('HIPSPARSE_OPERATION_NON_TRANSPOSE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE', ('HIPSPARSE_OPERATION_CONJUGATE_TRANSPOSE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_INDEX_BASE_ZERO', ('HIPSPARSE_INDEX_BASE_ZERO', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_INDEX_BASE_ONE', ('HIPSPARSE_INDEX_BASE_ONE', CONV_NUMERIC_LITERAL, API_SPARSE)), ('CUSPARSE_MATRIX_TYPE_GENERAL', ('HIPSPARSE_MATRIX_TYPE_GENERAL', CONV_NUMERIC_LITERAL, API_SPARSE))])
A:torch.utils.hipify.cuda_to_hip_mappings.PYTORCH_SPECIFIC_MAPPINGS->collections.OrderedDict([('USE_CUDA', ('USE_ROCM', API_PYTORCH)), ('CUDA_VERSION', ('TORCH_HIP_VERSION', API_PYTORCH)), ('cudaHostAllocator', ('hipHostAllocator', API_PYTORCH)), ('cudaDeviceAllocator', ('hipDeviceAllocator', API_PYTORCH)), ('define MAX_NUM_BLOCKS 200', ('define MAX_NUM_BLOCKS 64', API_PYTORCH)), ('cuda::CUDAGuard', ('hip::HIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('CUDAGuard', ('HIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::OptionalCUDAGuard', ('hip::OptionalHIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('OptionalCUDAGuard', ('OptionalHIPGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDAStreamGuard', ('hip::HIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('CUDAStreamGuard', ('HIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::OptionalCUDAStreamGuard', ('hip::OptionalHIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('OptionalCUDAStreamGuard', ('OptionalHIPStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDAMultiStreamGuard', ('hip::HIPMultiStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('CUDAMultiStreamGuard', ('HIPMultiStreamGuardMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDACachingAllocator::get', ('hip::HIPCachingAllocatorMasqueradingAsCUDA::get', API_PYTORCH)), ('CUDACachingAllocator::get', ('HIPCachingAllocatorMasqueradingAsCUDA::get', API_PYTORCH)), ('cuda::CUDACachingAllocator::recordStream', ('hip::HIPCachingAllocatorMasqueradingAsCUDA::recordStreamMasqueradingAsCUDA', API_PYTORCH)), ('CUDACachingAllocator::recordStream', ('HIPCachingAllocatorMasqueradingAsCUDA::recordStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::CUDAStream', ('hip::HIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('CUDAStream', ('HIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getStreamFromPool', ('hip::getStreamFromPoolMasqueradingAsCUDA', API_PYTORCH)), ('getStreamFromPool', ('getStreamFromPoolMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getDefaultCUDAStream', ('hip::getDefaultHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getStreamFromExternal', ('hip::getStreamFromExternalMasqueradingAsCUDA', API_PYTORCH)), ('getStreamFromExternal', ('getStreamFromExternalMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getDefaultCUDAStream', ('hip::getDefaultHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('getDefaultCUDAStream', ('getDefaultHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::getCurrentCUDAStream', ('hip::getCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('getCurrentCUDAStream', ('getCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('cuda::setCurrentCUDAStream', ('hip::setCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('setCurrentCUDAStream', ('setCurrentHIPStreamMasqueradingAsCUDA', API_PYTORCH)), ('c10/cuda/CUDAGuard.h', ('ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h', API_PYTORCH)), ('c10/cuda/CUDACachingAllocator.h', ('ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h', API_PYTORCH)), ('c10/cuda/CUDAStream.h', ('ATen/hip/impl/HIPStreamMasqueradingAsCUDA.h', API_PYTORCH)), ('gloo/cuda.h', ('gloo/hip.h', API_PYTORCH)), ('gloo/cuda_allreduce_halving_doubling.h', ('gloo/hip_allreduce_halving_doubling.h', API_PYTORCH)), ('gloo/cuda_allreduce_halving_doubling_pipelined.h', ('gloo/hip_allreduce_halving_doubling_pipelined.h', API_PYTORCH)), ('gloo/cuda_allreduce_ring.h', ('gloo/hip_allreduce_ring.h', API_PYTORCH)), ('gloo/cuda_broadcast_one_to_all.h', ('gloo/hip_broadcast_one_to_all.h', API_PYTORCH)), ('gloo::CudaAllreduceHalvingDoublingPipelined', ('gloo::HipAllreduceHalvingDoublingPipelined', API_PYTORCH)), ('gloo::CudaBroadcastOneToAll', ('gloo::HipBroadcastOneToAll', API_PYTORCH)), ('gloo::CudaHostWorkspace', ('gloo::HipHostWorkspace', API_PYTORCH)), ('gloo::CudaDeviceWorkspace', ('gloo::HipDeviceWorkspace', API_PYTORCH)), ('CUDNN_RNN_RELU', ('miopenRNNRELU', API_PYTORCH)), ('CUDNN_RNN_TANH', ('miopenRNNTANH', API_PYTORCH)), ('CUDNN_LSTM', ('miopenLSTM', API_PYTORCH)), ('CUDNN_GRU', ('miopenGRU', API_PYTORCH)), ('cudnnRNNMode_t', ('miopenRNNMode_t', API_PYTORCH)), ('magma_queue_create_from_cuda', ('magma_queue_create_from_hip', API_PYTORCH))])
A:torch.utils.hipify.cuda_to_hip_mappings.CAFFE2_SPECIFIC_MAPPINGS->collections.OrderedDict([('cuda_stream', ('hip_stream', API_CAFFE2)), ('/hip/', ('/hip/', API_CAFFE2)), ('/context_gpu', ('/hip/context_gpu', API_CAFFE2)), ('/common_gpu', ('/hip/common_gpu', API_CAFFE2)), ('/cuda_nccl_gpu', ('/hip/hip_nccl_gpu', API_CAFFE2)), ('/mixed_utils', ('/hip/mixed_utils', API_CAFFE2)), ('/operator_fallback_gpu', ('/hip/operator_fallback_gpu', API_CAFFE2)), ('/spatial_batch_norm_op_impl', ('/hip/spatial_batch_norm_op_impl', API_CAFFE2)), ('/recurrent_network_executor_gpu', ('/hip/recurrent_network_executor_gpu', API_CAFFE2)), ('/generate_proposals_op_util_nms_gpu', ('/hip/generate_proposals_op_util_nms_gpu', API_CAFFE2)), ('/max_pool_with_index_gpu', ('/hip/max_pool_with_index_gpu', API_CAFFE2)), ('/THCCachingAllocator_gpu', ('/hip/THCCachingAllocator_gpu', API_CAFFE2)), ('/top_k_heap_selection', ('/hip/top_k_heap_selection', API_CAFFE2)), ('/top_k_radix_selection', ('/hip/top_k_radix_selection', API_CAFFE2)), ('/GpuAtomics', ('/hip/GpuAtomics', API_CAFFE2)), ('/GpuDefs', ('/hip/GpuDefs', API_CAFFE2)), ('/GpuScanUtils', ('/hip/GpuScanUtils', API_CAFFE2)), ('/GpuBitonicSort', ('/hip/GpuBitonicSort', API_CAFFE2)), ('/math/reduce.cuh', ('/math/hip/reduce.cuh', API_CAFFE2)), ('/sgd/adagrad_fused_op_gpu.cuh', ('/sgd/hip/adagrad_fused_op_gpu.cuh', API_CAFFE2)), ('/operators/segment_reduction_op_gpu.cuh', ('/operators/hip/segment_reduction_op_gpu.cuh', API_CAFFE2)), ('/gather_op.cuh', ('/hip/gather_op.cuh', API_CAFFE2)), ('caffe2/core/common_cudnn.h', ('caffe2/core/hip/common_miopen.h', API_CAFFE2)), ('REGISTER_CUDA_OPERATOR', ('REGISTER_HIP_OPERATOR', API_CAFFE2)), ('CUDA_1D_KERNEL_LOOP', ('HIP_1D_KERNEL_LOOP', API_CAFFE2)), ('CUDAContext', ('HIPContext', API_CAFFE2)), ('CAFFE_CUDA_NUM_THREADS', ('CAFFE_HIP_NUM_THREADS', API_CAFFE2)), ('HasCudaGPU', ('HasHipGPU', API_CAFFE2)), ('__expf', ('expf', API_CAFFE2)), ('CUBLAS_ENFORCE', ('ROCBLAS_ENFORCE', API_CAFFE2)), ('CUBLAS_CHECK', ('ROCBLAS_CHECK', API_CAFFE2)), ('cublas_handle', ('rocblashandle', API_CAFFE2)), ('CURAND_ENFORCE', ('HIPRAND_ENFORCE', API_CAFFE2)), ('CURAND_CHECK', ('HIPRAND_CHECK', API_CAFFE2)), ('curandGenerateUniform', ('hiprandGenerateUniform', API_CAFFE2)), ('curand_generator', ('hiprand_generator', API_CAFFE2)), ('CaffeCudaGetDevice', ('CaffeHipGetDevice', API_CAFFE2)), ('CUDA_KERNEL_ASSERT', ('CUDA_KERNEL_ASSERT', API_CAFFE2)), ('lazyInitCUDA', ('lazyInitCUDA', API_CAFFE2)), ('CUDA_VERSION', ('TORCH_HIP_VERSION', API_CAFFE2)), ('CUDA', ('HIP', API_CAFFE2)), ('Cuda', ('Hip', API_CAFFE2)), ('cuda_', ('hip_', API_CAFFE2)), ('_cuda', ('_hip', API_CAFFE2)), ('CUDNN', ('MIOPEN', API_CAFFE2)), ('CuDNN', ('MIOPEN', API_CAFFE2)), ('cudnn', ('miopen', API_CAFFE2)), ('namespace cuda', ('namespace hip', API_CAFFE2)), ('cuda::CUDAGuard', ('hip::HIPGuard', API_CAFFE2)), ('cuda::OptionalCUDAGuard', ('hip::OptionalHIPGuard', API_CAFFE2)), ('cuda::CUDAStreamGuard', ('hip::HIPStreamGuard', API_CAFFE2)), ('cuda::OptionalCUDAStreamGuard', ('hip::OptionalHIPStreamGuard', API_CAFFE2)), ('c10/cuda/CUDAGuard.h', ('c10/hip/HIPGuard.h', API_CAFFE2)), ('gloo/cuda', ('gloo/hip', API_CAFFE2))])
A:torch.utils.hipify.cuda_to_hip_mappings.C10_MAPPINGS->collections.OrderedDict([('cuda::compat::', ('hip::compat::', API_C10)), ('c10/cuda/CUDAException.h', ('c10/hip/HIPException.h', API_C10)), ('c10/cuda/CUDAMacros.h', ('c10/hip/HIPMacros.h', API_C10)), ('c10/cuda/CUDAMathCompat.h', ('c10/hip/HIPMathCompat.h', API_C10)), ('c10/cuda/CUDAFunctions.h', ('c10/hip/HIPFunctions.h', API_C10)), ('c10/cuda/CUDAMiscFunctions.h', ('c10/hip/HIPMiscFunctions.h', API_C10)), ('c10/cuda/CUDAStream.h', ('c10/hip/HIPStream.h', API_C10)), ('c10/cuda/CUDAGraphsC10Utils.h', ('c10/hip/HIPGraphsC10Utils.h', API_C10)), ('c10/cuda/CUDACachingAllocator.h', ('c10/hip/HIPCachingAllocator.h', API_C10)), ('c10/cuda/impl/CUDATest.h', ('c10/hip/impl/HIPTest.h', API_C10)), ('c10/cuda/impl/CUDAGuardImpl.h', ('c10/hip/impl/HIPGuardImpl.h', API_C10)), ('c10/cuda/impl/cuda_cmake_macros.h', ('c10/hip/impl/hip_cmake_macros.h', API_C10)), ('C10_CUDA_CHECK', ('C10_HIP_CHECK', API_C10)), ('C10_CUDA_CHECK_WARN', ('C10_HIP_CHECK_WARN', API_C10)), ('c10::cuda', ('c10::hip', API_C10)), ('cuda::CUDAStream', ('hip::HIPStream', API_C10)), ('CUDAStream', ('HIPStream', API_C10)), ('cuda::current_device', ('hip::current_device', API_C10)), ('cuda::set_device', ('hip::set_device', API_C10)), ('cuda::device_synchronize', ('hip::device_synchronize', API_C10)), ('cuda::getStreamFromPool', ('hip::getStreamFromPool', API_C10)), ('getStreamFromPool', ('getStreamFromPool', API_C10)), ('cuda::getDefaultCUDAStream', ('hip::getDefaultHIPStream', API_C10)), ('getDefaultCUDAStream', ('getDefaultHIPStream', API_C10)), ('cuda::getCurrentCUDAStream', ('hip::getCurrentHIPStream', API_C10)), ('getCurrentCUDAStream', ('getCurrentHIPStream', API_C10)), ('cuda::get_cuda_check_prefix', ('hip::get_cuda_check_prefix', API_C10)), ('cuda::setCurrentCUDAStream', ('hip::setCurrentHIPStream', API_C10)), ('setCurrentCUDAStream', ('setCurrentHIPStream', API_C10)), ('cuda::CUDACachingAllocator', ('hip::HIPCachingAllocator', API_C10)), ('CUDACachingAllocator', ('HIPCachingAllocator', API_C10)), ('C10_CUDA_KERNEL_LAUNCH_CHECK', ('C10_HIP_KERNEL_LAUNCH_CHECK', API_C10))])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/hipify/version.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/hipify/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/hipify/hipify_python.py----------------------------------------
A:torch.utils.hipify.hipify_python.self.files_to_clean->set()
A:torch.utils.hipify.hipify_python.(parent, n)->os.path.split(parent)
A:torch.utils.hipify.hipify_python.exact_matches->set(includes)
A:torch.utils.hipify.hipify_python.rel_dirpath->os.path.relpath(abs_dirpath, root_path)
A:torch.utils.hipify.hipify_python.filepath->os.path.join(rel_dirpath, filename)
A:torch.utils.hipify.hipify_python.result->preprocessor(output_directory, filepath, all_files, includes, stats, hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)
A:torch.utils.hipify.hipify_python.fin_path->os.path.abspath(os.path.join(output_directory, filepath))
A:torch.utils.hipify.hipify_python.clean_ctx->GeneratedFileCleaner(keep_intermediates=True)
A:torch.utils.hipify.hipify_python.kernel_string->kernel_string.replace('<<<', '').replace('>>>', '').replace('<<<', '').replace('>>>', '')
A:torch.utils.hipify.hipify_python.first_arg_clean->kernel_string[arg_locs[0]['start']:arg_locs[0]['end']].replace('\n', '').strip(' ')
A:torch.utils.hipify.hipify_python.second_arg_clean->kernel_string[arg_locs[1]['start']:arg_locs[1]['end']].replace('\n', '').strip(' ')
A:torch.utils.hipify.hipify_python.first_arg_dim3->'dim3({})'.format(first_arg_clean)
A:torch.utils.hipify.hipify_python.second_arg_dim3->'dim3({})'.format(second_arg_clean)
A:torch.utils.hipify.hipify_python.first_arg_raw_dim3->first_arg_raw.replace(first_arg_clean, first_arg_dim3)
A:torch.utils.hipify.hipify_python.second_arg_raw_dim3->second_arg_raw.replace(second_arg_clean, second_arg_dim3)
A:torch.utils.hipify.hipify_python.cuda_kernel->cuda_kernel.replace(first_arg_raw + second_arg_raw, first_arg_raw_dim3 + second_arg_raw_dim3).replace(first_arg_raw + second_arg_raw, first_arg_raw_dim3 + second_arg_raw_dim3)
A:torch.utils.hipify.hipify_python.RE_KERNEL_LAUNCH->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+')
A:torch.utils.hipify.hipify_python.string->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+').sub(lambda inp: '{0}{1}::'.format(inp.group(1), inp.group(2)), string)
A:torch.utils.hipify.hipify_python.kernel_start->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+').sub(lambda inp: '{0}{1}::'.format(inp.group(1), inp.group(2)), string).find('<<<', kernel_end)
A:torch.utils.hipify.hipify_python.get_kernel_positions->list(find_kernel_bounds(mask_comments(string)))
A:torch.utils.hipify.hipify_python.params->grab_method_and_template(kernel)
A:torch.utils.hipify.hipify_python.parenthesis->re.compile('([ ]+)(detail?)::[ ]+\\\\\\n[ ]+').sub(lambda inp: '{0}{1}::'.format(inp.group(1), inp.group(2)), string).find('(', kernel['end'])
A:torch.utils.hipify.hipify_python.cuda_kernel_dim3->add_dim3(kernel_string, cuda_kernel)
A:torch.utils.hipify.hipify_python.num_klp->len(extract_arguments(0, kernel['group'].replace('<<<', '(').replace('>>>', ')')))
A:torch.utils.hipify.hipify_python.output_string->re.compile('extern\\s+([\\w\\(\\)]+)?\\s*__shared__\\s+([\\w:<>\\s]+)\\s+(\\w+)\\s*\\[\\s*\\]\\s*;').sub(lambda inp: 'HIP_DYNAMIC_SHARED({0} {1}, {2})'.format(inp.group(1) or '', inp.group(2), inp.group(3)), output_string)
A:torch.utils.hipify.hipify_python.RE_ASSERT->re.compile('\\bassert[ ]*\\(')
A:torch.utils.hipify.hipify_python.RE_SYNCTHREADS->re.compile(':?:?\\b(__syncthreads)\\b(\\w*\\()')
A:torch.utils.hipify.hipify_python.RE_EXTERN_SHARED->re.compile('extern\\s+([\\w\\(\\)]+)?\\s*__shared__\\s+([\\w:<>\\s]+)\\s+(\\w+)\\s*\\[\\s*\\]\\s*;')
A:torch.utils.hipify.hipify_python.(dirpath, filename)->os.path.split(f)
A:torch.utils.hipify.hipify_python.(root, ext)->os.path.splitext(filename)
A:torch.utils.hipify.hipify_python.dirpath->os.path.join(dirpath, 'hip')
A:torch.utils.hipify.hipify_python.root->root.replace('THC', 'THH').replace('THC', 'THH')
A:torch.utils.hipify.hipify_python.filename->os.path.basename(filepath)
A:torch.utils.hipify.hipify_python.(_, ext)->os.path.splitext(filename)
A:torch.utils.hipify.hipify_python.recurse->self._pattern(data[char])
A:torch.utils.hipify.hipify_python.CAFFE2_TRIE->Trie()
A:torch.utils.hipify.hipify_python.PYTORCH_TRIE->Trie()
A:torch.utils.hipify.hipify_python.RE_CAFFE2_PREPROCESSOR->re.compile(CAFFE2_TRIE.pattern())
A:torch.utils.hipify.hipify_python.RE_PYTORCH_PREPROCESSOR->re.compile('(?<=\\W)({0})(?=\\W)'.format(PYTORCH_TRIE.pattern()))
A:torch.utils.hipify.hipify_python.RE_QUOTE_HEADER->re.compile('#include "([^"]+)"')
A:torch.utils.hipify.hipify_python.RE_ANGLE_HEADER->re.compile('#include <([^>]+)>')
A:torch.utils.hipify.hipify_python.RE_THC_GENERIC_FILE->re.compile('#define THC_GENERIC_FILE "([^"]+)"')
A:torch.utils.hipify.hipify_python.RE_CU_SUFFIX->re.compile('\\.cu\\b')
A:torch.utils.hipify.hipify_python.output_source->replace_extern_shared(output_source)
A:torch.utils.hipify.hipify_python.fout_path->os.path.abspath(os.path.join(output_directory, get_hip_file_path(filepath, is_pytorch_extension)))
A:torch.utils.hipify.hipify_python.f->m.group(1)
A:torch.utils.hipify.hipify_python.header_dir_to_check->os.path.join(output_directory, os.path.dirname(include))
A:torch.utils.hipify.hipify_python.header_path_to_check->os.path.abspath(os.path.join(header_dir_to_check, f))
A:torch.utils.hipify.hipify_python.contents->m.group(1).read()
A:torch.utils.hipify.hipify_python.header->'"{0}"'.format(header)
A:torch.utils.hipify.hipify_python.in_txt->in_txt.replace(' __global__ static', '__global__').replace(' __global__ static', '__global__')
A:torch.utils.hipify.hipify_python.RE_INCLUDE->re.compile('#include .*\\n')
A:torch.utils.hipify.hipify_python.project_directory->os.getcwd()
A:torch.utils.hipify.hipify_python.all_files->list(matched_files_iter(output_directory, includes=includes, ignores=ignores, extensions=extensions, out_of_place_only=out_of_place_only, is_pytorch_extension=is_pytorch_extension))
A:torch.utils.hipify.hipify_python.all_files_set->set(all_files)
A:torch.utils.hipify.hipify_python.f_rel->os.path.relpath(f, output_directory)
torch.utils.hipify.hipify_python.GeneratedFileCleaner(self,keep_intermediates=False)
torch.utils.hipify.hipify_python.GeneratedFileCleaner.__enter__(self)
torch.utils.hipify.hipify_python.GeneratedFileCleaner.__exit__(self,type,value,traceback)
torch.utils.hipify.hipify_python.GeneratedFileCleaner.__init__(self,keep_intermediates=False)
torch.utils.hipify.hipify_python.GeneratedFileCleaner.makedirs(self,dn,exist_ok=False)
torch.utils.hipify.hipify_python.GeneratedFileCleaner.open(self,fn,*args,**kwargs)
torch.utils.hipify.hipify_python.InputError(self,message)
torch.utils.hipify.hipify_python.InputError.__init__(self,message)
torch.utils.hipify.hipify_python.InputError.__str__(self)
torch.utils.hipify.hipify_python.Trie(self)
torch.utils.hipify.hipify_python.Trie.__init__(self)
torch.utils.hipify.hipify_python.Trie._pattern(self,pData)
torch.utils.hipify.hipify_python.Trie.add(self,word)
torch.utils.hipify.hipify_python.Trie.dump(self)
torch.utils.hipify.hipify_python.Trie.pattern(self)
torch.utils.hipify.hipify_python.Trie.quote(self,char)
torch.utils.hipify.hipify_python.add_dim3(kernel_string,cuda_kernel)
torch.utils.hipify.hipify_python.bcolors
torch.utils.hipify.hipify_python.compute_stats(stats)
torch.utils.hipify.hipify_python.extract_arguments(start,string)
torch.utils.hipify.hipify_python.file_add_header(filepath,header)
torch.utils.hipify.hipify_python.file_specific_replacement(filepath,search_string,replace_string,strict=False)
torch.utils.hipify.hipify_python.find_bracket_group(input_string,start)
torch.utils.hipify.hipify_python.find_closure_group(input_string,start,group)
torch.utils.hipify.hipify_python.find_parentheses_group(input_string,start)
torch.utils.hipify.hipify_python.fix_static_global_kernels(in_txt)
torch.utils.hipify.hipify_python.get_hip_file_path(filepath,is_pytorch_extension=False)
torch.utils.hipify.hipify_python.hip_header_magic(input_string)
torch.utils.hipify.hipify_python.hipify(project_directory:str,show_detailed:bool=False,extensions:Iterable=('.cu','.cuh','.c','.cc','.cpp','.h','.in','.hpp'),output_directory:str='',includes:Iterable=(),extra_files:Iterable=(),out_of_place_only:bool=False,ignores:Iterable=(),show_progress:bool=True,hip_clang_launch:bool=False,is_pytorch_extension:bool=False,clean_ctx:Optional[GeneratedFileCleaner]=None)->HipifyFinalResult
torch.utils.hipify.hipify_python.is_caffe2_gpu_file(filepath)
torch.utils.hipify.hipify_python.is_cusparse_file(filepath)
torch.utils.hipify.hipify_python.is_out_of_place(filepath)
torch.utils.hipify.hipify_python.is_pytorch_file(filepath)
torch.utils.hipify.hipify_python.match_extensions(filename:str,extensions:Iterable)->bool
torch.utils.hipify.hipify_python.matched_files_iter(root_path:str,includes:Iterable=('*',),ignores:Iterable=(),extensions:Iterable=(),out_of_place_only:bool=False,is_pytorch_extension:bool=False)->Iterator[str]
torch.utils.hipify.hipify_python.openf(filename,mode)
torch.utils.hipify.hipify_python.preprocess(output_directory:str,all_files:Iterable,includes:Iterable,show_detailed:bool=False,show_progress:bool=True,hip_clang_launch:bool=False,is_pytorch_extension:bool=False,clean_ctx:Optional[GeneratedFileCleaner]=None)->HipifyFinalResult
torch.utils.hipify.hipify_python.preprocess_file_and_save_result(output_directory:str,filepath:str,all_files:Iterable,includes:Iterable,stats:Dict[str,List],hip_clang_launch:bool,is_pytorch_extension:bool,clean_ctx:GeneratedFileCleaner,show_progress:bool)->None
torch.utils.hipify.hipify_python.preprocessor(output_directory:str,filepath:str,all_files:Iterable,includes:Iterable,stats:Dict[str,List],hip_clang_launch:bool,is_pytorch_extension:bool,clean_ctx:GeneratedFileCleaner,show_progress:bool)->HipifyResult
torch.utils.hipify.hipify_python.processKernelLaunches(string,stats)
torch.utils.hipify.hipify_python.replace_extern_shared(input_string)
torch.utils.hipify.hipify_python.replace_math_functions(input_string)
torch.utils.hipify.hipify_python.str2bool(v)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/hipify/constants.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/model_dump/__main__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/model_dump/__init__.py----------------------------------------
A:torch.utils.model_dump.__init__.storage_info->get_storage_info(storage)
A:torch.utils.model_dump.__init__.quantizer_extra->list(quantizer[1:3])
A:torch.utils.model_dump.__init__.default_title->os.fspath(path_or_file)
A:torch.utils.model_dump.__init__.file_size->path_or_file.tell()
A:torch.utils.model_dump.__init__.prefix->re.sub('/.*', '', zi.filename)
A:torch.utils.model_dump.__init__.version->zf.read(path_prefix + '/version').decode('utf-8').strip()
A:torch.utils.model_dump.__init__.raw->torch.utils.show_pickle.DumpUnpickler(handle, catch_invalid_utf8=True).load()
A:torch.utils.model_dump.__init__.model_data->get_pickle('data')
A:torch.utils.model_dump.__init__.constants->get_pickle('constants')
A:torch.utils.model_dump.__init__.interned_strings[s]->len(interned_strings)
A:torch.utils.model_dump.__init__.raw_code->handle.read()
A:torch.utils.model_dump.__init__.raw_debug->handle.read()
A:torch.utils.model_dump.__init__.debug_info_t->pickle.loads(raw_debug)
A:torch.utils.model_dump.__init__.debug_info->list(debug_info_t)
A:torch.utils.model_dump.__init__.extra_files_json_pattern->re.compile(re.escape(path_prefix) + '/extra/.*\\.json')
A:torch.utils.model_dump.__init__.json_content->json.load(handle)
A:torch.utils.model_dump.__init__.obj->torch.utils.show_pickle.DumpUnpickler(handle, catch_invalid_utf8=True).load()
A:torch.utils.model_dump.__init__.buf->io.StringIO()
A:torch.utils.model_dump.__init__.contents->io.StringIO().getvalue()
A:torch.utils.model_dump.__init__.skeleton->get_inline_skeleton()
A:torch.utils.model_dump.__init__.js_code->js_code.replace(f'https://unpkg.com/{js_module}?module', js_url).replace(f'https://unpkg.com/{js_module}?module', js_url)
A:torch.utils.model_dump.__init__.js_lib->importlib.resources.read_binary(__package__, f'{js_module}.mjs')
A:torch.utils.model_dump.__init__.model_info->get_model_info(path_or_bytesio, **kwargs)
A:torch.utils.model_dump.__init__.page->burn_in_info(skeleton, info)
A:torch.utils.model_dump.__init__.parser->argparse.ArgumentParser()
A:torch.utils.model_dump.__init__.args->argparse.ArgumentParser().parse_args(argv[1:])
A:torch.utils.model_dump.__init__.info->get_model_info(args.model, title=args.title)
torch.utils.model_dump.__init__.burn_in_info(skeleton,info)
torch.utils.model_dump.__init__.get_info_and_burn_skeleton(path_or_bytesio,**kwargs)
torch.utils.model_dump.__init__.get_inline_skeleton()
torch.utils.model_dump.__init__.get_model_info(path_or_file,title=None,extra_file_size_limit=DEFAULT_EXTRA_FILE_SIZE_LIMIT)
torch.utils.model_dump.__init__.get_storage_info(storage)
torch.utils.model_dump.__init__.hierarchical_pickle(data)
torch.utils.model_dump.__init__.main(argv,*,stdout=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/ffi/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/_stubs.py----------------------------------------
torch.utils.benchmark.utils._stubs.CallgrindModuleType(Protocol)
torch.utils.benchmark.utils._stubs.CallgrindModuleType._valgrind_supported_platform(self)->bool
torch.utils.benchmark.utils._stubs.CallgrindModuleType._valgrind_toggle(self)->None
torch.utils.benchmark.utils._stubs.TimeitModuleType(Protocol)
torch.utils.benchmark.utils._stubs.TimeitModuleType.timeit(self,number:int)->float
torch.utils.benchmark.utils._stubs.TimerClass(self,stmt:str,setup:str,timer:Callable[[],float],globals:Dict[str,Any],**kwargs:Any)
torch.utils.benchmark.utils._stubs.TimerClass.__init__(self,stmt:str,setup:str,timer:Callable[[],float],globals:Dict[str,Any],**kwargs:Any)
torch.utils.benchmark.utils._stubs.TimerClass.timeit(self,number:int)->float


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/timer.py----------------------------------------
A:torch.utils.benchmark.utils.timer.self._timeit_module->torch.utils.benchmark.utils.cpp_jit.compile_timeit_template(stmt=self._stmt, setup=self._setup, global_setup=self._global_setup)
A:torch.utils.benchmark.utils.timer.self._globals->dict(globals or {})
A:torch.utils.benchmark.utils.timer.stmt->(stmt[1:] if stmt and stmt[0] == '\n' else stmt).rstrip()
A:torch.utils.benchmark.utils.timer.setup->(setup[1:] if setup and setup[0] == '\n' else setup).rstrip()
A:torch.utils.benchmark.utils.timer.self._timer->self._timer_cls(stmt=stmt, setup=setup, timer=timer, globals=valgrind_timer_interface.CopyIfCallgrind.unwrap_all(self._globals), **timer_kwargs)
A:torch.utils.benchmark.utils.timer.self._task_spec->torch.utils.benchmark.utils.common.TaskSpec(stmt=stmt, setup=setup, global_setup=global_setup, label=label, sub_label=sub_label, description=description, env=env, num_threads=num_threads)
A:torch.utils.benchmark.utils.timer.time_spent->time_hook()
A:torch.utils.benchmark.utils.timer.can_stop->stop_hook(times)
A:torch.utils.benchmark.utils.timer.overhead->torch.tensor([self._timeit(0) for _ in range(5)]).median().item()
A:torch.utils.benchmark.utils.timer.time_taken->self._timeit(number)
A:torch.utils.benchmark.utils.timer.number->self._estimate_block_size(min_run_time)
A:torch.utils.benchmark.utils.timer.times->self._threaded_measurement_loop(number, time_hook, stop_hook, min_run_time=min_run_time, callback=callback)
A:torch.utils.benchmark.utils.timer.result->torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.wrapper_singleton().collect_callgrind(task_spec=self._task_spec, globals=self._globals, number=number, repeats=repeats or 1, collect_baseline=collect_baseline and is_python, is_python=is_python, retain_out_file=retain_out_file)
torch.utils.benchmark.CPPTimer(self,stmt:str,setup:str,global_setup:str,timer:Callable[[],float],globals:Dict[str,Any])
torch.utils.benchmark.CPPTimer.timeit(self,number:int)->float
torch.utils.benchmark.Language(enum.Enum)
torch.utils.benchmark.Timer(self,stmt:str='pass',setup:str='pass',global_setup:str='',timer:Callable[[],float]=timer,globals:Optional[Dict[str,Any]]=None,label:Optional[str]=None,sub_label:Optional[str]=None,description:Optional[str]=None,env:Optional[str]=None,num_threads:int=1,language:Union[Language,str]=Language.PYTHON)
torch.utils.benchmark.Timer._estimate_block_size(self,min_run_time:float)->int
torch.utils.benchmark.Timer._threaded_measurement_loop(self,number:int,time_hook:Callable[[],float],stop_hook:Callable[[List[float]],bool],min_run_time:float,max_run_time:Optional[float]=None,callback:Optional[Callable[[int,float],NoReturn]]=None)->List[float]
torch.utils.benchmark.Timer._timeit(self,number:int)->float
torch.utils.benchmark.Timer.adaptive_autorange(self,threshold:float=0.1,*,min_run_time:float=0.01,max_run_time:float=10.0,callback:Optional[Callable[[int,float],NoReturn]]=None)->common.Measurement
torch.utils.benchmark.Timer.autorange(self,callback:Optional[Callable[[int,float],NoReturn]]=None)->None
torch.utils.benchmark.Timer.blocked_autorange(self,callback:Optional[Callable[[int,float],NoReturn]]=None,min_run_time:float=0.2)->common.Measurement
torch.utils.benchmark.Timer.collect_callgrind(self,number:int=100,*,repeats:Optional[int]=None,collect_baseline:bool=True,retain_out_file:bool=False)->Any
torch.utils.benchmark.Timer.repeat(self,repeat:int=-1,number:int=-1)->None
torch.utils.benchmark.Timer.timeit(self,number:int=1000000)->common.Measurement
torch.utils.benchmark.utils.timer.CPPTimer(self,stmt:str,setup:str,global_setup:str,timer:Callable[[],float],globals:Dict[str,Any])
torch.utils.benchmark.utils.timer.CPPTimer.__init__(self,stmt:str,setup:str,global_setup:str,timer:Callable[[],float],globals:Dict[str,Any])
torch.utils.benchmark.utils.timer.CPPTimer.timeit(self,number:int)->float
torch.utils.benchmark.utils.timer.Language(enum.Enum)
torch.utils.benchmark.utils.timer.Timer(self,stmt:str='pass',setup:str='pass',global_setup:str='',timer:Callable[[],float]=timer,globals:Optional[Dict[str,Any]]=None,label:Optional[str]=None,sub_label:Optional[str]=None,description:Optional[str]=None,env:Optional[str]=None,num_threads:int=1,language:Union[Language,str]=Language.PYTHON)
torch.utils.benchmark.utils.timer.Timer.__init__(self,stmt:str='pass',setup:str='pass',global_setup:str='',timer:Callable[[],float]=timer,globals:Optional[Dict[str,Any]]=None,label:Optional[str]=None,sub_label:Optional[str]=None,description:Optional[str]=None,env:Optional[str]=None,num_threads:int=1,language:Union[Language,str]=Language.PYTHON)
torch.utils.benchmark.utils.timer.Timer._estimate_block_size(self,min_run_time:float)->int
torch.utils.benchmark.utils.timer.Timer._threaded_measurement_loop(self,number:int,time_hook:Callable[[],float],stop_hook:Callable[[List[float]],bool],min_run_time:float,max_run_time:Optional[float]=None,callback:Optional[Callable[[int,float],NoReturn]]=None)->List[float]
torch.utils.benchmark.utils.timer.Timer._timeit(self,number:int)->float
torch.utils.benchmark.utils.timer.Timer.adaptive_autorange(self,threshold:float=0.1,*,min_run_time:float=0.01,max_run_time:float=10.0,callback:Optional[Callable[[int,float],NoReturn]]=None)->common.Measurement
torch.utils.benchmark.utils.timer.Timer.autorange(self,callback:Optional[Callable[[int,float],NoReturn]]=None)->None
torch.utils.benchmark.utils.timer.Timer.blocked_autorange(self,callback:Optional[Callable[[int,float],NoReturn]]=None,min_run_time:float=0.2)->common.Measurement
torch.utils.benchmark.utils.timer.Timer.collect_callgrind(self,number:int=100,*,repeats:Optional[int]=None,collect_baseline:bool=True,retain_out_file:bool=False)->Any
torch.utils.benchmark.utils.timer.Timer.repeat(self,repeat:int=-1,number:int=-1)->None
torch.utils.benchmark.utils.timer.Timer.timeit(self,number:int=1000000)->common.Measurement


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/fuzzer.py----------------------------------------
A:torch.utils.benchmark.utils.fuzzer.self._distribution->self._check_distribution(distribution)
A:torch.utils.benchmark.utils.fuzzer.output->int(2 ** state.uniform(low=np.log2(self._minval) if self._minval is not None else None, high=np.log2(self._maxval) if self._maxval is not None else None))
A:torch.utils.benchmark.utils.fuzzer.index->numpy.random.RandomState(self._seed).choice(np.arange(len(self._distribution)), p=tuple(self._distribution.values()))
A:torch.utils.benchmark.utils.fuzzer.(size, steps, allocation_size)->self._get_size_and_steps(params)
A:torch.utils.benchmark.utils.fuzzer.raw_tensor->raw_tensor.permute(tuple(np.argsort(order))).permute(tuple(np.argsort(order)))
A:torch.utils.benchmark.utils.fuzzer.dim->len(size)
A:torch.utils.benchmark.utils.fuzzer.order->numpy.random.RandomState(self._seed).permutation(raw_tensor.dim())
A:torch.utils.benchmark.utils.fuzzer.values->tuple((params.get(i, i) for i in values))
A:torch.utils.benchmark.utils.fuzzer.size->resolve(self._size, dim)
A:torch.utils.benchmark.utils.fuzzer.steps->resolve(self._steps or (), dim)
A:torch.utils.benchmark.utils.fuzzer.allocation_size->tuple((size_i * step_i for (size_i, step_i) in zip(size, steps)))
A:torch.utils.benchmark.utils.fuzzer.(size, _, allocation_size)->self._get_size_and_steps(params)
A:torch.utils.benchmark.utils.fuzzer.num_elements->prod(size)
A:torch.utils.benchmark.utils.fuzzer.allocation_bytes->prod(allocation_size, base=dtype_size(self._dtype))
A:torch.utils.benchmark.utils.fuzzer.seed->numpy.random.RandomState().randint(0, 2 ** 63)
A:torch.utils.benchmark.utils.fuzzer.self._parameters->Fuzzer._unpack(parameters, FuzzedParameter)
A:torch.utils.benchmark.utils.fuzzer.self._tensors->Fuzzer._unpack(tensors, FuzzedTensor)
A:torch.utils.benchmark.utils.fuzzer.name_overlap->p_names.intersection(t_names)
A:torch.utils.benchmark.utils.fuzzer.state->numpy.random.RandomState(self._seed)
A:torch.utils.benchmark.utils.fuzzer.params->dict(params)
A:torch.utils.benchmark.utils.fuzzer.(tensor, properties)->t._make_tensor(params, state)
A:torch.utils.benchmark.utils.fuzzer.candidate_params[p.name]->p.sample(state)
A:torch.utils.benchmark.utils.fuzzer.candidate_params->self._resolve_aliases(candidate_params)
A:torch.utils.benchmark.utils.fuzzer.alias_count->sum((isinstance(v, ParameterAlias) for v in params.values()))
A:torch.utils.benchmark.utils.fuzzer.keys->list(params.keys())
A:torch.utils.benchmark.utils.fuzzer.alias_count_new->sum((isinstance(v, ParameterAlias) for v in params.values()))
torch.utils.benchmark.FuzzedParameter(self,name:str,minval:Optional[Union[int,float]]=None,maxval:Optional[Union[int,float]]=None,distribution:Optional[Union[str,Dict[Any,float]]]=None,strict:bool=False)
torch.utils.benchmark.FuzzedParameter._check_distribution(self,distribution)
torch.utils.benchmark.FuzzedParameter._custom_distribution(self,state)
torch.utils.benchmark.FuzzedParameter._loguniform(self,state)
torch.utils.benchmark.FuzzedParameter._uniform(self,state)
torch.utils.benchmark.FuzzedParameter.name(self)
torch.utils.benchmark.FuzzedParameter.sample(self,state)
torch.utils.benchmark.FuzzedTensor(self,name:str,size:Tuple[Union[str,int],...],steps:Optional[Tuple[Union[str,int],...]]=None,probability_contiguous:float=0.5,min_elements:Optional[int]=None,max_elements:Optional[int]=None,max_allocation_bytes:Optional[int]=None,dim_parameter:Optional[str]=None,roll_parameter:Optional[str]=None,dtype=torch.float32,cuda=False,tensor_constructor:Optional[Callable]=None)
torch.utils.benchmark.FuzzedTensor._get_size_and_steps(self,params)
torch.utils.benchmark.FuzzedTensor._make_tensor(self,params,state)
torch.utils.benchmark.FuzzedTensor.default_tensor_constructor(size,dtype,**kwargs)
torch.utils.benchmark.FuzzedTensor.name(self)
torch.utils.benchmark.FuzzedTensor.satisfies_constraints(self,params)
torch.utils.benchmark.Fuzzer(self,parameters:List[Union[FuzzedParameter,List[FuzzedParameter]]],tensors:List[Union[FuzzedTensor,List[FuzzedTensor]]],constraints:Optional[List[Callable]]=None,seed:Optional[int]=None)
torch.utils.benchmark.Fuzzer._generate(self,state)
torch.utils.benchmark.Fuzzer._resolve_aliases(params)
torch.utils.benchmark.Fuzzer._unpack(values,cls)
torch.utils.benchmark.Fuzzer.rejection_rate(self)
torch.utils.benchmark.Fuzzer.take(self,n)
torch.utils.benchmark.ParameterAlias(self,alias_to)
torch.utils.benchmark.ParameterAlias.__repr__(self)
torch.utils.benchmark.dtype_size(dtype)
torch.utils.benchmark.prod(values,base=1)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter(self,name:str,minval:Optional[Union[int,float]]=None,maxval:Optional[Union[int,float]]=None,distribution:Optional[Union[str,Dict[Any,float]]]=None,strict:bool=False)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter.__init__(self,name:str,minval:Optional[Union[int,float]]=None,maxval:Optional[Union[int,float]]=None,distribution:Optional[Union[str,Dict[Any,float]]]=None,strict:bool=False)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter._check_distribution(self,distribution)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter._custom_distribution(self,state)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter._loguniform(self,state)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter._uniform(self,state)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter.name(self)
torch.utils.benchmark.utils.fuzzer.FuzzedParameter.sample(self,state)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor(self,name:str,size:Tuple[Union[str,int],...],steps:Optional[Tuple[Union[str,int],...]]=None,probability_contiguous:float=0.5,min_elements:Optional[int]=None,max_elements:Optional[int]=None,max_allocation_bytes:Optional[int]=None,dim_parameter:Optional[str]=None,roll_parameter:Optional[str]=None,dtype=torch.float32,cuda=False,tensor_constructor:Optional[Callable]=None)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor.__init__(self,name:str,size:Tuple[Union[str,int],...],steps:Optional[Tuple[Union[str,int],...]]=None,probability_contiguous:float=0.5,min_elements:Optional[int]=None,max_elements:Optional[int]=None,max_allocation_bytes:Optional[int]=None,dim_parameter:Optional[str]=None,roll_parameter:Optional[str]=None,dtype=torch.float32,cuda=False,tensor_constructor:Optional[Callable]=None)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor._get_size_and_steps(self,params)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor._make_tensor(self,params,state)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor.default_tensor_constructor(size,dtype,**kwargs)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor.name(self)
torch.utils.benchmark.utils.fuzzer.FuzzedTensor.satisfies_constraints(self,params)
torch.utils.benchmark.utils.fuzzer.Fuzzer(self,parameters:List[Union[FuzzedParameter,List[FuzzedParameter]]],tensors:List[Union[FuzzedTensor,List[FuzzedTensor]]],constraints:Optional[List[Callable]]=None,seed:Optional[int]=None)
torch.utils.benchmark.utils.fuzzer.Fuzzer.__init__(self,parameters:List[Union[FuzzedParameter,List[FuzzedParameter]]],tensors:List[Union[FuzzedTensor,List[FuzzedTensor]]],constraints:Optional[List[Callable]]=None,seed:Optional[int]=None)
torch.utils.benchmark.utils.fuzzer.Fuzzer._generate(self,state)
torch.utils.benchmark.utils.fuzzer.Fuzzer._resolve_aliases(params)
torch.utils.benchmark.utils.fuzzer.Fuzzer._unpack(values,cls)
torch.utils.benchmark.utils.fuzzer.Fuzzer.rejection_rate(self)
torch.utils.benchmark.utils.fuzzer.Fuzzer.take(self,n)
torch.utils.benchmark.utils.fuzzer.ParameterAlias(self,alias_to)
torch.utils.benchmark.utils.fuzzer.ParameterAlias.__init__(self,alias_to)
torch.utils.benchmark.utils.fuzzer.ParameterAlias.__repr__(self)
torch.utils.benchmark.utils.fuzzer.dtype_size(dtype)
torch.utils.benchmark.utils.fuzzer.prod(values,base=1)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/compare.py----------------------------------------
A:torch.utils.benchmark.utils.compare.self._flat_results->list(it.chain(*grouped_results))
A:torch.utils.benchmark.utils.compare.unit_digits->max((d for d in leading_digits if d is not None))
A:torch.utils.benchmark.utils.compare.value->torch.utils.benchmark.utils.common.trim_sigfig(value, estimated_sigfigs)
A:torch.utils.benchmark.utils.compare.l->list(seq)
A:torch.utils.benchmark.utils.compare.env->env.ljust(self._env_str_len + 4).ljust(self._env_str_len + 4)
A:torch.utils.benchmark.utils.compare.row_min->min((r.median for r in self._results if r is not None))
A:torch.utils.benchmark.utils.compare.col_str->self.color_segment(col_str, result.median, best_value)
A:torch.utils.benchmark.utils.compare.(self.time_unit, self.time_scale)->torch.utils.benchmark.utils.common.select_unit(min((r.median for r in results)))
A:torch.utils.benchmark.utils.compare.self.row_keys->torch.utils.benchmark.utils.common.ordered_unique([self.row_fn(i) for i in results])
A:torch.utils.benchmark.utils.compare.self.column_keys->torch.utils.benchmark.utils.common.ordered_unique([self.col_fn(i) for i in results])
A:torch.utils.benchmark.utils.compare.(self.rows, self.columns)->self.populate_rows_and_columns()
A:torch.utils.benchmark.utils.compare.row_name_str_len->max((len(r.as_row_name) for r in self.results))
A:torch.utils.benchmark.utils.compare.column->_Column(grouped_results=grouped_results, time_scale=self.time_scale, time_unit=self.time_unit, trim_significant_figures=self._trim_significant_figures, highlight_warnings=self._highlight_warnings)
A:torch.utils.benchmark.utils.compare.num_cols->max((len(i) for i in string_rows))
A:torch.utils.benchmark.utils.compare.overall_width->len(finalized_columns[0])
A:torch.utils.benchmark.utils.compare.results->torch.utils.benchmark.utils.common.Measurement.merge(self._results)
A:torch.utils.benchmark.utils.compare.grouped_results->self._group_by_label(results)
A:torch.utils.benchmark.utils.compare.table->Table(results, self._colorize, self._trim_significant_figures, self._highlight_warnings)
torch.utils.benchmark.Colorize(enum.Enum)
torch.utils.benchmark.Compare(self,results:List[common.Measurement])
torch.utils.benchmark.Compare.__str__(self)
torch.utils.benchmark.Compare._group_by_label(self,results:List[common.Measurement])
torch.utils.benchmark.Compare._layout(self,results:List[common.Measurement])
torch.utils.benchmark.Compare._render(self)
torch.utils.benchmark.Compare.colorize(self,rowwise=False)
torch.utils.benchmark.Compare.extend_results(self,results)
torch.utils.benchmark.Compare.highlight_warnings(self)
torch.utils.benchmark.Compare.print(self)
torch.utils.benchmark.Compare.trim_significant_figures(self)
torch.utils.benchmark.Table(self,results:List[common.Measurement],colorize:Colorize,trim_significant_figures:bool,highlight_warnings:bool)
torch.utils.benchmark.Table.col_fn(m:common.Measurement)->Optional[str]
torch.utils.benchmark.Table.populate_rows_and_columns(self)->Tuple[Tuple[_Row, ...], Tuple[_Column, ...]]
torch.utils.benchmark.Table.render(self)->str
torch.utils.benchmark.Table.row_fn(m:common.Measurement)->Tuple[int, Optional[str], str]
torch.utils.benchmark._Column(self,grouped_results:List[Tuple[Optional[common.Measurement],...]],time_scale:float,time_unit:str,trim_significant_figures:bool,highlight_warnings:bool)
torch.utils.benchmark._Column.get_results_for(self,group)
torch.utils.benchmark._Column.num_to_str(self,value:Optional[float],estimated_sigfigs:int,spread:Optional[float])
torch.utils.benchmark._Row(self,results,row_group,render_env,env_str_len,row_name_str_len,time_scale,colorize,num_threads=None)
torch.utils.benchmark._Row.as_column_strings(self)
torch.utils.benchmark._Row.color_segment(segment,value,best_value)
torch.utils.benchmark._Row.finalize_column_strings(self,column_strings,col_widths)
torch.utils.benchmark._Row.register_columns(self,columns:Tuple[_Column,...])
torch.utils.benchmark._Row.row_separator(self,overall_width)
torch.utils.benchmark.optional_min(seq)
torch.utils.benchmark.utils.compare.Colorize(enum.Enum)
torch.utils.benchmark.utils.compare.Compare(self,results:List[common.Measurement])
torch.utils.benchmark.utils.compare.Compare.__init__(self,results:List[common.Measurement])
torch.utils.benchmark.utils.compare.Compare.__str__(self)
torch.utils.benchmark.utils.compare.Compare._group_by_label(self,results:List[common.Measurement])
torch.utils.benchmark.utils.compare.Compare._layout(self,results:List[common.Measurement])
torch.utils.benchmark.utils.compare.Compare._render(self)
torch.utils.benchmark.utils.compare.Compare.colorize(self,rowwise=False)
torch.utils.benchmark.utils.compare.Compare.extend_results(self,results)
torch.utils.benchmark.utils.compare.Compare.highlight_warnings(self)
torch.utils.benchmark.utils.compare.Compare.print(self)
torch.utils.benchmark.utils.compare.Compare.trim_significant_figures(self)
torch.utils.benchmark.utils.compare.Table(self,results:List[common.Measurement],colorize:Colorize,trim_significant_figures:bool,highlight_warnings:bool)
torch.utils.benchmark.utils.compare.Table.__init__(self,results:List[common.Measurement],colorize:Colorize,trim_significant_figures:bool,highlight_warnings:bool)
torch.utils.benchmark.utils.compare.Table.col_fn(m:common.Measurement)->Optional[str]
torch.utils.benchmark.utils.compare.Table.populate_rows_and_columns(self)->Tuple[Tuple[_Row, ...], Tuple[_Column, ...]]
torch.utils.benchmark.utils.compare.Table.render(self)->str
torch.utils.benchmark.utils.compare.Table.row_fn(m:common.Measurement)->Tuple[int, Optional[str], str]
torch.utils.benchmark.utils.compare._Column(self,grouped_results:List[Tuple[Optional[common.Measurement],...]],time_scale:float,time_unit:str,trim_significant_figures:bool,highlight_warnings:bool)
torch.utils.benchmark.utils.compare._Column.__init__(self,grouped_results:List[Tuple[Optional[common.Measurement],...]],time_scale:float,time_unit:str,trim_significant_figures:bool,highlight_warnings:bool)
torch.utils.benchmark.utils.compare._Column.get_results_for(self,group)
torch.utils.benchmark.utils.compare._Column.num_to_str(self,value:Optional[float],estimated_sigfigs:int,spread:Optional[float])
torch.utils.benchmark.utils.compare._Row(self,results,row_group,render_env,env_str_len,row_name_str_len,time_scale,colorize,num_threads=None)
torch.utils.benchmark.utils.compare._Row.__init__(self,results,row_group,render_env,env_str_len,row_name_str_len,time_scale,colorize,num_threads=None)
torch.utils.benchmark.utils.compare._Row.as_column_strings(self)
torch.utils.benchmark.utils.compare._Row.color_segment(segment,value,best_value)
torch.utils.benchmark.utils.compare._Row.finalize_column_strings(self,column_strings,col_widths)
torch.utils.benchmark.utils.compare._Row.register_columns(self,columns:Tuple[_Column,...])
torch.utils.benchmark.utils.compare._Row.row_separator(self,overall_width)
torch.utils.benchmark.utils.compare.optional_min(seq)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/common.py----------------------------------------
A:torch.utils.benchmark.utils.common._TASKSPEC_FIELDS->tuple((i.name for i in dataclasses.fields(TaskSpec)))
A:torch.utils.benchmark.utils.common.n_total->len(self._sorted_times)
A:torch.utils.benchmark.utils.common.lower_bound->int(n_total // 4)
A:torch.utils.benchmark.utils.common.upper_bound->int(torch.tensor(3 * n_total / 4).ceil())
A:torch.utils.benchmark.utils.common.std->torch.tensor(interquartile_points).std(unbiased=False).item()
A:torch.utils.benchmark.utils.common.sqrt_n->torch.tensor(len(interquartile_points)).sqrt().item()
A:torch.utils.benchmark.utils.common.confidence_interval->max(1.645 * std / sqrt_n, _MIN_CONFIDENCE_INTERVAL)
A:torch.utils.benchmark.utils.common.relative_ci->torch.tensor(self._median / confidence_interval).log10().item()
A:torch.utils.benchmark.utils.common.num_significant_figures->int(torch.tensor(relative_ci).floor())
A:torch.utils.benchmark.utils.common.self._sorted_times->tuple(sorted(self.times))
A:torch.utils.benchmark.utils.common._sorted_times->torch.tensor(self._sorted_times, dtype=torch.float64)
A:torch.utils.benchmark.utils.common.self._median->torch.tensor(self._sorted_times, dtype=torch.float64).quantile(0.5).item()
A:torch.utils.benchmark.utils.common.self._mean->torch.tensor(self._sorted_times, dtype=torch.float64).mean().item()
A:torch.utils.benchmark.utils.common.self._p25->torch.tensor(self._sorted_times, dtype=torch.float64).quantile(0.25).item()
A:torch.utils.benchmark.utils.common.self._p75->torch.tensor(self._sorted_times, dtype=torch.float64).quantile(0.75).item()
A:torch.utils.benchmark.utils.common.n->len(self._sorted_times)
A:torch.utils.benchmark.utils.common.(time_unit, time_scale)->select_unit(self._median)
A:torch.utils.benchmark.utils.common.repr_str->f"\n{super().__repr__()}\n{self.task_spec.summarize()}\n  {('Median: ' if n > 1 else '')}{self._median / time_scale:.2f} {time_unit}\n  {iqr_filter}IQR:    {self.iqr / time_scale:.2f} {time_unit} ({self._p25 / time_scale:.2f} to {self._p75 / time_scale:.2f})\n  {n} measurement{('s' if n > 1 else '')}, {self.number_per_run} runs {('per measurement,' if n > 1 else ',')} {self.num_threads} thread{('s' if self.num_threads > 1 else '')}\n{newline.join(self._warnings)}".strip()
A:torch.utils.benchmark.utils.common.time_unit->{-3: 'ns', -2: 'us', -1: 'ms'}.get(int(torch.tensor(t).log10().item() // 3), 's')
A:torch.utils.benchmark.utils.common.magnitude->int(torch.tensor(x).abs().log10().ceil().item())
A:torch.utils.benchmark.utils.common.prior_num_threads->torch.get_num_threads()
A:torch.utils.benchmark.utils.common.owner_file->os.path.join(root, i, 'owner.pid')
A:torch.utils.benchmark.utils.common.owner_pid->int(f.read())
A:torch.utils.benchmark.utils.common.root->tempfile.gettempdir()
A:torch.utils.benchmark.utils.common.path->os.path.join(root, name)
torch.utils.benchmark.Measurement
torch.utils.benchmark.Measurement.__getattr__(self,name:str)->Any
torch.utils.benchmark.Measurement.__post_init__(self)->None
torch.utils.benchmark.Measurement.__repr__(self)->str
torch.utils.benchmark.Measurement._lazy_init(self)->None
torch.utils.benchmark.Measurement.as_row_name(self)->str
torch.utils.benchmark.Measurement.env(self)->str
torch.utils.benchmark.Measurement.has_warnings(self)->bool
torch.utils.benchmark.Measurement.iqr(self)->float
torch.utils.benchmark.Measurement.mean(self)->float
torch.utils.benchmark.Measurement.median(self)->float
torch.utils.benchmark.Measurement.meets_confidence(self,threshold:float=_IQR_WARN_THRESHOLD)->bool
torch.utils.benchmark.Measurement.merge(measurements)
torch.utils.benchmark.Measurement.significant_figures(self)->int
torch.utils.benchmark.Measurement.times(self)->List[float]
torch.utils.benchmark.Measurement.title(self)->str
torch.utils.benchmark.TaskSpec
torch.utils.benchmark.TaskSpec.setup_str(self)->str
torch.utils.benchmark.TaskSpec.summarize(self)->str
torch.utils.benchmark.TaskSpec.title(self)->str
torch.utils.benchmark._make_temp_dir(prefix:Optional[str]=None,gc_dev_shm:bool=False)->str
torch.utils.benchmark.ordered_unique(elements:Iterable[Any])->List[Any]
torch.utils.benchmark.select_unit(t:float)->Tuple[str, float]
torch.utils.benchmark.set_torch_threads(n:int)->Iterator[None]
torch.utils.benchmark.trim_sigfig(x:float,n:int)->float
torch.utils.benchmark.unit_to_english(u:str)->str
torch.utils.benchmark.utils.common.Measurement
torch.utils.benchmark.utils.common.Measurement.__getattr__(self,name:str)->Any
torch.utils.benchmark.utils.common.Measurement.__post_init__(self)->None
torch.utils.benchmark.utils.common.Measurement.__repr__(self)->str
torch.utils.benchmark.utils.common.Measurement._lazy_init(self)->None
torch.utils.benchmark.utils.common.Measurement.as_row_name(self)->str
torch.utils.benchmark.utils.common.Measurement.env(self)->str
torch.utils.benchmark.utils.common.Measurement.has_warnings(self)->bool
torch.utils.benchmark.utils.common.Measurement.iqr(self)->float
torch.utils.benchmark.utils.common.Measurement.mean(self)->float
torch.utils.benchmark.utils.common.Measurement.median(self)->float
torch.utils.benchmark.utils.common.Measurement.meets_confidence(self,threshold:float=_IQR_WARN_THRESHOLD)->bool
torch.utils.benchmark.utils.common.Measurement.merge(measurements)
torch.utils.benchmark.utils.common.Measurement.significant_figures(self)->int
torch.utils.benchmark.utils.common.Measurement.times(self)->List[float]
torch.utils.benchmark.utils.common.Measurement.title(self)->str
torch.utils.benchmark.utils.common.TaskSpec
torch.utils.benchmark.utils.common.TaskSpec.setup_str(self)->str
torch.utils.benchmark.utils.common.TaskSpec.summarize(self)->str
torch.utils.benchmark.utils.common.TaskSpec.title(self)->str
torch.utils.benchmark.utils.common._make_temp_dir(prefix:Optional[str]=None,gc_dev_shm:bool=False)->str
torch.utils.benchmark.utils.common.ordered_unique(elements:Iterable[Any])->List[Any]
torch.utils.benchmark.utils.common.select_unit(t:float)->Tuple[str, float]
torch.utils.benchmark.utils.common.set_torch_threads(n:int)->Iterator[None]
torch.utils.benchmark.utils.common.trim_sigfig(x:float,n:int)->float
torch.utils.benchmark.utils.common.unit_to_english(u:str)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/sparse_fuzzer.py----------------------------------------
A:torch.utils.benchmark.utils.sparse_fuzzer.v->torch.cat([v, torch.randn_like(v)], 0)
A:torch.utils.benchmark.utils.sparse_fuzzer.i->torch.cat([i, i], 1)
A:torch.utils.benchmark.utils.sparse_fuzzer.x->x.coalesce().coalesce()
A:torch.utils.benchmark.utils.sparse_fuzzer.(size, _, _)->self._get_size_and_steps(params)
A:torch.utils.benchmark.utils.sparse_fuzzer.nnz->math.ceil(sum(size) * density)
A:torch.utils.benchmark.utils.sparse_fuzzer.tensor->tensor.cuda().cuda()
A:torch.utils.benchmark.utils.sparse_fuzzer.sparse_dim->tensor.cuda().cuda().sparse_dim()
A:torch.utils.benchmark.utils.sparse_fuzzer.dense_dim->tensor.cuda().cuda().dense_dim()
torch.utils.benchmark.FuzzedSparseTensor(self,name:str,size:Tuple[Union[str,int],...],min_elements:Optional[int]=None,max_elements:Optional[int]=None,dim_parameter:Optional[str]=None,sparse_dim:Optional[str]=None,nnz:Optional[str]=None,density:Optional[str]=None,coalesced:Optional[str]=None,dtype=torch.float32,cuda=False)
torch.utils.benchmark.FuzzedSparseTensor._make_tensor(self,params,state)
torch.utils.benchmark.FuzzedSparseTensor.sparse_tensor_constructor(size,dtype,sparse_dim,nnz,is_coalesced)
torch.utils.benchmark.utils.sparse_fuzzer.FuzzedSparseTensor(self,name:str,size:Tuple[Union[str,int],...],min_elements:Optional[int]=None,max_elements:Optional[int]=None,dim_parameter:Optional[str]=None,sparse_dim:Optional[str]=None,nnz:Optional[str]=None,density:Optional[str]=None,coalesced:Optional[str]=None,dtype=torch.float32,cuda=False)
torch.utils.benchmark.utils.sparse_fuzzer.FuzzedSparseTensor.__init__(self,name:str,size:Tuple[Union[str,int],...],min_elements:Optional[int]=None,max_elements:Optional[int]=None,dim_parameter:Optional[str]=None,sparse_dim:Optional[str]=None,nnz:Optional[str]=None,density:Optional[str]=None,coalesced:Optional[str]=None,dtype=torch.float32,cuda=False)
torch.utils.benchmark.utils.sparse_fuzzer.FuzzedSparseTensor._make_tensor(self,params,state)
torch.utils.benchmark.utils.sparse_fuzzer.FuzzedSparseTensor.sparse_tensor_constructor(size,dtype,sparse_dim,nnz,is_coalesced)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/cpp_jit.py----------------------------------------
A:torch.utils.benchmark.utils.cpp_jit.LOCK->threading.Lock()
A:torch.utils.benchmark.utils.cpp_jit._BUILD_ROOT->_make_temp_dir(prefix='benchmark_utils_jit_build')
A:torch.utils.benchmark.utils.cpp_jit.CXX_FLAGS->torch.__config__._cxx_flags().strip().split()
A:torch.utils.benchmark.utils.cpp_jit.CONDA_PREFIX->os.getenv('CONDA_PREFIX')
A:torch.utils.benchmark.utils.cpp_jit.COMPAT_CALLGRIND_BINDINGS->torch.utils.cpp_extension.load(name='callgrind_bindings', sources=[os.path.join(SOURCE_ROOT, 'valgrind_wrapper', 'compat_bindings.cpp')], extra_cflags=CXX_FLAGS, extra_include_paths=EXTRA_INCLUDE_PATHS)
A:torch.utils.benchmark.utils.cpp_jit.src->re.sub(before, textwrap.indent(after, ' ' * indentation)[indentation:], src)
A:torch.utils.benchmark.utils.cpp_jit.build_dir->os.path.join(_get_build_root(), name)
A:torch.utils.benchmark.utils.cpp_jit.src_path->os.path.join(build_dir, 'timer_src.cpp')
A:torch.utils.benchmark.utils.cpp_jit.module->_compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=False)
A:torch.utils.benchmark.utils.cpp_jit.target->_compile_template(stmt=stmt, setup=setup, global_setup=global_setup, src=src, is_standalone=True)
torch.utils.benchmark.utils.cpp_jit._compile_template(*,stmt:str,setup:str,global_setup:str,src:str,is_standalone:bool)->Any
torch.utils.benchmark.utils.cpp_jit._get_build_root()->str
torch.utils.benchmark.utils.cpp_jit.compile_callgrind_template(*,stmt:str,setup:str,global_setup:str)->str
torch.utils.benchmark.utils.cpp_jit.compile_timeit_template(*,stmt:str,setup:str,global_setup:str)->TimeitModuleType
torch.utils.benchmark.utils.cpp_jit.get_compat_bindings()->CallgrindModuleType


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/valgrind_wrapper/timer_interface.py----------------------------------------
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCount->NamedTuple('FunctionCount', [('count', int), ('function', str)])
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.count_len->max(count_len, len(str(c)) + int(c < 0))
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.fn_str_len->max(linewidth - count_len - 4, 40)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.left_len->int((fn_str_len - 5) // 2)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.output->f"\n{super().__repr__()}\n{self.task_spec.summarize()}\n  {'':>25}All{'':>10}Noisy symbols removed\n    Instructions: {self.counts(denoise=False):>12}{'':>15}{self.counts(denoise=True):>12}\n    Baseline:     {base_stats.sum():>12}{'':>15}{base_stats.denoise().sum():>12}\n{self.number_per_run} runs per measurement, {self.task_spec.num_threads} thread{('s' if self.task_spec.num_threads > 1 else '')}\n".strip()
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.stats->stats.transform(lambda fn: re.sub(before, after, fn)).transform(lambda fn: re.sub(before, after, fn))
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.supported_str->'\n'.join([getattr(t, '__name__', repr(t)) for t in it.chain(_GLOBALS_ALLOWED_TYPES.values())])
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.path->os.path.join(self._data_dir, f'{name}.pt')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.self._bindings_module->torch.utils.benchmark.utils.cpp_jit.get_compat_bindings()
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.self._supported_platform->self._bindings_module._valgrind_supported_platform()
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.build_search->re.search('BUILD_TYPE=(.+),', torch.__config__.show())
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.(*task_stats, baseline_stats)->self._invoke(task_spec=task_spec, globals=globals, number=number, repeats=repeats, collect_baseline=collect_baseline, is_python=is_python, retain_out_file=retain_out_file)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.working_dir->torch.utils.benchmark.utils.common._make_temp_dir(prefix='callgrind')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.data_dir->os.path.join(working_dir, 'data')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.script_file->os.path.join(working_dir, 'timer_callgrind.py')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.callgrind_out->os.path.join(working_dir, 'callgrind.out')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.error_log->os.path.join(working_dir, 'error.txt')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.stat_log->os.path.join(working_dir, 'callgrind_stat.txt')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.stdout_stderr_log->os.path.join(working_dir, 'stdout_stderr.log')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.f_stdout_stderr->open(stdout_stderr_log, 'wb')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.invocation->subprocess.run(args, stdout=f_stdout_stderr, stderr=subprocess.STDOUT, **kwargs)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.run_loop_exec->torch.utils.benchmark.utils.cpp_jit.compile_callgrind_template(stmt=task_spec.stmt, setup=task_spec.setup, global_setup=task_spec.global_setup)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.(valgrind_invocation, valgrind_invocation_output)->run(['valgrind', '--tool=callgrind', f'--callgrind-out-file={callgrind_out}', '--dump-line=yes', '--dump-instr=yes', '--instr-atstart=yes', '--collect-atstart=no'] + run_loop_cmd)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.error_report->f.read()
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.(annotate_invocation, annotate_invocation_output)->run(['callgrind_annotate', f"--inclusive={('yes' if inclusive else 'no')}", '--threshold=100', '--show-percs=no', fpath], check=True)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.total_pattern->re.compile('^([0-9,]+)\\s+PROGRAM TOTALS')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.begin_pattern->re.compile('Ir\\s+file:function')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.function_pattern->re.compile('^\\s*([0-9,]+)\\s+(.+:.+)$')
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.total_match->re.compile('^([0-9,]+)\\s+PROGRAM TOTALS').match(l)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.program_totals->int(total_match.groups()[0].replace(',', ''))
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.fn_match->re.compile('^\\s*([0-9,]+)\\s+(.+:.+)$').match(l)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.(ir_str, file_function)->re.compile('^\\s*([0-9,]+)\\s+(.+:.+)$').match(l).groups()
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.ir->int(ir_str.replace(',', ''))
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.callgrind_out_contents->f.read()
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.unrolled_stmts->textwrap.indent('\n'.join([stmt] * block_size), ' ' * 4)
A:torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CALLGRIND_SINGLETON->_ValgrindWrapper()
torch.utils.benchmark.CallgrindStats(object)
torch.utils.benchmark.CallgrindStats.__repr__(self)->str
torch.utils.benchmark.CallgrindStats.as_standardized(self)->'CallgrindStats'
torch.utils.benchmark.CallgrindStats.counts(self,*,denoise:bool=False)->int
torch.utils.benchmark.CallgrindStats.delta(self,other,inclusive:bool=False)->FunctionCounts
torch.utils.benchmark.CallgrindStats.stats(self,inclusive:bool=False)->FunctionCounts
torch.utils.benchmark.CopyIfCallgrind(self,value:Any,*,setup:Optional[str]=None)
torch.utils.benchmark.CopyIfCallgrind.serialization(self)->Serialization
torch.utils.benchmark.CopyIfCallgrind.setup(self)->Optional[str]
torch.utils.benchmark.CopyIfCallgrind.unwrap_all(globals:Dict[str,Any])->Dict[str, Any]
torch.utils.benchmark.CopyIfCallgrind.value(self)->Any
torch.utils.benchmark.FunctionCounts(object)
torch.utils.benchmark.FunctionCounts.__add__(self,other)->'FunctionCounts'
torch.utils.benchmark.FunctionCounts.__getitem__(self,item:Any)->'Union[FunctionCount, FunctionCounts]'
torch.utils.benchmark.FunctionCounts.__iter__(self)->Generator[FunctionCount, None, None]
torch.utils.benchmark.FunctionCounts.__len__(self)->int
torch.utils.benchmark.FunctionCounts.__mul__(self,other:Union[int,float])->'FunctionCounts'
torch.utils.benchmark.FunctionCounts.__repr__(self)->str
torch.utils.benchmark.FunctionCounts.__sub__(self,other)->'FunctionCounts'
torch.utils.benchmark.FunctionCounts._from_dict(counts:Dict[str,int],inclusive:bool)->'FunctionCounts'
torch.utils.benchmark.FunctionCounts._merge(self,second,merge_fn:Callable[[int],int])->'FunctionCounts'
torch.utils.benchmark.FunctionCounts.denoise(self)->'FunctionCounts'
torch.utils.benchmark.FunctionCounts.filter(self,filter_fn:Callable[[str],bool])->'FunctionCounts'
torch.utils.benchmark.FunctionCounts.sum(self)->int
torch.utils.benchmark.FunctionCounts.transform(self,map_fn:Callable[[str],str])->'FunctionCounts'
torch.utils.benchmark.GlobalsBridge(self,globals:Dict[str,Any],data_dir:str)
torch.utils.benchmark.GlobalsBridge.construct(self)->str
torch.utils.benchmark.Serialization(enum.Enum)
torch.utils.benchmark._ValgrindWrapper(self)
torch.utils.benchmark._ValgrindWrapper._construct_script(task_spec:common.TaskSpec,globals:GlobalsBridge,*,number:int,repeats:int,collect_baseline:bool,error_log:str,stat_log:str,bindings:Optional[CallgrindModuleType])->str
torch.utils.benchmark._ValgrindWrapper._invoke(self,*,task_spec:common.TaskSpec,globals:Dict[str,Any],number:int,repeats:int,collect_baseline:bool,is_python:bool,retain_out_file:bool)->Tuple[Tuple[FunctionCounts, FunctionCounts, Optional[str]], ...]
torch.utils.benchmark._ValgrindWrapper._validate(self)->None
torch.utils.benchmark._ValgrindWrapper.collect_callgrind(self,task_spec:common.TaskSpec,globals:Dict[str,Any],*,number:int,repeats:int,collect_baseline:bool,is_python:bool,retain_out_file:bool)->Tuple[CallgrindStats, ...]
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats(object)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats.__repr__(self)->str
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats.as_standardized(self)->'CallgrindStats'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats.counts(self,*,denoise:bool=False)->int
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats.delta(self,other,inclusive:bool=False)->FunctionCounts
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats.stats(self,inclusive:bool=False)->FunctionCounts
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CopyIfCallgrind(self,value:Any,*,setup:Optional[str]=None)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CopyIfCallgrind.__init__(self,value:Any,*,setup:Optional[str]=None)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CopyIfCallgrind.serialization(self)->Serialization
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CopyIfCallgrind.setup(self)->Optional[str]
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CopyIfCallgrind.unwrap_all(globals:Dict[str,Any])->Dict[str, Any]
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CopyIfCallgrind.value(self)->Any
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts(object)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__add__(self,other)->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__getitem__(self,item:Any)->'Union[FunctionCount, FunctionCounts]'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__iter__(self)->Generator[FunctionCount, None, None]
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__len__(self)->int
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__mul__(self,other:Union[int,float])->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__repr__(self)->str
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.__sub__(self,other)->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts._from_dict(counts:Dict[str,int],inclusive:bool)->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts._merge(self,second,merge_fn:Callable[[int],int])->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.denoise(self)->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.filter(self,filter_fn:Callable[[str],bool])->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.sum(self)->int
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts.transform(self,map_fn:Callable[[str],str])->'FunctionCounts'
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.GlobalsBridge(self,globals:Dict[str,Any],data_dir:str)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.GlobalsBridge.__init__(self,globals:Dict[str,Any],data_dir:str)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.GlobalsBridge.construct(self)->str
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.Serialization(enum.Enum)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface._ValgrindWrapper(self)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface._ValgrindWrapper.__init__(self)
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface._ValgrindWrapper._construct_script(task_spec:common.TaskSpec,globals:GlobalsBridge,*,number:int,repeats:int,collect_baseline:bool,error_log:str,stat_log:str,bindings:Optional[CallgrindModuleType])->str
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface._ValgrindWrapper._invoke(self,*,task_spec:common.TaskSpec,globals:Dict[str,Any],number:int,repeats:int,collect_baseline:bool,is_python:bool,retain_out_file:bool)->Tuple[Tuple[FunctionCounts, FunctionCounts, Optional[str]], ...]
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface._ValgrindWrapper._validate(self)->None
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface._ValgrindWrapper.collect_callgrind(self,task_spec:common.TaskSpec,globals:Dict[str,Any],*,number:int,repeats:int,collect_baseline:bool,is_python:bool,retain_out_file:bool)->Tuple[CallgrindStats, ...]
torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.wrapper_singleton()->_ValgrindWrapper
torch.utils.benchmark.wrapper_singleton()->_ValgrindWrapper


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/utils/valgrind_wrapper/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/op_fuzzers/sparse_unary.py----------------------------------------
A:torch.utils.benchmark.op_fuzzers.sparse_unary._POW_TWO_SIZES->tuple((2 ** i for i in range(int(np.log2(_MIN_DIM_SIZE)), int(np.log2(_MAX_DIM_SIZE)) + 1)))
torch.utils.benchmark.op_fuzzers.sparse_unary.UnaryOpSparseFuzzer(self,seed,dtype=torch.float32,cuda=False)
torch.utils.benchmark.op_fuzzers.sparse_unary.UnaryOpSparseFuzzer.__init__(self,seed,dtype=torch.float32,cuda=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/op_fuzzers/spectral.py----------------------------------------
torch.utils.benchmark.op_fuzzers.spectral.SpectralOpFuzzer(self,*,seed:int,dtype=torch.float64,cuda:bool=False,probability_regular:float=1.0)
torch.utils.benchmark.op_fuzzers.spectral.SpectralOpFuzzer.__init__(self,*,seed:int,dtype=torch.float64,cuda:bool=False,probability_regular:float=1.0)
torch.utils.benchmark.op_fuzzers.spectral.power_range(upper_bound,base)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/op_fuzzers/sparse_binary.py----------------------------------------
A:torch.utils.benchmark.op_fuzzers.sparse_binary._POW_TWO_SIZES->tuple((2 ** i for i in range(int(np.log2(_MIN_DIM_SIZE)), int(np.log2(_MAX_DIM_SIZE)) + 1)))
torch.utils.benchmark.op_fuzzers.sparse_binary.BinaryOpSparseFuzzer(self,seed,dtype=torch.float32,cuda=False)
torch.utils.benchmark.op_fuzzers.sparse_binary.BinaryOpSparseFuzzer.__init__(self,seed,dtype=torch.float32,cuda=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/op_fuzzers/unary.py----------------------------------------
A:torch.utils.benchmark.op_fuzzers.unary._POW_TWO_SIZES->tuple((2 ** i for i in range(int(np.log2(_MIN_DIM_SIZE)), int(np.log2(_MAX_DIM_SIZE)) + 1)))
torch.utils.benchmark.op_fuzzers.unary.UnaryOpFuzzer(self,seed,dtype=torch.float32,cuda=False)
torch.utils.benchmark.op_fuzzers.unary.UnaryOpFuzzer.__init__(self,seed,dtype=torch.float32,cuda=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/op_fuzzers/binary.py----------------------------------------
A:torch.utils.benchmark.op_fuzzers.binary._POW_TWO_SIZES->tuple((2 ** i for i in range(int(np.log2(_MIN_DIM_SIZE)), int(np.log2(_MAX_DIM_SIZE)) + 1)))
torch.utils.benchmark.op_fuzzers.binary.BinaryOpFuzzer(self,seed,dtype=torch.float32,cuda=False)
torch.utils.benchmark.op_fuzzers.binary.BinaryOpFuzzer.__init__(self,seed,dtype=torch.float32,cuda=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/op_fuzzers/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/spectral_ops_fuzz_test.py----------------------------------------
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.spectral_fuzzer->SpectralOpFuzzer(seed=seed, dtype=dtype, cuda=cuda, probability_regular=probability_regular)
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.str_shape->' x '.join(['{:<4}'.format(s) for s in shape])
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.measurement->torch.utils.benchmark.Timer(stmt='func(x, dim=dim)', globals={'func': function, 'x': tensors['x'], 'dim': dim}, label=f'{name}_{device}', sub_label=sub_label, description=f'dim={dim}', num_threads=nthreads).blocked_autorange(min_run_time=1)
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.Benchmark->namedtuple('Benchmark', ['name', 'function', 'dtype'])
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.dim_str->str(dim)
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.shape_str->'x'.join((str(s) for s in shape))
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.parser->ArgumentParser(description=__doc__)
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.args->ArgumentParser(description=__doc__).parse_args()
A:torch.utils.benchmark.examples.spectral_ops_fuzz_test.compare->torch.utils.benchmark.Compare(results)
torch.utils.benchmark.examples.spectral_ops_fuzz_test._dim_options(ndim)
torch.utils.benchmark.examples.spectral_ops_fuzz_test._output_csv(file,results)
torch.utils.benchmark.examples.spectral_ops_fuzz_test.run_benchmark(name:str,function:object,dtype:torch.dtype,seed:int,device:str,samples:int,probability_regular:float)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/end_to_end.py----------------------------------------
A:torch.utils.benchmark.examples.end_to_end._AVAILABLE_GPUS->queue.Queue[int]()
A:torch.utils.benchmark.examples.end_to_end.parser->argparse.ArgumentParser()
A:torch.utils.benchmark.examples.end_to_end.args->parse_args()
A:torch.utils.benchmark.examples.end_to_end.args.num_gpus->torch.cuda.device_count()
A:torch.utils.benchmark.examples.end_to_end.state->numpy.random.RandomState(params['random_value'])
A:torch.utils.benchmark.examples.end_to_end.topk_dim->numpy.random.RandomState(params['random_value']).randint(low=0, high=dim)
A:torch.utils.benchmark.examples.end_to_end.k->max(int(np.floor(2 ** state.uniform(low=0, high=np.log2(dim_size)))), 1)
A:torch.utils.benchmark.examples.end_to_end.sort_dim->numpy.random.RandomState(params['random_value']).randint(low=0, high=params['dim'])
A:torch.utils.benchmark.examples.end_to_end.iterator->torch.utils.benchmark.op_fuzzers.unary.UnaryOpFuzzer(seed=seed, dtype=dtype, cuda=cuda).take(_RUNS_PER_LOOP)
A:torch.utils.benchmark.examples.end_to_end.(stmt, label)->construct_stmt_and_label(args.pr, params)
A:torch.utils.benchmark.examples.end_to_end.timer->Timer(stmt=stmt, globals=tensors, label=label, description=f'[{i}, seed={seed}] ({dtype_str}), stmt = {stmt}', env=args.DETAIL_env)
A:torch.utils.benchmark.examples.end_to_end.measurement->Timer(stmt=stmt, globals=tensors, label=label, description=f'[{i}, seed={seed}] ({dtype_str}), stmt = {stmt}', env=args.DETAIL_env).blocked_autorange(min_run_time=_MIN_RUN_SEC)
A:torch.utils.benchmark.examples.end_to_end.pools[_GPU]->multiprocessing.dummy.Pool(args.num_gpus)
A:torch.utils.benchmark.examples.end_to_end.map_iters[_GPU]->multiprocessing.dummy.Pool(args.num_gpus).imap(map_fn, trials)
A:torch.utils.benchmark.examples.end_to_end.cpu_workers->int(multiprocessing.cpu_count() / 3)
A:torch.utils.benchmark.examples.end_to_end.pools[_CPU]->multiprocessing.dummy.Pool(cpu_workers)
A:torch.utils.benchmark.examples.end_to_end.map_iters[_CPU]->multiprocessing.dummy.Pool(cpu_workers).imap(map_fn, trials)
A:torch.utils.benchmark.examples.end_to_end.merged_state['times']->list(it.chain(*times))
A:torch.utils.benchmark.examples.end_to_end.flagged_for_removal->set()
A:torch.utils.benchmark.examples.end_to_end.device_str->f"== {device_str} {(' (Variance Test)' if test_variance else '')}  ".ljust(40, '=')
A:torch.utils.benchmark.examples.end_to_end.results->sorted(((key, (r_ref, r_pr), r_pr.median / r_ref.median - 1) for (key, (r_ref, r_pr)) in results), key=lambda i: i[2])
A:torch.utils.benchmark.examples.end_to_end.n->len(results)
A:torch.utils.benchmark.examples.end_to_end.n_regressed->len([i for i in results if i[2] > 0.05])
A:torch.utils.benchmark.examples.end_to_end.n_improved->len([i for i in results if i[2] < -0.05])
A:torch.utils.benchmark.examples.end_to_end.(_, result_log_file)->tempfile.mkstemp(suffix='.log')
A:torch.utils.benchmark.examples.end_to_end.row->row_str(rel_diff, r_pr.median - r_ref.median, r_ref)
A:torch.utils.benchmark.examples.end_to_end.order->str('' if all((i == j for (i, j) in zip(order, range(dim)))) else order)
A:torch.utils.benchmark.examples.end_to_end.(dim_str, k_str)->Timer(stmt=stmt, globals=tensors, label=label, description=f'[{i}, seed={seed}] ({dtype_str}), stmt = {stmt}', env=args.DETAIL_env).blocked_autorange(min_run_time=_MIN_RUN_SEC).stmt[:-1].replace('torch.topk(x, ', '').split(', ')
A:torch.utils.benchmark.examples.end_to_end.task_specific->Timer(stmt=stmt, globals=tensors, label=label, description=f'[{i}, seed={seed}] ({dtype_str}), stmt = {stmt}', env=args.DETAIL_env).blocked_autorange(min_run_time=_MIN_RUN_SEC).stmt[:-1].replace('torch.sort(x, ', '')
A:torch.utils.benchmark.examples.end_to_end.result->run(f'source activate {env}')
A:torch.utils.benchmark.examples.end_to_end.(_, result_file)->tempfile.mkstemp(suffix='.pkl')
A:torch.utils.benchmark.examples.end_to_end.cmd->_SUBPROCESS_CMD_TEMPLATE.format(source_env=envs[0] if test_variance else env, env=env, pr=pr, device=_GPU if use_gpu else _CPU, result_file=result_file, seed=seed)
torch.utils.benchmark.examples.end_to_end._main(args)
torch.utils.benchmark.examples.end_to_end.construct_stmt_and_label(pr,params)
torch.utils.benchmark.examples.end_to_end.construct_table(results,device_str,test_variance)
torch.utils.benchmark.examples.end_to_end.main(args)
torch.utils.benchmark.examples.end_to_end.map_fn(args)
torch.utils.benchmark.examples.end_to_end.merge(measurements)
torch.utils.benchmark.examples.end_to_end.parse_args()
torch.utils.benchmark.examples.end_to_end.process_results(results,test_variance)
torch.utils.benchmark.examples.end_to_end.read_results(result_file:str)
torch.utils.benchmark.examples.end_to_end.row_str(rel_diff,diff_seconds,measurement)
torch.utils.benchmark.examples.end_to_end.run(cmd,cuda_visible_devices='')
torch.utils.benchmark.examples.end_to_end.subprocess_main(args)
torch.utils.benchmark.examples.end_to_end.test_source(envs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/blas_compare_setup.py----------------------------------------
A:torch.utils.benchmark.examples.blas_compare_setup.SubEnvSpec->collections.namedtuple('SubEnvSpec', ('generic_installs', 'special_installs', 'environment_variables', 'expected_blas_symbols', 'expected_mkl_version'))
A:torch.utils.benchmark.examples.blas_compare_setup.(stdout, stderr, retcode)->conda.cli.python_api.run_command(*args)
A:torch.utils.benchmark.examples.blas_compare_setup.git_root->subprocess.check_output('git rev-parse --show-toplevel', shell=True, cwd=os.path.dirname(os.path.realpath(__file__))).decode('utf-8').strip()
A:torch.utils.benchmark.examples.blas_compare_setup.env_path->os.path.join(WORKING_ROOT, env_name)
A:torch.utils.benchmark.examples.blas_compare_setup.base_source->subprocess.run(f'source activate {env_path}', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
A:torch.utils.benchmark.examples.blas_compare_setup.env_set->subprocess.run(f"source activate {env_path} && conda env config vars set {' '.join(env_spec.environment_variables)}", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
A:torch.utils.benchmark.examples.blas_compare_setup.actual_env_vars->subprocess.run(f'source activate {env_path} && env', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout.decode('utf-8').strip().splitlines()
A:torch.utils.benchmark.examples.blas_compare_setup.build_run->subprocess.run(f'source activate {env_path} && cd {git_root} && python setup.py install --cmake', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
A:torch.utils.benchmark.examples.blas_compare_setup.check_run->subprocess.run(f'''source activate {env_path} && python -c "import torch;from torch.utils.benchmark import Timer;print(torch.__config__.show());setup = 'x=torch.ones((128, 128));y=torch.ones((128, 128))';counts = Timer('torch.mm(x, y)', setup).collect_callgrind(collect_baseline=False);stats = counts.as_standardized().stats(inclusive=True);print(stats.filter(lambda l: 'blas' in l.lower()))"''', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
A:torch.utils.benchmark.examples.blas_compare_setup.check_run_stdout->subprocess.run(f'''source activate {env_path} && python -c "import torch;from torch.utils.benchmark import Timer;print(torch.__config__.show());setup = 'x=torch.ones((128, 128));y=torch.ones((128, 128))';counts = Timer('torch.mm(x, y)', setup).collect_callgrind(collect_baseline=False);stats = counts.as_standardized().stats(inclusive=True);print(stats.filter(lambda l: 'blas' in l.lower()))"''', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout.decode('utf-8')
torch.utils.benchmark.examples.blas_compare_setup.conda_run(*args)
torch.utils.benchmark.examples.blas_compare_setup.main()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/fuzzer.py----------------------------------------
A:torch.utils.benchmark.examples.fuzzer.add_fuzzer->torch.utils.benchmark.Fuzzer(parameters=[[benchmark_utils.FuzzedParameter(name=f'k{i}', minval=16, maxval=16 * 1024, distribution='loguniform') for i in range(3)], benchmark_utils.FuzzedParameter(name='d', distribution={2: 0.6, 3: 0.4})], tensors=[[benchmark_utils.FuzzedTensor(name=name, size=('k0', 'k1', 'k2'), dim_parameter='d', probability_contiguous=0.75, min_elements=64 * 1024, max_elements=128 * 1024) for name in ('x', 'y')]], seed=0)
A:torch.utils.benchmark.examples.fuzzer.shape->', '.join(tuple((f'{i:>4}' for i in x.shape)))
A:torch.utils.benchmark.examples.fuzzer.description->''.join([f'{x.numel():>7} | {shape:<16} | ', f"{('contiguous' if x.is_contiguous() else x_order):<12} | ", f"{('contiguous' if y.is_contiguous() else y_order):<12} | "])
A:torch.utils.benchmark.examples.fuzzer.timer->torch.utils.benchmark.Timer(stmt='x + y', globals=tensors, description=description)
torch.utils.benchmark.examples.fuzzer.main()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/compare.py----------------------------------------
A:torch.utils.benchmark.examples.compare.numel->int(result.numel())
A:torch.utils.benchmark.examples.compare.comparison->torch.utils.benchmark.Compare([pickle.loads(i) for i in serialized_results])
torch.utils.benchmark.examples.compare.FauxTorch(self,real_torch,extra_ns_per_element)
torch.utils.benchmark.examples.compare.FauxTorch.__init__(self,real_torch,extra_ns_per_element)
torch.utils.benchmark.examples.compare.FauxTorch.add(self,*args,**kwargs)
torch.utils.benchmark.examples.compare.FauxTorch.cat(self,*args,**kwargs)
torch.utils.benchmark.examples.compare.FauxTorch.extra_overhead(self,result)
torch.utils.benchmark.examples.compare.FauxTorch.matmul(self,*args,**kwargs)
torch.utils.benchmark.examples.compare.FauxTorch.mul(self,*args,**kwargs)
torch.utils.benchmark.examples.compare.main()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/op_benchmark.py----------------------------------------
A:torch.utils.benchmark.examples.op_benchmark.float_iter->fuzzer_cls(seed=0, dtype=torch.float32).take(n)
A:torch.utils.benchmark.examples.op_benchmark.int_iter->fuzzer_cls(seed=0, dtype=torch.int32).take(n)
A:torch.utils.benchmark.examples.op_benchmark.name_len->max(name_len, len(name))
A:torch.utils.benchmark.examples.op_benchmark.shape_len->max(shape_len, len(shape))
A:torch.utils.benchmark.examples.op_benchmark.order_len->max(order_len, len(order))
A:torch.utils.benchmark.examples.op_benchmark.steps_len->max(steps_len, len(steps))
A:torch.utils.benchmark.examples.op_benchmark.name->f'{name}:'.ljust(name_len + 1)
A:torch.utils.benchmark.examples.op_benchmark.shape->shape.ljust(shape_len + 10).ljust(shape_len + 10)
A:torch.utils.benchmark.examples.op_benchmark.order->order.ljust(order_len).ljust(order_len)
torch.utils.benchmark.examples.op_benchmark.assert_dicts_equal(dict_0,dict_1)
torch.utils.benchmark.examples.op_benchmark.main()
torch.utils.benchmark.examples.op_benchmark.run(n,stmt,fuzzer_cls)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/simple_timeit.py----------------------------------------
A:torch.utils.benchmark.examples.simple_timeit.timer->torch.utils.benchmark.Timer(stmt='x + y', globals={'x': torch.ones((4, 8)), 'y': torch.ones((1, 8))}, label='Broadcasting add (4x8)')
torch.utils.benchmark.examples.simple_timeit.main()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/benchmark/examples/blas_compare.py----------------------------------------
A:torch.utils.benchmark.examples.blas_compare.RESULT_FILE->os.path.join(blas_compare_setup.WORKING_ROOT, 'blas_results.pkl')
A:torch.utils.benchmark.examples.blas_compare.SCRATCH_DIR->os.path.join(blas_compare_setup.WORKING_ROOT, 'scratch')
A:torch.utils.benchmark.examples.blas_compare._RESULT_FILE_LOCK->threading.Lock()
A:torch.utils.benchmark.examples.blas_compare.(_, result_file, _)->_WORKER_POOL.get_nowait()
A:torch.utils.benchmark.examples.blas_compare.step->max(n, 2)
A:torch.utils.benchmark.examples.blas_compare.(_, result_file)->tempfile.mkstemp(suffix='.pkl', prefix=SCRATCH_DIR)
A:torch.utils.benchmark.examples.blas_compare.conda_prefix->os.getenv('CONDA_PREFIX')
A:torch.utils.benchmark.examples.blas_compare.t->Timer(stmt='torch.mm(x, y)', label=f'torch.mm {shape_str} {blas_type} ({dtype_name})', sub_label=sub_label, description=f'n = {n}', env=os.path.split(env or '')[1] or None, globals={'x': torch.rand(x_shape, dtype=dtype), 'y': torch.rand(y_shape, dtype=dtype)}, num_threads=num_threads).blocked_autorange(min_run_time=MIN_RUN_TIME)
A:torch.utils.benchmark.examples.blas_compare.(core_str, result_file, num_threads)->_WORKER_POOL.get()
A:torch.utils.benchmark.examples.blas_compare.result_bytes->f.read()
A:torch.utils.benchmark.examples.blas_compare.comparison->Compare(results)
A:torch.utils.benchmark.examples.blas_compare.workers->_WORKER_POOL.qsize()
A:torch.utils.benchmark.examples.blas_compare.env_path->os.path.join(blas_compare_setup.WORKING_ROOT, BLAS_CONFIGS[0][1])
A:torch.utils.benchmark.examples.blas_compare.n->len(trials)
A:torch.utils.benchmark.examples.blas_compare.start_time->time.time()
A:torch.utils.benchmark.examples.blas_compare.eta->int((n - n_trials_done) * time_per_result)
A:torch.utils.benchmark.examples.blas_compare.parser->argparse.ArgumentParser()
A:torch.utils.benchmark.examples.blas_compare.args->argparse.ArgumentParser().parse_args()
torch.utils.benchmark.examples.blas_compare._compare_main()
torch.utils.benchmark.examples.blas_compare._subprocess_main(seed=0,num_threads=1,sub_label='N/A',result_file=None,env=None)
torch.utils.benchmark.examples.blas_compare.clear_worker_pool()
torch.utils.benchmark.examples.blas_compare.fill_core_pool(n:int)
torch.utils.benchmark.examples.blas_compare.main()
torch.utils.benchmark.examples.blas_compare.run_subprocess(args)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_utils.py----------------------------------------
A:torch.utils.tensorboard._utils.canvas->numpy.zeros((3, H * nrows, W * ncols), dtype=I.dtype)
A:torch.utils.tensorboard._utils.data->numpy.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)
A:torch.utils.tensorboard._utils.(w, h)->figure.canvas.get_width_height()
A:torch.utils.tensorboard._utils.image_chw->numpy.moveaxis(image_hwc, source=2, destination=0)
A:torch.utils.tensorboard._utils.image->render_to_rgb(figures)
A:torch.utils.tensorboard._utils.len_addition->int(2 ** V.shape[0].bit_length() - V.shape[0])
A:torch.utils.tensorboard._utils.V->numpy.reshape(V, newshape=(t, n_rows * h, n_cols * w, c))
A:torch.utils.tensorboard._utils.I->numpy.concatenate([I, I, I], 1)
A:torch.utils.tensorboard._utils.ncols->min(nimg, ncols)
A:torch.utils.tensorboard._utils.nrows->int(np.ceil(float(nimg) / ncols))
A:torch.utils.tensorboard._utils.input_format->input_format.upper().upper()
A:torch.utils.tensorboard._utils.tensor_NCHW->numpy.stack([tensor, tensor, tensor], 2).transpose(index)
A:torch.utils.tensorboard._utils.tensor_CHW->make_grid(tensor_NCHW)
A:torch.utils.tensorboard._utils.tensor_HWC->numpy.concatenate([tensor_HWC, tensor_HWC, tensor_HWC], 2)
A:torch.utils.tensorboard._utils.tensor->numpy.stack([tensor, tensor, tensor], 2)
torch.utils.tensorboard._utils._prepare_video(V)
torch.utils.tensorboard._utils.convert_to_HWC(tensor,input_format)
torch.utils.tensorboard._utils.figure_to_image(figures,close=True)
torch.utils.tensorboard._utils.make_grid(I,ncols=8)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_convert_np.py----------------------------------------
A:torch.utils.tensorboard._convert_np.x->caffe2.python.workspace.FetchBlob(x)
torch.utils.tensorboard._convert_np._prepare_caffe2(x)
torch.utils.tensorboard._convert_np._prepare_pytorch(x)
torch.utils.tensorboard._convert_np.make_np(x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/writer.py----------------------------------------
A:torch.utils.tensorboard.writer.log_dir->os.path.join('runs', current_time + '_' + socket.gethostname() + comment)
A:torch.utils.tensorboard.writer.self.event_writer->EventFileWriter(log_dir, max_queue, flush_secs, filename_suffix)
A:torch.utils.tensorboard.writer.event.step->int(step)
A:torch.utils.tensorboard.writer.event->tensorboard.compat.proto.event_pb2.Event(graph_def=current_graph.SerializeToString())
A:torch.utils.tensorboard.writer.trm->tensorboard.compat.proto.event_pb2.TaggedRunMetadata(tag='step1', run_metadata=stepstats.SerializeToString())
A:torch.utils.tensorboard.writer.current_time->datetime.datetime.now().strftime('%b%d_%H-%M-%S')
A:torch.utils.tensorboard.writer.self.file_writer->FileWriter(self.log_dir, self.max_queue, self.flush_secs, self.filename_suffix)
A:torch.utils.tensorboard.writer.(exp, ssi, sei)->hparams(hparam_dict, metric_dict, hparam_domain_discrete)
A:torch.utils.tensorboard.writer.run_name->str(time.time())
A:torch.utils.tensorboard.writer.logdir->os.path.join(self._get_file_writer().get_logdir(), run_name)
A:torch.utils.tensorboard.writer.scalar_value->caffe2.python.workspace.FetchBlob(scalar_value)
A:torch.utils.tensorboard.writer.summary->scalar(tag, scalar_value, new_style=new_style, double_precision=double_precision)
A:torch.utils.tensorboard.writer.fw_logdir->self._get_file_writer().get_logdir()
A:torch.utils.tensorboard.writer.fw->FileWriter(fw_tag, self.max_queue, self.flush_secs, self.filename_suffix)
A:torch.utils.tensorboard.writer.values->caffe2.python.workspace.FetchBlob(values)
A:torch.utils.tensorboard.writer.img_tensor->caffe2.python.workspace.FetchBlob(img_tensor)
A:torch.utils.tensorboard.writer.box_tensor->caffe2.python.workspace.FetchBlob(box_tensor)
A:torch.utils.tensorboard.writer.snd_tensor->caffe2.python.workspace.FetchBlob(snd_tensor)
A:torch.utils.tensorboard.writer.current_graph->model_to_graph_def(model)
A:torch.utils.tensorboard.writer.retval->retval.replace('\\', '%%%02x' % ord('\\')).replace('\\', '%%%02x' % ord('\\'))
A:torch.utils.tensorboard.writer.mat->make_np(mat)
A:torch.utils.tensorboard.writer.save_path->os.path.join(self._get_file_writer().get_logdir(), subdir)
A:torch.utils.tensorboard.writer.fs->tensorboard.compat.tf.io.gfile.get_filesystem(save_path)
A:torch.utils.tensorboard.writer.self._projector_config->ProjectorConfig()
A:torch.utils.tensorboard.writer.embedding_info->get_embedding_info(metadata, label_img, fs, subdir, global_step, tag)
A:torch.utils.tensorboard.writer.config_pbtxt->google.protobuf.text_format.MessageToString(self._projector_config)
torch.utils.tensorboard.FileWriter(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.FileWriter.add_event(self,event,step=None,walltime=None)
torch.utils.tensorboard.FileWriter.add_graph(self,graph_profile,walltime=None)
torch.utils.tensorboard.FileWriter.add_onnx_graph(self,graph,walltime=None)
torch.utils.tensorboard.FileWriter.add_summary(self,summary,global_step=None,walltime=None)
torch.utils.tensorboard.FileWriter.close(self)
torch.utils.tensorboard.FileWriter.flush(self)
torch.utils.tensorboard.FileWriter.get_logdir(self)
torch.utils.tensorboard.FileWriter.reopen(self)
torch.utils.tensorboard.SummaryWriter(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.SummaryWriter.__enter__(self)
torch.utils.tensorboard.SummaryWriter.__exit__(self,exc_type,exc_val,exc_tb)
torch.utils.tensorboard.SummaryWriter._check_caffe2_blob(self,item)
torch.utils.tensorboard.SummaryWriter._encode(rawstr)
torch.utils.tensorboard.SummaryWriter._get_file_writer(self)
torch.utils.tensorboard.SummaryWriter.add_audio(self,tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_custom_scalars(self,layout)
torch.utils.tensorboard.SummaryWriter.add_custom_scalars_marginchart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.SummaryWriter.add_custom_scalars_multilinechart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.SummaryWriter.add_embedding(self,mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)
torch.utils.tensorboard.SummaryWriter.add_figure(self,tag,figure,global_step=None,close=True,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_graph(self,model,input_to_model=None,verbose=False,use_strict_trace=True)
torch.utils.tensorboard.SummaryWriter.add_histogram(self,tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)
torch.utils.tensorboard.SummaryWriter.add_histogram_raw(self,tag,min,max,num,sum,sum_squares,bucket_limits,bucket_counts,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_hparams(self,hparam_dict,metric_dict,hparam_domain_discrete=None,run_name=None)
torch.utils.tensorboard.SummaryWriter.add_image(self,tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')
torch.utils.tensorboard.SummaryWriter.add_image_with_boxes(self,tag,img_tensor,box_tensor,global_step=None,walltime=None,rescale=1,dataformats='CHW',labels=None)
torch.utils.tensorboard.SummaryWriter.add_images(self,tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')
torch.utils.tensorboard.SummaryWriter.add_mesh(self,tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_onnx_graph(self,prototxt)
torch.utils.tensorboard.SummaryWriter.add_pr_curve(self,tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_pr_curve_raw(self,tag,true_positive_counts,false_positive_counts,true_negative_counts,false_negative_counts,precision,recall,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_scalar(self,tag,scalar_value,global_step=None,walltime=None,new_style=False,double_precision=False)
torch.utils.tensorboard.SummaryWriter.add_scalars(self,main_tag,tag_scalar_dict,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_text(self,tag,text_string,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_video(self,tag,vid_tensor,global_step=None,fps=4,walltime=None)
torch.utils.tensorboard.SummaryWriter.close(self)
torch.utils.tensorboard.SummaryWriter.flush(self)
torch.utils.tensorboard.SummaryWriter.get_logdir(self)
torch.utils.tensorboard.writer.FileWriter(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.FileWriter.__init__(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.FileWriter.add_event(self,event,step=None,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_graph(self,graph_profile,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_onnx_graph(self,graph,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_summary(self,summary,global_step=None,walltime=None)
torch.utils.tensorboard.writer.FileWriter.close(self)
torch.utils.tensorboard.writer.FileWriter.flush(self)
torch.utils.tensorboard.writer.FileWriter.get_logdir(self)
torch.utils.tensorboard.writer.FileWriter.reopen(self)
torch.utils.tensorboard.writer.SummaryWriter(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.SummaryWriter.__enter__(self)
torch.utils.tensorboard.writer.SummaryWriter.__exit__(self,exc_type,exc_val,exc_tb)
torch.utils.tensorboard.writer.SummaryWriter.__init__(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.SummaryWriter._check_caffe2_blob(self,item)
torch.utils.tensorboard.writer.SummaryWriter._encode(rawstr)
torch.utils.tensorboard.writer.SummaryWriter._get_file_writer(self)
torch.utils.tensorboard.writer.SummaryWriter.add_audio(self,tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars(self,layout)
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars_marginchart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars_multilinechart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.writer.SummaryWriter.add_embedding(self,mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)
torch.utils.tensorboard.writer.SummaryWriter.add_figure(self,tag,figure,global_step=None,close=True,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_graph(self,model,input_to_model=None,verbose=False,use_strict_trace=True)
torch.utils.tensorboard.writer.SummaryWriter.add_histogram(self,tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)
torch.utils.tensorboard.writer.SummaryWriter.add_histogram_raw(self,tag,min,max,num,sum,sum_squares,bucket_limits,bucket_counts,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_hparams(self,hparam_dict,metric_dict,hparam_domain_discrete=None,run_name=None)
torch.utils.tensorboard.writer.SummaryWriter.add_image(self,tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')
torch.utils.tensorboard.writer.SummaryWriter.add_image_with_boxes(self,tag,img_tensor,box_tensor,global_step=None,walltime=None,rescale=1,dataformats='CHW',labels=None)
torch.utils.tensorboard.writer.SummaryWriter.add_images(self,tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')
torch.utils.tensorboard.writer.SummaryWriter.add_mesh(self,tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_onnx_graph(self,prototxt)
torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve(self,tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve_raw(self,tag,true_positive_counts,false_positive_counts,true_negative_counts,false_negative_counts,precision,recall,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_scalar(self,tag,scalar_value,global_step=None,walltime=None,new_style=False,double_precision=False)
torch.utils.tensorboard.writer.SummaryWriter.add_scalars(self,main_tag,tag_scalar_dict,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_text(self,tag,text_string,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_video(self,tag,vid_tensor,global_step=None,fps=4,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.close(self)
torch.utils.tensorboard.writer.SummaryWriter.flush(self)
torch.utils.tensorboard.writer.SummaryWriter.get_logdir(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_caffe2_graph.py----------------------------------------
A:torch.utils.tensorboard._caffe2_graph.WEIGHT->re.compile('(_w)$')
A:torch.utils.tensorboard._caffe2_graph.WEIGHT_->re.compile('(_w_)')
A:torch.utils.tensorboard._caffe2_graph.BN->re.compile('(_bn)$')
A:torch.utils.tensorboard._caffe2_graph.BN_->re.compile('(_bn_)')
A:torch.utils.tensorboard._caffe2_graph.BIAS->re.compile('(_b)$')
A:torch.utils.tensorboard._caffe2_graph.BIAS_->re.compile('(_b_)')
A:torch.utils.tensorboard._caffe2_graph.SCALE->re.compile('(_s)$')
A:torch.utils.tensorboard._caffe2_graph.SCALE_->re.compile('(_s_)')
A:torch.utils.tensorboard._caffe2_graph.SUM->re.compile('(_sum)$')
A:torch.utils.tensorboard._caffe2_graph.SUM_->re.compile('(_sum_)')
A:torch.utils.tensorboard._caffe2_graph.BRANCH->re.compile('(_branch)')
A:torch.utils.tensorboard._caffe2_graph.inter_name->re.compile('(_sum_)').sub('/sum_', SUM.sub('/sum', inter_name))
A:torch.utils.tensorboard._caffe2_graph.new_name->_make_unique_name(seen, rename_fn(name))
A:torch.utils.tensorboard._caffe2_graph.ir->caffe2.python.core.IR(ops)
A:torch.utils.tensorboard._caffe2_graph.inputs->list(op.input)
A:torch.utils.tensorboard._caffe2_graph.outputs->list(op.output)
A:torch.utils.tensorboard._caffe2_graph.names->set()
A:torch.utils.tensorboard._caffe2_graph.op.name->_make_unique_name(seen, name)
A:torch.utils.tensorboard._caffe2_graph.seen->set(input_blobs)
A:torch.utils.tensorboard._caffe2_graph.scope->os.path.commonprefix(name_list)
A:torch.utils.tensorboard._caffe2_graph.name->os.path.join(scope, op.type)
A:torch.utils.tensorboard._caffe2_graph.shape_proto->TensorShapeProto()
A:torch.utils.tensorboard._caffe2_graph.dim->tensorboard.compat.proto.tensor_shape_pb2.TensorShapeProto.Dim()
A:torch.utils.tensorboard._caffe2_graph.n->NodeDef()
A:torch.utils.tensorboard._caffe2_graph.n.device->_tf_device(device)
A:torch.utils.tensorboard._caffe2_graph.len_outputs->len(outputs)
A:torch.utils.tensorboard._caffe2_graph.name_list->list(outputs)
A:torch.utils.tensorboard._caffe2_graph.device->_tf_device(op.device_option)
A:torch.utils.tensorboard._caffe2_graph.produced_by->producing_ops.get(name, [])
A:torch.utils.tensorboard._caffe2_graph.in_blobs->set()
A:torch.utils.tensorboard._caffe2_graph.out_blobs->set()
A:torch.utils.tensorboard._caffe2_graph.input_blobs->list(in_blobs.difference(out_blobs))
A:torch.utils.tensorboard._caffe2_graph.output_blobs->list(out_blobs.difference(in_blobs))
A:torch.utils.tensorboard._caffe2_graph.ops->_filter_ops(ops, _check_if_cpu, show_simplified)
A:torch.utils.tensorboard._caffe2_graph.blobs->set()
A:torch.utils.tensorboard._caffe2_graph.(input_blobs, inter_blobs, _)->_compute_in_out(ops)
A:torch.utils.tensorboard._caffe2_graph.current_graph->GraphDef()
A:torch.utils.tensorboard._caffe2_graph.(shapes, _)->caffe2.python.workspace.InferShapesAndTypes(nets)
A:torch.utils.tensorboard._caffe2_graph.shapes->copy.deepcopy(shapes or {})
torch.utils.tensorboard._caffe2_graph._add_gradient_scope(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._add_tf_shape(attr_dict,ints)
torch.utils.tensorboard._caffe2_graph._blob_to_node(producing_ops,shapes,name)
torch.utils.tensorboard._caffe2_graph._check_if_cpu(blob)
torch.utils.tensorboard._caffe2_graph._check_if_forward(blob)
torch.utils.tensorboard._caffe2_graph._clear_debug_info(ops,perform_clear)
torch.utils.tensorboard._caffe2_graph._compute_in_out(ops)
torch.utils.tensorboard._caffe2_graph._convert_to_ssa(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._fill_missing_operator_names(ops)
torch.utils.tensorboard._caffe2_graph._filter_ops(ops,filter_fn,perform_filter)
torch.utils.tensorboard._caffe2_graph._get_blob_names(ops)
torch.utils.tensorboard._caffe2_graph._make_unique_name(seen:Set[str],name:str,min_version:int=0)
torch.utils.tensorboard._caffe2_graph._operator_to_node(shapes,op)
torch.utils.tensorboard._caffe2_graph._operator_to_node_simp(op,inter_blobs,seen)
torch.utils.tensorboard._caffe2_graph._operators_to_graph_def(shapes,ops,colon_replacement='$',with_ssa=True,with_gradient_scope=True,blob_name_tracker=None,show_simplified=False,custom_rename=None)
torch.utils.tensorboard._caffe2_graph._propagate_device_option(net_def)
torch.utils.tensorboard._caffe2_graph._remap_keys(old_dict,rename_fn)
torch.utils.tensorboard._caffe2_graph._rename_all(shapes,blob_name_tracker,ops,rename_fn)
torch.utils.tensorboard._caffe2_graph._rename_tensorflow_style(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._replace_colons(shapes,blob_name_tracker,ops,repl)
torch.utils.tensorboard._caffe2_graph._set_tf_attr(attr_dict,arg)
torch.utils.tensorboard._caffe2_graph._tf_device(device_option)
torch.utils.tensorboard._caffe2_graph._try_get_shapes(nets)
torch.utils.tensorboard._caffe2_graph.model_to_graph_def(model,**kwargs)
torch.utils.tensorboard._caffe2_graph.nets_to_graph_def(nets,shapes=None,**kwargs)
torch.utils.tensorboard._caffe2_graph.protos_to_graph_def(net_defs,shapes=None,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_proto_graph.py----------------------------------------
A:torch.utils.tensorboard._proto_graph.attr['attr']->AttrValue(s=s.encode(encoding='utf_8'))
A:torch.utils.tensorboard._proto_graph.shapeproto->tensor_shape_proto(shape)
A:torch.utils.tensorboard._proto_graph.attr['_output_shapes']->AttrValue(list=AttrValue.ListValue(shape=[shapeproto]))
torch.utils.tensorboard._proto_graph.attr_value_proto(dtype,shape,s)
torch.utils.tensorboard._proto_graph.node_proto(name,op='UnSpecified',input=None,dtype=None,shape=None,outputsize=None,attributes='')
torch.utils.tensorboard._proto_graph.tensor_shape_proto(outputsize)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_embedding.py----------------------------------------
A:torch.utils.tensorboard._embedding.metadata_bytes->tensorboard.compat.tf.compat.as_bytes('\n'.join(metadata) + '\n')
A:torch.utils.tensorboard._embedding.fs->tensorboard.compat.tf.io.gfile.get_filesystem(save_path)
A:torch.utils.tensorboard._embedding.nrow->int(math.ceil(label_img.size(0) ** 0.5))
A:torch.utils.tensorboard._embedding.arranged_img_CHW->make_grid(make_np(label_img), ncols=nrow)
A:torch.utils.tensorboard._embedding.arranged_augment_square_HWC->numpy.zeros((arranged_img_CHW.shape[2], arranged_img_CHW.shape[2], 3))
A:torch.utils.tensorboard._embedding.arranged_img_HWC->make_grid(make_np(label_img), ncols=nrow).transpose(1, 2, 0)
A:torch.utils.tensorboard._embedding.im->PIL.Image.fromarray(np.uint8((arranged_augment_square_HWC * 255).clip(0, 255)))
A:torch.utils.tensorboard._embedding.im_bytes->buf.getvalue()
A:torch.utils.tensorboard._embedding.info->EmbeddingInfo()
A:torch.utils.tensorboard._embedding.info.tensor_name->'{}:{}'.format(tag, str(global_step).zfill(5))
A:torch.utils.tensorboard._embedding.info.tensor_path->filesys.join(subdir, 'tensors.tsv')
A:torch.utils.tensorboard._embedding.info.metadata_path->filesys.join(subdir, 'metadata.tsv')
A:torch.utils.tensorboard._embedding.info.sprite.image_path->filesys.join(subdir, 'sprite.png')
A:torch.utils.tensorboard._embedding.config_path->tensorboard.compat.tf.io.gfile.get_filesystem(save_path).join(save_path, 'projector_config.pbtxt')
torch.utils.tensorboard._embedding.get_embedding_info(metadata,label_img,filesys,subdir,global_step,tag)
torch.utils.tensorboard._embedding.make_mat(matlist,save_path)
torch.utils.tensorboard._embedding.make_sprite(label_img,save_path)
torch.utils.tensorboard._embedding.make_tsv(metadata,save_path,metadata_header=None)
torch.utils.tensorboard._embedding.write_pbtxt(save_path,contents)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_pytorch_graph.py----------------------------------------
A:torch.utils.tensorboard._pytorch_graph.list_of_node->list(getattr(node_cpp, m)())
A:torch.utils.tensorboard._pytorch_graph.tensor_size->node_cpp.type().sizes()
A:torch.utils.tensorboard._pytorch_graph.self.attributes->str({k: node_cpp[k] for k in node_cpp.attributeNames()}).replace("'", ' ')
A:torch.utils.tensorboard._pytorch_graph.self.kind->node_cpp.kind()
A:torch.utils.tensorboard._pytorch_graph.self.nodes_io->OrderedDict()
A:torch.utils.tensorboard._pytorch_graph.self.nodes_io[node_output]->NodeBase(node_output, node.inputs, node.scopeName, outputSize, op_type=node.kind, attributes=node.attributes)
A:torch.utils.tensorboard._pytorch_graph.n_inputs->len(args)
A:torch.utils.tensorboard._pytorch_graph.nodes_py->GraphPy()
A:torch.utils.tensorboard._pytorch_graph.attr_name->node.s('name')
A:torch.utils.tensorboard._pytorch_graph.attr_key->node.output().debugName()
A:torch.utils.tensorboard._pytorch_graph.parent->node.input().node()
A:torch.utils.tensorboard._pytorch_graph.parent_attr_name->node.input().node().s('name')
A:torch.utils.tensorboard._pytorch_graph.parent_attr_key->node.input().node().output().debugName()
A:torch.utils.tensorboard._pytorch_graph.attr_to_scope[attr_key]->'__module.{}'.format(attr_name)
A:torch.utils.tensorboard._pytorch_graph.node_py->NodePyOP(node)
A:torch.utils.tensorboard._pytorch_graph.node_pyio->NodePyIO(node, 'output')
A:torch.utils.tensorboard._pytorch_graph.node_pyio.debugName->'output.{}'.format(i + 1)
A:torch.utils.tensorboard._pytorch_graph.module_name->getattr(module, 'original_name', 'Module')
A:torch.utils.tensorboard._pytorch_graph.alias_to_name->dict()
A:torch.utils.tensorboard._pytorch_graph.base_name->parse_traced_name(trace)
A:torch.utils.tensorboard._pytorch_graph.mod_name->parse_traced_name(module)
A:torch.utils.tensorboard._pytorch_graph.alias_to_name[name]->'{}[{}]'.format(mod_name, attr_name)
A:torch.utils.tensorboard._pytorch_graph.module_aliases->node.scopeName.split('/')
A:torch.utils.tensorboard._pytorch_graph.trace->torch.jit.trace(model, args, strict=use_strict_trace)
A:torch.utils.tensorboard._pytorch_graph.list_of_nodes->parse(graph, trace, args)
A:torch.utils.tensorboard._pytorch_graph.stepstats->RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device='/device:CPU:0')]))
torch.utils.tensorboard._pytorch_graph.GraphPy(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.__init__(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.append(self,x)
torch.utils.tensorboard._pytorch_graph.GraphPy.find_common_root(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.populate_namespace_from_OP_to_IO(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.printall(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.to_proto(self)
torch.utils.tensorboard._pytorch_graph.NodeBase(self,debugName=None,inputs=None,scope=None,tensor_size=None,op_type='UnSpecified',attributes='')
torch.utils.tensorboard._pytorch_graph.NodeBase.__init__(self,debugName=None,inputs=None,scope=None,tensor_size=None,op_type='UnSpecified',attributes='')
torch.utils.tensorboard._pytorch_graph.NodeBase.__repr__(self)
torch.utils.tensorboard._pytorch_graph.NodePy(self,node_cpp,valid_methods)
torch.utils.tensorboard._pytorch_graph.NodePy.__init__(self,node_cpp,valid_methods)
torch.utils.tensorboard._pytorch_graph.NodePyIO(self,node_cpp,input_or_output=None)
torch.utils.tensorboard._pytorch_graph.NodePyIO.__init__(self,node_cpp,input_or_output=None)
torch.utils.tensorboard._pytorch_graph.NodePyOP(self,node_cpp)
torch.utils.tensorboard._pytorch_graph.NodePyOP.__init__(self,node_cpp)
torch.utils.tensorboard._pytorch_graph.graph(model,args,verbose=False,use_strict_trace=True)
torch.utils.tensorboard._pytorch_graph.parse(graph,trace,args=None,omit_useless_nodes=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/summary.py----------------------------------------
A:torch.utils.tensorboard.summary.font->PIL.ImageFont.load_default()
A:torch.utils.tensorboard.summary.draw->PIL.ImageDraw.Draw(image)
A:torch.utils.tensorboard.summary.(text_width, text_height)->PIL.ImageFont.load_default().getsize(display_str)
A:torch.utils.tensorboard.summary.margin->numpy.ceil(0.05 * text_height)
A:torch.utils.tensorboard.summary.ssi->Summary(value=[Summary.Value(tag=SESSION_START_INFO_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.domain_discrete->google.protobuf.struct_pb2.ListValue(values=[struct_pb2.Value(bool_value=d) for d in hparam_domain_discrete[k]])
A:torch.utils.tensorboard.summary.content->HParamsPluginData(session_end_info=sei, version=PLUGIN_DATA_VERSION)
A:torch.utils.tensorboard.summary.smd->SummaryMetadata(plugin_data=plugin_data)
A:torch.utils.tensorboard.summary.exp->Summary(value=[Summary.Value(tag=EXPERIMENT_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.sei->Summary(value=[Summary.Value(tag=SESSION_END_INFO_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.scalar->float(scalar)
A:torch.utils.tensorboard.summary.tensor->TensorProto(dtype='DT_FLOAT', float_val=tensor.reshape(-1).tolist(), tensor_shape=TensorShapeProto(dim=[TensorShapeProto.Dim(size=tensor.shape[0]), TensorShapeProto.Dim(size=tensor.shape[1]), TensorShapeProto.Dim(size=tensor.shape[2])]))
A:torch.utils.tensorboard.summary.plugin_data->tensorboard.compat.proto.summary_pb2.SummaryMetadata.PluginData(plugin_name='pr_curves', content=pr_curve_plugin_data)
A:torch.utils.tensorboard.summary.hist->make_histogram(values.astype(float), bins, max_bins)
A:torch.utils.tensorboard.summary.values->values.reshape(-1).reshape(-1)
A:torch.utils.tensorboard.summary.(counts, limits)->numpy.histogram(values, bins=bins)
A:torch.utils.tensorboard.summary.num_bins->len(counts)
A:torch.utils.tensorboard.summary.counts->counts.reshape(-1, subsampling).sum(axis=-1).reshape(-1, subsampling).sum(axis=-1)
A:torch.utils.tensorboard.summary.new_limits->numpy.empty((counts.size + 1,), limits.dtype)
A:torch.utils.tensorboard.summary.cum_counts->numpy.cumsum(np.greater(counts, 0, dtype=np.int32))
A:torch.utils.tensorboard.summary.(start, end)->numpy.searchsorted(cum_counts, [0, cum_counts[-1] - 1], side='right')
A:torch.utils.tensorboard.summary.start->int(start)
A:torch.utils.tensorboard.summary.sum_sq->values.reshape(-1).reshape(-1).dot(values)
A:torch.utils.tensorboard.summary.scale_factor->_calc_scale_factor(tensor)
A:torch.utils.tensorboard.summary.image->image.resize((scaled_width, scaled_height), Image.ANTIALIAS).resize((scaled_width, scaled_height), Image.ANTIALIAS)
A:torch.utils.tensorboard.summary.tensor_image->convert_to_HWC(tensor_image, dataformats)
A:torch.utils.tensorboard.summary.tensor_boxes->make_np(tensor_boxes)
A:torch.utils.tensorboard.summary.list_gt->range(num_boxes)
A:torch.utils.tensorboard.summary.disp_image->_draw_single_box(disp_image, boxes[i, 0], boxes[i, 1], boxes[i, 2], boxes[i, 3], display_str=None if labels is None else labels[i], color='Red')
A:torch.utils.tensorboard.summary.scaled_height->int(height * rescale)
A:torch.utils.tensorboard.summary.scaled_width->int(width * rescale)
A:torch.utils.tensorboard.summary.output->io.BytesIO()
A:torch.utils.tensorboard.summary.image_string->io.BytesIO().getvalue()
A:torch.utils.tensorboard.summary.video->make_video(tensor, fps)
A:torch.utils.tensorboard.summary.clip->moviepy.editor.ImageSequenceClip(list(tensor), fps=fps)
A:torch.utils.tensorboard.summary.tensor_string->f.read()
A:torch.utils.tensorboard.summary.array->(array * np.iinfo(np.int16).max).astype('<i2')
A:torch.utils.tensorboard.summary.fio->io.BytesIO()
A:torch.utils.tensorboard.summary.wave_write->wave.open(fio, 'wb')
A:torch.utils.tensorboard.summary.audio_string->io.BytesIO().getvalue()
A:torch.utils.tensorboard.summary.audio->tensorboard.compat.proto.summary_pb2.Summary.Audio(sample_rate=sample_rate, num_channels=1, length_frames=array.shape[-1], encoded_audio_string=audio_string, content_type='audio/wav')
A:torch.utils.tensorboard.summary.mgcc->tensorboard.plugins.custom_scalar.layout_pb2.MarginChartContent(series=[layout_pb2.MarginChartContent.Series(value=tags[0], lower=tags[1], upper=tags[2])])
A:torch.utils.tensorboard.summary.chart->tensorboard.plugins.custom_scalar.layout_pb2.Chart(title=chart_name, multiline=mlcc)
A:torch.utils.tensorboard.summary.mlcc->tensorboard.plugins.custom_scalar.layout_pb2.MultilineChartContent(tag=tags)
A:torch.utils.tensorboard.summary.layout->tensorboard.plugins.custom_scalar.layout_pb2.Layout(category=categories)
A:torch.utils.tensorboard.summary.data->compute_curve(labels, predictions, num_thresholds=num_thresholds, weights=weights)
A:torch.utils.tensorboard.summary.pr_curve_plugin_data->PrCurvePluginData(version=0, num_thresholds=num_thresholds).SerializeToString()
A:torch.utils.tensorboard.summary.num_thresholds->min(num_thresholds, 127)
A:torch.utils.tensorboard.summary.bucket_indices->numpy.int32(np.floor(predictions * (num_thresholds - 1)))
A:torch.utils.tensorboard.summary.float_labels->labels.astype(np.float64)
A:torch.utils.tensorboard.summary.(tp_buckets, _)->numpy.histogram(bucket_indices, bins=num_thresholds, range=histogram_range, weights=float_labels * weights)
A:torch.utils.tensorboard.summary.(fp_buckets, _)->numpy.histogram(bucket_indices, bins=num_thresholds, range=histogram_range, weights=(1.0 - float_labels) * weights)
A:torch.utils.tensorboard.summary.tensor_metadata->tensorboard.plugins.mesh.metadata.create_summary_metadata(name, display_name, content_type, components, tensor.shape, description, json_config=json_config)
A:torch.utils.tensorboard.summary.tensor_summary->tensorboard.compat.proto.summary_pb2.Summary.Value(tag=metadata.get_instance_name(name, content_type), tensor=tensor, metadata=tensor_metadata)
A:torch.utils.tensorboard.summary.json_config->_get_json_config(config_dict)
A:torch.utils.tensorboard.summary.components->tensorboard.plugins.mesh.metadata.get_components_bitmask([content_type for (tensor, content_type) in tensors])
torch.utils.tensorboard.summary._calc_scale_factor(tensor)
torch.utils.tensorboard.summary._draw_single_box(image,xmin,ymin,xmax,ymax,display_str,color='black',color_text='black',thickness=2)
torch.utils.tensorboard.summary._get_json_config(config_dict)
torch.utils.tensorboard.summary._get_tensor_summary(name,display_name,description,tensor,content_type,components,json_config)
torch.utils.tensorboard.summary.audio(tag,tensor,sample_rate=44100)
torch.utils.tensorboard.summary.compute_curve(labels,predictions,num_thresholds=None,weights=None)
torch.utils.tensorboard.summary.custom_scalars(layout)
torch.utils.tensorboard.summary.draw_boxes(disp_image,boxes,labels=None)
torch.utils.tensorboard.summary.histogram(name,values,bins,max_bins=None)
torch.utils.tensorboard.summary.histogram_raw(name,min,max,num,sum,sum_squares,bucket_limits,bucket_counts)
torch.utils.tensorboard.summary.hparams(hparam_dict=None,metric_dict=None,hparam_domain_discrete=None)
torch.utils.tensorboard.summary.image(tag,tensor,rescale=1,dataformats='NCHW')
torch.utils.tensorboard.summary.image_boxes(tag,tensor_image,tensor_boxes,rescale=1,dataformats='CHW',labels=None)
torch.utils.tensorboard.summary.make_histogram(values,bins,max_bins=None)
torch.utils.tensorboard.summary.make_image(tensor,rescale=1,rois=None,labels=None)
torch.utils.tensorboard.summary.make_video(tensor,fps)
torch.utils.tensorboard.summary.mesh(tag,vertices,colors,faces,config_dict,display_name=None,description=None)
torch.utils.tensorboard.summary.pr_curve(tag,labels,predictions,num_thresholds=127,weights=None)
torch.utils.tensorboard.summary.pr_curve_raw(tag,tp,fp,tn,fn,precision,recall,num_thresholds=127,weights=None)
torch.utils.tensorboard.summary.scalar(name,scalar,collections=None,new_style=False,double_precision=False)
torch.utils.tensorboard.summary.text(tag,text)
torch.utils.tensorboard.summary.video(tag,tensor,fps=4)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/tensorboard/_onnx_graph.py----------------------------------------
A:torch.utils.tensorboard._onnx_graph.m->onnx.load(fname)
A:torch.utils.tensorboard._onnx_graph.shapeproto->TensorShapeProto(dim=[TensorShapeProto.Dim(size=d.dim_value) for d in node.type.tensor_type.shape.dim])
A:torch.utils.tensorboard._onnx_graph.attr->', '.join(_attr).encode(encoding='utf_8')
torch.utils.tensorboard._onnx_graph.load_onnx_graph(fname)
torch.utils.tensorboard._onnx_graph.parse(graph)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/bottleneck/__main__.py----------------------------------------
A:torch.utils.bottleneck.__main__.env_summary->run_env_analysis()
A:torch.utils.bottleneck.__main__.info->get_env_info()
A:torch.utils.bottleneck.__main__.prof->cProfile.Profile()
A:torch.utils.bottleneck.__main__.cprof_summary->'\n--------------------------------------------------------------------------------\n  cProfile output\n--------------------------------------------------------------------------------\n'.strip()
A:torch.utils.bottleneck.__main__.cprofile_stats->pstats.Stats(prof).sort_stats(sortby)
A:torch.utils.bottleneck.__main__.autograd_prof_summary->'\n--------------------------------------------------------------------------------\n  autograd profiler output ({mode} mode)\n--------------------------------------------------------------------------------\n        {description}\n{cuda_warning}\n{output}\n'.strip()
A:torch.utils.bottleneck.__main__.sorted_events->sorted(prof.function_events, key=lambda x: getattr(x, sortby), reverse=True)
A:torch.utils.bottleneck.__main__.descript->"\n`bottleneck` is a tool that can be used as an initial step for debugging\nbottlenecks in your program.\n\nIt summarizes runs of your script with the Python profiler and PyTorch's\nautograd profiler. Because your script will be profiled, please ensure that it\nexits in a finite amount of time.\n\nFor more complicated uses of the profilers, please see\nhttps://docs.python.org/3/library/profile.html and\nhttps://pytorch.org/docs/master/autograd.html#profiler for more information.\n".strip()
A:torch.utils.bottleneck.__main__.parser->argparse.ArgumentParser(description=descript)
A:torch.utils.bottleneck.__main__.args->parse_args()
A:torch.utils.bottleneck.__main__.code->compile(stream.read(), scriptfile, 'exec')
A:torch.utils.bottleneck.__main__.cprofile_prof->run_cprofile(code, globs)
A:torch.utils.bottleneck.__main__.(autograd_prof_cpu, autograd_prof_cuda)->run_autograd_prof(code, globs)
A:torch.utils.bottleneck.__main__.cuda_prof_exec_time->cpu_time_total(autograd_prof_cuda)
A:torch.utils.bottleneck.__main__.cpu_prof_exec_time->cpu_time_total(autograd_prof_cpu)
torch.utils.bottleneck.__main__.compiled_with_cuda(sysinfo)
torch.utils.bottleneck.__main__.cpu_time_total(autograd_prof)
torch.utils.bottleneck.__main__.main()
torch.utils.bottleneck.__main__.parse_args()
torch.utils.bottleneck.__main__.print_autograd_prof_summary(prof,mode,sortby='cpu_time',topk=15)
torch.utils.bottleneck.__main__.print_cprofile_summary(prof,sortby='tottime',topk=15)
torch.utils.bottleneck.__main__.redirect_argv(new_argv)
torch.utils.bottleneck.__main__.run_autograd_prof(code,globs)
torch.utils.bottleneck.__main__.run_cprofile(code,globs,launch_blocking=False)
torch.utils.bottleneck.__main__.run_env_analysis()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/bottleneck/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/backcompat/__init__.py----------------------------------------
A:torch.utils.backcompat.__init__.enabled->property(get_enabled, set_enabled)
A:torch.utils.backcompat.__init__.broadcast_warning->Warning(_set_backcompat_broadcast_warn, _get_backcompat_broadcast_warn)
A:torch.utils.backcompat.__init__.keepdim_warning->Warning(_set_backcompat_keepdim_warn, _get_backcompat_keepdim_warn)
torch.utils.backcompat.__init__.Warning(self,setter,getter)
torch.utils.backcompat.__init__.Warning.__init__(self,setter,getter)
torch.utils.backcompat.__init__.Warning.get_enabled(self)
torch.utils.backcompat.__init__.Warning.set_enabled(self,value)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/dataset.py----------------------------------------
A:torch.utils.data.dataset.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.dataset.T->TypeVar('T')
A:torch.utils.data.dataset.function->functools.partial(class_function, cls_to_register, enable_df_api_tracing)
A:torch.utils.data.dataset.result_pipe->result_pipe.trace_as_dataframe().trace_as_dataframe()
A:torch.utils.data.dataset.l->len(e)
A:torch.utils.data.dataset.self.datasets->list(datasets)
A:torch.utils.data.dataset.self.cumulative_sizes->self.cumsum(self.datasets)
A:torch.utils.data.dataset.dataset_idx->bisect.bisect_right(self.cumulative_sizes, idx)
A:torch.utils.data.dataset.indices->randperm(sum(lengths), generator=generator).tolist()
torch.utils.data.ChainDataset(self,datasets:Iterable[Dataset])
torch.utils.data.ChainDataset.__iter__(self)
torch.utils.data.ChainDataset.__len__(self)
torch.utils.data.ConcatDataset(self,datasets:Iterable[Dataset])
torch.utils.data.ConcatDataset.__getitem__(self,idx)
torch.utils.data.ConcatDataset.__len__(self)
torch.utils.data.ConcatDataset.cummulative_sizes(self)
torch.utils.data.ConcatDataset.cumsum(sequence)
torch.utils.data.DFIterDataPipe(IterDataPipe)
torch.utils.data.DFIterDataPipe._is_dfpipe(self)
torch.utils.data.DataChunk(self,items)
torch.utils.data.DataChunk.__iter__(self)->Iterator[T]
torch.utils.data.DataChunk.as_str(self,indent='')
torch.utils.data.DataChunk.raw_iterator(self)->T
torch.utils.data.Dataset(Generic[T_co])
torch.utils.data.Dataset.__add__(self,other:'Dataset[T_co]')->'ConcatDataset[T_co]'
torch.utils.data.Dataset.__getitem__(self,index)->T_co
torch.utils.data.IterDataPipe(IterableDataset[T_co],metaclass=_DataPipeMeta)
torch.utils.data.IterDataPipe.__getattr__(self,attribute_name)
torch.utils.data.IterDataPipe.__getstate__(self)
torch.utils.data.IterDataPipe.__reduce_ex__(self,*args,**kwargs)
torch.utils.data.IterDataPipe.register_datapipe_as_function(cls,function_name,cls_to_register,enable_df_api_tracing=False)
torch.utils.data.IterDataPipe.register_function(cls,function_name,function)
torch.utils.data.IterDataPipe.set_getstate_hook(cls,hook_fn)
torch.utils.data.IterDataPipe.set_reduce_ex_hook(cls,hook_fn)
torch.utils.data.IterableDataset(Dataset[T_co])
torch.utils.data.IterableDataset.__add__(self,other:Dataset[T_co])
torch.utils.data.IterableDataset.__iter__(self)->Iterator[T_co]
torch.utils.data.MapDataPipe(Dataset[T_co],metaclass=_DataPipeMeta)
torch.utils.data.MapDataPipe.__getattr__(self,attribute_name)
torch.utils.data.MapDataPipe.register_datapipe_as_function(cls,function_name,cls_to_register)
torch.utils.data.MapDataPipe.register_function(cls,function_name,function)
torch.utils.data.Subset(self,dataset:Dataset[T_co],indices:Sequence[int])
torch.utils.data.Subset.__getitem__(self,idx)
torch.utils.data.Subset.__len__(self)
torch.utils.data.TensorDataset(self,*tensors:Tensor)
torch.utils.data.TensorDataset.__getitem__(self,index)
torch.utils.data.TensorDataset.__len__(self)
torch.utils.data.dataset.ChainDataset(self,datasets:Iterable[Dataset])
torch.utils.data.dataset.ChainDataset.__init__(self,datasets:Iterable[Dataset])
torch.utils.data.dataset.ChainDataset.__iter__(self)
torch.utils.data.dataset.ChainDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset(self,datasets:Iterable[Dataset])
torch.utils.data.dataset.ConcatDataset.__getitem__(self,idx)
torch.utils.data.dataset.ConcatDataset.__init__(self,datasets:Iterable[Dataset])
torch.utils.data.dataset.ConcatDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset.cummulative_sizes(self)
torch.utils.data.dataset.ConcatDataset.cumsum(sequence)
torch.utils.data.dataset.DFIterDataPipe(IterDataPipe)
torch.utils.data.dataset.DFIterDataPipe._is_dfpipe(self)
torch.utils.data.dataset.DataChunk(self,items)
torch.utils.data.dataset.DataChunk.__init__(self,items)
torch.utils.data.dataset.DataChunk.__iter__(self)->Iterator[T]
torch.utils.data.dataset.DataChunk.as_str(self,indent='')
torch.utils.data.dataset.DataChunk.raw_iterator(self)->T
torch.utils.data.dataset.Dataset(Generic[T_co])
torch.utils.data.dataset.Dataset.__add__(self,other:'Dataset[T_co]')->'ConcatDataset[T_co]'
torch.utils.data.dataset.Dataset.__getitem__(self,index)->T_co
torch.utils.data.dataset.IterDataPipe(IterableDataset[T_co],metaclass=_DataPipeMeta)
torch.utils.data.dataset.IterDataPipe.__getattr__(self,attribute_name)
torch.utils.data.dataset.IterDataPipe.__getstate__(self)
torch.utils.data.dataset.IterDataPipe.__reduce_ex__(self,*args,**kwargs)
torch.utils.data.dataset.IterDataPipe.register_datapipe_as_function(cls,function_name,cls_to_register,enable_df_api_tracing=False)
torch.utils.data.dataset.IterDataPipe.register_function(cls,function_name,function)
torch.utils.data.dataset.IterDataPipe.set_getstate_hook(cls,hook_fn)
torch.utils.data.dataset.IterDataPipe.set_reduce_ex_hook(cls,hook_fn)
torch.utils.data.dataset.IterableDataset(Dataset[T_co])
torch.utils.data.dataset.IterableDataset.__add__(self,other:Dataset[T_co])
torch.utils.data.dataset.IterableDataset.__iter__(self)->Iterator[T_co]
torch.utils.data.dataset.MapDataPipe(Dataset[T_co],metaclass=_DataPipeMeta)
torch.utils.data.dataset.MapDataPipe.__getattr__(self,attribute_name)
torch.utils.data.dataset.MapDataPipe.register_datapipe_as_function(cls,function_name,cls_to_register)
torch.utils.data.dataset.MapDataPipe.register_function(cls,function_name,function)
torch.utils.data.dataset.Subset(self,dataset:Dataset[T_co],indices:Sequence[int])
torch.utils.data.dataset.Subset.__getitem__(self,idx)
torch.utils.data.dataset.Subset.__init__(self,dataset:Dataset[T_co],indices:Sequence[int])
torch.utils.data.dataset.Subset.__len__(self)
torch.utils.data.dataset.TensorDataset(self,*tensors:Tensor)
torch.utils.data.dataset.TensorDataset.__getitem__(self,index)
torch.utils.data.dataset.TensorDataset.__init__(self,*tensors:Tensor)
torch.utils.data.dataset.TensorDataset.__len__(self)
torch.utils.data.dataset.random_split(dataset:Dataset[T],lengths:Sequence[int],generator:Optional[Generator]=default_generator)->List[Subset[T]]
torch.utils.data.random_split(dataset:Dataset[T],lengths:Sequence[int],generator:Optional[Generator]=default_generator)->List[Subset[T]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/dataset.pyi----------------------------------------
torch.utils.data.IterDataPipe.batch(self,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)->IterDataPipe
torch.utils.data.IterDataPipe.collate(self,collate_fn:Callable=...)->IterDataPipe
torch.utils.data.IterDataPipe.concat(self,*datapipes:IterDataPipe)->IterDataPipe
torch.utils.data.IterDataPipe.demux(self,num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool=False,buffer_size:int=1000)->List[IterDataPipe]
torch.utils.data.IterDataPipe.filter(self,filter_fn:Callable,drop_empty_batches:bool=True)->IterDataPipe
torch.utils.data.IterDataPipe.fork(self,num_instances:int,buffer_size:int=1000)->List[IterDataPipe]
torch.utils.data.IterDataPipe.groupby(self,group_key_fn:Callable,*,buffer_size:int=10000,group_size:Optional[int]=None,guaranteed_group_size:Optional[int]=None,drop_remaining:bool=False)->IterDataPipe
torch.utils.data.IterDataPipe.map(self,fn:Callable,input_col=None,output_col=None)->IterDataPipe
torch.utils.data.IterDataPipe.mux(self,*datapipes)->IterDataPipe
torch.utils.data.IterDataPipe.routed_decode(self,*handlers:Callable,key_fn:Callable=...)->IterDataPipe
torch.utils.data.IterDataPipe.sharding_filter(self)->IterDataPipe
torch.utils.data.IterDataPipe.shuffle(self,*,default:bool=True,buffer_size:int=10000,unbatch_level:int=0)->IterDataPipe
torch.utils.data.IterDataPipe.unbatch(self,unbatch_level:int=1)->IterDataPipe
torch.utils.data.IterDataPipe.zip(self,*datapipes:IterDataPipe)->IterDataPipe
torch.utils.data.MapDataPipe.batch(self,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)->MapDataPipe
torch.utils.data.MapDataPipe.concat(self,*datapipes:MapDataPipe)->MapDataPipe
torch.utils.data.MapDataPipe.map(self,fn:Callable=...)->MapDataPipe
torch.utils.data.MapDataPipe.shuffle(self,*,indices:Optional[List]=None)->MapDataPipe
torch.utils.data.MapDataPipe.zip(self,*datapipes:MapDataPipe[T_co])->MapDataPipe
torch.utils.data.dataset.IterDataPipe.batch(self,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.collate(self,collate_fn:Callable=...)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.concat(self,*datapipes:IterDataPipe)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.demux(self,num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool=False,buffer_size:int=1000)->List[IterDataPipe]
torch.utils.data.dataset.IterDataPipe.filter(self,filter_fn:Callable,drop_empty_batches:bool=True)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.fork(self,num_instances:int,buffer_size:int=1000)->List[IterDataPipe]
torch.utils.data.dataset.IterDataPipe.groupby(self,group_key_fn:Callable,*,buffer_size:int=10000,group_size:Optional[int]=None,guaranteed_group_size:Optional[int]=None,drop_remaining:bool=False)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.map(self,fn:Callable,input_col=None,output_col=None)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.mux(self,*datapipes)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.routed_decode(self,*handlers:Callable,key_fn:Callable=...)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.sharding_filter(self)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.shuffle(self,*,default:bool=True,buffer_size:int=10000,unbatch_level:int=0)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.unbatch(self,unbatch_level:int=1)->IterDataPipe
torch.utils.data.dataset.IterDataPipe.zip(self,*datapipes:IterDataPipe)->IterDataPipe
torch.utils.data.dataset.MapDataPipe.batch(self,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)->MapDataPipe
torch.utils.data.dataset.MapDataPipe.concat(self,*datapipes:MapDataPipe)->MapDataPipe
torch.utils.data.dataset.MapDataPipe.map(self,fn:Callable=...)->MapDataPipe
torch.utils.data.dataset.MapDataPipe.shuffle(self,*,indices:Optional[List]=None)->MapDataPipe
torch.utils.data.dataset.MapDataPipe.zip(self,*datapipes:MapDataPipe[T_co])->MapDataPipe


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_typing.py----------------------------------------
A:torch.utils.data._typing.left->TYPE2ABC.get(left, left)
A:torch.utils.data._typing.right->TYPE2ABC.get(right, right)
A:torch.utils.data._typing.constraints->_decompose_type(right)
A:torch.utils.data._typing.variants->_decompose_type(left)
A:torch.utils.data._typing.ts->list((TYPE2ABC.get(_t, _t) for _t in ts))
A:torch.utils.data._typing.vs->_decompose_type(variant, to_list=False)
A:torch.utils.data._typing.v_args->getattr(variant, '__args__', None)
A:torch.utils.data._typing.cs->_decompose_type(constraint, to_list=False)
A:torch.utils.data._typing.c_args->getattr(constraint, '__args__', None)
A:torch.utils.data._typing.dt_args->getattr(data_type, '__args__', None)
A:torch.utils.data._typing.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data._typing._DEFAULT_TYPE->_DataPipeType(Generic[T_co])
A:torch.utils.data._typing.params->tuple((_type_check(p, msg) for p in params))
A:torch.utils.data._typing.msg->'{}[t]: t must be a type'.format(self.__name__)
A:torch.utils.data._typing.orig->getattr(self.type.param, '__origin__', None)
A:torch.utils.data._typing.t->_DataPipeType(params[0])
A:torch.utils.data._typing.gen->func(*args, **kwargs)
A:torch.utils.data._typing.response->func(*args, **kwargs).send(request)
A:torch.utils.data._typing.iter_ret->func(*args, **kwargs)
A:torch.utils.data._typing.param->_eval_type(sub_cls.type.param, base_globals, locals())
A:torch.utils.data._typing.hints->get_type_hints(iter_fn)
A:torch.utils.data._typing.self.type->_DataPipeType(expected_type)
torch.utils.data._typing.Boolean(numbers.Integral)
torch.utils.data._typing.GenericMeta(ABCMeta)
torch.utils.data._typing.Integer(numbers.Integral)
torch.utils.data._typing._DataPipeMeta(self,name,bases,namespace,**kwargs)
torch.utils.data._typing._DataPipeMeta.__init__(self,name,bases,namespace,**kwargs)
torch.utils.data._typing._DataPipeMeta._eq_(self,other)
torch.utils.data._typing._DataPipeMeta._getitem_(self,params)
torch.utils.data._typing._DataPipeMeta._hash_(self)
torch.utils.data._typing._DataPipeType(self,param)
torch.utils.data._typing._DataPipeType.__eq__(self,other)
torch.utils.data._typing._DataPipeType.__hash__(self)
torch.utils.data._typing._DataPipeType.__init__(self,param)
torch.utils.data._typing._DataPipeType.__repr__(self)
torch.utils.data._typing._DataPipeType.issubtype(self,other)
torch.utils.data._typing._DataPipeType.issubtype_of_instance(self,other)
torch.utils.data._typing._decompose_type(t,to_list=True)
torch.utils.data._typing._dp_init_subclass(sub_cls,*args,**kwargs)
torch.utils.data._typing._issubtype_with_constraints(variant,constraints,recursive=True)
torch.utils.data._typing.hook_iterator(namespace,profile_name)
torch.utils.data._typing.issubinstance(data,data_type)
torch.utils.data._typing.issubtype(left,right,recursive=True)
torch.utils.data._typing.reinforce_type(self,expected_type)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/gen_pyi.py----------------------------------------
A:torch.utils.data.gen_pyi.all_files->os.listdir(dir_path)
A:torch.utils.data.gen_pyi.method_name->extract_method_name(line)
A:torch.utils.data.gen_pyi.class_name->extract_class_name(line)
A:torch.utils.data.gen_pyi.end->', '.join(tokens).rfind(')')
A:torch.utils.data.gen_pyi.method_to_signature[method_name]->process_signature(signature)
A:torch.utils.data.gen_pyi.(method_to_signature, method_to_class_name, methods_needing_special_output_types)->parse_datapipe_file(path)
A:torch.utils.data.gen_pyi.tokens[i]->token.strip(' ')
A:torch.utils.data.gen_pyi.(head, default_arg)->token.rsplit('=', 2)
A:torch.utils.data.gen_pyi.line->', '.join(tokens)
A:torch.utils.data.gen_pyi.file_paths->find_file_paths([file_path], files_to_exclude=files_to_exclude.union(deprecated_files))
A:torch.utils.data.gen_pyi.(methods_and_signatures, methods_and_class_names, methods_w_special_output_types)->parse_datapipe_files(file_paths)
A:torch.utils.data.gen_pyi.iter_method_definitions->get_method_definitions(iterDP_file_path, iterDP_files_to_exclude, iterDP_deprecated_files, 'IterDataPipe', iterDP_method_to_special_output_type)
A:torch.utils.data.gen_pyi.map_method_definitions->get_method_definitions(mapDP_file_path, mapDP_files_to_exclude, mapDP_deprecated_files, 'MapDataPipe', mapDP_method_to_special_output_type)
A:torch.utils.data.gen_pyi.fm->FileManager(install_dir='.', template_dir='.', dry_run=False)
torch.utils.data.gen_pyi.extract_class_name(line:str)->str
torch.utils.data.gen_pyi.extract_method_name(line:str)->str
torch.utils.data.gen_pyi.find_file_paths(dir_paths:List[str],files_to_exclude:Set[str])->Set[str]
torch.utils.data.gen_pyi.get_method_definitions(file_path:str,files_to_exclude:Set[str],deprecated_files:Set[str],default_output_type:str,method_to_special_output_type:Dict[str,str])->List[str]
torch.utils.data.gen_pyi.main()->None
torch.utils.data.gen_pyi.parse_datapipe_file(file_path:str)->Tuple[Dict[str, str], Dict[str, str], Set[str]]
torch.utils.data.gen_pyi.parse_datapipe_files(file_paths:Set[str])->Tuple[Dict[str, str], Dict[str, str], Set[str]]
torch.utils.data.gen_pyi.process_signature(line:str)->str
torch.utils.data.gen_pyi.split_outside_bracket(line:str,delimiter:str=',')->List[str]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_decorator.py----------------------------------------
A:torch.utils.data._decorator.res->self.deterministic_fn(*args, **kwargs)
A:torch.utils.data._decorator.signature->inspect.signature(f)
A:torch.utils.data._decorator.hints->get_type_hints(f)
A:torch.utils.data._decorator.bound->inspect.signature(f).bind(*args, **kwargs)
A:torch.utils.data._decorator.it->f(self)
torch.utils.data._decorator.argument_validation(f)
torch.utils.data._decorator.functional_datapipe(self,name:str,enable_df_api_tracing=False)
torch.utils.data._decorator.functional_datapipe.__init__(self,name:str,enable_df_api_tracing=False)
torch.utils.data._decorator.guaranteed_datapipes_determinism(self)
torch.utils.data._decorator.guaranteed_datapipes_determinism.__enter__(self)->None
torch.utils.data._decorator.guaranteed_datapipes_determinism.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.utils.data._decorator.guaranteed_datapipes_determinism.__init__(self)
torch.utils.data._decorator.non_deterministic(self,arg:Union[Type[IterDataPipe],Callable[[],bool]])
torch.utils.data._decorator.non_deterministic.__init__(self,arg:Union[Type[IterDataPipe],Callable[[],bool]])
torch.utils.data._decorator.non_deterministic.deterministic_wrapper_fn(self,*args,**kwargs)->IterDataPipe
torch.utils.data._decorator.runtime_validation(f)
torch.utils.data._decorator.runtime_validation_disabled(self)
torch.utils.data._decorator.runtime_validation_disabled.__enter__(self)->None
torch.utils.data._decorator.runtime_validation_disabled.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.utils.data._decorator.runtime_validation_disabled.__init__(self)
torch.utils.data.argument_validation(f)
torch.utils.data.functional_datapipe(self,name:str,enable_df_api_tracing=False)
torch.utils.data.guaranteed_datapipes_determinism(self)
torch.utils.data.guaranteed_datapipes_determinism.__enter__(self)->None
torch.utils.data.guaranteed_datapipes_determinism.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.utils.data.non_deterministic(self,arg:Union[Type[IterDataPipe],Callable[[],bool]])
torch.utils.data.non_deterministic.deterministic_wrapper_fn(self,*args,**kwargs)->IterDataPipe
torch.utils.data.runtime_validation(f)
torch.utils.data.runtime_validation_disabled(self)
torch.utils.data.runtime_validation_disabled.__enter__(self)->None
torch.utils.data.runtime_validation_disabled.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/backward_compatibility.py----------------------------------------
A:torch.utils.data.backward_compatibility.info->torch.utils.data.get_worker_info()
torch.utils.data.backward_compatibility.worker_init_fn(worker_id)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/distributed.py----------------------------------------
A:torch.utils.data.distributed.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.distributed.num_replicas->torch.distributed.get_world_size()
A:torch.utils.data.distributed.rank->torch.distributed.get_rank()
A:torch.utils.data.distributed.self.num_samples->math.ceil(len(self.dataset) / self.num_replicas)
A:torch.utils.data.distributed.g->torch.Generator()
A:torch.utils.data.distributed.indices->list(range(len(self.dataset)))
torch.utils.data.DistributedSampler(self,dataset:Dataset,num_replicas:Optional[int]=None,rank:Optional[int]=None,shuffle:bool=True,seed:int=0,drop_last:bool=False)
torch.utils.data.DistributedSampler.__iter__(self)->Iterator[T_co]
torch.utils.data.DistributedSampler.__len__(self)->int
torch.utils.data.DistributedSampler.set_epoch(self,epoch:int)->None
torch.utils.data.distributed.DistributedSampler(self,dataset:Dataset,num_replicas:Optional[int]=None,rank:Optional[int]=None,shuffle:bool=True,seed:int=0,drop_last:bool=False)
torch.utils.data.distributed.DistributedSampler.__init__(self,dataset:Dataset,num_replicas:Optional[int]=None,rank:Optional[int]=None,shuffle:bool=True,seed:int=0,drop_last:bool=False)
torch.utils.data.distributed.DistributedSampler.__iter__(self)->Iterator[T_co]
torch.utils.data.distributed.DistributedSampler.__len__(self)->int
torch.utils.data.distributed.DistributedSampler.set_epoch(self,epoch:int)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/dataloader_experimental.py----------------------------------------
A:torch.utils.data.dataloader_experimental.(thread, req_queue, res_queue, thread_localdatapipe)->torch.utils.data.communication.eventloop.SpawnThreadForDataPipeline(datapipe)
A:torch.utils.data.dataloader_experimental.local_datapipe->torch.utils.data.communication.iter.QueueWrapper(communication.protocol.IterDataPipeQueueProtocolClient(req_queue, res_queue))
A:torch.utils.data.dataloader_experimental.value->dp.nonblocking_next()
A:torch.utils.data.dataloader_experimental._->res_queue.get()
A:torch.utils.data.dataloader_experimental.datapipe->IterableWrapper(data_loader).batch(batch_size, drop_last=drop_last).map(collate_fn)
A:torch.utils.data.dataloader_experimental.my_worker_init_fn->functools.partial(_sharding_worker_init_fn, worker_init_fn)
A:torch.utils.data.dataloader_experimental.data_loader->_ThreadingDataLoader2(datapipe, num_workers=num_workers, collate_fn=collate_fn)
torch.utils.data.DataLoader2(cls,dataset,batch_size=1,shuffle=None,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,*,prefetch_factor=2,persistent_workers=False,batch_outside_worker=False,parallelism_mode='mp')
torch.utils.data.dataloader_experimental.DataLoader2(cls,dataset,batch_size=1,shuffle=None,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,*,prefetch_factor=2,persistent_workers=False,batch_outside_worker=False,parallelism_mode='mp')
torch.utils.data.dataloader_experimental.DataLoader2.__new__(cls,dataset,batch_size=1,shuffle=None,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,*,prefetch_factor=2,persistent_workers=False,batch_outside_worker=False,parallelism_mode='mp')
torch.utils.data.dataloader_experimental._ThreadingDataLoader2(self,datapipe,num_workers=0,collate_fn=None)
torch.utils.data.dataloader_experimental._ThreadingDataLoader2.__del__(self)
torch.utils.data.dataloader_experimental._ThreadingDataLoader2.__init__(self,datapipe,num_workers=0,collate_fn=None)
torch.utils.data.dataloader_experimental._ThreadingDataLoader2.__iter__(self)
torch.utils.data.dataloader_experimental._ThreadingDataLoader2._cleanup_all_threads(self)
torch.utils.data.dataloader_experimental._sharding_worker_init_fn(worker_init_fn,worker_id)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/dataloader.py----------------------------------------
A:torch.utils.data.dataloader.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.dataloader.T->TypeVar('T')
A:torch.utils.data.dataloader.sampler->SequentialSampler(dataset)
A:torch.utils.data.dataloader.batch_sampler->BatchSampler(sampler, batch_size, drop_last)
A:torch.utils.data.dataloader.valid_start_methods->torch.multiprocessing.get_all_start_methods()
A:torch.utils.data.dataloader.multiprocessing_context->torch.multiprocessing.get_context(multiprocessing_context)
A:torch.utils.data.dataloader.self._iterator->self._get_iterator()
A:torch.utils.data.dataloader.lengthself._IterableDataset_len_called->len(self.dataset)
A:torch.utils.data.dataloader.length->ceil(length / self.batch_size)
A:torch.utils.data.dataloader.warn_msg->'Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} samples have been fetched. '.format(self._dataset, self._IterableDataset_len_called, self._num_yielded)
A:torch.utils.data.dataloader.max_num_worker_suggest->len(os.sched_getaffinity(0))
A:torch.utils.data.dataloader.cpu_count->os.cpu_count()
A:torch.utils.data.dataloader.self._sampler_iter->iter(self._index_sampler)
A:torch.utils.data.dataloader.self._base_seed->torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()
A:torch.utils.data.dataloader.self._profile_name->'enumerate(DataLoader)#{}.__next__'.format(self.__class__.__name__)
A:torch.utils.data.dataloader.data->self._data_queue.get(timeout=timeout)
A:torch.utils.data.dataloader.self._dataset_fetcher->_DatasetKind.create_fetcher(self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)
A:torch.utils.data.dataloader.index->self._next_index()
A:torch.utils.data.dataloader.self._worker_queue_idx_cycle->itertools.cycle(range(self._num_workers))
A:torch.utils.data.dataloader.self._worker_result_queue->torch.multiprocessing.get_context(multiprocessing_context).Queue()
A:torch.utils.data.dataloader.self._workers_done_event->torch.multiprocessing.get_context(multiprocessing_context).Event()
A:torch.utils.data.dataloader.index_queue->torch.multiprocessing.get_context(multiprocessing_context).Queue()
A:torch.utils.data.dataloader.w->torch.multiprocessing.get_context(multiprocessing_context).Process(target=_utils.worker._worker_loop, args=(self._dataset_kind, self._dataset, index_queue, self._worker_result_queue, self._workers_done_event, self._auto_collation, self._collate_fn, self._drop_last, self._base_seed, self._worker_init_fn, i, self._num_workers, self._persistent_workers))
A:torch.utils.data.dataloader.self._pin_memory_thread_done_event->threading.Event()
A:torch.utils.data.dataloader.self._data_queue->queue.Queue()
A:torch.utils.data.dataloader.pin_memory_thread->threading.Thread(target=_utils.pin_memory._pin_memory_loop, args=(self._worker_result_queue, self._data_queue, torch.cuda.current_device(), self._pin_memory_thread_done_event))
A:torch.utils.data.dataloader.(return_idx, return_data)->self._get_data()
A:torch.utils.data.dataloader.pids_str->', '.join((str(w.pid) for w in failed_workers))
A:torch.utils.data.dataloader.(success, data)->self._try_get_data()
A:torch.utils.data.dataloader.(idx, data)->self._get_data()
A:torch.utils.data.dataloader.worker_queue_idx->next(self._worker_queue_idx_cycle)
torch.utils.data.DataLoader(self,dataset:Dataset[T_co],batch_size:Optional[int]=1,shuffle:bool=False,sampler:Union[Sampler,Iterable,None]=None,batch_sampler:Union[Sampler[Sequence],Iterable[Sequence],None]=None,num_workers:int=0,collate_fn:Optional[_collate_fn_t]=None,pin_memory:bool=False,drop_last:bool=False,timeout:float=0,worker_init_fn:Optional[_worker_init_fn_t]=None,multiprocessing_context=None,generator=None,*,prefetch_factor:int=2,persistent_workers:bool=False)
torch.utils.data.DataLoader.__iter__(self)->'_BaseDataLoaderIter'
torch.utils.data.DataLoader.__len__(self)->int
torch.utils.data.DataLoader.__setattr__(self,attr,val)
torch.utils.data.DataLoader._auto_collation(self)
torch.utils.data.DataLoader._get_iterator(self)->'_BaseDataLoaderIter'
torch.utils.data.DataLoader._index_sampler(self)
torch.utils.data.DataLoader.check_worker_number_rationality(self)
torch.utils.data.DataLoader.multiprocessing_context(self)
torch.utils.data.DataLoader.multiprocessing_context(self,multiprocessing_context)
torch.utils.data._DatasetKind(object)
torch.utils.data._DatasetKind.create_fetcher(kind,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data.dataloader.DataLoader(self,dataset:Dataset[T_co],batch_size:Optional[int]=1,shuffle:bool=False,sampler:Union[Sampler,Iterable,None]=None,batch_sampler:Union[Sampler[Sequence],Iterable[Sequence],None]=None,num_workers:int=0,collate_fn:Optional[_collate_fn_t]=None,pin_memory:bool=False,drop_last:bool=False,timeout:float=0,worker_init_fn:Optional[_worker_init_fn_t]=None,multiprocessing_context=None,generator=None,*,prefetch_factor:int=2,persistent_workers:bool=False)
torch.utils.data.dataloader.DataLoader.__init__(self,dataset:Dataset[T_co],batch_size:Optional[int]=1,shuffle:bool=False,sampler:Union[Sampler,Iterable,None]=None,batch_sampler:Union[Sampler[Sequence],Iterable[Sequence],None]=None,num_workers:int=0,collate_fn:Optional[_collate_fn_t]=None,pin_memory:bool=False,drop_last:bool=False,timeout:float=0,worker_init_fn:Optional[_worker_init_fn_t]=None,multiprocessing_context=None,generator=None,*,prefetch_factor:int=2,persistent_workers:bool=False)
torch.utils.data.dataloader.DataLoader.__iter__(self)->'_BaseDataLoaderIter'
torch.utils.data.dataloader.DataLoader.__len__(self)->int
torch.utils.data.dataloader.DataLoader.__setattr__(self,attr,val)
torch.utils.data.dataloader.DataLoader._auto_collation(self)
torch.utils.data.dataloader.DataLoader._get_iterator(self)->'_BaseDataLoaderIter'
torch.utils.data.dataloader.DataLoader._index_sampler(self)
torch.utils.data.dataloader.DataLoader.check_worker_number_rationality(self)
torch.utils.data.dataloader.DataLoader.multiprocessing_context(self)
torch.utils.data.dataloader.DataLoader.multiprocessing_context(self,multiprocessing_context)
torch.utils.data.dataloader._BaseDataLoaderIter(self,loader:DataLoader)
torch.utils.data.dataloader._BaseDataLoaderIter.__getstate__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__init__(self,loader:DataLoader)
torch.utils.data.dataloader._BaseDataLoaderIter.__iter__(self)->'_BaseDataLoaderIter'
torch.utils.data.dataloader._BaseDataLoaderIter.__len__(self)->int
torch.utils.data.dataloader._BaseDataLoaderIter.__next__(self)->Any
torch.utils.data.dataloader._BaseDataLoaderIter._next_data(self)
torch.utils.data.dataloader._BaseDataLoaderIter._next_index(self)
torch.utils.data.dataloader._BaseDataLoaderIter._reset(self,loader,first_iter=False)
torch.utils.data.dataloader._DatasetKind(object)
torch.utils.data.dataloader._DatasetKind.create_fetcher(kind,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data.dataloader._InfiniteConstantSampler(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__init__(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__iter__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter(self,loader)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__del__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._clean_up_worker(w)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._get_data(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._mark_worker_as_unavailable(self,worker_id,shutdown=False)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._next_data(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._process_data(self,data)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._reset(self,loader,first_iter=False)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._shutdown_workers(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._try_get_data(self,timeout=_utils.MP_STATUS_CHECK_INTERVAL)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._try_put_index(self)
torch.utils.data.dataloader._SingleProcessDataLoaderIter(self,loader)
torch.utils.data.dataloader._SingleProcessDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._SingleProcessDataLoaderIter._next_data(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/sampler.py----------------------------------------
A:torch.utils.data.sampler.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.sampler.n->len(self.data_source)
A:torch.utils.data.sampler.seed->int(torch.empty((), dtype=torch.int64).random_().item())
A:torch.utils.data.sampler.generator->torch.Generator()
A:torch.utils.data.sampler.self.weights->torch.as_tensor(weights, dtype=torch.double)
A:torch.utils.data.sampler.rand_tensor->torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)
torch.utils.data.BatchSampler(self,sampler:Union[Sampler[int],Iterable[int]],batch_size:int,drop_last:bool)
torch.utils.data.BatchSampler.__iter__(self)->Iterator[List[int]]
torch.utils.data.BatchSampler.__len__(self)->int
torch.utils.data.RandomSampler(self,data_source:Sized,replacement:bool=False,num_samples:Optional[int]=None,generator=None)
torch.utils.data.RandomSampler.__iter__(self)->Iterator[int]
torch.utils.data.RandomSampler.__len__(self)->int
torch.utils.data.RandomSampler.num_samples(self)->int
torch.utils.data.Sampler(self,data_source:Optional[Sized])
torch.utils.data.Sampler.__iter__(self)->Iterator[T_co]
torch.utils.data.SequentialSampler(self,data_source:Sized)
torch.utils.data.SequentialSampler.__iter__(self)->Iterator[int]
torch.utils.data.SequentialSampler.__len__(self)->int
torch.utils.data.SubsetRandomSampler(self,indices:Sequence[int],generator=None)
torch.utils.data.SubsetRandomSampler.__iter__(self)->Iterator[int]
torch.utils.data.SubsetRandomSampler.__len__(self)->int
torch.utils.data.WeightedRandomSampler(self,weights:Sequence[float],num_samples:int,replacement:bool=True,generator=None)
torch.utils.data.WeightedRandomSampler.__iter__(self)->Iterator[int]
torch.utils.data.WeightedRandomSampler.__len__(self)->int
torch.utils.data.sampler.BatchSampler(self,sampler:Union[Sampler[int],Iterable[int]],batch_size:int,drop_last:bool)
torch.utils.data.sampler.BatchSampler.__init__(self,sampler:Union[Sampler[int],Iterable[int]],batch_size:int,drop_last:bool)
torch.utils.data.sampler.BatchSampler.__iter__(self)->Iterator[List[int]]
torch.utils.data.sampler.BatchSampler.__len__(self)->int
torch.utils.data.sampler.RandomSampler(self,data_source:Sized,replacement:bool=False,num_samples:Optional[int]=None,generator=None)
torch.utils.data.sampler.RandomSampler.__init__(self,data_source:Sized,replacement:bool=False,num_samples:Optional[int]=None,generator=None)
torch.utils.data.sampler.RandomSampler.__iter__(self)->Iterator[int]
torch.utils.data.sampler.RandomSampler.__len__(self)->int
torch.utils.data.sampler.RandomSampler.num_samples(self)->int
torch.utils.data.sampler.Sampler(self,data_source:Optional[Sized])
torch.utils.data.sampler.Sampler.__init__(self,data_source:Optional[Sized])
torch.utils.data.sampler.Sampler.__iter__(self)->Iterator[T_co]
torch.utils.data.sampler.SequentialSampler(self,data_source:Sized)
torch.utils.data.sampler.SequentialSampler.__init__(self,data_source:Sized)
torch.utils.data.sampler.SequentialSampler.__iter__(self)->Iterator[int]
torch.utils.data.sampler.SequentialSampler.__len__(self)->int
torch.utils.data.sampler.SubsetRandomSampler(self,indices:Sequence[int],generator=None)
torch.utils.data.sampler.SubsetRandomSampler.__init__(self,indices:Sequence[int],generator=None)
torch.utils.data.sampler.SubsetRandomSampler.__iter__(self)->Iterator[int]
torch.utils.data.sampler.SubsetRandomSampler.__len__(self)->int
torch.utils.data.sampler.WeightedRandomSampler(self,weights:Sequence[float],num_samples:int,replacement:bool=True,generator=None)
torch.utils.data.sampler.WeightedRandomSampler.__init__(self,weights:Sequence[float],num_samples:int,replacement:bool=True,generator=None)
torch.utils.data.sampler.WeightedRandomSampler.__iter__(self)->Iterator[int]
torch.utils.data.sampler.WeightedRandomSampler.__len__(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/graph.py----------------------------------------
A:torch.utils.data.graph.f->io.BytesIO()
A:torch.utils.data.graph.p->pickle.Pickler(f)
A:torch.utils.data.graph.items->list_connected_datapipes(datapipe, only_datapipe)
torch.utils.data.graph.list_connected_datapipes(scan_obj,only_datapipe)
torch.utils.data.graph.stub_unpickler()
torch.utils.data.graph.traverse(datapipe,only_datapipe=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/graph_settings.py----------------------------------------
A:torch.utils.data.graph_settings.results->set()
A:torch.utils.data.graph_settings.sub_items->get_all_graph_pipes(sub_graph)
A:torch.utils.data.graph_settings.graph->torch.utils.data.graph.traverse(datapipe, only_datapipe=True)
A:torch.utils.data.graph_settings.all_pipes->get_all_graph_pipes(graph)
torch.utils.data.graph_settings.apply_sharding(datapipe,num_of_instances,instance_id)
torch.utils.data.graph_settings.apply_shuffle_settings(datapipe,shuffle)
torch.utils.data.graph_settings.get_all_graph_pipes(graph)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/messages.py----------------------------------------
torch.utils.data.communication.messages.DataLoaderQueueMessage(object)
torch.utils.data.communication.messages.GetItemRequest(self,key)
torch.utils.data.communication.messages.GetItemRequest.__init__(self,key)
torch.utils.data.communication.messages.GetItemResponse(self,key,value)
torch.utils.data.communication.messages.GetItemResponse.__init__(self,key,value)
torch.utils.data.communication.messages.GetNextRequest(Request)
torch.utils.data.communication.messages.GetNextResponse(self,value)
torch.utils.data.communication.messages.GetNextResponse.__init__(self,value)
torch.utils.data.communication.messages.InvalidStateResponse(Response)
torch.utils.data.communication.messages.LenRequest(Request)
torch.utils.data.communication.messages.LenResponse(self,len)
torch.utils.data.communication.messages.LenResponse.__init__(self,len)
torch.utils.data.communication.messages.Request(DataLoaderQueueMessage)
torch.utils.data.communication.messages.ResetIteratorRequest(Request)
torch.utils.data.communication.messages.ResetIteratorResponse(Response)
torch.utils.data.communication.messages.Response(DataLoaderQueueMessage)
torch.utils.data.communication.messages.StopIterationResponse(Response)
torch.utils.data.communication.messages.TerminateRequest(Request)
torch.utils.data.communication.messages.TerminateResponse(Response)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/map.py----------------------------------------
A:torch.utils.data.communication.map.validated_datapipe.nonblocking_len->types.MethodType(nonblocking_len, validated_datapipe)
A:torch.utils.data.communication.map.validated_datapipe.nonblocking_getitem->types.MethodType(nonblocking_getitem, validated_datapipe)
A:torch.utils.data.communication.map.source_datapipe->EnsureNonBlockingMapDataPipe(source_datapipe)
A:torch.utils.data.communication.map.request->protocol.get_new_request(block=blocking_request_get)
A:torch.utils.data.communication.map.size->EnsureNonBlockingMapDataPipe(source_datapipe).nonblocking_len()
A:torch.utils.data.communication.map.value->EnsureNonBlockingMapDataPipe(source_datapipe).nonblocking_getitem(request.key)
A:torch.utils.data.communication.map.response->self.protocol.get_response_len(block=True, timeout=self._response_wait_time)
torch.utils.data.communication.map.DataPipeBehindQueues(source_datapipe,protocol,full_stop=False,blocking_request_get=False)
torch.utils.data.communication.map.EnsureNonBlockingMapDataPipe(validated_datapipe)
torch.utils.data.communication.map.NonBlockingMap(MapDataPipe)
torch.utils.data.communication.map.NonBlockingMap.__getitem__(self,index)
torch.utils.data.communication.map.NonBlockingMap.__len__(self)
torch.utils.data.communication.map.NonBlockingMap.nonblocking_getitem(self,index)
torch.utils.data.communication.map.NonBlockingMap.nonblocking_len(self)
torch.utils.data.communication.map.NonBlockingMap.register_not_available_hook(hook_function)
torch.utils.data.communication.map.NotAvailable(Exception)
torch.utils.data.communication.map.QueueWrapperForMap(self,protocol,response_wait_time=1e-05)
torch.utils.data.communication.map.QueueWrapperForMap.__init__(self,protocol,response_wait_time=1e-05)
torch.utils.data.communication.map.QueueWrapperForMap.nonblocking_getitem(self,index)
torch.utils.data.communication.map.QueueWrapperForMap.nonblocking_len(self)
torch.utils.data.communication.map.default_not_available_hook()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/queue.py----------------------------------------
A:torch.utils.data.communication.queue.self.lock->threading.Lock()
torch.utils.data.communication.queue.LocalQueue(self,name='unnamed')
torch.utils.data.communication.queue.LocalQueue.__init__(self,name='unnamed')
torch.utils.data.communication.queue.LocalQueue.get(self,block=True,timeout=0)
torch.utils.data.communication.queue.LocalQueue.put(self,item,block=True)
torch.utils.data.communication.queue.ThreadingQueue(self,name='unnamed')
torch.utils.data.communication.queue.ThreadingQueue.__init__(self,name='unnamed')
torch.utils.data.communication.queue.ThreadingQueue.get(self,block=True,timeout=0)
torch.utils.data.communication.queue.ThreadingQueue.put(self,item,block=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/protocol.py----------------------------------------
A:torch.utils.data.communication.protocol.response->self.response_queue.get(block=block, timeout=timeout)
A:torch.utils.data.communication.protocol.request->torch.utils.data.communication.messages.GetNextRequest()
torch.utils.data.communication.protocol.EmptyQueue(Exception)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolClient(ProtocolClient)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolClient.get_response_next(self,block=False,timeout=None)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolClient.get_response_reset_iterator(self,block=False)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolClient.request_next(self)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolClient.request_reset_iterator(self)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolServer(ProtocolServer)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolServer.response_invalid_state(self)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolServer.response_next(self,value)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolServer.response_reset_iterator(self)
torch.utils.data.communication.protocol.IterDataPipeQueueProtocolServer.response_stop_iteration(self)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolClient(ProtocolClient)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolClient.get_response_item(self,block=False,timeout=None)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolClient.get_response_len(self,block=False,timeout=None)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolClient.request_item(self,index)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolClient.request_len(self)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolServer(ProtocolServer)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolServer.response_index_out_of_bound(self)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolServer.response_item(self,key,value)
torch.utils.data.communication.protocol.MapDataPipeQueueProtocolServer.response_len(self,size)
torch.utils.data.communication.protocol.Protocol(self,request_queue,response_queue)
torch.utils.data.communication.protocol.Protocol.__init__(self,request_queue,response_queue)
torch.utils.data.communication.protocol.ProtocolClient(self,request_queue,response_queue)
torch.utils.data.communication.protocol.ProtocolClient.__init__(self,request_queue,response_queue)
torch.utils.data.communication.protocol.ProtocolClient.can_take_request(self)
torch.utils.data.communication.protocol.ProtocolClient.request_sent(self,request=True)
torch.utils.data.communication.protocol.ProtocolClient.request_served(self,result=None)
torch.utils.data.communication.protocol.ProtocolClient.waiting_for_response(self)
torch.utils.data.communication.protocol.ProtocolServer(self,request_queue,response_queue)
torch.utils.data.communication.protocol.ProtocolServer.__init__(self,request_queue,response_queue)
torch.utils.data.communication.protocol.ProtocolServer.get_new_request(self,block=False)
torch.utils.data.communication.protocol.ProtocolServer.have_pending_request(self)
torch.utils.data.communication.protocol.ProtocolServer.response_terminate(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/eventloop.py----------------------------------------
A:torch.utils.data.communication.eventloop.req_queue->torch.utils.data.communication.queue.ThreadingQueue()
A:torch.utils.data.communication.eventloop.res_queue->torch.utils.data.communication.queue.ThreadingQueue()
A:torch.utils.data.communication.eventloop.process->threading.Thread(target=DataPipeToQueuesLoop, args=(new_datapipe, req_queue, res_queue), daemon=True)
A:torch.utils.data.communication.eventloop.new_datapipe->pickle.loads(pickle.dumps(datapipe))
torch.utils.data.communication.eventloop.DataPipeToQueuesLoop(source_datapipe,req_queue,res_queue)
torch.utils.data.communication.eventloop.SpawnProcessForDataPipeline(multiprocessing_ctx,datapipe)
torch.utils.data.communication.eventloop.SpawnThreadForDataPipeline(datapipe)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/iter.py----------------------------------------
A:torch.utils.data.communication.iter.self._as_iterator->iter(self)
A:torch.utils.data.communication.iter.validated_datapipe.nonblocking_next->types.MethodType(nonblocking_next, validated_datapipe)
A:torch.utils.data.communication.iter.validated_datapipe.reset_iterator->types.MethodType(reset_iterator, validated_datapipe)
A:torch.utils.data.communication.iter.source_datapipe->EnsureNonBlockingDataPipe(source_datapipe)
A:torch.utils.data.communication.iter.request->protocol.get_new_request(block=blocking_request_get)
A:torch.utils.data.communication.iter.value->EnsureNonBlockingDataPipe(source_datapipe).nonblocking_next()
A:torch.utils.data.communication.iter.response->self.protocol.get_response_next(block=True, timeout=self._response_wait_time)
torch.utils.data.communication.iter.DataPipeBehindQueues(source_datapipe,protocol,full_stop=False,blocking_request_get=False)
torch.utils.data.communication.iter.EnsureNonBlockingDataPipe(validated_datapipe)
torch.utils.data.communication.iter.InvalidStateResetRequired(Exception)
torch.utils.data.communication.iter.NonBlocking(IterDataPipe)
torch.utils.data.communication.iter.NonBlocking.__iter__(self)
torch.utils.data.communication.iter.NonBlocking.__next__(self)
torch.utils.data.communication.iter.NonBlocking.nonblocking_next(self)
torch.utils.data.communication.iter.NonBlocking.register_not_available_hook(hook_function)
torch.utils.data.communication.iter.NonBlocking.reset_iterator(self)
torch.utils.data.communication.iter.NotAvailable(Exception)
torch.utils.data.communication.iter.QueueWrapper(self,protocol,response_wait_time=1e-05)
torch.utils.data.communication.iter.QueueWrapper.__init__(self,protocol,response_wait_time=1e-05)
torch.utils.data.communication.iter.QueueWrapper.nonblocking_next(self)
torch.utils.data.communication.iter.QueueWrapper.reset_iterator(self)
torch.utils.data.communication.iter.default_not_available_hook()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/communication/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/utils/common.py----------------------------------------
A:torch.utils.data.datapipes.utils.common.path->os.path.abspath(path)
A:torch.utils.data.datapipes.utils.common.fname->os.path.basename(path)
torch.utils.data.datapipes.utils.common.StreamWrapper(self,file_obj)
torch.utils.data.datapipes.utils.common.StreamWrapper.__del__(self)
torch.utils.data.datapipes.utils.common.StreamWrapper.__dir__(self)
torch.utils.data.datapipes.utils.common.StreamWrapper.__getattr__(self,name)
torch.utils.data.datapipes.utils.common.StreamWrapper.__getstate__(self)
torch.utils.data.datapipes.utils.common.StreamWrapper.__init__(self,file_obj)
torch.utils.data.datapipes.utils.common.StreamWrapper.__iter__(self)
torch.utils.data.datapipes.utils.common.StreamWrapper.__next__(self)
torch.utils.data.datapipes.utils.common.StreamWrapper.__repr__(self)
torch.utils.data.datapipes.utils.common.StreamWrapper.__setstate__(self,obj)
torch.utils.data.datapipes.utils.common.check_lambda_fn(fn)
torch.utils.data.datapipes.utils.common.deprecation_warning(name,new_name:str='')
torch.utils.data.datapipes.utils.common.get_file_binaries_from_pathnames(pathnames:Iterable,mode:str,encoding:Optional[str]=None)
torch.utils.data.datapipes.utils.common.get_file_pathnames_from_root(root:str,masks:Union[str,List[str]],recursive:bool=False,abspath:bool=False,non_deterministic:bool=False)->Iterable[str]
torch.utils.data.datapipes.utils.common.match_masks(name:str,masks:Union[str,List[str]])->bool
torch.utils.data.datapipes.utils.common.validate_pathname_binary_tuple(data:Tuple[str,IOBase])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/utils/decoder.py----------------------------------------
A:torch.utils.data.datapipes.utils.decoder.stream->io.BytesIO(data)
A:torch.utils.data.datapipes.utils.decoder.extensions->extensions.lower().split().lower().split()
A:torch.utils.data.datapipes.utils.decoder.extension->key.lower().split('.')
A:torch.utils.data.datapipes.utils.decoder.target->target.split('.').split('.')
A:torch.utils.data.datapipes.utils.decoder.self.imagespec->imagespec.lower()
A:torch.utils.data.datapipes.utils.decoder.img->img.convert(mode.upper()).convert(mode.upper())
A:torch.utils.data.datapipes.utils.decoder.result->f(key, data)
A:torch.utils.data.datapipes.utils.decoder.fname->os.path.join(dirname, f'file.{extension}')
A:torch.utils.data.datapipes.utils.decoder.data->b''.join(data)
A:torch.utils.data.datapipes.utils.decoder.v->v.decode('utf-8').decode('utf-8')
A:torch.utils.data.datapipes.utils.decoder.result[k]->self.decode1(self.key_fn(k), v)
torch.utils.data.datapipes.utils.decoder.Decoder(self,*handler,key_fn=extension_extract_fn)
torch.utils.data.datapipes.utils.decoder.Decoder.__init__(self,*handler,key_fn=extension_extract_fn)
torch.utils.data.datapipes.utils.decoder.Decoder._is_stream_handle(data)
torch.utils.data.datapipes.utils.decoder.Decoder.add_handler(self,*handler)
torch.utils.data.datapipes.utils.decoder.Decoder.decode(self,data)
torch.utils.data.datapipes.utils.decoder.Decoder.decode1(self,key,data)
torch.utils.data.datapipes.utils.decoder.ImageHandler(self,imagespec)
torch.utils.data.datapipes.utils.decoder.ImageHandler.__init__(self,imagespec)
torch.utils.data.datapipes.utils.decoder.MatHandler(self,**loadmat_kwargs)
torch.utils.data.datapipes.utils.decoder.MatHandler.__init__(self,**loadmat_kwargs)
torch.utils.data.datapipes.utils.decoder.audiohandler(extension,data)
torch.utils.data.datapipes.utils.decoder.basichandlers(extension,data)
torch.utils.data.datapipes.utils.decoder.extension_extract_fn(pathname)
torch.utils.data.datapipes.utils.decoder.handle_extension(extensions,f)
torch.utils.data.datapipes.utils.decoder.imagehandler(imagespec)
torch.utils.data.datapipes.utils.decoder.mathandler(**loadmat_kwargs)
torch.utils.data.datapipes.utils.decoder.videohandler(extension,data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/utils/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/dataframe/structures.py----------------------------------------
torch.utils.data.datapipes.dataframe.structures.DataChunkDF(DataChunk)
torch.utils.data.datapipes.dataframe.structures.DataChunkDF.__iter__(self)
torch.utils.data.datapipes.dataframe.structures.DataChunkDF.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/dataframe/dataframe_wrapper.py----------------------------------------
A:torch.utils.data.datapipes.dataframe.dataframe_wrapper._WITH_PANDAS->_try_import_pandas()
A:torch.utils.data.datapipes.dataframe.dataframe_wrapper.wrapper->get_df_wrapper()
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.concat(cls,buffer)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.create_dataframe(cls,data,columns)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.get_item(cls,data,idx)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.get_len(cls,df)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.is_column(cls,data)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.is_dataframe(cls,data)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.PandasWrapper.iterate(cls,data)
torch.utils.data.datapipes.dataframe.dataframe_wrapper._try_import_pandas()->bool
torch.utils.data.datapipes.dataframe.dataframe_wrapper._with_pandas()->bool
torch.utils.data.datapipes.dataframe.dataframe_wrapper.concat(buffer)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.create_dataframe(data,columns=None)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.get_df_wrapper()
torch.utils.data.datapipes.dataframe.dataframe_wrapper.get_item(data,idx)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.get_len(df)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.is_column(data)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.is_dataframe(data)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.iterate(data)
torch.utils.data.datapipes.dataframe.dataframe_wrapper.set_df_wrapper(wrapper)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/dataframe/datapipes.py----------------------------------------
A:torch.utils.data.datapipes.dataframe.datapipes.size->len(df.index)
torch.utils.data.datapipes.dataframe.DataFramesAsTuplesPipe(self,source_datapipe)
torch.utils.data.datapipes.dataframe.DataFramesAsTuplesPipe.__iter__(self)
torch.utils.data.datapipes.dataframe.datapipes.ConcatDataFramesPipe(self,source_datapipe,batch=3)
torch.utils.data.datapipes.dataframe.datapipes.ConcatDataFramesPipe.__init__(self,source_datapipe,batch=3)
torch.utils.data.datapipes.dataframe.datapipes.ConcatDataFramesPipe.__iter__(self)
torch.utils.data.datapipes.dataframe.datapipes.DataFramesAsTuplesPipe(self,source_datapipe)
torch.utils.data.datapipes.dataframe.datapipes.DataFramesAsTuplesPipe.__init__(self,source_datapipe)
torch.utils.data.datapipes.dataframe.datapipes.DataFramesAsTuplesPipe.__iter__(self)
torch.utils.data.datapipes.dataframe.datapipes.ExampleAggregateAsDataFrames(self,source_datapipe,dataframe_size=10,columns=None)
torch.utils.data.datapipes.dataframe.datapipes.ExampleAggregateAsDataFrames.__init__(self,source_datapipe,dataframe_size=10,columns=None)
torch.utils.data.datapipes.dataframe.datapipes.ExampleAggregateAsDataFrames.__iter__(self)
torch.utils.data.datapipes.dataframe.datapipes.ExampleAggregateAsDataFrames._as_list(self,item)
torch.utils.data.datapipes.dataframe.datapipes.FilterDataFramesPipe(self,source_datapipe,filter_fn)
torch.utils.data.datapipes.dataframe.datapipes.FilterDataFramesPipe.__init__(self,source_datapipe,filter_fn)
torch.utils.data.datapipes.dataframe.datapipes.FilterDataFramesPipe.__iter__(self)
torch.utils.data.datapipes.dataframe.datapipes.PerRowDataFramesPipe(self,source_datapipe)
torch.utils.data.datapipes.dataframe.datapipes.PerRowDataFramesPipe.__init__(self,source_datapipe)
torch.utils.data.datapipes.dataframe.datapipes.PerRowDataFramesPipe.__iter__(self)
torch.utils.data.datapipes.dataframe.datapipes.ShuffleDataFramesPipe(self,source_datapipe)
torch.utils.data.datapipes.dataframe.datapipes.ShuffleDataFramesPipe.__init__(self,source_datapipe)
torch.utils.data.datapipes.dataframe.datapipes.ShuffleDataFramesPipe.__iter__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/dataframe/dataframes.py----------------------------------------
A:torch.utils.data.datapipes.dataframe.dataframes.res->CaptureMul(self, add_val, ctx=self.ctx)
A:torch.utils.data.datapipes.dataframe.dataframes.var->CaptureVariable(res, ctx=self.ctx)
A:torch.utils.data.datapipes.dataframe.dataframes.t->CaptureVariableAssign(variable=var, value=res, ctx=self.ctx)
A:torch.utils.data.datapipes.dataframe.dataframes.self.kwargs['variable'].calculated_value->self.kwargs['value'].execute()
A:torch.utils.data.datapipes.dataframe.dataframes.self.left.execute()[self.key]->self.value.execute()
A:torch.utils.data.datapipes.dataframe.dataframes.val->get_val(self.src)
A:torch.utils.data.datapipes.dataframe.dataframes.dp->dp.as_datapipe().groupby(group_key_fn, buffer_size=buffer_size, group_size=group_size, guaranteed_group_size=guaranteed_group_size, drop_remaining=drop_remaining).as_datapipe().groupby(group_key_fn, buffer_size=buffer_size, group_size=group_size, guaranteed_group_size=guaranteed_group_size, drop_remaining=drop_remaining)
torch.utils.data.datapipes.dataframe.CaptureDataFrame(CaptureInitial)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps(CaptureDataFrame)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.__getattr__(self,attrname)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.__iter__(self)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.as_datapipe(self)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.batch(self,batch_size=10,drop_last:bool=False,wrapper_class=DataChunkDF)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.filter(self,*args,**kwargs)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.groupby(self,group_key_fn,*,buffer_size=10000,group_size=None,guaranteed_group_size=None,drop_remaining=False)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.raw_iterator(self)
torch.utils.data.datapipes.dataframe.CaptureDataFrameWithDataPipeOps.shuffle(self,*args,**kwargs)
torch.utils.data.datapipes.dataframe.dataframes.Capture(self)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__add__(self,add_val)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__getattr__(self,attrname)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__getitem__(self,key)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__init__(self)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__mul__(self,add_val)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__setitem__(self,key,value)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.Capture.__sub__(self,add_val)
torch.utils.data.datapipes.dataframe.dataframes.Capture._ops_str(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureAdd(self,left,right,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureAdd.__init__(self,left,right,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureAdd.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureAdd.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureCall(CaptureF)
torch.utils.data.datapipes.dataframe.dataframes.CaptureCall.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureCall.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrame(CaptureInitial)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps(CaptureDataFrame)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.__getattr__(self,attrname)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.__iter__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.as_datapipe(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.batch(self,batch_size=10,drop_last:bool=False,wrapper_class=DataChunkDF)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.filter(self,*args,**kwargs)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.groupby(self,group_key_fn,*,buffer_size=10000,group_size=None,guaranteed_group_size=None,drop_remaining=False)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.raw_iterator(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureDataFrameWithDataPipeOps.shuffle(self,*args,**kwargs)
torch.utils.data.datapipes.dataframe.dataframes.CaptureF(self,ctx=None,**kwargs)
torch.utils.data.datapipes.dataframe.dataframes.CaptureF.__init__(self,ctx=None,**kwargs)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetAttr(self,src,name,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetAttr.__init__(self,src,name,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetAttr.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetAttr.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetItem(self,left,key,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetItem.__init__(self,left,key,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetItem.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureGetItem.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureInitial(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureInitial.__init__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureMul(self,left,right,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureMul.__init__(self,left,right,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureMul.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureMul.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSetItem(self,left,key,value,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSetItem.__init__(self,left,key,value,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSetItem.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSetItem.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSub(self,left,right,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSub.__init__(self,left,right,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSub.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureSub.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariable(self,value,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariable.__init__(self,value,ctx)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariable.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariable.apply_ops(self,dataframe)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariable.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariableAssign(CaptureF)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariableAssign.__str__(self)
torch.utils.data.datapipes.dataframe.dataframes.CaptureVariableAssign.execute(self)
torch.utils.data.datapipes.dataframe.dataframes.DataFrameTracedOps(self,source_datapipe,output_var)
torch.utils.data.datapipes.dataframe.dataframes.DataFrameTracedOps.__init__(self,source_datapipe,output_var)
torch.utils.data.datapipes.dataframe.dataframes.DataFrameTracedOps.__iter__(self)
torch.utils.data.datapipes.dataframe.dataframes.DataFrameTracer(self,source_datapipe)
torch.utils.data.datapipes.dataframe.dataframes.DataFrameTracer.__init__(self,source_datapipe)
torch.utils.data.datapipes.dataframe.dataframes.get_val(capture)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/dataframe/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/routeddecoder.py----------------------------------------
A:torch.utils.data.datapipes.iter.routeddecoder.self.decoder->Decoder(*handlers, key_fn=key_fn)
A:torch.utils.data.datapipes.iter.routeddecoder.result->self.decoder(data)
torch.utils.data.datapipes.iter.RoutedDecoder(self,datapipe:Iterable[Tuple[str,BufferedIOBase]],*handlers:Callable,key_fn:Callable=extension_extract_fn)
torch.utils.data.datapipes.iter.RoutedDecoder.__iter__(self)->Iterator[Tuple[str, Any]]
torch.utils.data.datapipes.iter.RoutedDecoder.__len__(self)->int
torch.utils.data.datapipes.iter.RoutedDecoder.add_handler(self,*handler:Callable)->None
torch.utils.data.datapipes.iter.routeddecoder.RoutedDecoderIterDataPipe(self,datapipe:Iterable[Tuple[str,BufferedIOBase]],*handlers:Callable,key_fn:Callable=extension_extract_fn)
torch.utils.data.datapipes.iter.routeddecoder.RoutedDecoderIterDataPipe.__init__(self,datapipe:Iterable[Tuple[str,BufferedIOBase]],*handlers:Callable,key_fn:Callable=extension_extract_fn)
torch.utils.data.datapipes.iter.routeddecoder.RoutedDecoderIterDataPipe.__iter__(self)->Iterator[Tuple[str, Any]]
torch.utils.data.datapipes.iter.routeddecoder.RoutedDecoderIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.routeddecoder.RoutedDecoderIterDataPipe.add_handler(self,*handler:Callable)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/selecting.py----------------------------------------
A:torch.utils.data.datapipes.iter.selecting.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.iter.selecting.filtered->self._returnIfTrue(data)
A:torch.utils.data.datapipes.iter.selecting.condition->self.filter_fn(data)
A:torch.utils.data.datapipes.iter.selecting.dill_function->dill.dumps(self.filter_fn)
A:torch.utils.data.datapipes.iter.selecting.self.filter_fn->dill.loads(dill_function)
torch.utils.data.datapipes.iter.Filter(self,datapipe:IterDataPipe,filter_fn:Callable,drop_empty_batches:bool=True)
torch.utils.data.datapipes.iter.Filter.__getstate__(self)
torch.utils.data.datapipes.iter.Filter.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.Filter.__setstate__(self,state)
torch.utils.data.datapipes.iter.Filter._isNonEmpty(self,data)
torch.utils.data.datapipes.iter.Filter._returnIfTrue(self,data)
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe(self,datapipe:IterDataPipe,filter_fn:Callable,drop_empty_batches:bool=True)
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe.__getstate__(self)
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe.__init__(self,datapipe:IterDataPipe,filter_fn:Callable,drop_empty_batches:bool=True)
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe.__setstate__(self,state)
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe._isNonEmpty(self,data)
torch.utils.data.datapipes.iter.selecting.FilterIterDataPipe._returnIfTrue(self,data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/grouping.py----------------------------------------
A:torch.utils.data.datapipes.iter.grouping.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.iter.grouping.biggest_size->len(buffer_elements[findkey])
A:torch.utils.data.datapipes.iter.grouping.key->self.group_key_fn(x)
A:torch.utils.data.datapipes.iter.grouping.(result_to_yield, buffer_size)->self._remove_biggest_key(buffer_elements, buffer_size)
A:torch.utils.data.datapipes.iter.grouping.res->buffer_elements.pop(key)
A:torch.utils.data.datapipes.iter.grouping.dill_function->dill.dumps(self.group_key_fn)
A:torch.utils.data.datapipes.iter.grouping.self.group_key_fn->dill.loads(dill_function)
torch.utils.data.datapipes.iter.Batcher(self,datapipe:IterDataPipe,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)
torch.utils.data.datapipes.iter.Batcher.__iter__(self)->Iterator[DataChunk]
torch.utils.data.datapipes.iter.Batcher.__len__(self)->int
torch.utils.data.datapipes.iter.Grouper(self,datapipe:IterDataPipe[T_co],group_key_fn:Callable,*,buffer_size:int=10000,group_size:Optional[int]=None,guaranteed_group_size:Optional[int]=None,drop_remaining:bool=False)
torch.utils.data.datapipes.iter.Grouper.__getstate__(self)
torch.utils.data.datapipes.iter.Grouper.__iter__(self)
torch.utils.data.datapipes.iter.Grouper.__setstate__(self,state)
torch.utils.data.datapipes.iter.Grouper._remove_biggest_key(self,buffer_elements,buffer_size)
torch.utils.data.datapipes.iter.ShardingFilter(self,source_datapipe:IterDataPipe)
torch.utils.data.datapipes.iter.ShardingFilter.__iter__(self)
torch.utils.data.datapipes.iter.ShardingFilter.__len__(self)
torch.utils.data.datapipes.iter.ShardingFilter.apply_sharding(self,num_of_instances,instance_id)
torch.utils.data.datapipes.iter.ShardingFilter.is_shardable(self)
torch.utils.data.datapipes.iter.UnBatcher(self,datapipe:IterDataPipe,unbatch_level:int=1)
torch.utils.data.datapipes.iter.UnBatcher.__iter__(self)
torch.utils.data.datapipes.iter.UnBatcher._dive(self,element,unbatch_level)
torch.utils.data.datapipes.iter.grouping.BatcherIterDataPipe(self,datapipe:IterDataPipe,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)
torch.utils.data.datapipes.iter.grouping.BatcherIterDataPipe.__init__(self,datapipe:IterDataPipe,batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)
torch.utils.data.datapipes.iter.grouping.BatcherIterDataPipe.__iter__(self)->Iterator[DataChunk]
torch.utils.data.datapipes.iter.grouping.BatcherIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.grouping.GrouperIterDataPipe(self,datapipe:IterDataPipe[T_co],group_key_fn:Callable,*,buffer_size:int=10000,group_size:Optional[int]=None,guaranteed_group_size:Optional[int]=None,drop_remaining:bool=False)
torch.utils.data.datapipes.iter.grouping.GrouperIterDataPipe.__getstate__(self)
torch.utils.data.datapipes.iter.grouping.GrouperIterDataPipe.__init__(self,datapipe:IterDataPipe[T_co],group_key_fn:Callable,*,buffer_size:int=10000,group_size:Optional[int]=None,guaranteed_group_size:Optional[int]=None,drop_remaining:bool=False)
torch.utils.data.datapipes.iter.grouping.GrouperIterDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.grouping.GrouperIterDataPipe.__setstate__(self,state)
torch.utils.data.datapipes.iter.grouping.GrouperIterDataPipe._remove_biggest_key(self,buffer_elements,buffer_size)
torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe(self,source_datapipe:IterDataPipe)
torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe.__init__(self,source_datapipe:IterDataPipe)
torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe.__len__(self)
torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe.apply_sharding(self,num_of_instances,instance_id)
torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe.is_shardable(self)
torch.utils.data.datapipes.iter.grouping.UnBatcherIterDataPipe(self,datapipe:IterDataPipe,unbatch_level:int=1)
torch.utils.data.datapipes.iter.grouping.UnBatcherIterDataPipe.__init__(self,datapipe:IterDataPipe,unbatch_level:int=1)
torch.utils.data.datapipes.iter.grouping.UnBatcherIterDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.grouping.UnBatcherIterDataPipe._dive(self,element,unbatch_level)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/utils.py----------------------------------------
A:torch.utils.data.datapipes.iter.utils.source_data->copy.deepcopy(self.iterable)
torch.utils.data.datapipes.iter.IterableWrapper(self,iterable,deepcopy=True)
torch.utils.data.datapipes.iter.IterableWrapper.__iter__(self)
torch.utils.data.datapipes.iter.IterableWrapper.__len__(self)
torch.utils.data.datapipes.iter.utils.IterableWrapperIterDataPipe(self,iterable,deepcopy=True)
torch.utils.data.datapipes.iter.utils.IterableWrapperIterDataPipe.__init__(self,iterable,deepcopy=True)
torch.utils.data.datapipes.iter.utils.IterableWrapperIterDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.utils.IterableWrapperIterDataPipe.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/callable.py----------------------------------------
A:torch.utils.data.datapipes.iter.callable.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.iter.callable.res->self.fn(data[self.input_col])
A:torch.utils.data.datapipes.iter.callable.args->tuple((data[col] for col in self.input_col))
A:torch.utils.data.datapipes.iter.callable.data->list(data)
A:torch.utils.data.datapipes.iter.callable.dill_function->dill.dumps(self.fn)
A:torch.utils.data.datapipes.iter.callable.self.fn->dill.loads(dill_function)
torch.utils.data.datapipes.iter.Collator(self,datapipe:IterDataPipe,collate_fn:Callable=_utils.collate.default_collate)
torch.utils.data.datapipes.iter.Mapper(self,datapipe:IterDataPipe,fn:Callable,input_col=None,output_col=None)
torch.utils.data.datapipes.iter.Mapper.__getstate__(self)
torch.utils.data.datapipes.iter.Mapper.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.Mapper.__len__(self)->int
torch.utils.data.datapipes.iter.Mapper.__setstate__(self,state)
torch.utils.data.datapipes.iter.Mapper._apply_fn(self,data)
torch.utils.data.datapipes.iter.callable.CollatorIterDataPipe(self,datapipe:IterDataPipe,collate_fn:Callable=_utils.collate.default_collate)
torch.utils.data.datapipes.iter.callable.CollatorIterDataPipe.__init__(self,datapipe:IterDataPipe,collate_fn:Callable=_utils.collate.default_collate)
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe(self,datapipe:IterDataPipe,fn:Callable,input_col=None,output_col=None)
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe.__getstate__(self)
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe.__init__(self,datapipe:IterDataPipe,fn:Callable,input_col=None,output_col=None)
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe.__setstate__(self,state)
torch.utils.data.datapipes.iter.callable.MapperIterDataPipe._apply_fn(self,data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/filelister.py----------------------------------------
A:torch.utils.data.datapipes.iter.filelister.root->IterableWrapper(root)
torch.utils.data.datapipes.iter.FileLister(self,root:Union[str,Sequence[str],IterDataPipe]='.',masks:Union[str,List[str]]='',*,recursive:bool=False,abspath:bool=False,non_deterministic:bool=False,length:int=-1)
torch.utils.data.datapipes.iter.FileLister.__iter__(self)->Iterator[str]
torch.utils.data.datapipes.iter.FileLister.__len__(self)
torch.utils.data.datapipes.iter.filelister.FileListerIterDataPipe(self,root:Union[str,Sequence[str],IterDataPipe]='.',masks:Union[str,List[str]]='',*,recursive:bool=False,abspath:bool=False,non_deterministic:bool=False,length:int=-1)
torch.utils.data.datapipes.iter.filelister.FileListerIterDataPipe.__init__(self,root:Union[str,Sequence[str],IterDataPipe]='.',masks:Union[str,List[str]]='',*,recursive:bool=False,abspath:bool=False,non_deterministic:bool=False,length:int=-1)
torch.utils.data.datapipes.iter.filelister.FileListerIterDataPipe.__iter__(self)->Iterator[str]
torch.utils.data.datapipes.iter.filelister.FileListerIterDataPipe.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/combining.py----------------------------------------
A:torch.utils.data.datapipes.iter.combining.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.iter.combining.self.length->min((len(dp) for dp in self.datapipes))
A:torch.utils.data.datapipes.iter.combining.container->_DemultiplexerIterDataPipe(datapipe, num_instances, classifier_fn, drop_none, buffer_size)
A:torch.utils.data.datapipes.iter.combining.self._datapipe_iterator->iter(self.main_datapipe)
A:torch.utils.data.datapipes.iter.combining.new_min->min(self.child_pointers)
A:torch.utils.data.datapipes.iter.combining.self.buffer->deque()
A:torch.utils.data.datapipes.iter.combining.value->next(iterators[i])
A:torch.utils.data.datapipes.iter.combining.classification->self.classifier_fn(value)
A:torch.utils.data.datapipes.iter.combining.dill_function->dill.dumps(self.classifier_fn)
A:torch.utils.data.datapipes.iter.combining.self.classifier_fn->dill.loads(dill_function)
torch.utils.data.datapipes.iter.Concater(self,*datapipes:IterDataPipe)
torch.utils.data.datapipes.iter.Concater.__iter__(self)->Iterator
torch.utils.data.datapipes.iter.Concater.__len__(self)->int
torch.utils.data.datapipes.iter.Demultiplexer(cls,datapipe:IterDataPipe,num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool=False,buffer_size:int=1000)
torch.utils.data.datapipes.iter.Forker(cls,datapipe:IterDataPipe,num_instances:int,buffer_size:int=1000)
torch.utils.data.datapipes.iter.Multiplexer(self,*datapipes)
torch.utils.data.datapipes.iter.Multiplexer.__iter__(self)
torch.utils.data.datapipes.iter.Multiplexer.__len__(self)
torch.utils.data.datapipes.iter.Zipper(self,*datapipes:IterDataPipe)
torch.utils.data.datapipes.iter.Zipper.__iter__(self)->Iterator[Tuple[T_co]]
torch.utils.data.datapipes.iter.Zipper.__len__(self)->int
torch.utils.data.datapipes.iter.combining.ConcaterIterDataPipe(self,*datapipes:IterDataPipe)
torch.utils.data.datapipes.iter.combining.ConcaterIterDataPipe.__init__(self,*datapipes:IterDataPipe)
torch.utils.data.datapipes.iter.combining.ConcaterIterDataPipe.__iter__(self)->Iterator
torch.utils.data.datapipes.iter.combining.ConcaterIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.combining.DemultiplexerIterDataPipe(cls,datapipe:IterDataPipe,num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool=False,buffer_size:int=1000)
torch.utils.data.datapipes.iter.combining.DemultiplexerIterDataPipe.__new__(cls,datapipe:IterDataPipe,num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool=False,buffer_size:int=1000)
torch.utils.data.datapipes.iter.combining.ForkerIterDataPipe(cls,datapipe:IterDataPipe,num_instances:int,buffer_size:int=1000)
torch.utils.data.datapipes.iter.combining.ForkerIterDataPipe.__new__(cls,datapipe:IterDataPipe,num_instances:int,buffer_size:int=1000)
torch.utils.data.datapipes.iter.combining.MultiplexerIterDataPipe(self,*datapipes)
torch.utils.data.datapipes.iter.combining.MultiplexerIterDataPipe.__init__(self,*datapipes)
torch.utils.data.datapipes.iter.combining.MultiplexerIterDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.combining.MultiplexerIterDataPipe.__len__(self)
torch.utils.data.datapipes.iter.combining.ZipperIterDataPipe(self,*datapipes:IterDataPipe)
torch.utils.data.datapipes.iter.combining.ZipperIterDataPipe.__init__(self,*datapipes:IterDataPipe)
torch.utils.data.datapipes.iter.combining.ZipperIterDataPipe.__iter__(self)->Iterator[Tuple[T_co]]
torch.utils.data.datapipes.iter.combining.ZipperIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.combining._ChildDataPipe(self,main_datapipe,instance_id:int)
torch.utils.data.datapipes.iter.combining._ChildDataPipe.__init__(self,main_datapipe,instance_id:int)
torch.utils.data.datapipes.iter.combining._ChildDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.combining._ChildDataPipe.__len__(self)
torch.utils.data.datapipes.iter.combining._ChildDataPipe.get_generator_by_instance(self,instance_id:int)
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe(self,datapipe:IterDataPipe[T_co],num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool,buffer_size:int)
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.__getstate__(self)
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.__init__(self,datapipe:IterDataPipe[T_co],num_instances:int,classifier_fn:Callable[[T_co],Optional[int]],drop_none:bool,buffer_size:int)
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.__setstate__(self,state)
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe._find_next(self,instance_id:int)->T_co
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.get_next_element_by_instance(self,instance_id:int)
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.is_every_instance_exhausted(self)->bool
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.is_instance_started(self,instance_id:int)->bool
torch.utils.data.datapipes.iter.combining._DemultiplexerIterDataPipe.reset(self)
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe(self,datapipe:IterDataPipe,num_instances:int,buffer_size:int=1000)
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe.__init__(self,datapipe:IterDataPipe,num_instances:int,buffer_size:int=1000)
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe.__len__(self)
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe.get_next_element_by_instance(self,instance_id:int)
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe.is_every_instance_exhausted(self)->bool
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe.is_instance_started(self,instance_id:int)->bool
torch.utils.data.datapipes.iter.combining._ForkerIterDataPipe.reset(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/fileopener.py----------------------------------------
torch.utils.data.datapipes.iter.FileLoader(cls,datapipe:Iterable[str],mode:str='b',length:int=-1)
torch.utils.data.datapipes.iter.FileOpener(self,datapipe:Iterable[str],mode:str='r',encoding:Optional[str]=None,length:int=-1)
torch.utils.data.datapipes.iter.FileOpener.__iter__(self)
torch.utils.data.datapipes.iter.FileOpener.__len__(self)
torch.utils.data.datapipes.iter.fileopener.FileLoaderIterDataPipe(cls,datapipe:Iterable[str],mode:str='b',length:int=-1)
torch.utils.data.datapipes.iter.fileopener.FileLoaderIterDataPipe.__new__(cls,datapipe:Iterable[str],mode:str='b',length:int=-1)
torch.utils.data.datapipes.iter.fileopener.FileOpenerIterDataPipe(self,datapipe:Iterable[str],mode:str='r',encoding:Optional[str]=None,length:int=-1)
torch.utils.data.datapipes.iter.fileopener.FileOpenerIterDataPipe.__init__(self,datapipe:Iterable[str],mode:str='r',encoding:Optional[str]=None,length:int=-1)
torch.utils.data.datapipes.iter.fileopener.FileOpenerIterDataPipe.__iter__(self)
torch.utils.data.datapipes.iter.fileopener.FileOpenerIterDataPipe.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/streamreader.py----------------------------------------
A:torch.utils.data.datapipes.iter.streamreader.d->stream.read(self.chunk)
torch.utils.data.datapipes.iter.StreamReader(self,datapipe,chunk=None)
torch.utils.data.datapipes.iter.StreamReader.__iter__(self)
torch.utils.data.datapipes.iter.streamreader.StreamReaderIterDataPipe(self,datapipe,chunk=None)
torch.utils.data.datapipes.iter.streamreader.StreamReaderIterDataPipe.__init__(self,datapipe,chunk=None)
torch.utils.data.datapipes.iter.streamreader.StreamReaderIterDataPipe.__iter__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/iter/combinatorics.py----------------------------------------
A:torch.utils.data.datapipes.iter.combinatorics.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.iter.combinatorics.self.sampler->sampler(*self.sampler_args, data_source=self.datapipe, **self.sampler_kwargs)
A:torch.utils.data.datapipes.iter.combinatorics.self.datapipe->datapipe.unbatch(unbatch_level=unbatch_level)
A:torch.utils.data.datapipes.iter.combinatorics.idx->random.randint(0, len(buffer) - 1)
torch.utils.data.datapipes.iter.Sampler(self,datapipe:IterDataPipe,sampler:Type[Sampler]=SequentialSampler,sampler_args:Optional[Tuple]=None,sampler_kwargs:Optional[Dict]=None)
torch.utils.data.datapipes.iter.Sampler.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.Sampler.__len__(self)->int
torch.utils.data.datapipes.iter.Shuffler(self,datapipe:IterDataPipe[T_co],*,default:bool=True,buffer_size:int=10000,unbatch_level:int=0)
torch.utils.data.datapipes.iter.Shuffler.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.Shuffler.__len__(self)->int
torch.utils.data.datapipes.iter.Shuffler.buffer_replace(buffer,x)
torch.utils.data.datapipes.iter.Shuffler.set_shuffle_settings(self,shuffle=True)
torch.utils.data.datapipes.iter.combinatorics.SamplerIterDataPipe(self,datapipe:IterDataPipe,sampler:Type[Sampler]=SequentialSampler,sampler_args:Optional[Tuple]=None,sampler_kwargs:Optional[Dict]=None)
torch.utils.data.datapipes.iter.combinatorics.SamplerIterDataPipe.__init__(self,datapipe:IterDataPipe,sampler:Type[Sampler]=SequentialSampler,sampler_args:Optional[Tuple]=None,sampler_kwargs:Optional[Dict]=None)
torch.utils.data.datapipes.iter.combinatorics.SamplerIterDataPipe.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.combinatorics.SamplerIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe(self,datapipe:IterDataPipe[T_co],*,default:bool=True,buffer_size:int=10000,unbatch_level:int=0)
torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe.__init__(self,datapipe:IterDataPipe[T_co],*,default:bool=True,buffer_size:int=10000,unbatch_level:int=0)
torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe.__len__(self)->int
torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe.buffer_replace(buffer,x)
torch.utils.data.datapipes.iter.combinatorics.ShufflerIterDataPipe.set_shuffle_settings(self,shuffle=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/map/grouping.py----------------------------------------
A:torch.utils.data.datapipes.map.grouping.T->TypeVar('T')
A:torch.utils.data.datapipes.map.grouping.indices->range(index * self.batch_size, (index + 1) * self.batch_size)
torch.utils.data.datapipes.map.Batcher(self,datapipe:MapDataPipe[T],batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)
torch.utils.data.datapipes.map.Batcher.__getitem__(self,index)->DataChunk
torch.utils.data.datapipes.map.Batcher.__len__(self)->int
torch.utils.data.datapipes.map.grouping.BatcherMapDataPipe(self,datapipe:MapDataPipe[T],batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)
torch.utils.data.datapipes.map.grouping.BatcherMapDataPipe.__getitem__(self,index)->DataChunk
torch.utils.data.datapipes.map.grouping.BatcherMapDataPipe.__init__(self,datapipe:MapDataPipe[T],batch_size:int,drop_last:bool=False,wrapper_class=DataChunk)
torch.utils.data.datapipes.map.grouping.BatcherMapDataPipe.__len__(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/map/utils.py----------------------------------------
A:torch.utils.data.datapipes.map.utils.self.sequence->copy.deepcopy(sequence)
torch.utils.data.datapipes.map.SequenceWrapper(self,sequence,deepcopy=True)
torch.utils.data.datapipes.map.SequenceWrapper.__getitem__(self,index)
torch.utils.data.datapipes.map.SequenceWrapper.__len__(self)
torch.utils.data.datapipes.map.utils.SequenceWrapperMapDataPipe(self,sequence,deepcopy=True)
torch.utils.data.datapipes.map.utils.SequenceWrapperMapDataPipe.__getitem__(self,index)
torch.utils.data.datapipes.map.utils.SequenceWrapperMapDataPipe.__init__(self,sequence,deepcopy=True)
torch.utils.data.datapipes.map.utils.SequenceWrapperMapDataPipe.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/map/callable.py----------------------------------------
A:torch.utils.data.datapipes.map.callable.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.map.callable.dill_function->dill.dumps(self.fn)
A:torch.utils.data.datapipes.map.callable.self.fn->dill.loads(dill_function)
torch.utils.data.datapipes.map.Mapper(self,datapipe:MapDataPipe,fn:Callable=default_fn)
torch.utils.data.datapipes.map.Mapper.__getitem__(self,index)->T_co
torch.utils.data.datapipes.map.Mapper.__getstate__(self)
torch.utils.data.datapipes.map.Mapper.__len__(self)->int
torch.utils.data.datapipes.map.Mapper.__setstate__(self,state)
torch.utils.data.datapipes.map.callable.MapperMapDataPipe(self,datapipe:MapDataPipe,fn:Callable=default_fn)
torch.utils.data.datapipes.map.callable.MapperMapDataPipe.__getitem__(self,index)->T_co
torch.utils.data.datapipes.map.callable.MapperMapDataPipe.__getstate__(self)
torch.utils.data.datapipes.map.callable.MapperMapDataPipe.__init__(self,datapipe:MapDataPipe,fn:Callable=default_fn)
torch.utils.data.datapipes.map.callable.MapperMapDataPipe.__len__(self)->int
torch.utils.data.datapipes.map.callable.MapperMapDataPipe.__setstate__(self,state)
torch.utils.data.datapipes.map.callable.default_fn(data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/map/combining.py----------------------------------------
A:torch.utils.data.datapipes.map.combining.T_co->TypeVar('T_co', covariant=True)
A:torch.utils.data.datapipes.map.combining.self.length->min((len(dp) for dp in self.datapipes))
torch.utils.data.datapipes.map.Concater(self,*datapipes:MapDataPipe)
torch.utils.data.datapipes.map.Concater.__getitem__(self,index)->T_co
torch.utils.data.datapipes.map.Concater.__len__(self)->int
torch.utils.data.datapipes.map.Zipper(self,*datapipes:MapDataPipe[T_co])
torch.utils.data.datapipes.map.Zipper.__getitem__(self,index)->Tuple[T_co, ...]
torch.utils.data.datapipes.map.Zipper.__len__(self)->int
torch.utils.data.datapipes.map.combining.ConcaterMapDataPipe(self,*datapipes:MapDataPipe)
torch.utils.data.datapipes.map.combining.ConcaterMapDataPipe.__getitem__(self,index)->T_co
torch.utils.data.datapipes.map.combining.ConcaterMapDataPipe.__init__(self,*datapipes:MapDataPipe)
torch.utils.data.datapipes.map.combining.ConcaterMapDataPipe.__len__(self)->int
torch.utils.data.datapipes.map.combining.ZipperMapDataPipe(self,*datapipes:MapDataPipe[T_co])
torch.utils.data.datapipes.map.combining.ZipperMapDataPipe.__getitem__(self,index)->Tuple[T_co, ...]
torch.utils.data.datapipes.map.combining.ZipperMapDataPipe.__init__(self,*datapipes:MapDataPipe[T_co])
torch.utils.data.datapipes.map.combining.ZipperMapDataPipe.__len__(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/map/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/datapipes/map/combinatorics.py----------------------------------------
A:torch.utils.data.datapipes.map.combinatorics.T_co->TypeVar('T_co', covariant=True)
torch.utils.data.datapipes.map.Shuffler(self,datapipe:MapDataPipe[T_co],*,indices:Optional[List]=None)
torch.utils.data.datapipes.map.Shuffler.__getitem__(self,index)->T_co
torch.utils.data.datapipes.map.Shuffler.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.map.Shuffler.__len__(self)->int
torch.utils.data.datapipes.map.combinatorics.ShufflerMapDataPipe(self,datapipe:MapDataPipe[T_co],*,indices:Optional[List]=None)
torch.utils.data.datapipes.map.combinatorics.ShufflerMapDataPipe.__getitem__(self,index)->T_co
torch.utils.data.datapipes.map.combinatorics.ShufflerMapDataPipe.__init__(self,datapipe:MapDataPipe[T_co],*,indices:Optional[List]=None)
torch.utils.data.datapipes.map.combinatorics.ShufflerMapDataPipe.__iter__(self)->Iterator[T_co]
torch.utils.data.datapipes.map.combinatorics.ShufflerMapDataPipe.__len__(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_utils/worker.py----------------------------------------
A:torch.utils.data._utils.worker.self.manager_pid->os.getppid()
A:torch.utils.data._utils.worker.self.kernel32->ctypes.WinDLL('kernel32', use_last_error=True)
A:torch.utils.data._utils.worker.self.manager_handle->self.kernel32.OpenProcess(SYNCHRONIZE, 0, self.manager_pid)
A:torch.utils.data._utils.worker.self.__keys->tuple(kwargs.keys())
A:torch.utils.data._utils.worker.pool[i]->hash(entropy[i])
A:torch.utils.data._utils.worker.pool[i_dst]->mix(pool[i_dst], hash(pool[i_src]))
A:torch.utils.data._utils.worker.np_seed->_generate_state(base_seed, worker_id)
A:torch.utils.data._utils.worker._worker_info->WorkerInfo(id=worker_id, num_workers=num_workers, seed=seed, dataset=dataset)
A:torch.utils.data._utils.worker.fetcher->torch.utils.data._DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)
A:torch.utils.data._utils.worker.init_exception->ExceptionWrapper(where='in DataLoader worker process {}'.format(worker_id))
A:torch.utils.data._utils.worker.watchdog->ManagerWatchdog()
A:torch.utils.data._utils.worker.r->index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data._utils.worker.data->ExceptionWrapper(where='in DataLoader worker process {}'.format(worker_id))
torch.utils.data._utils.worker.WorkerInfo(self,**kwargs)
torch.utils.data._utils.worker.WorkerInfo.__init__(self,**kwargs)
torch.utils.data._utils.worker.WorkerInfo.__repr__(self)
torch.utils.data._utils.worker.WorkerInfo.__setattr__(self,key,val)
torch.utils.data._utils.worker._IterableDatasetStopIteration(object)
torch.utils.data._utils.worker._ResumeIteration(object)
torch.utils.data._utils.worker._generate_state(base_seed,worker_id)
torch.utils.data._utils.worker._worker_loop(dataset_kind,dataset,index_queue,data_queue,done_event,auto_collation,collate_fn,drop_last,base_seed,init_fn,worker_id,num_workers,persistent_workers)
torch.utils.data._utils.worker.get_worker_info()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_utils/pin_memory.py----------------------------------------
A:torch.utils.data._utils.pin_memory.r->in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data._utils.pin_memory.data->ExceptionWrapper(where='in pin memory thread for device {}'.format(device_id))
torch.utils.data._utils.pin_memory._pin_memory_loop(in_queue,out_queue,device_id,done_event)
torch.utils.data._utils.pin_memory.pin_memory(data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_utils/collate.py----------------------------------------
A:torch.utils.data._utils.collate.np_str_obj_array_pattern->re.compile('[SaUO]')
A:torch.utils.data._utils.collate.elem_type->type(elem)
A:torch.utils.data._utils.collate.numel->sum((x.numel() for x in batch))
A:torch.utils.data._utils.collate.storage->elem.storage()._new_shared(numel)
A:torch.utils.data._utils.collate.out->elem.new(storage).resize_(len(batch), *list(elem.size()))
A:torch.utils.data._utils.collate.it->iter(batch)
A:torch.utils.data._utils.collate.elem_size->len(next(it))
A:torch.utils.data._utils.collate.transposed->list(zip(*batch))
torch.utils.data._utils.collate.default_collate(batch)
torch.utils.data._utils.collate.default_convert(data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_utils/signal_handling.py----------------------------------------
A:torch.utils.data._utils.signal_handling.previous_handler->signal.getsignal(signal.SIGCHLD)
torch.utils.data._utils.signal_handling._set_SIGCHLD_handler()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_utils/__init__.py----------------------------------------
torch.utils.data._utils.__init__._set_python_exit_flag()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/utils/data/_utils/fetch.py----------------------------------------
A:torch.utils.data._utils.fetch.self.dataset_iter->iter(dataset)
A:torch.utils.data._utils.fetch.data->next(self.dataset_iter)
torch.utils.data._utils.fetch._BaseDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._BaseDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._BaseDatasetFetcher.fetch(self,possibly_batched_index)
torch.utils.data._utils.fetch._IterableDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._IterableDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._IterableDatasetFetcher.fetch(self,possibly_batched_index)
torch.utils.data._utils.fetch._MapDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._MapDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._MapDatasetFetcher.fetch(self,possibly_batched_index)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/special/__init__.py----------------------------------------
A:torch.special.__init__.entr->_add_docstr(_special.special_entr, '\nentr(input, *, out=None) -> Tensor\nComputes the entropy on :attr:`input` (as defined below), elementwise.\n\n.. math::\n    \\begin{align}\n    \\text{entr(x)} = \\begin{cases}\n        -x * \\ln(x)  & x > 0 \\\\\n        0 &  x = 0.0 \\\\\n        -\\infty & x < 0\n    \\end{cases}\n    \\end{align}\n' + '\n\nArgs:\n   input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n    >>> a = torch.arange(-0.5, 1, 0.5)\n    >>> a\n    tensor([-0.5000,  0.0000,  0.5000])\n    >>> torch.special.entr(a)\n    tensor([  -inf, 0.0000, 0.3466])\n')
A:torch.special.__init__.psi->_add_docstr(_special.special_psi, '\npsi(input, *, out=None) -> Tensor\n\nAlias for :func:`torch.special.digamma`.\n')
A:torch.special.__init__.digamma->_add_docstr(_special.special_digamma, "\ndigamma(input, *, out=None) -> Tensor\n\nComputes the logarithmic derivative of the gamma function on `input`.\n\n.. math::\n    \\digamma(x) = \\frac{d}{dx} \\ln\\left(\\Gamma\\left(x\\right)\\right) = \\frac{\\Gamma'(x)}{\\Gamma(x)}\n" + "\nArgs:\n    input (Tensor): the tensor to compute the digamma function on\n\nKeyword args:\n    {out}\n\n.. note::  This function is similar to SciPy's `scipy.special.digamma`.\n\n.. note::  From PyTorch 1.8 onwards, the digamma function returns `-Inf` for `0`.\n           Previously it returned `NaN` for `0`.\n\nExample::\n\n    >>> a = torch.tensor([1, 0.5])\n    >>> torch.special.digamma(a)\n    tensor([-0.5772, -1.9635])\n\n".format(**common_args))
A:torch.special.__init__.gammaln->_add_docstr(_special.special_gammaln, '\ngammaln(input, *, out=None) -> Tensor\n\nComputes the natural logarithm of the absolute value of the gamma function on :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\ln \\Gamma(|\\text{input}_{i}|)\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> a = torch.arange(0.5, 2, 0.5)\n    >>> torch.special.gammaln(a)\n    tensor([ 0.5724,  0.0000, -0.1208])\n\n'.format(**common_args))
A:torch.special.__init__.polygamma->_add_docstr(_special.special_polygamma, '\npolygamma(n, input, *, out=None) -> Tensor\n\nComputes the :math:`n^{th}` derivative of the digamma function on :attr:`input`.\n:math:`n \\geq 0` is called the order of the polygamma function.\n\n.. math::\n    \\psi^{(n)}(x) = \\frac{d^{(n)}}{dx^{(n)}} \\psi(x)\n\n.. note::\n    This function is implemented only for nonnegative integers :math:`n \\geq 0`.\n' + '\nArgs:\n    n (int): the order of the polygamma function\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> a = torch.tensor([1, 0.5])\n    >>> torch.special.polygamma(1, a)\n    tensor([1.64493, 4.9348])\n    >>> torch.special.polygamma(2, a)\n    tensor([ -2.4041, -16.8288])\n    >>> torch.special.polygamma(3, a)\n    tensor([ 6.4939, 97.4091])\n    >>> torch.special.polygamma(4, a)\n    tensor([ -24.8863, -771.4742])\n'.format(**common_args))
A:torch.special.__init__.erf->_add_docstr(_special.special_erf, '\nerf(input, *, out=None) -> Tensor\n\nComputes the error function of :attr:`input`. The error function is defined as follows:\n\n.. math::\n    \\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.special.erf(torch.tensor([0, -1., 10.]))\n    tensor([ 0.0000, -0.8427,  1.0000])\n'.format(**common_args))
A:torch.special.__init__.erfc->_add_docstr(_special.special_erfc, '\nerfc(input, *, out=None) -> Tensor\n\nComputes the complementary error function of :attr:`input`.\nThe complementary error function is defined as follows:\n\n.. math::\n    \\mathrm{erfc}(x) = 1 - \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.special.erfc(torch.tensor([0, -1., 10.]))\n    tensor([ 1.0000, 1.8427,  0.0000])\n'.format(**common_args))
A:torch.special.__init__.erfcx->_add_docstr(_special.special_erfcx, '\nerfcx(input, *, out=None) -> Tensor\n\nComputes the scaled complementary error function for each element of :attr:`input`.\nThe scaled complementary error function is defined as follows:\n\n.. math::\n    \\mathrm{erfcx}(x) = e^{x^2} \\mathrm{erfc}(x)\n' + '\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.special.erfcx(torch.tensor([0, -1., 10.]))\n    tensor([ 1.0000, 5.0090, 0.0561])\n'.format(**common_args))
A:torch.special.__init__.erfinv->_add_docstr(_special.special_erfinv, '\nerfinv(input, *, out=None) -> Tensor\n\nComputes the inverse error function of :attr:`input`.\nThe inverse error function is defined in the range :math:`(-1, 1)` as:\n\n.. math::\n    \\mathrm{erfinv}(\\mathrm{erf}(x)) = x\n' + '\n\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\n    tensor([ 0.0000,  0.4769,    -inf])\n'.format(**common_args))
A:torch.special.__init__.logit->_add_docstr(_special.special_logit, '\nlogit(input, eps=None, *, out=None) -> Tensor\n\nReturns a new tensor with the logit of the elements of :attr:`input`.\n:attr:`input` is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and :attr:`input` < 0 or :attr:`input` > 1, the function will yields NaN.\n\n.. math::\n    \\begin{align}\n    y_{i} &= \\ln(\\frac{z_{i}}{1 - z_{i}}) \\\\\n    z_{i} &= \\begin{cases}\n        x_{i} & \\text{if eps is None} \\\\\n        \\text{eps} & \\text{if } x_{i} < \\text{eps} \\\\\n        x_{i} & \\text{if } \\text{eps} \\leq x_{i} \\leq 1 - \\text{eps} \\\\\n        1 - \\text{eps} & \\text{if } x_{i} > 1 - \\text{eps}\n    \\end{cases}\n    \\end{align}\n' + '\nArgs:\n    {input}\n    eps (float, optional): the epsilon for input clamp bound. Default: ``None``\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> a = torch.rand(5)\n    >>> a\n    tensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n    >>> torch.special.logit(a, eps=1e-6)\n    tensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\n'.format(**common_args))
A:torch.special.__init__.logsumexp->_add_docstr(_special.special_logsumexp, '\nlogsumexp(input, dim, keepdim=False, *, out=None)\n\nAlias for :func:`torch.logsumexp`.\n'.format(**multi_dim_common))
A:torch.special.__init__.expit->_add_docstr(_special.special_expit, '\nexpit(input, *, out=None) -> Tensor\n\nComputes the expit (also known as the logistic sigmoid function) of the elements of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> t = torch.randn(4)\n    >>> t\n    tensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n    >>> torch.special.expit(t)\n    tensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n'.format(**common_args))
A:torch.special.__init__.exp2->_add_docstr(_special.special_exp2, '\nexp2(input, *, out=None) -> Tensor\n\nComputes the base two exponential function of :attr:`input`.\n\n.. math::\n    y_{i} = 2^{x_{i}}\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\n    tensor([ 1.,  2.,  8., 16.])\n'.format(**common_args))
A:torch.special.__init__.expm1->_add_docstr(_special.special_expm1, '\nexpm1(input, *, out=None) -> Tensor\n\nComputes the exponential of the elements minus 1\nof :attr:`input`.\n\n.. math::\n    y_{i} = e^{x_{i}} - 1\n\n.. note:: This function provides greater precision than exp(x) - 1 for small values of x.\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\n    tensor([ 0.,  1.])\n'.format(**common_args))
A:torch.special.__init__.xlog1py->_add_docstr(_special.special_xlog1py, "\nxlog1py(input, other, *, out=None) -> Tensor\n\nComputes ``input * log1p(other)`` with the following cases.\n\n.. math::\n    \\text{out}_{i} = \\begin{cases}\n        \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\\n        0 & \\text{if } \\text{input}_{i} = 0.0 \\text{ and } \\text{other}_{i} != \\text{NaN} \\\\\n        \\text{input}_{i} * \\text{log1p}(\\text{other}_{i})& \\text{otherwise}\n    \\end{cases}\n\nSimilar to SciPy's `scipy.special.xlog1py`.\n\n" + "\n\nArgs:\n    input (Number or Tensor) : Multiplier\n    other (Number or Tensor) : Argument\n\n.. note:: At least one of :attr:`input` or :attr:`other` must be a tensor.\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> x = torch.zeros(5,)\n    >>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n    >>> torch.special.xlog1py(x, y)\n    tensor([0., 0., 0., 0., nan])\n    >>> x = torch.tensor([1, 2, 3])\n    >>> y = torch.tensor([3, 2, 1])\n    >>> torch.special.xlog1py(x, y)\n    tensor([1.3863, 2.1972, 2.0794])\n    >>> torch.special.xlog1py(x, 4)\n    tensor([1.6094, 3.2189, 4.8283])\n    >>> torch.special.xlog1py(2, y)\n    tensor([2.7726, 2.1972, 1.3863])\n".format(**common_args))
A:torch.special.__init__.xlogy->_add_docstr(_special.special_xlogy, "\nxlogy(input, other, *, out=None) -> Tensor\n\nComputes ``input * log(other)`` with the following cases.\n\n.. math::\n    \\text{out}_{i} = \\begin{cases}\n        \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\\n        0 & \\text{if } \\text{input}_{i} = 0.0 \\\\\n        \\text{input}_{i} * \\log{(\\text{other}_{i})} & \\text{otherwise}\n    \\end{cases}\n\nSimilar to SciPy's `scipy.special.xlogy`.\n\n" + "\n\nArgs:\n    input (Number or Tensor) : Multiplier\n    other (Number or Tensor) : Argument\n\n.. note:: At least one of :attr:`input` or :attr:`other` must be a tensor.\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> x = torch.zeros(5,)\n    >>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n    >>> torch.special.xlogy(x, y)\n    tensor([0., 0., 0., 0., nan])\n    >>> x = torch.tensor([1, 2, 3])\n    >>> y = torch.tensor([3, 2, 1])\n    >>> torch.special.xlogy(x, y)\n    tensor([1.0986, 1.3863, 0.0000])\n    >>> torch.special.xlogy(x, 4)\n    tensor([1.3863, 2.7726, 4.1589])\n    >>> torch.special.xlogy(2, y)\n    tensor([2.1972, 1.3863, 0.0000])\n".format(**common_args))
A:torch.special.__init__.i0->_add_docstr(_special.special_i0, '\ni0(input, *, out=None) -> Tensor\n\nComputes the zeroth order modified Bessel function of the first kind for each element of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = I_0(\\text{input}_{i}) = \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2}\n\n' + '\nArgs:\n    input (Tensor): the input tensor\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> torch.i0(torch.arange(5, dtype=torch.float32))\n    tensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\n\n'.format(**common_args))
A:torch.special.__init__.i0e->_add_docstr(_special.special_i0e, '\ni0e(input, *, out=None) -> Tensor\nComputes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\exp(-|x|) * i0(x) = \\exp(-|x|) * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2}\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\n    tensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\n'.format(**common_args))
A:torch.special.__init__.i1->_add_docstr(_special.special_i1, '\ni1(input, *, out=None) -> Tensor\nComputes the first order modified Bessel function of the first kind (as defined below)\nfor each element of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\frac{(\\text{input}_{i})}{2} * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!) * (k+1)!}\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> torch.special.i1(torch.arange(5, dtype=torch.float32))\n    tensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])\n'.format(**common_args))
A:torch.special.__init__.i1e->_add_docstr(_special.special_i1e, '\ni1e(input, *, out=None) -> Tensor\nComputes the exponentially scaled first order modified Bessel function of the first kind (as defined below)\nfor each element of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\exp(-|x|) * i1(x) =\n        \\exp(-|x|) * \\frac{(\\text{input}_{i})}{2} * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!) * (k+1)!}\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> torch.special.i1e(torch.arange(5, dtype=torch.float32))\n    tensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])\n'.format(**common_args))
A:torch.special.__init__.ndtr->_add_docstr(_special.special_ndtr, '\nndtr(input, *, out=None) -> Tensor\nComputes the area under the standard Gaussian probability density function,\nintegrated from minus infinity to :attr:`input`, elementwise.\n\n.. math::\n    \\text{ndtr}(x) = \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{x} e^{-\\frac{1}{2}t^2} dt\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> torch.special.ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\n    tensor([0.0013, 0.0228, 0.1587, 0.5000, 0.8413, 0.9772, 0.9987])\n'.format(**common_args))
A:torch.special.__init__.ndtri->_add_docstr(_special.special_ndtri, '\nndtri(input, *, out=None) -> Tensor\nComputes the argument, x, for which the area under the Gaussian probability density function\n(integrated from minus infinity to x) is equal to :attr:`input`, elementwise.\n\n.. math::\n    \\text{ndtri}(p) = \\sqrt{2}\\text{erf}^{-1}(2p - 1)\n\n.. note::\n    Also known as quantile function for Normal Distribution.\n\n' + '\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> torch.special.ndtri(torch.tensor([0, 0.25, 0.5, 0.75, 1]))\n    tensor([   -inf, -0.6745,  0.0000,  0.6745,     inf])\n'.format(**common_args))
A:torch.special.__init__.log1p->_add_docstr(_special.special_log1p, '\nlog1p(input, *, out=None) -> Tensor\n\nAlias for :func:`torch.log1p`.\n')
A:torch.special.__init__.sinc->_add_docstr(_special.special_sinc, '\nsinc(input, *, out=None) -> Tensor\n\nComputes the normalized sinc of :attr:`input.`\n\n.. math::\n    \\text{out}_{i} =\n    \\begin{cases}\n      1, & \\text{if}\\ \\text{input}_{i}=0 \\\\\n      \\sin(\\pi \\text{input}_{i}) / (\\pi \\text{input}_{i}), & \\text{otherwise}\n    \\end{cases}\n' + '\n\nArgs:\n    {input}\n\nKeyword args:\n    {out}\n\nExample::\n    >>> t = torch.randn(4)\n    >>> t\n    tensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n    >>> torch.special.sinc(t)\n    tensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n'.format(**common_args))
A:torch.special.__init__.round->_add_docstr(_special.special_round, '\nround(input, *, out=None) -> Tensor\n\nAlias for :func:`torch.round`.\n')
A:torch.special.__init__.softmax->_add_docstr(_special.special_softmax, '\nsoftmax(input, dim, *, dtype=None) -> Tensor\n\nComputes the softmax function.\n\nSoftmax is defined as:\n\n:math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n\nIt is applied to all slices along dim, and will re-scale them so that the elements\nlie in the range `[0, 1]` and sum to 1.\n\nArgs:\n    input (Tensor): input\n    dim (int): A dimension along which softmax will be computed.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is cast to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExamples::\n    >>> t = torch.ones(2, 2)\n    >>> torch.special.softmax(t, 0)\n    tensor([[0.5000, 0.5000],\n            [0.5000, 0.5000]])\n\n')
A:torch.special.__init__.log_softmax->_add_docstr(_special.special_log_softmax, '\nlog_softmax(input, dim, *, dtype=None) -> Tensor\n\nComputes softmax followed by a logarithm.\n\nWhile mathematically equivalent to log(softmax(x)), doing these two\noperations separately is slower and numerically unstable. This function\nis computed as:\n\n.. math::\n    \\text{log\\_softmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n' + '\n\nArgs:\n    input (Tensor): input\n    dim (int): A dimension along which log_softmax will be computed.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is cast to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExample::\n    >>> t = torch.ones(2, 2)\n    >>> torch.special.log_softmax(t, 0)\n    tensor([[-0.6931, -0.6931],\n            [-0.6931, -0.6931]])\n')
A:torch.special.__init__.zeta->_add_docstr(_special.special_zeta, '\nzeta(input, other, *, out=None) -> Tensor\n\nComputes the Hurwitz zeta function, elementwise.\n\n.. math::\n    \\zeta(x, q) = \\sum_{k=0}^{\\infty} \\frac{1}{(k + q)^x}\n\n' + '\nArgs:\n    input (Tensor): the input tensor corresponding to `x`.\n    other (Tensor): the input tensor corresponding to `q`.\n\n.. note::\n    The Riemann zeta function corresponds to the case when `q = 1`\n\nKeyword args:\n    {out}\n\nExample::\n    >>> x = torch.tensor([2., 4.])\n    >>> torch.special.zeta(x, 1)\n    tensor([1.6449, 1.0823])\n    >>> torch.special.zeta(x, torch.tensor([1., 2.]))\n    tensor([1.6449, 0.0823])\n    >>> torch.special.zeta(2, torch.tensor([1., 2.]))\n    tensor([1.6449, 0.6449])\n'.format(**common_args))
A:torch.special.__init__.multigammaln->_add_docstr(_special.special_multigammaln, '\nmultigammaln(input, p, *, out=None) -> Tensor\n\nComputes the `multivariate log-gamma function\n<https://en.wikipedia.org/wiki/Multivariate_gamma_function>`_ with dimension\n:math:`p` element-wise, given by\n\n.. math::\n    \\log(\\Gamma_{p}(a)) = C + \\displaystyle \\sum_{i=1}^{p} \\log\\left(\\Gamma\\left(a - \\frac{i - 1}{2}\\right)\\right)\n\nwhere :math:`C = \\log(\\pi) \\times \\frac{p (p - 1)}{4}` and :math:`\\Gamma(\\cdot)` is the Gamma function.\n\nAll elements must be greater than :math:`\\frac{p - 1}{2}`, otherwise an error would be thrown.\n' + '\n\nArgs:\n    input (Tensor): the tensor to compute the multivariate log-gamma function\n    p (int): the number of dimensions\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> a = torch.empty(2, 3).uniform_(1, 2)\n    >>> a\n    tensor([[1.6835, 1.8474, 1.1929],\n            [1.0475, 1.7162, 1.4180]])\n    >>> torch.special.multigammaln(a, 2)\n    tensor([[0.3928, 0.4007, 0.7586],\n            [1.0311, 0.3901, 0.5049]])\n'.format(**common_args))
A:torch.special.__init__.gammainc->_add_docstr(_special.special_gammainc, "\ngammainc(input, other, *, out=None) -> Tensor\n\nComputes the regularized lower incomplete gamma function:\n\n.. math::\n    \\text{out}_{i} = \\frac{1}{\\Gamma(\\text{input}_i)} \\int_0^{\\text{other}_i} t^{\\text{input}_i-1} e^{-t} dt\n\nwhere both :math:`\\text{input}_i` and :math:`\\text{other}_i` are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then :math:`\\text{out}_i=\\text{nan}`.\n:math:`\\Gamma(\\cdot)` in the equation above is the gamma function,\n\n.. math::\n    \\Gamma(\\text{input}_i) = \\int_0^\\infty t^{(\\text{input}_i-1)} e^{-t} dt.\n\nSee :func:`torch.special.gammaincc` and :func:`torch.special.gammaln` for related functions.\n\nSupports :ref:`broadcasting to a common shape <broadcasting-semantics>`\nand float inputs.\n\n.. note::\n    The backward pass with respect to :attr:`input` is not yet supported.\n    Please open an issue on PyTorch's Github to request it.\n\n" + '\nArgs:\n    input (Tensor): the first non-negative input tensor\n    other (Tensor): the second non-negative input tensor\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> a1 = torch.tensor([4.0])\n    >>> a2 = torch.tensor([3.0, 4.0, 5.0])\n    >>> a = torch.special.gammaincc(a1, a2)\n    tensor([0.3528, 0.5665, 0.7350])\n    tensor([0.3528, 0.5665, 0.7350])\n    >>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)\n    tensor([1., 1., 1.])\n\n'.format(**common_args))
A:torch.special.__init__.gammaincc->_add_docstr(_special.special_gammaincc, "\ngammaincc(input, other, *, out=None) -> Tensor\n\nComputes the regularized upper incomplete gamma function:\n\n.. math::\n    \\text{out}_{i} = \\frac{1}{\\Gamma(\\text{input}_i)} \\int_{\\text{other}_i}^{\\infty} t^{\\text{input}_i-1} e^{-t} dt\n\nwhere both :math:`\\text{input}_i` and :math:`\\text{other}_i` are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then :math:`\\text{out}_i=\\text{nan}`.\n:math:`\\Gamma(\\cdot)` in the equation above is the gamma function,\n\n.. math::\n    \\Gamma(\\text{input}_i) = \\int_0^\\infty t^{(\\text{input}_i-1)} e^{-t} dt.\n\nSee :func:`torch.special.gammainc` and :func:`torch.special.gammaln` for related functions.\n\nSupports :ref:`broadcasting to a common shape <broadcasting-semantics>`\nand float inputs.\n\n.. note::\n    The backward pass with respect to :attr:`input` is not yet supported.\n    Please open an issue on PyTorch's Github to request it.\n\n" + '\nArgs:\n    input (Tensor): the first non-negative input tensor\n    other (Tensor): the second non-negative input tensor\n\nKeyword args:\n    {out}\n\nExample::\n\n    >>> a1 = torch.tensor([4.0])\n    >>> a2 = torch.tensor([3.0, 4.0, 5.0])\n    >>> a = torch.special.gammaincc(a1, a2)\n    tensor([0.6472, 0.4335, 0.2650])\n    >>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)\n    tensor([1., 1., 1.])\n\n'.format(**common_args))


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/multiprocessing/pool.py----------------------------------------
A:torch.multiprocessing.pool.self._inqueue->SimpleQueue()
A:torch.multiprocessing.pool.self._outqueue->SimpleQueue()
A:torch.multiprocessing.pool.w->self.Process(target=clean_worker, args=args)
A:torch.multiprocessing.pool.w.name->self.Process(target=clean_worker, args=args).name.replace('Process', 'PoolWorker')
torch.multiprocessing.pool.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.pool.Pool._repopulate_pool(self)
torch.multiprocessing.pool.Pool._setup_queues(self)
torch.multiprocessing.pool.clean_worker(*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/multiprocessing/queue.py----------------------------------------
A:torch.multiprocessing.queue.buf->self.recv_bytes()
torch.multiprocessing.queue.ConnectionWrapper(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.__getattr__(self,name)
torch.multiprocessing.queue.ConnectionWrapper.__init__(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.recv(self)
torch.multiprocessing.queue.ConnectionWrapper.send(self,obj)
torch.multiprocessing.queue.Queue(self,*args,**kwargs)
torch.multiprocessing.queue.Queue.__init__(self,*args,**kwargs)
torch.multiprocessing.queue.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.queue.SimpleQueue._make_methods(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/multiprocessing/_atfork.py----------------------------------------
torch.multiprocessing._atfork.register_after_fork(func)
torch.register_after_fork(func)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/multiprocessing/reductions.py----------------------------------------
A:torch.multiprocessing.reductions.self.cdata->cls._new_shared_filename(manager, handle, size)._weak_ref()
A:torch.multiprocessing.reductions.self.lock->threading.Lock()
A:torch.multiprocessing.reductions.self.limit->max(128, live * 2)
A:torch.multiprocessing.reductions.shared_cache->SharedCache()
A:torch.multiprocessing.reductions.handle->event.ipc_handle()
A:torch.multiprocessing.reductions.t->torch.nn.parameter.Parameter(t, requires_grad=requires_grad)
A:torch.multiprocessing.reductions.storage->cls._new_shared_filename(manager, handle, size)
A:torch.multiprocessing.reductions.shared_cache[storage_handle, storage_offset_bytes]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.(device, handle, storage_size_bytes, storage_offset_bytes, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required)->cls._new_shared_filename(manager, handle, size)._share_cuda_()
A:torch.multiprocessing.reductions.tensor_offset->tensor.storage_offset()
A:torch.multiprocessing.reductions.shared_cache[handle]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.stat->os.fstat(fd)
A:torch.multiprocessing.reductions.storage_ref->SharedCache().get(key)
A:torch.multiprocessing.reductions.fd->multiprocessing.reduction.DupFd(fd).detach()
A:torch.multiprocessing.reductions.shared_cache[fd_id(fd)]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.metadata->cls._new_shared_filename(manager, handle, size)._share_filename_()
A:torch.multiprocessing.reductions.(fd, size)->cls._new_shared_filename(manager, handle, size)._share_fd_()
A:torch.multiprocessing.reductions.df->multiprocessing.reduction.DupFd(fd)
A:torch.multiprocessing.reductions.cache_key->fd_id(fd)
A:torch.multiprocessing.reductions.shared_cache[cache_key]->StorageWeakRef(storage)
torch.multiprocessing.init_reductions()
torch.multiprocessing.reductions.SharedCache(self)
torch.multiprocessing.reductions.SharedCache.__init__(self)
torch.multiprocessing.reductions.SharedCache.__setitem__(self,key,storage_ref)
torch.multiprocessing.reductions.SharedCache._after_fork(self)
torch.multiprocessing.reductions.SharedCache.free_dead_references(self)
torch.multiprocessing.reductions.SharedCache.get(self,key)
torch.multiprocessing.reductions.StorageWeakRef(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.__del__(self)
torch.multiprocessing.reductions.StorageWeakRef.__init__(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.expired(self)
torch.multiprocessing.reductions.fd_id(fd)
torch.multiprocessing.reductions.init_reductions()
torch.multiprocessing.reductions.rebuild_cuda_tensor(tensor_cls,tensor_size,tensor_stride,tensor_offset,storage_cls,dtype,storage_device,storage_handle,storage_size_bytes,storage_offset_bytes,requires_grad,ref_counter_handle,ref_counter_offset,event_handle,event_sync_required)
torch.multiprocessing.reductions.rebuild_event(device,handle)
torch.multiprocessing.reductions.rebuild_storage_empty(cls)
torch.multiprocessing.reductions.rebuild_storage_fd(cls,df,size)
torch.multiprocessing.reductions.rebuild_storage_filename(cls,manager,handle,size)
torch.multiprocessing.reductions.rebuild_tensor(cls,storage,metadata)
torch.multiprocessing.reductions.rebuild_typed_storage(storage,dtype)
torch.multiprocessing.reductions.rebuild_typed_storage_child(storage,storage_type)
torch.multiprocessing.reductions.reduce_event(event)
torch.multiprocessing.reductions.reduce_storage(storage)
torch.multiprocessing.reductions.reduce_tensor(tensor)
torch.multiprocessing.reductions.reduce_typed_storage(storage)
torch.multiprocessing.reductions.reduce_typed_storage_child(storage)
torch.multiprocessing.reductions.storage_from_cache(cls,key)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/multiprocessing/__init__.py----------------------------------------
torch.multiprocessing.__init__.get_all_sharing_strategies()
torch.multiprocessing.__init__.get_sharing_strategy()
torch.multiprocessing.__init__.set_sharing_strategy(new_strategy)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/multiprocessing/spawn.py----------------------------------------
A:torch.multiprocessing.spawn.ready->multiprocessing.connection.wait(self.sentinels.keys(), timeout=timeout)
A:torch.multiprocessing.spawn.index->self.sentinels.pop(sentinel)
A:torch.multiprocessing.spawn.original_trace->self.error_queues[error_index].get()
A:torch.multiprocessing.spawn.mp->multiprocessing.get_context(start_method)
A:torch.multiprocessing.spawn.error_queue->multiprocessing.get_context(start_method).SimpleQueue()
A:torch.multiprocessing.spawn.process->multiprocessing.get_context(start_method).Process(target=_wrap, args=(fn, i, args, error_queue), daemon=daemon)
A:torch.multiprocessing.spawn.context->ProcessContext(processes, error_queues)
torch.multiprocessing.ProcessContext(self,processes,error_queues)
torch.multiprocessing.ProcessContext.join(self,timeout=None)
torch.multiprocessing.ProcessContext.pids(self)
torch.multiprocessing.ProcessExitedException(self,msg:str,error_index:int,error_pid:int,exit_code:int,signal_name:Optional[str]=None)
torch.multiprocessing.ProcessExitedException.__reduce__(self)
torch.multiprocessing.ProcessRaisedException(self,msg:str,error_index:int,error_pid:int)
torch.multiprocessing.SpawnContext(self,processes,error_queues)
torch.multiprocessing.spawn(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')
torch.multiprocessing.spawn.ProcessContext(self,processes,error_queues)
torch.multiprocessing.spawn.ProcessContext.__init__(self,processes,error_queues)
torch.multiprocessing.spawn.ProcessContext.join(self,timeout=None)
torch.multiprocessing.spawn.ProcessContext.pids(self)
torch.multiprocessing.spawn.ProcessException(self,msg:str,error_index:int,pid:int)
torch.multiprocessing.spawn.ProcessException.__init__(self,msg:str,error_index:int,pid:int)
torch.multiprocessing.spawn.ProcessException.__reduce__(self)
torch.multiprocessing.spawn.ProcessExitedException(self,msg:str,error_index:int,error_pid:int,exit_code:int,signal_name:Optional[str]=None)
torch.multiprocessing.spawn.ProcessExitedException.__init__(self,msg:str,error_index:int,error_pid:int,exit_code:int,signal_name:Optional[str]=None)
torch.multiprocessing.spawn.ProcessExitedException.__reduce__(self)
torch.multiprocessing.spawn.ProcessRaisedException(self,msg:str,error_index:int,error_pid:int)
torch.multiprocessing.spawn.ProcessRaisedException.__init__(self,msg:str,error_index:int,error_pid:int)
torch.multiprocessing.spawn.SpawnContext(self,processes,error_queues)
torch.multiprocessing.spawn.SpawnContext.__init__(self,processes,error_queues)
torch.multiprocessing.spawn._wrap(fn,i,args,error_queue)
torch.multiprocessing.spawn.spawn(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')
torch.multiprocessing.spawn.start_processes(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')
torch.multiprocessing.start_processes(fn,args=(),nprocs=1,join=True,daemon=False,start_method='spawn')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/__init__.py----------------------------------------
torch.backends.__init__.ContextProp(self,getter,setter)
torch.backends.__init__.ContextProp.__get__(self,obj,objtype)
torch.backends.__init__.ContextProp.__init__(self,getter,setter)
torch.backends.__init__.ContextProp.__set__(self,obj,val)
torch.backends.__init__.PropModule(self,m,name)
torch.backends.__init__.PropModule.__getattr__(self,attr)
torch.backends.__init__.PropModule.__init__(self,m,name)
torch.backends.__init__.__allow_nonbracketed_mutation()
torch.backends.__init__.disable_global_flags()
torch.backends.__init__.flags_frozen()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/cuda/__init__.py----------------------------------------
A:torch.backends.cuda.__init__.size->cuFFTPlanCacheAttrContextProp(torch._cufft_get_plan_cache_size, '.size is a read-only property showing the number of plans currently in the cache. To change the cache capacity, set cufft_plan_cache.max_size.')
A:torch.backends.cuda.__init__.max_size->cuFFTPlanCacheAttrContextProp(torch._cufft_get_plan_cache_max_size, torch._cufft_set_plan_cache_max_size)
A:torch.backends.cuda.__init__.index->torch.cuda._utils._get_device_index(device)
A:torch.backends.cuda.__init__._LinalgBackends_str->', '.join(_LinalgBackends.keys())
A:torch.backends.cuda.__init__.cufft_plan_cache->cuFFTPlanCacheManager()
A:torch.backends.cuda.__init__.matmul->cuBLASModule()
torch.backends.cuda.__init__.cuBLASModule
torch.backends.cuda.__init__.cuBLASModule.__getattr__(self,name)
torch.backends.cuda.__init__.cuBLASModule.__setattr__(self,name,value)
torch.backends.cuda.__init__.cuFFTPlanCache(self,device_index)
torch.backends.cuda.__init__.cuFFTPlanCache.__init__(self,device_index)
torch.backends.cuda.__init__.cuFFTPlanCache.clear(self)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp(self,getter,setter)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__get__(self,obj,objtype)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__init__(self,getter,setter)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__set__(self,obj,val)
torch.backends.cuda.__init__.cuFFTPlanCacheManager(self)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__getattr__(self,name)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__getitem__(self,device)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__init__(self)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__setattr__(self,name,value)
torch.backends.cuda.__init__.is_built()
torch.backends.cuda.__init__.preferred_linalg_library(backend:Union[None,str,torch._C._LinalgBackend]=None)->torch._C._LinalgBackend


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/cudnn/__init__.py----------------------------------------
A:torch.backends.cudnn.__init__.__cudnn_version->torch._C._cudnn.getVersionInt()
A:torch.backends.cudnn.__init__.runtime_version->torch._C._cudnn.getRuntimeVersion()
A:torch.backends.cudnn.__init__.compile_version->torch._C._cudnn.getCompileVersion()
A:torch.backends.cudnn.__init__.orig_flags->set_flags(enabled, benchmark, deterministic, allow_tf32)
A:torch.backends.cudnn.__init__.enabled->ContextProp(torch._C._get_cudnn_enabled, torch._C._set_cudnn_enabled)
A:torch.backends.cudnn.__init__.deterministic->ContextProp(torch._C._get_cudnn_deterministic, torch._C._set_cudnn_deterministic)
A:torch.backends.cudnn.__init__.benchmark->ContextProp(torch._C._get_cudnn_benchmark, torch._C._set_cudnn_benchmark)
A:torch.backends.cudnn.__init__.allow_tf32->ContextProp(torch._C._get_cudnn_allow_tf32, torch._C._set_cudnn_allow_tf32)
A:torch.backends.cudnn.__init__.sys.modules[__name__]->CudnnModule(sys.modules[__name__], __name__)
torch.backends.cudnn.__init__.CudnnModule(self,m,name)
torch.backends.cudnn.__init__.CudnnModule.__init__(self,m,name)
torch.backends.cudnn.__init__.flags(enabled=False,benchmark=False,deterministic=False,allow_tf32=True)
torch.backends.cudnn.__init__.is_acceptable(tensor)
torch.backends.cudnn.__init__.is_available()
torch.backends.cudnn.__init__.set_flags(_enabled=None,_benchmark=None,_deterministic=None,_allow_tf32=None)
torch.backends.cudnn.__init__.version()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/cudnn/rnn.py----------------------------------------
A:torch.backends.cudnn.rnn.dropout_state[dropout_desc_name]->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda')))
A:torch.backends.cudnn.rnn.dropout_ts->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda'))).get()
torch.backends.cudnn.rnn.Unserializable(self,inner)
torch.backends.cudnn.rnn.Unserializable.__getstate__(self)
torch.backends.cudnn.rnn.Unserializable.__init__(self,inner)
torch.backends.cudnn.rnn.Unserializable.__setstate__(self,state)
torch.backends.cudnn.rnn.Unserializable.get(self)
torch.backends.cudnn.rnn.get_cudnn_mode(mode)
torch.backends.cudnn.rnn.init_dropout_state(dropout,train,dropout_seed,dropout_state)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/_coreml/preprocess.py----------------------------------------
A:torch.backends._coreml.preprocess.ml_type->_convert_to_mil_type(input_spec, name)
A:torch.backends._coreml.preprocess.forward_spec->_CompileSpec(*spec)
A:torch.backends._coreml.preprocess.input_spec->_TensorSpec(*input_spec)
A:torch.backends._coreml.preprocess.model->torch.jit.RecursiveScriptModule._construct(script_module, lambda x: None)
A:torch.backends._coreml.preprocess.mlmodel->mlmodel.get_spec().SerializeToString()
A:torch.backends._coreml.preprocess.spec->mlmodel.get_spec().SerializeToString().get_spec()
A:torch.backends._coreml.preprocess.output_spec->_TensorSpec(*output_spec)
torch.backends._coreml.preprocess.CompileSpec(*args,**kwargs)
torch.backends._coreml.preprocess.CoreMLComputeUnit
torch.backends._coreml.preprocess.ScalarType
torch.backends._coreml.preprocess.TensorSpec(*args,**kwargs)
torch.backends._coreml.preprocess._CompileSpec
torch.backends._coreml.preprocess._TensorSpec
torch.backends._coreml.preprocess._convert_to_mil_type(spec:_TensorSpec,name:str)
torch.backends._coreml.preprocess.preprocess(script_module:torch._C.ScriptObject,compile_spec:Dict[str,Tuple])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/_coreml/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/openmp/__init__.py----------------------------------------
torch.backends.openmp.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/_nnapi/prepare.py----------------------------------------
A:torch.backends._nnapi.prepare.self.out_templates->self.shape_compute_module.prepare(self.ser_model, args)
A:torch.backends._nnapi.prepare.comp->torch.classes._nnapi.Compilation()
A:torch.backends._nnapi.prepare.outs[idx]->outs[idx].permute(0, 3, 1, 2).permute(0, 3, 1, 2)
A:torch.backends._nnapi.prepare.(shape_compute_module, ser_model_tensor, used_weights, inp_mem_fmts, out_mem_fmts, retval_count)->process_for_nnapi(model, inputs, serializer, return_shapes, use_int16_for_qint16)
A:torch.backends._nnapi.prepare.nnapi_model->NnapiModule(shape_compute_module, ser_model_tensor, used_weights, inp_mem_fmts, out_mem_fmts)
A:torch.backends._nnapi.prepare.wrapper_model_py->NnapiInterfaceWrapper(nnapi_model)
A:torch.backends._nnapi.prepare.wrapper_model->torch.jit.script(wrapper_model_py)
A:torch.backends._nnapi.prepare.arg_list->', '.join((f'arg_{idx}' for idx in range(len(inputs))))
A:torch.backends._nnapi.prepare.ret_expr->''.join((f'retvals[{idx}], ' for idx in range(retval_count)))
A:torch.backends._nnapi.prepare.model->torch.jit.freeze(model)
A:torch.backends._nnapi.prepare.(ser_model, used_weights, inp_mem_fmts, out_mem_fmts, shape_compute_lines, retval_count)->serializer.serialize_model(model, inputs, return_shapes)
A:torch.backends._nnapi.prepare.ser_model_tensor->torch.tensor(ser_model, dtype=torch.int32)
A:torch.backends._nnapi.prepare.shape_compute_module->torch.jit.script(ShapeComputeModule())
torch.backends._nnapi.prepare.NnapiModule(self,shape_compute_module:torch.nn.Module,ser_model:torch.Tensor,weights:List[torch.Tensor],inp_mem_fmts:List[int],out_mem_fmts:List[int])
torch.backends._nnapi.prepare.NnapiModule.__init__(self,shape_compute_module:torch.nn.Module,ser_model:torch.Tensor,weights:List[torch.Tensor],inp_mem_fmts:List[int],out_mem_fmts:List[int])
torch.backends._nnapi.prepare.NnapiModule.forward(self,args:List[torch.Tensor])->List[torch.Tensor]
torch.backends._nnapi.prepare.NnapiModule.init(self,args:List[torch.Tensor])
torch.backends._nnapi.prepare.convert_model_to_nnapi(model,inputs,serializer=None,return_shapes=None,use_int16_for_qint16=False)
torch.backends._nnapi.prepare.process_for_nnapi(model,inputs,serializer=None,return_shapes=None,use_int16_for_qint16=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/_nnapi/serializer.py----------------------------------------
A:torch.backends._nnapi.serializer.LOG->logging.getLogger('nnapi_serialize')
A:torch.backends._nnapi.serializer.ls->list(tup)
A:torch.backends._nnapi.serializer.s1->list(shape1)
A:torch.backends._nnapi.serializer.s2->list(shape2)
A:torch.backends._nnapi.serializer.operand_id->self.add_tensor_operand_for_weight(value)
A:torch.backends._nnapi.serializer.dtype->str(tensor.dtype).replace('torch.', '')
A:torch.backends._nnapi.serializer.scale->tensor.permute(0, 2, 3, 1).q_scale()
A:torch.backends._nnapi.serializer.zero_point->tensor.permute(0, 2, 3, 1).q_zero_point()
A:torch.backends._nnapi.serializer.nnapi_dtype->getattr(tensor, 'nnapi_dtype', None)
A:torch.backends._nnapi.serializer.toper->self.torch_tensor_to_operand(tensor, dim_order)
A:torch.backends._nnapi.serializer.tsize->tensor_size(toper.op_type, toper.shape)
A:torch.backends._nnapi.serializer.buf_num->len(self.used_weights)
A:torch.backends._nnapi.serializer.tensor->tensor.permute(0, 2, 3, 1).permute(0, 2, 3, 1)
A:torch.backends._nnapi.serializer.(op_id, oper)->self.get_tensor_operand_by_jitval(jitval)
A:torch.backends._nnapi.serializer.(_, value)->self.get_constant_value(jitval, 'TensorType')
A:torch.backends._nnapi.serializer.record->self.constants.get(jitval)
A:torch.backends._nnapi.serializer.shape_code->''.join(shape_parts)
A:torch.backends._nnapi.serializer.out_oper->image_oper._replace(shape=out_shape, scale=out_scale, zero_point=out_zero_point)
A:torch.backends._nnapi.serializer.inputs[1]->self.add_immediate_float_scalar(1)
A:torch.backends._nnapi.serializer.outputs[0]->self.add_tensor_operand(node.outputsAt(0), input_oper._replace(shape=out_shape))
A:torch.backends._nnapi.serializer.(ctype, value)->self.get_constant_value(jit_bias)
A:torch.backends._nnapi.serializer.strides->self.get_size_arg(stride)
A:torch.backends._nnapi.serializer.paddings->self.get_size_arg(padding)
A:torch.backends._nnapi.serializer.dilations->self.get_size_arg(dilation)
A:torch.backends._nnapi.serializer.(_, group_num)->self.get_constant_value(group, 'IntType')
A:torch.backends._nnapi.serializer.kernels->list(kernel_size)
A:torch.backends._nnapi.serializer.self_jitval->next(model.graph.inputs())
A:torch.backends._nnapi.serializer.op_id->self.add_tensor_operand_for_input(arg_idx, input_value, input_tensor)
A:torch.backends._nnapi.serializer.retn->model.graph.return_node()
A:torch.backends._nnapi.serializer.retn_input->model.graph.return_node().inputsAt(0)
A:torch.backends._nnapi.serializer.retval_count->len(return_values)
A:torch.backends._nnapi.serializer.header->struct.pack('iiiiii', version, len(self.operands), len(self.values), len(self.operations), len(self.inputs), len(self.outputs))
A:torch.backends._nnapi.serializer.(serialized_values, serialized_value_data)->self.serialize_values()
A:torch.backends._nnapi.serializer.model_offset->int(model_offset / 4)
A:torch.backends._nnapi.serializer.shape->' + '.join((flex_name(ip_id, dim) for ip_id in in_ids))
A:torch.backends._nnapi.serializer.pt_d->reverse_map_dim(dim_order, d)
A:torch.backends._nnapi.serializer.source_length->len(data)
A:torch.backends._nnapi.serializer.adder->self.ADDER_MAP.get(node.kind())
A:torch.backends._nnapi.serializer.(in_id, in_oper)->self.get_tensor_operand_by_jitval(node.inputsAt(0))
A:torch.backends._nnapi.serializer.jitval->node.outputsAt(0)
A:torch.backends._nnapi.serializer.(obj_ctype, obj)->self.get_constant_value(node.inputsAt(0))
A:torch.backends._nnapi.serializer.name->node.s('name')
A:torch.backends._nnapi.serializer.value->node.outputsAt(0).toIValue()
A:torch.backends._nnapi.serializer.output->node.outputsAt(0)
A:torch.backends._nnapi.serializer.ctype->node.outputsAt(0).type()
A:torch.backends._nnapi.serializer.(_, val)->self.get_constant_value(inp)
A:torch.backends._nnapi.serializer.(_, dim)->self.get_constant_value(jit_dim, 'IntType')
A:torch.backends._nnapi.serializer.out_shape_list->list(in_oper.shape)
A:torch.backends._nnapi.serializer.out_shape->get_conv_pool_shape(image_oper.shape, args, out_c, transpose)
A:torch.backends._nnapi.serializer.(shape_ctype, shape)->self.get_constant_value(node.inputsAt(1))
A:torch.backends._nnapi.serializer.(start_ctype, start_dim)->self.get_constant_value(node.inputsAt(1), 'IntType')
A:torch.backends._nnapi.serializer.(end_ctype, end_dim)->self.get_constant_value(node.inputsAt(2), 'IntType')
A:torch.backends._nnapi.serializer.out_id->self.add_tensor_operand(jit_out, out_oper)
A:torch.backends._nnapi.serializer.inputs_1->tuple((dim if dim != 0 else -1 for dim in out_shape))
A:torch.backends._nnapi.serializer.(_, dim_value)->self.get_constant_value(node.inputsAt(1))
A:torch.backends._nnapi.serializer.(_, start_value)->self.get_constant_value(node.inputsAt(2))
A:torch.backends._nnapi.serializer.(_, stop_value)->self.get_constant_value(node.inputsAt(3))
A:torch.backends._nnapi.serializer.(_, step_value)->self.get_constant_value(node.inputsAt(4))
A:torch.backends._nnapi.serializer.inputs[2]->self.add_immediate_int_scalar(dim)
A:torch.backends._nnapi.serializer.inputs[3]->self.add_immediate_int_scalar(args.pad_l)
A:torch.backends._nnapi.serializer.inputs[4]->self.add_immediate_int_scalar(args.pad_r)
A:torch.backends._nnapi.serializer.inputs[5]->self.add_immediate_int_scalar(args.pad_t)
A:torch.backends._nnapi.serializer.inputs[6]->self.add_immediate_int_scalar(args.pad_b)
A:torch.backends._nnapi.serializer.(_, in_oper)->self.get_tensor_operand_by_jitval_fixed_size(node.inputsAt(0))
A:torch.backends._nnapi.serializer.(dim_ctype, dim)->self.get_constant_value(node.inputsAt(1))
A:torch.backends._nnapi.serializer.(_, keep_dim)->self.get_constant_value(node.inputsAt(2), 'BoolType')
A:torch.backends._nnapi.serializer.collapsed_dims->set()
A:torch.backends._nnapi.serializer.(_, scale)->self.get_constant_value(node.inputsAt(2), 'FloatType')
A:torch.backends._nnapi.serializer.(_, zero_point)->self.get_constant_value(node.inputsAt(3), 'IntType')
A:torch.backends._nnapi.serializer.(_, scalar_type)->self.get_constant_value(node.inputsAt(3), 'IntType')
A:torch.backends._nnapi.serializer.(in0_id, in0_oper)->self.get_tensor_operand_or_constant(node.inputsAt(0), in1_oper.dim_order)
A:torch.backends._nnapi.serializer.(in1_id, in1_oper)->self.get_tensor_operand_by_jitval(node.inputsAt(1))
A:torch.backends._nnapi.serializer.(in0_id, in0_oper, in1_id, in1_oper)->self.transpose_for_broadcast(in0_id, in0_oper, in1_id, in1_oper)
A:torch.backends._nnapi.serializer.(_, alpha)->self.get_constant_value(node.inputsAt(2), 'IntType')
A:torch.backends._nnapi.serializer.(_, softmax_dim)->self.get_constant_value(node.inputsAt(1), 'IntType')
A:torch.backends._nnapi.serializer.(_, min_val)->self.get_constant_value(node.inputsAt(1), 'FloatType')
A:torch.backends._nnapi.serializer.(_, max_val)->self.get_constant_value(node.inputsAt(2), 'FloatType')
A:torch.backends._nnapi.serializer.opcode->op_map.get((min_val, max_val))
A:torch.backends._nnapi.serializer.(w_id, w_oper)->self.get_tensor_operand_for_weight(node.inputsAt(1))
A:torch.backends._nnapi.serializer.(image, kernel, stride, padding, dilation, ceil_mode)->node.inputs()
A:torch.backends._nnapi.serializer.args->self.get_conv_pool_args_2d_from_pack(raw_weight.shape[2:4], packed_config)
A:torch.backends._nnapi.serializer.(image_id, image_oper)->self.get_tensor_operand_by_jitval(jit_image)
A:torch.backends._nnapi.serializer.use_nchw->image_oper.use_nchw()
A:torch.backends._nnapi.serializer.inputs[7]->self.add_immediate_int_scalar(args.stride_w)
A:torch.backends._nnapi.serializer.inputs[8]->self.add_immediate_int_scalar(args.stride_h)
A:torch.backends._nnapi.serializer.inputs[9]->self.add_immediate_int_scalar(fuse_code)
A:torch.backends._nnapi.serializer.inputs[10]->self.add_immediate_bool_scalar(use_nchw)
A:torch.backends._nnapi.serializer.(image, kernel, stride, padding, ceil_mode, count_include_pad, divisor_override)->node.inputs()
A:torch.backends._nnapi.serializer.(_, count_include_pad_value)->self.get_constant_value(count_include_pad)
A:torch.backends._nnapi.serializer.(_, divisor_override_value)->self.get_constant_value(divisor_override)
A:torch.backends._nnapi.serializer.(size_ctype, size_arg)->self.get_constant_value(size_jit)
A:torch.backends._nnapi.serializer.(image, size_jit, scale_jit)->node.inputs()
A:torch.backends._nnapi.serializer.(scale_ctype, scale_arg)->self.get_constant_value(scale_jit)
A:torch.backends._nnapi.serializer.arg_h->self.add_immediate_float_scalar(scale_arg[0])
A:torch.backends._nnapi.serializer.arg_w->self.add_immediate_float_scalar(scale_arg[1])
A:torch.backends._nnapi.serializer.out_h->int(scale_arg[0] * image_oper.shape[2])
A:torch.backends._nnapi.serializer.out_w->int(scale_arg[1] * image_oper.shape[3])
A:torch.backends._nnapi.serializer.(jit_bias, jit_input, jit_weight, jit_beta, jit_alpha)->node.inputs()
A:torch.backends._nnapi.serializer.(scale_ctype, scale_value)->self.get_constant_value(jitval)
A:torch.backends._nnapi.serializer.(jit_input, jit_weight, jit_bias)->node.inputs()
A:torch.backends._nnapi.serializer.(input_id, input_oper)->self.get_tensor_operand_by_jitval_fixed_size(jit_input)
A:torch.backends._nnapi.serializer.(bias_id, bias_oper)->self.get_optional_bias(jit_bias, weight_tensor, transpose)
A:torch.backends._nnapi.serializer.(_, weight_tensor)->self.get_constant_value(jit_weight, 'TensorType')
A:torch.backends._nnapi.serializer.nnapi_weight_tensor->weight_tensor.permute(*weight_permutation).contiguous()
A:torch.backends._nnapi.serializer.weight_id->self.add_tensor_operand_for_weight(nnapi_weight_tensor)
A:torch.backends._nnapi.serializer.(jit_input, jit_packed_weight, jit_scale, jit_zero_point)->node.inputs()
A:torch.backends._nnapi.serializer.(_, out_scale)->self.get_constant_value(jit_scale, 'FloatType')
A:torch.backends._nnapi.serializer.(_, out_zero_point)->self.get_constant_value(jit_zero_point, 'IntType')
A:torch.backends._nnapi.serializer.(weight_ctype, packed_weight)->self.get_constant_value(jit_packed_weight)
A:torch.backends._nnapi.serializer.unsigned_weight->torch._make_per_tensor_quantized_tensor((raw_weight.int_repr().int() + 128).to(torch.uint8), scale=raw_weight.q_scale(), zero_point=raw_weight.q_zero_point() + 128)
A:torch.backends._nnapi.serializer.weight_scale->torch._make_per_tensor_quantized_tensor((raw_weight.int_repr().int() + 128).to(torch.uint8), scale=raw_weight.q_scale(), zero_point=raw_weight.q_zero_point() + 128).q_scale()
A:torch.backends._nnapi.serializer.int_bias->torch.quantize_per_tensor(raw_bias, bias_scale, 0, torch.qint32)
A:torch.backends._nnapi.serializer.bias_id->self.add_tensor_operand_for_weight(int_bias)
A:torch.backends._nnapi.serializer.nnapi_bias_tensor->torch.zeros(weight_tensor.size()[bias_idx], dtype=weight_tensor.dtype)
A:torch.backends._nnapi.serializer.(jit_image, jit_weight, jit_bias, jit_stride, jit_pad, jit_dilation, jit_groups)->node.inputs()
A:torch.backends._nnapi.serializer.(jit_image, jit_weight, jit_bias, jit_stride, jit_pad, jit_dilation, jit_transpose, _, jit_groups, _, _, _, _)->node.inputs()
A:torch.backends._nnapi.serializer.(_, transpose)->self.get_constant_value(jit_transpose)
A:torch.backends._nnapi.serializer.(jit_input, jit_dim, jit_half_to_float)->node.inputs()
A:torch.backends._nnapi.serializer.(jit_image, jit_packed_weight, jit_scale, jit_zero_point)->node.inputs()
A:torch.backends._nnapi.serializer.(_, image_oper)->self.get_tensor_operand_by_jitval(jit_image)
A:torch.backends._nnapi.serializer.inputs[11]->self.add_immediate_bool_scalar(use_nchw)
torch.backends._nnapi.serializer.ConvPoolArgs2d(NamedTuple)
torch.backends._nnapi.serializer.DimOrder(enum.Enum)
torch.backends._nnapi.serializer.NNAPI_FuseCode(object)
torch.backends._nnapi.serializer.NNAPI_OperandCode(object)
torch.backends._nnapi.serializer.NNAPI_OperationCode(object)
torch.backends._nnapi.serializer.Operand(NamedTuple)
torch.backends._nnapi.serializer.Operand.use_nchw(self)
torch.backends._nnapi.serializer.OperandValueSourceType(object)
torch.backends._nnapi.serializer.TorchScalarTypes(enum.Enum)
torch.backends._nnapi.serializer._NnapiSerializer(self,config,use_int16_for_qint16=False)
torch.backends._nnapi.serializer._NnapiSerializer.__init__(self,config,use_int16_for_qint16=False)
torch.backends._nnapi.serializer._NnapiSerializer._do_add_binary(self,node,opcode,fuse_code,*,qparams=None)
torch.backends._nnapi.serializer._NnapiSerializer._handle_conv_pool_flexible_input(self,out_id,jit_image,args,transpose)
torch.backends._nnapi.serializer._NnapiSerializer._identity(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_adaptive_avg_pool2d(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_add_sub_op(self,node,opcode,fuse_code)
torch.backends._nnapi.serializer._NnapiSerializer.add_addmm(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_addmm_or_linear(self,node,transpose_weight,jit_input,jit_weight,jit_bias)
torch.backends._nnapi.serializer._NnapiSerializer.add_anonymous_tensor_operand(self,oper)
torch.backends._nnapi.serializer._NnapiSerializer.add_avg_pool2d(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_cat(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_constant_node(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_constant_value(self,jitval,ctype,value)
torch.backends._nnapi.serializer._NnapiSerializer.add_conv2d(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_conv2d_common(self,jit_out,out_scale,out_zero_point,jit_image,weight_tensor,bias_id,args,transpose,fuse_code)
torch.backends._nnapi.serializer._NnapiSerializer.add_conv_underscore(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_dequantize(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_flatten(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_getattr(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_hardtanh(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_immediate_bool_scalar(self,value)
torch.backends._nnapi.serializer._NnapiSerializer.add_immediate_float_scalar(self,value)
torch.backends._nnapi.serializer._NnapiSerializer.add_immediate_int_scalar(self,value)
torch.backends._nnapi.serializer._NnapiSerializer.add_immediate_int_vector(self,value)
torch.backends._nnapi.serializer._NnapiSerializer.add_immediate_operand(self,code,value,dims)
torch.backends._nnapi.serializer._NnapiSerializer.add_linear(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_list_construct(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_log_softmax(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_mean(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_node(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_operation(self,opcode,inputs,outputs)
torch.backends._nnapi.serializer._NnapiSerializer.add_pointwise_simple_binary_broadcast_op(self,node,opcode,fuse_code)
torch.backends._nnapi.serializer._NnapiSerializer.add_pointwise_simple_unary_op(self,node,opcode)
torch.backends._nnapi.serializer._NnapiSerializer.add_pool2d_node(self,node,opcode)
torch.backends._nnapi.serializer._NnapiSerializer.add_prelu_op(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_qadd(self,node,opcode,fuse_code)
torch.backends._nnapi.serializer._NnapiSerializer.add_qconv2d(self,node,fuse_code,transpose=False)
torch.backends._nnapi.serializer._NnapiSerializer.add_qlinear(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_quantize(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_reshape(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_size(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_slice(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_softmax(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_tensor_operand(self,jitval,oper)
torch.backends._nnapi.serializer._NnapiSerializer.add_tensor_operand_for_input(self,arg_idx,jitval,tensor)
torch.backends._nnapi.serializer._NnapiSerializer.add_tensor_operand_for_weight(self,tensor,dim_order=DimOrder.UNKNOWN_CONSTANT)
torch.backends._nnapi.serializer._NnapiSerializer.add_tensor_sequence(self,jitval,values)
torch.backends._nnapi.serializer._NnapiSerializer.add_to(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_tuple_construct(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_unsqueeze(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.add_upsample_nearest2d(self,node)
torch.backends._nnapi.serializer._NnapiSerializer.compute_operand_shape(self,op_id,dim,expr)
torch.backends._nnapi.serializer._NnapiSerializer.forward_operand_shape(self,out_op_id,out_dim,in_op_id,in_dim)
torch.backends._nnapi.serializer._NnapiSerializer.get_constant_value(self,jitval,typekind=None)
torch.backends._nnapi.serializer._NnapiSerializer.get_conv_pool_args_2d_common(self,kernel_size,strides,paddings,dilations,group_num)
torch.backends._nnapi.serializer._NnapiSerializer.get_conv_pool_args_2d_from_jit(self,kernel_size,stride,padding,dilation=None,group=None)
torch.backends._nnapi.serializer._NnapiSerializer.get_conv_pool_args_2d_from_pack(self,kernel_size,packed_config)
torch.backends._nnapi.serializer._NnapiSerializer.get_next_operand_id(self)
torch.backends._nnapi.serializer._NnapiSerializer.get_optional_bias(self,jit_bias,weight_tensor,transpose=False)
torch.backends._nnapi.serializer._NnapiSerializer.get_size_arg(self,jitval)
torch.backends._nnapi.serializer._NnapiSerializer.get_tensor_operand_by_jitval(self,jitval)
torch.backends._nnapi.serializer._NnapiSerializer.get_tensor_operand_by_jitval_fixed_size(self,jitval)
torch.backends._nnapi.serializer._NnapiSerializer.get_tensor_operand_for_weight(self,jitval)
torch.backends._nnapi.serializer._NnapiSerializer.get_tensor_operand_or_constant(self,jitval,dim_order=DimOrder.PRESUMED_CONTIGUOUS)
torch.backends._nnapi.serializer._NnapiSerializer.has_operand_for_jitval(self,jitval)
torch.backends._nnapi.serializer._NnapiSerializer.operand_to_template_torchscript(self,op_id,oper,shape=None)
torch.backends._nnapi.serializer._NnapiSerializer.serialize_ints(ints)
torch.backends._nnapi.serializer._NnapiSerializer.serialize_model(self,model,inputs,return_shapes=None)
torch.backends._nnapi.serializer._NnapiSerializer.serialize_values(self)
torch.backends._nnapi.serializer._NnapiSerializer.torch_tensor_to_operand(self,tensor,dim_order)
torch.backends._nnapi.serializer._NnapiSerializer.transpose_for_broadcast(self,in0_id,in0_oper,in1_id,in1_oper)
torch.backends._nnapi.serializer._NnapiSerializer.transpose_to_nhwc(self,in_id,oper)
torch.backends._nnapi.serializer.approx_equal(lhs,rhs,tolerance=1e-06)
torch.backends._nnapi.serializer.broadcast_shapes(shape1,shape2)
torch.backends._nnapi.serializer.change_element(tup,index,value)
torch.backends._nnapi.serializer.fix_shape(shape,dim_order)
torch.backends._nnapi.serializer.flex_name(op_id,dim)
torch.backends._nnapi.serializer.get_conv_pool_shape(image_shape,args,out_ch,transpose)
torch.backends._nnapi.serializer.reverse_map_dim(dim_order,d)
torch.backends._nnapi.serializer.serialize_model(module,inputs,*,config=None,return_shapes=None,use_int16_for_qint16=False)
torch.backends._nnapi.serializer.tensor_size(op_type,dims)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/_nnapi/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/mkl/__init__.py----------------------------------------
torch.backends.mkl.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/mkldnn/__init__.py----------------------------------------
A:torch.backends.mkldnn.__init__.orig_flags->set_flags(enabled)
A:torch.backends.mkldnn.__init__.enabled->ContextProp(torch._C._get_mkldnn_enabled, torch._C._set_mkldnn_enabled)
A:torch.backends.mkldnn.__init__.sys.modules[__name__]->MkldnnModule(sys.modules[__name__], __name__)
torch.backends.mkldnn.__init__.MkldnnModule(self,m,name)
torch.backends.mkldnn.__init__.MkldnnModule.__init__(self,m,name)
torch.backends.mkldnn.__init__.flags(enabled=False)
torch.backends.mkldnn.__init__.is_available()
torch.backends.mkldnn.__init__.set_flags(_enabled)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/xnnpack/__init__.py----------------------------------------
A:torch.backends.xnnpack.__init__.enabled->_XNNPACKEnabled()
A:torch.backends.xnnpack.__init__.sys.modules[__name__]->XNNPACKEngine(sys.modules[__name__], __name__)
torch.backends.xnnpack.__init__.XNNPACKEngine(self,m,name)
torch.backends.xnnpack.__init__.XNNPACKEngine.__getattr__(self,attr)
torch.backends.xnnpack.__init__.XNNPACKEngine.__init__(self,m,name)
torch.backends.xnnpack.__init__._XNNPACKEnabled(object)
torch.backends.xnnpack.__init__._XNNPACKEnabled.__get__(self,obj,objtype)
torch.backends.xnnpack.__init__._XNNPACKEnabled.__set__(self,obj,val)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/backends/quantized/__init__.py----------------------------------------
A:torch.backends.quantized.__init__.qengines->torch._C._supported_qengines()
A:torch.backends.quantized.__init__.engine->_QEngineProp()
A:torch.backends.quantized.__init__.supported_engines->_SupportedQEnginesProp()
A:torch.backends.quantized.__init__.sys.modules[__name__]->QuantizedEngine(sys.modules[__name__], __name__)
torch.backends.quantized.__init__.QuantizedEngine(self,m,name)
torch.backends.quantized.__init__.QuantizedEngine.__getattr__(self,attr)
torch.backends.quantized.__init__.QuantizedEngine.__init__(self,m,name)
torch.backends.quantized.__init__._QEngineProp(object)
torch.backends.quantized.__init__._QEngineProp.__get__(self,obj,objtype)->str
torch.backends.quantized.__init__._QEngineProp.__set__(self,obj,val:str)->None
torch.backends.quantized.__init__._SupportedQEnginesProp(object)
torch.backends.quantized.__init__._SupportedQEnginesProp.__get__(self,obj,objtype)->List[str]
torch.backends.quantized.__init__._SupportedQEnginesProp.__set__(self,obj,val)->None
torch.backends.quantized.__init__._get_qengine_id(qengine:str)->int
torch.backends.quantized.__init__._get_qengine_str(qengine:int)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_package_pickler.py----------------------------------------
A:torch.package._package_pickler.self.dispatch->pickle._Pickler.dispatch.copy()
A:torch.package._package_pickler.(module_name, name)->self.importer.get_name(obj, name)
A:torch.package._package_pickler.module->self.importer.import_module(module_name)
A:torch.package._package_pickler.(_, parent)->_getattribute(module, name)
A:torch.package._package_pickler.code->pickle._extension_registry.get((module_name, name))
torch.package._package_pickler.PackagePickler(self,importer:Importer,*args,**kwargs)
torch.package._package_pickler.PackagePickler.__init__(self,importer:Importer,*args,**kwargs)
torch.package._package_pickler.PackagePickler.save_global(self,obj,name=None)
torch.package._package_pickler.create_pickler(data_buf,importer,protocol=4)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_mangling.py----------------------------------------
A:torch.package._mangling.(first, sep, last)->name.partition('.')
torch.package._mangling.PackageMangler(self)
torch.package._mangling.PackageMangler.__init__(self)
torch.package._mangling.PackageMangler.demangle(self,mangled:str)->str
torch.package._mangling.PackageMangler.mangle(self,name)->str
torch.package._mangling.PackageMangler.parent_name(self)
torch.package._mangling.demangle(name:str)->str
torch.package._mangling.get_mangle_prefix(name:str)->str
torch.package._mangling.is_mangled(name:str)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/importer.py----------------------------------------
A:torch.package.importer.reduce->getattr(obj, '__reduce__', None)
A:torch.package.importer.rv->reduce()
A:torch.package.importer.name->getattr(obj, '__qualname__', None)
A:torch.package.importer.orig_module_name->self.whichmodule(obj, name)
A:torch.package.importer.module_name->importer.whichmodule(obj, name)
A:torch.package.importer.module->self.import_module(module_name)
A:torch.package.importer.(obj2, _)->_getattribute(module, name)
A:torch.package.importer.is_mangled_->is_mangled(module_name)
A:torch.package.importer.(obj_module_name, obj_location, obj_importer_name)->get_obj_info(obj)
A:torch.package.importer.(obj2_module_name, obj2_location, obj2_importer_name)->get_obj_info(obj2)
A:torch.package.importer.sys_importer->_SysImporter()
torch.package.Importer(ABC)
torch.package.Importer.get_name(self,obj:Any,name:Optional[str]=None)->Tuple[str, str]
torch.package.Importer.import_module(self,module_name:str)->ModuleType
torch.package.Importer.whichmodule(self,obj:Any,name:str)->str
torch.package.ObjMismatchError(Exception)
torch.package.ObjNotFoundError(Exception)
torch.package.OrderedImporter(self,*args)
torch.package.OrderedImporter.import_module(self,module_name:str)->ModuleType
torch.package.OrderedImporter.whichmodule(self,obj:Any,name:str)->str
torch.package.importer.Importer(ABC)
torch.package.importer.Importer.get_name(self,obj:Any,name:Optional[str]=None)->Tuple[str, str]
torch.package.importer.Importer.import_module(self,module_name:str)->ModuleType
torch.package.importer.Importer.whichmodule(self,obj:Any,name:str)->str
torch.package.importer.ObjMismatchError(Exception)
torch.package.importer.ObjNotFoundError(Exception)
torch.package.importer.OrderedImporter(self,*args)
torch.package.importer.OrderedImporter.__init__(self,*args)
torch.package.importer.OrderedImporter.import_module(self,module_name:str)->ModuleType
torch.package.importer.OrderedImporter.whichmodule(self,obj:Any,name:str)->str
torch.package.importer._SysImporter(Importer)
torch.package.importer._SysImporter.import_module(self,module_name:str)
torch.package.importer._SysImporter.whichmodule(self,obj:Any,name:str)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/package_importer.py----------------------------------------
A:torch.package.package_importer.self.filename->str(file_or_buffer)
A:torch.package.package_importer.self.zip_reader->torch._C.PyTorchFileReader(file_or_buffer)
A:torch.package.package_importer.self.root->_PackageNode(None)
A:torch.package.package_importer.self.extern_modules->self._read_extern()
A:torch.package.package_importer.self.patched_builtins->builtins.__dict__.copy()
A:torch.package.package_importer.self._mangler->PackageMangler()
A:torch.package.package_importer.name->'.'.join(atoms[:i])
A:torch.package.package_importer.path->self.importer._zipfile_path(self.fullname, name)
A:torch.package.package_importer.data->self.load_binary(package, resource)
A:torch.package.package_importer.pickle_file->self._zipfile_path(package, resource)
A:torch.package.package_importer.restore_location->_get_restore_location(map_location)
A:torch.package.package_importer.storage_context->torch._C.DeserializationStorageContext()
A:torch.package.package_importer.storage->self.zip_reader.get_storage_from_record('.data/' + name, size, dtype).storage()
A:torch.package.package_importer.tensor->self.zip_reader.get_storage_from_record('.data/' + name, size, dtype)
A:torch.package.package_importer.loaded_storages[key]->restore_location(storage, location)
A:torch.package.package_importer.typename->_maybe_decode_ascii(saved_id[0])
A:torch.package.package_importer.loaded_reduces[reduce_id]->func(self, *args)
A:torch.package.package_importer.data_file->io.BytesIO(self.zip_reader.get_record(pickle_file))
A:torch.package.package_importer.unpickler->self.Unpickler(data_file)
A:torch.package.package_importer.result->self.Unpickler(data_file).load()
A:torch.package.package_importer.spec->importlib.machinery.ModuleSpec(name, self, origin='<package_importer>', is_package=is_package)
A:torch.package.package_importer.module->self.import_module(package)
A:torch.package.package_importer.module.__name__->self._mangler.mangle(name)
A:torch.package.package_importer.code->self._compile_source(filename, mangled_filename)
A:torch.package.package_importer.moduleself.modules[name]->importlib.import_module(name)
A:torch.package.package_importer.source->_normalize_line_endings(source)
A:torch.package.package_importer.package->self._get_or_create_package(prefix)
A:torch.package.package_importer.msg->(_ERR_MSG + '; {!r} is not a package').format(name, parent)
A:torch.package.package_importer.message->'import of {} halted; None in sys.modules'.format(name)
A:torch.package.package_importer.module_name->demangle(module.__name__)
A:torch.package.package_importer.from_name->'{}.{}'.format(module_name, x)
A:torch.package.package_importer.resource->_normalize_path(resource)
A:torch.package.package_importer.node->cur.children.get(atom, None)
A:torch.package.package_importer.nodecur.children[atom]->_PackageNode(None)
A:torch.package.package_importer.(*prefix, last)->extern_name.split('.')
A:torch.package.package_importer.package.children[package_name]->_ModuleNode(filename)
A:torch.package.package_importer.package.children[last]->_ExternNode()
A:torch.package.package_importer._NEEDS_LOADING->object()
A:torch.package.package_importer.filename->self.fullname.replace('.', '/')
A:torch.package.package_importer.fullname_path->Path(self.importer._zipfile_path(self.fullname))
A:torch.package.package_importer.files->self.importer.zip_reader.get_all_records()
A:torch.package.package_importer.subdirs_seen->set()
A:torch.package.package_importer.relative->Path(filename).relative_to(fullname_path)
torch.package.PackageImporter(self,file_or_buffer:Union[str,torch._C.PyTorchFileReader,Path,BinaryIO],module_allowed:Callable[[str],bool]=lambdamodule_name:True)
torch.package.PackageImporter.__import__(self,name,globals=None,locals=None,fromlist=(),level=0)
torch.package.PackageImporter._add_extern(self,extern_name:str)
torch.package.PackageImporter._add_file(self,filename:str)
torch.package.PackageImporter._compile_source(self,fullpath:str,mangled_filename:str)
torch.package.PackageImporter._do_find_and_load(self,name)
torch.package.PackageImporter._find_and_load(self,name)
torch.package.PackageImporter._gcd_import(self,name,package=None,level=0)
torch.package.PackageImporter._get_or_create_package(self,atoms:List[str])->'Union[_PackageNode, _ExternNode]'
torch.package.PackageImporter._get_package(self,package)
torch.package.PackageImporter._handle_fromlist(self,module,fromlist,*,recursive=False)
torch.package.PackageImporter._install_on_parent(self,parent:str,name:str,module:types.ModuleType)
torch.package.PackageImporter._load_module(self,name:str,parent:str)
torch.package.PackageImporter._make_module(self,name:str,filename:Optional[str],is_package:bool,parent:str)
torch.package.PackageImporter._read_extern(self)
torch.package.PackageImporter._zipfile_path(self,package,resource=None)
torch.package.PackageImporter.file_structure(self,*,include:'GlobPattern'='**',exclude:'GlobPattern'=())->Directory
torch.package.PackageImporter.get_resource_reader(self,fullname)
torch.package.PackageImporter.get_source(self,module_name)->str
torch.package.PackageImporter.id(self)
torch.package.PackageImporter.import_module(self,name:str,package=None)
torch.package.PackageImporter.load_binary(self,package:str,resource:str)->bytes
torch.package.PackageImporter.load_pickle(self,package:str,resource:str,map_location=None)->Any
torch.package.PackageImporter.load_text(self,package:str,resource:str,encoding:str='utf-8',errors:str='strict')->str
torch.package.package_importer.PackageImporter(self,file_or_buffer:Union[str,torch._C.PyTorchFileReader,Path,BinaryIO],module_allowed:Callable[[str],bool]=lambdamodule_name:True)
torch.package.package_importer.PackageImporter.__import__(self,name,globals=None,locals=None,fromlist=(),level=0)
torch.package.package_importer.PackageImporter.__init__(self,file_or_buffer:Union[str,torch._C.PyTorchFileReader,Path,BinaryIO],module_allowed:Callable[[str],bool]=lambdamodule_name:True)
torch.package.package_importer.PackageImporter._add_extern(self,extern_name:str)
torch.package.package_importer.PackageImporter._add_file(self,filename:str)
torch.package.package_importer.PackageImporter._compile_source(self,fullpath:str,mangled_filename:str)
torch.package.package_importer.PackageImporter._do_find_and_load(self,name)
torch.package.package_importer.PackageImporter._find_and_load(self,name)
torch.package.package_importer.PackageImporter._gcd_import(self,name,package=None,level=0)
torch.package.package_importer.PackageImporter._get_or_create_package(self,atoms:List[str])->'Union[_PackageNode, _ExternNode]'
torch.package.package_importer.PackageImporter._get_package(self,package)
torch.package.package_importer.PackageImporter._handle_fromlist(self,module,fromlist,*,recursive=False)
torch.package.package_importer.PackageImporter._install_on_parent(self,parent:str,name:str,module:types.ModuleType)
torch.package.package_importer.PackageImporter._load_module(self,name:str,parent:str)
torch.package.package_importer.PackageImporter._make_module(self,name:str,filename:Optional[str],is_package:bool,parent:str)
torch.package.package_importer.PackageImporter._read_extern(self)
torch.package.package_importer.PackageImporter._zipfile_path(self,package,resource=None)
torch.package.package_importer.PackageImporter.file_structure(self,*,include:'GlobPattern'='**',exclude:'GlobPattern'=())->Directory
torch.package.package_importer.PackageImporter.get_resource_reader(self,fullname)
torch.package.package_importer.PackageImporter.get_source(self,module_name)->str
torch.package.package_importer.PackageImporter.id(self)
torch.package.package_importer.PackageImporter.import_module(self,name:str,package=None)
torch.package.package_importer.PackageImporter.load_binary(self,package:str,resource:str)->bytes
torch.package.package_importer.PackageImporter.load_pickle(self,package:str,resource:str,map_location=None)->Any
torch.package.package_importer.PackageImporter.load_text(self,package:str,resource:str,encoding:str='utf-8',errors:str='strict')->str
torch.package.package_importer._ExternNode(_PathNode)
torch.package.package_importer._ModuleNode(self,source_file:str)
torch.package.package_importer._ModuleNode.__init__(self,source_file:str)
torch.package.package_importer._PackageNode(self,source_file:Optional[str])
torch.package.package_importer._PackageNode.__init__(self,source_file:Optional[str])
torch.package.package_importer._PackageResourceReader(self,importer,fullname)
torch.package.package_importer._PackageResourceReader.__init__(self,importer,fullname)
torch.package.package_importer._PackageResourceReader.contents(self)
torch.package.package_importer._PackageResourceReader.is_resource(self,name)
torch.package.package_importer._PackageResourceReader.open_resource(self,resource)
torch.package.package_importer._PackageResourceReader.resource_path(self,resource)
torch.package.package_importer._PathNode
torch.package.package_importer.patched_getfile(object)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/find_file_dependencies.py----------------------------------------
A:torch.package.find_file_dependencies.visitor->cls(package)
A:torch.package.find_file_dependencies.tree->ast.parse(src)
A:torch.package.find_file_dependencies.name->self._absmodule(name, level)
A:torch.package.find_file_dependencies.level->self._grab_node_int(keyword.value)
A:torch.package.find_file_dependencies.top_name->self._absmodule(top_name, level)
torch.package.find_file_dependencies._ExtractModuleReferences(self,package)
torch.package.find_file_dependencies._ExtractModuleReferences.__init__(self,package)
torch.package.find_file_dependencies._ExtractModuleReferences._absmodule(self,module_name:str,level:int)->str
torch.package.find_file_dependencies._ExtractModuleReferences._grab_node_int(self,node)
torch.package.find_file_dependencies._ExtractModuleReferences._grab_node_str(self,node)
torch.package.find_file_dependencies._ExtractModuleReferences.run(cls,src:str,package:str)->List[Tuple[str, Optional[str]]]
torch.package.find_file_dependencies._ExtractModuleReferences.visit_Call(self,node)
torch.package.find_file_dependencies._ExtractModuleReferences.visit_Import(self,node)
torch.package.find_file_dependencies._ExtractModuleReferences.visit_ImportFrom(self,node)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/package_exporter.py----------------------------------------
A:torch.package.package_exporter.error->attrs.get('error')
A:torch.package.package_exporter.message->io.StringIO()
A:torch.package.package_exporter.error_context->dependency_graph.nodes[module_name].get('error_context')
A:torch.package.package_exporter.f->str(f)
A:torch.package.package_exporter.self.zip_file->torch._C.PyTorchFileWriter(f)
A:torch.package.package_exporter.self.dependency_graph->DiGraph()
A:torch.package.package_exporter.self.script_module_serializer->torch._C.ScriptModuleSerializer(self.zip_file)
A:torch.package.package_exporter.self.storage_context->self.script_module_serializer.storage_context()
A:torch.package.package_exporter.self.importer->OrderedImporter(*importer)
A:torch.package.package_exporter.path->Path(file_or_directory)
A:torch.package.package_exporter.module_path->demangle(module_name).replace('.', '/')
A:torch.package.package_exporter.relative_path->self._filename(package, resource).relative_to(path).as_posix()
A:torch.package.package_exporter.submodule_name->archivename[:-len('.py')].replace('/', '.')
A:torch.package.package_exporter.ret->str(self._unique_id)
A:torch.package.package_exporter.dep_pairs->find_files_source_depends_on(src, package_name)
A:torch.package.package_exporter.deps->self._get_dependencies(source, module_name, is_package)
A:torch.package.package_exporter.filename->self._filename(package, resource)
A:torch.package.package_exporter.module_obj->self._import_module(module_name)
A:torch.package.package_exporter.module_name->demangle(module_name)
A:torch.package.package_exporter.is_package->hasattr(self._import_module(module_name), '__path__')
A:torch.package.package_exporter.source->self._get_source_of_module(module_obj)
A:torch.package.package_exporter.data_buf->io.BytesIO()
A:torch.package.package_exporter.pickler->create_pickler(data_buf, self.importer, protocol=pickle_protocol)
A:torch.package.package_exporter.data_value->io.BytesIO().getvalue()
A:torch.package.package_exporter.field->memo.get(arg, None)
A:torch.package.package_exporter.(module, field)->arg.split(' ')
A:torch.package.package_exporter.handle->RemovableHandle(self._intern_hooks)
A:torch.package.package_exporter.self.patterns[GlobGroup(include, exclude=exclude)]->_PatternInfo(_ModuleProviderAction.DENY, allow_empty=True)
A:torch.package.package_exporter.storage_type_str->obj.pickle_storage_type()
A:torch.package.package_exporter.storage_type->normalize_storage_type(type(storage))
A:torch.package.package_exporter.storage_numel->storage.cpu().nbytes()
A:torch.package.package_exporter.storage->storage.cpu().cpu()
A:torch.package.package_exporter.location->location_tag(storage)
A:torch.package.package_exporter.storage_present->self.storage_context.has_storage(storage)
A:torch.package.package_exporter.storage_id->self.storage_context.get_or_add_storage(storage)
A:torch.package.package_exporter.num_bytes->storage.cpu().cpu().nbytes()
A:torch.package.package_exporter.str_or_bytes->str_or_bytes.encode('utf-8').encode('utf-8')
A:torch.package.package_exporter.mock_file->str(Path(__file__).parent / '_mock.py')
A:torch.package.package_exporter.package_path->package.replace('.', '/')
A:torch.package.package_exporter.resource->_normalize_path(resource)
A:torch.package.package_exporter.node_action->node_dict.get('action', None)
A:torch.package.package_exporter.b->str(f).read()
torch.package.EmptyMatchError(Exception)
torch.package.PackageExporter(self,f:Union[str,Path,BinaryIO],importer:Union[Importer,Sequence[Importer]]=sys_importer)
torch.package.PackageExporter.__enter__(self)
torch.package.PackageExporter.__exit__(self,exc_type,exc_value,traceback)
torch.package.PackageExporter._can_implicitly_extern(self,module_name:str)
torch.package.PackageExporter._execute_dependency_graph(self)
torch.package.PackageExporter._filename(self,package,resource)
torch.package.PackageExporter._finalize_zip(self)
torch.package.PackageExporter._get_dependencies(self,src:str,module_name:str,is_package:bool)->List[str]
torch.package.PackageExporter._get_source_of_module(self,module:types.ModuleType)->Optional[str]
torch.package.PackageExporter._import_module(self,module_name:str)
torch.package.PackageExporter._intern_module(self,module_name:str,dependencies:bool)
torch.package.PackageExporter._module_exists(self,module_name:str)->bool
torch.package.PackageExporter._nodes_with_action_type(self,action:Optional[_ModuleProviderAction])->List[str]
torch.package.PackageExporter._persistent_id(self,obj)
torch.package.PackageExporter._validate_dependency_graph(self)
torch.package.PackageExporter._write(self,filename,str_or_bytes)
torch.package.PackageExporter._write_mock_file(self)
torch.package.PackageExporter._write_source_string(self,module_name:str,src:str,is_package:bool=False)
torch.package.PackageExporter.add_dependency(self,module_name:str,dependencies=True)
torch.package.PackageExporter.all_paths(self,src:str,dst:str)->str
torch.package.PackageExporter.close(self)
torch.package.PackageExporter.denied_modules(self)->List[str]
torch.package.PackageExporter.deny(self,include:'GlobPattern',*,exclude:'GlobPattern'=())
torch.package.PackageExporter.dependency_graph_string(self)->str
torch.package.PackageExporter.extern(self,include:'GlobPattern',*,exclude:'GlobPattern'=(),allow_empty:bool=True)
torch.package.PackageExporter.externed_modules(self)->List[str]
torch.package.PackageExporter.get_rdeps(self,module_name:str)->List[str]
torch.package.PackageExporter.get_unique_id(self)->str
torch.package.PackageExporter.intern(self,include:'GlobPattern',*,exclude:'GlobPattern'=(),allow_empty:bool=True)
torch.package.PackageExporter.interned_modules(self)->List[str]
torch.package.PackageExporter.mock(self,include:'GlobPattern',*,exclude:'GlobPattern'=(),allow_empty:bool=True)
torch.package.PackageExporter.mocked_modules(self)->List[str]
torch.package.PackageExporter.register_extern_hook(self,hook:ActionHook)->RemovableHandle
torch.package.PackageExporter.register_intern_hook(self,hook:ActionHook)->RemovableHandle
torch.package.PackageExporter.register_mock_hook(self,hook:ActionHook)->RemovableHandle
torch.package.PackageExporter.save_binary(self,package,resource,binary:bytes)
torch.package.PackageExporter.save_module(self,module_name:str,dependencies=True)
torch.package.PackageExporter.save_pickle(self,package:str,resource:str,obj:Any,dependencies:bool=True,pickle_protocol:int=3)
torch.package.PackageExporter.save_source_file(self,module_name:str,file_or_directory:str,dependencies=True)
torch.package.PackageExporter.save_source_string(self,module_name:str,src:str,is_package:bool=False,dependencies:bool=True)
torch.package.PackageExporter.save_text(self,package:str,resource:str,text:str)
torch.package.PackagingError(self,dependency_graph:DiGraph)
torch.package.PackagingErrorReason(Enum)
torch.package.PackagingErrorReason.__repr__(self)
torch.package.package_exporter.EmptyMatchError(Exception)
torch.package.package_exporter.PackageExporter(self,f:Union[str,Path,BinaryIO],importer:Union[Importer,Sequence[Importer]]=sys_importer)
torch.package.package_exporter.PackageExporter.__enter__(self)
torch.package.package_exporter.PackageExporter.__exit__(self,exc_type,exc_value,traceback)
torch.package.package_exporter.PackageExporter.__init__(self,f:Union[str,Path,BinaryIO],importer:Union[Importer,Sequence[Importer]]=sys_importer)
torch.package.package_exporter.PackageExporter._can_implicitly_extern(self,module_name:str)
torch.package.package_exporter.PackageExporter._execute_dependency_graph(self)
torch.package.package_exporter.PackageExporter._filename(self,package,resource)
torch.package.package_exporter.PackageExporter._finalize_zip(self)
torch.package.package_exporter.PackageExporter._get_dependencies(self,src:str,module_name:str,is_package:bool)->List[str]
torch.package.package_exporter.PackageExporter._get_source_of_module(self,module:types.ModuleType)->Optional[str]
torch.package.package_exporter.PackageExporter._import_module(self,module_name:str)
torch.package.package_exporter.PackageExporter._intern_module(self,module_name:str,dependencies:bool)
torch.package.package_exporter.PackageExporter._module_exists(self,module_name:str)->bool
torch.package.package_exporter.PackageExporter._nodes_with_action_type(self,action:Optional[_ModuleProviderAction])->List[str]
torch.package.package_exporter.PackageExporter._persistent_id(self,obj)
torch.package.package_exporter.PackageExporter._validate_dependency_graph(self)
torch.package.package_exporter.PackageExporter._write(self,filename,str_or_bytes)
torch.package.package_exporter.PackageExporter._write_mock_file(self)
torch.package.package_exporter.PackageExporter._write_source_string(self,module_name:str,src:str,is_package:bool=False)
torch.package.package_exporter.PackageExporter.add_dependency(self,module_name:str,dependencies=True)
torch.package.package_exporter.PackageExporter.all_paths(self,src:str,dst:str)->str
torch.package.package_exporter.PackageExporter.close(self)
torch.package.package_exporter.PackageExporter.denied_modules(self)->List[str]
torch.package.package_exporter.PackageExporter.deny(self,include:'GlobPattern',*,exclude:'GlobPattern'=())
torch.package.package_exporter.PackageExporter.dependency_graph_string(self)->str
torch.package.package_exporter.PackageExporter.extern(self,include:'GlobPattern',*,exclude:'GlobPattern'=(),allow_empty:bool=True)
torch.package.package_exporter.PackageExporter.externed_modules(self)->List[str]
torch.package.package_exporter.PackageExporter.get_rdeps(self,module_name:str)->List[str]
torch.package.package_exporter.PackageExporter.get_unique_id(self)->str
torch.package.package_exporter.PackageExporter.intern(self,include:'GlobPattern',*,exclude:'GlobPattern'=(),allow_empty:bool=True)
torch.package.package_exporter.PackageExporter.interned_modules(self)->List[str]
torch.package.package_exporter.PackageExporter.mock(self,include:'GlobPattern',*,exclude:'GlobPattern'=(),allow_empty:bool=True)
torch.package.package_exporter.PackageExporter.mocked_modules(self)->List[str]
torch.package.package_exporter.PackageExporter.register_extern_hook(self,hook:ActionHook)->RemovableHandle
torch.package.package_exporter.PackageExporter.register_intern_hook(self,hook:ActionHook)->RemovableHandle
torch.package.package_exporter.PackageExporter.register_mock_hook(self,hook:ActionHook)->RemovableHandle
torch.package.package_exporter.PackageExporter.save_binary(self,package,resource,binary:bytes)
torch.package.package_exporter.PackageExporter.save_module(self,module_name:str,dependencies=True)
torch.package.package_exporter.PackageExporter.save_pickle(self,package:str,resource:str,obj:Any,dependencies:bool=True,pickle_protocol:int=3)
torch.package.package_exporter.PackageExporter.save_source_file(self,module_name:str,file_or_directory:str,dependencies=True)
torch.package.package_exporter.PackageExporter.save_source_string(self,module_name:str,src:str,is_package:bool=False,dependencies:bool=True)
torch.package.package_exporter.PackageExporter.save_text(self,package:str,resource:str,text:str)
torch.package.package_exporter.PackagingError(self,dependency_graph:DiGraph)
torch.package.package_exporter.PackagingError.__init__(self,dependency_graph:DiGraph)
torch.package.package_exporter.PackagingErrorReason(Enum)
torch.package.package_exporter.PackagingErrorReason.__repr__(self)
torch.package.package_exporter._ModuleProviderAction(Enum)
torch.package.package_exporter._PatternInfo(self,action,allow_empty)
torch.package.package_exporter._PatternInfo.__init__(self,action,allow_empty)
torch.package.package_exporter._read_file(filename:str)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_importlib.py----------------------------------------
A:torch.package._importlib.source->source.replace(b'\r', b'\n').replace(b'\r', b'\n')
A:torch.package._importlib.bits->globals.get('__package__').rsplit('.', level - 1)
A:torch.package._importlib.package->globals.get('__package__')
A:torch.package._importlib.spec->globals.get('__spec__')
A:torch.package._importlib.(parent, file_name)->os.path.split(path)
torch.package._importlib._calc___package__(globals)
torch.package._importlib._normalize_line_endings(source)
torch.package._importlib._normalize_path(path)
torch.package._importlib._resolve_name(name,package,level)
torch.package._importlib._sanity_check(name,package,level)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_stdlib.py----------------------------------------
torch.package._stdlib._get_stdlib_modules()
torch.package._stdlib.is_stdlib_module(module:str)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/file_structure_representation.py----------------------------------------
A:torch.package.file_structure_representation.self.children[dir_name]->Directory(dir_name, True)
A:torch.package.file_structure_representation.(*dirs, file)->file_path.split('/')
A:torch.package.file_structure_representation.dir->self._get_dir(dirs)
A:torch.package.file_structure_representation.dir.children[file]->Directory(file, False)
A:torch.package.file_structure_representation.lineage->filename.split('/', maxsplit=1)
A:torch.package.file_structure_representation.glob_pattern->GlobGroup(include, exclude=exclude, separator='/')
A:torch.package.file_structure_representation.top_dir->Directory(filename, True)
torch.package.Directory(self,name:str,is_dir:bool)
torch.package.Directory.__str__(self)
torch.package.Directory._add_file(self,file_path:str)
torch.package.Directory._get_dir(self,dirs:List[str])->'Directory'
torch.package.Directory._stringify_tree(self,str_list:List[str],preamble:str='',dir_ptr:str='───')
torch.package.Directory.has_file(self,filename:str)->bool
torch.package.file_structure_representation.Directory(self,name:str,is_dir:bool)
torch.package.file_structure_representation.Directory.__init__(self,name:str,is_dir:bool)
torch.package.file_structure_representation.Directory.__str__(self)
torch.package.file_structure_representation.Directory._add_file(self,file_path:str)
torch.package.file_structure_representation.Directory._get_dir(self,dirs:List[str])->'Directory'
torch.package.file_structure_representation.Directory._stringify_tree(self,str_list:List[str],preamble:str='',dir_ptr:str='───')
torch.package.file_structure_representation.Directory.has_file(self,filename:str)->bool
torch.package.file_structure_representation._create_directory_from_file_list(filename:str,file_list:List[str],include:'GlobPattern'='**',exclude:'GlobPattern'=())->Directory


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_digraph.py----------------------------------------
A:torch.package._digraph.result->set(src)
A:torch.package._digraph.working_set->deque(dst)
A:torch.package._digraph.cur->deque(dst).popleft()
A:torch.package._digraph.result_graph->DiGraph()
A:torch.package._digraph.forward_reachable_from_src->self.forward_transitive_closure(src)
A:torch.package._digraph.edges->'\n'.join((f'"{f}" -> "{t}";' for (f, t) in self.edges))
torch.package._digraph.DiGraph(self)
torch.package._digraph.DiGraph.__contains__(self,n)
torch.package._digraph.DiGraph.__init__(self)
torch.package._digraph.DiGraph.__iter__(self)
torch.package._digraph.DiGraph.add_edge(self,u,v)
torch.package._digraph.DiGraph.add_node(self,n,**kwargs)
torch.package._digraph.DiGraph.all_paths(self,src:str,dst:str)
torch.package._digraph.DiGraph.backward_transitive_closure(self,src:str)->Set[str]
torch.package._digraph.DiGraph.edges(self)
torch.package._digraph.DiGraph.forward_transitive_closure(self,src:str)->Set[str]
torch.package._digraph.DiGraph.nodes(self)
torch.package._digraph.DiGraph.predecessors(self,n)
torch.package._digraph.DiGraph.successors(self,n)
torch.package._digraph.DiGraph.to_dot(self)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_directory_reader.py----------------------------------------
A:torch.package._directory_reader.storage->cast(Storage, torch._UntypedStorage)
A:torch.package._directory_reader.full_path->os.path.join(self.directory, path)
torch.package._directory_reader.DirectoryReader(self,directory)
torch.package._directory_reader.DirectoryReader.__init__(self,directory)
torch.package._directory_reader.DirectoryReader.get_all_records(self)
torch.package._directory_reader.DirectoryReader.get_record(self,name)
torch.package._directory_reader.DirectoryReader.get_storage_from_record(self,name,numel,dtype)
torch.package._directory_reader.DirectoryReader.has_record(self,path)
torch.package._directory_reader._HasStorage(self,storage)
torch.package._directory_reader._HasStorage.__init__(self,storage)
torch.package._directory_reader._HasStorage.storage(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_package_unpickler.py----------------------------------------
A:torch.package._package_unpickler.mod->self._importer.import_module(module)
torch.package._package_unpickler.PackageUnpickler(self,importer:Importer,*args,**kwargs)
torch.package._package_unpickler.PackageUnpickler.__init__(self,importer:Importer,*args,**kwargs)
torch.package._package_unpickler.PackageUnpickler.find_class(self,module,name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/glob_group.py----------------------------------------
A:torch.package.glob_group.self.include->GlobGroup._glob_list(include, separator)
A:torch.package.glob_group.self.exclude->GlobGroup._glob_list(exclude, separator)
A:torch.package.glob_group.result->''.join((component_to_re(c) for c in pattern.split(separator)))
torch.package.GlobGroup(self,include:GlobPattern,*,exclude:GlobPattern=(),separator:str='.')
torch.package.GlobGroup.__repr__(self)
torch.package.GlobGroup.__str__(self)
torch.package.GlobGroup._glob_list(elems:GlobPattern,separator:str='.')
torch.package.GlobGroup._glob_to_re(pattern:str,separator:str='.')
torch.package.GlobGroup.matches(self,candidate:str)->bool
torch.package.glob_group.GlobGroup(self,include:GlobPattern,*,exclude:GlobPattern=(),separator:str='.')
torch.package.glob_group.GlobGroup.__init__(self,include:GlobPattern,*,exclude:GlobPattern=(),separator:str='.')
torch.package.glob_group.GlobGroup.__repr__(self)
torch.package.glob_group.GlobGroup.__str__(self)
torch.package.glob_group.GlobGroup._glob_list(elems:GlobPattern,separator:str='.')
torch.package.glob_group.GlobGroup._glob_to_re(pattern:str,separator:str='.')
torch.package.glob_group.GlobGroup.matches(self,candidate:str)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/_mock.py----------------------------------------
torch.package._mock.MockedObject(self,name:str,_suppress_err:bool)
torch.package._mock.MockedObject.__init__(self,name:str,_suppress_err:bool)
torch.package._mock.MockedObject.__repr__(self)
torch.package._mock.install_method(method_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/analyze/trace_dependencies.py----------------------------------------
A:torch.package.analyze.trace_dependencies.modules_used->set()
A:torch.package.analyze.trace_dependencies.method->getattr(frame.f_locals['self'], name, None)
torch.package.analyze.trace_dependencies(callable:Callable[[Any],Any],inputs:Iterable[Tuple[Any,...]])->List[str]
torch.package.analyze.trace_dependencies.trace_dependencies(callable:Callable[[Any],Any],inputs:Iterable[Tuple[Any,...]])->List[str]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/analyze/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/package/analyze/is_from_package.py----------------------------------------
torch.package.analyze.is_from_package.is_from_package(obj:Any)->bool
torch.package.is_from_package(obj:Any)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/futures/__init__.py----------------------------------------
A:torch.futures.__init__.T->TypeVar('T')
A:torch.futures.__init__.S->TypeVar('S')
torch.futures.__init__.Future(self,*,devices:Optional[List[Union[int,str,torch.device]]]=None)
torch.futures.__init__.Future.__init__(self,*,devices:Optional[List[Union[int,str,torch.device]]]=None)
torch.futures.__init__.Future.add_done_callback(self,callback)
torch.futures.__init__.Future.done(self)->bool
torch.futures.__init__.Future.set_exception(self,result:T)->None
torch.futures.__init__.Future.set_result(self,result:T)->None
torch.futures.__init__.Future.then(self,callback)
torch.futures.__init__.Future.value(self)->T
torch.futures.__init__.Future.wait(self)->T
torch.futures.__init__._PyFutureMeta(type(torch._C.Future),type(Generic))
torch.futures.__init__.collect_all(futures:List[Future])->Future[List[Future]]
torch.futures.__init__.wait_all(futures:List[Future])->List


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/operator_schemas.py----------------------------------------
A:torch.fx.operator_schemas._manual_overrides[torch.nonzero]->_nonzero_schemas()
A:torch.fx.operator_schemas._type_eval_globals[k]->getattr(typing, k)
A:torch.fx.operator_schemas.arg_type->_torchscript_type_to_python_type(arg.type)
A:torch.fx.operator_schemas.return_type->tuple(return_types)
A:torch.fx.operator_schemas.(signatures, schemas)->get_signature_for_torch_op(target, return_schemas=True)
A:torch.fx.operator_schemas.override->_manual_overrides.get(op)
A:torch.fx.operator_schemas.aten_fn->torch.jit._builtins._find_builtin(op)
A:torch.fx.operator_schemas.schemas->torch._C._jit_get_schemas_for_operator(aten_fn)
A:torch.fx.operator_schemas.sig_origin_type->getattr(signature_type, '__origin__', signature_type)
A:torch.fx.operator_schemas.sig->inspect.signature(inspect.unwrap(submod.forward))
A:torch.fx.operator_schemas.new_args_and_kwargs->_args_kwargs_to_normalized_args_kwargs(sig, args, kwargs, normalize_to_only_use_kwargs)
A:torch.fx.operator_schemas.torch_op_schemas->get_signature_for_torch_op(target)
A:torch.fx.operator_schemas.bound_types->candidate_signature.bind(*arg_types, **kwarg_types)
A:torch.fx.operator_schemas.schema_printouts->'\n'.join((str(schema) for schema in matched_schemas))
A:torch.fx.operator_schemas.submod->root.get_submodule(target)
A:torch.fx.operator_schemas.bound_args->inspect.signature(inspect.unwrap(submod.forward)).bind(*args, **kwargs)
torch.fx.operator_schemas.ArgsKwargsPair(NamedTuple)
torch.fx.operator_schemas._FakeGlobalNamespace
torch.fx.operator_schemas._FakeGlobalNamespace.__getattr__(self,name)
torch.fx.operator_schemas._args_kwargs_to_normalized_args_kwargs(sig:inspect.Signature,args:Tuple[Any,...],kwargs:Dict[str,Any],normalize_to_only_use_kwargs:bool)->Optional[ArgsKwargsPair]
torch.fx.operator_schemas._nonzero_schemas()
torch.fx.operator_schemas._torchscript_schema_to_signature(ts_schema:torch._C.FunctionSchema)->inspect.Signature
torch.fx.operator_schemas._torchscript_type_to_python_type(ts_type:'torch._C.JitType')->Any
torch.fx.operator_schemas.check_for_mutable_operation(target:Callable,args:Tuple['Argument',...],kwargs:Dict[str,'Argument'])
torch.fx.operator_schemas.create_type_hint(x)
torch.fx.operator_schemas.get_signature_for_torch_op(op:Callable,return_schemas:bool=False)
torch.fx.operator_schemas.normalize_function(target:Callable,args:Tuple[Any],kwargs:Optional[Dict[str,Any]]=None,arg_types:Optional[Tuple[Any]]=None,kwarg_types:Optional[Dict[str,Any]]=None,normalize_to_only_use_kwargs:bool=False)->Optional[ArgsKwargsPair]
torch.fx.operator_schemas.normalize_module(root:torch.nn.Module,target:str,args:Tuple[Any],kwargs:Optional[Dict[str,Any]]=None,normalize_to_only_use_kwargs:bool=False)->Optional[ArgsKwargsPair]
torch.fx.operator_schemas.type_matches(signature_type:Any,argument_type:Any)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/annotate.py----------------------------------------
torch.fx.annotate.annotate(val,type)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/_pytree.py----------------------------------------
A:torch.fx._pytree.child_pytrees->flatten_fn_spec(pytree, spec)
A:torch.fx._pytree.flat->tree_flatten_spec(child, child_spec)
torch.fx._pytree._dict_flatten_spec(d:Dict[Any,Any],spec:TreeSpec)->List[Any]
torch.fx._pytree._list_flatten_spec(d:List[Any],spec:TreeSpec)->List[Any]
torch.fx._pytree._namedtuple_flatten_spec(d:NamedTuple,spec:TreeSpec)->List[Any]
torch.fx._pytree._tuple_flatten_spec(d:Tuple[Any],spec:TreeSpec)->List[Any]
torch.fx._pytree.register_pytree_flatten_spec(typ:Any,flatten_fn_spec:FlattenFuncSpec)->None
torch.fx._pytree.tree_flatten_spec(pytree:PyTree,spec:TreeSpec)->List[Any]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/_compatibility.py----------------------------------------
A:torch.fx._compatibility.docstring->textwrap.dedent(getattr(fn, '__doc__', None) or '')
torch.fx._compatibility.compatibility(is_backward_compatible:bool)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/subgraph_rewriter.py----------------------------------------
A:torch.fx.subgraph_rewriter.self.pattern_anchor->next(iter(reversed(pattern.nodes)))
A:torch.fx.subgraph_rewriter.match_found->any((self._match_nodes(pn.all_input_nodes[0], gn_) for gn_ in gn.all_input_nodes))
A:torch.fx.subgraph_rewriter.mod_match->mod.get_submodule(target)
A:torch.fx.subgraph_rewriter.gm_submod->try_get_submodule(gm, node.target)
A:torch.fx.subgraph_rewriter.replacement_submod->try_get_submodule(replacement, node.target)
A:torch.fx.subgraph_rewriter.new_submod->copy.deepcopy(getattr(replacement, node.target))
A:torch.fx.subgraph_rewriter.matcher->_SubgraphMatcher(pattern_graph)
A:torch.fx.subgraph_rewriter.original_graph_node->match_changed_node.get(match.nodes_map[pattern_node], match.nodes_map[pattern_node])
A:torch.fx.subgraph_rewriter.copied_output->original_graph.graph_copy(replacement_graph, val_map)
torch.fx.replace_pattern(gm:GraphModule,pattern:Callable,replacement:Callable)->List[Match]
torch.fx.subgraph_rewriter.Match(NamedTuple)
torch.fx.subgraph_rewriter._SubgraphMatcher(self,pattern:Graph)
torch.fx.subgraph_rewriter._SubgraphMatcher.__init__(self,pattern:Graph)
torch.fx.subgraph_rewriter._SubgraphMatcher._match_nodes(self,pn:Node,gn:Node)->bool
torch.fx.subgraph_rewriter._SubgraphMatcher.matches_subgraph_from_anchor(self,anchor:Node)->bool
torch.fx.subgraph_rewriter._replace_submodules(gm:GraphModule,replacement:torch.nn.Module)->None
torch.fx.subgraph_rewriter.replace_pattern(gm:GraphModule,pattern:Callable,replacement:Callable)->List[Match]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/interpreter.py----------------------------------------
A:torch.fx.interpreter.self.submodules->dict(self.module.named_modules())
A:torch.fx.interpreter.self.env[node]->self.run_node(node)
A:torch.fx.interpreter.(args, kwargs)->self.fetch_args_kwargs_from_env(n)
A:torch.fx.interpreter.submod->self.fetch_attr(target)
A:torch.fx.interpreter.target_atoms->target.split('.')
A:torch.fx.interpreter.attr_itr->getattr(attr_itr, atom)
A:torch.fx.interpreter.args->self.map_nodes_to_values(n.args, n)
A:torch.fx.interpreter.kwargs->self.map_nodes_to_values(n.kwargs, n)
A:torch.fx.interpreter.self.new_graph->Graph()
A:torch.fx.interpreter.self.tracer->TransformerTracer(self.new_graph)
A:torch.fx.interpreter.result->super().run()
torch.fx.Interpreter(self,module:GraphModule,garbage_collect_values:bool=True)
torch.fx.Interpreter.call_function(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Interpreter.call_method(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Interpreter.call_module(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Interpreter.fetch_args_kwargs_from_env(self,n:Node)->Tuple[Tuple, Dict]
torch.fx.Interpreter.fetch_attr(self,target:str)
torch.fx.Interpreter.get_attr(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Interpreter.map_nodes_to_values(self,args:Argument,n:Node)->Argument
torch.fx.Interpreter.output(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Interpreter.placeholder(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Interpreter.run(self,*args,initial_env:Optional[Dict[Node,Any]]=None)->Any
torch.fx.Interpreter.run_node(self,n:Node)->Any
torch.fx.Transformer(self,module)
torch.fx.Transformer.call_function(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Transformer.call_module(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.Transformer.get_attr(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Proxy
torch.fx.Transformer.placeholder(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Proxy
torch.fx.Transformer.transform(self)->GraphModule
torch.fx.interpreter.Interpreter(self,module:GraphModule,garbage_collect_values:bool=True)
torch.fx.interpreter.Interpreter.__init__(self,module:GraphModule,garbage_collect_values:bool=True)
torch.fx.interpreter.Interpreter.call_function(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Interpreter.call_method(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Interpreter.call_module(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Interpreter.fetch_args_kwargs_from_env(self,n:Node)->Tuple[Tuple, Dict]
torch.fx.interpreter.Interpreter.fetch_attr(self,target:str)
torch.fx.interpreter.Interpreter.get_attr(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Interpreter.map_nodes_to_values(self,args:Argument,n:Node)->Argument
torch.fx.interpreter.Interpreter.output(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Interpreter.placeholder(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Interpreter.run(self,*args,initial_env:Optional[Dict[Node,Any]]=None)->Any
torch.fx.interpreter.Interpreter.run_node(self,n:Node)->Any
torch.fx.interpreter.Transformer(self,module)
torch.fx.interpreter.Transformer.__init__(self,module)
torch.fx.interpreter.Transformer.call_function(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Transformer.call_module(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Any
torch.fx.interpreter.Transformer.get_attr(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Proxy
torch.fx.interpreter.Transformer.placeholder(self,target:'Target',args:Tuple[Argument,...],kwargs:Dict[str,Any])->Proxy
torch.fx.interpreter.Transformer.transform(self)->GraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/node.py----------------------------------------
A:torch.fx.node.module->module.replace('torch._ops', 'torch.ops').replace('torch._ops', 'torch.ops')
A:torch.fx.node.items->', '.join((_format_arg(a) for a in arg))
A:torch.fx.node.items_str->', '.join((f'{k}: {_format_arg(v)}' for (k, v) in arg.items()))
A:torch.fx.node.args->list(self.args)
A:torch.fx.node.self.args->tuple(args)
A:torch.fx.node.kwargs->dict(self.kwargs)
A:torch.fx.node.to_process->list(self.users)
A:torch.fx.node.new_args->map_arg(self.args, maybe_replace_node)
A:torch.fx.node.new_kwargs->map_arg(self.kwargs, maybe_replace_node)
A:torch.fx.node.target_mod->self.graph.owning_module.get_submodule(self.target)
torch.fx.Node(self,graph:'Graph',name:str,op:str,target:'Target',args:Tuple['Argument',...],kwargs:Dict[str,'Argument'],return_type:Optional[Any]=None)
torch.fx.Node.__repr__(self)->str
torch.fx.Node.__update_args_kwargs(self,new_args:Tuple['Argument',...],new_kwargs:Dict[str,'Argument'])
torch.fx.Node._pretty_print_target(self,target)
torch.fx.Node._remove_from_list(self)
torch.fx.Node.all_input_nodes(self)->List['Node']
torch.fx.Node.append(self,x:'Node')->None
torch.fx.Node.args(self)->Tuple[Argument, ...]
torch.fx.Node.args(self,a:Tuple[Argument,...])
torch.fx.Node.format_node(self,placeholder_names:Optional[List[str]]=None,maybe_return_typename:Optional[List[str]]=None)->Optional[str]
torch.fx.Node.is_impure(self)
torch.fx.Node.kwargs(self)->Dict[str, Argument]
torch.fx.Node.kwargs(self,k:Dict[str,Argument])
torch.fx.Node.next(self)->'Node'
torch.fx.Node.normalized_arguments(self,root:torch.nn.Module,arg_types:Optional[Tuple[Any]]=None,kwarg_types:Optional[Dict[str,Any]]=None,normalize_to_only_use_kwargs:bool=False)->Optional[ArgsKwargsPair]
torch.fx.Node.prepend(self,x:'Node')->None
torch.fx.Node.prev(self)->'Node'
torch.fx.Node.replace_all_uses_with(self,replace_with:'Node')->List['Node']
torch.fx.Node.replace_input_with(self,old_input:'Node',new_input:'Node')
torch.fx.Node.stack_trace(self)->Optional[str]
torch.fx.Node.stack_trace(self,trace:Optional[str])
torch.fx.Node.update_arg(self,idx:int,arg:Argument)->None
torch.fx.Node.update_kwarg(self,key:str,arg:Argument)->None
torch.fx.map_arg(a:Argument,fn:Callable[[Node],Argument])->Argument
torch.fx.node.Node(self,graph:'Graph',name:str,op:str,target:'Target',args:Tuple['Argument',...],kwargs:Dict[str,'Argument'],return_type:Optional[Any]=None)
torch.fx.node.Node.__init__(self,graph:'Graph',name:str,op:str,target:'Target',args:Tuple['Argument',...],kwargs:Dict[str,'Argument'],return_type:Optional[Any]=None)
torch.fx.node.Node.__repr__(self)->str
torch.fx.node.Node.__update_args_kwargs(self,new_args:Tuple['Argument',...],new_kwargs:Dict[str,'Argument'])
torch.fx.node.Node._pretty_print_target(self,target)
torch.fx.node.Node._remove_from_list(self)
torch.fx.node.Node.all_input_nodes(self)->List['Node']
torch.fx.node.Node.append(self,x:'Node')->None
torch.fx.node.Node.args(self)->Tuple[Argument, ...]
torch.fx.node.Node.args(self,a:Tuple[Argument,...])
torch.fx.node.Node.format_node(self,placeholder_names:Optional[List[str]]=None,maybe_return_typename:Optional[List[str]]=None)->Optional[str]
torch.fx.node.Node.is_impure(self)
torch.fx.node.Node.kwargs(self)->Dict[str, Argument]
torch.fx.node.Node.kwargs(self,k:Dict[str,Argument])
torch.fx.node.Node.next(self)->'Node'
torch.fx.node.Node.normalized_arguments(self,root:torch.nn.Module,arg_types:Optional[Tuple[Any]]=None,kwarg_types:Optional[Dict[str,Any]]=None,normalize_to_only_use_kwargs:bool=False)->Optional[ArgsKwargsPair]
torch.fx.node.Node.prepend(self,x:'Node')->None
torch.fx.node.Node.prev(self)->'Node'
torch.fx.node.Node.replace_all_uses_with(self,replace_with:'Node')->List['Node']
torch.fx.node.Node.replace_input_with(self,old_input:'Node',new_input:'Node')
torch.fx.node.Node.stack_trace(self)->Optional[str]
torch.fx.node.Node.stack_trace(self,trace:Optional[str])
torch.fx.node.Node.update_arg(self,idx:int,arg:Argument)->None
torch.fx.node.Node.update_kwarg(self,key:str,arg:Argument)->None
torch.fx.node._find_module_of_method(orig_method:Callable[...,Any])->str
torch.fx.node._format_arg(arg)->str
torch.fx.node._get_qualified_name(func:Callable[...,Any])->str
torch.fx.node._type_repr(obj)
torch.fx.node.map_aggregate(a:Argument,fn:Callable[[Argument],Argument])->Argument
torch.fx.node.map_arg(a:Argument,fn:Callable[[Node],Argument])->Argument


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/proxy.py----------------------------------------
A:torch.fx.proxy.args_->self.create_arg(args)
A:torch.fx.proxy.kwargs_->self.create_arg(kwargs)
A:torch.fx.proxy.node->self.create_node(kind, target, args_, kwargs_, name, type_expr)
A:torch.fx.proxy.proxy->proxy_factory_fn(node)
A:torch.fx.proxy.user_frame->self._find_user_frame()
A:torch.fx.proxy.walk_stack_gen->traceback.walk_stack(user_frame)
A:torch.fx.proxy.summary->traceback.StackSummary.extract(walk_stack_gen)
A:torch.fx.proxy.tb_lines->traceback.StackSummary.extract(walk_stack_gen).format()
A:torch.fx.proxy.proxy.node.stack_trace->''.join(tb_lines)
A:torch.fx.proxy.frame->inspect.currentframe()
A:torch.fx.proxy.args->tuple((self.create_arg(elem) for elem in a))
A:torch.fx.proxy.k->self.create_arg(k)
A:torch.fx.proxy.r[k]->self.create_arg(v)
A:torch.fx.proxy.tracer->next(iter(tracers.keys()))
A:torch.fx.proxy.insts->list(dis.get_instructions(calling_frame.f_code))
A:torch.fx.proxy.target->getattr(operator, orig_method_name)
torch.fx.Proxy(self,node:Node,tracer:'Optional[TracerBase]'=None)
torch.fx.Proxy.__bool__(self)->bool
torch.fx.Proxy.__getattr__(self,k)->'Attribute'
torch.fx.Proxy.__iter__(self)->Iterable['Proxy']
torch.fx.Proxy.__len__(self)
torch.fx.Proxy.__repr__(self)->str
torch.fx.Proxy.__torch_function__(cls,orig_method,types,args=None,kwargs=None)
torch.fx.Proxy.keys(self)
torch.fx.proxy.Attribute(self,root:Proxy,attr:str)
torch.fx.proxy.Attribute.__init__(self,root:Proxy,attr:str)
torch.fx.proxy.Attribute.node(self)
torch.fx.proxy.GraphAppendingTracer(self,graph:Graph)
torch.fx.proxy.GraphAppendingTracer.__init__(self,graph:Graph)
torch.fx.proxy.ParameterProxy(self,tracer:TracerBase,node:Node,name,param)
torch.fx.proxy.ParameterProxy.__init__(self,tracer:TracerBase,node:Node,name,param)
torch.fx.proxy.ParameterProxy.__repr__(self)->str
torch.fx.proxy.ParameterProxy.dim(self)
torch.fx.proxy.ParameterProxy.ndim(self)
torch.fx.proxy.ParameterProxy.nelement(self)
torch.fx.proxy.ParameterProxy.numel(self)
torch.fx.proxy.ParameterProxy.shape(self)
torch.fx.proxy.ParameterProxy.size(self)
torch.fx.proxy.Proxy(self,node:Node,tracer:'Optional[TracerBase]'=None)
torch.fx.proxy.Proxy.__bool__(self)->bool
torch.fx.proxy.Proxy.__getattr__(self,k)->'Attribute'
torch.fx.proxy.Proxy.__init__(self,node:Node,tracer:'Optional[TracerBase]'=None)
torch.fx.proxy.Proxy.__iter__(self)->Iterable['Proxy']
torch.fx.proxy.Proxy.__len__(self)
torch.fx.proxy.Proxy.__repr__(self)->str
torch.fx.proxy.Proxy.__torch_function__(cls,orig_method,types,args=None,kwargs=None)
torch.fx.proxy.Proxy.keys(self)
torch.fx.proxy.TraceError(ValueError)
torch.fx.proxy.TracerBase
torch.fx.proxy.TracerBase._find_user_frame(self)
torch.fx.proxy.TracerBase.create_arg(self,a:Any)->Argument
torch.fx.proxy.TracerBase.create_node(self,kind:str,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:Optional[str]=None,type_expr:Optional[Any]=None)->Node
torch.fx.proxy.TracerBase.create_proxy(self,kind:str,target:Target,args:Tuple[Any,...],kwargs:Dict[str,Any],name:Optional[str]=None,type_expr:Optional[Any]=None,proxy_factory_fn:Callable[[Node],'Proxy']=None)
torch.fx.proxy.TracerBase.iter(self,obj:'Proxy')->Iterator
torch.fx.proxy.TracerBase.keys(self,obj:'Proxy')->Any
torch.fx.proxy.TracerBase.proxy(self,node:Node)->'Proxy'
torch.fx.proxy.TracerBase.to_bool(self,obj:'Proxy')->bool
torch.fx.proxy._define_reflectable(orig_method_name)
torch.fx.proxy.assert_fn(x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/_symbolic_trace.py----------------------------------------
A:torch.fx._symbolic_trace.instance->cls.__new__(cls)
A:torch.fx._symbolic_trace.new_code->CodeType(*co_args)
A:torch.fx._symbolic_trace.PH->PHBase()
A:torch.fx._symbolic_trace.args->tuple((self.create_arg(elem) for elem in a))
A:torch.fx._symbolic_trace.path->self.submodule_paths.get(mod)
A:torch.fx._symbolic_trace.module_qualified_name->self.path_of_module(m)
A:torch.fx._symbolic_trace.fn_for_analysis->inspect.unwrap(root_fn)
A:torch.fx._symbolic_trace.orig_args->list(co.co_varnames)
A:torch.fx._symbolic_trace.names_iter->iter(co.co_varnames)
A:torch.fx._symbolic_trace.sig->inspect.signature(fn_for_analysis)
A:torch.fx._symbolic_trace.out->self.create_proxy('placeholder', f'{name}_{str(cnt)}', default, {})
A:torch.fx._symbolic_trace.root_fn->_patch_function(root_fn, len(args))
A:torch.fx._symbolic_trace.(flat_args, in_spec)->torch.utils._pytree.tree_flatten(tuple(args))
A:torch.fx._symbolic_trace.self.graph._pytree_info->self.graph._pytree_info._replace(out_spec=out_spec)
A:torch.fx._symbolic_trace.tree_args->torch.utils._pytree.tree_unflatten(list(args), in_spec)
A:torch.fx._symbolic_trace.tree_out->root_fn(*tree_args)
A:torch.fx._symbolic_trace.(out_args, out_spec)->torch.utils._pytree.tree_flatten(tree_out)
A:torch.fx._symbolic_trace.val_proxy->self.create_proxy('get_attr', n, (), {}, **kwargs)
A:torch.fx._symbolic_trace.self.root->torch.nn.Module()
A:torch.fx._symbolic_trace.self.graph->Graph(tracer_cls=tracer_cls)
A:torch.fx._symbolic_trace.self.tensor_attrs[v]->'.'.join(prefix_atoms + [k])
A:torch.fx._symbolic_trace.(fn, args)->self.create_args_for_root(fn, isinstance(root, torch.nn.Module), concrete_args)
A:torch.fx._symbolic_trace.attr_val->_orig_module_getattr(mod, attr)
A:torch.fx._symbolic_trace.proxy->_find_proxy(args, kwargs)
A:torch.fx._symbolic_trace.return_proxy->_find_proxy(args, kwargs).tracer.create_proxy('call_function', orig_fn, args, kwargs)
A:torch.fx._symbolic_trace.orig_fn->getattr(builtins, name)
A:torch.fx._symbolic_trace.idx->id(thing)
A:torch.fx._symbolic_trace.currentframe->inspect.currentframe()
A:torch.fx._symbolic_trace.tracer->Tracer()
A:torch.fx._symbolic_trace.graph->Tracer().trace(root, concrete_args)
torch.fx.PHBase(object)
torch.fx.PHBase.__repr__(self)
torch.fx.ProxyableClassMeta(cls,name,bases,attrs)
torch.fx.Tracer(self,autowrap_modules:Tuple[ModuleType]=(math,),autowrap_functions:Tuple[Callable,...]=(),param_shapes_constant:bool=False)
torch.fx.Tracer._module_getattr(self,attr,attr_val,parameter_proxy_cache)
torch.fx.Tracer.call_module(self,m:torch.nn.Module,forward:Callable[...,Any],args:Tuple[Any,...],kwargs:Dict[str,Any])->Any
torch.fx.Tracer.create_arg(self,a:Any)->'Argument'
torch.fx.Tracer.create_args_for_root(self,root_fn,is_module,concrete_args=None)
torch.fx.Tracer.is_leaf_module(self,m:torch.nn.Module,module_qualified_name:str)->bool
torch.fx.Tracer.path_of_module(self,mod:torch.nn.Module)->str
torch.fx.Tracer.trace(self,root:Union[torch.nn.Module,Callable[...,Any]],concrete_args:Optional[Dict[str,Any]]=None)->Graph
torch.fx._symbolic_trace.PHBase(object)
torch.fx._symbolic_trace.PHBase.__repr__(self)
torch.fx._symbolic_trace.ProxyableClassMeta(cls,name,bases,attrs)
torch.fx._symbolic_trace.ProxyableClassMeta.__init__(cls,name,bases,attrs)
torch.fx._symbolic_trace.Tracer(self,autowrap_modules:Tuple[ModuleType]=(math,),autowrap_functions:Tuple[Callable,...]=(),param_shapes_constant:bool=False)
torch.fx._symbolic_trace.Tracer.__init__(self,autowrap_modules:Tuple[ModuleType]=(math,),autowrap_functions:Tuple[Callable,...]=(),param_shapes_constant:bool=False)
torch.fx._symbolic_trace.Tracer._module_getattr(self,attr,attr_val,parameter_proxy_cache)
torch.fx._symbolic_trace.Tracer.call_module(self,m:torch.nn.Module,forward:Callable[...,Any],args:Tuple[Any,...],kwargs:Dict[str,Any])->Any
torch.fx._symbolic_trace.Tracer.create_arg(self,a:Any)->'Argument'
torch.fx._symbolic_trace.Tracer.create_args_for_root(self,root_fn,is_module,concrete_args=None)
torch.fx._symbolic_trace.Tracer.is_leaf_module(self,m:torch.nn.Module,module_qualified_name:str)->bool
torch.fx._symbolic_trace.Tracer.path_of_module(self,mod:torch.nn.Module)->str
torch.fx._symbolic_trace.Tracer.trace(self,root:Union[torch.nn.Module,Callable[...,Any]],concrete_args:Optional[Dict[str,Any]]=None)->Graph
torch.fx._symbolic_trace._PatchedFn(NamedTuple)
torch.fx._symbolic_trace._PatchedFn.revert(self)
torch.fx._symbolic_trace._PatchedFnDel(_PatchedFn)
torch.fx._symbolic_trace._PatchedFnDel.revert(self)
torch.fx._symbolic_trace._PatchedFnSetAttr(_PatchedFn)
torch.fx._symbolic_trace._PatchedFnSetAttr.revert(self)
torch.fx._symbolic_trace._PatchedFnSetItem(_PatchedFn)
torch.fx._symbolic_trace._PatchedFnSetItem.revert(self)
torch.fx._symbolic_trace._Patcher(self)
torch.fx._symbolic_trace._Patcher.__enter__(self)
torch.fx._symbolic_trace._Patcher.__exit__(self,exc_type,exc_val,exc_tb)
torch.fx._symbolic_trace._Patcher.__init__(self)
torch.fx._symbolic_trace._Patcher.patch(self,frame_dict:Dict[str,Any],name:str,new_fn:Callable,deduplicate:bool=True)
torch.fx._symbolic_trace._Patcher.patch_method(self,cls:type,name:str,new_fn:Callable,deduplicate:bool=True)
torch.fx._symbolic_trace._Patcher.visit_once(self,thing:Any)
torch.fx._symbolic_trace._autowrap_check(patcher:_Patcher,frame_dict:Dict[str,Any],function_ids:Set[int])
torch.fx._symbolic_trace._create_wrapped_func(orig_fn)
torch.fx._symbolic_trace._create_wrapped_method(cls,name)
torch.fx._symbolic_trace._find_proxy(*objects_to_search)
torch.fx._symbolic_trace._patch_function(fn:FunctionType,nargs:int)->FunctionType
torch.fx._symbolic_trace._patch_wrapped_functions(patcher:_Patcher)
torch.fx._symbolic_trace.symbolic_trace(root:Union[torch.nn.Module,Callable[...,Any]],concrete_args:Optional[Dict[str,Any]]=None)->GraphModule
torch.fx._symbolic_trace.wrap(fn_or_name:Union[str,Callable])
torch.fx.symbolic_trace(root:Union[torch.nn.Module,Callable[...,Any]],concrete_args:Optional[Dict[str,Any]]=None)->GraphModule
torch.fx.wrap(fn_or_name:Union[str,Callable])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/graph_module.py----------------------------------------
A:torch.fx.graph_module.key->_EvalCacheLoader().cache(src, globals)
A:torch.fx.graph_module.globals_copy->globals.copy()
A:torch.fx.graph_module._loader->_EvalCacheLoader()
A:torch.fx.graph_module.(module_name, attr_name)->importer.get_name(obj)
A:torch.fx.graph_module.forward->_forward_from_src(import_block + fn_src, ns)
A:torch.fx.graph_module.ns->dict()
A:torch.fx.graph_module.fn_src->body.get('_code')
A:torch.fx.graph_module.tracer_cls->body.get('_tracer_cls')
A:torch.fx.graph_module.graphmodule_cls_name->body.get('_graphmodule_cls_name', 'GraphModule')
A:torch.fx.graph_module.com->CodeOnlyModule(body)
A:torch.fx.graph_module.graph->KeepModules().trace(com)
A:torch.fx.graph_module.gm->GraphModule(com, graph, class_name=graphmodule_cls_name)
A:torch.fx.graph_module.(*prefix, field)->target.split('.')
A:torch.fx.graph_module.f->getattr(from_module, item)
A:torch.fx.graph_module.t->torch.nn.Module()
A:torch.fx.graph_module.orig->getattr(from_module, field)
A:torch.fx.graph_module.folder->Path(folder)
A:torch.fx.graph_module.module_str->_gen_model_repr(module_name, module)
A:torch.fx.graph_module.module_repr->module.__repr__().replace('\r', ' ').replace('\n', ' ')
A:torch.fx.graph_module.submod->self.get_submodule(node.target)
A:torch.fx.graph_module.atoms->target.split('.')
A:torch.fx.graph_module.mod->getattr(mod, item)
A:torch.fx.graph_module.fullpath->node.target.split('.')
A:torch.fx.graph_module.python_code->self.recompile()
A:torch.fx.graph_module.cls->type(self)
A:torch.fx.graph_module.cls.forward->_forward_from_src(self._code, python_code.globals)
A:torch.fx.graph_module.err_line_len->len(frame_summary.line)
A:torch.fx.graph_module.all_src_lines->linecache.getlines(frame_summary.filename)
A:torch.fx.graph_module.tb_repr->traceback.format_exc()
A:torch.fx.graph_module.before_err->''.join(all_src_lines[err_lineno - 2:err_lineno])
A:torch.fx.graph_module.err_and_after_err->'\n'.join(all_src_lines[err_lineno:err_lineno + 2])
A:torch.fx.graph_module.dict_without_graph->self.__dict__.copy()
A:torch.fx.graph_module.import_block->_format_import_block(python_code.globals, sys_importer)
A:torch.fx.graph_module.fake_mod->torch.nn.Module()
A:torch.fx.graph_module.fake_mod.__dict__->copy.deepcopy(self.__dict__)
A:torch.fx.graph_module.orig_str->super().__str__()
A:torch.fx.graph_module.new_gm->self.__copy__()
torch.fx.GraphModule(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,class_name:str='GraphModule')
torch.fx.GraphModule.__copy__(self)
torch.fx.GraphModule.__deepcopy__(self,memo)
torch.fx.GraphModule.__reduce__(self)
torch.fx.GraphModule.__reduce_deploy__(self,importer:Importer)
torch.fx.GraphModule.__reduce_package__(self,exporter:PackageExporter)
torch.fx.GraphModule.__str__(self)->str
torch.fx.GraphModule._replicate_for_data_parallel(self)
torch.fx.GraphModule.add_submodule(self,target:str,m:torch.nn.Module)->bool
torch.fx.GraphModule.code(self)->str
torch.fx.GraphModule.delete_all_unused_submodules(self)->None
torch.fx.GraphModule.delete_submodule(self,target:str)->bool
torch.fx.GraphModule.graph(self)->Graph
torch.fx.GraphModule.graph(self,g:Graph)->None
torch.fx.GraphModule.recompile(self)->PythonCode
torch.fx.GraphModule.to_folder(self,folder:Union[str,os.PathLike],module_name:str='FxModule')
torch.fx.graph_module.GraphModule(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,class_name:str='GraphModule')
torch.fx.graph_module.GraphModule.__copy__(self)
torch.fx.graph_module.GraphModule.__deepcopy__(self,memo)
torch.fx.graph_module.GraphModule.__init__(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,class_name:str='GraphModule')
torch.fx.graph_module.GraphModule.__reduce__(self)
torch.fx.graph_module.GraphModule.__reduce_deploy__(self,importer:Importer)
torch.fx.graph_module.GraphModule.__reduce_package__(self,exporter:PackageExporter)
torch.fx.graph_module.GraphModule.__str__(self)->str
torch.fx.graph_module.GraphModule._replicate_for_data_parallel(self)
torch.fx.graph_module.GraphModule.add_submodule(self,target:str,m:torch.nn.Module)->bool
torch.fx.graph_module.GraphModule.code(self)->str
torch.fx.graph_module.GraphModule.delete_all_unused_submodules(self)->None
torch.fx.graph_module.GraphModule.delete_submodule(self,target:str)->bool
torch.fx.graph_module.GraphModule.graph(self)->Graph
torch.fx.graph_module.GraphModule.graph(self,g:Graph)->None
torch.fx.graph_module.GraphModule.recompile(self)->PythonCode
torch.fx.graph_module.GraphModule.to_folder(self,folder:Union[str,os.PathLike],module_name:str='FxModule')
torch.fx.graph_module._EvalCacheLoader(self)
torch.fx.graph_module._EvalCacheLoader.__init__(self)
torch.fx.graph_module._EvalCacheLoader._get_key(self)
torch.fx.graph_module._EvalCacheLoader.cache(self,src:str,globals:Dict[str,Any])
torch.fx.graph_module._EvalCacheLoader.get_source(self,module_name)->Optional[str]
torch.fx.graph_module._assign_attr(from_obj:Any,to_module:torch.nn.Module,target:str)
torch.fx.graph_module._copy_attr(from_module:torch.nn.Module,to_module:torch.nn.Module,target:str)
torch.fx.graph_module._deserialize_graph_module(forward,body:Dict[Any,Any])->torch.nn.Module
torch.fx.graph_module._exec_with_source(src:str,globals:Dict[str,Any])
torch.fx.graph_module._format_import_block(globals:Dict[str,Any],importer:Importer)
torch.fx.graph_module._format_import_statement(name:str,obj:Any,importer:Importer)->str
torch.fx.graph_module._forward_from_src(src:str,globals:Dict[str,Any])
torch.fx.graph_module.reduce_deploy_graph_module(importer:PackageImporter,body:Dict[Any,Any],import_block:str)->torch.nn.Module
torch.fx.graph_module.reduce_graph_module(body:Dict[Any,Any],import_block:str)->torch.nn.Module
torch.fx.graph_module.reduce_package_graph_module(importer:PackageImporter,body:Dict[Any,Any],generated_module_name:str)->torch.nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/graph.py----------------------------------------
A:torch.fx.graph._custom_builtins[name]->_CustomBuiltin(import_str, obj)
A:torch.fx.graph.prev_lower->c.islower()
A:torch.fx.graph.module_name->getattr(obj, '__module__', None)
A:torch.fx.graph.name->self._graph_namespace.create_name(candidate, None)
A:torch.fx.graph.self._unassociated_names->set()
A:torch.fx.graph.self._illegal_char_regex->re.compile('[^0-9a-zA-Z_]+')
A:torch.fx.graph.self._name_suffix_regex->re.compile('(.*)_(\\d+)$')
A:torch.fx.graph.candidate->self._illegal_char_regex.sub('_', candidate)
A:torch.fx.graph.match->self._name_suffix_regex.match(candidate)
A:torch.fx.graph.(base, num_str)->self._name_suffix_regex.match(candidate).group(1, 2)
A:torch.fx.graph.num->int(num_str)
A:torch.fx.graph.args_s->', '.join((repr(a) for a in args))
A:torch.fx.graph.kwargs_s->', '.join((f'{k} = {repr(v)}' for (k, v) in kwargs.items()))
A:torch.fx.graph.elems->target.split('.')
A:torch.fx.graph.cur->getattr(cur, direction)
A:torch.fx.graph.self._graph_namespace->_Namespace()
A:torch.fx.graph.rv->map_arg(node.args[0], lambda n: val_map[n])
A:torch.fx.graph.val_map[node]->self.node_copy(node, lambda n: val_map[n])
A:torch.fx.graph.g->Graph(tracer_cls=self._tracer_cls)
A:torch.fx.graph.output_vals->Graph(tracer_cls=self._tracer_cls).graph_copy(self, val_map=memo, return_output_node=True)
A:torch.fx.graph.n->Node(self, name, op, target, args, kwargs, type_expr)
A:torch.fx.graph.(flat_args, args_spec)->torch.utils._pytree.tree_flatten(args)
A:torch.fx.graph.new_args->map_arg(to_erase.args, lambda n: None)
A:torch.fx.graph.new_kwargs->map_arg(to_erase.kwargs, lambda n: None)
A:torch.fx.graph.(module_path, _, name)->_get_qualified_name(node.target).rpartition('.')
A:torch.fx.graph.res->getattr(submod, name)
A:torch.fx.graph.args->map_arg(node.args, arg_transform)
A:torch.fx.graph.kwargs->map_arg(node.kwargs, arg_transform)
A:torch.fx.graph.result_node->self.create_node(node.op, node.target, args, kwargs, node.name, node.type)
A:torch.fx.graph.result_node.meta->copy.copy(node.meta)
A:torch.fx.graph.op->_snake_case(op)
A:torch.fx.graph.namespace->_Namespace()
A:torch.fx.graph.global_name->add_global(qualified_name, node.target)
A:torch.fx.graph.typename->_type_repr(o)
A:torch.fx.graph.origin_type->_origin_type_map.get(o.__origin__, o.__origin__)
A:torch.fx.graph.origin_typename->add_global(_type_repr(origin_type), origin_type)
A:torch.fx.graph.nodes_to_delete->user_to_last_uses.get(user, [])
A:torch.fx.graph.to_delete_str->' = '.join([repr(n) for n in nodes_to_delete] + ['None'])
A:torch.fx.graph.raw_name->node.target.replace('*', '')
A:torch.fx.graph.qualified_name->_get_qualified_name(node.target)
A:torch.fx.graph.wrap_name->add_global('wrap', torch.fx.wrap)
A:torch.fx.graph.wrap_stmts->'\n'.join([f'{wrap_name}("{name}")' for name in wrapped_fns])
A:torch.fx.graph.body->self._on_generate_code(body)
A:torch.fx.graph.code->'\n'.join(('    ' + line for line in code.split('\n')))
A:torch.fx.graph.param_str->', '.join(placeholder_names)
A:torch.fx.graph.target_atoms->node.target.split('.')
A:torch.fx.graph.new_m_itr->getattr(m_itr, atom, None)
A:torch.fx.graph.seen_qualname->'.'.join(target_atoms[:i])
A:torch.fx.graph.self._on_generate_code->make_transformer(on_gen_code_old)
A:torch.fx.graph.magic_methods->dict({'eq': '{} == {}', 'ne': '{} != {}', 'lt': '{} < {}', 'gt': '{} > {}', 'le': '{} <= {}', 'ge': '{} >= {}', 'pos': '+{}', 'neg': '-{}', 'invert': '~{}'}, **reflectable_magic_methods)
torch.fx.Graph(self,owning_module:Optional['GraphModule']=None,tracer_cls:Optional[Type['Tracer']]=None)
torch.fx.Graph.__deepcopy__(self,memo=None)->'Graph'
torch.fx.Graph.__str__(self)->str
torch.fx.Graph._python_code(self,root_module:str,namespace:_Namespace)->PythonCode
torch.fx.Graph._target_to_str(self,target:Target)->str
torch.fx.Graph.call_function(self,the_function:Callable[...,Any],args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,type_expr:Optional[Any]=None)->Node
torch.fx.Graph.call_method(self,method_name:str,args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,type_expr:Optional[Any]=None)->Node
torch.fx.Graph.call_module(self,module_name:str,args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,type_expr:Optional[Any]=None)->Node
torch.fx.Graph.create_node(self,op:str,target:'Target',args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,name:Optional[str]=None,type_expr:Optional[Any]=None)->Node
torch.fx.Graph.eliminate_dead_code(self)
torch.fx.Graph.erase_node(self,to_erase:Node)->None
torch.fx.Graph.flatten_inps(self,*args)
torch.fx.Graph.get_attr(self,qualified_name:str,type_expr:Optional[Any]=None)->Node
torch.fx.Graph.graph_copy(self,g:'Graph',val_map:Dict[Node,Node],return_output_node=False)->'Optional[Argument]'
torch.fx.Graph.inserting_after(self,n:Optional[Node]=None)
torch.fx.Graph.inserting_before(self,n:Optional[Node]=None)
torch.fx.Graph.lint(self)
torch.fx.Graph.node_copy(self,node:Node,arg_transform:Callable[[Node],'Argument']=lambdax:x)->Node
torch.fx.Graph.nodes(self)->_node_list
torch.fx.Graph.on_generate_code(self,make_transformer:Callable[[Optional[TransformCodeFunc]],TransformCodeFunc])
torch.fx.Graph.output(self,result:'Argument',type_expr:Optional[Any]=None)
torch.fx.Graph.owning_module(self)
torch.fx.Graph.owning_module(self,mod:Optional['GraphModule'])
torch.fx.Graph.placeholder(self,name:str,type_expr:Optional[Any]=None,default_value:Any=inspect.Signature.empty)->Node
torch.fx.Graph.print_tabular(self)
torch.fx.Graph.python_code(self,root_module:str)->PythonCode
torch.fx.Graph.unflatten_outs(self,out)
torch.fx.graph.Graph(self,owning_module:Optional['GraphModule']=None,tracer_cls:Optional[Type['Tracer']]=None)
torch.fx.graph.Graph.__deepcopy__(self,memo=None)->'Graph'
torch.fx.graph.Graph.__init__(self,owning_module:Optional['GraphModule']=None,tracer_cls:Optional[Type['Tracer']]=None)
torch.fx.graph.Graph.__str__(self)->str
torch.fx.graph.Graph._python_code(self,root_module:str,namespace:_Namespace)->PythonCode
torch.fx.graph.Graph._target_to_str(self,target:Target)->str
torch.fx.graph.Graph.call_function(self,the_function:Callable[...,Any],args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,type_expr:Optional[Any]=None)->Node
torch.fx.graph.Graph.call_method(self,method_name:str,args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,type_expr:Optional[Any]=None)->Node
torch.fx.graph.Graph.call_module(self,module_name:str,args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,type_expr:Optional[Any]=None)->Node
torch.fx.graph.Graph.create_node(self,op:str,target:'Target',args:Optional[Tuple['Argument',...]]=None,kwargs:Optional[Dict[str,'Argument']]=None,name:Optional[str]=None,type_expr:Optional[Any]=None)->Node
torch.fx.graph.Graph.eliminate_dead_code(self)
torch.fx.graph.Graph.erase_node(self,to_erase:Node)->None
torch.fx.graph.Graph.flatten_inps(self,*args)
torch.fx.graph.Graph.get_attr(self,qualified_name:str,type_expr:Optional[Any]=None)->Node
torch.fx.graph.Graph.graph_copy(self,g:'Graph',val_map:Dict[Node,Node],return_output_node=False)->'Optional[Argument]'
torch.fx.graph.Graph.inserting_after(self,n:Optional[Node]=None)
torch.fx.graph.Graph.inserting_before(self,n:Optional[Node]=None)
torch.fx.graph.Graph.lint(self)
torch.fx.graph.Graph.node_copy(self,node:Node,arg_transform:Callable[[Node],'Argument']=lambdax:x)->Node
torch.fx.graph.Graph.nodes(self)->_node_list
torch.fx.graph.Graph.on_generate_code(self,make_transformer:Callable[[Optional[TransformCodeFunc]],TransformCodeFunc])
torch.fx.graph.Graph.output(self,result:'Argument',type_expr:Optional[Any]=None)
torch.fx.graph.Graph.owning_module(self)
torch.fx.graph.Graph.owning_module(self,mod:Optional['GraphModule'])
torch.fx.graph.Graph.placeholder(self,name:str,type_expr:Optional[Any]=None,default_value:Any=inspect.Signature.empty)->Node
torch.fx.graph.Graph.print_tabular(self)
torch.fx.graph.Graph.python_code(self,root_module:str)->PythonCode
torch.fx.graph.Graph.unflatten_outs(self,out)
torch.fx.graph.PythonCode
torch.fx.graph._CustomBuiltin(NamedTuple)
torch.fx.graph._InsertPoint(self,graph,new_insert)
torch.fx.graph._InsertPoint.__enter__(self)
torch.fx.graph._InsertPoint.__exit__(self,type,value,tb)
torch.fx.graph._InsertPoint.__init__(self,graph,new_insert)
torch.fx.graph._Namespace(self)
torch.fx.graph._Namespace.__init__(self)
torch.fx.graph._Namespace._is_illegal_name(self,name:str,obj:Any)->bool
torch.fx.graph._Namespace.associate_name_with_obj(self,name:str,obj:Any)
torch.fx.graph._Namespace.create_name(self,candidate:str,obj:Optional[Any])->str
torch.fx.graph._PyTreeInfo(NamedTuple)
torch.fx.graph._format_args(args:Tuple[Argument,...],kwargs:Dict[str,Argument])->str
torch.fx.graph._format_target(base:str,target:str)->str
torch.fx.graph._is_from_torch(obj:Any)->bool
torch.fx.graph._is_magic(x:str)->bool
torch.fx.graph._node_list(self,graph:'Graph',direction:str='_next')
torch.fx.graph._node_list.__init__(self,graph:'Graph',direction:str='_next')
torch.fx.graph._node_list.__iter__(self)
torch.fx.graph._node_list.__len__(self)
torch.fx.graph._node_list.__reversed__(self)
torch.fx.graph._register_custom_builtin(name:str,import_str:str,obj:Any)
torch.fx.graph._snake_case(s:str)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/tensor_type.py----------------------------------------
A:torch.fx.tensor_type.Dyn->_DynType()
torch.fx.tensor_type.TensorType(self,dim)
torch.fx.tensor_type.TensorType.__class_getitem__(*args)
torch.fx.tensor_type.TensorType.__eq__(self,other)
torch.fx.tensor_type.TensorType.__init__(self,dim)
torch.fx.tensor_type.TensorType.__repr__(self)
torch.fx.tensor_type._DynType(self)
torch.fx.tensor_type._DynType.__eq__(self,other)
torch.fx.tensor_type._DynType.__init__(self)
torch.fx.tensor_type._DynType.__repr__(self)
torch.fx.tensor_type._DynType.__str__(self)
torch.fx.tensor_type.is_consistent(t1,t2)
torch.fx.tensor_type.is_more_precise(t1,t2)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/immutable_collections.py----------------------------------------
A:torch.fx.immutable_collections.container->type('immutable_' + base.__name__, (base,), {})
A:torch.fx.immutable_collections.immutable_list->_create_immutable_container(list, ['__delitem__', '__iadd__', '__imul__', '__setitem__', 'append', 'clear', 'extend', 'insert', 'pop', 'remove'])
A:torch.fx.immutable_collections.immutable_dict->_create_immutable_container(dict, ['__delitem__', '__setitem__', 'clear', 'pop', 'popitem', 'update'])
torch.fx.immutable_collections._create_immutable_container(base,mutable_functions)
torch.fx.immutable_collections._no_mutation(self,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/normalize.py----------------------------------------
A:torch.fx.experimental.normalize.(args, kwargs)->self.fetch_args_kwargs_from_env(n)
A:torch.fx.experimental.normalize.arg_types->tuple([create_type_hint(i) for i in arg_types])
A:torch.fx.experimental.normalize.out->super().run_node(n)
A:torch.fx.experimental.normalize.new_args_and_kwargs->normalize_module(self.module, target, args, kwargs, self.normalize_to_only_use_kwargs)
torch.fx.experimental.normalize.NormalizeArgs(self,module:torch.nn.Module,normalize_to_only_use_kwargs:bool=True)
torch.fx.experimental.normalize.NormalizeArgs.__init__(self,module:torch.nn.Module,normalize_to_only_use_kwargs:bool=True)
torch.fx.experimental.normalize.NormalizeArgs.call_function(self,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Any],arg_types:Optional[Tuple[Any,...]]=None,kwarg_types:Optional[Dict[str,Any]]=None)
torch.fx.experimental.normalize.NormalizeArgs.call_module(self,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Any])
torch.fx.experimental.normalize.NormalizeArgs.run_node(self,n:Node)->Any
torch.fx.experimental.normalize.NormalizeOperators(AnnotateTypesWithSchema)
torch.fx.experimental.normalize.NormalizeOperators.call_function(self,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Any])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/schema_type_annotation.py----------------------------------------
A:torch.fx.experimental.schema_type_annotation.python_ret_type->self._extract_python_return_type(submod.forward)
A:torch.fx.experimental.schema_type_annotation.return_proxy->super().call_module(target, args, kwargs)
A:torch.fx.experimental.schema_type_annotation.submod->self.fetch_attr(target)
A:torch.fx.experimental.schema_type_annotation.attr_proxy->super().get_attr(target, args, kwargs)
A:torch.fx.experimental.schema_type_annotation.atoms->target.split('.')
A:torch.fx.experimental.schema_type_annotation.module_itr->getattr(module_itr, atom)
A:torch.fx.experimental.schema_type_annotation.maybe_inferred_ts_type->torch._C._jit_try_infer_type(module_itr)
A:torch.fx.experimental.schema_type_annotation.python_type->_torchscript_type_to_python_type(maybe_inferred_ts_type.type())
A:torch.fx.experimental.schema_type_annotation.sig->inspect.signature(target)
torch.fx.experimental.schema_type_annotation.AnnotateTypesWithSchema(self,module:torch.nn.Module,annotate_functionals:bool=True,annotate_modules:bool=True,annotate_get_attrs:bool=True)
torch.fx.experimental.schema_type_annotation.AnnotateTypesWithSchema.__init__(self,module:torch.nn.Module,annotate_functionals:bool=True,annotate_modules:bool=True,annotate_get_attrs:bool=True)
torch.fx.experimental.schema_type_annotation.AnnotateTypesWithSchema._extract_python_return_type(self,target:Target)->Optional[Any]
torch.fx.experimental.schema_type_annotation.AnnotateTypesWithSchema.call_function(self,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Any])
torch.fx.experimental.schema_type_annotation.AnnotateTypesWithSchema.call_module(self,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Any])
torch.fx.experimental.schema_type_annotation.AnnotateTypesWithSchema.get_attr(self,target:torch.fx.node.Target,args:Tuple[Argument,...],kwargs:Dict[str,Any])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/const_fold.py----------------------------------------
A:torch.fx.experimental.const_fold.folded_attrs->node.graph.get_attr(fx_const_folded_attrs_name)
A:torch.fx.experimental.const_fold.new_node.meta->node.meta.copy()
A:torch.fx.experimental.const_fold.output_replacements->map_arg(outputs, replacement_fn)
A:torch.fx.experimental.const_fold.new_node->torch.fx.GraphModule(split, const_gm.graph).graph.get_attr(in_node.target)
A:torch.fx.experimental.const_fold.mod_traced->torch.fx.symbolic_trace(module)
A:torch.fx.experimental.const_fold.split->split_module(mod_traced, module, mod_partition)
A:torch.fx.experimental.const_fold.root_const_gm->torch.fx.GraphModule(split, const_gm.graph)
A:torch.fx.experimental.const_fold.multiple_outputs->isinstance(node.args[0], tuple)
A:torch.fx.experimental.const_fold.in_node->next((n for n in call_const_gm_args if n.name == node.target))
A:torch.fx.experimental.const_fold.fx_const_folded_attrs_name->torch.fx.experimental.fx_acc.acc_utils.get_unique_attr_name_in_module(split, '_FX_CONST_FOLDED_ATTRS')
A:torch.fx.experimental.const_fold.folded_attrs.meta->node.meta.copy()
torch.fx.experimental.const_fold.FoldedGraphModule(self,root:torch.nn.Module,graph:torch.fx.Graph,const_subgraph:Optional[torch.fx.Graph]=None,fx_const_folded_attrs_name:str=None)
torch.fx.experimental.const_fold.FoldedGraphModule.__init__(self,root:torch.nn.Module,graph:torch.fx.Graph,const_subgraph:Optional[torch.fx.Graph]=None,fx_const_folded_attrs_name:str=None)
torch.fx.experimental.const_fold.FoldedGraphModule.run_folding(self)
torch.fx.experimental.const_fold._inline_module(gm:torch.fx.GraphModule,inline_mod_name:str)
torch.fx.experimental.const_fold.split_const_subgraphs(module:Union[torch.nn.Module,torch.fx.GraphModule],skip_folding_node_fn:Optional[Callable[[torch.fx.Node],bool]]=None)->FoldedGraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/debug.py----------------------------------------
torch.fx.experimental.debug.set_trace(gm:fx.GraphModule)->fx.GraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unify_refinements.py----------------------------------------
A:torch.fx.experimental.unify_refinements.r->Refine(traced)
A:torch.fx.experimental.unify_refinements.mgu->unify_eq(r.constraints)
A:torch.fx.experimental.unify_refinements.(lhs, rhs)->convert_eq(list_of_eq)
A:torch.fx.experimental.unify_refinements.n.type->substitute_solution_one_type(mapping, n.type)
torch.fx.experimental.unify_refinements.check_for_type_equality(g1,g2)
torch.fx.experimental.unify_refinements.convert_eq(list_of_eq)
torch.fx.experimental.unify_refinements.infer_symbolic_types(traced)
torch.fx.experimental.unify_refinements.infer_symbolic_types_single_pass(traced)
torch.fx.experimental.unify_refinements.substitute_all_types(graph,mapping)
torch.fx.experimental.unify_refinements.substitute_solution_one_type(mapping,t)
torch.fx.experimental.unify_refinements.unify_eq(list_of_eq)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/graph_gradual_typechecker.py----------------------------------------
A:torch.fx.experimental.graph_gradual_typechecker.s1->len(t1.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.s2->len(t2.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.new_t1->list(t1.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.new_t2->list(t2.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.(new_t1, new_t2)->broadcast_types(t1, t2)
A:torch.fx.experimental.graph_gradual_typechecker.new_type->TensorType((n.type.__args__[0], n.type.__args__[1], h_out, w_out))
A:torch.fx.experimental.graph_gradual_typechecker.final->TensorType(new_type)
A:torch.fx.experimental.graph_gradual_typechecker.n.type->torch.fx.node.map_arg(n.args[0], get_node_type)
A:torch.fx.experimental.graph_gradual_typechecker.t2_type->TensorType([Dyn if elem == -1 else elem for elem in t2])
A:torch.fx.experimental.graph_gradual_typechecker.p1->reduce(lambda x, y: x * y, a)
A:torch.fx.experimental.graph_gradual_typechecker.p2->reduce(lambda x, y: x * y, t2)
A:torch.fx.experimental.graph_gradual_typechecker.n.args[0].type->expand_to_tensor_dim(n.args[0].type, len(n.type.__args__))
A:torch.fx.experimental.graph_gradual_typechecker.curr_node_type->expand_to_tensor_dim(n.type, 4)
A:torch.fx.experimental.graph_gradual_typechecker.h_out->calculate_out_dimension(h_in, module_instance, 0)
A:torch.fx.experimental.graph_gradual_typechecker.w_out->calculate_out_dimension(w_in, module_instance, 1)
A:torch.fx.experimental.graph_gradual_typechecker.gub->get_greatest_upper_bound(new_type, curr_node_type)
A:torch.fx.experimental.graph_gradual_typechecker.new_type_list->list(tensor_type.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.output->maxpool2d_check(n.args[0].type, module_instance)
A:torch.fx.experimental.graph_gradual_typechecker.new_type_args->list(tensor_type.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.output_type->flatten_check(n.args[0].type, start_dim, end_dim)
A:torch.fx.experimental.graph_gradual_typechecker.output_size->list(output_size)
A:torch.fx.experimental.graph_gradual_typechecker.l->len(n.type.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.my_args->list(tensor_type.__args__)
A:torch.fx.experimental.graph_gradual_typechecker.t->get_parameter(self.traced, n.target)
A:torch.fx.experimental.graph_gradual_typechecker.module_instance->self.traced.get_submodule(n.target)
A:torch.fx.experimental.graph_gradual_typechecker.(args1, args2)->broadcast_types(arg_type1, arg_type2)
A:torch.fx.experimental.graph_gradual_typechecker.self.symbol_iter->itertools.count(start=0, step=1)
A:torch.fx.experimental.graph_gradual_typechecker.new_symbol->Var(next(self.symbol_iter))
A:torch.fx.experimental.graph_gradual_typechecker.(module_path, _, param_name)->target.rpartition('.')
torch.fx.experimental.graph_gradual_typechecker.GraphTypeChecker(self,env,traced)
torch.fx.experimental.graph_gradual_typechecker.GraphTypeChecker.__init__(self,env,traced)
torch.fx.experimental.graph_gradual_typechecker.GraphTypeChecker.type_check(self)
torch.fx.experimental.graph_gradual_typechecker.GraphTypeChecker.type_check_node(self,n:Node)
torch.fx.experimental.graph_gradual_typechecker.Refine(self,traced)
torch.fx.experimental.graph_gradual_typechecker.Refine.__init__(self,traced)
torch.fx.experimental.graph_gradual_typechecker.Refine.convert_to_sympy_symbols(self,typ)
torch.fx.experimental.graph_gradual_typechecker.Refine.infer_symbolic_relations(self,n:Node)
torch.fx.experimental.graph_gradual_typechecker.Refine.refine(self)
torch.fx.experimental.graph_gradual_typechecker.Refine.refine_node(self,n:Node)
torch.fx.experimental.graph_gradual_typechecker.Refine.replace_dyn_with_fresh_var(self,typ)
torch.fx.experimental.graph_gradual_typechecker.Refine.symbolic_relations(self)
torch.fx.experimental.graph_gradual_typechecker.adaptiveavgpool2d_check(tensor_type,module_instance)
torch.fx.experimental.graph_gradual_typechecker.adaptiveavgpool2d_inference_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.add_inference_rule(n:Node)
torch.fx.experimental.graph_gradual_typechecker.all_eq(n:Node)
torch.fx.experimental.graph_gradual_typechecker.bn2d_inference_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.broadcast_types(t1,t2)
torch.fx.experimental.graph_gradual_typechecker.calculate_out_dimension(d_in,module_instance,index)
torch.fx.experimental.graph_gradual_typechecker.conv2d_inference_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.conv_refinement_rule(n:Node)
torch.fx.experimental.graph_gradual_typechecker.conv_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.element_wise_eq(n:Node)
torch.fx.experimental.graph_gradual_typechecker.expand_to_tensor_dim(t,n)
torch.fx.experimental.graph_gradual_typechecker.first_two_eq(n:Node)
torch.fx.experimental.graph_gradual_typechecker.flatten_check(tensor_type,start_dim,end_dim)
torch.fx.experimental.graph_gradual_typechecker.flatten_inference_rule(n:Node)
torch.fx.experimental.graph_gradual_typechecker.flatten_refinement_rule(n:Node)
torch.fx.experimental.graph_gradual_typechecker.get_attr_inference_rule(n:Node,traced)
torch.fx.experimental.graph_gradual_typechecker.get_greatest_upper_bound(type1,type2)
torch.fx.experimental.graph_gradual_typechecker.get_parameter(traced,target:str)
torch.fx.experimental.graph_gradual_typechecker.linear_check(tensor_type,module_instance)
torch.fx.experimental.graph_gradual_typechecker.linear_inference_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.linear_refinement_rule(n:Node)
torch.fx.experimental.graph_gradual_typechecker.maxpool2d_check(typ,module_instance)
torch.fx.experimental.graph_gradual_typechecker.maxpool2d_inference_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.register_algebraic_expressions_inference_rule(call_target)
torch.fx.experimental.graph_gradual_typechecker.register_inference_rule(call_target)
torch.fx.experimental.graph_gradual_typechecker.register_refinement_rule(call_target)
torch.fx.experimental.graph_gradual_typechecker.relu_inference_rule(n:Node,module_instance)
torch.fx.experimental.graph_gradual_typechecker.reshape_inference_rule(n:Node)
torch.fx.experimental.graph_gradual_typechecker.transpose_inference_rule(n:Node)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/optimization.py----------------------------------------
A:torch.fx.experimental.optimization.(*parent, name)->target.rsplit('.', 1)
A:torch.fx.experimental.optimization.(parent_name, name)->_parent_name(node.target)
A:torch.fx.experimental.optimization.model->remove_dropout(model)
A:torch.fx.experimental.optimization.fx_model->torch.fx.GraphModule(cur_tracer.root, fx_graph)
A:torch.fx.experimental.optimization.modules->dict(fx_model.named_modules())
A:torch.fx.experimental.optimization.new_graph->torch.fx.Graph()
A:torch.fx.experimental.optimization.fused_conv->fuse_conv_bn_eval(conv, bn)
A:torch.fx.experimental.optimization.new_node->torch.fx.Graph().node_copy(node, lambda x: env[x])
A:torch.fx.experimental.optimization.new_module->mkldnn_map[type(cur_module)](cur_module, torch.float)
A:torch.fx.experimental.optimization.old_modules[new_module]->copy.deepcopy(cur_module)
A:torch.fx.experimental.optimization.output_args->cast(List[fx.Node], [node.args[0] for node in graph.end_nodes])
A:torch.fx.experimental.optimization.submodule->extract_subgraph(fx_model, graph.nodes, input_nodes, output_args)
A:torch.fx.experimental.optimization.begin->time.time()
A:torch.fx.experimental.optimization.out->f()
A:torch.fx.experimental.optimization.mkl_time->benchmark(lambda : [i.to_dense() for i in submodule(*[i.to_mkldnn() for i in sample_inputs])])
A:torch.fx.experimental.optimization.no_mkl_time->benchmark(lambda : submodule(*sample_inputs))
A:torch.fx.experimental.optimization.self.parent[v]->self.find(par)
A:torch.fx.experimental.optimization.cur_tracer->tracer()
A:torch.fx.experimental.optimization.fx_graph->tracer().trace(copy.deepcopy(model))
A:torch.fx.experimental.optimization.sample_parameter->next(cur_module.parameters(), None)
A:torch.fx.experimental.optimization.mkldnn_args->torch.fx.map_arg(node.args, lambda n: fx_graph.call_method('to_mkldnn', (n,)))
A:torch.fx.experimental.optimization.node.args->cast(Tuple[fx.node.Argument], mkldnn_args)
A:torch.fx.experimental.optimization.dense_x->tracer().trace(copy.deepcopy(model)).create_node('call_method', 'to_dense', (node,))
A:torch.fx.experimental.optimization.old_modules->modules_to_mkldnn(list(fx_graph.nodes), modules)
A:torch.fx.experimental.optimization.users->list(node.users)
A:torch.fx.experimental.optimization.num_nodes->len(fx_graph.nodes)
A:torch.fx.experimental.optimization.uf->UnionFind(num_nodes)
A:torch.fx.experimental.optimization.node.end_color->get_color(node.args[0])
A:torch.fx.experimental.optimization.cur_colors->sorted(cur_colors)
A:torch.fx.experimental.optimization.result->torch.fx.GraphModule(model, fx_graph)
torch.fx.experimental.optimization.MklSubgraph(self,fx_graph:fx.Graph)
torch.fx.experimental.optimization.MklSubgraph.__init__(self,fx_graph:fx.Graph)
torch.fx.experimental.optimization.UnionFind(self,n)
torch.fx.experimental.optimization.UnionFind.__init__(self,n)
torch.fx.experimental.optimization.UnionFind.find(self,v:int)->int
torch.fx.experimental.optimization.UnionFind.join(self,a:int,b:int)
torch.fx.experimental.optimization.UnionFind.make_set(self,v:int)
torch.fx.experimental.optimization._parent_name(target:str)->Tuple[str, str]
torch.fx.experimental.optimization.extract_subgraph(orig_module:nn.Module,nodes:List[fx.Node],inputs:List[fx.Node],outputs:List[fx.Node])
torch.fx.experimental.optimization.fuse(model:torch.nn.Module,inplace=False)->torch.nn.Module
torch.fx.experimental.optimization.gen_mkl_autotuner(example_inputs,iters=10,warmup=1)
torch.fx.experimental.optimization.matches_module_pattern(pattern:Iterable[Type],node:fx.Node,modules:Dict[str,Any])
torch.fx.experimental.optimization.modules_to_mkldnn(nodes:List[fx.Node],modules:Dict[str,nn.Module])
torch.fx.experimental.optimization.optimize_for_inference(model:torch.nn.Module,pass_config:Optional[Dict[str,Any]]=None,tracer:Type[fx.Tracer]=fx.Tracer)->torch.nn.Module
torch.fx.experimental.optimization.remove_dropout(model:nn.Module)->nn.Module
torch.fx.experimental.optimization.replace_node_module(node:fx.Node,modules:Dict[str,Any],new_module:torch.nn.Module)
torch.fx.experimental.optimization.reset_modules(nodes:List[fx.Node],modules:Dict[str,nn.Module],old_modules:Dict[nn.Module,nn.Module])
torch.fx.experimental.optimization.use_mkl_length(graph:MklSubgraph)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/partitioner_utils.py----------------------------------------
A:torch.fx.experimental.partitioner_utils.size_bytes->getattr(n, 'size_bytes', None)
A:torch.fx.experimental.partitioner_utils.users->set(node.users).intersection(partition.nodes)
A:torch.fx.experimental.partitioner_utils.max_latency->PartitionLatency(mem_latency_sec=0.0, computer_latency_sec=0.0, overall_latency_sec=0.0)
A:torch.fx.experimental.partitioner_utils.new_partition_latency->dfs_helper(n, PartitionLatency(mem_latency_sec, computer_latency_sec, overall_latency_sec))
A:torch.fx.experimental.partitioner_utils.top_nodes->get_top_nodes(partition)
A:torch.fx.experimental.partitioner_utils.critical_path_latency->PartitionLatency(mem_latency_sec=0.0, computer_latency_sec=0.0, overall_latency_sec=0.0)
A:torch.fx.experimental.partitioner_utils.partition_latency->get_latency_of_one_partition(partition, node_to_latency_mapping)
A:torch.fx.experimental.partitioner_utils.visited_nodes->set()
A:torch.fx.experimental.partitioner_utils.comm_latency_sec->get_comm_latency_between(partition, child, transfer_rate_bytes_per_sec)
A:torch.fx.experimental.partitioner_utils.new_latency_sec->dfs_helper(child, latency_so_far_sec + comm_latency_sec)
A:torch.fx.experimental.partitioner_utils.top_partitions->get_top_partitions(partitions)
A:torch.fx.experimental.partitioner_utils.latency_sec->dfs_helper(partition, 0.0)
torch.fx.experimental.partitioner_utils.Device(NamedTuple)
torch.fx.experimental.partitioner_utils.NodeLatency(NamedTuple)
torch.fx.experimental.partitioner_utils.Partition(self,partition_id:int)
torch.fx.experimental.partitioner_utils.Partition.__init__(self,partition_id:int)
torch.fx.experimental.partitioner_utils.Partition.__str__(self)
torch.fx.experimental.partitioner_utils.Partition.add_node(self,node)
torch.fx.experimental.partitioner_utils.Partition.recalculate_mem_size(self)
torch.fx.experimental.partitioner_utils.Partition.remove_node(self,node)
torch.fx.experimental.partitioner_utils.PartitionLatency(NamedTuple)
torch.fx.experimental.partitioner_utils.PartitionMode(Enum)
torch.fx.experimental.partitioner_utils.PartitionerConfig(NamedTuple)
torch.fx.experimental.partitioner_utils.get_comm_latency_between(parent_partition:Partition,child_partition:Partition,transfer_rate_bytes_per_sec:float)
torch.fx.experimental.partitioner_utils.get_extra_size_of(node:Node,nodes:Set[Node])->int
torch.fx.experimental.partitioner_utils.get_latency_of_one_partition(partition:Partition,node_to_latency_mapping:Dict[Node,NodeLatency])->PartitionLatency
torch.fx.experimental.partitioner_utils.get_latency_of_partitioned_graph(partitions:List[Partition],partition_to_latency_mapping:Dict[Partition,PartitionLatency],transfer_rate_bytes_per_sec:float)
torch.fx.experimental.partitioner_utils.get_partition_to_latency_mapping(partitions:List[Partition],node_to_latency_mapping:Dict[Node,NodeLatency])->Dict[Partition, PartitionLatency]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/merge_matmul.py----------------------------------------
A:torch.fx.experimental.merge_matmul.new_graph->Graph()
A:torch.fx.experimental.merge_matmul.value_remap[node]->Graph().node_copy(node, lambda n: value_remap[n])
A:torch.fx.experimental.merge_matmul.gm->symbolic_trace(in_mod)
A:torch.fx.experimental.merge_matmul.merge_mm_cat->symbolic_trace(in_mod).graph.call_function(torch.cat, (lhs,), {})
A:torch.fx.experimental.merge_matmul.merge_mm->symbolic_trace(in_mod).graph.call_function(torch.matmul, (merge_mm_cat, rhs), {})
A:torch.fx.experimental.merge_matmul.merge_mm_split->symbolic_trace(in_mod).graph.call_function(torch.split, (merge_mm, merge_mm_sizes), {})
torch.fx.experimental.merge_matmul.are_nodes_independent(nodes:List[Node])
torch.fx.experimental.merge_matmul.get_first_dim(t:torch.Tensor)->int
torch.fx.experimental.merge_matmul.legalize_graph(gm:GraphModule)
torch.fx.experimental.merge_matmul.may_depend_on(a:Node,b:Node,search_depth:int=6)
torch.fx.experimental.merge_matmul.merge_matmul(in_mod:torch.nn.Module)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/rewriter.py----------------------------------------
A:torch.fx.experimental.rewriter.(sourcelines, _)->inspect.getsourcelines(fn)
A:torch.fx.experimental.rewriter.sourcelines->normalize_source_lines(sourcelines)
A:torch.fx.experimental.rewriter.source->''.join(sourcelines)
A:torch.fx.experimental.rewriter.normalized_str->textwrap.dedent(source)
A:torch.fx.experimental.rewriter.source_ast->ast.parse(normalized_str)
A:torch.fx.experimental.rewriter.dest_ast->ast.fix_missing_locations(self.visit(source_ast))
A:torch.fx.experimental.rewriter.code->compile(dest_ast, '', 'exec')
A:torch.fx.experimental.rewriter.globals_dict->copy.copy(fn.__globals__)
A:torch.fx.experimental.rewriter.keys_before->set(globals_dict.keys())
A:torch.fx.experimental.rewriter.new_keys->list(set(globals_dict.keys()) - keys_before)
A:torch.fx.experimental.rewriter.g->functools.update_wrapper(g, f)
A:torch.fx.experimental.rewriter.g.__kwdefaults__->copy.copy(f.__kwdefaults__)
A:torch.fx.experimental.rewriter.n->ast.parse('torch._assert()', mode='eval')
A:torch.fx.experimental.rewriter.expr_wrapper->ast.Expr(value=call_node)
A:torch.fx.experimental.rewriter.self.__dict__[k]->copy.copy(v)
A:torch.fx.experimental.rewriter.RewrittenModule.forward->AST_Rewriter().rewrite(cast(FunctionType, m.forward))
torch.fx.experimental.rewriter.AST_Rewriter(ast.NodeTransformer)
torch.fx.experimental.rewriter.AST_Rewriter.rewrite(self,fn:FunctionType)
torch.fx.experimental.rewriter.AST_Rewriter.visit_AnnAssign(self,node)
torch.fx.experimental.rewriter.AST_Rewriter.visit_Assert(self,node)
torch.fx.experimental.rewriter.RewritingTracer(Tracer)
torch.fx.experimental.rewriter.RewritingTracer.trace(self,root:Union[torch.nn.Module,Callable],concrete_args:Optional[Dict[str,Any]]=None)->Graph
torch.fx.experimental.rewriter._rewrite(fn:Union[torch.nn.Module,Callable])->Union[torch.nn.Module, Callable]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/accelerator_partitioner.py----------------------------------------
A:torch.fx.experimental.accelerator_partitioner.node->DAGNode(submodule_node, input_nodes, output_nodes, logical_devices, size_bytes)
A:torch.fx.experimental.accelerator_partitioner.partition->Partition(partition_id)
A:torch.fx.experimental.accelerator_partitioner.partition.nodes->self.create_partition().nodes.union(partition_1.nodes)
A:torch.fx.experimental.accelerator_partitioner.partition.children->set()
A:torch.fx.experimental.accelerator_partitioner.partition.parents->set()
A:torch.fx.experimental.accelerator_partitioner.current_level->set().copy()
A:torch.fx.experimental.accelerator_partitioner.next_level->set()
A:torch.fx.experimental.accelerator_partitioner.logical_id_to_device->get_logical_id_to_device(devices)
A:torch.fx.experimental.accelerator_partitioner.all_nodes->all_nodes.union(partition.nodes).union(partition.nodes)
A:torch.fx.experimental.accelerator_partitioner.extra_size_needed->calculate_extra_mem_bytes_needed_for(partition, device_to_partitions[d])
A:torch.fx.experimental.accelerator_partitioner.(device_to_partitions, device_to_left_mem_bytes, no_device_partitions)->get_device_partition_stats(self.partitions, self.devices)
A:torch.fx.experimental.accelerator_partitioner.found_device->get_device_to_partitions_mapping(self.partitions, self.devices)
A:torch.fx.experimental.accelerator_partitioner.p->queue.pop(0)
A:torch.fx.experimental.accelerator_partitioner.device_with_max_mem->max(self.devices, key=lambda d: d.available_mem_bytes)
A:torch.fx.experimental.accelerator_partitioner.module_with_submodules->split_module(self.graph_module, self.torch_module, lambda node: self.node_to_partition[node])
A:torch.fx.experimental.accelerator_partitioner.dag->DAG()
A:torch.fx.experimental.accelerator_partitioner.ret->PartitionResult(dag, module_with_submodules)
A:torch.fx.experimental.accelerator_partitioner.partition_0->self.create_partition()
A:torch.fx.experimental.accelerator_partitioner.self.node_to_partition->get_node_to_partition_mapping(self.partitions)
A:torch.fx.experimental.accelerator_partitioner.mem_size_needed->get_extra_size_of(node, set())
A:torch.fx.experimental.accelerator_partitioner.device->find_device_based_on_size(node)
A:torch.fx.experimental.accelerator_partitioner.total_size_of_input_nodes->get_extra_size_of(node, partition.nodes)
A:torch.fx.experimental.accelerator_partitioner.found_partition_to_device_mapping->get_device_to_partitions_mapping(self.partitions, self.devices)
A:torch.fx.experimental.accelerator_partitioner.new_device->min(available_devices, key=lambda d: d.available_mem_bytes)
A:torch.fx.experimental.accelerator_partitioner.output_nodes->list(node.users)
A:torch.fx.experimental.accelerator_partitioner.partition_id->len(self.partitions)
A:torch.fx.experimental.accelerator_partitioner.sorted_partitions->sorted(partitions, key=lambda p: p.used_mem_bytes)
A:torch.fx.experimental.accelerator_partitioner.(find_combination, partitions)->find_partition_to_combine_based_on_size(sorted_partitions, available_mem_bytes, partitions)
A:torch.fx.experimental.accelerator_partitioner.nodes->p1.nodes.union(p2.nodes)
A:torch.fx.experimental.accelerator_partitioner.smallest_partition->sorted(partitions, key=lambda p: p.used_mem_bytes).pop(0)
A:torch.fx.experimental.accelerator_partitioner.mem_bytes_needed->calculate_mem_bytes_needed(p, smallest_partition)
A:torch.fx.experimental.accelerator_partitioner.submodule->getattr(submodule, atom)
A:torch.fx.experimental.accelerator_partitioner.found_deivce->get_device_to_partitions_mapping(partitions, self.devices)
A:torch.fx.experimental.accelerator_partitioner.partition_to_latency_mapping->get_partition_to_latency_mapping(self.partitions, node_to_latency_mapping)
A:torch.fx.experimental.accelerator_partitioner.cost->get_latency_of_partitioned_graph(self.partitions, partition_to_latency_mapping, transfer_rate_bytes_per_sec)
A:torch.fx.experimental.accelerator_partitioner.new_cost->try_combining_partitions(i, j, self.partitions[:])
A:torch.fx.experimental.accelerator_partitioner.find_combination->search_combination(transfer_rate_bytes_per_sec, node_to_latency_mapping)
A:torch.fx.experimental.accelerator_partitioner.min_cost->float('inf')
A:torch.fx.experimental.accelerator_partitioner.(new_cost, new_node_pair)->swap_node_to_partition(node, p0, p1, node_to_latency_mapping, transfer_rate_bytes_per_sec)
torch.fx.experimental.accelerator_partitioner.DAG(self)
torch.fx.experimental.accelerator_partitioner.DAG.__init__(self)
torch.fx.experimental.accelerator_partitioner.DAG.create_node(self,submodule_node:Node,input_nodes:List[Node],output_nodes:List[Node],logical_devices:List[int],size_bytes:int)->None
torch.fx.experimental.accelerator_partitioner.DAGNode(self,submodule_node:Node,input_nodes:List[Node],output_nodes:List[Node],logical_device_ids:List[int],size_bytes:int)
torch.fx.experimental.accelerator_partitioner.DAGNode.__init__(self,submodule_node:Node,input_nodes:List[Node],output_nodes:List[Node],logical_device_ids:List[int],size_bytes:int)
torch.fx.experimental.accelerator_partitioner.DAGNode.__str__(self)->str
torch.fx.experimental.accelerator_partitioner.PartitionResult(NamedTuple)
torch.fx.experimental.accelerator_partitioner.Partitioner(self)
torch.fx.experimental.accelerator_partitioner.Partitioner.__init__(self)
torch.fx.experimental.accelerator_partitioner.Partitioner.aot_based_partition(self,node_to_partition_mapping,partition_to_logical_device_mapping)
torch.fx.experimental.accelerator_partitioner.Partitioner.cost_aware_partition(self,transfer_rate_bytes_per_sec:float,node_to_latency_mapping:Dict[Node,NodeLatency])->None
torch.fx.experimental.accelerator_partitioner.Partitioner.create_partition(self)->Partition
torch.fx.experimental.accelerator_partitioner.Partitioner.create_single_node_partition(self,node)
torch.fx.experimental.accelerator_partitioner.Partitioner.do_partition(self)->GraphModule
torch.fx.experimental.accelerator_partitioner.Partitioner.dump_dag(self,module_with_submodules:GraphModule)->DAG
torch.fx.experimental.accelerator_partitioner.Partitioner.find_single_partition(self,total_size_of_graph,logical_device_id:int=0)->None
torch.fx.experimental.accelerator_partitioner.Partitioner.kl_based_partition(self,transfer_rate_bytes_per_sec:float,node_to_latency_mapping:Dict[Node,NodeLatency])->None
torch.fx.experimental.accelerator_partitioner.Partitioner.partition_graph(self,fx_module:GraphModule,torch_module:torch.nn.Module,partitioner_config:PartitionerConfig)->PartitionResult
torch.fx.experimental.accelerator_partitioner.Partitioner.saturate_host(self)->None
torch.fx.experimental.accelerator_partitioner.Partitioner.size_based_partition(self)->None
torch.fx.experimental.accelerator_partitioner.Partitioner.sparse_nn_partition(self,available_mem_bytes:int)->None
torch.fx.experimental.accelerator_partitioner.check_dependency(partition)
torch.fx.experimental.accelerator_partitioner.combine_two_partitions(partition_0:Partition,partition_1:Partition,partitions:List[Partition])->None
torch.fx.experimental.accelerator_partitioner.get_bfs_level_partition(partitions:List[Partition])->None
torch.fx.experimental.accelerator_partitioner.get_device_partition_stats(partitions:List[Partition],devices:List[Device])->Tuple[Dict[Device, List[Partition]], Dict[Device, int], List[Partition]]
torch.fx.experimental.accelerator_partitioner.get_device_to_partitions_mapping(partitions:List[Partition],devices:List[Device])
torch.fx.experimental.accelerator_partitioner.get_logical_id_to_device(devices:List[Device])->Dict[int, Device]
torch.fx.experimental.accelerator_partitioner.get_node_to_partition_mapping(partitions:List[Partition])->Dict[Node, int]
torch.fx.experimental.accelerator_partitioner.reorganize_partitions(partitions:List[Partition])->None
torch.fx.experimental.accelerator_partitioner.reset_partition_device(partitions)
torch.fx.experimental.accelerator_partitioner.set_parents_and_children(partitions:List[Partition])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/refinement_types.py----------------------------------------
torch.fx.experimental.refinement_types.Equality(self,lhs,rhs)
torch.fx.experimental.refinement_types.Equality.__eq__(self,other)
torch.fx.experimental.refinement_types.Equality.__init__(self,lhs,rhs)
torch.fx.experimental.refinement_types.Equality.__repr__(self)
torch.fx.experimental.refinement_types.Equality.__str__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/utils.py----------------------------------------
A:torch.fx.experimental.unification.utils.incoming_edges->dict(((k, set(val)) for (k, val) in incoming_edges.items()))
A:torch.fx.experimental.unification.utils.S->set((v for v in edges if v not in incoming_edges))
A:torch.fx.experimental.unification.utils.n->set((v for v in edges if v not in incoming_edges)).pop()
torch.fx.experimental.unification.utils._toposort(edges)
torch.fx.experimental.unification.utils.freeze(d)
torch.fx.experimental.unification.utils.hashable(x)
torch.fx.experimental.unification.utils.raises(err,lamda)
torch.fx.experimental.unification.utils.reverse_dict(d)
torch.fx.experimental.unification.utils.transitive_get(key,d)
torch.fx.experimental.unification.utils.xfail(func)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/core.py----------------------------------------
A:torch.fx.experimental.unification.core.s->unify(uu, vv, s)
A:torch.fx.experimental.unification.core.u->walk(u, s)
A:torch.fx.experimental.unification.core.v->walk(v, s)
torch.fx.experimental.unification.core._reify(d,s)
torch.fx.experimental.unification.core._reify(o,s)
torch.fx.experimental.unification.core._reify(t,s)
torch.fx.experimental.unification.core._reify(t,s)
torch.fx.experimental.unification.core._reify(t,s)
torch.fx.experimental.unification.core._unify(u,v,s)
torch.fx.experimental.unification.core.reify(e,s)
torch.fx.experimental.unification.core.unify(u,v)
torch.fx.experimental.unification.core.unify(u,v,s)
torch.fx.experimental.unification.reify(e,s)
torch.fx.experimental.unification.unify(u,v)
torch.fx.experimental.unification.unify(u,v,s)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/variable.py----------------------------------------
A:torch.fx.experimental.unification.variable._global_logic_variables->set()
A:torch.fx.experimental.unification.variable.obj->object.__new__(cls)
A:torch.fx.experimental.unification.variable.old_global_logic_variables->set().copy()
torch.fx.experimental.unification.Var(cls,*token)
torch.fx.experimental.unification.Var.__eq__(self,other)
torch.fx.experimental.unification.Var.__hash__(self)
torch.fx.experimental.unification.Var.__str__(self)
torch.fx.experimental.unification.isvar(o)
torch.fx.experimental.unification.isvar(v)
torch.fx.experimental.unification.var()
torch.fx.experimental.unification.variable.Var(cls,*token)
torch.fx.experimental.unification.variable.Var.__eq__(self,other)
torch.fx.experimental.unification.variable.Var.__hash__(self)
torch.fx.experimental.unification.variable.Var.__new__(cls,*token)
torch.fx.experimental.unification.variable.Var.__str__(self)
torch.fx.experimental.unification.variable.isvar(o)
torch.fx.experimental.unification.variable.isvar(v)
torch.fx.experimental.unification.variable.var()
torch.fx.experimental.unification.variable.variables(*variables)
torch.fx.experimental.unification.variable.vars()
torch.fx.experimental.unification.variables(*variables)
torch.fx.experimental.unification.vars()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/unification_tools.py----------------------------------------
A:torch.fx.experimental.unification.unification_tools.factory->_get_factory(dissoc, kwargs)
A:torch.fx.experimental.unification.unification_tools.rv->factory()
A:torch.fx.experimental.unification.unification_tools.result->factory()
A:torch.fx.experimental.unification.unification_tools.d2->factory()
A:torch.fx.experimental.unification.unification_tools.remaining->set(d)
A:torch.fx.experimental.unification.unification_tools.ks->iter(keys)
A:torch.fx.experimental.unification.unification_tools.k->next(ks)
A:torch.fx.experimental.unification.unification_tools.rvinner->factory()
A:torch.fx.experimental.unification.unification_tools.dtemp->factory()
A:torch.fx.experimental.unification.unification_tools.ddtemp->factory()
A:torch.fx.experimental.unification.unification_tools.inner[k]->func(default)
A:torch.fx.experimental.unification.unification_tools.key->getter(key)
A:torch.fx.experimental.unification.unification_tools.d->collections.defaultdict(lambda : [].append)
torch.fx.experimental.unification.unification_tools._get_factory(f,kwargs)
torch.fx.experimental.unification.unification_tools.assoc(d,key,value,factory=dict)
torch.fx.experimental.unification.unification_tools.assoc_in(d,keys,value,factory=dict)
torch.fx.experimental.unification.unification_tools.dissoc(d,*keys,**kwargs)
torch.fx.experimental.unification.unification_tools.first(seq)
torch.fx.experimental.unification.unification_tools.get_in(keys,coll,default=None,no_default=False)
torch.fx.experimental.unification.unification_tools.getter(index)
torch.fx.experimental.unification.unification_tools.groupby(key,seq)
torch.fx.experimental.unification.unification_tools.itemfilter(predicate,d,factory=dict)
torch.fx.experimental.unification.unification_tools.itemmap(func,d,factory=dict)
torch.fx.experimental.unification.unification_tools.keyfilter(predicate,d,factory=dict)
torch.fx.experimental.unification.unification_tools.keymap(func,d,factory=dict)
torch.fx.experimental.unification.unification_tools.merge(*dicts,**kwargs)
torch.fx.experimental.unification.unification_tools.merge_with(func,*dicts,**kwargs)
torch.fx.experimental.unification.unification_tools.update_in(d,keys,func,default=None,factory=dict)
torch.fx.experimental.unification.unification_tools.valfilter(predicate,d,factory=dict)
torch.fx.experimental.unification.unification_tools.valmap(func,d,factory=dict)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/more.py----------------------------------------
A:torch.fx.experimental.unification.more.obj->object.__new__(type(o))
A:torch.fx.experimental.unification.more.d->reify(o.__dict__, s)
A:torch.fx.experimental.unification.more.new_attrs->reify(attrs, s)
A:torch.fx.experimental.unification.more.newobj->object.__new__(type(o))
torch.fx.experimental.unification.more._reify(o,s)
torch.fx.experimental.unification.more._reify_object_dict(o,s)
torch.fx.experimental.unification.more._reify_object_slots(o,s)
torch.fx.experimental.unification.more._unify(u,v,s)
torch.fx.experimental.unification.more.reify_object(o,s)
torch.fx.experimental.unification.more.unifiable(cls)
torch.fx.experimental.unification.more.unify_object(u,v,s)
torch.fx.experimental.unification.unifiable(cls)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/dispatch.py----------------------------------------
A:torch.fx.experimental.unification.dispatch.namespace->dict()
A:torch.fx.experimental.unification.dispatch.dispatch->partial(dispatch, namespace=namespace)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/match.py----------------------------------------
A:torch.fx.experimental.unification.match.self.funcs->dict()
A:torch.fx.experimental.unification.match.self.ordering->ordering(self.funcs)
A:torch.fx.experimental.unification.match.(func, s)->self.resolve(args)
A:torch.fx.experimental.unification.match.n->len(args)
A:torch.fx.experimental.unification.match.s->dict(((k, v) for (k, v) in s.items() if not isvar(k) or not isvar(v)))
A:torch.fx.experimental.unification.match.d->dict(((k.token, v) for (k, v) in s.items()))
A:torch.fx.experimental.unification.match.global_namespace->dict()
A:torch.fx.experimental.unification.match.namespace->kwargs.get('namespace', global_namespace)
A:torch.fx.experimental.unification.match.dispatcher->kwargs.get('Dispatcher', Dispatcher)
A:torch.fx.experimental.unification.match.namespace[name]->dispatcher(name)
A:torch.fx.experimental.unification.match.signatures->list(map(tuple, signatures))
A:torch.fx.experimental.unification.match.edges->dict(((k, [b for (a, b) in v]) for (k, v) in edges.items()))
torch.fx.experimental.unification.match.Dispatcher(self,name)
torch.fx.experimental.unification.match.Dispatcher.__init__(self,name)
torch.fx.experimental.unification.match.Dispatcher.add(self,signature,func)
torch.fx.experimental.unification.match.Dispatcher.register(self,*signature)
torch.fx.experimental.unification.match.Dispatcher.resolve(self,args)
torch.fx.experimental.unification.match.VarDispatcher(self,*args,**kwargs)
torch.fx.experimental.unification.match.VarDispatcher.__call__(self,*args,**kwargs)
torch.fx.experimental.unification.match.edge(a,b,tie_breaker=hash)
torch.fx.experimental.unification.match.match(*signature,**kwargs)
torch.fx.experimental.unification.match.ordering(signatures)
torch.fx.experimental.unification.match.supercedes(a,b)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/multipledispatch/utils.py----------------------------------------
A:torch.fx.experimental.unification.multipledispatch.utils.rest->expand_tuples(L[1:])
A:torch.fx.experimental.unification.multipledispatch.utils.incoming_edges->OrderedDict(((k, set(val)) for (k, val) in incoming_edges.items()))
A:torch.fx.experimental.unification.multipledispatch.utils.S->collections.OrderedDict.fromkeys((v for v in edges if v not in incoming_edges))
A:torch.fx.experimental.unification.multipledispatch.utils.(n, _)->collections.OrderedDict.fromkeys((v for v in edges if v not in incoming_edges)).popitem()
A:torch.fx.experimental.unification.multipledispatch.utils.result->OrderedDict()
A:torch.fx.experimental.unification.multipledispatch.utils.d->OrderedDict()
A:torch.fx.experimental.unification.multipledispatch.utils.key->func(item)
A:torch.fx.experimental.unification.multipledispatch.utils.d[key]->list()
torch.fx.experimental.unification.multipledispatch.utils._toposort(edges)
torch.fx.experimental.unification.multipledispatch.utils.expand_tuples(L)
torch.fx.experimental.unification.multipledispatch.utils.groupby(func,seq)
torch.fx.experimental.unification.multipledispatch.utils.raises(err,lamda)
torch.fx.experimental.unification.multipledispatch.utils.reverse_dict(d)
torch.fx.experimental.unification.multipledispatch.utils.typename(type)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/multipledispatch/conflict.py----------------------------------------
A:torch.fx.experimental.unification.multipledispatch.conflict.signatures->list(map(tuple, signatures))
A:torch.fx.experimental.unification.multipledispatch.conflict.n->len(signatures[0])
A:torch.fx.experimental.unification.multipledispatch.conflict.edges->dict(((k, [b for (a, b) in v]) for (k, v) in edges.items()))
torch.fx.experimental.unification.multipledispatch.conflict.AmbiguityWarning(Warning)
torch.fx.experimental.unification.multipledispatch.conflict.ambiguities(signatures)
torch.fx.experimental.unification.multipledispatch.conflict.ambiguous(a,b)
torch.fx.experimental.unification.multipledispatch.conflict.consistent(a,b)
torch.fx.experimental.unification.multipledispatch.conflict.edge(a,b,tie_breaker=hash)
torch.fx.experimental.unification.multipledispatch.conflict.ordering(signatures)
torch.fx.experimental.unification.multipledispatch.conflict.super_signature(signatures)
torch.fx.experimental.unification.multipledispatch.conflict.supercedes(a,b)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/multipledispatch/core.py----------------------------------------
A:torch.fx.experimental.unification.multipledispatch.core.global_namespace->dict()
A:torch.fx.experimental.unification.multipledispatch.core.namespace->kwargs.get('namespace', global_namespace)
A:torch.fx.experimental.unification.multipledispatch.core.types->tuple(types)
A:torch.fx.experimental.unification.multipledispatch.core.dispatcher->inspect.currentframe().f_back.f_locals.get(name, MethodDispatcher(name))
A:torch.fx.experimental.unification.multipledispatch.core.namespace[name]->Dispatcher(name)
A:torch.fx.experimental.unification.multipledispatch.core.signature->inspect.signature(func)
A:torch.fx.experimental.unification.multipledispatch.core.spec->inspect.getfullargspec(func)
torch.fx.experimental.unification.multipledispatch.core.dispatch(*types,**kwargs)
torch.fx.experimental.unification.multipledispatch.core.ismethod(func)
torch.fx.experimental.unification.multipledispatch.dispatch(*types,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/multipledispatch/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/multipledispatch/variadic.py----------------------------------------
torch.fx.experimental.unification.multipledispatch.variadic.Variadic(six.with_metaclass(VariadicSignatureMeta))
torch.fx.experimental.unification.multipledispatch.variadic.VariadicSignatureMeta(type)
torch.fx.experimental.unification.multipledispatch.variadic.VariadicSignatureMeta.__getitem__(cls,variadic_type)
torch.fx.experimental.unification.multipledispatch.variadic.VariadicSignatureType(type)
torch.fx.experimental.unification.multipledispatch.variadic.VariadicSignatureType.__eq__(cls,other)
torch.fx.experimental.unification.multipledispatch.variadic.VariadicSignatureType.__hash__(cls)
torch.fx.experimental.unification.multipledispatch.variadic.VariadicSignatureType.__subclasscheck__(cls,subclass)
torch.fx.experimental.unification.multipledispatch.variadic.isvariadic(obj)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/unification/multipledispatch/dispatcher.py----------------------------------------
A:torch.fx.experimental.unification.multipledispatch.dispatcher.sigiter->iter(full_signature)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.sig->inspect.signature(func)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.matches->issubclass(typ, sig)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.params->cls.get_func_params(func)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.annotations->self.get_func_annotations(func)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.str_sig->', '.join((c.__name__ if isinstance(c, type) else str(c) for c in signature))
A:torch.fx.experimental.unification.multipledispatch.dispatcher.self._orderingod->ordering(self.funcs)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.amb->ambiguities(self.funcs)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.types->tuple([type(arg) for arg in args])
A:torch.fx.experimental.unification.multipledispatch.dispatcher.func->self.dispatch(*types)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.funcs->self.dispatch_iter(*types)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.n->len(types)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.self._ordering->ordering(self.funcs)
A:torch.fx.experimental.unification.multipledispatch.dispatcher.self._cache->dict()
torch.fx.experimental.unification.multipledispatch.Dispatcher(self,name,doc=None)
torch.fx.experimental.unification.multipledispatch.Dispatcher.__doc__(self)
torch.fx.experimental.unification.multipledispatch.Dispatcher.__getstate__(self)
torch.fx.experimental.unification.multipledispatch.Dispatcher.__setstate__(self,d)
torch.fx.experimental.unification.multipledispatch.Dispatcher.__str__(self)
torch.fx.experimental.unification.multipledispatch.Dispatcher._help(self,*args)
torch.fx.experimental.unification.multipledispatch.Dispatcher._source(self,*args)
torch.fx.experimental.unification.multipledispatch.Dispatcher.add(self,signature,func)
torch.fx.experimental.unification.multipledispatch.Dispatcher.dispatch(self,*types)
torch.fx.experimental.unification.multipledispatch.Dispatcher.dispatch_iter(self,*types)
torch.fx.experimental.unification.multipledispatch.Dispatcher.get_func_annotations(cls,func)
torch.fx.experimental.unification.multipledispatch.Dispatcher.get_func_params(cls,func)
torch.fx.experimental.unification.multipledispatch.Dispatcher.help(self,*args,**kwargs)
torch.fx.experimental.unification.multipledispatch.Dispatcher.ordering(self)
torch.fx.experimental.unification.multipledispatch.Dispatcher.register(self,*types,**kwargs)
torch.fx.experimental.unification.multipledispatch.Dispatcher.reorder(self,on_ambiguity=ambiguity_warn)
torch.fx.experimental.unification.multipledispatch.Dispatcher.resolve(self,types)
torch.fx.experimental.unification.multipledispatch.Dispatcher.source(self,*args,**kwargs)
torch.fx.experimental.unification.multipledispatch.MDNotImplementedError(NotImplementedError)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher(self,name,doc=None)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.__doc__(self)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.__getstate__(self)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.__init__(self,name,doc=None)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.__setstate__(self,d)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.__str__(self)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher._help(self,*args)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher._source(self,*args)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.add(self,signature,func)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.dispatch(self,*types)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.dispatch_iter(self,*types)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.get_func_annotations(cls,func)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.get_func_params(cls,func)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.help(self,*args,**kwargs)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.ordering(self)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.register(self,*types,**kwargs)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.reorder(self,on_ambiguity=ambiguity_warn)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.resolve(self,types)
torch.fx.experimental.unification.multipledispatch.dispatcher.Dispatcher.source(self,*args,**kwargs)
torch.fx.experimental.unification.multipledispatch.dispatcher.MDNotImplementedError(NotImplementedError)
torch.fx.experimental.unification.multipledispatch.dispatcher.MethodDispatcher(self,*args,**kwargs)
torch.fx.experimental.unification.multipledispatch.dispatcher.MethodDispatcher.__call__(self,*args,**kwargs)
torch.fx.experimental.unification.multipledispatch.dispatcher.MethodDispatcher.__get__(self,instance,owner)
torch.fx.experimental.unification.multipledispatch.dispatcher.MethodDispatcher.get_func_params(cls,func)
torch.fx.experimental.unification.multipledispatch.dispatcher.ambiguity_warn(dispatcher,ambiguities)
torch.fx.experimental.unification.multipledispatch.dispatcher.halt_ordering()
torch.fx.experimental.unification.multipledispatch.dispatcher.restart_ordering(on_ambiguity=ambiguity_warn)
torch.fx.experimental.unification.multipledispatch.dispatcher.source(func)
torch.fx.experimental.unification.multipledispatch.dispatcher.str_signature(sig)
torch.fx.experimental.unification.multipledispatch.dispatcher.variadic_signature_matches(types,full_signature)
torch.fx.experimental.unification.multipledispatch.dispatcher.variadic_signature_matches_iter(types,full_signature)
torch.fx.experimental.unification.multipledispatch.dispatcher.warning_text(name,amb)
torch.fx.experimental.unification.multipledispatch.halt_ordering()
torch.fx.experimental.unification.multipledispatch.restart_ordering(on_ambiguity=ambiguity_warn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/types.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/trt_module.py----------------------------------------
A:torch.fx.experimental.fx2trt.trt_module.self.context->self.engine.create_execution_context()
A:torch.fx.experimental.fx2trt.trt_module.primary_input_outputs->set()
A:torch.fx.experimental.fx2trt.trt_module.state_dict[prefix + 'engine']->bytearray(self.engine.serialize())
A:torch.fx.experimental.fx2trt.trt_module.logger->tensorrt.Logger()
A:torch.fx.experimental.fx2trt.trt_module.runtime->tensorrt.Runtime(logger)
A:torch.fx.experimental.fx2trt.trt_module.self.engine->tensorrt.Runtime(logger).deserialize_cuda_engine(engine_bytes)
A:torch.fx.experimental.fx2trt.trt_module.state->self.__dict__.copy()
A:torch.fx.experimental.fx2trt.trt_module.state['engine']->tensorrt.Runtime(logger).deserialize_cuda_engine(state['engine'])
A:torch.fx.experimental.fx2trt.trt_module.bindings[idx]->torch.empty(size=shape, dtype=self.hidden_output_dtypes[i], device=torch.cuda.current_device()).data_ptr()
A:torch.fx.experimental.fx2trt.trt_module.shape->tuple(self.context.get_binding_shape(idx))
A:torch.fx.experimental.fx2trt.trt_module.output->torch.empty(size=shape, dtype=self.hidden_output_dtypes[i], device=torch.cuda.current_device())
A:torch.fx.experimental.fx2trt.trt_module.self.context.profiler->tensorrt.Profiler()
torch.fx.experimental.fx2trt.TRTModule(self,engine=None,input_names=None,output_names=None,cuda_graph_batch_size=-1)
torch.fx.experimental.fx2trt.TRTModule.__getstate__(self)
torch.fx.experimental.fx2trt.TRTModule.__setstate__(self,state)
torch.fx.experimental.fx2trt.TRTModule._check_initialized(self)
torch.fx.experimental.fx2trt.TRTModule._initialize(self)
torch.fx.experimental.fx2trt.TRTModule._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.fx.experimental.fx2trt.TRTModule._on_state_dict(self,state_dict,prefix,local_metadata)
torch.fx.experimental.fx2trt.TRTModule.disable_profiling(self)
torch.fx.experimental.fx2trt.TRTModule.enable_profiling(self)
torch.fx.experimental.fx2trt.TRTModule.forward(self,*inputs)
torch.fx.experimental.fx2trt.trt_module.TRTModule(self,engine=None,input_names=None,output_names=None,cuda_graph_batch_size=-1)
torch.fx.experimental.fx2trt.trt_module.TRTModule.__getstate__(self)
torch.fx.experimental.fx2trt.trt_module.TRTModule.__init__(self,engine=None,input_names=None,output_names=None,cuda_graph_batch_size=-1)
torch.fx.experimental.fx2trt.trt_module.TRTModule.__setstate__(self,state)
torch.fx.experimental.fx2trt.trt_module.TRTModule._check_initialized(self)
torch.fx.experimental.fx2trt.trt_module.TRTModule._initialize(self)
torch.fx.experimental.fx2trt.trt_module.TRTModule._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.fx.experimental.fx2trt.trt_module.TRTModule._on_state_dict(self,state_dict,prefix,local_metadata)
torch.fx.experimental.fx2trt.trt_module.TRTModule.disable_profiling(self)
torch.fx.experimental.fx2trt.trt_module.TRTModule.enable_profiling(self)
torch.fx.experimental.fx2trt.trt_module.TRTModule.forward(self,*inputs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/lower.py----------------------------------------
A:torch.fx.experimental.fx2trt.lower.logger->logging.getLogger(__name__)
A:torch.fx.experimental.fx2trt.lower.TModule->typing.TypeVar('TModule', bound=nn.Module)
A:torch.fx.experimental.fx2trt.lower.lower_setting->LowerSetting()
A:torch.fx.experimental.fx2trt.lower.lowerer->Lowerer.create(lower_setting=lower_setting)
A:torch.fx.experimental.fx2trt.lower.timing_cache_manager->TimingCacheManager(lower_setting.timing_cache_prefix, lower_setting.save_timing_cache)
A:torch.fx.experimental.fx2trt.lower.mod->fuse_unsqueeze_cat_sum(mod)
A:torch.fx.experimental.fx2trt.lower.algo_selector->self.lower_setting.algo_selector(f'{split_name}.json')
A:torch.fx.experimental.fx2trt.lower.cache_data->self.timing_cache_manager.get_timing_cache_trt(split_name)
A:torch.fx.experimental.fx2trt.lower.interpreter->TRTInterpreter(mod, input_specs=input_specs_val, explicit_batch_dimension=self.lower_setting.explicit_batch_dimension, explicit_precision=self.lower_setting.explicit_precision, logger_level=trt.Logger.VERBOSE if self.lower_setting.verbose_log else trt.Logger.WARNING)
A:torch.fx.experimental.fx2trt.lower.interp_result->TRTInterpreter(mod, input_specs=input_specs_val, explicit_batch_dimension=self.lower_setting.explicit_batch_dimension, explicit_precision=self.lower_setting.explicit_precision, logger_level=trt.Logger.VERBOSE if self.lower_setting.verbose_log else trt.Logger.WARNING).run(max_batch_size=self.lower_setting.max_batch_size, max_workspace_size=self.lower_setting.max_workspace_size, fp16_mode=self.lower_setting.fp16_mode, int8_mode=self.lower_setting.int8_mode, strict_type_constraints=self.lower_setting.strict_type_constraints, algorithm_selector=algo_selector, timing_cache=cache_data, profiling_verbosity=trt.ProfilingVerbosity.DETAILED)
A:torch.fx.experimental.fx2trt.lower.lower->Lowerer.create(lower_setting=lower_setting)
A:torch.fx.experimental.fx2trt.lower.module_lowered->lower(module, sample_input)
A:torch.fx.experimental.fx2trt.lower.input->tuple((x.half() if x.dtype == torch.float32 else x for x in input))
A:torch.fx.experimental.fx2trt.lower.module->self.acc_trace(module, input)
A:torch.fx.experimental.fx2trt.lower.const_split_mod->self.acc_trace(const_split_mod, input)
A:torch.fx.experimental.fx2trt.lower.(split_module, splits)->self.split(const_split_mod, input)
A:torch.fx.experimental.fx2trt.lower.interp_res->self.trt_interpreter(_split.module, _split.input, _split.name)
A:torch.fx.experimental.fx2trt.lower.trt_module->TRTModule(engine=interp_res.engine, input_names=interp_res.input_names, output_names=interp_res.output_names, cuda_graph_batch_size=cuda_graph_batch_size)
torch.fx.experimental.fx2trt.LowerSetting
torch.fx.experimental.fx2trt.Lowerer(self,module:nn.Module,input:Input,cuda_graph_batch_size:int=-1,skip_folding_node_fn:t.Optional[t.Callable[[fx.Node],bool]]=None)
torch.fx.experimental.fx2trt.Lowerer.create(cls,lower_setting:LowerSetting,trt_module_observer:Optional[Callable[[str,nn.Module,List[torch.Tensor]],None]]=None)->'Lowerer'
torch.fx.experimental.fx2trt.lower.LowerFunc(self,module:fx.GraphModule,input:Input)
torch.fx.experimental.fx2trt.lower.LowerFunc.__call__(self,module:fx.GraphModule,input:Input)
torch.fx.experimental.fx2trt.lower.LowerSetting
torch.fx.experimental.fx2trt.lower.LowerTrtInterpreter(self,mod,input,split_name)
torch.fx.experimental.fx2trt.lower.LowerTrtInterpreter.__call__(self,mod,input,split_name)
torch.fx.experimental.fx2trt.lower.LowerTrtInterpreter.create(cls,lower_setting)
torch.fx.experimental.fx2trt.lower.Lowerer(self,module:nn.Module,input:Input,cuda_graph_batch_size:int=-1,skip_folding_node_fn:t.Optional[t.Callable[[fx.Node],bool]]=None)
torch.fx.experimental.fx2trt.lower.Lowerer.__call__(self,module:nn.Module,input:Input,cuda_graph_batch_size:int=-1,skip_folding_node_fn:t.Optional[t.Callable[[fx.Node],bool]]=None)
torch.fx.experimental.fx2trt.lower.Lowerer.create(cls,lower_setting:LowerSetting,trt_module_observer:Optional[Callable[[str,nn.Module,List[torch.Tensor]],None]]=None)->'Lowerer'
torch.fx.experimental.fx2trt.lower.fx2trt_lower(module:nn.Module,sample_input:t.Any)->fx.GraphModule
torch.fx.experimental.fx2trt.lower.lower_to_trt(module:nn.Module,input,max_batch_size:int=2048,max_workspace_size=1<<25,explicit_batch_dimension=False,fp16_mode=True,enable_fuse=True,verbose_log=False,timing_cache_prefix='',save_timing_cache=False,cuda_graph_batch_size=-1)->nn.Module
torch.fx.experimental.fx2trt.lower_to_trt(module:nn.Module,input,max_batch_size:int=2048,max_workspace_size=1<<25,explicit_batch_dimension=False,fp16_mode=True,enable_fuse=True,verbose_log=False,timing_cache_prefix='',save_timing_cache=False,cuda_graph_batch_size=-1)->nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/utils.py----------------------------------------
torch.fx.experimental.fx2trt.utils.get_dynamic_dims(shape:Shape)->List[int]
torch.fx.experimental.fx2trt.utils.torch_dtype_from_trt(dtype:TRTDataType)->torch.dtype
torch.fx.experimental.fx2trt.utils.torch_dtype_to_trt(dtype:torch.dtype)->TRTDataType


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/split.py----------------------------------------
A:torch.fx.experimental.fx2trt.split.logger->logging.getLogger(__name__)
A:torch.fx.experimental.fx2trt.split.trt_split_result->self._trt_split(module, input)
A:torch.fx.experimental.fx2trt.split.(device, order)->cls._parse_splitter_subgraph_name(name)
A:torch.fx.experimental.fx2trt.split.input->getattr(graph, cls._INPUT_ATTR)
A:torch.fx.experimental.fx2trt.split.match->re.match('_run_on_([a-z]+)_([0-9]+)', name)
A:torch.fx.experimental.fx2trt.split.splitter_settings->TRTSplitterSetting()
A:torch.fx.experimental.fx2trt.split.splitter->TRTSplitter(graph, input, self.operator_supported, settings=splitter_settings)
torch.fx.experimental.fx2trt.split.SplitFunc(self,module:fx.GraphModule,input:Input)
torch.fx.experimental.fx2trt.split.SplitFunc.__call__(self,module:fx.GraphModule,input:Input)
torch.fx.experimental.fx2trt.split.SplitInfo
torch.fx.experimental.fx2trt.split.Splitter(self,module,input)
torch.fx.experimental.fx2trt.split.Splitter.__call__(self,module,input)
torch.fx.experimental.fx2trt.split.Splitter._create_split_info(cls,name,graph,parent)->SplitInfo
torch.fx.experimental.fx2trt.split.Splitter._parse_splitter_subgraph_name(cls,name:str)->t.Tuple[str, int]
torch.fx.experimental.fx2trt.split.Splitter._propagate_split_inputs(cls,graph:fx.GraphModule,input:Input,target_modules:t.Collection[str])->None
torch.fx.experimental.fx2trt.split.Splitter._trt_split(self,graph:fx.GraphModule,input:Input)->fx.GraphModule
torch.fx.experimental.fx2trt.split.Splitter.create(cls,use_implicit_batch_dim:bool,min_acc_module_size:int=20)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/input_tensor_spec.py----------------------------------------
A:torch.fx.experimental.fx2trt.input_tensor_spec.batch_size->tensors[0].size(0)
A:torch.fx.experimental.fx2trt.input_tensor_spec.shape->tuple(self.shape_ranges[0][1])
torch.fx.experimental.fx2trt.InputTensorSpec(NamedTuple)
torch.fx.experimental.fx2trt.InputTensorSpec.create_inputs_from_specs(input_specs:Iterable['InputTensorSpec'])
torch.fx.experimental.fx2trt.InputTensorSpec.from_tensor(cls,tensor:torch.Tensor)->'InputTensorSpec'
torch.fx.experimental.fx2trt.InputTensorSpec.from_tensors(cls,tensors:Iterable[torch.Tensor])->List['InputTensorSpec']
torch.fx.experimental.fx2trt.InputTensorSpec.from_tensors_with_dynamic_batch_size(cls,tensors:Sequence[torch.Tensor],batch_size_range:Tuple[int,int,int])->List['InputTensorSpec']
torch.fx.experimental.fx2trt.InputTensorSpec.to_random_tensor(self)
torch.fx.experimental.fx2trt.input_tensor_spec.InputTensorSpec(NamedTuple)
torch.fx.experimental.fx2trt.input_tensor_spec.InputTensorSpec.create_inputs_from_specs(input_specs:Iterable['InputTensorSpec'])
torch.fx.experimental.fx2trt.input_tensor_spec.InputTensorSpec.from_tensor(cls,tensor:torch.Tensor)->'InputTensorSpec'
torch.fx.experimental.fx2trt.input_tensor_spec.InputTensorSpec.from_tensors(cls,tensors:Iterable[torch.Tensor])->List['InputTensorSpec']
torch.fx.experimental.fx2trt.input_tensor_spec.InputTensorSpec.from_tensors_with_dynamic_batch_size(cls,tensors:Sequence[torch.Tensor],batch_size_range:Tuple[int,int,int])->List['InputTensorSpec']
torch.fx.experimental.fx2trt.input_tensor_spec.InputTensorSpec.to_random_tensor(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/fx2trt.py----------------------------------------
A:torch.fx.experimental.fx2trt.fx2trt.self.logger->tensorrt.Logger(logger_level or trt.Logger.WARNING)
A:torch.fx.experimental.fx2trt.fx2trt.self.builder->tensorrt.Builder(self.logger)
A:torch.fx.experimental.fx2trt.fx2trt.self.network->self.builder.create_network(flag)
A:torch.fx.experimental.fx2trt.fx2trt.missing_ops->self.validate_conversion()
A:torch.fx.experimental.fx2trt.fx2trt.dynamic_dims->get_dynamic_dims(shape)
A:torch.fx.experimental.fx2trt.fx2trt.missing_converter->set()
A:torch.fx.experimental.fx2trt.fx2trt.submod->self.fetch_attr(target)
A:torch.fx.experimental.fx2trt.fx2trt.submod_type->getattr(submod, '_base_class_origin', type(submod))
A:torch.fx.experimental.fx2trt.fx2trt.builder_config->self.builder.create_builder_config()
A:torch.fx.experimental.fx2trt.fx2trt.cache_file->numpy.array(timing_cache)
A:torch.fx.experimental.fx2trt.fx2trt.cache->self.builder.create_builder_config().create_timing_cache(b'')
A:torch.fx.experimental.fx2trt.fx2trt.engine->self.builder.build_engine(self.network, builder_config)
A:torch.fx.experimental.fx2trt.fx2trt.self._cur_node_name->str(n)
A:torch.fx.experimental.fx2trt.fx2trt.kwargs->dict(n.kwargs)
A:torch.fx.experimental.fx2trt.fx2trt.trt_node->super().run_node(n)
A:torch.fx.experimental.fx2trt.fx2trt.self._itensor_to_tensor_meta[trt_node]->n.meta.get('tensor_meta')
A:torch.fx.experimental.fx2trt.fx2trt.converter->converter_registry.CONVERTERS.get(target)
torch.fx.experimental.fx2trt.TRTInterpreter(self,module:torch.fx.GraphModule,input_specs:List[InputTensorSpec],explicit_batch_dimension:bool=False,explicit_precision:bool=False,logger_level=None)
torch.fx.experimental.fx2trt.TRTInterpreter.call_function(self,target,args,kwargs)
torch.fx.experimental.fx2trt.TRTInterpreter.call_method(self,target,args,kwargs)
torch.fx.experimental.fx2trt.TRTInterpreter.call_module(self,target,args,kwargs)
torch.fx.experimental.fx2trt.TRTInterpreter.output(self,target,args,kwargs)
torch.fx.experimental.fx2trt.TRTInterpreter.placeholder(self,target,args,kwargs)
torch.fx.experimental.fx2trt.TRTInterpreter.run(self,max_batch_size=64,max_workspace_size=1<<25,fp16_mode=True,int8_mode=False,sparse_weights=False,force_fp32_output=False,strict_type_constraints=False,algorithm_selector=None,timing_cache=None,profiling_verbosity=None)->TRTInterpreterResult
torch.fx.experimental.fx2trt.TRTInterpreter.run_node(self,n)
torch.fx.experimental.fx2trt.TRTInterpreter.validate_conversion(self)
torch.fx.experimental.fx2trt.TRTInterpreter.validate_input_specs(self)
torch.fx.experimental.fx2trt.TRTInterpreterResult(NamedTuple)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter(self,module:torch.fx.GraphModule,input_specs:List[InputTensorSpec],explicit_batch_dimension:bool=False,explicit_precision:bool=False,logger_level=None)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.__init__(self,module:torch.fx.GraphModule,input_specs:List[InputTensorSpec],explicit_batch_dimension:bool=False,explicit_precision:bool=False,logger_level=None)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.call_function(self,target,args,kwargs)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.call_method(self,target,args,kwargs)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.call_module(self,target,args,kwargs)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.output(self,target,args,kwargs)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.placeholder(self,target,args,kwargs)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.run(self,max_batch_size=64,max_workspace_size=1<<25,fp16_mode=True,int8_mode=False,sparse_weights=False,force_fp32_output=False,strict_type_constraints=False,algorithm_selector=None,timing_cache=None,profiling_verbosity=None)->TRTInterpreterResult
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.run_node(self,n)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.validate_conversion(self)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreter.validate_input_specs(self)
torch.fx.experimental.fx2trt.fx2trt.TRTInterpreterResult(NamedTuple)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converter_registry.py----------------------------------------
torch.fx.experimental.fx2trt.converter_registry.tensorrt_converter(key:Target,no_implicit_batch_dim:bool=False,no_explicit_batch_dim:bool=False,enabled:bool=True)->Callable[[Any], Any]
torch.fx.experimental.fx2trt.tensorrt_converter(key:Target,no_implicit_batch_dim:bool=False,no_explicit_batch_dim:bool=False,enabled:bool=True)->Callable[[Any], Any]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/transformation.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.transformation.layer->network.add_shuffle(input_val)
A:torch.fx.experimental.fx2trt.converters.transformation.layer.reshape_dims->tuple(new_shape)
torch.fx.experimental.fx2trt.converters.transformation.torch_flatten(network,target,args,kwargs,name)
torch.fx.experimental.fx2trt.torch_flatten(network,target,args,kwargs,name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/quantization.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.quantization.(input_val, scale, zero_point, dtype)->get_inputs_from_args_and_kwargs(args, kwargs, quantize_per_tensor_inputs)
A:torch.fx.experimental.fx2trt.converters.quantization.input_val.dynamic_range->get_dyn_range(scale, zero_point, dtype)
torch.fx.experimental.fx2trt.converters.quantization.dequantize(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.quantization.identity(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.quantization.quantize(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.dequantize(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.identity(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantize(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/activation.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.activation.layer->network.add_activation(input=input_val, type=activation_type)
A:torch.fx.experimental.fx2trt.converters.activation.dyn_range->activation_dyn_range_fn(input_val.dynamic_range)
torch.fx.experimental.fx2trt.common_activation(network,mod,input_val,activation_type,activation_dyn_range_fn,layer_name)
torch.fx.experimental.fx2trt.converters.activation.common_activation(network,mod,input_val,activation_type,activation_dyn_range_fn,layer_name)
torch.fx.experimental.fx2trt.converters.activation.relu(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.activation.sigmoid(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.relu(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.sigmoid(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/converter_utils.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.converter_utils.plugin_registry->tensorrt.get_plugin_registry()
A:torch.fx.experimental.fx2trt.converters.converter_utils.plugin_creator->tensorrt.get_plugin_registry().get_plugin_creator(plugin_name, version, plugin_namespace)
A:torch.fx.experimental.fx2trt.converters.converter_utils.plugin->tensorrt.get_plugin_registry().get_plugin_creator(plugin_name, version, plugin_namespace).create_plugin(name=plugin_name, field_collection=field_collection)
A:torch.fx.experimental.fx2trt.converters.converter_utils.val->getattr(mod, name)
A:torch.fx.experimental.fx2trt.converters.converter_utils.tensor->tensor.dequantize().dequantize()
A:torch.fx.experimental.fx2trt.converters.converter_utils.value->value.to(dtype).to(dtype)
A:torch.fx.experimental.fx2trt.converters.converter_utils.constant->network.add_constant(value.shape, to_numpy(value))
A:torch.fx.experimental.fx2trt.converters.converter_utils.layer->network.add_reduce(input_val, operation_type, get_axes_for_reduce_op(dim, network.has_implicit_batch_dimension), keepdim)
A:torch.fx.experimental.fx2trt.converters.converter_utils.tensor_shape_layer->network.add_shape(tensor)
A:torch.fx.experimental.fx2trt.converters.converter_utils.prepend_shape_layer->network.add_constant((num_prepend_ones,), np.ones((num_prepend_ones,), dtype=np.int32))
A:torch.fx.experimental.fx2trt.converters.converter_utils.reshape_dim_layer->network.add_concatenation([prepend_shape_layer.get_output(0), tensor_shape_layer.get_output(0)])
A:torch.fx.experimental.fx2trt.converters.converter_utils.a_shape->tuple(a.shape)
A:torch.fx.experimental.fx2trt.converters.converter_utils.b_shape->tuple(b.shape)
A:torch.fx.experimental.fx2trt.converters.converter_utils.b->prepend_ones(network, b, f'{b_name}_broadcast', diff)
A:torch.fx.experimental.fx2trt.converters.converter_utils.a->prepend_ones(network, a, f'{a_name}_broadcast', -diff)
A:torch.fx.experimental.fx2trt.converters.converter_utils.dtype->torch_dtype_from_trt(rhs_val.dtype)
A:torch.fx.experimental.fx2trt.converters.converter_utils.lhs_val->get_trt_tensor(network, lhs_val, f'{name}_lhs', dtype)
A:torch.fx.experimental.fx2trt.converters.converter_utils.rhs_val->get_trt_tensor(network, rhs_val, f'{name}_rhs', dtype)
A:torch.fx.experimental.fx2trt.converters.converter_utils.(lhs_val, rhs_val)->broadcast(network, lhs_val, rhs_val, f'{name}_lhs', f'{name}_rhs')
A:torch.fx.experimental.fx2trt.converters.converter_utils.dim->range(0, len(input_val.shape))
A:torch.fx.experimental.fx2trt.converters.converter_utils.output_val->network.add_reduce(input_val, operation_type, get_axes_for_reduce_op(dim, network.has_implicit_batch_dimension), keepdim).get_output(i)
A:torch.fx.experimental.fx2trt.converters.converter_utils.input_exp_output->add_unary_layer(network, input_val, trt.UnaryOperation.EXP, target, f'{name}_prod_exp')
A:torch.fx.experimental.fx2trt.converters.converter_utils.input_abs_output->add_unary_layer(network, input_val, trt.UnaryOperation.ABS, target, f'{name}_prod_abs')
A:torch.fx.experimental.fx2trt.converters.converter_utils.input_abs_exp_output->add_unary_layer(network, input_abs_output, trt.UnaryOperation.EXP, target, f'{name}_prod_abs_exp')
A:torch.fx.experimental.fx2trt.converters.converter_utils.floor_div_output->add_binary_elementwise_layer(network, input_exp_output, input_abs_exp_output, trt.ElementWiseOperation.FLOOR_DIV, target, f'{name}_exp_floor_div')
A:torch.fx.experimental.fx2trt.converters.converter_utils.double_floor_div_output->add_binary_elementwise_layer(network, floor_div_output, 2, trt.ElementWiseOperation.PROD, target, f'{name}_floor_div*2')
A:torch.fx.experimental.fx2trt.converters.converter_utils.prod_output->add_binary_elementwise_layer(network, input, other, trt.ElementWiseOperation.PROD, target, f'{name}_prod')
A:torch.fx.experimental.fx2trt.converters.converter_utils.sign_output->sign(network, prod_output, target, name)
A:torch.fx.experimental.fx2trt.converters.converter_utils.input->get_trt_tensor(network, input, f'{name}_input')
A:torch.fx.experimental.fx2trt.converters.converter_utils.other->get_trt_tensor(network, other, f'{name}_other', dtype=torch_dtype_from_trt(input.dtype))
A:torch.fx.experimental.fx2trt.converters.converter_utils.abs_input_output->add_unary_layer(network, input, trt.UnaryOperation.ABS, target, f'{name}_abs_input')
A:torch.fx.experimental.fx2trt.converters.converter_utils.abs_other_output->add_unary_layer(network, other, trt.UnaryOperation.ABS, target, f'{name}_abs_other')
A:torch.fx.experimental.fx2trt.converters.converter_utils.abs_floor_output->add_binary_elementwise_layer(network, abs_input_output, abs_other_output, trt.ElementWiseOperation.FLOOR_DIV, target, f'{name}_floor_div')
A:torch.fx.experimental.fx2trt.converters.converter_utils.output->add_binary_elementwise_layer(network, abs_floor_output, sign_output, trt.ElementWiseOperation.PROD, target, f'{name}_output')
torch.fx.experimental.fx2trt.converter_utils.add_activation_layer(network:TRTNetwork,input_val:TRTTensor,operation_type:trt.ActivationType,target:Target,name:str,alpha:Optional[Any]=None,beta:Optional[Any]=None)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.add_binary_elementwise_layer(network:TRTNetwork,lhs_val:Union[int,float,TRTTensor,torch.Tensor],rhs_val:Union[int,float,TRTTensor,torch.Tensor],op_type:trt.ElementWiseOperation,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.add_reduce_layer(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],operation_type:trt.ActivationType,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.add_unary_layer(network:TRTNetwork,input_val:TRTTensor,operation_type:trt.UnaryOperation,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.broadcast(network:TRTNetwork,a:TRTTensor,b:TRTTensor,a_name:str,b_name:str,preset_diff:int=0)->Tuple[TRTTensor, TRTTensor]
torch.fx.experimental.fx2trt.converter_utils.create_constant(network:TRTNetwork,value:Union[int,float,torch.Tensor],name:str,dtype:Optional[torch.dtype])->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.extend_attr_to_tuple(val:Any,num_elem:int)->Tuple[Any, ...]
torch.fx.experimental.fx2trt.converter_utils.extend_mod_attr_to_tuple(mod:torch.nn.Module,name:str,size:int)
torch.fx.experimental.fx2trt.converter_utils.get_axes_for_reduce_op(dim:Union[int,Sequence[int]],has_implicit_batch_dimension:bool)->int
torch.fx.experimental.fx2trt.converter_utils.get_dyn_range(scale,zero_point,dtype)
torch.fx.experimental.fx2trt.converter_utils.get_inputs_from_args_and_kwargs(args,kwargs,input_names)
torch.fx.experimental.fx2trt.converter_utils.get_positive_dim(dim:int,dim_size:int)->int
torch.fx.experimental.fx2trt.converter_utils.get_python_op_from_trt_elementwise_op(trt_op:TRTElementWiseOp)->Callable[[Any, Any], Any]
torch.fx.experimental.fx2trt.converter_utils.get_trt_plugin(plugin_name:str,field_collection:List[TRTPluginFieldCollection],version:str,plugin_namespace:str='')->TRTPlugin
torch.fx.experimental.fx2trt.converter_utils.get_trt_tensor(network:TRTNetwork,input_val:Any,name:str,dtype:Optional[torch.dtype]=None)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.has_dynamic_shape(shape:Shape)->bool
torch.fx.experimental.fx2trt.converter_utils.mark_as_int8_layer(layer,dynamic_range)
torch.fx.experimental.fx2trt.converter_utils.prepend_ones(network:TRTNetwork,tensor:TRTTensor,name:str,num_prepend_ones:int)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.set_layer_name(layer:TRTLayer,target:Target,name:str)->None
torch.fx.experimental.fx2trt.converter_utils.sign(network:TRTNetwork,input_val:TRTTensor,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converter_utils.to_numpy(tensor:Optional[torch.Tensor])->Optional[np.ndarray]
torch.fx.experimental.fx2trt.converter_utils.trunc_div(input:TRTTensor,other:TRTTensor,network:TRTNetwork,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.add_activation_layer(network:TRTNetwork,input_val:TRTTensor,operation_type:trt.ActivationType,target:Target,name:str,alpha:Optional[Any]=None,beta:Optional[Any]=None)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.add_binary_elementwise_layer(network:TRTNetwork,lhs_val:Union[int,float,TRTTensor,torch.Tensor],rhs_val:Union[int,float,TRTTensor,torch.Tensor],op_type:trt.ElementWiseOperation,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.add_reduce_layer(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],operation_type:trt.ActivationType,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.add_unary_layer(network:TRTNetwork,input_val:TRTTensor,operation_type:trt.UnaryOperation,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.broadcast(network:TRTNetwork,a:TRTTensor,b:TRTTensor,a_name:str,b_name:str,preset_diff:int=0)->Tuple[TRTTensor, TRTTensor]
torch.fx.experimental.fx2trt.converters.converter_utils.create_constant(network:TRTNetwork,value:Union[int,float,torch.Tensor],name:str,dtype:Optional[torch.dtype])->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.extend_attr_to_tuple(val:Any,num_elem:int)->Tuple[Any, ...]
torch.fx.experimental.fx2trt.converters.converter_utils.extend_mod_attr_to_tuple(mod:torch.nn.Module,name:str,size:int)
torch.fx.experimental.fx2trt.converters.converter_utils.get_axes_for_reduce_op(dim:Union[int,Sequence[int]],has_implicit_batch_dimension:bool)->int
torch.fx.experimental.fx2trt.converters.converter_utils.get_dyn_range(scale,zero_point,dtype)
torch.fx.experimental.fx2trt.converters.converter_utils.get_inputs_from_args_and_kwargs(args,kwargs,input_names)
torch.fx.experimental.fx2trt.converters.converter_utils.get_positive_dim(dim:int,dim_size:int)->int
torch.fx.experimental.fx2trt.converters.converter_utils.get_python_op_from_trt_elementwise_op(trt_op:TRTElementWiseOp)->Callable[[Any, Any], Any]
torch.fx.experimental.fx2trt.converters.converter_utils.get_trt_plugin(plugin_name:str,field_collection:List[TRTPluginFieldCollection],version:str,plugin_namespace:str='')->TRTPlugin
torch.fx.experimental.fx2trt.converters.converter_utils.get_trt_tensor(network:TRTNetwork,input_val:Any,name:str,dtype:Optional[torch.dtype]=None)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.has_dynamic_shape(shape:Shape)->bool
torch.fx.experimental.fx2trt.converters.converter_utils.mark_as_int8_layer(layer,dynamic_range)
torch.fx.experimental.fx2trt.converters.converter_utils.prepend_ones(network:TRTNetwork,tensor:TRTTensor,name:str,num_prepend_ones:int)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.set_layer_name(layer:TRTLayer,target:Target,name:str)->None
torch.fx.experimental.fx2trt.converters.converter_utils.sign(network:TRTNetwork,input_val:TRTTensor,target:Target,name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.converter_utils.to_numpy(tensor:Optional[torch.Tensor])->Optional[np.ndarray]
torch.fx.experimental.fx2trt.converters.converter_utils.trunc_div(input:TRTTensor,other:TRTTensor,network:TRTNetwork,target:Target,name:str)->TRTTensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/convolution.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.convolution.kernel_size->extend_mod_attr_to_tuple(mod, 'kernel_size', dimension)
A:torch.fx.experimental.fx2trt.converters.convolution.stride->extend_mod_attr_to_tuple(mod, 'stride', dimension)
A:torch.fx.experimental.fx2trt.converters.convolution.padding->extend_mod_attr_to_tuple(mod, 'padding', dimension)
A:torch.fx.experimental.fx2trt.converters.convolution.dilation->extend_mod_attr_to_tuple(mod, 'dilation', dimension)
A:torch.fx.experimental.fx2trt.converters.convolution.kernel->to_numpy(mod.weight() if is_quantized else mod.weight)
A:torch.fx.experimental.fx2trt.converters.convolution.bias->to_numpy(mod.bias() if is_quantized else mod.bias)
A:torch.fx.experimental.fx2trt.converters.convolution.layer->network.add_activation(input=conv_output, type=trt.ActivationType.RELU)
A:torch.fx.experimental.fx2trt.converters.convolution.conv_output->common_conv(network, mod, dimension=2, input_val=input_val, layer_name=f'{layer_name}_conv', is_quantized=is_quantized)
torch.fx.experimental.fx2trt.common_conv(network,mod,dimension,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.common_conv_relu(network,mod,dimension,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.conv2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.convolution.common_conv(network,mod,dimension,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.converters.convolution.common_conv_relu(network,mod,dimension,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.converters.convolution.conv2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.convolution.quantized_conv2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.convolution.quantized_conv_relu2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_conv2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_conv_relu2d(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/linear.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.linear.layer->network.add_shuffle(layer.get_output(0))
A:torch.fx.experimental.fx2trt.converters.linear.kernel->to_numpy(mod.weight if not is_quantized else mod.weight())
A:torch.fx.experimental.fx2trt.converters.linear.bias->to_numpy(mod.bias if not is_quantized else mod.bias())
A:torch.fx.experimental.fx2trt.converters.linear.dyn_range->get_dyn_range(mod.scale, mod.zero_point, torch.quint8)
torch.fx.experimental.fx2trt.common_linear(network,mod,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.converters.linear.common_linear(network,mod,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.converters.linear.linear(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.linear.quantized_linear(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.linear(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_linear(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/maxpool.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.maxpool.kernel_size->extend_mod_attr_to_tuple(mod, 'kernel_size', dimension)
A:torch.fx.experimental.fx2trt.converters.maxpool.stride->extend_mod_attr_to_tuple(mod, 'stride', dimension)
A:torch.fx.experimental.fx2trt.converters.maxpool.padding->extend_mod_attr_to_tuple(mod, 'padding', dimension)
A:torch.fx.experimental.fx2trt.converters.maxpool.layer->network.add_pooling(input=input_val, type=trt.PoolingType.MAX, window_size=kernel_size)
torch.fx.experimental.fx2trt.common_maxpool(network,mod,dimension,input_val,layer_name)
torch.fx.experimental.fx2trt.converters.maxpool.common_maxpool(network,mod,dimension,input_val,layer_name)
torch.fx.experimental.fx2trt.converters.maxpool.maxpool2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.maxpool2d(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/batchnorm.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.batchnorm.power->numpy.ones_like(scale)
A:torch.fx.experimental.fx2trt.converters.batchnorm.layer->network.add_scale(input_val, trt.ScaleMode.CHANNEL, bias, scale, power)
torch.fx.experimental.fx2trt.batchnorm2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.common_batchnorm(network,mod,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.converters.batchnorm.batchnorm2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.batchnorm.common_batchnorm(network,mod,input_val,layer_name,is_quantized)
torch.fx.experimental.fx2trt.converters.batchnorm.quantized_batchnorm2d(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_batchnorm2d(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/acc_ops_converters.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.bias->to_numpy(kwargs['bias'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.weight->to_numpy(kwargs['weight'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.weight_shape->tuple(kwargs['weight'].shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dummy_weight->tensorrt.Weights()
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.layer->network.add_slice(input_val, start=start, shape=shape, stride=stride)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.pad->cast(Sequence[int], kwargs['pad'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.rank->len(input_val.shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.pre_padding->tuple((pad[len(pad) - i - 2] for i in range(0, len(pad), 2)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.post_padding->tuple((pad[len(pad) - i - 1] for i in range(0, len(pad), 2)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.pre_start->tuple((i - 1 for i in input_shape))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.pre_shape->tuple((input_shape[i] + (pad[-(i - prefix_len) * 2 - 2] if i >= prefix_len else 0) for i in range(0, len(input_shape))))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.half_pad_output->network.add_slice(input_val, start=start, shape=shape, stride=stride).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.mid_start->tuple((i - 1 for i in shape))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.transpose_output->network.add_slice(input_val, start=start, shape=shape, stride=stride).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.post_start->tuple([0] * len(shape))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.post_shape->tuple((shape[i] + (pad[-(i - prefix_len) * 2 - 1] if i >= prefix_len else 0) for i in range(0, len(shape))))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.post_stride->tuple([1] * len(shape))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.start_dim->get_positive_dim(cast(int, kwargs['start_dim'] if 'start_dim' in kwargs else 0), num_dims)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.end_dim->get_positive_dim(cast(int, kwargs['end_dim'] if 'end_dim' in kwargs else -1), num_dims)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.input_shape_layer->network.add_shape(input_val)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.prefix_shape_layer->network.add_slice(input_shape_layer.get_output(0), start=(0,), shape=(start_dim,), stride=(1,))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.flatten_shape_layer->network.add_reduce(flatten_shape_layer.get_output(0), trt.ReduceOperation.PROD, axes=get_axes_for_reduce_op(0, False), keep_dims=True)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.suffix_shape_layer->network.add_slice(input_shape_layer.get_output(0), start=(end_dim + 1,), shape=(len(input_val.shape) - end_dim - 1,), stride=(1,))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.final_shape_layer->network.add_concatenation(final_shapes)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.layer.reshape_dims->tuple(shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.power->numpy.ones_like(scale)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.gamma->to_numpy(kwargs['weight'].reshape(*shape))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.beta->to_numpy(kwargs['bias'].reshape(*shape))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.mean_expected_layer->network.add_reduce(input_val, trt.ReduceOperation.AVG, axes, keep_dims=True)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.sub_trt->add_binary_elementwise_layer(network, input_val, mean_expected_layer.get_output(0), trt.ElementWiseOperation.SUB, target, f'{name}_sub')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.pow_tensor->network.add_constant((1,) * len(input_val.shape), trt.Weights(np.ascontiguousarray([2.0], dtype=np.float32)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.pow_var->add_binary_elementwise_layer(network, sub_trt, pow_tensor.get_output(0), trt.ElementWiseOperation.POW, target, f'{name}_pow_var')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.mean_trt_layer->network.add_reduce(pow_var, trt.ReduceOperation.AVG, axes, keep_dims=True)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.eps_tensor->network.add_constant((1,) * len(input_val.shape), trt.Weights(np.ascontiguousarray([eps], dtype=np.float32)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.add_trt->add_binary_elementwise_layer(network, mean_trt_layer.get_output(0), eps_tensor.get_output(0), trt.ElementWiseOperation.SUM, target, f'{name}_add')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.sqrt_trt->add_unary_layer(network, add_trt, trt.UnaryOperation.SQRT, target, f'{name}_sqrt')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.div_trt->add_binary_elementwise_layer(network, sub_trt, sqrt_trt, trt.ElementWiseOperation.DIV, target, f'{name}_div_trt')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.gamma_tensor->network.add_constant(gamma.shape, trt.Weights(np.ascontiguousarray(gamma)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.beta_tensor->network.add_constant(gamma.shape, trt.Weights(np.ascontiguousarray(beta)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.scale_layer->network.add_constant(scale_shape, trt.Weights(np.ascontiguousarray(q_scale, dtype=np.float32)))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dim->get_positive_dim(dim, input_dim_size)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dims->tuple(cast(Sequence[int], kwargs['dims']))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.preceding_ones->network.add_constant((num_preceding_ones,), np.ascontiguousarray([1] * num_preceding_ones, np.int32)).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.reshape_layer->network.add_concatenation([preceding_ones, input_shape_layer.get_output(0)])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.input_val->get_trt_tensor(network, kwargs['input'], f'{name}_input')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.starts_tensor->network.add_constant((len(dims),), np.ascontiguousarray([0] * len(dims), np.int32)).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dims_tensor->network.add_constant((len(dims),), np.ascontiguousarray(dims, np.int32)).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.slice_shapes_tensor->add_binary_elementwise_layer(network, input_shape_layer.get_output(0), dims_tensor, trt.ElementWiseOperation.PROD, target, f'{name}_slice_shapes')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.new_kwargs->kwargs.copy()
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.(topk_out0, topk_out1)->acc_ops_topk(network, target, args, new_kwargs, name + '_topk')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shuffle_layer0->network.add_shuffle(input_val)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shuffle_layer0.reshape_dims->tuple(output_shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shuffle_layer1->network.add_shuffle(input_val)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shuffle_layer1.reshape_dims->tuple(output_shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.kernel_size->extend_attr_to_tuple(kwargs['kernel_size'], 2)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.stride->extend_attr_to_tuple(kwargs['stride'], 2)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.padding->extend_attr_to_tuple(kwargs['padding'], 2)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dilation->extend_attr_to_tuple(kwargs['dilation'], 2)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.output_size->cast(Sequence[int], extend_attr_to_tuple(kwargs['output_size'], 2))
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.s->prepend_ones(network, s, f'{name}_{i}', 1)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shape_layer->network.add_concatenation(inputs=trt_shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.start_int->cast(int, kwargs['start'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.stop_int->cast(int, kwargs['stop'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.step_int->cast(int, kwargs['step'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.output_shape->list(input_val.shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.split_size->cast(int, kwargs['split_size'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shape->list(input_val.shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.shape[dim]->min(split_size, max_offset - offset)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dynamic_dims->get_dynamic_dims(input_val.shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_clamp_trt->network.add_constant(acc_ops_clamp_shape, acc_ops_clamp_tensor)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.clamp_min_layer->add_clamp(network, input_val, min_val, trt.ElementWiseOperation.MAX)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.clamp_max_layer->add_clamp(network, input_val, max_val, trt.ElementWiseOperation.MIN)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.size->math.ceil((stop - start) * 1.0 / stride)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.params->slice_to_trt_params(s, input_val.shape[i])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.slice_out->network.add_slice(input_val, start=start, shape=shape, stride=stride).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.other_val->get_trt_tensor(network, kwargs['other'], f'{name}_other')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.(input_val, other_val)->broadcast(network, input_val, other_val, f'{name}_input', f'{name}_other', preset_diff)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.layer.second_transpose->tuple(permutation)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.scale->network.add_constant(scale_shape, trt.Weights(np.ascontiguousarray(q_scale, dtype=np.float32))).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.type_id->tensorrt.PluginField('type_id', np.array(0, dtype=np.int32), trt.PluginFieldType.INT32)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.field_collection->TRTPluginFieldCollection([type_id])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.plugin->get_trt_plugin(plugin_name, field_collection, plugin_version)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.chunks->cast(int, kwargs['chunks'])
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.input_dim_size->len(input_val.shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.loop->network.add_loop()
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.axis->network.add_constant(dim_value.shape, to_numpy(dim_value)).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.trip_limit_layer->network.add_gather(input_shape, axis, 0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.input_shape->network.add_shape(input_val).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.dim_value->torch.tensor(dim, dtype=torch.int32)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.trip_limit->network.add_gather(input_shape, axis, 0).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.iterator->network.add_loop().add_iterator(input_val, dim, False)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.data->network.add_loop().add_iterator(input_val, dim, False).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.new_dims->tuple(data.shape)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.zero_tensor->network.add_constant(zero_tensor.shape, to_numpy(zero_tensor)).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.running_sum->network.add_loop().add_recurrence(zero_tensor)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.running_sum_tensor->network.add_loop().add_recurrence(zero_tensor).get_output(0)
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.current_sum->add_binary_elementwise_layer(network, data, running_sum_tensor, trt.ElementWiseOperation.SUM, target, f'{name}_sum_2')
A:torch.fx.experimental.fx2trt.converters.acc_ops_converters.loop_output->network.add_loop().add_loop_output(current_sum, trt.LoopOutput.CONCATENATE, dim)
torch.fx.experimental.fx2trt.acc_ops_abs(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_acos(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_adaptive_avg_pool2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_add(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_asin(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_atan(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_avg_pool2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_batch_norm(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_cat(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_ceil(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_chunk(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_clamp(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_contiguous(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_conv2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_cos(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_cosh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_cumsum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_dequantize(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_div(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_elu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_exp(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_flatten(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_floor(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_floor_div(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_gelu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_getitem(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_hard_sigmoid(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_hardtanh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_layer_norm(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_leaky_relu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_linear(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_log(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_matmul(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_max_dim_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_max_full_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_max_pool2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_maximum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_mean(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->TRTTensor
torch.fx.experimental.fx2trt.acc_ops_min_dim_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_min_full_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_minimum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_mul(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_neg(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_pad_with_padding_layer(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_pad_with_slice_layer(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_permute(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_pow(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_quantize_per_channel(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_quantize_per_tensor(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_reciprocal(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_relu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_reshape(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_selu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sigmoid(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sign(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sin(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sinh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_size(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_slice_tensor(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_softmax(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_softsign(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_split(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sqrt(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_squeeze(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sub(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_sum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->TRTTensor
torch.fx.experimental.fx2trt.acc_ops_tan(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_tanh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_tile(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_topk(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_trunc_div(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_tuple_construct(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.acc_ops_unsqueeze(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.add_acc_ops_dim_reduce(network,target,args,kwargs,name,reduce_op)
torch.fx.experimental.fx2trt.add_acc_ops_full_reduce(network,target,args,kwargs,name,reduce_op)
torch.fx.experimental.fx2trt.add_clamp(network,input,val,op)
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_abs(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_acos(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_adaptive_avg_pool2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_add(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_asin(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_atan(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_avg_pool2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_batch_norm(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_cat(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_ceil(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_chunk(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_clamp(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_contiguous(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_conv2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_cos(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_cosh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_cumsum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_dequantize(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_div(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_elu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_exp(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_flatten(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_floor(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_floor_div(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_gelu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_getitem(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_hard_sigmoid(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_hardtanh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_layer_norm(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_leaky_relu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_linear(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_log(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_matmul(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_max_dim_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_max_full_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_max_pool2d(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_maximum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_mean(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_min_dim_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_min_full_reduce(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_minimum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_mul(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_neg(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_pad_with_padding_layer(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_pad_with_slice_layer(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_permute(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_pow(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_quantize_per_channel(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_quantize_per_tensor(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_reciprocal(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_relu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_reshape(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_selu(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sigmoid(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sign(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sin(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sinh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_size(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_slice_tensor(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_softmax(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_softsign(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_split(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sqrt(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_squeeze(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sub(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_sum(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->TRTTensor
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_tan(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_tanh(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_tile(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_topk(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_trunc_div(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_tuple_construct(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.acc_ops_unsqueeze(network:TRTNetwork,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:str)->Union[TRTTensor, Sequence[TRTTensor]]
torch.fx.experimental.fx2trt.converters.acc_ops_converters.add_acc_ops_dim_reduce(network,target,args,kwargs,name,reduce_op)
torch.fx.experimental.fx2trt.converters.acc_ops_converters.add_acc_ops_full_reduce(network,target,args,kwargs,name,reduce_op)
torch.fx.experimental.fx2trt.converters.acc_ops_converters.add_clamp(network,input,val,op)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/add.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.add.len_kwargs->len([x for x in kwargs.keys() if x != '_itensor_to_tensor_meta'])
A:torch.fx.experimental.fx2trt.converters.add.layer->network.add_activation(input=layer.get_output(0), type=trt.ActivationType.RELU)
A:torch.fx.experimental.fx2trt.converters.add.dyn_range->get_dyn_range(kwargs['scale'], kwargs['zero_point'], torch.quint8)
torch.fx.experimental.fx2trt.add(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.add.add(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.add.quantized_add(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.add.quantized_add_relu(network,submod,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_add(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_add_relu(network,submod,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/__init__.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.__init__.TRT_LOGGER->tensorrt.Logger()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/adaptive_avgpool.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.adaptive_avgpool.output_size->extend_mod_attr_to_tuple(submod, 'output_size', 2)
A:torch.fx.experimental.fx2trt.converters.adaptive_avgpool.layer->network.add_pooling(input=input_val, type=trt.PoolingType.AVERAGE, window_size=kernel_size)
torch.fx.experimental.fx2trt.adaptive_avgpool2d(network,submod,args,kwargs,name)
torch.fx.experimental.fx2trt.converters.adaptive_avgpool.adaptive_avgpool2d(network,submod,args,kwargs,name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/converters/mul.py----------------------------------------
A:torch.fx.experimental.fx2trt.converters.mul.layer->network.add_elementwise(lhs_val, rhs_val, trt.ElementWiseOperation.PROD)
A:torch.fx.experimental.fx2trt.converters.mul.dyn_range->get_dyn_range(kwargs['scale'], kwargs['zero_point'], torch.quint8)
torch.fx.experimental.fx2trt.converters.mul.mul(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.converters.mul.quantized_mul(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.mul(network,target,args,kwargs,layer_name)
torch.fx.experimental.fx2trt.quantized_mul(network,target,args,kwargs,layer_name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/tools/trt_splitter.py----------------------------------------
A:torch.fx.experimental.fx2trt.tools.trt_splitter.supported_if_converter_registered->torch.fx.passes.operator_support.OperatorSupport(support_dict=support_dict)
A:torch.fx.experimental.fx2trt.tools.trt_splitter.settings->TRTSplitterSetting()
A:torch.fx.experimental.fx2trt.tools.trt_splitter.operator_support->create_trt_operator_support(settings.use_implicit_batch_dim)
A:torch.fx.experimental.fx2trt.tools.trt_splitter.interp->TRTInterpreter(mod, InputTensorSpec.from_tensors(inputs))
A:torch.fx.experimental.fx2trt.tools.trt_splitter.interpreter_result->TRTInterpreter(mod, InputTensorSpec.from_tensors(inputs)).run(*inputs)
A:torch.fx.experimental.fx2trt.tools.trt_splitter.minimizer->TensorRTMinimizer(mod, inputs, lambda a, b, c: (1, True))
A:torch.fx.experimental.fx2trt.tools.trt_splitter.culprits->TensorRTMinimizer(mod, inputs, lambda a, b, c: (1, True)).minimize()
torch.fx.experimental.fx2trt.tools.trt_splitter.TRTSplitter(self,module:torch.fx.GraphModule,sample_input:Tuple[torch.Tensor],operator_support:ops.OperatorSupportBase=None,settings:TRTSplitterSetting=None)
torch.fx.experimental.fx2trt.tools.trt_splitter.TRTSplitter.__init__(self,module:torch.fx.GraphModule,sample_input:Tuple[torch.Tensor],operator_support:ops.OperatorSupportBase=None,settings:TRTSplitterSetting=None)
torch.fx.experimental.fx2trt.tools.trt_splitter.TRTSplitter._find_culprit(self,mod:torch.fx.GraphModule,inputs:Tensors)
torch.fx.experimental.fx2trt.tools.trt_splitter.TRTSplitter._lower_model_to_backend(self,mod:torch.fx.GraphModule,inputs:Iterable[torch.Tensor])
torch.fx.experimental.fx2trt.tools.trt_splitter.TRTSplitterSetting(self)
torch.fx.experimental.fx2trt.tools.trt_splitter.TRTSplitterSetting.__init__(self)
torch.fx.experimental.fx2trt.tools.trt_splitter.create_trt_operator_support(use_implicit_batch_dim=True)->ops.OperatorSupportBase


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/tools/timing_cache_utils.py----------------------------------------
A:torch.fx.experimental.fx2trt.tools.timing_cache_utils.logger->logging.getLogger(__name__)
A:torch.fx.experimental.fx2trt.tools.timing_cache_utils.tc->os.environ.get('TRT_TIMING_CACHE_PREFIX', '')
A:torch.fx.experimental.fx2trt.tools.timing_cache_utils.timing_cache_file->self.get_file_full_name(timing_cache_file)
A:torch.fx.experimental.fx2trt.tools.timing_cache_utils.cache_data->raw_cache.read()
torch.fx.experimental.fx2trt.tools.timing_cache_utils.TimingCacheManager(self,timing_cache_prefix:str='',save_timing_cache=False)
torch.fx.experimental.fx2trt.tools.timing_cache_utils.TimingCacheManager.__init__(self,timing_cache_prefix:str='',save_timing_cache=False)
torch.fx.experimental.fx2trt.tools.timing_cache_utils.TimingCacheManager.get_file_full_name(self,name:str)
torch.fx.experimental.fx2trt.tools.timing_cache_utils.TimingCacheManager.get_timing_cache_trt(self,timing_cache_file:str)->bytearray
torch.fx.experimental.fx2trt.tools.timing_cache_utils.TimingCacheManager.update_timing_cache(self,timing_cache_file:str,serilized_cache:bytearray)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/tools/graph_util.py----------------------------------------
A:torch.fx.experimental.fx2trt.tools.graph_util.dot->graphviz.Digraph(comment='Network')
A:torch.fx.experimental.fx2trt.tools.graph_util.layer->network.get_layer(i)
A:torch.fx.experimental.fx2trt.tools.graph_util.layer_a->network.get_layer(a)
A:torch.fx.experimental.fx2trt.tools.graph_util.layer_b->network.get_layer(b)
A:torch.fx.experimental.fx2trt.tools.graph_util.output_i->network.get_layer(a).get_output(i)
A:torch.fx.experimental.fx2trt.tools.graph_util.input_j->network.get_layer(b).get_output(j)
A:torch.fx.experimental.fx2trt.tools.graph_util.input_i->network.get_output(i)
torch.fx.experimental.fx2trt.tools.graph_util.get_layer_name_type(layer)
torch.fx.experimental.fx2trt.tools.graph_util.trt_network_to_dot_graph(network)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/tools/trt_minimizer.py----------------------------------------
A:torch.fx.experimental.fx2trt.tools.trt_minimizer.interp->TRTInterpreter(mod, InputTensorSpec.from_tensors(inputs), explicit_batch_dimension=True)
A:torch.fx.experimental.fx2trt.tools.trt_minimizer.interpreter_result->TRTInterpreter(mod, InputTensorSpec.from_tensors(inputs), explicit_batch_dimension=True).run(max_batch_size=batch_size)
A:torch.fx.experimental.fx2trt.tools.trt_minimizer.res_mod->TRTModule(interpreter_result.engine, interpreter_result.input_names, interpreter_result.output_names)
A:torch.fx.experimental.fx2trt.tools.trt_minimizer.mod->self.lower_fn(mod, inputs, self.max_batch_size)
A:torch.fx.experimental.fx2trt.tools.trt_minimizer.output->mod(*inputs)
A:torch.fx.experimental.fx2trt.tools.trt_minimizer.nodes->self._collect_nodes(start, end)
torch.fx.experimental.fx2trt.tools.TensorRTMinimizer(self,module:torch.fx.GraphModule,sample_input:Tensors,compare_fn:Callable[[Any,Any,Any],Tuple[float,bool]],settings:TensorRTMinizerSetting=TensorRTMinizerSetting(),max_batch_size:Any=2048,lower_fn:Callable[[torch.fx.GraphModule,Tensors,Any],TRTModule]=lower_mod_default)
torch.fx.experimental.fx2trt.tools.TensorRTMinimizer.get_nodes(self,start=None,end=None,enable_print=False)
torch.fx.experimental.fx2trt.tools.TensorRTMinimizer.run_a(self,mod,inputs)
torch.fx.experimental.fx2trt.tools.TensorRTMinimizer.run_b(self,mod,inputs)
torch.fx.experimental.fx2trt.tools.TensorRTMinizerSetting(self,explicit_batch_dimension:Any=True)
torch.fx.experimental.fx2trt.tools.lower_mod_default(mod:torch.fx.GraphModule,inputs:Tensors,batch_size:Any=2048)->TRTModule
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinimizer(self,module:torch.fx.GraphModule,sample_input:Tensors,compare_fn:Callable[[Any,Any,Any],Tuple[float,bool]],settings:TensorRTMinizerSetting=TensorRTMinizerSetting(),max_batch_size:Any=2048,lower_fn:Callable[[torch.fx.GraphModule,Tensors,Any],TRTModule]=lower_mod_default)
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinimizer.__init__(self,module:torch.fx.GraphModule,sample_input:Tensors,compare_fn:Callable[[Any,Any,Any],Tuple[float,bool]],settings:TensorRTMinizerSetting=TensorRTMinizerSetting(),max_batch_size:Any=2048,lower_fn:Callable[[torch.fx.GraphModule,Tensors,Any],TRTModule]=lower_mod_default)
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinimizer.get_nodes(self,start=None,end=None,enable_print=False)
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinimizer.run_a(self,mod,inputs)
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinimizer.run_b(self,mod,inputs)
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinizerSetting(self,explicit_batch_dimension:Any=True)
torch.fx.experimental.fx2trt.tools.trt_minimizer.TensorRTMinizerSetting.__init__(self,explicit_batch_dimension:Any=True)
torch.fx.experimental.fx2trt.tools.trt_minimizer.lower_mod_default(mod:torch.fx.GraphModule,inputs:Tensors,batch_size:Any=2048)->TRTModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/tools/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/tools/engine_layer_visualize.py----------------------------------------
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.parser->argparse.ArgumentParser()
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.args->argparse.ArgumentParser().parse_args()
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.inputs->re.findall('[, ]*(.+?)\\[([Half|Float|Int8]+\\(\\d[,\\d]*\\))\\]', inputs)
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.layer_name->layer.layer_name.replace('|', '\\|')
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.label->label.replace('>', '\\>').replace('>', '\\>')
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.from_node->pydot.Node(input_name, label='{reformatter|kernel: Reformat\\l|tactic: 0\\l}', **style)
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.edge_name->input_name.replace('>', '\\>')
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.times->f.readlines()
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.t->t.strip('\n').split(': ').strip('\n').split(': ')
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.lines->f.readlines()
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.line->line.strip('\n').strip('\n')
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.dot_graph->pydot.Dot('Layer Graph')
A:torch.fx.experimental.fx2trt.tools.engine_layer_visualize.node->build_node(layer)
torch.fx.experimental.fx2trt.tools.engine_layer_visualize.LayerInfo(NamedTuple)
torch.fx.experimental.fx2trt.tools.engine_layer_visualize.LayerInfo.from_string(cls,string,tactic_names,layer_times=None)
torch.fx.experimental.fx2trt.tools.engine_layer_visualize.build_edge(layer,graph,reformat_layers,output_name2node,layer_name2node)
torch.fx.experimental.fx2trt.tools.engine_layer_visualize.build_node(layer)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/passes/fuse_pass.py----------------------------------------
A:torch.fx.experimental.fx2trt.passes.fuse_pass.weight->get_trt_tensor(network, weight.t(), f'{name}_weight')
A:torch.fx.experimental.fx2trt.passes.fuse_pass.weight_t->get_trt_tensor(network, weight.t(), f'{name}_weight').transpose(0, 1)
A:torch.fx.experimental.fx2trt.passes.fuse_pass.weight_t_attr->gm.graph.get_attr(weight_t_name)
A:torch.fx.experimental.fx2trt.passes.fuse_pass.fused_node->gm.graph.call_function(acc_ops.add, kwargs={'input': left, 'other': right})
A:torch.fx.experimental.fx2trt.passes.fuse_pass.lhs->lhs.transpose(-1, -2).transpose(-1, -2)
A:torch.fx.experimental.fx2trt.passes.fuse_pass.rhs->rhs.transpose(-1, -2).transpose(-1, -2)
A:torch.fx.experimental.fx2trt.passes.fuse_pass.ranks->len(node.meta['tensor_meta'].shape)
A:torch.fx.experimental.fx2trt.passes.fuse_pass.permutation->list((i % ranks for i in node.kwargs['permutation']))
A:torch.fx.experimental.fx2trt.passes.fuse_pass.allowed_permutation->list((i for i in range(ranks)))
A:torch.fx.experimental.fx2trt.passes.fuse_pass.layer->network.add_matrix_multiply(input, trt.MatrixOperation.TRANSPOSE, weight, trt.MatrixOperation.NONE)
A:torch.fx.experimental.fx2trt.passes.fuse_pass.bias->get_trt_tensor(network, bias.reshape(1, -1), f'{name}_bias')
A:torch.fx.experimental.fx2trt.passes.fuse_pass.(input, weight)->broadcast(network, input, weight, f'{input.name}_broadcast', f'{weight.name}_broadcast')
torch.fx.experimental.fx2trt.passes.fuse_pass.check_permute(node:torch.fx.Node)
torch.fx.experimental.fx2trt.passes.fuse_pass.fuse_permute_linear(gm:torch.fx.GraphModule)
torch.fx.experimental.fx2trt.passes.fuse_pass.fuse_permute_matmul(gm:torch.fx.GraphModule)
torch.fx.experimental.fx2trt.passes.fuse_pass.fuse_sparse_matmul_add(gm:torch.fx.GraphModule)
torch.fx.experimental.fx2trt.passes.fuse_pass.fuse_unsqueeze_cat_sum(gm:torch.fx.GraphModule)
torch.fx.experimental.fx2trt.passes.fuse_pass.trt_transposed_linear(input:torch.Tensor,weight:torch.Tensor,bias:torch.Tensor)
torch.fx.experimental.fx2trt.passes.fuse_pass.trt_transposed_matmul(lhs:torch.Tensor,rhs:torch.Tensor,lhs_transposed:bool,rhs_transposed:bool)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/passes/remove_duplicate_output_args.py----------------------------------------
A:torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args._LOGGER->logging.getLogger(__name__)
A:torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args.sub_gm->top_level.get_submodule(node.target)
A:torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args.replace_res->_remove_duplicate_output_args(sub_gm)
A:torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args.idx->_ensure_proper_output_use(user, node)
torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args.RemoveDuplicateResult
torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args._ensure_proper_output_use(user:fx.Node,target_node:fx.Node)->int
torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args._remove_duplicate_output_args(gm:fx.GraphModule)->RemoveDuplicateResult
torch.fx.experimental.fx2trt.passes.remove_duplicate_output_args.remove_duplicate_output_args(top_level:fx.GraphModule,target_subnets:t.Collection[str])->t.Mapping[str, 'RemoveDuplicateResult']


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx2trt/passes/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx_acc/acc_ops.py----------------------------------------
A:torch.fx.experimental.fx_acc.acc_ops.size_node->node.graph.call_function(size, kwargs={'input': node.kwargs['input']})
A:torch.fx.experimental.fx_acc.acc_ops.size_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.getitem_node->node.graph.call_function(getitem, kwargs=getitem_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.getitem_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.new_node->node.graph.call_function(quantized_conv2d, kwargs=kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.cat_node->node.graph.create_node('call_function', cat, kwargs={'tensors': unsqueeze_nodes, 'dim': node.kwargs['dim']})
A:torch.fx.experimental.fx_acc.acc_ops.cat_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.ranks->len(node.meta['tensor_meta'].shape)
A:torch.fx.experimental.fx_acc.acc_ops.shuffle->list((i for i in range(ranks)))
A:torch.fx.experimental.fx_acc.acc_ops.dim0->cast(int, node.kwargs['dim0'])
A:torch.fx.experimental.fx_acc.acc_ops.dim1->cast(int, node.kwargs['dim1'])
A:torch.fx.experimental.fx_acc.acc_ops.permute_node->node.graph.call_function(the_function=permute, kwargs={'input': node.kwargs.get('input'), 'permutation': shuffle})
A:torch.fx.experimental.fx_acc.acc_ops.permute_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.mm_node->node.graph.create_node('call_function', mul, kwargs=mul_kwargs, name=f'{mm_node.name}_mul')
A:torch.fx.experimental.fx_acc.acc_ops.mm_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.new_input_node->node.graph.create_node('call_function', mul, kwargs=mul_kwargs, name=f'{node.name}_input_mul')
A:torch.fx.experimental.fx_acc.acc_ops.new_input_node.meta->input_node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.add_node->node.graph.call_function(quantized_add, kwargs=add_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.add_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.new_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.sigmoid_node->node.graph.call_function(sigmoid, kwargs={'input': input_node})
A:torch.fx.experimental.fx_acc.acc_ops.sigmoid_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.new_sigmoid_node->node.graph.call_function(hardsigmoid, kwargs={'input': input_node})
A:torch.fx.experimental.fx_acc.acc_ops.new_sigmoid_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.d->dequantize(input=input)
A:torch.fx.experimental.fx_acc.acc_ops.div_kwargs->dict(node.kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.div_node->node.graph.call_function(floor_div, kwargs={'input': div_kwargs['input'], 'other': div_kwargs['other']})
A:torch.fx.experimental.fx_acc.acc_ops.div_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.log_node->node.graph.call_function(log, kwargs=log_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.log_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.kwargs->dict(node.kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.new_targets->target_map(node.op, node.target)
A:torch.fx.experimental.fx_acc.acc_ops.max_kwargs->dict()
A:torch.fx.experimental.fx_acc.acc_ops.max_node->node.graph.call_function(nt, kwargs=max_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.max_kwargs['keepdim']->node.kwargs.get('keepdim', False)
A:torch.fx.experimental.fx_acc.acc_ops.max_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.flatten_node->node.graph.call_function(flatten, kwargs=flatten_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.topk_node->node.graph.call_function(topk, kwargs=topk_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.output_node->node.graph.call_function(squeeze, kwargs=squeeze_kwargs)
A:torch.fx.experimental.fx_acc.acc_ops.output_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_ops.slc->slice(start, stop, step)
A:torch.fx.experimental.fx_acc.acc_ops.mem_format->node.kwargs.get('memory_format')
A:torch.fx.experimental.fx_acc.acc_ops.device->node.kwargs.get('device')
A:torch.fx.experimental.fx_acc.acc_ops.other_node->node.graph.create_node('call_function', mul, kwargs={'input': node.kwargs['other'], 'other': node.kwargs['alpha']}, name=node.name + '_mul_alpha')
A:torch.fx.experimental.fx_acc.acc_ops.prefix->node.target.replace('.', '_')
A:torch.fx.experimental.fx_acc.acc_ops.get_weight->node.graph.get_attr(weight_name)
A:torch.fx.experimental.fx_acc.acc_ops.get_weight.meta['tensor_meta']->_extract_tensor_metadata(conv_module.weight())
A:torch.fx.experimental.fx_acc.acc_ops.get_bias->node.graph.get_attr(bias_name)
A:torch.fx.experimental.fx_acc.acc_ops.get_bias.meta['tensor_meta']->_extract_tensor_metadata(conv_module.bias())
A:torch.fx.experimental.fx_acc.acc_ops.relu_node->node.graph.call_function(relu, kwargs={'input': conv2d_node, 'inplace': False})
A:torch.fx.experimental.fx_acc.acc_ops.conv2d_node->packed_quantized_conv2d_mapper(node, mod)
torch.fx.experimental.fx_acc.acc_ops.abs(*,input)
torch.fx.experimental.fx_acc.acc_ops.acos(*,input)
torch.fx.experimental.fx_acc.acc_ops.adaptive_avg_pool2d(*,input,output_size)
torch.fx.experimental.fx_acc.acc_ops.add(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.add_maximum_minimum_mapper(node:torch.fx.Node,mod:torch.fx.GraphModule)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.add_relu_unfuse_mapper(node:torch.fx.Node,mod:torch.fx.GraphModule)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.addmm_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.argmin_max_mapper_impl(node:torch.fx.Node,largest:bool)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.asin(*,input)
torch.fx.experimental.fx_acc.acc_ops.atan(*,input)
torch.fx.experimental.fx_acc.acc_ops.avg_pool2d(*,input,kernel_size,stride,padding,ceil_mode,count_include_pad,divisor_override)
torch.fx.experimental.fx_acc.acc_ops.batch_norm(*,input,running_mean,running_var,weight,bias,training,momentum,eps)
torch.fx.experimental.fx_acc.acc_ops.cat(*,tensors,dim)
torch.fx.experimental.fx_acc.acc_ops.ceil(*,input)
torch.fx.experimental.fx_acc.acc_ops.chunk(*,input,chunks,dim=0)
torch.fx.experimental.fx_acc.acc_ops.clamp(*,input,min=None,max=None)
torch.fx.experimental.fx_acc.acc_ops.contiguous(*,input)
torch.fx.experimental.fx_acc.acc_ops.conv2d(*,input,weight,bias,stride,padding,dilation,groups)
torch.fx.experimental.fx_acc.acc_ops.cos(*,input)
torch.fx.experimental.fx_acc.acc_ops.cosh(*,input)
torch.fx.experimental.fx_acc.acc_ops.cumsum(*,input,dim,dtype=None)
torch.fx.experimental.fx_acc.acc_ops.custom_getattr_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.custom_narrow_mapper(node:torch.fx.Node,mod:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.custom_tensor_reshape_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.custom_tensor_to_mapper(node:torch.fx.Node,_:nn.Module)
torch.fx.experimental.fx_acc.acc_ops.custom_torch_add_mapper(node:torch.fx.Node,mod:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.dequantize(*,input)
torch.fx.experimental.fx_acc.acc_ops.div(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.div_mapper(node:torch.fx.Node,mod:torch.fx.GraphModule)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.dropout_mapper(node:torch.fx.Node,mod:nn.Module)
torch.fx.experimental.fx_acc.acc_ops.elu(*,input,alpha=1.0,inplace=False)
torch.fx.experimental.fx_acc.acc_ops.embedding_bag(*,input,weight,offsets,max_norm,norm_type,scale_grad_by_freq,mode,sparse,per_sample_weights,include_last_offset,padding_idx)
torch.fx.experimental.fx_acc.acc_ops.embedding_bag_4bit_rowwise_offsets(*,weight,indices,offsets,scale_grad_by_freq,mode,pruned_weights,per_sample_weights,compressed_indices_mapping,include_last_offset)
torch.fx.experimental.fx_acc.acc_ops.embedding_bag_byte_rowwise_offsets(*,weight,indices,offsets,scale_grad_by_freq,mode,pruned_weights,per_sample_weights,compressed_indices_mapping,include_last_offset)
torch.fx.experimental.fx_acc.acc_ops.exp(*,input)
torch.fx.experimental.fx_acc.acc_ops.flatten(*,input,start_dim=0,end_dim=-1)
torch.fx.experimental.fx_acc.acc_ops.floor(*,input)
torch.fx.experimental.fx_acc.acc_ops.floor_div(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.gelu(*,input)
torch.fx.experimental.fx_acc.acc_ops.getitem(*,input,idx)
torch.fx.experimental.fx_acc.acc_ops.hardsigmoid(*,input)
torch.fx.experimental.fx_acc.acc_ops.hardswish_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.hardtanh(*,input,min_val=-1.0,max_val=1.0)
torch.fx.experimental.fx_acc.acc_ops.layer_norm(*,input,normalized_shape,weight,bias,eps)
torch.fx.experimental.fx_acc.acc_ops.leaky_relu(*,input,negative_slope=0.01,inplace=False)
torch.fx.experimental.fx_acc.acc_ops.linalg_norm(*,input,ord,dim,keepdim)
torch.fx.experimental.fx_acc.acc_ops.linear(*,input,weight,bias)
torch.fx.experimental.fx_acc.acc_ops.log(*,input)
torch.fx.experimental.fx_acc.acc_ops.matmul(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.max_dim_reduce(*,input,dim=None,keepdim=False)
torch.fx.experimental.fx_acc.acc_ops.max_full_reduce(*,input)
torch.fx.experimental.fx_acc.acc_ops.max_pool2d(*,input,kernel_size,stride,padding,dilation,ceil_mode,return_indices)
torch.fx.experimental.fx_acc.acc_ops.maximum(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.mean(*,input,dim=None,keepdim=False,dtype=None)
torch.fx.experimental.fx_acc.acc_ops.mean_mapper(node,mod)
torch.fx.experimental.fx_acc.acc_ops.min_dim_reduce(*,input,dim=None,keepdim=False)
torch.fx.experimental.fx_acc.acc_ops.min_full_reduce(*,input)
torch.fx.experimental.fx_acc.acc_ops.minimum(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.mul(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.neg(*,input)
torch.fx.experimental.fx_acc.acc_ops.packed_quantized_conv2d_mapper(node:torch.fx.Node,mod:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.packed_quantized_convrelu2d_mapper(node:torch.fx.Node,mod:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.packed_quantized_linear_mapper(node:torch.fx.Node,mod:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.pad(*,input,pad,mode,value)
torch.fx.experimental.fx_acc.acc_ops.permute(*,input,permutation)
torch.fx.experimental.fx_acc.acc_ops.pow(*,input,exponent)
torch.fx.experimental.fx_acc.acc_ops.quantize_per_channel(*,input,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.quantize_per_tensor(*,input,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.quantized_add(*,input,other,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.quantized_batch_norm2d(*,input,running_mean,running_var,weight,bias,eps,acc_out_ty)
torch.fx.experimental.fx_acc.acc_ops.quantized_conv2d(*,input,weight,bias,stride,padding,dilation,groups,padding_mode,acc_out_ty)
torch.fx.experimental.fx_acc.acc_ops.quantized_linear(*,input,weight,bias,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.quantized_mul(*,input,other,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.reciprocal(*,input)
torch.fx.experimental.fx_acc.acc_ops.reduce_op_mapper(node:torch.fx.Node,mod:torch.fx.GraphModule,func)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.relu(*,input,inplace=False)
torch.fx.experimental.fx_acc.acc_ops.rescale_quantize_per_channel(*,input,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.rescale_quantize_per_tensor(*,input,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.reshape(*,input,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.selu(*,input,inplace=False)
torch.fx.experimental.fx_acc.acc_ops.sigmoid(*,input)
torch.fx.experimental.fx_acc.acc_ops.sign(*,input)
torch.fx.experimental.fx_acc.acc_ops.silu(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.sin(*,input)
torch.fx.experimental.fx_acc.acc_ops.sinh(*,input)
torch.fx.experimental.fx_acc.acc_ops.size(*,input)
torch.fx.experimental.fx_acc.acc_ops.slice_tensor(*,input,dim,start,stop,step)
torch.fx.experimental.fx_acc.acc_ops.softmax(*,input,dim,dtype)
torch.fx.experimental.fx_acc.acc_ops.softsign(*,input)
torch.fx.experimental.fx_acc.acc_ops.split(*,input,split_size,dim)
torch.fx.experimental.fx_acc.acc_ops.sqrt(*,input)
torch.fx.experimental.fx_acc.acc_ops.square_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.squeeze(*,input,dim=None)
torch.fx.experimental.fx_acc.acc_ops.stack_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.sub(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.sum(*,input,dim=None,keepdim=False,dtype=None)
torch.fx.experimental.fx_acc.acc_ops.sum_mapper(node:torch.fx.Node,mod:torch.fx.GraphModule)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.t_mapper(node:torch.fx.Node,_:nn.Module)
torch.fx.experimental.fx_acc.acc_ops.tan(*,input)
torch.fx.experimental.fx_acc.acc_ops.tanh(*,input)
torch.fx.experimental.fx_acc.acc_ops.tensor_size_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.tile(*,input,dims)
torch.fx.experimental.fx_acc.acc_ops.to_dtype(input,acc_out_ty=None)
torch.fx.experimental.fx_acc.acc_ops.topk(*,input,k,dim,largest,sorted)
torch.fx.experimental.fx_acc.acc_ops.torch_argmin_mapper(node:torch.fx.Node,_:torch.nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.torch_log1p_mapper(node:torch.fx.Node,_:torch.nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.torch_split_mapper(node:torch.fx.Node,mod:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.transpose_mapper(node:torch.fx.Node,_:nn.Module)->torch.fx.Node
torch.fx.experimental.fx_acc.acc_ops.trunc_div(*,input,other)
torch.fx.experimental.fx_acc.acc_ops.tuple_construct(*,tensors)
torch.fx.experimental.fx_acc.acc_ops.unsqueeze(*,input,dim)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx_acc/acc_normalizer.py----------------------------------------
A:torch.fx.experimental.fx_acc.acc_normalizer.orig_kwarg_set->set(orig_kwarg)
A:torch.fx.experimental.fx_acc.acc_normalizer.norm_info->NormalizationInfo(new_fn_target=new_fn_target, arg_replacement_tuples=final_arg_replacement_tuples, custom_mapping_fn=custom_mapping_fn, kwargs_to_move_to_acc_out_ty=kwargs_to_move_to_acc_out_ty, needs_shapes_for_normalization=needs_shapes_for_normalization)
A:torch.fx.experimental.fx_acc.acc_normalizer.final_arg_replacement_tuples->_get_dup_signature_tuples(new_fn_target)
A:torch.fx.experimental.fx_acc.acc_normalizer.normalization_info->_normalization_dict.get(torch_package_op_and_target)
A:torch.fx.experimental.fx_acc.acc_normalizer.new_kwargs['acc_out_ty']->torch.fx.experimental.fx_acc.acc_utils.build_raw_tensor_meta(**tmd_dict)
A:torch.fx.experimental.fx_acc.acc_normalizer.orig_kwargs_name->next((key for key in orig_kwargs_names if key in node.kwargs), None)
A:torch.fx.experimental.fx_acc.acc_normalizer.new_node->graph.create_node('call_function', normalization_info.new_fn_target, args=normalized_args, kwargs=normalized_kwargs, name=node.name)
A:torch.fx.experimental.fx_acc.acc_normalizer.new_node.meta->node.meta.copy()
A:torch.fx.experimental.fx_acc.acc_normalizer.target->re.sub('\\A<torch_package_\\d+>', '<torch_package_>', _get_qualified_name(node.target))
A:torch.fx.experimental.fx_acc.acc_normalizer.normalized_kwargs->get_normalized_kwargs(node, normalization_info.arg_replacement_tuples)
torch.fx.experimental.fx_acc.acc_normalizer.NormalizationInfo(NamedTuple)
torch.fx.experimental.fx_acc.acc_normalizer._get_dup_signature_tuples(fn:Callable)->List[Tuple[str, str]]
torch.fx.experimental.fx_acc.acc_normalizer._insert_fun(op_and_target:Tuple[str,Union[str,Callable]],arg_replacement_tuples:List[Tuple],new_fn_target:Optional[Callable]=None,custom_mapping_fn:Optional[Callable]=None,kwargs_to_move_to_acc_out_ty:Optional[List[Union[Tuple[str,str,bool],Tuple[str,str]]]]=None,needs_shapes_for_normalization=False,allow_normalize_from_torch_package=False)
torch.fx.experimental.fx_acc.acc_normalizer.get_normalized_kwargs(node:torch.fx.Node,arg_replacement_tuples:ArgReplacementTuplesType)
torch.fx.experimental.fx_acc.acc_normalizer.move_kwargs_to_acc_out_ty(node_or_normalization_info:Union[NormalizationInfo,torch.fx.Node],new_kwargs:Dict[str,Any])
torch.fx.experimental.fx_acc.acc_normalizer.normalize(mod:torch.fx.GraphModule,expect_nodes_have_shapes:bool=False)
torch.fx.experimental.fx_acc.acc_normalizer.register_acc_op(acc_op:Callable)
torch.fx.experimental.fx_acc.acc_normalizer.register_acc_op_mapping(op_and_target:Tuple[str,Union[str,Callable]],arg_replacement_tuples:Optional[List[Union[Tuple[Union[str,Tuple[str,...]],str],Tuple[Union[str,Tuple[str,...]],str,bool]]]]=None,kwargs_to_move_to_acc_out_ty:Optional[List[Union[Tuple[str,str,bool],Tuple[str,str]]]]=None)
torch.fx.experimental.fx_acc.acc_normalizer.register_custom_acc_mapper_fn(op_and_target:Tuple[str,Union[str,Callable]],arg_replacement_tuples:List[Union[Tuple[Union[str,Tuple[str,...]],str],Tuple[Union[str,Tuple[str,...]],str,bool]]],needs_shapes_for_normalization=False,allow_normalize_from_torch_package=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx_acc/acc_op_properties.py----------------------------------------
A:torch.fx.experimental.fx_acc.acc_op_properties.pointwise->auto()
A:torch.fx.experimental.fx_acc.acc_op_properties.quantized->auto()
A:torch.fx.experimental.fx_acc.acc_op_properties.unary->auto()
torch.fx.experimental.fx_acc.acc_op_properties.AccOpProperty(Flag)
torch.fx.experimental.fx_acc.acc_op_properties.add_optimization_properties_to_meta(mod:torch.fx.GraphModule)->None
torch.fx.experimental.fx_acc.acc_op_properties.register_acc_op_properties(*properties:AccOpProperty)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx_acc/acc_utils.py----------------------------------------
A:torch.fx.experimental.fx_acc.acc_utils.target_atoms->target.split('.')
A:torch.fx.experimental.fx_acc.acc_utils.curr_obj->getattr(curr_obj, atom)
A:torch.fx.experimental.fx_acc.acc_utils.serialized_json->json.dumps(serialize_module(fx_module, weights), indent=2)
A:torch.fx.experimental.fx_acc.acc_utils.(base, ext)->os.path.splitext(fname)
A:torch.fx.experimental.fx_acc.acc_utils.g->torch.fx.passes.graph_drawer.FxGraphDrawer(traced, figname)
A:torch.fx.experimental.fx_acc.acc_utils.x->torch.fx.passes.graph_drawer.FxGraphDrawer(traced, figname).get_main_dot_graph()
A:torch.fx.experimental.fx_acc.acc_utils.name->re.sub('[^0-9a-zA-Z_]+', '_', name)
A:torch.fx.experimental.fx_acc.acc_utils.match->re.match('(.*)_(\\d+)$', name)
A:torch.fx.experimental.fx_acc.acc_utils.(base, num)->re.match('(.*)_(\\d+)$', name).group(1, 2)
torch.fx.experimental.fx_acc.acc_utils.build_raw_tensor_meta(shape=None,dtype=None,requires_grad=None,stride=None,memory_format=None,is_quantized=None,qparams=None)
torch.fx.experimental.fx_acc.acc_utils.draw_graph(traced:torch.fx.GraphModule,fname:str,figname:str='fx_graph')
torch.fx.experimental.fx_acc.acc_utils.get_attr(node:torch.fx.Node)->Any
torch.fx.experimental.fx_acc.acc_utils.get_model_info_str(gm:torch.fx.GraphModule,header:Optional[str]=None)
torch.fx.experimental.fx_acc.acc_utils.get_target_from_module(mod:torch.nn.Module,target:str)
torch.fx.experimental.fx_acc.acc_utils.get_unique_attr_name_in_module(mod_traced:torch.fx.GraphModule,name:str)->str
torch.fx.experimental.fx_acc.acc_utils.is_acc_op(node_or_target:Union[Callable,torch.fx.Node])->bool
torch.fx.experimental.fx_acc.acc_utils.is_acc_op_with_kwarg(node_or_target:Union[Callable,torch.fx.Node],kwarg:str)->bool
torch.fx.experimental.fx_acc.acc_utils.serialize_module_json_to_file(fx_module:GraphModule,fname:str)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx_acc/acc_tracer.py----------------------------------------
A:torch.fx.experimental.fx_acc.acc_tracer._LOGGER->logging.getLogger(__name__)
A:torch.fx.experimental.fx_acc.acc_tracer.(sourcelines, _)->inspect.getsourcelines(fn)
A:torch.fx.experimental.fx_acc.acc_tracer.sourcelines->normalize_source_lines(sourcelines)
A:torch.fx.experimental.fx_acc.acc_tracer.source->''.join(sourcelines)
A:torch.fx.experimental.fx_acc.acc_tracer.normalized_str->textwrap.dedent(source)
A:torch.fx.experimental.fx_acc.acc_tracer.source_ast->ast.parse(normalized_str)
A:torch.fx.experimental.fx_acc.acc_tracer.dest_ast->ast.fix_missing_locations(self.visit(source_ast))
A:torch.fx.experimental.fx_acc.acc_tracer.code->compile(dest_ast, '', 'exec')
A:torch.fx.experimental.fx_acc.acc_tracer.globals_dict->copy.copy(fn.__globals__)
A:torch.fx.experimental.fx_acc.acc_tracer.keys_before->set(globals_dict.keys())
A:torch.fx.experimental.fx_acc.acc_tracer.new_keys->list(set(globals_dict.keys()) - keys_before)
A:torch.fx.experimental.fx_acc.acc_tracer.n->ast.parse('torch._assert()', mode='eval')
A:torch.fx.experimental.fx_acc.acc_tracer.expr_wrapper->_reuse_loc(ast.Expr(_reuse_loc(exc_wrapper_call_node)))
A:torch.fx.experimental.fx_acc.acc_tracer.exc_msg->_reuse_loc(ast.Constant(None))
A:torch.fx.experimental.fx_acc.acc_tracer.exc_type->eval(name_of_exc)
A:torch.fx.experimental.fx_acc.acc_tracer.exc_wrapper_node->ast.parse(f'self.{_get_exception_wrapper_attr_name(exc_type)}()', mode='eval')
A:torch.fx.experimental.fx_acc.acc_tracer.rewritten->_rewrite(root, ast_rewriter_allow_list, self.leaf_module_list)
A:torch.fx.experimental.fx_acc.acc_tracer.allow_list->allow_list.union(DEFAULT_REWRITE_ALLOW_LIST).union(DEFAULT_REWRITE_ALLOW_LIST)
A:torch.fx.experimental.fx_acc.acc_tracer.leaf_module_list->set()
A:torch.fx.experimental.fx_acc.acc_tracer.method->getattr(base_class, method_name, None)
A:torch.fx.experimental.fx_acc.acc_tracer.(vars()[method_name], added_wrappers)->Acc_Rewriter().rewrite(method)
A:torch.fx.experimental.fx_acc.acc_tracer.wrapper_name->_get_exception_wrapper_attr_name(exc_type)
A:torch.fx.experimental.fx_acc.acc_tracer.self._modules[mod_k]->rewrite_module(mod_v)
A:torch.fx.experimental.fx_acc.acc_tracer.(rewritten_graph, rewritten_mod)->AccRewritingTracer().trace(mod, ast_rewriter_allow_list=ast_rewriter_allow_list, leaf_module_list=leaf_module_list)
A:torch.fx.experimental.fx_acc.acc_tracer.traced->NormalizeArgs(traced, normalize_to_only_use_kwargs=False).transform()
torch.fx.experimental.fx_acc.acc_tracer.AccRewritingTracer(Tracer)
torch.fx.experimental.fx_acc.acc_tracer.AccRewritingTracer.is_leaf_module(self,m:nn.Module,mod_qual_name:str)->bool
torch.fx.experimental.fx_acc.acc_tracer.AccRewritingTracer.trace(self,root:nn.Module,concrete_args:Optional[Dict[str,Any]]=None,ast_rewriter_allow_list:Optional[Set]=None,leaf_module_list:Optional[Set]=None)->Tuple[Graph, nn.Module]
torch.fx.experimental.fx_acc.acc_tracer.Acc_Rewriter(self)
torch.fx.experimental.fx_acc.acc_tracer.Acc_Rewriter.__init__(self)
torch.fx.experimental.fx_acc.acc_tracer.Acc_Rewriter.rewrite(self,fn:FunctionType)->Tuple[FunctionType, Set[Type[Exception]]]
torch.fx.experimental.fx_acc.acc_tracer.Acc_Rewriter.visit_Assert(self,node:ast.Assert)
torch.fx.experimental.fx_acc.acc_tracer.Acc_Rewriter.visit_If(self,if_node:ast.If)
torch.fx.experimental.fx_acc.acc_tracer.ConditionalExceptionWrapper(self,exc:Type[Exception])
torch.fx.experimental.fx_acc.acc_tracer.ConditionalExceptionWrapper.__init__(self,exc:Type[Exception])
torch.fx.experimental.fx_acc.acc_tracer.ConditionalExceptionWrapper.forward(self,cond:bool,msg:str)
torch.fx.experimental.fx_acc.acc_tracer._get_exception_wrapper_attr_name(exc_type:Type[Exception])->str
torch.fx.experimental.fx_acc.acc_tracer._remove_assertions(gm:torch.fx.GraphModule)->bool
torch.fx.experimental.fx_acc.acc_tracer._remove_exceptions(gm:torch.fx.GraphModule)->bool
torch.fx.experimental.fx_acc.acc_tracer._rewrite(mod_to_rewrite:nn.Module,allow_list:Optional[Set]=None,leaf_module_list:Optional[Set]=None)->nn.Module
torch.fx.experimental.fx_acc.acc_tracer.trace(mod:nn.Module,sample_inputs:List[torch.Tensor],remove_assertions:bool=True,remove_exceptions:bool=True,use_acc_normalization:bool=True,ast_rewriter_allow_list:Optional[Set[Type[nn.Module]]]=None,leaf_module_list:Optional[Set[Type[nn.Module]]]=None)->torch.fx.GraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/experimental/fx_acc/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/graph_drawer.py----------------------------------------
A:torch.fx.passes.graph_drawer.leaf_node->self._get_leaf_node(graph_module, node)
A:torch.fx.passes.graph_drawer.self._dot_graphs[f'{name}_{node.target}']->self._to_dot(leaf_node, f'{name}_{node.target}', ignore_getattr)
A:torch.fx.passes.graph_drawer.target_name->node._pretty_print_target(node.target)
A:torch.fx.passes.graph_drawer.target_hash->int(hashlib.md5(target_name.encode()).hexdigest()[:8], 16)
A:torch.fx.passes.graph_drawer.atoms->node.target.split('.')
A:torch.fx.passes.graph_drawer.py_obj->getattr(py_obj, atom)
A:torch.fx.passes.graph_drawer.leaf_module->self._get_leaf_node(graph_module, node)
A:torch.fx.passes.graph_drawer.extra->'\\l'.join([f'{c}: {getattr(leaf_module, c)}' for c in leaf_module.__constants__])
A:torch.fx.passes.graph_drawer.tensor_meta->node.meta.get('tensor_meta')
A:torch.fx.passes.graph_drawer.dot_graph->pydot.Dot(name, rankdir='TB')
A:torch.fx.passes.graph_drawer.style->self._get_node_style(node)
A:torch.fx.passes.graph_drawer.dot_node->pydot.Node(node.name, label=self._get_node_label(graph_module, node), **style)
A:torch.fx.passes.graph_drawer.dot_w_node->pydot.Node(pname1, label='{' + label1 + self._get_tensor_label(ptensor) + '}', **_WEIGHT_TEMPLATE)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/split_module.py----------------------------------------
A:torch.fx.passes.split_module.def_partition_name->getattr(def_node, '_fx_partition', None)
A:torch.fx.passes.split_module.use_partition_name->getattr(use_node, '_fx_partition', None)
A:torch.fx.passes.split_module.partition_name->str(split_callback(node))
A:torch.fx.passes.split_module.partition->partitions.get(partition_name)
A:torch.fx.passes.split_module.partitions[partition_name]partition->Partition(partition_name)
A:torch.fx.passes.split_module.root_partition->root_partitions.pop()
A:torch.fx.passes.split_module.placeholder->partitions.get(partition_name).graph.placeholder(input)
A:torch.fx.passes.split_module.placeholder.meta->orig_nodes[input].meta.copy()
A:torch.fx.passes.split_module.gathered_args->torch.fx.graph.map_arg(node.args, lambda n: environment[n])
A:torch.fx.passes.split_module.gathered_kwargs->torch.fx.graph.map_arg(node.kwargs, lambda n: environment[n])
A:torch.fx.passes.split_module.target_atoms->node.target.split('.')
A:torch.fx.passes.split_module.target_attr->getattr(target_attr, atom)
A:torch.fx.passes.split_module.target->'_'.join(target_atoms)
A:torch.fx.passes.split_module.new_node->partitions.get(partition_name).graph.create_node(op=node.op, target=target, args=gathered_args, kwargs=gathered_kwargs)
A:torch.fx.passes.split_module.new_node.meta->node.meta.copy()
A:torch.fx.passes.split_module.base_mod_env[node.name]->base_mod_graph.get_attr(node.target)
A:torch.fx.passes.split_module.base_mod_env[node.name].meta->node.meta.copy()
A:torch.fx.passes.split_module.attr_val->getattr(attr_val, atom)
A:torch.fx.passes.split_module.output_vals->tuple((partition.environment[orig_nodes[name]] for name in partition.outputs))
A:torch.fx.passes.split_module.base_mod_attrs[submod_name]->torch.fx.graph_module.GraphModule(partition.targets, partition.graph)
A:torch.fx.passes.split_module.output_val->base_mod_graph.call_module(submod_name, tuple((base_mod_env[name] for name in partition.inputs)))
A:torch.fx.passes.split_module.output_val_proxy->torch.fx.proxy.Proxy(output_val)
torch.fx.passes.split_module.Partition(self,name:str)
torch.fx.passes.split_module.Partition.__init__(self,name:str)
torch.fx.passes.split_module.Partition.__repr__(self)->str
torch.fx.passes.split_module.split_module(m:GraphModule,root_m:torch.nn.Module,split_callback:Callable[[torch.fx.node.Node],int])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/param_fetch.py----------------------------------------
A:torch.fx.passes.param_fetch.attrs_for_lowering['name']->torch.typename(mod)
A:torch.fx.passes.param_fetch.attrs_for_lowering[attr]->getattr(mod, matching_method(attr, mod._version))
A:torch.fx.passes.param_fetch.submodules->dict(fx_module.named_modules())
A:torch.fx.passes.param_fetch.node.attrs_for_lowering->extract_attrs_for_lowering(submodules[node.target])
torch.fx.passes.param_fetch.default_matching(name:str,target_version:int)->str
torch.fx.passes.param_fetch.extract_attrs_for_lowering(mod:nn.Module)->Dict[str, Any]
torch.fx.passes.param_fetch.lift_lowering_attrs_to_nodes(fx_module:GraphModule)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/splitter_base.py----------------------------------------
A:torch.fx.passes.splitter_base._LOGGER->logging.getLogger(__name__)
A:torch.fx.passes.splitter_base.parser->argparse.ArgumentParser()
A:torch.fx.passes.splitter_base.(args, unknown)->argparse.ArgumentParser().parse_known_args()
A:torch.fx.passes.splitter_base.node->next((n for n in current_nodes if self.deps[n] <= visited_nodes), None)
A:torch.fx.passes.splitter_base.submodules->dict(self.module.named_modules())
A:torch.fx.passes.splitter_base.self.acc_nodes->FxNetAccNodesFinder(self.module, self.operator_support, self.settings.allow_non_tensor)()
A:torch.fx.passes.splitter_base.self.fusions->FxNetAccFusionsFinder(module, self.acc_nodes)()
A:torch.fx.passes.splitter_base.self.deps->self.find_deps()
A:torch.fx.passes.splitter_base.template->super()._get_node_style(node)
A:torch.fx.passes.splitter_base.drawer->FxGraphDrawer(split_mod, 'preview', ignore_getattr=True)
A:torch.fx.passes.splitter_base.dot_graph->FxGraphDrawer(split_mod, 'preview', ignore_getattr=True).get_main_dot_graph()
A:torch.fx.passes.splitter_base.supported_node_types->defaultdict(set)
A:torch.fx.passes.splitter_base.unsupported_node_types->defaultdict(set)
A:torch.fx.passes.splitter_base.tensor_meta->arg.meta.get('tensor_meta')
A:torch.fx.passes.splitter_base.target->get_node_target(submodules, node)
A:torch.fx.passes.splitter_base.arg_dtypes_tuple->tuple(arg_dtypes[:last_index])
A:torch.fx.passes.splitter_base.kwarg_dtypes_tuple->tuple(((k, get_dtype(arg)) for (k, arg) in node.kwargs.items() if isinstance(arg, torch.fx.Node)))
A:torch.fx.passes.splitter_base.subgraphs->self.remove_small_acc_subgraphs(subgraphs)
A:torch.fx.passes.splitter_base.acc_subgraphs_num->len([g for g in subgraphs if g.is_acc])
A:torch.fx.passes.splitter_base.split_mod->self.split(remove_tag=True)
A:torch.fx.passes.splitter_base.dot_graphs->FxGraphDrawer(split_mod, 'preview', ignore_getattr=True).get_all_dot_graphs()
A:torch.fx.passes.splitter_base.submod->getattr(split_mod, node.target)
A:torch.fx.passes.splitter_base.handle->getattr(split_mod, node.target).register_forward_pre_hook(get_inputs)
A:torch.fx.passes.splitter_base.submod_inputs->get_submod_inputs(split_mod, submod, self.sample_input)
A:torch.fx.passes.splitter_base.lowered_submod->self._lower_model_to_backend(submod, submod_inputs)
A:torch.fx.passes.splitter_base.processed_node->set()
A:torch.fx.passes.splitter_base.new_dep->set()
A:torch.fx.passes.splitter_base.parent_nodes->self.find_parent_nodes_of_subgraph(tag)
A:torch.fx.passes.splitter_base.deps->self.find_reverse_deps(tag_id=int(tag.split('_')[-1]))
A:torch.fx.passes.splitter_base.(current_cpu_nodes, current_acc_nodes)->self.starter_nodes()
A:torch.fx.passes.splitter_base.split_module->split_by_tags(self.module, self.tags)
torch.fx.passes.splitter_base.FxNetAccNodesFinder(self,module:torch.fx.GraphModule,operator_support:OperatorSupportBase,allow_non_tensor:bool)
torch.fx.passes.splitter_base.FxNetAccNodesFinder.__init__(self,module:torch.fx.GraphModule,operator_support:OperatorSupportBase,allow_non_tensor:bool)
torch.fx.passes.splitter_base.FxNetAccNodesFinder.reduce_acc_nodes_non_tensor_input(self)
torch.fx.passes.splitter_base.FxNetAccNodesFinder.reduce_acc_nodes_non_tensor_input_helper(self,cpu_worklist:NodeList)
torch.fx.passes.splitter_base.FxNetAccNodesFinder.reduce_acc_nodes_non_tensor_output(self)
torch.fx.passes.splitter_base.FxNetSplitterInternalError(Exception)
torch.fx.passes.splitter_base.Subgraph
torch.fx.passes.splitter_base._SplitterBase(self,module:torch.fx.GraphModule,sample_input:Tensors,operator_support:OperatorSupportBase,settings:_SplitterSettingBase,non_acc_submodule_name:str='_run_on_cpu_')
torch.fx.passes.splitter_base._SplitterBase.__init__(self,module:torch.fx.GraphModule,sample_input:Tensors,operator_support:OperatorSupportBase,settings:_SplitterSettingBase,non_acc_submodule_name:str='_run_on_cpu_')
torch.fx.passes.splitter_base._SplitterBase._draw_graph_based_on_node_support(self,mod:torch.fx.GraphModule,supported_nodes:NodeList)
torch.fx.passes.splitter_base._SplitterBase._find_culprit(self,mod:torch.fx.GraphModule,inputs:Tensors)->str
torch.fx.passes.splitter_base._SplitterBase._lower_model_to_backend(self,mod:torch.fx.GraphModule,inputs:Tensors)->torch.nn.Module
torch.fx.passes.splitter_base._SplitterBase.extend_acc_subgraph(self,tag:str)
torch.fx.passes.splitter_base._SplitterBase.find_deps(self)->Dict[torch.fx.Node, NodeSet]
torch.fx.passes.splitter_base._SplitterBase.find_parent_nodes_of_subgraph(self,tag:str)->NodeSet
torch.fx.passes.splitter_base._SplitterBase.find_reverse_deps(self,tag_id:Optional[int]=None)->Dict[torch.fx.Node, NodeSet]
torch.fx.passes.splitter_base._SplitterBase.node_support_preview(self,dump_graph:bool=False)
torch.fx.passes.splitter_base._SplitterBase.put_nodes_into_subgraphs(self)->List[Subgraph]
torch.fx.passes.splitter_base._SplitterBase.remove_small_acc_subgraphs(self,subgraphs:List[Subgraph])->List[Subgraph]
torch.fx.passes.splitter_base._SplitterBase.split(self,remove_tag:bool=False)->torch.fx.GraphModule
torch.fx.passes.splitter_base._SplitterBase.split_preview(self,dump_graph:bool=False)
torch.fx.passes.splitter_base._SplitterBase.starter_nodes(self)->Tuple[NodeSet, NodeSet]
torch.fx.passes.splitter_base._SplitterBase.tag(self,subgraphs:List[Subgraph])
torch.fx.passes.splitter_base._SplitterBase.update_deps_for_fusions(self)
torch.fx.passes.splitter_base._SplitterBase.update_reverse_deps_for_fusions(self,deps:Dict[torch.fx.Node,NodeSet])
torch.fx.passes.splitter_base._SplitterSettingBase(self)
torch.fx.passes.splitter_base._SplitterSettingBase.__init__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/operator_support.py----------------------------------------
A:torch.fx.passes.operator_support.target->get_node_target(submodules, node)
A:torch.fx.passes.operator_support.arg_dtype->_get_arg_dtype(arg)
A:torch.fx.passes.operator_support.kwarg_dtype->_get_arg_dtype(node.kwargs[k])
A:torch.fx.passes.operator_support.tensor_meta->arg.meta.get('tensor_meta')
torch.fx.passes.operator_support.OpSupports
torch.fx.passes.operator_support.OpSupports.decline_if_input_dtype(cls,dtype:torch.dtype)->OperatorSupportBase
torch.fx.passes.operator_support.OpSupports.decline_if_node_in_names(cls,disallow_set:t.Set[str])->OperatorSupportBase
torch.fx.passes.operator_support.OperatorSupport(self,support_dict:t.Optional[SupportDict]=None)
torch.fx.passes.operator_support.OperatorSupport.__init__(self,support_dict:t.Optional[SupportDict]=None)
torch.fx.passes.operator_support.OperatorSupport.is_node_supported(self,submodules:t.Mapping[str,torch.nn.Module],node:torch.fx.Node)->bool
torch.fx.passes.operator_support.OperatorSupportBase(abc.ABC)
torch.fx.passes.operator_support.OperatorSupportBase.is_node_supported(self,submodules:t.Mapping[str,torch.nn.Module],node:torch.fx.Node)->bool
torch.fx.passes.operator_support._get_arg_dtype(arg:torch.fx.Node)->t.Any
torch.fx.passes.operator_support.chain(*op_support:OperatorSupportBase)->OperatorSupportBase
torch.fx.passes.operator_support.create_op_support(is_node_supported:IsNodeSupported)->OperatorSupportBase


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/tools_common.py----------------------------------------
A:torch.fx.passes.tools_common.submod_type->getattr(submod, '_base_class_origin', type(submod))
A:torch.fx.passes.tools_common.type_->fusion_group.nodes_need_process.pop().meta.get('type', None)
A:torch.fx.passes.tools_common.self.nodes->list(module.graph.nodes)
A:torch.fx.passes.tools_common.acc_nodes->list(self.acc_nodes)
A:torch.fx.passes.tools_common.node->fusion_group.nodes_need_process.pop()
A:torch.fx.passes.tools_common.fusion_group.top_node_idx->min(fusion_group.top_node_idx, self.nodes.index(arg))
torch.fx.passes.tools_common.FxNetAccFusionsFinder(self,module:torch.fx.GraphModule,acc_nodes:NodeSet)
torch.fx.passes.tools_common.FxNetAccFusionsFinder.FusionGroup
torch.fx.passes.tools_common.FxNetAccFusionsFinder.FusionGroup.add_node(self,node)
torch.fx.passes.tools_common.FxNetAccFusionsFinder.__init__(self,module:torch.fx.GraphModule,acc_nodes:NodeSet)
torch.fx.passes.tools_common.FxNetAccFusionsFinder.recursive_add_node(self,fusion_group:'FxNetAccFusionsFinder.FusionGroup',inputs:Union[NodeSet,NodeList])
torch.fx.passes.tools_common.get_acc_ops_name(k)
torch.fx.passes.tools_common.get_node_target(submodules:Mapping[str,torch.nn.Module],node:torch.fx.Node)->str
torch.fx.passes.tools_common.is_node_output_tensor(node:torch.fx.Node)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/graph_manipulation.py----------------------------------------
A:torch.fx.passes.graph_manipulation.new_graph->Graph()
A:torch.fx.passes.graph_manipulation.args->map_arg(node.args, lambda n: val_map[n])
A:torch.fx.passes.graph_manipulation.kwargs->map_arg(node.kwargs, lambda n: val_map[n])
A:torch.fx.passes.graph_manipulation.val_map[node]->Graph().node_copy(node, lambda n: val_map[n])
A:torch.fx.passes.graph_manipulation.node.size_bytes->get_size_of_node(fx_module, node)
A:torch.fx.passes.graph_manipulation.tensor_meta->get_tensor_meta(node)
A:torch.fx.passes.graph_manipulation.submodule_dict->dict(fx_module.named_modules())
A:torch.fx.passes.graph_manipulation.parameters->submodule.named_parameters()
A:torch.fx.passes.graph_manipulation.output_elem->get_tensor_meta(node).shape.numel()
A:torch.fx.passes.graph_manipulation.size_per_elem_bytes->torch.tensor([], dtype=tensor_meta.dtype).element_size()
A:torch.fx.passes.graph_manipulation.scheme['qscheme']->str(tensor.qscheme())
A:torch.fx.passes.graph_manipulation.scheme['q_scale']->tensor.q_scale()
A:torch.fx.passes.graph_manipulation.scheme['q_zero_point']->tensor.q_zero_point()
A:torch.fx.passes.graph_manipulation.weights[f'{pcq_prefix}_per_channel_scales']->tensor.q_per_channel_scales().float()
A:torch.fx.passes.graph_manipulation.weights[f'{pcq_prefix}_per_channel_zero_points']->tensor.q_per_channel_zero_points().int()
A:torch.fx.passes.graph_manipulation.scheme['q_per_channel_axis']->tensor.q_per_channel_axis()
A:torch.fx.passes.graph_manipulation.weight_dict[name]['dtype']->str(tensor.dtype)
A:torch.fx.passes.graph_manipulation.weight_dict[name]['shape']->serialize_shape(tensor.shape)
A:torch.fx.passes.graph_manipulation.weight_dict[name]['requires_grad']->str(tensor.requires_grad)
A:torch.fx.passes.graph_manipulation.weight_dict[name]['stride']->serialize_stride(tensor.stride())
A:torch.fx.passes.graph_manipulation.(quantization_info, per_channel_dict)->serialize_tensor_quantization(tensor, weights, name)
A:torch.fx.passes.graph_manipulation.parameters[p_name]->str(p_value)
A:torch.fx.passes.graph_manipulation.submodules->dict(fx_module.named_modules())
A:torch.fx.passes.graph_manipulation.weight_dict->serialize_weight(p, weights, prefix + name)
A:torch.fx.passes.graph_manipulation.node_rep['qscheme']->str(tensor_meta.qparams['qscheme'])
A:torch.fx.passes.graph_manipulation.lowering_info->node.meta.get('lowering_info')
A:torch.fx.passes.graph_manipulation.serialized_module->serialize_module(getattr(fx_module, node.target), weights, node.target)
A:torch.fx.passes.graph_manipulation.node_rep['parameters']->serialize_leaf_module(node, serialized_dict['weights'], weights, prefix + node.target)
A:torch.fx.passes.graph_manipulation.node_rep['target']->str(node.target)
A:torch.fx.passes.graph_manipulation.weight->serialize_weight(target, weights, qualname)
A:torch.fx.passes.graph_manipulation.(submod_path, _, target_name)->node.target.rpartition('.')
A:torch.fx.passes.graph_manipulation.target->getattr(submod, target_name, None)
A:torch.fx.passes.graph_manipulation.node_rep['args']->map_aggregate(node.args, get_arg_info)
A:torch.fx.passes.graph_manipulation.node_rep['kwargs']->map_aggregate(node.kwargs, get_arg_info)
A:torch.fx.passes.graph_manipulation.node_rep['users']->map_aggregate(list(node.users.keys()), get_user_info)
torch.fx.passes.graph_manipulation.get_size_of_all_nodes(fx_module:GraphModule,args:Optional[List[torch.Tensor]]=None)->None
torch.fx.passes.graph_manipulation.get_size_of_node(fx_module:GraphModule,node:Node)->size_bytes
torch.fx.passes.graph_manipulation.get_tensor_meta(node:Node)->Any
torch.fx.passes.graph_manipulation.replace_target_nodes_with(fx_module:GraphModule,old_op:str,old_target:Target,new_op:str,new_target:Target)
torch.fx.passes.graph_manipulation.serialize_leaf_module(node:Node,weights_metadata:Dict,weights:Dict,name_prefix:str)->Dict
torch.fx.passes.graph_manipulation.serialize_module(fx_module:GraphModule,weights:Dict,name_prefix='')->Dict
torch.fx.passes.graph_manipulation.serialize_shape(shape:torch.Size)->str
torch.fx.passes.graph_manipulation.serialize_stride(stride:Tuple[int])->str
torch.fx.passes.graph_manipulation.serialize_tensor_quantization(tensor:torch.Tensor,weights:Dict,pcq_prefix:str)->Tuple[Dict, Dict]
torch.fx.passes.graph_manipulation.serialize_weight(tensor:torch.Tensor,weights:Dict,name:str)->Dict
torch.fx.passes.graph_manipulation.size_bytes(NamedTuple)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/shape_prop.py----------------------------------------
A:torch.fx.passes.shape_prop.stride->super().run_node(n).stride()
A:torch.fx.passes.shape_prop.qscheme->super().run_node(n).qscheme()
A:torch.fx.passes.shape_prop.qparams['scale']->super().run_node(n).q_per_channel_scales().tolist()
A:torch.fx.passes.shape_prop.qparams['zero_point']->super().run_node(n).q_per_channel_zero_points().tolist()
A:torch.fx.passes.shape_prop.qparams['axis']->super().run_node(n).q_per_channel_axis()
A:torch.fx.passes.shape_prop.result->super().run_node(n)
A:torch.fx.passes.shape_prop.meta->map_aggregate(result, extract_tensor_meta)
A:torch.fx.passes.shape_prop.n.meta['type']->type(result)
torch.fx.passes.shape_prop.ShapeProp(torch.fx.Interpreter)
torch.fx.passes.shape_prop.ShapeProp.propagate(self,*args)
torch.fx.passes.shape_prop.ShapeProp.run_node(self,n:Node)->Any
torch.fx.passes.shape_prop.TensorMetadata(NamedTuple)
torch.fx.passes.shape_prop._extract_tensor_metadata(result:torch.Tensor)->TensorMetadata


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/split_utils.py----------------------------------------
A:torch.fx.passes.split_utils.main_g->torch.fx.Graph()
A:torch.fx.passes.split_utils.comp->Component(torch.fx.Graph(), len(all_components), f'{tag}')
A:torch.fx.passes.split_utils.main_remapping[node]->torch.fx.Graph().placeholder(node.name, type_expr=node.type)
A:torch.fx.passes.split_utils.mx->max((c.order for c in upstream_components), default=0)
A:torch.fx.passes.split_utils.comp.getattr_maps[x]->Component(torch.fx.Graph(), len(all_components), f'{tag}').graph.get_attr(x.target, type_expr=x.type)
A:torch.fx.passes.split_utils.n->Component(torch.fx.Graph(), len(all_components), f'{tag}').graph.node_copy(node, remap_func)
A:torch.fx.passes.split_utils.main_remapping[x]->torch.fx.Graph().get_attr(x.name, type_expr=x.type)
A:torch.fx.passes.split_utils.outs->tuple(map(node_remapping.__getitem__, comp.orig_outputs))
A:torch.fx.passes.split_utils.root->HolderModule({})
A:torch.fx.passes.split_utils.target_name_parts->target.split('.')
A:torch.fx.passes.split_utils.curr->getattr(curr, name)
A:torch.fx.passes.split_utils.orig_gm->getattr(orig_gm, name)
A:torch.fx.passes.split_utils.leaf_node->getattr(orig_gm, leaf_node_name)
A:torch.fx.passes.split_utils.comp.gm->torch.fx.GraphModule(root, comp.graph)
A:torch.fx.passes.split_utils.main_node->torch.fx.Graph().call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)
A:torch.fx.passes.split_utils.main_root->HolderModule({comp.name: comp.gm for comp in all_components})
torch.fx.passes.split_utils.Component
torch.fx.passes.split_utils.HolderModule(self,d)
torch.fx.passes.split_utils.HolderModule.__init__(self,d)
torch.fx.passes.split_utils.split_by_tags(gm:torch.fx.GraphModule,tags:List[str])->torch.fx.GraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fx/passes/net_min_base.py----------------------------------------
A:torch.fx.passes.net_min_base._LOGGER->logging.getLogger(__name__)
A:torch.fx.passes.net_min_base.parser->argparse.ArgumentParser()
A:torch.fx.passes.net_min_base.(args, unknown)->argparse.ArgumentParser().parse_known_args()
A:torch.fx.passes.net_min_base.self.fusions->FxNetAccFusionsFinder(self.module, callable_nodes)()
A:torch.fx.passes.net_min_base.output_node->next((node for node in submodule.graph.nodes if node.op == 'output'))
A:torch.fx.passes.net_min_base.submodule->getattr(split_module, submod_name)
A:torch.fx.passes.net_min_base.handle->getattr(split_module, submod_name).register_forward_pre_hook(get_inputs)
A:torch.fx.passes.net_min_base.split_module->split_by_tags(self.module, ['main_0', 'minimize', 'main_1'])
A:torch.fx.passes.net_min_base.(a_input, b_input)->self._get_submod_inputs(split_module, submod_name)
A:torch.fx.passes.net_min_base.result_key->map_arg(node.args, lambda x: x.name)
A:torch.fx.passes.net_min_base.a_result->self.run_a(submodule, a_input)
A:torch.fx.passes.net_min_base.b_result->self.run_b(submodule, b_input)
A:torch.fx.passes.net_min_base.(numeric_result, bool_result)->self.compare_fn(a_result, b_result, names)
A:torch.fx.passes.net_min_base.(split_module, submod_name)->self._build_submodule(cur_nodes)
A:torch.fx.passes.net_min_base.culprits->self._binary_search_impl(nodes[:mid])
A:torch.fx.passes.net_min_base.nodes->self._collect_nodes(start, end)
A:torch.fx.passes.net_min_base.cur_nodes->set(nodes)
torch.fx.passes.net_min_base.FxNetMinimizerBadModuleError(Exception)
torch.fx.passes.net_min_base.FxNetMinimizerResultMismatchError(Exception)
torch.fx.passes.net_min_base.FxNetMinimizerRunFuncError(Exception)
torch.fx.passes.net_min_base._MinimizerBase(self,module:torch.fx.GraphModule,sample_input:Tensors,compare_fn:Callable[[TensorOrTensors,TensorOrTensors,Names],Tuple[float,bool]],settings:_MinimizerSettingBase)
torch.fx.passes.net_min_base._MinimizerBase.__init__(self,module:torch.fx.GraphModule,sample_input:Tensors,compare_fn:Callable[[TensorOrTensors,TensorOrTensors,Names],Tuple[float,bool]],settings:_MinimizerSettingBase)
torch.fx.passes.net_min_base._MinimizerBase._accumulate_traverse(self,nodes:NodeList)->NodeSet
torch.fx.passes.net_min_base._MinimizerBase._binary_search_impl(self,nodes:NodeList)->NodeSet
torch.fx.passes.net_min_base._MinimizerBase._binary_traverse(self,nodes:NodeList)->NodeSet
torch.fx.passes.net_min_base._MinimizerBase._build_submodule(self,nodes:NodeSet)->Tuple[torch.fx.GraphModule, str]
torch.fx.passes.net_min_base._MinimizerBase._collect_nodes(self,start:Optional[str],end:Optional[str])->NodeList
torch.fx.passes.net_min_base._MinimizerBase._get_submod_inputs(self,main_module:torch.fx.GraphModule,submod_path:str)->Tuple[Tensors, Tensors]
torch.fx.passes.net_min_base._MinimizerBase._run_and_compare(self,split_module:torch.fx.GraphModule,submod_name:str,output_names:Names)
torch.fx.passes.net_min_base._MinimizerBase._sequential_traverse(self,nodes:NodeList)->NodeSet
torch.fx.passes.net_min_base._MinimizerBase._store_outputs(self,a_result:TensorOrTensors,b_result:TensorOrTensors,submodule:torch.fx.GraphModule)
torch.fx.passes.net_min_base._MinimizerBase._tag_nodes(self,selected_nodes:NodeSet)
torch.fx.passes.net_min_base._MinimizerBase.minimize(self,start:Optional[str]=None,end:Optional[str]=None)->NodeSet
torch.fx.passes.net_min_base._MinimizerBase.run_a(self,mod:torch.fx.GraphModule,inputs:Tensors)->TensorOrTensors
torch.fx.passes.net_min_base._MinimizerBase.run_b(self,mod:torch.fx.GraphModule,inputs:Tensors)->TensorOrTensors
torch.fx.passes.net_min_base._MinimizerBase.run_nodes(self,start:Optional[str]=None,end:Optional[str]=None)
torch.fx.passes.net_min_base._MinimizerSettingBase(self)
torch.fx.passes.net_min_base._MinimizerSettingBase.__init__(self)
torch.fx.passes.net_min_base._MinimizerSettingBase.__str__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/for_onnx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/fft/__init__.py----------------------------------------
A:torch.fft.__init__.fft->_add_docstr(_fft.fft_fft, '\nfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor\n\nComputes the one dimensional discrete Fourier transform of :attr:`input`.\n\nNote:\n\n    The Fourier domain representation of any real signal satisfies the\n    Hermitian property: `X[i] = conj(X[-i])`. This function always returns both\n    the positive and negative frequency terms even though, for real inputs, the\n    negative frequencies are redundant. :func:`~torch.fft.rfft` returns the\n    more compact one-sided representation where only the positive frequencies\n    are returned.\n\nArgs:\n    input (Tensor): the input tensor\n    n (int, optional): Signal length. If given, the input will either be zero-padded\n        or trimmed to this length before computing the FFT.\n    dim (int, optional): The dimension along which to take the one dimensional FFT.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.fft`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the FFT orthonormal)\n\n        Calling the backward transform (:func:`~torch.fft.ifft`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ifft`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.arange(4)\n    >>> t\n    tensor([0, 1, 2, 3])\n    >>> torch.fft.fft(t)\n    tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n\n    >>> t = torch.tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\n    >>> torch.fft.fft(t)\n    tensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\n'.format(**common_args))
A:torch.fft.__init__.ifft->_add_docstr(_fft.fft_ifft, '\nifft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor\n\nComputes the one dimensional inverse discrete Fourier transform of :attr:`input`.\n\nArgs:\n    input (Tensor): the input tensor\n    n (int, optional): Signal length. If given, the input will either be zero-padded\n        or trimmed to this length before computing the IFFT.\n    dim (int, optional): The dimension along which to take the one dimensional IFFT.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.ifft`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the IFFT orthonormal)\n\n        Calling the forward transform (:func:`~torch.fft.fft`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ifft`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n    >>> torch.fft.ifft(t)\n    tensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])\n'.format(**common_args))
A:torch.fft.__init__.fft2->_add_docstr(_fft.fft_fft2, '\nfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor\n\nComputes the 2 dimensional discrete Fourier transform of :attr:`input`.\nEquivalent to :func:`~torch.fft.fftn` but FFTs only the last two dimensions by default.\n\nNote:\n    The Fourier domain representation of any real signal satisfies the\n    Hermitian property: ``X[i, j] = conj(X[-i, -j])``. This\n    function always returns all positive and negative frequency terms even\n    though, for real inputs, half of these values are redundant.\n    :func:`~torch.fft.rfft2` returns the more compact one-sided representation\n    where only the positive frequencies of the last dimension are returned.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: last two dimensions.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.fft2`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the FFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical FFT size.\n        Calling the backward transform (:func:`~torch.fft.ifft2`) with the same\n        normalization mode will apply an overall normalization of ``1/n``\n        between the two transforms. This is required to make\n        :func:`~torch.fft.ifft2` the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> x = torch.rand(10, 10, dtype=torch.complex64)\n    >>> fft2 = torch.fft.fft2(x)\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.fft2`\n    here is equivalent to two one-dimensional :func:`~torch.fft.fft` calls:\n\n    >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n    >>> torch.testing.assert_close(fft2, two_ffts, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.ifft2->_add_docstr(_fft.fft_ifft2, '\nifft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor\n\nComputes the 2 dimensional inverse discrete Fourier transform of :attr:`input`.\nEquivalent to :func:`~torch.fft.ifftn` but IFFTs only the last two dimensions by default.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the IFFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: last two dimensions.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.ifft2`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the IFFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical IFFT size.\n        Calling the forward transform (:func:`~torch.fft.fft2`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ifft2`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> x = torch.rand(10, 10, dtype=torch.complex64)\n    >>> ifft2 = torch.fft.ifft2(x)\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.ifft2`\n    here is equivalent to two one-dimensional :func:`~torch.fft.ifft` calls:\n\n    >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n    >>> torch.testing.assert_close(ifft2, two_iffts, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.fftn->_add_docstr(_fft.fft_fftn, '\nfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor\n\nComputes the N dimensional discrete Fourier transform of :attr:`input`.\n\nNote:\n\n    The Fourier domain representation of any real signal satisfies the\n    Hermitian property: ``X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])``. This\n    function always returns all positive and negative frequency terms even\n    though, for real inputs, half of these values are redundant.\n    :func:`~torch.fft.rfftn` returns the more compact one-sided representation\n    where only the positive frequencies of the last dimension are returned.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: all dimensions, or the last ``len(s)`` dimensions if :attr:`s` is given.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.fftn`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the FFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical FFT size.\n        Calling the backward transform (:func:`~torch.fft.ifftn`) with the same\n        normalization mode will apply an overall normalization of ``1/n``\n        between the two transforms. This is required to make\n        :func:`~torch.fft.ifftn` the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> x = torch.rand(10, 10, dtype=torch.complex64)\n    >>> fftn = torch.fft.fftn(x)\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.fftn`\n    here is equivalent to two one-dimensional :func:`~torch.fft.fft` calls:\n\n    >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n    >>> torch.testing.assert_close(fftn, two_ffts, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.ifftn->_add_docstr(_fft.fft_ifftn, '\nifftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor\n\nComputes the N dimensional inverse discrete Fourier transform of :attr:`input`.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the IFFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: all dimensions, or the last ``len(s)`` dimensions if :attr:`s` is given.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.ifftn`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the IFFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical IFFT size.\n        Calling the forward transform (:func:`~torch.fft.fftn`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ifftn`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> x = torch.rand(10, 10, dtype=torch.complex64)\n    >>> ifftn = torch.fft.ifftn(x)\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.ifftn`\n    here is equivalent to two one-dimensional :func:`~torch.fft.ifft` calls:\n\n    >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n    >>> torch.testing.assert_close(ifftn, two_iffts, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.rfft->_add_docstr(_fft.fft_rfft, '\nrfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor\n\nComputes the one dimensional Fourier transform of real-valued :attr:`input`.\n\nThe FFT of a real signal is Hermitian-symmetric, ``X[i] = conj(X[-i])`` so\nthe output contains only the positive frequencies below the Nyquist frequency.\nTo compute the full output, use :func:`~torch.fft.fft`\n\nArgs:\n    input (Tensor): the real input tensor\n    n (int, optional): Signal length. If given, the input will either be zero-padded\n        or trimmed to this length before computing the real FFT.\n    dim (int, optional): The dimension along which to take the one dimensional real FFT.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.rfft`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the FFT orthonormal)\n\n        Calling the backward transform (:func:`~torch.fft.irfft`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.irfft`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.arange(4)\n    >>> t\n    tensor([0, 1, 2, 3])\n    >>> torch.fft.rfft(t)\n    tensor([ 6.+0.j, -2.+2.j, -2.+0.j])\n\n    Compare against the full output from :func:`~torch.fft.fft`:\n\n    >>> torch.fft.fft(t)\n    tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n\n    Notice that the symmetric element ``T[-1] == T[1].conj()`` is omitted.\n    At the Nyquist frequency ``T[-2] == T[2]`` is it\'s own symmetric pair,\n    and therefore must always be real-valued.\n'.format(**common_args))
A:torch.fft.__init__.irfft->_add_docstr(_fft.fft_irfft, '\nirfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor\n\nComputes the inverse of :func:`~torch.fft.rfft`.\n\n:attr:`input` is interpreted as a one-sided Hermitian signal in the Fourier\ndomain, as produced by :func:`~torch.fft.rfft`. By the Hermitian property, the\noutput will be real-valued.\n\nNote:\n    Some input frequencies must be real-valued to satisfy the Hermitian\n    property. In these cases the imaginary component will be ignored.\n    For example, any imaginary component in the zero-frequency term cannot\n    be represented in a real output and so will always be ignored.\n\nNote:\n    The correct interpretation of the Hermitian input depends on the length of\n    the original data, as given by :attr:`n`. This is because each input shape\n    could correspond to either an odd or even length signal. By default, the\n    signal is assumed to be even length and odd signals will not round-trip\n    properly. So, it is recommended to always pass the signal length :attr:`n`.\n\nArgs:\n    input (Tensor): the input tensor representing a half-Hermitian signal\n    n (int, optional): Output signal length. This determines the length of the\n        output signal. If given, the input will either be zero-padded or trimmed to this\n        length before computing the real IFFT.\n        Defaults to even output: ``n=2*(input.size(dim) - 1)``.\n    dim (int, optional): The dimension along which to take the one dimensional real IFFT.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.irfft`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the real IFFT orthonormal)\n\n        Calling the forward transform (:func:`~torch.fft.rfft`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.irfft`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.linspace(0, 1, 5)\n    >>> t\n    tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n    >>> T = torch.fft.rfft(t)\n    >>> T\n    tensor([ 2.5000+0.0000j, -0.6250+0.8602j, -0.6250+0.2031j])\n\n    Without specifying the output length to :func:`~torch.fft.irfft`, the output\n    will not round-trip properly because the input is odd-length:\n\n    >>> torch.fft.irfft(T)\n    tensor([0.1562, 0.3511, 0.7812, 1.2114])\n\n    So, it is recommended to always pass the signal length :attr:`n`:\n\n    >>> roundtrip = torch.fft.irfft(T, t.numel())\n    >>> torch.testing.assert_close(roundtrip, t, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.rfft2->_add_docstr(_fft.fft_rfft2, '\nrfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor\n\nComputes the 2-dimensional discrete Fourier transform of real :attr:`input`.\nEquivalent to :func:`~torch.fft.rfftn` but FFTs only the last two dimensions by default.\n\nThe FFT of a real signal is Hermitian-symmetric, ``X[i, j] = conj(X[-i, -j])``,\nso the full :func:`~torch.fft.fft2` output contains redundant information.\n:func:`~torch.fft.rfft2` instead omits the negative frequencies in the last\ndimension.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the real FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: last two dimensions.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.rfft2`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the real FFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical FFT size.\n        Calling the backward transform (:func:`~torch.fft.irfft2`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.irfft2`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.rand(10, 10)\n    >>> rfft2 = torch.fft.rfft2(t)\n    >>> rfft2.size()\n    torch.Size([10, 6])\n\n    Compared against the full output from :func:`~torch.fft.fft2`, we have all\n    elements up to the Nyquist frequency.\n\n    >>> fft2 = torch.fft.fft2(t)\n    >>> torch.testing.assert_close(fft2[..., :6], rfft2, check_stride=False)\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.rfft2`\n    here is equivalent to a combination of :func:`~torch.fft.fft` and\n    :func:`~torch.fft.rfft`:\n\n    >>> two_ffts = torch.fft.fft(torch.fft.rfft(t, dim=1), dim=0)\n    >>> torch.testing.assert_close(rfft2, two_ffts, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.irfft2->_add_docstr(_fft.fft_irfft2, '\nirfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor\n\nComputes the inverse of :func:`~torch.fft.rfft2`.\nEquivalent to :func:`~torch.fft.irfftn` but IFFTs only the last two dimensions by default.\n\n:attr:`input` is interpreted as a one-sided Hermitian signal in the Fourier\ndomain, as produced by :func:`~torch.fft.rfft2`. By the Hermitian property, the\noutput will be real-valued.\n\nNote:\n    Some input frequencies must be real-valued to satisfy the Hermitian\n    property. In these cases the imaginary component will be ignored.\n    For example, any imaginary component in the zero-frequency term cannot\n    be represented in a real output and so will always be ignored.\n\nNote:\n    The correct interpretation of the Hermitian input depends on the length of\n    the original data, as given by :attr:`s`. This is because each input shape\n    could correspond to either an odd or even length signal. By default, the\n    signal is assumed to be even length and odd signals will not round-trip\n    properly. So, it is recommended to always pass the signal shape :attr:`s`.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the real FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Defaults to even output in the last dimension:\n        ``s[-1] = 2*(input.size(dim[-1]) - 1)``.\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        The last dimension must be the half-Hermitian compressed dimension.\n        Default: last two dimensions.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.irfft2`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the real IFFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical IFFT size.\n        Calling the forward transform (:func:`~torch.fft.rfft2`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.irfft2`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.rand(10, 9)\n    >>> T = torch.fft.rfft2(t)\n\n    Without specifying the output length to :func:`~torch.fft.irfft2`, the output\n    will not round-trip properly because the input is odd-length in the last\n    dimension:\n\n    >>> torch.fft.irfft2(T).size()\n    torch.Size([10, 8])\n\n    So, it is recommended to always pass the signal shape :attr:`s`.\n\n    >>> roundtrip = torch.fft.irfft2(T, t.size())\n    >>> roundtrip.size()\n    torch.Size([10, 9])\n    >>> torch.testing.assert_close(roundtrip, t, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.rfftn->_add_docstr(_fft.fft_rfftn, '\nrfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor\n\nComputes the N-dimensional discrete Fourier transform of real :attr:`input`.\n\nThe FFT of a real signal is Hermitian-symmetric,\n``X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])`` so the full\n:func:`~torch.fft.fftn` output contains redundant information.\n:func:`~torch.fft.rfftn` instead omits the negative frequencies in the\nlast dimension.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the real FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: all dimensions, or the last ``len(s)`` dimensions if :attr:`s` is given.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.rfftn`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the real FFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical FFT size.\n        Calling the backward transform (:func:`~torch.fft.irfftn`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.irfftn`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.rand(10, 10)\n    >>> rfftn = torch.fft.rfftn(t)\n    >>> rfftn.size()\n    torch.Size([10, 6])\n\n    Compared against the full output from :func:`~torch.fft.fftn`, we have all\n    elements up to the Nyquist frequency.\n\n    >>> fftn = torch.fft.fftn(t)\n    >>> torch.testing.assert_close(fftn[..., :6], rfftn, check_stride=False)\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.rfftn`\n    here is equivalent to a combination of :func:`~torch.fft.fft` and\n    :func:`~torch.fft.rfft`:\n\n    >>> two_ffts = torch.fft.fft(torch.fft.rfft(t, dim=1), dim=0)\n    >>> torch.testing.assert_close(rfftn, two_ffts, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.irfftn->_add_docstr(_fft.fft_irfftn, '\nirfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor\n\nComputes the inverse of :func:`~torch.fft.rfftn`.\n\n:attr:`input` is interpreted as a one-sided Hermitian signal in the Fourier\ndomain, as produced by :func:`~torch.fft.rfftn`. By the Hermitian property, the\noutput will be real-valued.\n\nNote:\n    Some input frequencies must be real-valued to satisfy the Hermitian\n    property. In these cases the imaginary component will be ignored.\n    For example, any imaginary component in the zero-frequency term cannot\n    be represented in a real output and so will always be ignored.\n\nNote:\n    The correct interpretation of the Hermitian input depends on the length of\n    the original data, as given by :attr:`s`. This is because each input shape\n    could correspond to either an odd or even length signal. By default, the\n    signal is assumed to be even length and odd signals will not round-trip\n    properly. So, it is recommended to always pass the signal shape :attr:`s`.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the real FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Defaults to even output in the last dimension:\n        ``s[-1] = 2*(input.size(dim[-1]) - 1)``.\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        The last dimension must be the half-Hermitian compressed dimension.\n        Default: all dimensions, or the last ``len(s)`` dimensions if :attr:`s` is given.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.irfftn`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the real IFFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical IFFT size.\n        Calling the forward transform (:func:`~torch.fft.rfftn`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.irfftn`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.rand(10, 9)\n    >>> T = torch.fft.rfftn(t)\n\n    Without specifying the output length to :func:`~torch.fft.irfft`, the output\n    will not round-trip properly because the input is odd-length in the last\n    dimension:\n\n    >>> torch.fft.irfftn(T).size()\n    torch.Size([10, 8])\n\n    So, it is recommended to always pass the signal shape :attr:`s`.\n\n    >>> roundtrip = torch.fft.irfftn(T, t.size())\n    >>> roundtrip.size()\n    torch.Size([10, 9])\n    >>> torch.testing.assert_close(roundtrip, t, check_stride=False)\n\n'.format(**common_args))
A:torch.fft.__init__.hfft->_add_docstr(_fft.fft_hfft, '\nhfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor\n\nComputes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric :attr:`input` signal.\n\nNote:\n\n    :func:`~torch.fft.hfft`/:func:`~torch.fft.ihfft` are analogous to\n    :func:`~torch.fft.rfft`/:func:`~torch.fft.irfft`. The real FFT expects\n    a real signal in the time-domain and gives a Hermitian symmetry in the\n    frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in\n    the time-domain and real-valued in the frequency-domain. For this reason,\n    special care needs to be taken with the length argument :attr:`n`, in the\n    same way as with :func:`~torch.fft.irfft`.\n\nNote:\n    Because the signal is Hermitian in the time-domain, the result will be\n    real in the frequency domain. Note that some input frequencies must be\n    real-valued to satisfy the Hermitian property. In these cases the imaginary\n    component will be ignored. For example, any imaginary component in\n    ``input[0]`` would result in one or more complex frequency terms which\n    cannot be represented in a real output and so will always be ignored.\n\nNote:\n    The correct interpretation of the Hermitian input depends on the length of\n    the original data, as given by :attr:`n`. This is because each input shape\n    could correspond to either an odd or even length signal. By default, the\n    signal is assumed to be even length and odd signals will not round-trip\n    properly. So, it is recommended to always pass the signal length :attr:`n`.\n\nArgs:\n    input (Tensor): the input tensor representing a half-Hermitian signal\n    n (int, optional): Output signal length. This determines the length of the\n        real output. If given, the input will either be zero-padded or trimmed to this\n        length before computing the Hermitian FFT.\n        Defaults to even output: ``n=2*(input.size(dim) - 1)``.\n    dim (int, optional): The dimension along which to take the one dimensional Hermitian FFT.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.hfft`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the Hermitian FFT orthonormal)\n\n        Calling the backward transform (:func:`~torch.fft.ihfft`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ihfft`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    Taking a real-valued frequency signal and bringing it into the time domain\n    gives Hermitian symmetric output:\n\n    >>> t = torch.linspace(0, 1, 5)\n    >>> t\n    tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n    >>> T = torch.fft.ifft(t)\n    >>> T\n    tensor([ 0.5000-0.0000j, -0.1250-0.1720j, -0.1250-0.0406j, -0.1250+0.0406j,\n            -0.1250+0.1720j])\n\n    Note that ``T[1] == T[-1].conj()`` and ``T[2] == T[-2].conj()`` is\n    redundant. We can thus compute the forward transform without considering\n    negative frequencies:\n\n    >>> torch.fft.hfft(T[:3], n=5)\n    tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n\n    Like with :func:`~torch.fft.irfft`, the output length must be given in order\n    to recover an even length output:\n\n    >>> torch.fft.hfft(T[:3])\n    tensor([0.1250, 0.2809, 0.6250, 0.9691])\n'.format(**common_args))
A:torch.fft.__init__.ihfft->_add_docstr(_fft.fft_ihfft, '\nihfft(input, n=None, dim=-1, norm=None, *, out=None) -> Tensor\n\nComputes the inverse of :func:`~torch.fft.hfft`.\n\n:attr:`input` must be a real-valued signal, interpreted in the Fourier domain.\nThe IFFT of a real signal is Hermitian-symmetric, ``X[i] = conj(X[-i])``.\n:func:`~torch.fft.ihfft` represents this in the one-sided form where only the\npositive frequencies below the Nyquist frequency are included. To compute the\nfull output, use :func:`~torch.fft.ifft`.\n\nArgs:\n    input (Tensor): the real input tensor\n    n (int, optional): Signal length. If given, the input will either be zero-padded\n        or trimmed to this length before computing the Hermitian IFFT.\n    dim (int, optional): The dimension along which to take the one dimensional Hermitian IFFT.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.ihfft`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the IFFT orthonormal)\n\n        Calling the forward transform (:func:`~torch.fft.hfft`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ihfft`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> t = torch.arange(5)\n    >>> t\n    tensor([0, 1, 2, 3, 4])\n    >>> torch.fft.ihfft(t)\n    tensor([ 2.0000-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j])\n\n    Compare against the full output from :func:`~torch.fft.ifft`:\n\n    >>> torch.fft.ifft(t)\n    tensor([ 2.0000-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n            -0.5000+0.6882j])\n'.format(**common_args))
A:torch.fft.__init__.hfft2->_add_docstr(_fft.fft_hfft2, '\nhfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor\n\nComputes the 2-dimensional discrete Fourier transform of a Hermitian symmetric\n:attr:`input` signal. Equivalent to :func:`~torch.fft.hfftn` but only\ntransforms the last two dimensions by default.\n\n:attr:`input` is interpreted as a one-sided Hermitian signal in the time\ndomain. By the Hermitian property, the Fourier transform will be real-valued.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the Hermitian FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Defaults to even output in the last dimension:\n        ``s[-1] = 2*(input.size(dim[-1]) - 1)``.\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        The last dimension must be the half-Hermitian compressed dimension.\n        Default: last two dimensions.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.hfft2`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the Hermitian FFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical FFT size.\n        Calling the backward transform (:func:`~torch.fft.ihfft2`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ihfft2`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    Starting from a real frequency-space signal, we can generate a\n    Hermitian-symmetric time-domain signal:\n    >>> T = torch.rand(10, 9)\n    >>> t = torch.fft.ihfft2(T)\n\n    Without specifying the output length to :func:`~torch.fft.hfftn`, the\n    output will not round-trip properly because the input is odd-length in the\n    last dimension:\n\n    >>> torch.fft.hfft2(t).size()\n    torch.Size([10, 10])\n\n    So, it is recommended to always pass the signal shape :attr:`s`.\n\n    >>> roundtrip = torch.fft.hfft2(t, T.size())\n    >>> roundtrip.size()\n    torch.Size([10, 9])\n    >>> torch.allclose(roundtrip, T)\n    True\n\n'.format(**common_args))
A:torch.fft.__init__.ihfft2->_add_docstr(_fft.fft_ihfft2, '\nihfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None) -> Tensor\n\nComputes the 2-dimensional inverse discrete Fourier transform of real\n:attr:`input`. Equivalent to :func:`~torch.fft.ihfftn` but transforms only the\ntwo last dimensions by default.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the Hermitian IFFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: last two dimensions.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.ihfft2`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the Hermitian IFFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical IFFT size.\n        Calling the forward transform (:func:`~torch.fft.hfft2`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ihfft2`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> T = torch.rand(10, 10)\n    >>> t = torch.fft.ihfft2(t)\n    >>> t.size()\n    torch.Size([10, 6])\n\n    Compared against the full output from :func:`~torch.fft.ifft2`, the\n    Hermitian time-space signal takes up only half the space.\n\n    >>> fftn = torch.fft.ifft2(t)\n    >>> torch.allclose(fftn[..., :6], rfftn)\n    True\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.ihfft2`\n    here is equivalent to a combination of :func:`~torch.fft.ifft` and\n    :func:`~torch.fft.ihfft`:\n\n    >>> two_ffts = torch.fft.ifft(torch.fft.ihfft(t, dim=1), dim=0)\n    >>> torch.allclose(t, two_ffts)\n    True\n\n'.format(**common_args))
A:torch.fft.__init__.hfftn->_add_docstr(_fft.fft_hfftn, '\nhfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor\n\nComputes the n-dimensional discrete Fourier transform of a Herimitian symmetric\n:attr:`input` signal.\n\n:attr:`input` is interpreted as a one-sided Hermitian signal in the time\ndomain. By the Hermitian property, the Fourier transform will be real-valued.\n\nNote:\n    :func:`~torch.fft.hfftn`/:func:`~torch.fft.ihfftn` are analogous to\n    :func:`~torch.fft.rfftn`/:func:`~torch.fft.irfftn`. The real FFT expects\n    a real signal in the time-domain and gives Hermitian symmetry in the\n    frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in\n    the time-domain and real-valued in the frequency-domain. For this reason,\n    special care needs to be taken with the shape argument :attr:`s`, in the\n    same way as with :func:`~torch.fft.irfftn`.\n\nNote:\n    Some input frequencies must be real-valued to satisfy the Hermitian\n    property. In these cases the imaginary component will be ignored.\n    For example, any imaginary component in the zero-frequency term cannot\n    be represented in a real output and so will always be ignored.\n\nNote:\n    The correct interpretation of the Hermitian input depends on the length of\n    the original data, as given by :attr:`s`. This is because each input shape\n    could correspond to either an odd or even length signal. By default, the\n    signal is assumed to be even length and odd signals will not round-trip\n    properly. It is recommended to always pass the signal shape :attr:`s`.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the real FFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Defaults to even output in the last dimension:\n        ``s[-1] = 2*(input.size(dim[-1]) - 1)``.\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        The last dimension must be the half-Hermitian compressed dimension.\n        Default: all dimensions, or the last ``len(s)`` dimensions if :attr:`s` is given.\n    norm (str, optional): Normalization mode. For the forward transform\n        (:func:`~torch.fft.hfftn`), these correspond to:\n\n        * ``"forward"`` - normalize by ``1/n``\n        * ``"backward"`` - no normalization\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the Hermitian FFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical FFT size.\n        Calling the backward transform (:func:`~torch.fft.ihfftn`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ihfftn`\n        the exact inverse.\n\n        Default is ``"backward"`` (no normalization).\n\nKeyword args:\n    {out}\n\nExample:\n\n    Starting from a real frequency-space signal, we can generate a\n    Hermitian-symmetric time-domain signal:\n    >>> T = torch.rand(10, 9)\n    >>> t = torch.fft.ihfftn(T)\n\n    Without specifying the output length to :func:`~torch.fft.hfftn`, the\n    output will not round-trip properly because the input is odd-length in the\n    last dimension:\n\n    >>> torch.fft.hfftn(t).size()\n    torch.Size([10, 10])\n\n    So, it is recommended to always pass the signal shape :attr:`s`.\n\n    >>> roundtrip = torch.fft.hfftn(t, T.size())\n    >>> roundtrip.size()\n    torch.Size([10, 9])\n    >>> torch.allclose(roundtrip, T)\n    True\n\n'.format(**common_args))
A:torch.fft.__init__.ihfftn->_add_docstr(_fft.fft_ihfftn, '\nihfftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor\n\nComputes the N-dimensional inverse discrete Fourier transform of real :attr:`input`.\n\n:attr:`input` must be a real-valued signal, interpreted in the Fourier domain.\nThe n-dimensional IFFT of a real signal is Hermitian-symmetric,\n``X[i, j, ...] = conj(X[-i, -j, ...])``. :func:`~torch.fft.ihfftn` represents\nthis in the one-sided form where only the positive frequencies below the\nNyquist frequency are included in the last signal dimension. To compute the\nfull output, use :func:`~torch.fft.ifftn`.\n\nArgs:\n    input (Tensor): the input tensor\n    s (Tuple[int], optional): Signal size in the transformed dimensions.\n        If given, each dimension ``dim[i]`` will either be zero-padded or\n        trimmed to the length ``s[i]`` before computing the Hermitian IFFT.\n        If a length ``-1`` is specified, no padding is done in that dimension.\n        Default: ``s = [input.size(d) for d in dim]``\n    dim (Tuple[int], optional): Dimensions to be transformed.\n        Default: all dimensions, or the last ``len(s)`` dimensions if :attr:`s` is given.\n    norm (str, optional): Normalization mode. For the backward transform\n        (:func:`~torch.fft.ihfftn`), these correspond to:\n\n        * ``"forward"`` - no normalization\n        * ``"backward"`` - normalize by ``1/n``\n        * ``"ortho"`` - normalize by ``1/sqrt(n)`` (making the Hermitian IFFT orthonormal)\n\n        Where ``n = prod(s)`` is the logical IFFT size.\n        Calling the forward transform (:func:`~torch.fft.hfftn`) with the same\n        normalization mode will apply an overall normalization of ``1/n`` between\n        the two transforms. This is required to make :func:`~torch.fft.ihfftn`\n        the exact inverse.\n\n        Default is ``"backward"`` (normalize by ``1/n``).\n\nKeyword args:\n    {out}\n\nExample:\n\n    >>> T = torch.rand(10, 10)\n    >>> ihfftn = torch.fft.ihfftn(T)\n    >>> ihfftn.size()\n    torch.Size([10, 6])\n\n    Compared against the full output from :func:`~torch.fft.ifftn`, we have all\n    elements up to the Nyquist frequency.\n\n    >>> ifftn = torch.fft.ifftn(t)\n    >>> torch.allclose(ifftn[..., :6], ihfftn)\n    True\n\n    The discrete Fourier transform is separable, so :func:`~torch.fft.ihfftn`\n    here is equivalent to a combination of :func:`~torch.fft.ihfft` and\n    :func:`~torch.fft.ifft`:\n\n    >>> two_iffts = torch.fft.ifft(torch.fft.ihfft(t, dim=1), dim=0)\n    >>> torch.allclose(ihfftn, two_iffts)\n    True\n\n'.format(**common_args))
A:torch.fft.__init__.fftfreq->_add_docstr(_fft.fft_fftfreq, "\nfftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nComputes the discrete Fourier Transform sample frequencies for a signal of size :attr:`n`.\n\nNote:\n    By convention, :func:`~torch.fft.fft` returns positive frequency terms\n    first, followed by the negative frequencies in reverse order, so that\n    ``f[-i]`` for all :math:`0 < i \\leq n/2`` in Python gives the negative\n    frequency terms. For an FFT of length :attr:`n` and with inputs spaced in\n    length unit :attr:`d`, the frequencies are::\n\n        f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)\n\nNote:\n    For even lengths, the Nyquist frequency at ``f[n/2]`` can be thought of as\n    either negative or positive. :func:`~torch.fft.fftfreq` follows NumPy's\n    convention of taking it to be negative.\n\nArgs:\n    n (int): the FFT length\n    d (float, optional): The sampling length scale.\n        The spacing between individual samples of the FFT input.\n        The default assumes unit spacing, dividing that result by the actual\n        spacing gives the result in physical frequency units.\n\nKeyword Args:\n    {out}\n    {dtype}\n    {layout}\n    {device}\n    {requires_grad}\n\nExample:\n\n    >>> torch.fft.fftfreq(5)\n    tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n\n    For even input, we can see the Nyquist frequency at ``f[2]`` is given as\n    negative:\n\n    >>> torch.fft.fftfreq(4)\n    tensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n\n".format(**factory_common_args))
A:torch.fft.__init__.rfftfreq->_add_docstr(_fft.fft_rfftfreq, '\nrfftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nComputes the sample frequencies for :func:`~torch.fft.rfft` with a signal of size :attr:`n`.\n\nNote:\n    :func:`~torch.fft.rfft` returns Hermitian one-sided output, so only the\n    positive frequency terms are returned. For a real FFT of length :attr:`n`\n    and with inputs spaced in length unit :attr:`d`, the frequencies are::\n\n        f = torch.arange((n + 1) // 2) / (d * n)\n\nNote:\n    For even lengths, the Nyquist frequency at ``f[n/2]`` can be thought of as\n    either negative or positive. Unlike :func:`~torch.fft.fftfreq`,\n    :func:`~torch.fft.rfftfreq` always returns it as positive.\n\nArgs:\n    n (int): the real FFT length\n    d (float, optional): The sampling length scale.\n        The spacing between individual samples of the FFT input.\n        The default assumes unit spacing, dividing that result by the actual\n        spacing gives the result in physical frequency units.\n\nKeyword Args:\n    {out}\n    {dtype}\n    {layout}\n    {device}\n    {requires_grad}\n\nExample:\n\n    >>> torch.fft.rfftfreq(5)\n    tensor([0.0000, 0.2000, 0.4000])\n\n    >>> torch.fft.rfftfreq(4)\n    tensor([0.0000, 0.2500, 0.5000])\n\n    Compared to the output from :func:`~torch.fft.fftfreq`, we see that the\n    Nyquist frequency at ``f[2]`` has changed sign:\n    >>> torch.fft.fftfreq(4)\n    tensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n\n'.format(**factory_common_args))
A:torch.fft.__init__.fftshift->_add_docstr(_fft.fft_fftshift, '\nfftshift(input, dim=None) -> Tensor\n\nReorders n-dimensional FFT data, as provided by :func:`~torch.fft.fftn`, to have\nnegative frequency terms first.\n\nThis performs a periodic shift of n-dimensional data such that the origin\n``(0, ..., 0)`` is moved to the center of the tensor. Specifically, to\n``input.shape[dim] // 2`` in each selected dimension.\n\nNote:\n    By convention, the FFT returns positive frequency terms first, followed by\n    the negative frequencies in reverse order, so that ``f[-i]`` for all\n    :math:`0 < i \\leq n/2` in Python gives the negative frequency terms.\n    :func:`~torch.fft.fftshift` rearranges all frequencies into ascending order\n    from negative to positive with the zero-frequency term in the center.\n\nNote:\n    For even lengths, the Nyquist frequency at ``f[n/2]`` can be thought of as\n    either negative or positive. :func:`~torch.fft.fftshift` always puts the\n    Nyquist term at the 0-index. This is the same convention used by\n    :func:`~torch.fft.fftfreq`.\n\nArgs:\n    input (Tensor): the tensor in FFT order\n    dim (int, Tuple[int], optional): The dimensions to rearrange.\n        Only dimensions specified here will be rearranged, any other dimensions\n        will be left in their original order.\n        Default: All dimensions of :attr:`input`.\n\nExample:\n\n    >>> f = torch.fft.fftfreq(4)\n    >>> f\n    tensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n\n    >>> torch.fft.fftshift(f)\n    tensor([-0.5000, -0.2500,  0.0000,  0.2500])\n\n    Also notice that the Nyquist frequency term at ``f[2]`` was moved to the\n    beginning of the tensor.\n\n    This also works for multi-dimensional transforms:\n\n    >>> x = torch.fft.fftfreq(5, d=1/5) + 0.1 * torch.fft.fftfreq(5, d=1/5).unsqueeze(1)\n    >>> x\n    tensor([[ 0.0000,  1.0000,  2.0000, -2.0000, -1.0000],\n            [ 0.1000,  1.1000,  2.1000, -1.9000, -0.9000],\n            [ 0.2000,  1.2000,  2.2000, -1.8000, -0.8000],\n            [-0.2000,  0.8000,  1.8000, -2.2000, -1.2000],\n            [-0.1000,  0.9000,  1.9000, -2.1000, -1.1000]])\n\n    >>> torch.fft.fftshift(x)\n    tensor([[-2.2000, -1.2000, -0.2000,  0.8000,  1.8000],\n            [-2.1000, -1.1000, -0.1000,  0.9000,  1.9000],\n            [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n            [-1.9000, -0.9000,  0.1000,  1.1000,  2.1000],\n            [-1.8000, -0.8000,  0.2000,  1.2000,  2.2000]])\n\n    :func:`~torch.fft.fftshift` can also be useful for spatial data. If our\n    data is defined on a centered grid (``[-(N//2), (N-1)//2]``) then we can\n    use the standard FFT defined on an uncentered grid (``[0, N)``) by first\n    applying an :func:`~torch.fft.ifftshift`.\n\n    >>> x_centered = torch.arange(-5, 5)\n    >>> x_uncentered = torch.fft.ifftshift(x_centered)\n    >>> fft_uncentered = torch.fft.fft(x_uncentered)\n\n    Similarly, we can convert the frequency domain components to centered\n    convention by applying :func:`~torch.fft.fftshift`.\n\n    >>> fft_centered = torch.fft.fftshift(fft_uncentered)\n\n    The inverse transform, from centered Fourier space back to centered spatial\n    data, can be performed by applying the inverse shifts in reverse order:\n\n    >>> x_centered_2 = torch.fft.fftshift(torch.fft.ifft(torch.fft.ifftshift(fft_centered)))\n    >>> torch.testing.assert_close(x_centered.to(torch.complex64), x_centered_2, check_stride=False)\n\n\n')
A:torch.fft.__init__.ifftshift->_add_docstr(_fft.fft_ifftshift, '\nifftshift(input, dim=None) -> Tensor\n\nInverse of :func:`~torch.fft.fftshift`.\n\nArgs:\n    input (Tensor): the tensor in FFT order\n    dim (int, Tuple[int], optional): The dimensions to rearrange.\n        Only dimensions specified here will be rearranged, any other dimensions\n        will be left in their original order.\n        Default: All dimensions of :attr:`input`.\n\nExample:\n\n    >>> f = torch.fft.fftfreq(5)\n    >>> f\n    tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n\n    A round-trip through :func:`~torch.fft.fftshift` and\n    :func:`~torch.fft.ifftshift` gives the same result:\n\n    >>> shifted = torch.fft.fftshift(f)\n    >>> torch.fft.ifftshift(shifted)\n    tensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n\n')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/sparse/__init__.py----------------------------------------
A:torch.sparse.__init__.addmm->_add_docstr(_sparse._sparse_addmm, '\nsparse.addmm(mat, mat1, mat2, *, beta=1., alpha=1.) -> Tensor\n\nThis function does exact same thing as :func:`torch.addmm` in the forward,\nexcept that it supports backward for sparse matrix :attr:`mat1`. :attr:`mat1`\nneed to have `sparse_dim = 2`. Note that the gradients of :attr:`mat1` is a\ncoalesced sparse tensor.\n\nArgs:\n    mat (Tensor): a dense matrix to be added\n    mat1 (Tensor): a sparse matrix to be multiplied\n    mat2 (Tensor): a dense matrix to be multiplied\n    beta (Number, optional): multiplier for :attr:`mat` (:math:`\\beta`)\n    alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\\alpha`)\n')
A:torch.sparse.__init__.sampled_addmm->_add_docstr(_sparse.sparse_sampled_addmm, "\nsparse.sampled_addmm(input, mat1, mat2, *, beta=1., alpha=1., out=None) -> Tensor\n\nPerforms a matrix multiplication of the dense matrices :attr:`mat1` and :attr:`mat2` at the locations\nspecified by the sparsity pattern of :attr:`input`. The matrix :attr:`input` is added to the final result.\n\nMathematically this performs the following operation:\n\n.. math::\n\n    \\text{out} = \\alpha\\ (\\text{mat1} \\mathbin{@} \\text{mat2})*\\text{spy}(\\text{input}) + \\beta\\ \\text{input}\n\nwhere :math:`\\text{spy}(\\text{input})` is the sparsity pattern matrix of :attr:`input`, :attr:`alpha`\nand :attr:`beta` are the scaling factors.\n:math:`\\text{spy}(\\text{input})` has value 1 at the positions where :attr:`input` has non-zero values, and 0 elsewhere.\n\n.. note::\n    :attr:`input` must be a sparse CSR tensor. :attr:`mat1` and :attr:`mat2` must be dense tensors.\n    This function is implemented only for tensors on CUDA devices.\n\nArgs:\n    input (Tensor): a sparse CSR matrix of shape `(m, n)` to be added and used to compute\n        the sampled matrix multiplication\n    mat1 (Tensor): a dense matrix of shape `(m, k)` to be multiplied\n    mat2 (Tensor): a dense matrix of shape `(k, n)` to be multiplied\n\nKeyword args:\n    beta (Number, optional): multiplier for :attr:`input` (:math:`\\beta`)\n    alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\\alpha`)\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> input = torch.eye(3, device='cuda').to_sparse_csr()\n    >>> mat1 = torch.randn(3, 5, device='cuda')\n    >>> mat2 = torch.randn(5, 3, device='cuda')\n    >>> torch.sparse.sampled_addmm(input, mat1, mat2)\n    tensor(crow_indices=tensor([0, 1, 2, 3]),\n        col_indices=tensor([0, 1, 2]),\n        values=tensor([ 0.2847, -0.7805, -0.1900]), device='cuda:0',\n        size=(3, 3), nnz=3, layout=torch.sparse_csr)\n    >>> torch.sparse.sampled_addmm(input, mat1, mat2).to_dense()\n    tensor([[ 0.2847,  0.0000,  0.0000],\n        [ 0.0000, -0.7805,  0.0000],\n        [ 0.0000,  0.0000, -0.1900]], device='cuda:0')\n    >>> torch.sparse.sampled_addmm(input, mat1, mat2, beta=0.5, alpha=0.5)\n    tensor(crow_indices=tensor([0, 1, 2, 3]),\n        col_indices=tensor([0, 1, 2]),\n        values=tensor([ 0.1423, -0.3903, -0.0950]), device='cuda:0',\n        size=(3, 3), nnz=3, layout=torch.sparse_csr)\n")
A:torch.sparse.__init__.softmax->_add_docstr(_sparse._sparse_softmax, '\nsparse.softmax(input, dim, *, dtype=None) -> Tensor\n\nApplies a softmax function.\n\nSoftmax is defined as:\n\n:math:`\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}`\n\nwhere :math:`i, j` run over sparse tensor indices and unspecified\nentries are ignores. This is equivalent to defining unspecified\nentries as negative infinity so that :math:`exp(x_k) = 0` when the\nentry with index :math:`k` has not specified.\n\nIt is applied to all slices along `dim`, and will re-scale them so\nthat the elements lie in the range `[0, 1]` and sum to 1.\n\nArgs:\n    input (Tensor): input\n    dim (int): A dimension along which softmax will be computed.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n        of returned tensor.  If specified, the input tensor is\n        casted to :attr:`dtype` before the operation is\n        performed. This is useful for preventing data type\n        overflows. Default: None\n')
A:torch.sparse.__init__.log_softmax->_add_docstr(_sparse._sparse_log_softmax, '\nsparse.log_softmax(input, dim, *, dtype=None) -> Tensor\n\nApplies a softmax function followed by logarithm.\n\nSee :class:`~torch.sparse.softmax` for more details.\n\nArgs:\n    input (Tensor): input\n    dim (int): A dimension along which softmax will be computed.\n    dtype (:class:`torch.dtype`, optional): the desired data type\n        of returned tensor.  If specified, the input tensor is\n        casted to :attr:`dtype` before the operation is\n        performed. This is useful for preventing data type\n        overflows. Default: None\n')
torch.sparse.__init__.mm(mat1:Tensor,mat2:Tensor)->Tensor
torch.sparse.__init__.sum(input:Tensor,dim:DimOrDims=None,dtype:Optional[DType]=None)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/anomaly_mode.py----------------------------------------
A:torch.autograd.anomaly_mode.self.prev->torch.is_anomaly_enabled()
torch.autograd.anomaly_mode.detect_anomaly(self)
torch.autograd.anomaly_mode.detect_anomaly.__enter__(self)->None
torch.autograd.anomaly_mode.detect_anomaly.__exit__(self,*args:Any)->None
torch.autograd.anomaly_mode.detect_anomaly.__init__(self)
torch.autograd.anomaly_mode.set_detect_anomaly(self,mode:bool)
torch.autograd.anomaly_mode.set_detect_anomaly.__enter__(self)->None
torch.autograd.anomaly_mode.set_detect_anomaly.__exit__(self,*args:Any)->None
torch.autograd.anomaly_mode.set_detect_anomaly.__init__(self,mode:bool)
torch.autograd.detect_anomaly(self)
torch.autograd.detect_anomaly.__enter__(self)->None
torch.autograd.detect_anomaly.__exit__(self,*args:Any)->None
torch.autograd.set_detect_anomaly(self,mode:bool)
torch.autograd.set_detect_anomaly.__enter__(self)->None
torch.autograd.set_detect_anomaly.__exit__(self,*args:Any)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/profiler_legacy.py----------------------------------------
A:torch.autograd.profiler_legacy.records->_disable_profiler_legacy()
A:torch.autograd.profiler_legacy.parsed_results->_parse_legacy_records(records)
A:torch.autograd.profiler_legacy.self.function_events->EventList(parsed_results, use_cuda=self.use_cuda, profile_memory=self.profile_memory, with_flops=self.with_flops)
A:torch.autograd.profiler_legacy.name->record.name()
A:torch.autograd.profiler_legacy.filtered_handles->set()
A:torch.autograd.profiler_legacy.record_key->_get_record_key(record)
A:torch.autograd.profiler_legacy.is_remote_event->record.is_remote()
A:torch.autograd.profiler_legacy.start_flops->start.flops()
A:torch.autograd.profiler_legacy.fe->FunctionEvent(id=0, name=MEMORY_EVENT_NAME, trace_name=None, thread=0, start_us=0, end_us=0, stack=[], cpu_memory_usage=record.cpu_memory_usage(), cuda_memory_usage=record.cuda_memory_usage(), is_legacy=True)
A:torch.autograd.profiler_legacy.duration->start.cuda_elapsed_us(record)
A:torch.autograd.profiler_legacy.num_open_handles_cpu->len(cpu_memory_allocs)
A:torch.autograd.profiler_legacy.num_open_handles_cuda->len(cuda_memory_allocs)
torch.autograd.profiler_legacy._parse_legacy_records(thread_records)
torch.autograd.profiler_legacy.profile(self,enabled=True,*,use_cuda=False,record_shapes=False,with_flops=False,profile_memory=False,with_stack=False,with_modules=False)
torch.autograd.profiler_legacy.profile.__enter__(self)
torch.autograd.profiler_legacy.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler_legacy.profile.__init__(self,enabled=True,*,use_cuda=False,record_shapes=False,with_flops=False,profile_memory=False,with_stack=False,with_modules=False)
torch.autograd.profiler_legacy.profile.__repr__(self)
torch.autograd.profiler_legacy.profile.__str__(self)
torch.autograd.profiler_legacy.profile._check_finish(self)
torch.autograd.profiler_legacy.profile._start_trace(self)
torch.autograd.profiler_legacy.profile.config(self)
torch.autograd.profiler_legacy.profile.export_chrome_trace(self,path)
torch.autograd.profiler_legacy.profile.export_stacks(self,path:str,metric:str='self_cpu_time_total')
torch.autograd.profiler_legacy.profile.key_averages(self,group_by_input_shape=False,group_by_stack_n=0)
torch.autograd.profiler_legacy.profile.self_cpu_time_total(self)
torch.autograd.profiler_legacy.profile.table(self,sort_by=None,row_limit=100,max_src_column_width=75,header=None,top_level_events_only=False)
torch.autograd.profiler_legacy.profile.total_average(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/profiler.py----------------------------------------
A:torch.autograd.profiler.self.kineto_activities->set()
A:torch.autograd.profiler.self.kineto_results->_disable_profiler()
A:torch.autograd.profiler.parsed_results->self._parse_kineto_results(self.kineto_results)
A:torch.autograd.profiler.self.function_events->EventList(parsed_results, use_cuda=self.use_cuda, profile_memory=self.profile_memory, with_flops=self.with_flops)
A:torch.autograd.profiler.trace_start_us->result.trace_start_us()
A:torch.autograd.profiler.mem_records_acc->MemRecordsAcc(mem_records)
A:torch.autograd.profiler.fe->FunctionEvent(id=max_evt_id, name=MEMORY_EVENT_NAME, trace_name=None, thread=mem_record[0].start_thread_id(), start_us=rel_start_us, end_us=rel_start_us, fwd_thread=mem_record[0].start_thread_id(), input_shapes=[], stack=[], scope=0, cpu_memory_usage=_cpu_memory_usage(mem_record[0]), cuda_memory_usage=_cuda_memory_usage(mem_record[0]), is_async=False, sequence_nr=-1, device_type=DeviceType.CPU, device_index=0)
A:torch.autograd.profiler.cuda_time->kineto_event.cuda_elapsed_us()
A:torch.autograd.profiler.corr_id->kineto_event.linked_correlation_id()
A:torch.autograd.profiler.self.handle->torch.ops.profiler._record_function_enter(self.name, self.args)
A:torch.autograd.profiler.profiled_future->torch.ops.profiler._call_end_callbacks_on_jit_fut(self.handle, fut)
A:torch.autograd.profiler.self.seen->set()
A:torch.autograd.profiler.conn->sqlite3.connect(path)
A:torch.autograd.profiler.strings[r['id']]->torch._C._demangle(r['value'])
A:torch.autograd.profiler.unique->EnforceUnique()
A:torch.autograd.profiler.evt->FunctionEvent(id=row['marker_id'], node_id=0, name=strings[row['name']], start_us=row['start_time'], end_us=row['end_time'], thread=0)
torch.autograd.profiler.EnforceUnique(self)
torch.autograd.profiler.EnforceUnique.__init__(self)
torch.autograd.profiler.EnforceUnique.see(self,*key)
torch.autograd.profiler.emit_nvtx(self,enabled=True,record_shapes=False)
torch.autograd.profiler.emit_nvtx.__enter__(self)
torch.autograd.profiler.emit_nvtx.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.emit_nvtx.__init__(self,enabled=True,record_shapes=False)
torch.autograd.profiler.load_nvprof(path)
torch.autograd.profiler.parse_nvprof_trace(path)
torch.autograd.profiler.profile(self,enabled=True,*,use_cuda=False,record_shapes=False,with_flops=False,profile_memory=False,with_stack=False,with_modules=False,use_kineto=False,use_cpu=True)
torch.autograd.profiler.profile.__enter__(self)
torch.autograd.profiler.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.profile.__init__(self,enabled=True,*,use_cuda=False,record_shapes=False,with_flops=False,profile_memory=False,with_stack=False,with_modules=False,use_kineto=False,use_cpu=True)
torch.autograd.profiler.profile.__repr__(self)
torch.autograd.profiler.profile.__str__(self)
torch.autograd.profiler.profile._check_finish(self)
torch.autograd.profiler.profile._parse_kineto_results(self,result)
torch.autograd.profiler.profile._prepare_trace(self)
torch.autograd.profiler.profile._start_trace(self)
torch.autograd.profiler.profile.config(self)
torch.autograd.profiler.profile.export_chrome_trace(self,path)
torch.autograd.profiler.profile.export_stacks(self,path:str,metric:str='self_cpu_time_total')
torch.autograd.profiler.profile.key_averages(self,group_by_input_shape=False,group_by_stack_n=0)
torch.autograd.profiler.profile.self_cpu_time_total(self)
torch.autograd.profiler.profile.table(self,sort_by=None,row_limit=100,max_src_column_width=75,header=None,top_level_events_only=False)
torch.autograd.profiler.profile.total_average(self)
torch.autograd.profiler.record_function(self,name:str,args:Optional[str]=None)
torch.autograd.profiler.record_function.__enter__(self)
torch.autograd.profiler.record_function.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)
torch.autograd.profiler.record_function.__init__(self,name:str,args:Optional[str]=None)
torch.autograd.profiler.record_function._call_end_callbacks_on_future(self,fut:Future[Any])->Future[Any]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/profiler_util.py----------------------------------------
A:torch.autograd.profiler_util.use_cuda->kwargs.pop('use_cuda', True)
A:torch.autograd.profiler_util.profile_memory->kwargs.pop('profile_memory', False)
A:torch.autograd.profiler_util.with_flops->kwargs.pop('with_flops', False)
A:torch.autograd.profiler_util.to_delete->set()
A:torch.autograd.profiler_util.events->EventList(sorted(events, key=lambda evt: getattr(evt, sort_by), reverse=True), use_cuda=has_cuda_time, profile_memory=profile_memory, with_flops=with_flops)
A:torch.autograd.profiler_util.threads->itertools.groupby(events, key=lambda event: (event.thread, event.node_id))
A:torch.autograd.profiler_util.thread_events_->sorted(thread_events, key=lambda event: [event.time_range.start, -event.time_range.end])
A:torch.autograd.profiler_util.p->bw_parent(evt)
A:torch.autograd.profiler_util.translate_table->str.maketrans(' ;\t\n', '____')
A:torch.autograd.profiler_util.metric_value->getattr(evt, metric)
A:torch.autograd.profiler_util.avg_list->EventList(stats.values(), use_cuda=self._use_cuda, profile_memory=self._profile_memory, with_flops=self._with_flops)
A:torch.autograd.profiler_util.total_stat->FunctionEventAvg()
A:torch.autograd.profiler_util.cpu_time_str->_attr_formatter('cpu_time')
A:torch.autograd.profiler_util.cuda_time_str->_attr_formatter('cuda_time')
A:torch.autograd.profiler_util.cpu_time_total_str->_attr_formatter('cpu_time_total')
A:torch.autograd.profiler_util.cuda_time_total_str->_attr_formatter('cuda_time_total')
A:torch.autograd.profiler_util.self_cpu_time_total_str->_attr_formatter('self_cpu_time_total')
A:torch.autograd.profiler_util.self_cuda_time_total_str->_attr_formatter('self_cuda_time_total')
A:torch.autograd.profiler_util.Kernel->namedtuple('Kernel', ['name', 'device', 'duration'])
A:torch.autograd.profiler_util.tmp->sorted([(r[0].start_us(), i) for (i, r) in enumerate(mem_records)])
A:torch.autograd.profiler_util.(self._start_uses, self._indices)->zip(*tmp)
A:torch.autograd.profiler_util.start_idx->bisect.bisect_left(self._start_uses, start_us)
A:torch.autograd.profiler_util.end_idx->bisect.bisect_right(self._start_uses, end_us)
A:torch.autograd.profiler_util.string_table->StringTable()
A:torch.autograd.profiler_util.has_cuda_time->any([event.self_cuda_time_total > 0 for event in events])
A:torch.autograd.profiler_util.has_cuda_mem->any([event.self_cuda_memory_usage > 0 for event in events])
A:torch.autograd.profiler_util.has_input_shapes->any([event.input_shapes is not None and len(event.input_shapes) > 0 for event in events])
A:torch.autograd.profiler_util.name_column_width->min(name_column_width, MAX_NAME_COLUMN_WIDTH)
A:torch.autograd.profiler_util.shapes_column_width->min(shapes_column_width, MAX_SHAPES_COLUMN_WIDTH)
A:torch.autograd.profiler_util.src_column_width->min(src_column_width, max_src_column_width)
A:torch.autograd.profiler_util.append_node_id->any([evt.node_id != -1 for evt in events])
A:torch.autograd.profiler_util.log_flops->max(0, min(math.log10(flops) / 3, float(len(flop_headers) - 1)))
A:torch.autograd.profiler_util.(flops_scale, flops_header)->auto_scale_flops(min(raw_flops))
A:torch.autograd.profiler_util.sum_self_cpu_time_total->sum([event.self_cpu_time_total for event in events])
A:torch.autograd.profiler_util.src_field->trim_path(evt.stack[0], src_column_width)
torch.autograd.profiler_util.EventList(self,*args,**kwargs)
torch.autograd.profiler_util.EventList.__init__(self,*args,**kwargs)
torch.autograd.profiler_util.EventList.__str__(self)
torch.autograd.profiler_util.EventList._build_tree(self)
torch.autograd.profiler_util.EventList._populate_cpu_children(self)
torch.autograd.profiler_util.EventList._remove_dup_nodes(self)
torch.autograd.profiler_util.EventList._set_backward_stacktraces(self)
torch.autograd.profiler_util.EventList.export_chrome_trace(self,path)
torch.autograd.profiler_util.EventList.export_stacks(self,path:str,metric:str)
torch.autograd.profiler_util.EventList.key_averages(self,group_by_input_shapes=False,group_by_stack_n=0)
torch.autograd.profiler_util.EventList.self_cpu_time_total(self)
torch.autograd.profiler_util.EventList.supported_export_stacks_metrics(self)
torch.autograd.profiler_util.EventList.table(self,sort_by=None,row_limit=100,max_src_column_width=75,header=None,top_level_events_only=False)
torch.autograd.profiler_util.EventList.total_average(self)
torch.autograd.profiler_util.FormattedTimesMixin(object)
torch.autograd.profiler_util.FormattedTimesMixin.cpu_time(self)
torch.autograd.profiler_util.FormattedTimesMixin.cuda_time(self)
torch.autograd.profiler_util.FunctionEvent(self,id,name,thread,start_us,end_us,fwd_thread=None,input_shapes=None,stack=None,scope=0,cpu_memory_usage=0,cuda_memory_usage=0,is_async=False,is_remote=False,sequence_nr=-1,node_id=-1,device_type=DeviceType.CPU,device_index=0,is_legacy=False,flops=None,trace_name=None)
torch.autograd.profiler_util.FunctionEvent.__init__(self,id,name,thread,start_us,end_us,fwd_thread=None,input_shapes=None,stack=None,scope=0,cpu_memory_usage=0,cuda_memory_usage=0,is_async=False,is_remote=False,sequence_nr=-1,node_id=-1,device_type=DeviceType.CPU,device_index=0,is_legacy=False,flops=None,trace_name=None)
torch.autograd.profiler_util.FunctionEvent.__repr__(self)
torch.autograd.profiler_util.FunctionEvent.append_cpu_child(self,child)
torch.autograd.profiler_util.FunctionEvent.append_kernel(self,name,device,duration)
torch.autograd.profiler_util.FunctionEvent.cpu_time_total(self)
torch.autograd.profiler_util.FunctionEvent.cuda_time_total(self)
torch.autograd.profiler_util.FunctionEvent.key(self)
torch.autograd.profiler_util.FunctionEvent.self_cpu_memory_usage(self)
torch.autograd.profiler_util.FunctionEvent.self_cpu_time_total(self)
torch.autograd.profiler_util.FunctionEvent.self_cuda_memory_usage(self)
torch.autograd.profiler_util.FunctionEvent.self_cuda_time_total(self)
torch.autograd.profiler_util.FunctionEvent.set_cpu_parent(self,parent)
torch.autograd.profiler_util.FunctionEventAvg(self)
torch.autograd.profiler_util.FunctionEventAvg.__iadd__(self,other)
torch.autograd.profiler_util.FunctionEventAvg.__init__(self)
torch.autograd.profiler_util.FunctionEventAvg.__repr__(self)
torch.autograd.profiler_util.FunctionEventAvg.add(self,other)
torch.autograd.profiler_util.Interval(self,start,end)
torch.autograd.profiler_util.Interval.__init__(self,start,end)
torch.autograd.profiler_util.Interval.elapsed_us(self)
torch.autograd.profiler_util.MemRecordsAcc(self,mem_records)
torch.autograd.profiler_util.MemRecordsAcc.__init__(self,mem_records)
torch.autograd.profiler_util.MemRecordsAcc.in_interval(self,start_us,end_us)
torch.autograd.profiler_util.StringTable(defaultdict)
torch.autograd.profiler_util.StringTable.__missing__(self,key)
torch.autograd.profiler_util._attr_formatter(name)
torch.autograd.profiler_util._build_table(events,sort_by=None,header=None,row_limit=100,max_src_column_width=75,with_flops=False,profile_memory=False,top_level_events_only=False)
torch.autograd.profiler_util._filter_name(name)
torch.autograd.profiler_util._filter_stack_entry(entry)
torch.autograd.profiler_util._format_memory(nbytes)
torch.autograd.profiler_util._format_time(time_us)
torch.autograd.profiler_util._format_time_share(time_us,total_time_us)
torch.autograd.profiler_util._rewrite_name(name,with_wildcard=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/variable.py----------------------------------------
A:torch.autograd.variable.Variable._execution_engine->ImperativeEngine()
torch.autograd.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.VariableMeta(type)
torch.autograd.VariableMeta.__instancecheck__(cls,other)
torch.autograd.variable.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.variable.VariableMeta(type)
torch.autograd.variable.VariableMeta.__instancecheck__(cls,other)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/grad_mode.py----------------------------------------
A:torch.autograd.grad_mode.F->TypeVar('F', bound=FuncType)
A:torch.autograd.grad_mode.gen->func(*args, **kwargs)
A:torch.autograd.grad_mode.response->func(*args, **kwargs).send(request)
A:torch.autograd.grad_mode.self.prev->torch.is_grad_enabled()
A:torch.autograd.grad_mode.self._inference_mode_raii_guard->torch._C._InferenceMode(self.mode)
torch.autograd.grad_mode._DecoratorContextManager(self,func:F)
torch.autograd.grad_mode._DecoratorContextManager.__call__(self,func:F)
torch.autograd.grad_mode._DecoratorContextManager.__enter__(self)->None
torch.autograd.grad_mode._DecoratorContextManager.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.autograd.grad_mode._DecoratorContextManager._wrap_generator(self,func)
torch.autograd.grad_mode._DecoratorContextManager.clone(self)
torch.autograd.grad_mode.enable_grad(_DecoratorContextManager)
torch.autograd.grad_mode.enable_grad.__enter__(self)->None
torch.autograd.grad_mode.enable_grad.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.autograd.grad_mode.inference_mode(self,mode=True)
torch.autograd.grad_mode.inference_mode.__enter__(self)
torch.autograd.grad_mode.inference_mode.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.autograd.grad_mode.inference_mode.__init__(self,mode=True)
torch.autograd.grad_mode.inference_mode.clone(self)
torch.autograd.grad_mode.no_grad(self)
torch.autograd.grad_mode.no_grad.__enter__(self)
torch.autograd.grad_mode.no_grad.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.autograd.grad_mode.no_grad.__init__(self)
torch.autograd.grad_mode.set_grad_enabled(self,mode:bool)
torch.autograd.grad_mode.set_grad_enabled.__enter__(self)->None
torch.autograd.grad_mode.set_grad_enabled.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.autograd.grad_mode.set_grad_enabled.__init__(self,mode:bool)
torch.autograd.grad_mode.set_grad_enabled.clone(self)
torch.enable_grad(_DecoratorContextManager)
torch.enable_grad.__enter__(self)->None
torch.enable_grad.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.inference_mode(self,mode=True)
torch.inference_mode.__enter__(self)
torch.inference_mode.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.inference_mode.clone(self)
torch.no_grad(self)
torch.no_grad.__enter__(self)
torch.no_grad.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.set_grad_enabled(self,mode:bool)
torch.set_grad_enabled.__enter__(self)->None
torch.set_grad_enabled.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.set_grad_enabled.clone(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/functional.py----------------------------------------
A:torch.autograd.functional.res->jacobian(jac_func, inputs, create_graph=create_graph, strict=strict, vectorize=vectorize, strategy=outer_jacobian_strategy)
A:torch.autograd.functional.prepend->'Entry {} in '.format(idx)
A:torch.autograd.functional.grads_i->torch.zeros_like(refs[i])
A:torch.autograd.functional.(is_inputs_tuple, inputs)->_as_tuple(inputs, 'inputs', 'hvp')
A:torch.autograd.functional.inputs->_grad_preprocess(inputs, create_graph=create_graph, need_graph=True)
A:torch.autograd.functional.outputs->_grad_postprocess(outputs, create_graph)
A:torch.autograd.functional.(is_outputs_tuple, outputs)->_as_tuple(outputs, 'outputs of the user-provided function', 'hvp')
A:torch.autograd.functional.(_, v)->_as_tuple(v, 'v', 'hvp')
A:torch.autograd.functional.v->_grad_preprocess(v, create_graph=create_graph, need_graph=False)
A:torch.autograd.functional.grad_res->_autograd_grad(double_back, grad_jac, v, create_graph=create_graph)
A:torch.autograd.functional.vjp->_grad_postprocess(vjp, create_graph)
A:torch.autograd.functional.grad_outputs->_construct_standard_basis_for(outputs, output_numels)
A:torch.autograd.functional.grad_inputs->_autograd_grad(outputs, inputs, grad_outputs, create_graph=True)
A:torch.autograd.functional.jvp->_grad_postprocess(jvp, create_graph)
A:torch.autograd.functional.total_numel->sum(tensor_numels)
A:torch.autograd.functional.chunks->tuple((tensor.new_zeros(total_numel, tensor_numel) for (tensor, tensor_numel) in zip(tensors, tensor_numels)))
A:torch.autograd.functional.input_numels->tuple((input.numel() for input in inputs))
A:torch.autograd.functional.tangents->_construct_standard_basis_for(inputs, input_numels)
A:torch.autograd.functional.dual_inputs->tuple((fwAD.make_dual(input, tangent.view_as(input)) for (input, tangent) in zip(inputs, tangents)))
A:torch.autograd.functional.(_is_outputs_tuple, dual_outputs)->_as_tuple(func(*dual_inputs), 'outputs')
A:torch.autograd.functional.(primal, tangent)->fwAD.unpack_dual(dual_out)
A:torch.autograd.functional.outputs_before_split->_vmap(jvp)(tangents)
A:torch.autograd.functional.jacobian_input_i_output_j->_autograd_grad(outputs, inputs, create_graph=True).view(output_j.shape + input_i.shape)
A:torch.autograd.functional.output_numels->tuple((output.numel() for output in outputs))
A:torch.autograd.functional.flat_outputs->tuple((output.reshape(-1) for output in outputs))
A:torch.autograd.functional.vj->_autograd_grad((out.reshape(-1)[j],), inputs, retain_graph=True, create_graph=create_graph)
A:torch.autograd.functional.vj[el_idx]->torch.zeros_like(inputs[el_idx]).expand((sum(output_numels),) + inputs[el_idx].shape)
A:torch.autograd.functional.jacobians_of_flat_output->vjp(grad_outputs)
A:torch.autograd.functional.jacobian_output_input->_grad_postprocess(jacobian_output_input, create_graph)
A:torch.autograd.functional.msg->'Output {} of the user-provided function is independent of input {}. This is not allowed in strict mode.'.format(i, el_idx)
A:torch.autograd.functional.jacobian->_grad_postprocess(jacobian, create_graph)
A:torch.autograd.functional.out->func(*inp)
A:torch.autograd.functional.(is_out_tuple, t_out)->_as_tuple(out, 'outputs of the user-provided function', 'hessian')
A:torch.autograd.functional.inp->tuple((t.requires_grad_(True) for t in inp))
A:torch.autograd.functional.jac->_autograd_grad(outputs, inputs, create_graph=True)
A:torch.autograd.functional.vhp->_grad_postprocess(vhp, create_graph)
A:torch.autograd.functional.grad_jac->tuple((torch.zeros_like(inp, requires_grad=True) for inp in inputs))
A:torch.autograd.functional.double_back->_autograd_grad(jac, inputs, grad_jac, create_graph=True)
A:torch.autograd.functional.hvp->_grad_postprocess(hvp, create_graph)
torch.autograd.functional._as_tuple(inp,arg_name=None,fn_name=None)
torch.autograd.functional._as_tuple_nocheck(x)
torch.autograd.functional._autograd_grad(outputs,inputs,grad_outputs=None,create_graph=False,retain_graph=None,is_grads_batched=False)
torch.autograd.functional._check_requires_grad(inputs,input_type,strict)
torch.autograd.functional._construct_standard_basis_for(tensors:Tuple[torch.Tensor,...],tensor_numels:Tuple[int,...])->Tuple[torch.Tensor, ...]
torch.autograd.functional._fill_in_zeros(grads,refs,strict,create_graph,stage)
torch.autograd.functional._grad_postprocess(inputs,create_graph)
torch.autograd.functional._grad_preprocess(inputs,create_graph,need_graph)
torch.autograd.functional._jacfwd(func,inputs,strict=False,vectorize=False)
torch.autograd.functional._tuple_postprocess(res,to_unpack)
torch.autograd.functional._validate_v(v,other,is_other_tuple)
torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False,vectorize=False,outer_jacobian_strategy='reverse-mode')
torch.autograd.functional.hvp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.functional.jacobian(func,inputs,create_graph=False,strict=False,vectorize=False,strategy='reverse-mode')
torch.autograd.functional.jvp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.functional.vhp(func,inputs,v=None,create_graph=False,strict=False)
torch.autograd.functional.vjp(func,inputs,v=None,create_graph=False,strict=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/function.py----------------------------------------
A:torch.autograd.function.backward_hooks->OrderedDict()
A:torch.autograd.function.handle->torch.utils.hooks.RemovableHandle(backward_hooks)
A:torch.autograd.function.backward_fn->type(name + 'Backward', (BackwardCFunction,), {'_forward_cls': cls})
A:torch.autograd.function.outputs->fn(ctx, *args)
A:torch.autograd.function.requires_grad->any((isinstance(arg, torch.Tensor) and arg.requires_grad for arg in args))
A:torch.autograd.function.err_fn->torch._C._functions.DelayedError(b'trying to differentiate twice a function that was marked with @once_differentiable', len(outputs))
A:torch.autograd.function.var->var.detach().detach()
A:torch.autograd.function.obj->conversion(obj)
A:torch.autograd.function.(res_e, input)->unflatten_helper(input, e)
A:torch.autograd.function._iter_jit_values->_iter_filter(lambda o: o is None or isinstance(o, torch._C.Value), condition_msg="jit's Values or None")
A:torch.autograd.function._iter_tensors->_iter_filter(lambda x: isinstance(x, torch.Tensor), condition_msg='Tensors', conversion=_jit_unwrap_structured)
A:torch.autograd.function._iter_tensors_permissive->_iter_filter(lambda x: isinstance(x, torch.Tensor), allow_unknown=True, condition_msg='Tensors (permissive)')
A:torch.autograd.function._iter_None_tensors->_iter_filter(lambda o: o is None or isinstance(o, torch.Tensor), condition_msg='Tensors or None')
A:torch.autograd.function._map_tensor_data->_nested_map(lambda x: isinstance(x, torch.Tensor), lambda o: o.data, condition_msg='Tensors')
A:torch.autograd.function.flat_input->tuple(_iter_tensors(input))
A:torch.autograd.function.flat_output->super(NestedIOFunction, self)._do_forward(*flat_input)
A:torch.autograd.function.nested_tensors->_map_tensor_data(self._nested_input)
A:torch.autograd.function.result->self.forward_extended(*nested_tensors)
A:torch.autograd.function.nested_gradients->_unflatten(gradients, self._nested_output)
A:torch.autograd.function.self.to_save->tuple(_iter_tensors(args))
A:torch.autograd.function.self.dirty_tensors->tuple(_iter_tensors((args, kwargs)))
A:torch.autograd.function.self.non_differentiable->tuple(_iter_tensors((args, kwargs)))
torch.autograd.Function(self,*args,**kwargs)
torch.autograd.Function.backward(ctx:Any,*grad_outputs:Any)->Any
torch.autograd.Function.forward(ctx:Any,*args:Any,**kwargs:Any)->Any
torch.autograd.Function.jvp(ctx:Any,*grad_inputs:Any)->Any
torch.autograd.FunctionCtx(object)
torch.autograd.FunctionCtx.mark_dirty(self,*args:torch.Tensor)
torch.autograd.FunctionCtx.mark_non_differentiable(self,*args:torch.Tensor)
torch.autograd.FunctionCtx.mark_shared_storage(self,*pairs)
torch.autograd.FunctionCtx.save_for_backward(self,*tensors:torch.Tensor)
torch.autograd.FunctionCtx.save_for_forward(self,*tensors:torch.Tensor)
torch.autograd.FunctionCtx.set_materialize_grads(self,value:bool)
torch.autograd.FunctionMeta(cls,name,bases,attrs)
torch.autograd.NestedIOFunction(Function)
torch.autograd.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.NestedIOFunction._do_forward(self,*input)
torch.autograd.NestedIOFunction.backward(self,*gradients:Any)->Any
torch.autograd.NestedIOFunction.backward_extended(self,*grad_output:Any)->None
torch.autograd.NestedIOFunction.forward(self,*args:Any)->Any
torch.autograd.NestedIOFunction.forward_extended(self,*input:Any)->None
torch.autograd.NestedIOFunction.mark_dirty(self,*args:Any,**kwargs:Any)->None
torch.autograd.NestedIOFunction.mark_non_differentiable(self,*args:Any,**kwargs:Any)->None
torch.autograd.NestedIOFunction.save_for_backward(self,*args:Any)->None
torch.autograd.NestedIOFunction.saved_tensors(self)
torch.autograd.function.BackwardCFunction(_C._FunctionBase,FunctionCtx,_HookMixin)
torch.autograd.function.BackwardCFunction.apply(self,*args)
torch.autograd.function.BackwardCFunction.apply_jvp(self,*args)
torch.autograd.function.Function(self,*args,**kwargs)
torch.autograd.function.Function.__init__(self,*args,**kwargs)
torch.autograd.function.Function.backward(ctx:Any,*grad_outputs:Any)->Any
torch.autograd.function.Function.forward(ctx:Any,*args:Any,**kwargs:Any)->Any
torch.autograd.function.Function.jvp(ctx:Any,*grad_inputs:Any)->Any
torch.autograd.function.FunctionCtx(object)
torch.autograd.function.FunctionCtx.mark_dirty(self,*args:torch.Tensor)
torch.autograd.function.FunctionCtx.mark_non_differentiable(self,*args:torch.Tensor)
torch.autograd.function.FunctionCtx.mark_shared_storage(self,*pairs)
torch.autograd.function.FunctionCtx.save_for_backward(self,*tensors:torch.Tensor)
torch.autograd.function.FunctionCtx.save_for_forward(self,*tensors:torch.Tensor)
torch.autograd.function.FunctionCtx.set_materialize_grads(self,value:bool)
torch.autograd.function.FunctionMeta(cls,name,bases,attrs)
torch.autograd.function.FunctionMeta.__init__(cls,name,bases,attrs)
torch.autograd.function.InplaceFunction(self,inplace=False)
torch.autograd.function.InplaceFunction.__init__(self,inplace=False)
torch.autograd.function.NestedIOFunction(Function)
torch.autograd.function.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.function.NestedIOFunction._do_forward(self,*input)
torch.autograd.function.NestedIOFunction.backward(self,*gradients:Any)->Any
torch.autograd.function.NestedIOFunction.backward_extended(self,*grad_output:Any)->None
torch.autograd.function.NestedIOFunction.forward(self,*args:Any)->Any
torch.autograd.function.NestedIOFunction.forward_extended(self,*input:Any)->None
torch.autograd.function.NestedIOFunction.mark_dirty(self,*args:Any,**kwargs:Any)->None
torch.autograd.function.NestedIOFunction.mark_non_differentiable(self,*args:Any,**kwargs:Any)->None
torch.autograd.function.NestedIOFunction.save_for_backward(self,*args:Any)->None
torch.autograd.function.NestedIOFunction.saved_tensors(self)
torch.autograd.function._HookMixin(object)
torch.autograd.function._HookMixin._register_hook(backward_hooks,hook)
torch.autograd.function._iter_filter(condition,allow_unknown=False,condition_msg=None,conversion=None)
torch.autograd.function._jit_unwrap_structured(obj)
torch.autograd.function._nested_map(condition,fn,condition_msg=None)
torch.autograd.function._unflatten(input,proto)
torch.autograd.function.once_differentiable(fn)
torch.autograd.function.traceable(fn_cls)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/graph.py----------------------------------------
A:torch.autograd.graph.packed->torch.empty(tensor.size(), dtype=tensor.dtype, layout=tensor.layout, pin_memory=torch.cuda.is_available() and (not tensor.is_sparse))
torch.autograd.graph.save_on_cpu(self,pin_memory=False)
torch.autograd.graph.save_on_cpu.__init__(self,pin_memory=False)
torch.autograd.graph.saved_tensors_hooks(self,pack_hook:Callable[[torch.Tensor],Any],unpack_hook:Callable[[Any],torch.Tensor])
torch.autograd.graph.saved_tensors_hooks.__enter__(self)
torch.autograd.graph.saved_tensors_hooks.__exit__(self,*args:Any)
torch.autograd.graph.saved_tensors_hooks.__init__(self,pack_hook:Callable[[torch.Tensor],Any],unpack_hook:Callable[[Any],torch.Tensor])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/__init__.py----------------------------------------
A:torch.autograd.__init__.grad_tensors_->_make_grads(tensors, grad_tensors_, is_grads_batched=False)
A:torch.autograd.__init__.grad_outputs_->_make_grads(outputs, grad_outputs_, is_grads_batched=is_grads_batched)
torch.autograd.__init__._is_checkpoint_valid()
torch.autograd.__init__._make_grads(outputs:Sequence[torch.Tensor],grads:Sequence[_OptionalTensor],is_grads_batched:bool)->Tuple[_OptionalTensor, ...]
torch.autograd.__init__._register_py_tensor_class_for_device(device,cls)
torch.autograd.__init__._tensor_or_tensors_to_tuple(tensors:Optional[_TensorOrTensors],length:int)->Tuple[_OptionalTensor, ...]
torch.autograd.__init__.backward(tensors:_TensorOrTensors,grad_tensors:Optional[_TensorOrTensors]=None,retain_graph:Optional[bool]=None,create_graph:bool=False,grad_variables:Optional[_TensorOrTensors]=None,inputs:Optional[_TensorOrTensors]=None)->None
torch.autograd.__init__.grad(outputs:_TensorOrTensors,inputs:_TensorOrTensors,grad_outputs:Optional[_TensorOrTensors]=None,retain_graph:Optional[bool]=None,create_graph:bool=False,only_inputs:bool=True,allow_unused:bool=False,is_grads_batched:bool=False)->Tuple[torch.Tensor, ...]
torch.autograd.__init__.variable(*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/gradcheck.py----------------------------------------
A:torch.autograd.gradcheck.dim->len(size)
A:torch.autograd.gradcheck.x_nnz->x_tensor._nnz()
A:torch.autograd.gradcheck.x_size->list(x_tensor.size())
A:torch.autograd.gradcheck.x_indices->x_tensor._indices().t()
A:torch.autograd.gradcheck.x_values->x._values()
A:torch.autograd.gradcheck.x_stride->get_stride(x_size)
A:torch.autograd.gradcheck.d_idx->sum((indices[k] * x_stride[k] for k in range(len(x_size))))
A:torch.autograd.gradcheck.x_tensor_dense->x_tensor.to_dense()
A:torch.autograd.gradcheck.outputs->_differentiable_outputs(func(*input_args))
A:torch.autograd.gradcheck.jacobians->_check_analytical_jacobian_attributes(inputs, outputs[output_idx], nondet_tol=float('inf'), check_grad_dtypes=False)
A:torch.autograd.gradcheck.orig->entry.clone()
A:torch.autograd.gradcheck.outa->fn()
A:torch.autograd.gradcheck.outb->fn()
A:torch.autograd.gradcheck.ds_dx_tup->jvp_fn(delta[0] if isinstance(delta, tuple) else delta)
A:torch.autograd.gradcheck.wrapped_fn->_with_prepare_inputs(fn, inputs, input_idx, input_to_perturb, True)
A:torch.autograd.gradcheck.nbhd_checks_fn->functools.partial(check_outputs_same_dtype_and_shape, eps=eps)
A:torch.autograd.gradcheck.jvp_fn->_get_numerical_jvp_fn(wrapped_fn, input_to_perturb, eps, nbhd_checks_fn)
A:torch.autograd.gradcheck.jacobian_cols[d_idx]->_compute_numerical_jvps_wrt_specific_input(jvp_fn, eps, x.is_complex(), is_forward_ad)
A:torch.autograd.gradcheck.tensor_inputs->tuple((i for i in inputs if is_tensor_like(i) and i.requires_grad))
A:torch.autograd.gradcheck.inp->fwAD.make_dual(inp, torch.zeros_like(inp))
A:torch.autograd.gradcheck.raw_outputs->_as_tuple(func(*dual_inputs))
A:torch.autograd.gradcheck.dual_outputs->_as_tuple(func(*inputs_with_dual))
A:torch.autograd.gradcheck.(val, res)->fwAD.unpack_dual(d_o)
A:torch.autograd.gradcheck.input_to_perturb->_get_input_to_perturb(input)
A:torch.autograd.gradcheck.u->_mul_tensor_or_tuple(u, eps)
A:torch.autograd.gradcheck.all_Ju->_get_numerical_jvp_wrt_specific_input(fn, inp_idx, inputs, u, eps, is_forward_ad)
A:torch.autograd.gradcheck.func_out->func(*tupled_inputs)
A:torch.autograd.gradcheck.out_jacobians->_allocate_jacobians_with_inputs(inputs, numel_outputs)
A:torch.autograd.gradcheck.diff_input_list->list(_iter_tensors(input, True))
A:torch.autograd.gradcheck.out_jacobian[:, j]->dense.reshape(-1)
A:torch.autograd.gradcheck.vjps1->_compute_analytical_jacobian_rows(vjp_fn, output.clone())
A:torch.autograd.gradcheck.vjps2->_compute_analytical_jacobian_rows(vjp_fn, output.clone())
A:torch.autograd.gradcheck.(jacobians1, types_ok, sizes_ok)->_stack_and_check_tensors(vjps1, inputs, output_numel)
A:torch.autograd.gradcheck.(jacobians2, _, _)->_stack_and_check_tensors(vjps2, inputs, output_numel)
A:torch.autograd.gradcheck.reentrant->_check_jacobians_equal(jacobians1, jacobians2, nondet_tol)
A:torch.autograd.gradcheck.all_vJ->_check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes, fast_mode=True, v=v)
A:torch.autograd.gradcheck.vJ->vJ.T.squeeze(0).T.squeeze(0)
A:torch.autograd.gradcheck.tv->torch.view_as_real(vJ.resolve_conj())
A:torch.autograd.gradcheck.tr->torch.view_as_real(vJ.resolve_conj()).select(-1, 0)
A:torch.autograd.gradcheck.ti->torch.view_as_real(vJ.resolve_conj()).select(-1, 1)
A:torch.autograd.gradcheck.output_numel->output.numel()
A:torch.autograd.gradcheck.grad_out_base->torch.zeros_like(sample_output, memory_format=torch.legacy_contiguous_format)
A:torch.autograd.gradcheck.flat_grad_out->torch.zeros_like(sample_output, memory_format=torch.legacy_contiguous_format).view(-1)
A:torch.autograd.gradcheck.grad_inputs->tuple((g for g in grad_inputs if g is not None))
A:torch.autograd.gradcheck.jacobians_all_inputs_outputs->_get_numerical_jacobian(func, inputs, func_out, eps=eps)
A:torch.autograd.gradcheck.jvps->_get_numerical_jvp_wrt_specific_input(func, inp_idx, all_inputs, u, eps)
A:torch.autograd.gradcheck.FAILED_BATCHED_GRAD_MSG->"\ngradcheck or gradgradcheck failed while testing batched gradient computation.\nThis could have been invoked in a number of ways (via a test that calls\ngradcheck/gradgradcheck directly or via an autogenerated test).\n\nIf you are adding a new operator, please file an issue and then use one of the\nworkarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.\nIf the test\n- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck\n  with `check_batched_grad=False` as a keyword argument.\n- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test\n  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.\n\nIf you're modifying an existing operator that supports batched grad computation,\nor wish to make a new operator work with batched grad computation, please read\nthe following.\n\nTo compute batched grads (e.g., jacobians, hessians), we vmap over the backward\ncomputation. The most common failure case is if there is a 'vmap-incompatible\noperation' in the backward pass. Please see\nNOTE: [How to write vmap-compatible backward formulas]\nin the codebase for an explanation of how to fix this.\n".strip()
A:torch.autograd.gradcheck.dual->fwAD.make_dual(current_input, tangent)
A:torch.autograd.gradcheck.inputs_with_dual->tuple((dual if idx == input_idx else inp for (idx, inp) in enumerate(inputs)))
A:torch.autograd.gradcheck.(primal_out, tangent_out)->fwAD.unpack_dual(dual_output)
A:torch.autograd.gradcheck.result->vmap(vjp)(torch.stack(grad_outputs))
A:torch.autograd.gradcheck.grad->functools.partial(torch.autograd.grad, output, diff_input_list, retain_graph=True, allow_unused=True)
A:torch.autograd.gradcheck.results->tuple((grad if grad is not None else torch.zeros([], dtype=inp.dtype, device=inp.device).expand(inp.shape) for (grad, inp) in zip(results, diff_input_list)))
A:torch.autograd.gradcheck.grads_input->torch.autograd.grad(output_to_check, diff_input_list, grads_output, allow_unused=True)
A:torch.autograd.gradcheck.gi->gi.to_dense().to_dense()
A:torch.autograd.gradcheck.di->di.to_dense().to_dense()
A:torch.autograd.gradcheck.(inp_tensors_idx, inp_tensors)->_get_inp_tensors(inputs)
A:torch.autograd.gradcheck.(all_v, all_u, all_u_dense)->_make_vectors(inp_tensors, outputs, use_forward_ad=use_forward_ad)
A:torch.autograd.gradcheck.tensor_indices->set()
A:torch.autograd.gradcheck.dual_inputs[idx]->fwAD.make_dual(inp, torch.zeros_like(inp))
A:torch.autograd.gradcheck.dual_outputs1->filter(_is_float_or_complex_tensor, raw_outputs)
A:torch.autograd.gradcheck.dual_outputs2->filter(_is_float_or_complex_tensor, raw_outputs)
A:torch.autograd.gradcheck.(val1, res1)->fwAD.unpack_dual(d_o1)
A:torch.autograd.gradcheck.(val2, res2)->fwAD.unpack_dual(d_o2)
A:torch.autograd.gradcheck.output_to_check->_differentiable_outputs(func(*inputs))
A:torch.autograd.gradcheck.outs->_as_tuple(fn(*inputs))
A:torch.autograd.gradcheck.new_inputs->list(tupled_inputs)
A:torch.autograd.gradcheck.new_inputs[should_be_complex]->fn_to_apply(new_inputs[should_be_complex], tupled_inputs[should_be_complex])
A:torch.autograd.gradcheck.real_fn->apply_to_c_inps(fn, lambda inp, orig: inp + orig.imag * 1j)
A:torch.autograd.gradcheck.imag_fn->apply_to_c_inps(fn, lambda inp, orig: orig.real + inp * 1j)
A:torch.autograd.gradcheck.has_any_complex_output->any((o.is_complex() for o in _as_tuple(func_out)))
A:torch.autograd.gradcheck.(real_fn, imag_fn)->_real_and_imag_input(func, complex_inp_indices, tupled_inputs)
A:torch.autograd.gradcheck.imag_func_out->imag_fn(*imag_inputs)
A:torch.autograd.gradcheck.imag_outputs->_differentiable_outputs(imag_func_out)
A:torch.autograd.gradcheck.real_func_out->real_fn(*real_inputs)
A:torch.autograd.gradcheck.real_outputs->_differentiable_outputs(real_func_out)
A:torch.autograd.gradcheck.diff_imag_func_out->_differentiable_outputs(imag_func_out)
A:torch.autograd.gradcheck.diff_real_func_out->_differentiable_outputs(real_func_out)
A:torch.autograd.gradcheck.numerical->_transpose(_get_numerical_jacobian(func, tupled_inputs, outputs, eps=eps, is_forward_ad=use_forward_ad))
A:torch.autograd.gradcheck.analytical_forward->_get_analytical_jacobian_forward_ad(func, tupled_inputs, func_out, check_grad_dtypes=check_grad_dtypes)
A:torch.autograd.gradcheck.analytical->_check_analytical_jacobian_attributes(tupled_inputs, o, nondet_tol, check_grad_dtypes)
A:torch.autograd.gradcheck.promoted_type->torch.promote_types(a.dtype, b.dtype)
A:torch.autograd.gradcheck.a->a.to(dtype=promoted_type).to(dtype=promoted_type)
A:torch.autograd.gradcheck.b->b.to(dtype=promoted_type).to(dtype=promoted_type)
A:torch.autograd.gradcheck.values->torch.rand(x_values.numel(), generator=generator).to(dtype=dtype, device=x.device).reshape(x_values.shape)
A:torch.autograd.gradcheck.vec->torch.rand(x.numel(), generator=generator).to(dtype=dtype, device=x.device)
A:torch.autograd.gradcheck.FAST_FAIL_SLOW_OK_MSG->"\nFast gradcheck failed but element-wise differences are small. This means that the\ntest might've passed in slow_mode!\n\nIf you are adding a new operator, please file an issue and then use one of the\nworkarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck:\n\nIf the test\n- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck\n  with `fast_mode=False` as a keyword argument.\n- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test\n  to have `gradcheck_fast_mode=False`\n- is a Module test (e.g., in common_nn.py), then modify the corresponding\n  module_test entry to have `gradcheck_fast_mode=False`\n".strip()
A:torch.autograd.gradcheck.slow_analytical->_get_analytical_jacobian(tupled_inputs, outputs, input_idx, output_idx)
A:torch.autograd.gradcheck.slow_max_diff->(slow_numerical - slow_analytical).abs().max()
A:torch.autograd.gradcheck.slow_allclose->torch.allclose(slow_analytical, slow_numerical, rtol, atol)
A:torch.autograd.gradcheck.g_cpu->torch.Generator()
A:torch.autograd.gradcheck.ur->_vec_from_tensor(inp, g_cpu, True)
A:torch.autograd.gradcheck.ur_dense->_to_flat_dense_if_sparse(ur)
A:torch.autograd.gradcheck.ui->_vec_from_tensor(inp, g_cpu, True)
A:torch.autograd.gradcheck.ui_dense->_to_flat_dense_if_sparse(ui)
A:torch.autograd.gradcheck.n->n.to(device=a.device).to(device=a.device)
A:torch.autograd.gradcheck.updated_atol->_adjusted_atol(atol, all_u[i], all_v[j] if all_v else None)
A:torch.autograd.gradcheck.jacobians_str->_run_slow_mode_and_get_error(func, tupled_inputs, outputs, i, j, rtol, atol, is_forward_ad)
A:torch.autograd.gradcheck.numerical_vJu->_get_numerical_vJu(func, inputs, inp_tensors_idx, func_out, all_u, all_v, eps, is_forward_ad=use_forward_ad)
A:torch.autograd.gradcheck.analytical_vJu->_get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
A:torch.autograd.gradcheck.args->locals().copy()
A:torch.autograd.gradcheck.tupled_inputs->_as_tuple(inputs)
A:torch.autograd.gradcheck.y->torch.testing.make_non_contiguous(y)
A:torch.autograd.gradcheck.tupled_grad_outputs->_as_tuple(grad_outputs)
A:torch.autograd.gradcheck.num_outputs->len(tupled_grad_outputs)
A:torch.autograd.gradcheck.input_args->tuple((x for x in input_args if is_tensor_like(x) and x.requires_grad))
torch.autograd.gradcheck(func:Callable[...,Union[_TensorOrTensors]],inputs:_TensorOrTensors,*,eps:float=1e-06,atol:float=1e-05,rtol:float=0.001,raise_exception:bool=True,check_sparse_nnz:bool=False,nondet_tol:float=0.0,check_undefined_grad:bool=True,check_grad_dtypes:bool=False,check_batched_grad:bool=False,check_batched_forward_grad:bool=False,check_forward_ad:bool=False,check_backward_ad:bool=True,fast_mode:bool=False)->bool
torch.autograd.gradcheck.GradcheckError(RuntimeError)
torch.autograd.gradcheck._adjusted_atol(atol,u,v)
torch.autograd.gradcheck._allclose_with_type_promotion(a,b,rtol,atol)
torch.autograd.gradcheck._allocate_jacobians_with_inputs(input_tensors:Tuple,numel_output)->Tuple[torch.Tensor, ...]
torch.autograd.gradcheck._allocate_jacobians_with_outputs(output_tensors:Tuple,numel_input,dtype=None,device=None)->Tuple[torch.Tensor, ...]
torch.autograd.gradcheck._as_tuple(x)
torch.autograd.gradcheck._check_analytical_jacobian_attributes(inputs,output,nondet_tol,check_grad_dtypes,fast_mode=False,v=None)->Tuple[torch.Tensor, ...]
torch.autograd.gradcheck._check_analytical_numerical_equal(all_analytical,all_numerical,complex_indices,tupled_inputs,outputs,func,all_v,all_u,rtol,atol,test_imag,*,is_forward_ad=False)
torch.autograd.gradcheck._check_inputs(tupled_inputs,check_sparse_nnz)->bool
torch.autograd.gradcheck._check_jacobians_equal(j1,j2,atol)
torch.autograd.gradcheck._check_no_differentiable_outputs(func,inputs,func_out,eps)->bool
torch.autograd.gradcheck._check_no_differentiable_outputs_fast(func,func_out,all_inputs,inputs_indices,all_u,eps,nondet_tol)
torch.autograd.gradcheck._check_outputs(outputs)->None
torch.autograd.gradcheck._combine_jacobian_cols(jacobians_cols:Dict[int,List[torch.Tensor]],outputs,input,numel)->Tuple[torch.Tensor, ...]
torch.autograd.gradcheck._compute_analytical_jacobian_rows(vjp_fn,sample_output)->List[List[Optional[torch.Tensor]]]
torch.autograd.gradcheck._compute_numerical_gradient(fn,entry,v,norm_v,nbhd_checks_fn)
torch.autograd.gradcheck._compute_numerical_jvps_wrt_specific_input(jvp_fn,delta,input_is_complex,is_forward_ad=False)->List[torch.Tensor]
torch.autograd.gradcheck._differentiable_outputs(x)
torch.autograd.gradcheck._dot_with_type_promotion(u,v)
torch.autograd.gradcheck._fast_gradcheck(func,func_out,inputs,outputs,eps,rtol,atol,check_grad_dtypes,nondet_tol,*,use_forward_ad=False,complex_indices=None,test_imag=False)
torch.autograd.gradcheck._get_analytical_jacobian(inputs,outputs,input_idx,output_idx)
torch.autograd.gradcheck._get_analytical_jacobian_forward_ad(fn,inputs,outputs,*,check_grad_dtypes=False,all_u=None)->Tuple[Tuple[torch.Tensor, ...], ...]
torch.autograd.gradcheck._get_analytical_vJu_backward_mode(inputs,outputs,nondet_tol,check_grad_dtypes,all_v,all_u)
torch.autograd.gradcheck._get_analytical_vjps_wrt_specific_output(vjp_fn,sample_output,v)->List[List[Optional[torch.Tensor]]]
torch.autograd.gradcheck._get_failed_batched_grad_test_msg(output_idx,input_idx,res,exp,is_forward_ad=False)
torch.autograd.gradcheck._get_inp_tensors(tupled_inputs)
torch.autograd.gradcheck._get_input_to_perturb(input)
torch.autograd.gradcheck._get_notallclose_msg(analytical,numerical,output_idx,input_idx,complex_indices,test_imag=False,is_forward_ad=False)->str
torch.autograd.gradcheck._get_numerical_jacobian(fn,inputs,outputs=None,target=None,eps=0.001,is_forward_ad=False)->List[Tuple[torch.Tensor, ...]]
torch.autograd.gradcheck._get_numerical_jvp_fn(wrapped_fn,input_to_perturb,eps,nbhd_checks_fn)
torch.autograd.gradcheck._get_numerical_jvp_wrt_specific_input(fn,input_idx,inputs,u,eps,is_forward_ad=False)->List[torch.Tensor]
torch.autograd.gradcheck._get_numerical_vJu(fn,inputs,inp_indices,func_out,all_u,all_v,eps,is_forward_ad)
torch.autograd.gradcheck._gradcheck_helper(func,inputs,eps,atol,rtol,check_sparse_nnz,nondet_tol,check_undefined_grad,check_grad_dtypes,check_batched_grad,check_batched_forward_grad,check_forward_ad,check_backward_ad,fast_mode)
torch.autograd.gradcheck._gradcheck_real_imag(gradcheck_fn,func,func_out,tupled_inputs,outputs,eps,rtol,atol,check_grad_dtypes,check_forward_ad,check_backward_ad,nondet_tol,check_undefined_grad)
torch.autograd.gradcheck._is_float_or_complex_tensor(obj)
torch.autograd.gradcheck._iter_tensor(x_tensor)
torch.autograd.gradcheck._iter_tensors(x:Union[torch.Tensor,Iterable[torch.Tensor]],only_requiring_grad:bool=False)->Iterable[torch.Tensor]
torch.autograd.gradcheck._make_vectors(inp_tensors,outputs,*,use_forward_ad)
torch.autograd.gradcheck._mul_tensor_or_tuple(u,k)
torch.autograd.gradcheck._prepare_input(input:torch.Tensor,maybe_perturbed_input:Optional[torch.Tensor],fast_mode=False)->torch.Tensor
torch.autograd.gradcheck._real_and_imag_input(fn,complex_inp_indices,tupled_inputs)
torch.autograd.gradcheck._real_and_imag_output(fn)
torch.autograd.gradcheck._reshape_tensor_or_tuple(u,shape)
torch.autograd.gradcheck._run_slow_mode_and_get_error(func,tupled_inputs,outputs,input_idx,output_idx,rtol,atol,is_forward_ad)
torch.autograd.gradcheck._slow_gradcheck(func,func_out,tupled_inputs,outputs,eps,rtol,atol,check_grad_dtypes,nondet_tol,*,use_forward_ad=False,complex_indices=None,test_imag=False)
torch.autograd.gradcheck._stack_and_check_tensors(list_of_list_of_tensors,inputs,numel_outputs)->Tuple[Tuple[torch.Tensor, ...], bool, bool]
torch.autograd.gradcheck._test_backward_mul_by_grad_output(outputs,inputs,check_sparse_nnz)->bool
torch.autograd.gradcheck._test_batched_grad(input,output,output_idx)->bool
torch.autograd.gradcheck._test_batched_grad_forward_ad(func,inputs)->bool
torch.autograd.gradcheck._test_undefined_backward_mode(func,outputs,inputs)->bool
torch.autograd.gradcheck._test_undefined_forward_mode(func,outputs,inputs)
torch.autograd.gradcheck._to_flat_dense_if_sparse(tensor)
torch.autograd.gradcheck._to_real_dtype(dtype)
torch.autograd.gradcheck._transpose(matrix_of_tensors)
torch.autograd.gradcheck._vec_from_tensor(x,generator,downcast_complex=False)
torch.autograd.gradcheck._with_prepare_inputs(fn,inputs,input_idx,input_to_perturb,fast_mode=False)
torch.autograd.gradcheck.check_outputs_same_dtype_and_shape(output1,output2,eps,idx=None)->None
torch.autograd.gradcheck.get_analytical_jacobian(inputs,output,nondet_tol=0.0,grad_out=1.0)
torch.autograd.gradcheck.get_numerical_jacobian(fn,inputs,target=None,eps=0.001,grad_out=1.0)
torch.autograd.gradcheck.get_numerical_jacobian_wrt_specific_input(fn,input_idx,inputs,outputs,eps,input=None,is_forward_ad=False)->Tuple[torch.Tensor, ...]
torch.autograd.gradcheck.gradcheck(func:Callable[...,Union[_TensorOrTensors]],inputs:_TensorOrTensors,*,eps:float=1e-06,atol:float=1e-05,rtol:float=0.001,raise_exception:bool=True,check_sparse_nnz:bool=False,nondet_tol:float=0.0,check_undefined_grad:bool=True,check_grad_dtypes:bool=False,check_batched_grad:bool=False,check_batched_forward_grad:bool=False,check_forward_ad:bool=False,check_backward_ad:bool=True,fast_mode:bool=False)->bool
torch.autograd.gradcheck.gradgradcheck(func:Callable[...,_TensorOrTensors],inputs:_TensorOrTensors,grad_outputs:Optional[_TensorOrTensors]=None,*,eps:float=1e-06,atol:float=1e-05,rtol:float=0.001,gen_non_contig_grad_outputs:bool=False,raise_exception:bool=True,nondet_tol:float=0.0,check_undefined_grad:bool=True,check_grad_dtypes:bool=False,check_batched_grad:bool=False,check_fwd_over_rev:bool=False,check_rev_over_rev:bool=True,fast_mode:bool=False)->bool
torch.autograd.gradgradcheck(func:Callable[...,_TensorOrTensors],inputs:_TensorOrTensors,grad_outputs:Optional[_TensorOrTensors]=None,*,eps:float=1e-06,atol:float=1e-05,rtol:float=0.001,gen_non_contig_grad_outputs:bool=False,raise_exception:bool=True,nondet_tol:float=0.0,check_undefined_grad:bool=True,check_grad_dtypes:bool=False,check_batched_grad:bool=False,check_fwd_over_rev:bool=False,check_rev_over_rev:bool=True,fast_mode:bool=False)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/forward_ad.py----------------------------------------
A:torch.autograd.forward_ad.new_level->torch._C._enter_dual_level()
A:torch.autograd.forward_ad.UnpackedDualTensor->namedtuple('UnpackedDualTensor', ['primal', 'tangent'])
A:torch.autograd.forward_ad.(primal, dual)->torch._VF._unpack_dual(tensor, level=level)
torch.autograd.forward_ad.dual_level(self)
torch.autograd.forward_ad.dual_level.__enter__(self)
torch.autograd.forward_ad.dual_level.__exit__(self,exc_type:Any,exc_value:Any,traceback:Any)->None
torch.autograd.forward_ad.dual_level.__init__(self)
torch.autograd.forward_ad.enter_dual_level()
torch.autograd.forward_ad.exit_dual_level(*,level=None)
torch.autograd.forward_ad.make_dual(tensor,tangent,*,level=None)
torch.autograd.forward_ad.unpack_dual(tensor,*,level=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/_functions/utils.py----------------------------------------
A:torch.autograd._functions.utils.tensor->tensor.sum(dim, keepdim=True).sum(dim, keepdim=True)
A:torch.autograd._functions.utils.len1->len(dims1)
A:torch.autograd._functions.utils.len2->len(dims2)
A:torch.autograd._functions.utils.numel1->reduce(lambda x, y: x * y, dims1)
A:torch.autograd._functions.utils.numel2->reduce(lambda x, y: x * y, dims2)
torch.autograd._functions.utils.check_onnx_broadcast(dims1,dims2)
torch.autograd._functions.utils.maybe_unexpand(tensor,old_size,check_same_size=True)
torch.autograd._functions.utils.maybe_view(tensor,size,check_same_size=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/_functions/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/autograd/_functions/tensor.py----------------------------------------
A:torch.autograd._functions.tensor.ctx.input_type->type(i)
A:torch.autograd._functions.tensor.ctx.numel->reduce(lambda x, y: x * y, sizes, 1)
A:torch.autograd._functions.tensor.ctx.input_sizes->tensor.size()
A:torch.autograd._functions.tensor.result->tensor.new(tensor).contiguous().view(*sizes)
torch.autograd._functions.Resize(Function)
torch.autograd._functions.Resize.backward(ctx,grad_output)
torch.autograd._functions.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.Type(Function)
torch.autograd._functions.Type.backward(ctx,grad_output)
torch.autograd._functions.Type.forward(ctx,i,dest_type)
torch.autograd._functions.tensor.Resize(Function)
torch.autograd._functions.tensor.Resize.backward(ctx,grad_output)
torch.autograd._functions.tensor.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.tensor.Type(Function)
torch.autograd._functions.tensor.Type.backward(ctx,grad_output)
torch.autograd._functions.tensor.Type.forward(ctx,i,dest_type)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_core.py----------------------------------------
A:torch.testing._core.flat_index->operator.index(flat_index)
A:torch.testing._core.osize->list(tensor.size())
A:torch.testing._core.dim->random.randint(0, len(osize) - 1)
A:torch.testing._core.add->random.randint(4, 15)
A:torch.testing._core.input->input.narrow(i, bounds, tensor.size(i)).narrow(i, bounds, tensor.size(i))
A:torch.testing._core.bounds->random.randint(1, input.size(i) - tensor.size(i))
torch.testing._core._unravel_index(flat_index,shape)
torch.testing._core.is_integral(dtype:torch.dtype)->bool
torch.testing._core.is_quantized(dtype:torch.dtype)->bool
torch.testing._core.make_non_contiguous(tensor:torch.Tensor)->torch.Tensor
torch.testing._unravel_index(flat_index,shape)
torch.testing.is_integral(dtype:torch.dtype)->bool
torch.testing.is_quantized(dtype:torch.dtype)->bool
torch.testing.make_non_contiguous(tensor:torch.Tensor)->torch.Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_legacy.py----------------------------------------
A:torch.testing._legacy._empty_types->_dispatch_dtypes(())
A:torch.testing._legacy._floating_types->_dispatch_dtypes((torch.float32, torch.float64))
A:torch.testing._legacy._double_types->_dispatch_dtypes((torch.float64, torch.complex128))
A:torch.testing._legacy._integral_types->_dispatch_dtypes((torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64))
A:torch.testing._legacy._complex_types->_dispatch_dtypes((torch.cfloat, torch.cdouble))
torch.testing._legacy._dispatch_dtypes(tuple)
torch.testing._legacy._dispatch_dtypes.__add__(self,other)
torch.testing._legacy._validate_dtypes(*dtypes)
torch.testing._legacy.all_types()
torch.testing._legacy.all_types_and(*dtypes)
torch.testing._legacy.all_types_and_complex()
torch.testing._legacy.all_types_and_complex_and(*dtypes)
torch.testing._legacy.all_types_and_half()
torch.testing._legacy.complex_types()
torch.testing._legacy.double_types()
torch.testing._legacy.empty_types()
torch.testing._legacy.floating_and_complex_types()
torch.testing._legacy.floating_and_complex_types_and(*dtypes)
torch.testing._legacy.floating_types()
torch.testing._legacy.floating_types_and(*dtypes)
torch.testing._legacy.floating_types_and_half()
torch.testing._legacy.get_all_complex_dtypes()->List[torch.dtype]
torch.testing._legacy.get_all_device_types()->List[str]
torch.testing._legacy.get_all_dtypes(include_half=True,include_bfloat16=True,include_bool=True,include_complex=True)->List[torch.dtype]
torch.testing._legacy.get_all_fp_dtypes(include_half=True,include_bfloat16=True)->List[torch.dtype]
torch.testing._legacy.get_all_int_dtypes()->List[torch.dtype]
torch.testing._legacy.get_all_math_dtypes(device)->List[torch.dtype]
torch.testing._legacy.integral_types()
torch.testing._legacy.integral_types_and(*dtypes)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_comparison.py----------------------------------------
A:torch.testing._comparison.(rtols, atols)->zip(*[_DTYPE_PRECISIONS.get(dtype, (0.0, 0.0)) for dtype in dtypes])
A:torch.testing._comparison.identifier->identifier(default_identifier)
A:torch.testing._comparison.abs_diff->abs(self.actual - self.expected)
A:torch.testing._comparison.number_of_elements->mismatches.numel()
A:torch.testing._comparison.total_mismatches->torch.sum(mismatches).item()
A:torch.testing._comparison.a_flat->actual.to(dtype).flatten()
A:torch.testing._comparison.b_flat->expected.to(dtype).flatten()
A:torch.testing._comparison.(max_abs_diff, max_abs_diff_flat_idx)->torch.max(abs_diff, 0)
A:torch.testing._comparison.(max_rel_diff, max_rel_diff_flat_idx)->torch.max(rel_diff, 0)
A:torch.testing._comparison.(actual, expected)->self._promote_for_comparison(actual, expected)
A:torch.testing._comparison._NUMBER_TYPES->tuple(_TYPE_TO_DTYPE.keys())
A:torch.testing._comparison.(self.rtol, self.atol)->get_tolerances(actual, expected, rtol=rtol, atol=atol, id=self.id)
A:torch.testing._comparison.cls->list(self._NUMBER_TYPES)
A:torch.testing._comparison.actual->actual.to(dtype).to(dtype)
A:torch.testing._comparison.expected->expected.to(dtype).to(dtype)
A:torch.testing._comparison.dtype->torch.promote_types(actual.dtype, expected.dtype)
A:torch.testing._comparison.matches->torch.isclose(actual, expected, rtol=rtol, atol=atol, equal_nan=equal_nan)
A:torch.testing._comparison.msg->make_tensor_mismatch_msg(actual, expected, ~matches, rtol=rtol, atol=atol, identifier=identifier)
A:torch.testing._comparison.actual_len->len(actual)
A:torch.testing._comparison.expected_len->len(expected)
A:torch.testing._comparison.actual_keys->set(actual.keys())
A:torch.testing._comparison.expected_keys->set(expected.keys())
A:torch.testing._comparison.keys->sorted(keys)
A:torch.testing._comparison.pairs->originate_pairs(actual, expected, pair_types=pair_types, sequence_types=sequence_types, mapping_types=mapping_types, **options)
torch.testing._comparison.BooleanPair(self,actual:Any,expected:Any,*,id:Tuple[Any,...],**other_parameters:Any)
torch.testing._comparison.BooleanPair.__init__(self,actual:Any,expected:Any,*,id:Tuple[Any,...],**other_parameters:Any)
torch.testing._comparison.BooleanPair._process_inputs(self,actual:Any,expected:Any,*,id:Tuple[Any,...])->Tuple[bool, bool]
torch.testing._comparison.BooleanPair._supported_types(self)->Tuple[Type, ...]
torch.testing._comparison.BooleanPair._to_bool(self,bool_like:Any,*,id:Tuple[Any,...])->bool
torch.testing._comparison.BooleanPair.compare(self)->None
torch.testing._comparison.ErrorMeta(self,type:Type[Exception],msg:str,*,id:Tuple[Any,...]=())
torch.testing._comparison.ErrorMeta.__init__(self,type:Type[Exception],msg:str,*,id:Tuple[Any,...]=())
torch.testing._comparison.ErrorMeta.to_error(self)->Exception
torch.testing._comparison.NonePair(self,actual:Any,expected:Any,**other_parameters:Any)
torch.testing._comparison.NonePair.__init__(self,actual:Any,expected:Any,**other_parameters:Any)
torch.testing._comparison.NonePair.compare(self)->None
torch.testing._comparison.NumberPair(self,actual:Any,expected:Any,*,id:Tuple[Any,...]=(),rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=False,check_dtype:bool=False,**other_parameters:Any)
torch.testing._comparison.NumberPair.__init__(self,actual:Any,expected:Any,*,id:Tuple[Any,...]=(),rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=False,check_dtype:bool=False,**other_parameters:Any)
torch.testing._comparison.NumberPair._process_inputs(self,actual:Any,expected:Any,*,id:Tuple[Any,...])->Tuple[Union[int, float, complex], Union[int, float, complex]]
torch.testing._comparison.NumberPair._supported_types(self)->Tuple[Type, ...]
torch.testing._comparison.NumberPair._to_number(self,number_like:Any,*,id:Tuple[Any,...])->Union[int, float, complex]
torch.testing._comparison.NumberPair.compare(self)->None
torch.testing._comparison.NumberPair.extra_repr(self)->Sequence[str]
torch.testing._comparison.ObjectPair(Pair)
torch.testing._comparison.ObjectPair.compare(self)->None
torch.testing._comparison.Pair(self,actual:Any,expected:Any,*,id:Tuple[Any,...]=(),msg:Optional[str]=None,**unknown_parameters:Any)
torch.testing._comparison.Pair.__init__(self,actual:Any,expected:Any,*,id:Tuple[Any,...]=(),msg:Optional[str]=None,**unknown_parameters:Any)
torch.testing._comparison.Pair.__repr__(self)->str
torch.testing._comparison.Pair._check_inputs_isinstance(*inputs:Any,cls:Union[Type,Tuple[Type,...]])
torch.testing._comparison.Pair._make_error_meta(self,type:Type[Exception],msg:str,*,id:Tuple[Any,...]=())->ErrorMeta
torch.testing._comparison.Pair.compare(self)->None
torch.testing._comparison.Pair.extra_repr(self)->Sequence[Union[str, Tuple[str, Any]]]
torch.testing._comparison.TensorLikePair(self,actual:Any,expected:Any,*,id:Tuple[Any,...]=(),allow_subclasses:bool=True,rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=False,check_device:bool=True,check_dtype:bool=True,check_layout:bool=True,check_stride:bool=False,check_is_coalesced:bool=True,**other_parameters:Any)
torch.testing._comparison.TensorLikePair.__init__(self,actual:Any,expected:Any,*,id:Tuple[Any,...]=(),allow_subclasses:bool=True,rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=False,check_device:bool=True,check_dtype:bool=True,check_layout:bool=True,check_stride:bool=False,check_is_coalesced:bool=True,**other_parameters:Any)
torch.testing._comparison.TensorLikePair._check_supported(self,tensor:torch.Tensor,*,id:Tuple[Any,...])->None
torch.testing._comparison.TensorLikePair._compare_attributes(self,actual:torch.Tensor,expected:torch.Tensor)->None
torch.testing._comparison.TensorLikePair._compare_quantized_values(self,actual:torch.Tensor,expected:torch.Tensor,*,rtol:float,atol:float,equal_nan:bool)->None
torch.testing._comparison.TensorLikePair._compare_regular_values_close(self,actual:torch.Tensor,expected:torch.Tensor,*,rtol:float,atol:float,equal_nan:bool,identifier:Optional[Union[str,Callable[[str],str]]]=None)->None
torch.testing._comparison.TensorLikePair._compare_regular_values_equal(self,actual:torch.Tensor,expected:torch.Tensor,*,equal_nan:bool=False,identifier:Optional[Union[str,Callable[[str],str]]]=None)->None
torch.testing._comparison.TensorLikePair._compare_sparse_coo_values(self,actual:torch.Tensor,expected:torch.Tensor,*,rtol:float,atol:float,equal_nan:bool)->None
torch.testing._comparison.TensorLikePair._compare_sparse_csr_values(self,actual:torch.Tensor,expected:torch.Tensor,*,rtol:float,atol:float,equal_nan:bool)->None
torch.testing._comparison.TensorLikePair._compare_values(self,actual:torch.Tensor,expected:torch.Tensor)->None
torch.testing._comparison.TensorLikePair._equalize_attributes(self,actual:torch.Tensor,expected:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
torch.testing._comparison.TensorLikePair._handle_meta_tensor_data_access(self)
torch.testing._comparison.TensorLikePair._process_inputs(self,actual:Any,expected:Any,*,id:Tuple[Any,...],allow_subclasses:bool)->Tuple[torch.Tensor, torch.Tensor]
torch.testing._comparison.TensorLikePair._promote_for_comparison(self,actual:torch.Tensor,expected:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
torch.testing._comparison.TensorLikePair._to_tensor(self,tensor_like:Any)->torch.Tensor
torch.testing._comparison.TensorLikePair.compare(self)->None
torch.testing._comparison.TensorLikePair.extra_repr(self)->Sequence[str]
torch.testing._comparison.UnsupportedInputs(Exception)
torch.testing._comparison._make_mismatch_msg(*,default_identifier:str,identifier:Optional[Union[str,Callable[[str],str]]]=None,extra:Optional[str]=None,abs_diff:float,abs_diff_idx:Optional[Union[int,Tuple[int,...]]]=None,atol:float,rel_diff:float,rel_diff_idx:Optional[Union[int,Tuple[int,...]]]=None,rtol:float)->str
torch.testing._comparison.assert_close(actual:Any,expected:Any,*,allow_subclasses:bool=True,rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=False,check_device:bool=True,check_dtype:bool=True,check_layout:bool=True,check_stride:bool=False,msg:Optional[str]=None)
torch.testing._comparison.assert_equal(actual:Any,expected:Any,*,pair_types:Sequence[Type[Pair]]=(ObjectPair,),sequence_types:Tuple[Type,...]=(collections.abc.Sequence,),mapping_types:Tuple[Type,...]=(collections.abc.Mapping,),**options:Any)->None
torch.testing._comparison.default_tolerances(*inputs:Union[torch.Tensor,torch.dtype])->Tuple[float, float]
torch.testing._comparison.get_tolerances(*inputs:Union[torch.Tensor,torch.dtype],rtol:Optional[float],atol:Optional[float],id:Tuple[Any,...]=())->Tuple[float, float]
torch.testing._comparison.make_scalar_mismatch_msg(actual:Union[int,float,complex],expected:Union[int,float,complex],*,rtol:float,atol:float,identifier:Optional[Union[str,Callable[[str],str]]]=None)->str
torch.testing._comparison.make_tensor_mismatch_msg(actual:torch.Tensor,expected:torch.Tensor,mismatches:torch.Tensor,*,rtol:float,atol:float,identifier:Optional[Union[str,Callable[[str],str]]]=None)
torch.testing._comparison.originate_pairs(actual:Any,expected:Any,*,pair_types:Sequence[Type[Pair]],sequence_types:Tuple[Type,...]=(collections.abc.Sequence,),mapping_types:Tuple[Type,...]=(collections.abc.Mapping,),id:Tuple[Any,...]=(),**options:Any)->List[Pair]
torch.testing.assert_close(actual:Any,expected:Any,*,allow_subclasses:bool=True,rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=False,check_device:bool=True,check_dtype:bool=True,check_layout:bool=True,check_stride:bool=False,msg:Optional[str]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_deprecated.py----------------------------------------
A:torch.testing._deprecated.return_value->fn(*args, **kwargs)
A:torch.testing._deprecated.msg->(head + tail).strip()
A:torch.testing._deprecated.rand->warn_deprecated('Use torch.rand instead.')(torch.rand)
A:torch.testing._deprecated.randn->warn_deprecated('Use torch.randn instead.')(torch.randn)
A:torch.testing._deprecated.(actual_rtol, actual_atol)->_DTYPE_PRECISIONS.get(actual.dtype, (0.0, 0.0))
A:torch.testing._deprecated.(expected_rtol, expected_atol)->_DTYPE_PRECISIONS.get(expected.dtype, (0.0, 0.0))
A:torch.testing._deprecated.actual->torch.tensor(actual)
A:torch.testing._deprecated.expected->torch.tensor(expected, dtype=actual.dtype)
A:torch.testing._deprecated.(rtol, atol)->_get_default_rtol_and_atol(actual, expected)
A:torch.testing._deprecated.fn->getattr(_legacy, name)
A:torch.testing._deprecated.globals()[name]->warn_deprecated(instructions)(fn)
A:torch.testing._deprecated.get_all_device_types->warn_deprecated(instructions)(_legacy.get_all_device_types)
torch.testing._deprecated._get_default_rtol_and_atol(actual:torch.Tensor,expected:torch.Tensor)->Tuple[float, float]
torch.testing._deprecated.assert_allclose(actual:Any,expected:Any,rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=True,msg:str='')->None
torch.testing._deprecated.warn_deprecated(instructions:Union[str,Callable[[str,Tuple[Any,...],Dict[str,Any],Any],str]])->Callable
torch.testing._get_default_rtol_and_atol(actual:torch.Tensor,expected:torch.Tensor)->Tuple[float, float]
torch.testing.assert_allclose(actual:Any,expected:Any,rtol:Optional[float]=None,atol:Optional[float]=None,equal_nan:bool=True,msg:str='')->None
torch.testing.warn_deprecated(instructions:Union[str,Callable[[str,Tuple[Any,...],Dict[str,Any],Any],str]])->Callable


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_creation.py----------------------------------------
A:torch.testing._creation.low->clamp(low, lowest, highest)
A:torch.testing._creation.high->clamp(high, lowest, highest)
A:torch.testing._creation.result->torch.repeat_interleave(result, 2, dim=-1)
A:torch.testing._creation.(low, high)->_modify_low_high(low, high, ranges_floats[0], ranges_floats[1], -9, 9, dtype)
A:torch.testing._creation.rand_val->torch.rand(shape, device=device, dtype=dtype)
A:torch.testing._creation.real_rand_val->torch.rand(shape, device=device, dtype=float_dtype)
A:torch.testing._creation.imag_rand_val->torch.rand(shape, device=device, dtype=float_dtype)
A:torch.testing._creation.replace_with->torch.complex(float_eps, float_eps)
A:torch.testing._creation.float_eps->torch.tensor(torch.finfo(float_dtype).tiny, device=device, dtype=float_dtype)
torch.testing._creation.make_tensor(shape:Union[torch.Size,List[int],Tuple[int,...]],device:Union[str,torch.device],dtype:torch.dtype,*,low:Optional[float]=None,high:Optional[float]=None,requires_grad:bool=False,noncontiguous:bool=False,exclude_zero:bool=False)->torch.Tensor
torch.testing.make_tensor(shape:Union[torch.Size,List[int],Tuple[int,...]],device:Union[str,torch.device],dtype:torch.dtype,*,low:Optional[float]=None,high:Optional[float]=None,requires_grad:bool=False,noncontiguous:bool=False,exclude_zero:bool=False)->torch.Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_fsdp.py----------------------------------------
A:torch.testing._internal.common_fsdp.self.rank->torch.distributed.distributed_c10d._get_default_group().rank()
A:torch.testing._internal.common_fsdp.self.world_size->torch.distributed.distributed_c10d._get_default_group().size()
A:torch.testing._internal.common_fsdp.self.embed_tokens->torch.nn.Embedding(d_vocab, d_model)
A:torch.testing._internal.common_fsdp.self.transformer->torch.nn.Transformer(d_model=d_model, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=8, dropout=0.1)
A:torch.testing._internal.common_fsdp.self.output_proj->torch.nn.Linear(d_model, d_vocab)
A:torch.testing._internal.common_fsdp.self->cls(test_name)
A:torch.testing._internal.common_fsdp.src->cls(test_name).embed_tokens(src_ids)
A:torch.testing._internal.common_fsdp.tgt->cls(test_name).bn(tgt)
A:torch.testing._internal.common_fsdp.x->cls(test_name).transformer(src, tgt)
A:torch.testing._internal.common_fsdp.self.module->torch.nn.Sequential(_maybe_cuda(nn.Linear(d_input, d_shared), self.move_to_cuda), shared, expert, _maybe_cuda(nn.Linear(d_shared, d_input), self.move_to_cuda))
A:torch.testing._internal.common_fsdp.loss->FullyShardedDataParallel(TransformerWithSharedParams(group, **model_kwargs), group).cuda().module.get_loss(input, output).to(model_device)
A:torch.testing._internal.common_fsdp.expert->FullyShardedDataParallel(expert, expert_group, **kwargs)
A:torch.testing._internal.common_fsdp.self.num_expert_params->sum([p.numel() for p in expert.parameters()])
A:torch.testing._internal.common_fsdp.shared->FullyShardedDataParallel(shared, group, **kwargs)
A:torch.testing._internal.common_fsdp.expert_group->torch.distributed.new_group([group.rank()])
A:torch.testing._internal.common_fsdp.backend->os.environ.get('BACKEND', None)
A:torch.testing._internal.common_fsdp.optim->torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
A:torch.testing._internal.common_fsdp.input->FullyShardedDataParallel(TransformerWithSharedParams(group, **model_kwargs), group).cuda().module.get_input(torch.device('cuda'))
A:torch.testing._internal.common_fsdp.output->model(*input)
A:torch.testing._internal.common_fsdp.group->torch.distributed.distributed_c10d._get_default_group()
A:torch.testing._internal.common_fsdp.rank->torch.distributed.distributed_c10d._get_default_group().rank()
A:torch.testing._internal.common_fsdp.model->FullyShardedDataParallel(TransformerWithSharedParams(group, **model_kwargs), group).cuda()
A:torch.testing._internal.common_fsdp.ref_loss->cls(test_name)._train_for_several_steps(model, num_steps, autocast=False, lr=lr, fsdp_cpu_offload=cpu_offload)
A:torch.testing._internal.common_fsdp.ref_full_params->list(model.parameters())
A:torch.testing._internal.common_fsdp.shard_loss->shard_loss.cuda().cuda()
A:torch.testing._internal.common_fsdp.shard_full_params->list(model.parameters())
torch.testing._internal.common_fsdp.DummyDDP(self,module)
torch.testing._internal.common_fsdp.DummyDDP.__init__(self,module)
torch.testing._internal.common_fsdp.DummyDDP.forward(self,*args,**kwargs)
torch.testing._internal.common_fsdp.DummyProcessGroup(self,rank:int,size:int)
torch.testing._internal.common_fsdp.DummyProcessGroup.__init__(self,rank:int,size:int)
torch.testing._internal.common_fsdp.DummyProcessGroup.rank(self)->int
torch.testing._internal.common_fsdp.DummyProcessGroup.size(self)->int
torch.testing._internal.common_fsdp.FSDPInitMode(Enum)
torch.testing._internal.common_fsdp.FSDPTest(MultiProcessTestCase)
torch.testing._internal.common_fsdp.FSDPTest._check_backward_prefetch(self,fsdp_model,backward_prefetch)
torch.testing._internal.common_fsdp.FSDPTest._check_cpu_offload(self,fsdp_model,cpu_offload)
torch.testing._internal.common_fsdp.FSDPTest._get_wrapped_model(self,group,cuda_first=False,**model_kwargs)->FullyShardedDataParallel
torch.testing._internal.common_fsdp.FSDPTest._run(cls,rank,test_name,file_name,pipe)
torch.testing._internal.common_fsdp.FSDPTest._test_identical_outputs(self,model_init_fn,*args,ref_ddp_fn=None,num_steps=2,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,lr=0.01,cpu_offload=CPUOffload(),backward_prefetch=None,**kwargs)
torch.testing._internal.common_fsdp.FSDPTest._train_for_several_steps(self,model,num_steps,autocast,lr=0.01,fsdp_cpu_offload=None)
torch.testing._internal.common_fsdp.FSDPTest.init_method(self)
torch.testing._internal.common_fsdp.FSDPTest.setUp(self)
torch.testing._internal.common_fsdp.FSDPTest.world_size(self)
torch.testing._internal.common_fsdp.MixtureOfExperts(self,group,wrap_fsdp,*args,delay_before_free_ms=0,fsdp_init_mode=FSDPInitMode.CUDA_BEFORE,**kwargs)
torch.testing._internal.common_fsdp.MixtureOfExperts.__init__(self,group,wrap_fsdp,*args,delay_before_free_ms=0,fsdp_init_mode=FSDPInitMode.CUDA_BEFORE,**kwargs)
torch.testing._internal.common_fsdp.MixtureOfExperts.forward(self,x)
torch.testing._internal.common_fsdp.MixtureOfExperts.run_backward(self,loss)
torch.testing._internal.common_fsdp.ModuleWithDelay(self,module,delay_after_loss_ms=0,delay_before_reduction_ms=0)
torch.testing._internal.common_fsdp.ModuleWithDelay.__init__(self,module,delay_after_loss_ms=0,delay_before_reduction_ms=0)
torch.testing._internal.common_fsdp.ModuleWithDelay.forward(self,x)
torch.testing._internal.common_fsdp.ModuleWithDelay.get_input(self,device)
torch.testing._internal.common_fsdp.ModuleWithDelay.get_loss(self,input,output)
torch.testing._internal.common_fsdp.ModuleWithDelay.run_backward(self,loss)
torch.testing._internal.common_fsdp.NestedWrappedModule(self,group,wrap_fsdp,*args,wrap_everything=False,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,**kwargs)
torch.testing._internal.common_fsdp.NestedWrappedModule.__init__(self,group,wrap_fsdp,*args,wrap_everything=False,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,**kwargs)
torch.testing._internal.common_fsdp.NestedWrappedModule.forward(self,x)
torch.testing._internal.common_fsdp.NestedWrappedModule.get_input(self,device)
torch.testing._internal.common_fsdp.NestedWrappedModule.get_loss(self,input,output)
torch.testing._internal.common_fsdp.NestedWrappedModule.run_backward(self,loss)
torch.testing._internal.common_fsdp.NestedWrappedModuleWithDelay(self,group,wrap_fsdp,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,cpu_offload=None,backward_prefetch=None,**kwargs)
torch.testing._internal.common_fsdp.NestedWrappedModuleWithDelay.__init__(self,group,wrap_fsdp,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,cpu_offload=None,backward_prefetch=None,**kwargs)
torch.testing._internal.common_fsdp.TransformerWithSharedParams(self,group,*args,d_vocab=23,d_model=16,add_bn=True,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,**kwargs)
torch.testing._internal.common_fsdp.TransformerWithSharedParams.__init__(self,group,*args,d_vocab=23,d_model=16,add_bn=True,fsdp_init_mode=FSDPInitMode.CUDA_AFTER,**kwargs)
torch.testing._internal.common_fsdp.TransformerWithSharedParams.forward(self,src_ids,tgt_ids)
torch.testing._internal.common_fsdp.TransformerWithSharedParams.get_input(self,device)
torch.testing._internal.common_fsdp.TransformerWithSharedParams.get_loss(self,input,output)
torch.testing._internal.common_fsdp.TransformerWithSharedParams.run_backward(self,loss)
torch.testing._internal.common_fsdp._maybe_cuda(model,move_to_cuda)
torch.testing._internal.common_fsdp._maybe_wrap_fsdp(model,wrap_fsdp,*args,**kwargs)
torch.testing._internal.common_fsdp.get_full_params(model,recurse=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/opinfo_helper.py----------------------------------------
A:torch.testing._internal.opinfo_helper.supported_dtypes->set()
A:torch.testing._internal.opinfo_helper.samples->sample_inputs_fn(op, device_type, dtype, False)
A:torch.testing._internal.opinfo_helper.return_type->collections.namedtuple('return_type', 'dispatch_fn dispatch_fn_str')
A:torch.testing._internal.opinfo_helper.set_dtypes->set(dtypes)
A:torch.testing._internal.opinfo_helper.dispatch_dtypes->set(dispatch())
A:torch.testing._internal.opinfo_helper.score->len(dispatch_dtypes)
A:torch.testing._internal.opinfo_helper.fmt_str->'\n        OpInfo({name},\n               dtypes={dtypes},\n               dtypesIfCUDA={dtypesIfCUDA},\n        )\n        '.format(name=op.name, dtypes=dtypes_dispatch_hint(op.dtypes).dispatch_fn_str, dtypesIfCUDA=dtypes_dispatch_hint(op.dtypesIfCUDA).dispatch_fn_str)
torch.testing._internal.opinfo_helper._dynamic_dispatch_dtypes(_dispatch_dtypes)
torch.testing._internal.opinfo_helper.dtypes_dispatch_hint(dtypes)
torch.testing._internal.opinfo_helper.get_supported_dtypes(op,sample_inputs_fn,device_type)
torch.testing._internal.opinfo_helper.is_dynamic_dtype_set(op)
torch.testing._internal.opinfo_helper.str_format_dynamic_dtype(op)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_distributed.py----------------------------------------
A:torch.testing._internal.common_distributed.logger->logging.getLogger(__name__)
A:torch.testing._internal.common_distributed.skip_collective['reduce']->set()
A:torch.testing._internal.common_distributed.backend_feature['plugin']->set()
A:torch.testing._internal.common_distributed.world_size->int(os.environ['WORLD_SIZE'])
A:torch.testing._internal.common_distributed.ddp_logging_data->model_DDP._get_ddp_logging_data()
A:torch.testing._internal.common_distributed.ret->func(*args, **kwargs)
A:torch.testing._internal.common_distributed.old_level->os.environ.get('TORCH_DISTRIBUTED_DEBUG', None)
A:torch.testing._internal.common_distributed.port->find_free_port()
A:torch.testing._internal.common_distributed.timeout_millisecond->int(timeout / timedelta(milliseconds=1))
A:torch.testing._internal.common_distributed.indices->torch.cat((indices, torch.zeros(1, rank + 1)))
A:torch.testing._internal.common_distributed.values->torch.ones([rank + 1] + [2 for _ in range(dense_dims)])
A:torch.testing._internal.common_distributed.nGPUs->torch.cuda.device_count()
A:torch.testing._internal.common_distributed.visible_devices->range(nGPUs)
A:torch.testing._internal.common_distributed.tmp_dir->tempfile.TemporaryDirectory()
A:torch.testing._internal.common_distributed.init_dir_path->os.path.join(tmp_dir.name, 'init_dir')
A:torch.testing._internal.common_distributed.fn->getattr(self, method_name)
A:torch.testing._internal.common_distributed.(parent_conn, child_conn)->torch.multiprocessing.Pipe()
A:torch.testing._internal.common_distributed.process->proc(target=self.__class__._run, name='process ' + str(rank), args=(rank, self._current_test_name(), self.file_name, child_conn))
A:torch.testing._internal.common_distributed.ready_pipes->multiprocessing.connection.wait([parent_pipe, signal_pipe])
A:torch.testing._internal.common_distributed.event->parent_pipe.recv()
A:torch.testing._internal.common_distributed.self->cls(test_name)
A:torch.testing._internal.common_distributed.(signal_recv_pipe, signal_send_pipe)->torch.multiprocessing.Pipe(duplex=False)
A:torch.testing._internal.common_distributed.event_listener_thread->threading.Thread(target=MultiProcessTestCase._event_listener, args=(parent_pipe, signal_recv_pipe, self.rank), daemon=True)
A:torch.testing._internal.common_distributed.traceback->pipe.recv()
A:torch.testing._internal.common_distributed.timeout->get_timeout(self.id())
A:torch.testing._internal.common_distributed.start_time->time.time()
A:torch.testing._internal.common_distributed.active_children->torch.multiprocessing.active_children()
A:torch.testing._internal.common_distributed.error_message->cls(test_name).pid_to_pipe[process.pid].recv()
torch.testing._internal.common_distributed.DistTestCases
torch.testing._internal.common_distributed.MultiProcessTestCase(self,method_name:str='runTest')
torch.testing._internal.common_distributed.MultiProcessTestCase.Event(Enum)
torch.testing._internal.common_distributed.MultiProcessTestCase.__init__(self,method_name:str='runTest')
torch.testing._internal.common_distributed.MultiProcessTestCase._check_no_test_errors(self,elapsed_time)->None
torch.testing._internal.common_distributed.MultiProcessTestCase._check_return_codes(self,elapsed_time)->None
torch.testing._internal.common_distributed.MultiProcessTestCase._current_test_name(self)->str
torch.testing._internal.common_distributed.MultiProcessTestCase._event_listener(parent_pipe,signal_pipe,rank:int)
torch.testing._internal.common_distributed.MultiProcessTestCase._get_timedout_process_traceback(self)->None
torch.testing._internal.common_distributed.MultiProcessTestCase._join_processes(self,fn)->None
torch.testing._internal.common_distributed.MultiProcessTestCase._run(cls,rank:int,test_name:str,file_name:str,parent_pipe)->None
torch.testing._internal.common_distributed.MultiProcessTestCase._should_stop_test_suite(self)->bool
torch.testing._internal.common_distributed.MultiProcessTestCase._spawn_processes(self)->None
torch.testing._internal.common_distributed.MultiProcessTestCase._start_processes(self,proc)->None
torch.testing._internal.common_distributed.MultiProcessTestCase.is_master(self)->bool
torch.testing._internal.common_distributed.MultiProcessTestCase.join_or_run(self,fn)
torch.testing._internal.common_distributed.MultiProcessTestCase.run_test(self,test_name:str,parent_pipe)->None
torch.testing._internal.common_distributed.MultiProcessTestCase.setUp(self)->None
torch.testing._internal.common_distributed.MultiProcessTestCase.tearDown(self)->None
torch.testing._internal.common_distributed.MultiProcessTestCase.world_size(self)->int
torch.testing._internal.common_distributed.TestSkip(NamedTuple)
torch.testing._internal.common_distributed.captured_output()
torch.testing._internal.common_distributed.cleanup_temp_dir()->None
torch.testing._internal.common_distributed.create_device(interface=None)
torch.testing._internal.common_distributed.create_tcp_store(addr='localhost',world_size=1,is_master=True,timeout=timedelta(minutes=5),wait_for_workers=True,jit_class=False)
torch.testing._internal.common_distributed.get_timeout(test_id)->int
torch.testing._internal.common_distributed.init_multigpu_helper(world_size:int,backend:str)
torch.testing._internal.common_distributed.initialize_temp_directories(init_method:Optional[str]=None)->None
torch.testing._internal.common_distributed.nccl_skip_if_lt_x_gpu(backend,x)
torch.testing._internal.common_distributed.require_n_gpus_for_nccl_backend(n,backend)
torch.testing._internal.common_distributed.requires_gloo()
torch.testing._internal.common_distributed.requires_mpi()
torch.testing._internal.common_distributed.requires_nccl()
torch.testing._internal.common_distributed.requires_nccl_version(version,msg)
torch.testing._internal.common_distributed.simple_sparse_reduce_tests(rank:int,world_size:int,num_inputs:int=1)
torch.testing._internal.common_distributed.skip_if_lt_x_gpu(x)
torch.testing._internal.common_distributed.skip_if_no_gpu(func)
torch.testing._internal.common_distributed.skip_if_rocm(func)
torch.testing._internal.common_distributed.skip_if_small_worldsize(func)
torch.testing._internal.common_distributed.skip_if_win32()
torch.testing._internal.common_distributed.verify_ddp_error_logged(model_DDP,err_substr)
torch.testing._internal.common_distributed.with_dist_debug_levels(levels)
torch.testing._internal.common_distributed.with_nccl_blocking_wait(func)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/autocast_test_lists.py----------------------------------------
torch.testing._internal.autocast_test_lists.AutocastCPUTestLists(self,dev)
torch.testing._internal.autocast_test_lists.AutocastCPUTestLists.__init__(self,dev)
torch.testing._internal.autocast_test_lists.AutocastTestLists(self,dev)
torch.testing._internal.autocast_test_lists.AutocastTestLists.__init__(self,dev)
torch.testing._internal.autocast_test_lists.AutocastTestLists._rnn_cell_args(self,n,num_chunks,is_lstm,dev,dtype)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/logging_tensor.py----------------------------------------
A:torch.testing._internal.logging_tensor.guard->torch._C._DisableTorchDispatch()
A:torch.testing._internal.logging_tensor.r->torch.Tensor._make_wrapper_subclass(cls, elem.size(), strides=elem.stride(), storage_offset=elem.storage_offset(), dtype=elem.dtype, layout=elem.layout, device=elem.device, requires_grad=elem.requires_grad)
A:torch.testing._internal.logging_tensor.rs->tree_map(wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))
A:torch.testing._internal.logging_tensor.fmt_args->', '.join(itertools.chain((self._fmt(a) for a in record.args[0]), (f'{k}={self._fmt(v)}' for (k, v) in record.args[1].items())))
A:torch.testing._internal.logging_tensor.logger->logging.getLogger('LoggingTensor')
A:torch.testing._internal.logging_tensor.handler->LoggingTensorHandler(log_list)
torch.testing._internal.logging_tensor.LoggingTensor(cls,elem,*args,**kwargs)
torch.testing._internal.logging_tensor.LoggingTensor.__new__(cls,elem,*args,**kwargs)
torch.testing._internal.logging_tensor.LoggingTensor.__repr__(self)
torch.testing._internal.logging_tensor.LoggingTensor.__torch_dispatch__(cls,func,types,args=(),kwargs=None)
torch.testing._internal.logging_tensor.LoggingTensorHandler(self,log_list:List[str])
torch.testing._internal.logging_tensor.LoggingTensorHandler.__init__(self,log_list:List[str])
torch.testing._internal.logging_tensor.LoggingTensorHandler._fmt(self,a:object)->str
torch.testing._internal.logging_tensor.LoggingTensorHandler._shortid(self,o:object)->int
torch.testing._internal.logging_tensor.LoggingTensorHandler.emit(self,record)
torch.testing._internal.logging_tensor.capture_logs()->Iterator[List[str]]
torch.testing._internal.logging_tensor.log_input(name:str,var:object)
torch.testing._internal.logging_tensor.no_dispatch()->Iterator[None]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_dtype.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_jit.py----------------------------------------
A:torch.testing._internal.common_jit.graph->getattr(func, 'last_graph', None)
A:torch.testing._internal.common_jit.nn_functional_single_grad->frozenset(('test_nn_' + name for name in ['pdist', 'multilabel_margin_loss', 'max_unpool3d', 'multi_margin_loss', 'binary_cross_entropy', 'binary_cross_entropy_size_average', 'ctc_loss', 'grid_sample']))
A:torch.testing._internal.common_jit.nograd_inputs->clone_inputs(preserve_requires_grad=False)
A:torch.testing._internal.common_jit.outputs->output_func(self.runAndSaveRNG(reference_func, recording_inputs, kwargs))
A:torch.testing._internal.common_jit.outputs_test->output_func(self.runAndSaveRNG(func, recording_inputs, kwargs))
A:torch.testing._internal.common_jit.recording_inputs->clone_inputs(preserve_requires_grad=True)
A:torch.testing._internal.common_jit.recording_tensors->get_recording_tensors(recording_inputs)
A:torch.testing._internal.common_jit.grads->torch.autograd.grad(l1, recording_tensors, create_graph=True, allow_unused=allow_unused)
A:torch.testing._internal.common_jit.grads_test->torch.autograd.grad(l1_test, recording_tensors, create_graph=True, allow_unused=allow_unused)
A:torch.testing._internal.common_jit.l1->allSum(outputs)
A:torch.testing._internal.common_jit.grads2->torch.autograd.grad(l2, recording_tensors, allow_unused=allow_unused)
A:torch.testing._internal.common_jit.l1_test->allSum(outputs_test)
A:torch.testing._internal.common_jit.grads2_test->torch.autograd.grad(l2_test, recording_tensors, allow_unused=allow_unused)
A:torch.testing._internal.common_jit.m->self.createFunctionFromGraph(trace)
A:torch.testing._internal.common_jit.m_import->self.getExportImportCopy(m)
A:torch.testing._internal.common_jit.a->self.runAndSaveRNG(m, inputs)
A:torch.testing._internal.common_jit.b->self.runAndSaveRNG(m_import, inputs)
A:torch.testing._internal.common_jit.results->func(*inputs, **kwargs)
A:torch.testing._internal.common_jit.buffer->io.BytesIO()
A:torch.testing._internal.common_jit.imported->torch.jit.load(buffer, map_location=map_location)
A:torch.testing._internal.common_jit.diff_nodes->getattr(func, 'last_graph', None).findAllNodes('prim::DifferentiableGraph')
A:torch.testing._internal.common_jit.fusion_nodes->list(chain.from_iterable([g.findAllNodes('prim::FusionGroup') for g in diff_subgraphs]))
A:torch.testing._internal.common_jit.err_msg->self.autoDiffErrorMessage(should_autodiff_node, nodes_not_in_diff_graph, fusion_nodes_not_found, non_fusible_nodes_being_fused, fusion_nodes_found, nodes_in_diff_graph)
A:torch.testing._internal.common_jit.prev_symbolic_shapes_test_enabled->torch._C._jit_symbolic_shapes_test_mode_enabled()
A:torch.testing._internal.common_jit.output->next(traced_graph.outputs()).type()
A:torch.testing._internal.common_jit.sizes->type.symbolic_sizes()
A:torch.testing._internal.common_jit.out_type->torch._C.TensorType.get().with_sizes(sizes)
A:torch.testing._internal.common_jit.actual_type->torch._C.TensorType.get().with_sizes(actual_size)
A:torch.testing._internal.common_jit.tuple_elements->next(traced_graph.outputs()).type().elements()
torch.testing._internal.common_jit.JitCommonTestCase(TestCase)
torch.testing._internal.common_jit.JitCommonTestCase.assertAutodiffNode(self,graph,should_autodiff_node,nonfusible_nodes,fusible_nodes)
torch.testing._internal.common_jit.JitCommonTestCase.assertExportImport(self,trace,inputs)
torch.testing._internal.common_jit.JitCommonTestCase.assertExportImportModule(self,m,inputs)
torch.testing._internal.common_jit.JitCommonTestCase.autoDiffErrorMessage(self,should_autodiff_node,nodes_not_in_diff_graph,fusion_nodes_not_found,non_fusible_nodes_being_fused,fusion_nodes_found,nodes_in_diff_graph)
torch.testing._internal.common_jit.JitCommonTestCase.checkShapeAnalysis(self,out_sizes:Union[List[int],List[List[int]]],traced_graph,assert_propagation,constant_prop=True)
torch.testing._internal.common_jit.JitCommonTestCase.createFunctionFromGraph(self,trace)
torch.testing._internal.common_jit.JitCommonTestCase.getExportImportCopy(self,m,also_test_file=True,map_location=None)
torch.testing._internal.common_jit.JitCommonTestCase.runAndSaveRNG(self,func,inputs,kwargs=None)
torch.testing._internal.common_jit.check_against_reference(self,func,reference_func,output_func,args,kwargs=None,allow_unused=True,check_types=True,no_grad=False,no_gradgrad=False)
torch.testing._internal.common_jit.check_output_types(self,func,ref_outputs,args,kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_fx2trt.py----------------------------------------
A:torch.testing._internal.common_fx2trt.target_atoms->target.split('.')
A:torch.testing._internal.common_fx2trt.attr_itr->getattr(attr_itr, atom)
A:torch.testing._internal.common_fx2trt.interpreter_result->interpreter.run(fp16_mode=fp16_mode)
A:torch.testing._internal.common_fx2trt.trt_mod->TRTModule(interpreter_result.engine, interpreter_result.input_names, interpreter_result.output_names)
A:torch.testing._internal.common_fx2trt.ref_outputs->mod(*inputs)
A:torch.testing._internal.common_fx2trt.outputs->trt_mod(*cuda_inputs)
A:torch.testing._internal.common_fx2trt.res_trt->trt_mod(*cuda_inputs).cpu()
A:torch.testing._internal.common_fx2trt.res_cpu->mod(*inputs)
A:torch.testing._internal.common_fx2trt.ops_in_mod->set()
A:torch.testing._internal.common_fx2trt.mod->torch.fx.experimental.fx_acc.acc_tracer.trace(mod, inputs)
A:torch.testing._internal.common_fx2trt.interp->TRTInterpreter(mod, input_specs, explicit_batch_dimension=True)
A:torch.testing._internal.common_fx2trt.inputs->torch.fx.experimental.fx2trt.InputTensorSpec.create_inputs_from_specs(input_specs)
torch.testing._internal.common_fx2trt.AccTestCase(TRTTestCase)
torch.testing._internal.common_fx2trt.AccTestCase.run_test(self,mod,inputs,expected_ops,unexpected_ops=None,apply_passes=None,test_explicit_batch_dim=True,test_implicit_batch_dim=True,rtol=0.001,atol=0.001)
torch.testing._internal.common_fx2trt.AccTestCase.run_test_with_assert_error(self,mod,inputs,expect_error,test_explicit_batch_dim=True,test_implicit_batch_dim=True)
torch.testing._internal.common_fx2trt.AccTestCase.run_test_with_dynamic_shape(self,mod,input_specs,expected_ops,unexpected_ops=None,rtol=0.001,atol=0.001)
torch.testing._internal.common_fx2trt.TRTTestCase(TestCase)
torch.testing._internal.common_fx2trt.TRTTestCase.assert_has_op(self,mod,ops)
torch.testing._internal.common_fx2trt.TRTTestCase.assert_unexpected_op(self,mod,ops)
torch.testing._internal.common_fx2trt.TRTTestCase.run_test(self,mod,inputs,expected_ops,unexpected_ops,interpreter,rtol,atol)
torch.testing._internal.common_fx2trt.TRTTestCase.run_test_custom_compare_results(self,mod,inputs,expected_ops,interpreter,comparators:List[Tuple[Callable,List]],fp16_mode=False)
torch.testing._internal.common_fx2trt.TRTTestCase.run_test_with_error(self,mod,inputs,interpreter,expect_error)
torch.testing._internal.common_fx2trt.TRTTestCase.setUp(self)
torch.testing._internal.common_fx2trt.VanillaTestCase(TRTTestCase)
torch.testing._internal.common_fx2trt.VanillaTestCase.run_test(self,mod,inputs,expected_ops,rtol=1e-05,atol=1e-06)
torch.testing._internal.common_fx2trt.VanillaTestCase.run_test_custom_compare_results(self,mod,inputs,expected_ops,interpreter,comparators:List[Tuple[Callable,List]],fp16_mode=False)
torch.testing._internal.common_fx2trt.fetch_attr(mod,target)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/dist_utils.py----------------------------------------
A:torch.testing._internal.dist_utils.return_value->old_test_method(self, *arg, **kwargs)
A:torch.testing._internal.dist_utils.start->time.time()
A:torch.testing._internal.dist_utils.debug_info->_rref_context_get_debug_info()
A:torch.testing._internal.dist_utils.num_pending_futures->int(debug_info['num_pending_futures'])
A:torch.testing._internal.dist_utils.num_pending_users->int(debug_info['num_pending_users'])
A:torch.testing._internal.dist_utils.rref_dbg_info->_rref_context_get_debug_info()
A:torch.testing._internal.dist_utils.(num_owners_on_rank, num_forks_on_rank)->torch.distributed.rpc.rpc_sync(worker_name(rank), get_num_owners_and_forks, args=(), timeout=5)
A:torch.testing._internal.dist_utils.num_owners_on_rank->int(num_owners_on_rank)
A:torch.testing._internal.dist_utils.num_forks_on_rank->int(num_forks_on_rank)
torch.testing._internal.dist_utils.dist_init(old_test_method=None,setup_rpc:bool=True,clean_shutdown:bool=True,faulty_messages=None,messages_to_delay=None)
torch.testing._internal.dist_utils.get_function_event(function_events,partial_event_name)
torch.testing._internal.dist_utils.get_num_owners_and_forks()->Tuple[str, str]
torch.testing._internal.dist_utils.initialize_pg(init_method,rank:int,world_size:int)->None
torch.testing._internal.dist_utils.noop()->None
torch.testing._internal.dist_utils.wait_until_node_failure(rank:int,expected_error_regex:str='.*')->str
torch.testing._internal.dist_utils.wait_until_owners_and_forks_on_rank(num_owners:int,num_forks:int,rank:int,timeout:int=20)->None
torch.testing._internal.dist_utils.wait_until_pending_futures_and_users_flushed(timeout:int=20)->None
torch.testing._internal.dist_utils.worker_name(rank:int)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/check_kernel_launches.py----------------------------------------
A:torch.testing._internal.check_kernel_launches.kernel_launch_start->re.compile('^.*<<<[^>]+>>>\\s*\\(', flags=re.MULTILINE)
A:torch.testing._internal.check_kernel_launches.has_check->re.compile('\\s*;(?![^;}]*C10_CUDA_KERNEL_LAUNCH_CHECK\\(\\);)', flags=re.MULTILINE)
A:torch.testing._internal.check_kernel_launches.code->'\n'.join(code)
A:torch.testing._internal.check_kernel_launches.end_paren->find_matching_paren(code, m.end() - 1)
A:torch.testing._internal.check_kernel_launches.fo->open(filename, 'r')
A:torch.testing._internal.check_kernel_launches.contents->open(filename, 'r').read()
A:torch.testing._internal.check_kernel_launches.unsafeCount->check_code_for_cuda_kernel_launches(contents, filename)
A:torch.testing._internal.check_kernel_launches.torch_dir->os.path.dirname(torch_dir)
A:torch.testing._internal.check_kernel_launches.filename->os.path.join(root, x)
A:torch.testing._internal.check_kernel_launches.file_result->check_file(filename)
A:torch.testing._internal.check_kernel_launches.unsafe_launches->check_cuda_kernel_launches()
torch.testing._internal.check_kernel_launches.check_code_for_cuda_kernel_launches(code,filename=None)
torch.testing._internal.check_kernel_launches.check_cuda_kernel_launches()
torch.testing._internal.check_kernel_launches.check_file(filename)
torch.testing._internal.check_kernel_launches.find_matching_paren(s:str,startpos:int)->int
torch.testing._internal.check_kernel_launches.should_exclude_file(filename)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/hypothesis_utils.py----------------------------------------
A:torch.testing._internal.hypothesis_utils._ENFORCED_ZERO_POINT->defaultdict(lambda : None, {torch.quint8: None, torch.qint8: None, torch.qint32: 0})
A:torch.testing._internal.hypothesis_utils._long_type_info->torch.iinfo(torch.long)
A:torch.testing._internal.hypothesis_utils.min_value->max((long_min - zero_point) * scale, long_min / scale + zero_point)
A:torch.testing._internal.hypothesis_utils.max_value->min((long_max - zero_point) * scale, long_max / scale + zero_point)
A:torch.testing._internal.hypothesis_utils.(min_value, max_value)->_get_valid_min_max(qparams)
A:torch.testing._internal.hypothesis_utils.quantized_type->draw(st.sampled_from(dtypes))
A:torch.testing._internal.hypothesis_utils._type_info->torch.iinfo(quantized_type)
A:torch.testing._internal.hypothesis_utils.zero_point->draw(st.integers(min_value=_zp_min, max_value=_zp_max))
A:torch.testing._internal.hypothesis_utils.scale->draw(floats(min_value=scale_min, max_value=scale_max, width=32))
A:torch.testing._internal.hypothesis_utils.max_dims->min(min_dims + 2, 32)
A:torch.testing._internal.hypothesis_utils.candidate->candidate.filter(lambda x: reduce(int.__mul__, x, 1) <= max_numel).filter(lambda x: reduce(int.__mul__, x, 1) <= max_numel)
A:torch.testing._internal.hypothesis_utils._shape->draw(st.sampled_from(shapes))
A:torch.testing._internal.hypothesis_utils.elements->floats(min_value, max_value, allow_infinity=False, allow_nan=False, width=32)
A:torch.testing._internal.hypothesis_utils.X->draw(tensor(shapes=((batch_size, input_channels) + tuple(feature_map_shape),), elements=elements, qparams=qparams[0]))
A:torch.testing._internal.hypothesis_utils.qparams->draw(qparams)
A:torch.testing._internal.hypothesis_utils.(scale, zp)->_calculate_dynamic_per_channel_qparams(X, qparams[2])
A:torch.testing._internal.hypothesis_utils.enforced_zp->defaultdict(lambda : None, {torch.quint8: None, torch.qint8: None, torch.qint32: 0}).get(qparams[2], None)
A:torch.testing._internal.hypothesis_utils.axis->int(np.random.randint(0, X.ndim, 1))
A:torch.testing._internal.hypothesis_utils.permute_axes->numpy.arange(X.ndim)
A:torch.testing._internal.hypothesis_utils.batch_size->draw(st.integers(*batch_size_range))
A:torch.testing._internal.hypothesis_utils.input_channels_per_group->draw(st.integers(*input_channels_per_group_range))
A:torch.testing._internal.hypothesis_utils.output_channels_per_group->draw(st.integers(*output_channels_per_group_range))
A:torch.testing._internal.hypothesis_utils.groups->draw(st.integers(1, max_groups))
A:torch.testing._internal.hypothesis_utils.spatial_dim->draw(st.sampled_from(spatial_dim))
A:torch.testing._internal.hypothesis_utils.tr->draw(st.booleans())
A:torch.testing._internal.hypothesis_utils.W->draw(tensor(shapes=(weight_shape,), elements=elements, qparams=qparams[1]))
A:torch.testing._internal.hypothesis_utils.b->draw(tensor(shapes=(bias_shape,), elements=elements, qparams=qparams[2]))
A:torch.testing._internal.hypothesis_utils.warning_message->'Your version of hypothesis is outdated. To avoid `DeadlineExceeded` errors, please update. Current hypothesis version: {}'.format(hypothesis.__version__)
torch.testing._internal.hypothesis_utils._floats_wrapper(*args,**kwargs)
torch.testing._internal.hypothesis_utils._get_valid_min_max(qparams)
torch.testing._internal.hypothesis_utils.array_shapes(draw,min_dims=1,max_dims=None,min_side=1,max_side=None,max_numel=None)
torch.testing._internal.hypothesis_utils.assert_deadline_disabled()
torch.testing._internal.hypothesis_utils.assume_not_overflowing(tensor,qparams)
torch.testing._internal.hypothesis_utils.floats(*args,**kwargs)
torch.testing._internal.hypothesis_utils.per_channel_tensor(draw,shapes=None,elements=None,qparams=None)
torch.testing._internal.hypothesis_utils.qparams(draw,dtypes=None,scale_min=None,scale_max=None,zero_point_min=None,zero_point_max=None)
torch.testing._internal.hypothesis_utils.tensor(draw,shapes=None,elements=None,qparams=None)
torch.testing._internal.hypothesis_utils.tensor_conv(draw,spatial_dim=2,batch_size_range=(1,4),input_channels_per_group_range=(3,7),output_channels_per_group_range=(3,7),feature_map_range=(6,12),kernel_range=(3,7),max_groups=1,can_be_transposed=False,elements=None,qparams=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_utils.py----------------------------------------
A:torch.testing._internal.common_utils.fn.parametrize_fn->compose_parametrize_fns(old_parametrize_fn, new_parametrize_fn)
A:torch.testing._internal.common_utils.redundant_params->set(old_param_kwargs.keys()).intersection(new_param_kwargs.keys())
A:torch.testing._internal.common_utils.merged_test_name->'{}{}{}'.format(new_test_name, '_' if old_test_name != '' and new_test_name != '' else '', old_test_name)
A:torch.testing._internal.common_utils.class_attr->getattr(generic_cls, attr_name)
A:torch.testing._internal.common_utils.full_name->'{}_{}'.format(test.__name__, test_suffix)
A:torch.testing._internal.common_utils.self.arg_names->arg_str.split(',')
A:torch.testing._internal.common_utils.subtest_name->self._default_subtest_name(values)
A:torch.testing._internal.common_utils.test_wrapper->decorator(test_wrapper)
A:torch.testing._internal.common_utils.test_name->self._get_subtest_name(values, explicit_name=maybe_name)
A:torch.testing._internal.common_utils.old_prof_exec_state->kwargs.get('torch', globals()['torch'])._C._jit_set_profiling_executor(True)
A:torch.testing._internal.common_utils.old_prof_mode_state->kwargs.get('torch', globals()['torch'])._C._jit_set_profiling_mode(True)
A:torch.testing._internal.common_utils.old_num_runs->kwargs.get('torch', globals()['torch'])._C._jit_set_num_profiled_runs(num_runs)
A:torch.testing._internal.common_utils.override->os.environ.get('TEST_REPORT_SOURCE_OVERRIDE')
A:torch.testing._internal.common_utils.parser->argparse.ArgumentParser()
A:torch.testing._internal.common_utils.help_thread->threading.Thread(target=run_unittest_help, args=(sys.argv,))
A:torch.testing._internal.common_utils.(args, remaining)->argparse.ArgumentParser().parse_known_args()
A:torch.testing._internal.common_utils.GRAPH_EXECUTOR->cppProfilingFlagsToProfilingMode()
A:torch.testing._internal.common_utils.CI_TEST_PREFIX->str(Path(os.getcwd()))
A:torch.testing._internal.common_utils.exit_status->subprocess.Popen(command, universal_newlines=True, cwd=cwd, env=env).wait(timeout=5)
A:torch.testing._internal.common_utils.p->subprocess.Popen(command, universal_newlines=True, cwd=cwd, env=env)
A:torch.testing._internal.common_utils.suite->unittest.TestLoader().loadTestsFromModule(__main__)
A:torch.testing._internal.common_utils.test_cases->discover_test_cases_recursively(suite)
A:torch.testing._internal.common_utils.strip_py->re.sub('.py$', '', filename)
A:torch.testing._internal.common_utils.slow_tests_dict->json.load(fp)
A:torch.testing._internal.common_utils.disabled_tests_dict->json.load(fp)
A:torch.testing._internal.common_utils.string_cmd->' '.join(cmd)
A:torch.testing._internal.common_utils.exitcode->shell(cmd)
A:torch.testing._internal.common_utils.backend->os.environ.get('BACKEND', '')
A:torch.testing._internal.common_utils.world_size->os.environ.get('WORLD_SIZE', '')
A:torch.testing._internal.common_utils.test_batches->chunk_list(get_test_names(test_cases), RUN_PARALLEL)
A:torch.testing._internal.common_utils.test_filename->sanitize_test_filename(inspect.getfile(sys._getframe(1)))
A:torch.testing._internal.common_utils.test_report_path->os.path.join(test_report_path, test_filename)
A:torch.testing._internal.common_utils.lines->tempfile.NamedTemporaryFile(*args, **kwargs).read()
A:torch.testing._internal.common_utils.IS_AVX512_VNNI_SUPPORTED->is_avx512_vnni_supported()
A:torch.testing._internal.common_utils.f->tempfile.NamedTemporaryFile(*args, **kwargs)
A:torch.testing._internal.common_utils.dir_name->tempfile.mkdtemp(suffix=suffix)
A:torch.testing._internal.common_utils.spec->importlib.util.find_spec(name)
A:torch.testing._internal.common_utils.TEST_NUMPY->_check_module_exists('numpy')
A:torch.testing._internal.common_utils.TEST_SCIPY->_check_module_exists('scipy')
A:torch.testing._internal.common_utils.TEST_MKL->kwargs.get('torch', globals()['torch']).backends.mkl.is_available()
A:torch.testing._internal.common_utils.TEST_CUDA->kwargs.get('torch', globals()['torch']).cuda.is_available()
A:torch.testing._internal.common_utils.TEST_NUMBA->_check_module_exists('numba')
A:torch.testing._internal.common_utils.TEST_DILL->_check_module_exists('dill')
A:torch.testing._internal.common_utils.TEST_LIBROSA->_check_module_exists('librosa')
A:torch.testing._internal.common_utils.BUILD_WITH_CAFFE2->_check_module_exists('caffe2.python.caffe2_pybind11_state')
A:torch.testing._internal.common_utils.rocm_version->str(torch.version.hip)
A:torch.testing._internal.common_utils.rocm_version_tuple->tuple((int(x) for x in rocm_version.split('.')))
A:torch.testing._internal.common_utils.reason->'ROCm {0} is available but {1} required'.format(rocm_version_tuple, version)
A:torch.testing._internal.common_utils.self.deterministic_restore->kwargs.get('torch', globals()['torch']).are_deterministic_algorithms_enabled()
A:torch.testing._internal.common_utils.self.warn_only_restore->kwargs.get('torch', globals()['torch']).is_deterministic_algorithms_warn_only_enabled()
A:torch.testing._internal.common_utils.self.debug_mode_restore->kwargs.get('torch', globals()['torch']).cuda.get_sync_debug_mode()
A:torch.testing._internal.common_utils.self.cublas_config_restore->os.environ.get(self.cublas_var_name)
A:torch.testing._internal.common_utils.cur_cublas_config->os.environ.get(self.cublas_var_name)
A:torch.testing._internal.common_utils.skipper->unittest.skip('Cannot import `caffe2.python.core`')
A:torch.testing._internal.common_utils.t->make_tensor(shape, device=device, dtype=dtype)
A:torch.testing._internal.common_utils.res->obj.clone().to(dtype=t, device='cuda')
A:torch.testing._internal.common_utils.rng_state->kwargs.get('torch', globals()['torch']).get_rng_state()
A:torch.testing._internal.common_utils.cuda_rng_state->kwargs.get('torch', globals()['torch']).cuda.get_rng_state()
A:torch.testing._internal.common_utils.saved_dtype->kwargs.get('torch', globals()['torch']).get_default_dtype()
A:torch.testing._internal.common_utils.beforeDevice->kwargs.get('torch', globals()['torch']).cuda.current_device()
A:torch.testing._internal.common_utils.deviceStream->kwargs.get('torch', globals()['torch']).cuda.Stream(device=d)
A:torch.testing._internal.common_utils.num_devices->kwargs.get('torch', globals()['torch']).cuda.device_count()
A:torch.testing._internal.common_utils.caching_allocator_mem_allocated->kwargs.get('torch', globals()['torch']).cuda.memory_allocated(i)
A:torch.testing._internal.common_utils.(bytes_free, bytes_total)->kwargs.get('torch', globals()['torch']).cuda.mem_get_info(i)
A:torch.testing._internal.common_utils.msg->"could not download test file '{}'".format(url)
A:torch.testing._internal.common_utils.test_name_chunks->self._get_subtest_name(values, explicit_name=maybe_name).split('_')
A:torch.testing._internal.common_utils.sanitized_test_method_name->remove_device_and_dtype_suffixes(test._testMethodName)
A:torch.testing._internal.common_utils.disable_test_parts->disabled_test.split()
A:torch.testing._internal.common_utils.self.rtol->max(self.rtol, rtol_override)
A:torch.testing._internal.common_utils.self.atol->max(self.atol, atol_override)
A:torch.testing._internal.common_utils.number->int(number)
A:torch.testing._internal.common_utils.(actual, expected)->super()._equalize_attributes(actual, expected)
A:torch.testing._internal.common_utils.test_case->unittest.TestCase()
A:torch.testing._internal.common_utils.old_val->kwargs.get('torch', globals()['torch']).is_warn_always_enabled()
A:torch.testing._internal.common_utils.test_method->getattr(self, method_name)
A:torch.testing._internal.common_utils.fullname->self.id().lower()
A:torch.testing._internal.common_utils.using_unittest->isinstance(result, unittest.TestResult)
A:torch.testing._internal.common_utils.case->_TestInfo(result, case)
A:torch.testing._internal.common_utils.err->sys.exc_info()
A:torch.testing._internal.common_utils.counts->kwargs.get('torch', globals()['torch']).zeros(n_rows + 1, dtype=dtype, device=torch.device('cpu'))
A:torch.testing._internal.common_utils.N->sawteeth(n, m)
A:torch.testing._internal.common_utils.N_right->sawteeth(n, m_right)
A:torch.testing._internal.common_utils.N_middle->sawteeth(n, m_middle)
A:torch.testing._internal.common_utils.(q, r)->divmod(nnz - n * n_cols - m * (n_rows - n), (n_cols - m) * (n_cols - m + 1) // 2)
A:torch.testing._internal.common_utils.k->kwargs.get(key, None)
A:torch.testing._internal.common_utils.perm->kwargs.get('torch', globals()['torch']).randperm(n_rows, device=counts.device)
A:torch.testing._internal.common_utils.crow_indices->self._make_crow_indices(n_rows, n_cols, nnz, device=device, dtype=index_dtype)
A:torch.testing._internal.common_utils.col_indices->kwargs.get('torch', globals()['torch']).zeros(nnz, dtype=index_dtype, device=device)
A:torch.testing._internal.common_utils.(col_indices[crow_indices[i]:crow_indices[i + 1]], _)->kwargs.get('torch', globals()['torch']).sort(torch.randperm(n_cols, dtype=index_dtype, device=device)[:count])
A:torch.testing._internal.common_utils.values->kwargs.get('torch', globals()['torch']).randn(nonzero_elements, dtype=dtype, device=device)
A:torch.testing._internal.common_utils.(values, crow_indices, col_indices)->random_sparse_csr(size[0], size[1], nnz)
A:torch.testing._internal.common_utils.v->kwargs.get('torch', globals()['torch']).full(shape, fv, dtype=dtype, layout=layout, device=device, requires_grad=rg)
A:torch.testing._internal.common_utils.i->random.randint(0, matrix_size - 1)
A:torch.testing._internal.common_utils.x->kwargs.get('torch', globals()['torch']).rand(shape, dtype=dtype, device=device)
A:torch.testing._internal.common_utils.(n_inp, n_args, n_kwargs)->sample_input.numpy()
A:torch.testing._internal.common_utils.actual->torch_fn(t_inp, *t_args, **t_kwargs)
A:torch.testing._internal.common_utils.expected->expected.replace('producer_version: "CURRENT_VERSION"', 'producer_version: "{}"'.format(torch.onnx.producer_version)).replace('producer_version: "CURRENT_VERSION"', 'producer_version: "{}"'.format(torch.onnx.producer_version))
A:torch.testing._internal.common_utils.t_cpu->t_cpu.float().float()
A:torch.testing._internal.common_utils.a->numpy.array(tensor_like, dtype=d[dtype])
A:torch.testing._internal.common_utils.d->copy.copy(torch_to_numpy_dtype_dict)
A:torch.testing._internal.common_utils.np_result->kwargs.get('torch', globals()['torch']).from_numpy(np_result.copy())
A:torch.testing._internal.common_utils.torch_result->torch_result.to(torch.float).to(torch.float)
A:torch.testing._internal.common_utils.y->kwargs.get('torch', globals()['torch']).as_tensor(y, dtype=x.dtype, device=x.device)
A:torch.testing._internal.common_utils.context->AssertRaisesContextIgnoreNotImplementedError(expected_exception, self, expected_regex)
A:torch.testing._internal.common_utils.pattern->re.compile(regex)
A:torch.testing._internal.common_utils.munged_id->remove_prefix(self.id(), module_id + '.')
A:torch.testing._internal.common_utils.test_file->os.path.realpath(sys.modules[module_id].__file__)
A:torch.testing._internal.common_utils.expected_file->os.path.join(os.path.dirname(test_file), 'expect', munged_id)
A:torch.testing._internal.common_utils.subname_output->' ({})'.format(subname)
A:torch.testing._internal.common_utils.s_tag->re.sub('(producer_version): "[0-9.]*"', '\\1: "CURRENT_VERSION"', s)
A:torch.testing._internal.common_utils.s->kwargs.get('torch', globals()['torch']).linspace(1 / (k + 1), 1, k, dtype=dtype, device=device)
A:torch.testing._internal.common_utils.popen->subprocess.Popen([sys.executable, '-c', code], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
A:torch.testing._internal.common_utils.(stdout, stderr)->TestCase.run_process_no_exception(code, env=env)
A:torch.testing._internal.common_utils.env->os.environ.copy()
A:torch.testing._internal.common_utils.filename->os.path.basename(urlsplit(url)[2])
A:torch.testing._internal.common_utils.data_dir->get_writable_path(os.path.join(os.path.dirname(__file__), 'data'))
A:torch.testing._internal.common_utils.path->os.path.join(data_dir, filename)
A:torch.testing._internal.common_utils.data->dict([((i, i), float(i + 1) / matrix_size) for i in range(matrix_size)])
A:torch.testing._internal.common_utils.(_, port)->sock.getsockname()
A:torch.testing._internal.common_utils.A->kwargs.get('torch', globals()['torch']).sparse_coo_tensor(indices_tensor, values, (rows, columns), device=device)
A:torch.testing._internal.common_utils.(u, s, vh)->kwargs.get('torch', globals()['torch']).linalg.svd(A, full_matrices=False)
A:torch.testing._internal.common_utils.m->kwargs.get('torch', globals()['torch']).rand(shape, dtype=dtype, device=device).size(-2)
A:torch.testing._internal.common_utils.n->kwargs.get('torch', globals()['torch']).rand(shape, dtype=dtype, device=device).size(-1)
A:torch.testing._internal.common_utils.(u, _, vh)->kwargs.get('torch', globals()['torch']).linalg.svd(A, full_matrices=False)
A:torch.testing._internal.common_utils.result->kwargs.get('torch', globals()['torch']).repeat_interleave(t.detach(), 2, dim=-1)
A:torch.testing._internal.common_utils.strides->list(result.stride())
A:torch.testing._internal.common_utils.dtype->kwargs.get('dtype', torch.double)
A:torch.testing._internal.common_utils.device->kwargs.get('device', 'cpu')
A:torch.testing._internal.common_utils.silent->kwargs.get('silent', False)
A:torch.testing._internal.common_utils.singular->kwargs.get('singular', False)
A:torch.testing._internal.common_utils.B->random_matrix(rows, rank, *batch_dims, **kwargs)
A:torch.testing._internal.common_utils.C->random_matrix(rank, columns, *batch_dims, **kwargs)
A:torch.testing._internal.common_utils.nonzero_elements->max(min(rows, columns), int(rows * columns * density))
A:torch.testing._internal.common_utils.indices_tensor->kwargs.get('torch', globals()['torch']).tensor([icoords, jcoords])
A:torch.testing._internal.common_utils.torch->kwargs.get('torch', globals()['torch'])
A:torch.testing._internal.common_utils.j->random.randint(0, matrix_size - 1)
A:torch.testing._internal.common_utils.theta->random.uniform(0, 2 * math.pi)
A:torch.testing._internal.common_utils.cs->math.cos(theta)
A:torch.testing._internal.common_utils.sn->math.sin(theta)
A:torch.testing._internal.common_utils.out->kwargs.get('torch', globals()['torch']).full(shape, fv, dtype=dtype, layout=layout, device=device, requires_grad=rg).new()
A:torch.testing._internal.common_utils.shape->kwargs.get('torch', globals()['torch']).Size([2, 3])
A:torch.testing._internal.common_utils.fill->tensor.new(shape).fill_(value)
A:torch.testing._internal.common_utils.module->'.'.join(str(dtype).split('.')[1:-1])
A:torch.testing._internal.common_utils.default_dtype->kwargs.get('torch', globals()['torch']).get_default_dtype()
A:torch.testing._internal.common_utils.int64_dtype->get_int64_dtype(dtype)
A:torch.testing._internal.common_utils.running_file->os.path.abspath(os.path.realpath(sys.argv[0]))
A:torch.testing._internal.common_utils.test_case_class_file->os.path.abspath(os.path.realpath(inspect.getfile(test_case.__class__)))
A:torch.testing._internal.common_utils.test_suite->unittest.TestSuite()
A:torch.testing._internal.common_utils.old_cwd->os.getcwd()
A:torch.testing._internal.common_utils.mock->MagicMock()
A:torch.testing._internal.common_utils.num_bytes->ctypes.sizeof(ctype)
A:torch.testing._internal.common_utils.num_threads->kwargs.get('torch', globals()['torch']).get_num_threads()
A:torch.testing._internal.common_utils.start->kwargs.get('torch', globals()['torch']).cuda.Event(enable_timing=True)
A:torch.testing._internal.common_utils.end->kwargs.get('torch', globals()['torch']).cuda.Event(enable_timing=True)
A:torch.testing._internal.common_utils.vals->sorted(vals)
A:torch.testing._internal.common_utils.T->TypeVar('T')
torch.testing._internal.common_utils.AssertRaisesContextIgnoreNotImplementedError(unittest.case._AssertRaisesContext)
torch.testing._internal.common_utils.AssertRaisesContextIgnoreNotImplementedError.__exit__(self,exc_type,exc_value,tb)
torch.testing._internal.common_utils.BytesIOContext(io.BytesIO)
torch.testing._internal.common_utils.BytesIOContext.__enter__(self)
torch.testing._internal.common_utils.BytesIOContext.__exit__(self,*args)
torch.testing._internal.common_utils.CudaMemoryLeakCheck(self,testcase,name=None)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.__enter__(self)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.__exit__(self,exec_type,exec_value,traceback)
torch.testing._internal.common_utils.CudaMemoryLeakCheck.__init__(self,testcase,name=None)
torch.testing._internal.common_utils.CudaNonDefaultStream
torch.testing._internal.common_utils.CudaNonDefaultStream.__enter__(self)
torch.testing._internal.common_utils.CudaNonDefaultStream.__exit__(self,exec_type,exec_value,traceback)
torch.testing._internal.common_utils.CudaSyncGuard(self,sync_debug_mode)
torch.testing._internal.common_utils.CudaSyncGuard.__enter__(self)
torch.testing._internal.common_utils.CudaSyncGuard.__exit__(self,exception_type,exception_value,traceback)
torch.testing._internal.common_utils.CudaSyncGuard.__init__(self,sync_debug_mode)
torch.testing._internal.common_utils.DeterministicGuard(self,deterministic,*,warn_only=False)
torch.testing._internal.common_utils.DeterministicGuard.__enter__(self)
torch.testing._internal.common_utils.DeterministicGuard.__exit__(self,exception_type,exception_value,traceback)
torch.testing._internal.common_utils.DeterministicGuard.__init__(self,deterministic,*,warn_only=False)
torch.testing._internal.common_utils.ObjectPair(UnittestPair)
torch.testing._internal.common_utils.ProfilingMode(Enum)
torch.testing._internal.common_utils.RelaxedBooleanPair(BooleanPair)
torch.testing._internal.common_utils.RelaxedBooleanPair._process_inputs(self,actual,expected,*,id)
torch.testing._internal.common_utils.RelaxedBooleanPair._to_bool(self,bool_like,*,id)
torch.testing._internal.common_utils.RelaxedNumberPair(self,actual,expected,*,rtol_override=0.0,atol_override=0.0,check_dtype=None,**other_parameters)
torch.testing._internal.common_utils.RelaxedNumberPair.__init__(self,actual,expected,*,rtol_override=0.0,atol_override=0.0,check_dtype=None,**other_parameters)
torch.testing._internal.common_utils.RelaxedNumberPair._process_inputs(self,actual,expected,*,id)
torch.testing._internal.common_utils.RelaxedNumberPair._to_number(self,number_like,*,id)
torch.testing._internal.common_utils.SetPair(UnittestPair)
torch.testing._internal.common_utils.StringPair(UnittestPair)
torch.testing._internal.common_utils.TensorOrArrayPair(self,actual,expected,*,rtol_override=0.0,atol_override=0.0,**other_parameters)
torch.testing._internal.common_utils.TensorOrArrayPair.__init__(self,actual,expected,*,rtol_override=0.0,atol_override=0.0,**other_parameters)
torch.testing._internal.common_utils.TensorOrArrayPair._equalize_attributes(self,actual,expected)
torch.testing._internal.common_utils.TensorOrArrayPair._process_inputs(self,actual,expected,*,id,allow_subclasses)
torch.testing._internal.common_utils.TestCase(self,method_name='runTest')
torch.testing._internal.common_utils.TestCase.__init__(self,method_name='runTest')
torch.testing._internal.common_utils.TestCase._make_crow_indices(n_rows,n_cols,nnz,*,device,dtype,random=True)
torch.testing._internal.common_utils.TestCase._run_with_retry(self,result=None,num_runs_left=0,report_only=True)
torch.testing._internal.common_utils.TestCase._should_stop_test_suite(self)
torch.testing._internal.common_utils.TestCase.assertEqual(self,x,y,msg:Optional[str]=None,*,atol:Optional[float]=None,rtol:Optional[float]=None,equal_nan=True,exact_dtype=True,exact_device=False,exact_layout=False,exact_stride=False,exact_is_coalesced=False)
torch.testing._internal.common_utils.TestCase.assertEqualIgnoreType(self,*args,**kwargs)->None
torch.testing._internal.common_utils.TestCase.assertEqualTypeString(self,x,y)->None
torch.testing._internal.common_utils.TestCase.assertExpected(self,s,subname=None)
torch.testing._internal.common_utils.TestCase.assertExpectedRaises(self,exc_type,callable,*args,**kwargs)
torch.testing._internal.common_utils.TestCase.assertExpectedStripMangled(self,s,subname=None)
torch.testing._internal.common_utils.TestCase.assertGreaterAlmostEqual(self,first,second,places=None,msg=None,delta=None)
torch.testing._internal.common_utils.TestCase.assertLeaksNoCudaTensors(self,name=None)
torch.testing._internal.common_utils.TestCase.assertNotEqual(self,x,y,msg:Optional[str]=None,*,atol:Optional[float]=None,rtol:Optional[float]=None,**kwargs)->None
torch.testing._internal.common_utils.TestCase.assertNotWarn(self,callable,msg='')
torch.testing._internal.common_utils.TestCase.assertObjectIn(self,obj:Any,iterable:Iterable[Any])->None
torch.testing._internal.common_utils.TestCase.assertRaises(self,expected_exception,*args,**kwargs)
torch.testing._internal.common_utils.TestCase.assertRaisesRegex(self,expected_exception,expected_regex,*args,**kwargs)
torch.testing._internal.common_utils.TestCase.assertWarnsOnceRegex(self,category,regex='')
torch.testing._internal.common_utils.TestCase.compare_with_numpy(self,torch_fn,np_fn,tensor_like,device=None,dtype=None,**kwargs)
torch.testing._internal.common_utils.TestCase.compare_with_reference(self,torch_fn,ref_fn,sample_input,**kwargs)
torch.testing._internal.common_utils.TestCase.enforceNonDefaultStream(self)
torch.testing._internal.common_utils.TestCase.genSparseCSRTensor(self,size,nnz,*,device,dtype,index_dtype)
torch.testing._internal.common_utils.TestCase.genSparseTensor(self,size,sparse_dim,nnz,is_uncoalesced,device,dtype)
torch.testing._internal.common_utils.TestCase.precision(self)->float
torch.testing._internal.common_utils.TestCase.precision(self,prec:float)->None
torch.testing._internal.common_utils.TestCase.rel_tol(self)->float
torch.testing._internal.common_utils.TestCase.rel_tol(self,prec:float)->None
torch.testing._internal.common_utils.TestCase.run(self,result=None)
torch.testing._internal.common_utils.TestCase.runWithPytorchAPIUsageStderr(code)
torch.testing._internal.common_utils.TestCase.run_process_no_exception(code,env=None)
torch.testing._internal.common_utils.TestCase.safeToDense(self,t)
torch.testing._internal.common_utils.TestCase.setUp(self)
torch.testing._internal.common_utils.TestCase.wrap_method_with_policy(self,method,policy)
torch.testing._internal.common_utils.TestCase.wrap_with_cuda_memory_check(self,method)
torch.testing._internal.common_utils.TestCase.wrap_with_cuda_policy(self,method_name,policy)
torch.testing._internal.common_utils.TestCase.wrap_with_policy(self,method_name,policy)
torch.testing._internal.common_utils.TypePair(UnittestPair)
torch.testing._internal.common_utils.UnittestPair(self,actual,expected,**other_parameters)
torch.testing._internal.common_utils.UnittestPair.__init__(self,actual,expected,**other_parameters)
torch.testing._internal.common_utils.UnittestPair.compare(self)
torch.testing._internal.common_utils._TestParametrizer(self,fn)
torch.testing._internal.common_utils._TestParametrizer.__call__(self,fn)
torch.testing._internal.common_utils._TestParametrizer._parametrize_test(self,test,generic_cls,device_cls)
torch.testing._internal.common_utils._assertGradAndGradgradChecks(test_case,apply_fn,inputs,**kwargs)
torch.testing._internal.common_utils._check_module_exists(name:str)->bool
torch.testing._internal.common_utils._get_test_report_path()
torch.testing._internal.common_utils._print_test_names()
torch.testing._internal.common_utils._test_function(fn,device)
torch.testing._internal.common_utils.bytes_to_scalar(byte_list:List[int],dtype:torch.dtype,device:torch.device)
torch.testing._internal.common_utils.check_if_enable(test:unittest.TestCase)
torch.testing._internal.common_utils.check_test_defined_in_running_script(test_case)
torch.testing._internal.common_utils.chunk_list(lst,nchunks)
torch.testing._internal.common_utils.clone_input_helper(input)
torch.testing._internal.common_utils.coalescedonoff(f)
torch.testing._internal.common_utils.compose_parametrize_fns(old_parametrize_fn,new_parametrize_fn)
torch.testing._internal.common_utils.cppProfilingFlagsToProfilingMode()
torch.testing._internal.common_utils.disable_gc()
torch.testing._internal.common_utils.discover_test_cases_recursively(suite_or_case)
torch.testing._internal.common_utils.do_test_dtypes(self,dtypes,layout,device)
torch.testing._internal.common_utils.do_test_empty_full(self,dtypes,layout,device)
torch.testing._internal.common_utils.download_file(url,binary=True)
torch.testing._internal.common_utils.dtype_name(dtype)
torch.testing._internal.common_utils.enable_profiling_mode()
torch.testing._internal.common_utils.enable_profiling_mode_for_profiling_tests()
torch.testing._internal.common_utils.find_free_port()
torch.testing._internal.common_utils.find_library_location(lib_name:str)->Path
torch.testing._internal.common_utils.first_sample(self:unittest.TestCase,samples:Iterable[T])->T
torch.testing._internal.common_utils.freeze_rng_state()
torch.testing._internal.common_utils.get_cycles_per_ms()->float
torch.testing._internal.common_utils.get_function_arglist(func)
torch.testing._internal.common_utils.get_tensors_from(args,kwargs)
torch.testing._internal.common_utils.get_test_names(test_cases)
torch.testing._internal.common_utils.gradcheck(fn,inputs,**kwargs)
torch.testing._internal.common_utils.gradgradcheck(fn,inputs,grad_outputs=None,**kwargs)
torch.testing._internal.common_utils.has_breakpad()
torch.testing._internal.common_utils.has_corresponding_torch_dtype(np_dtype)
torch.testing._internal.common_utils.instantiate_parametrized_tests(generic_cls)
torch.testing._internal.common_utils.is_avx512_vnni_supported()
torch.testing._internal.common_utils.is_iterable(obj)
torch.testing._internal.common_utils.is_iterable_of_tensors(iterable,include_empty=False)
torch.testing._internal.common_utils.iter_indices(tensor)
torch.testing._internal.common_utils.lint_test_case_extension(suite)
torch.testing._internal.common_utils.load_tests(loader,tests,pattern)
torch.testing._internal.common_utils.make_fullrank_matrices_with_distinct_singular_values(*shape,device,dtype,requires_grad=False)
torch.testing._internal.common_utils.make_symmetric_matrices(*shape,device,dtype)
torch.testing._internal.common_utils.make_symmetric_pd_matrices(*shape,device,dtype)
torch.testing._internal.common_utils.mock_wrapper(method)
torch.testing._internal.common_utils.noarchTest(fn)
torch.testing._internal.common_utils.noncontiguous_like(t)
torch.testing._internal.common_utils.num_profiled_runs(num_runs)
torch.testing._internal.common_utils.numpy_to_torch_dtype(np_dtype)
torch.testing._internal.common_utils.parametrize(self,arg_str,arg_values,name_fn=None)
torch.testing._internal.common_utils.parametrize.__init__(self,arg_str,arg_values,name_fn=None)
torch.testing._internal.common_utils.parametrize._default_subtest_name(self,values)
torch.testing._internal.common_utils.parametrize._formatted_str_repr(self,name,value)
torch.testing._internal.common_utils.parametrize._get_subtest_name(self,values,explicit_name=None)
torch.testing._internal.common_utils.parametrize._parametrize_test(self,test,generic_cls,device_cls)
torch.testing._internal.common_utils.prof_callable(callable,*args,**kwargs)
torch.testing._internal.common_utils.prof_func_call(*args,**kwargs)
torch.testing._internal.common_utils.prof_meth_call(*args,**kwargs)
torch.testing._internal.common_utils.random_hermitian_matrix(l,*batches,**kwargs)
torch.testing._internal.common_utils.random_hermitian_pd_matrix(matrix_size,*batch_dims,dtype,device)
torch.testing._internal.common_utils.random_hermitian_psd_matrix(matrix_size,*batch_dims,dtype=torch.double,device='cpu')
torch.testing._internal.common_utils.random_lowrank_matrix(rank,rows,columns,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_matrix(rows,columns,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_sparse_matrix(rows,columns,density=0.01,**kwargs)
torch.testing._internal.common_utils.random_sparse_pd_matrix(matrix_size,density=0.01,**kwargs)
torch.testing._internal.common_utils.random_square_matrix_of_rank(l,rank,dtype=torch.double,device='cpu')
torch.testing._internal.common_utils.random_symmetric_matrix(l,*batches,**kwargs)
torch.testing._internal.common_utils.random_symmetric_pd_matrix(matrix_size,*batch_dims,**kwargs)
torch.testing._internal.common_utils.random_symmetric_psd_matrix(l,*batches,**kwargs)
torch.testing._internal.common_utils.random_well_conditioned_matrix(*shape,dtype,device,mean=1.0,sigma=0.001)
torch.testing._internal.common_utils.remove_device_and_dtype_suffixes(test_name:str)->str
torch.testing._internal.common_utils.retry(ExceptionToCheck,tries=3,delay=3,skip_after_retries=False)
torch.testing._internal.common_utils.retry_on_connect_failures(func=None,connect_errors=ADDRESS_IN_USE)
torch.testing._internal.common_utils.run_tests(argv=UNITTEST_ARGS)
torch.testing._internal.common_utils.run_unittest_help(argv)
torch.testing._internal.common_utils.sandcastle_skip(reason)
torch.testing._internal.common_utils.sandcastle_skip_if(condition,reason)
torch.testing._internal.common_utils.sanitize_test_filename(filename)
torch.testing._internal.common_utils.set_cwd(path:str)->Iterator[None]
torch.testing._internal.common_utils.set_default_dtype(dtype)
torch.testing._internal.common_utils.set_rng_seed(seed)
torch.testing._internal.common_utils.set_running_script_path()
torch.testing._internal.common_utils.set_single_threaded_if_parallel_tbb(fn)
torch.testing._internal.common_utils.set_warn_always_context(new_val:bool)
torch.testing._internal.common_utils.shell(command,cwd=None,env=None)
torch.testing._internal.common_utils.skipCUDAMemoryLeakCheckIf(condition)
torch.testing._internal.common_utils.skipCUDANonDefaultStreamIf(condition)
torch.testing._internal.common_utils.skipIfCompiledWithoutNumpy(fn)
torch.testing._internal.common_utils.skipIfNoLapack(fn)
torch.testing._internal.common_utils.skipIfNoSciPy(fn)
torch.testing._internal.common_utils.skipIfNotMiopenSuggestNHWC(fn)
torch.testing._internal.common_utils.skipIfNotRegistered(op_name,message)
torch.testing._internal.common_utils.skipIfOnGHA(fn)
torch.testing._internal.common_utils.skipIfRocm(fn)
torch.testing._internal.common_utils.skipIfRocmVersionLessThan(version=None)
torch.testing._internal.common_utils.skipIfTBB(message='ThistestmakesTBBsad')
torch.testing._internal.common_utils.skip_exception_type(exc_type)
torch.testing._internal.common_utils.slowAwareTest(fn)
torch.testing._internal.common_utils.slowTest(fn)
torch.testing._internal.common_utils.subtest(self,arg_values,name=None,decorators=None)
torch.testing._internal.common_utils.subtest.__init__(self,arg_values,name=None,decorators=None)
torch.testing._internal.common_utils.suppress_warnings(fn)
torch.testing._internal.common_utils.to_gpu(obj,type_map=None)
torch.testing._internal.common_utils.wait_for_process(p)
torch.testing._internal.common_utils.wrapDeterministicFlagAPITest(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/jit_metaprogramming_utils.py----------------------------------------
A:torch.testing._internal.jit_metaprogramming_utils.kwargs_str->', '.join([k + '=' + value_to_literal(v) for (k, v) in kwargs.items()])
A:torch.testing._internal.jit_metaprogramming_utils.argument_str->', '.join(args)
A:torch.testing._internal.jit_metaprogramming_utils.call->get_call(method_name, func_type, actuals, kwargs)
A:torch.testing._internal.jit_metaprogramming_utils.name->get_nn_module_name_from_kwargs(**kwargs)
A:torch.testing._internal.jit_metaprogramming_utils.(formals, tensors, actuals)->get_script_args(args)
A:torch.testing._internal.jit_metaprogramming_utils.script->script_template.format(', '.join(formals), call)
A:torch.testing._internal.jit_metaprogramming_utils.CU->torch.jit.CompilationUnit(script)
A:torch.testing._internal.jit_metaprogramming_utils.(fn, tensors)->gen_script_fn_and_args(method_name, func_type, *args, **kwargs)
A:torch.testing._internal.jit_metaprogramming_utils.output->traced(*inputs_tensors)
A:torch.testing._internal.jit_metaprogramming_utils.script_fn.last_graph->fn.graph_for(*tensors)
A:torch.testing._internal.jit_metaprogramming_utils.tensors->iter(tensors_)
A:torch.testing._internal.jit_metaprogramming_utils.(fn_tensors, inputs_tensors)->partial_apply_nontensors(fn, inputs, **kwargs)
A:torch.testing._internal.jit_metaprogramming_utils.traced->torch.jit.trace(fn_tensors, inputs_tensors, check_trace=False)
A:torch.testing._internal.jit_metaprogramming_utils.traced_fn.last_graph->torch.jit.trace(fn_tensors, inputs_tensors, check_trace=False).graph_for(*inputs_tensors)
A:torch.testing._internal.jit_metaprogramming_utils.(args_variable, kwargs_variable)->create_input(input, dtype=input_dtype)
A:torch.testing._internal.jit_metaprogramming_utils.self_tensor->deepcopy(self_variable.data)
A:torch.testing._internal.jit_metaprogramming_utils.args_tensor->deepcopy(unpack_variables(args_variable))
A:torch.testing._internal.jit_metaprogramming_utils.(script_fn, inputs)->gen_script_fn_and_args(name, 'nn_functional', *f_args_variable)
A:torch.testing._internal.jit_metaprogramming_utils.method_args->', '.join(['self'] + actuals)
A:torch.testing._internal.jit_metaprogramming_utils.call_args_str->', '.join(actuals)
A:torch.testing._internal.jit_metaprogramming_utils.self.submodule->nn_module(*constructor_args)
A:torch.testing._internal.jit_metaprogramming_utils.module->make_module(script)
A:torch.testing._internal.jit_metaprogramming_utils.test_name->get_nn_mod_test_name(**kwargs)
A:torch.testing._internal.jit_metaprogramming_utils.index->get_nn_module_name_from_kwargs(**kwargs).find('_')
A:torch.testing._internal.jit_metaprogramming_utils.nn_module->getattr(torch.nn, name)
A:torch.testing._internal.jit_metaprogramming_utils.constructor_args->kwargs.get('constructor_args', ())
A:torch.testing._internal.jit_metaprogramming_utils.input->kwargs['input_fn']()
A:torch.testing._internal.jit_metaprogramming_utils.f_args_variable->deepcopy(unpack_variables(args_variable))
A:torch.testing._internal.jit_metaprogramming_utils.out_var->deepcopy(f_args_variable)
torch.testing._internal.jit_metaprogramming_utils.check_alias_annotation(method_name,args,kwargs,*,aten_name,func_type='method')
torch.testing._internal.jit_metaprogramming_utils.create_script_fn(self,method_name,func_type)
torch.testing._internal.jit_metaprogramming_utils.create_script_module(self,nn_module,constructor_args,*args,**kwargs)
torch.testing._internal.jit_metaprogramming_utils.create_traced_fn(self,fn)
torch.testing._internal.jit_metaprogramming_utils.gen_script_fn_and_args(method_name,func_type,*args,**kwargs)
torch.testing._internal.jit_metaprogramming_utils.get_all_nn_module_tests()
torch.testing._internal.jit_metaprogramming_utils.get_call(method_name,func_type,args,kwargs)
torch.testing._internal.jit_metaprogramming_utils.get_constant(x)
torch.testing._internal.jit_metaprogramming_utils.get_nn_functional_compiled_fn_and_inputs(name,self_size,args,variant_name='',*extra_args)
torch.testing._internal.jit_metaprogramming_utils.get_nn_mod_test_name(**kwargs)
torch.testing._internal.jit_metaprogramming_utils.get_nn_module_class_from_kwargs(**kwargs)
torch.testing._internal.jit_metaprogramming_utils.get_nn_module_name_from_kwargs(**kwargs)
torch.testing._internal.jit_metaprogramming_utils.get_script_args(args)
torch.testing._internal.jit_metaprogramming_utils.partial_apply_nontensors(fn,args,**kwargs)
torch.testing._internal.jit_metaprogramming_utils.try_get_nn_module_compiled_mod_and_inputs(*args,**kwargs)
torch.testing._internal.jit_metaprogramming_utils.value_to_literal(value)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/quantization_torch_package_models.py----------------------------------------
A:torch.testing._internal.quantization_torch_package_models.self.w1->torch.nn.Parameter(torch.empty(N, N))
A:torch.testing._internal.quantization_torch_package_models.self.b1->torch.nn.Parameter(torch.zeros(N))
A:torch.testing._internal.quantization_torch_package_models.x->torch.nn.functional.relu(x)
A:torch.testing._internal.quantization_torch_package_models.self.child->LinearReluFunctionalChild(N)
torch.testing._internal.quantization_torch_package_models.LinearReluFunctional(self,N)
torch.testing._internal.quantization_torch_package_models.LinearReluFunctional.__init__(self,N)
torch.testing._internal.quantization_torch_package_models.LinearReluFunctional.forward(self,x)
torch.testing._internal.quantization_torch_package_models.LinearReluFunctionalChild(self,N)
torch.testing._internal.quantization_torch_package_models.LinearReluFunctionalChild.__init__(self,N)
torch.testing._internal.quantization_torch_package_models.LinearReluFunctionalChild.forward(self,x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_nn.py----------------------------------------
A:torch.testing._internal.common_nn.result->getattr(m, 'weight', None)
A:torch.testing._internal.common_nn.total->reduce(mul, size, 1)
A:torch.testing._internal.common_nn.t->torch.rand(5).mul(8).floor().long()
A:torch.testing._internal.common_nn.weights->torch.rand(10)
A:torch.testing._internal.common_nn.sigmoid->torch.nn.Sigmoid()
A:torch.testing._internal.common_nn.i->torch.rand(10, 10).log()
A:torch.testing._internal.common_nn.target->self._get_target()
A:torch.testing._internal.common_nn.weight->self._get_input().new(len(input)).fill_(1)
A:torch.testing._internal.common_nn.random_samples->torch.empty((2, 4, 3), dtype=torch.double).uniform_()
A:torch.testing._internal.common_nn.out->test_case._forward_criterion(module, input, target, extra_args=self.extra_args)
A:torch.testing._internal.common_nn.single_batch_input->unsqueeze_inp(input)
A:torch.testing._internal.common_nn.padding->tuple(range(1, d + 1))
A:torch.testing._internal.common_nn.activation_test_info->dict(module_name=non_linear_activation, input_size=(4,), reference_fn=single_batch_reference_fn, desc='no_batch_dim', test_cpp_api_parity=False)
A:torch.testing._internal.common_nn.extra_info->classification_criterion_no_batch_extra_info.get(name, {})
A:torch.testing._internal.common_nn.safe_target_log->(safe_target + (target <= 0).type_as(target)).log()
A:torch.testing._internal.common_nn.N->self._get_input().size(0)
A:torch.testing._internal.common_nn.C->self._get_input().size(1)
A:torch.testing._internal.common_nn.output->module(input)
A:torch.testing._internal.common_nn.input_index->list(tup)
A:torch.testing._internal.common_nn.input->self._get_input()
A:torch.testing._internal.common_nn.log_softmax_input->torch.log_softmax(input, 1)
A:torch.testing._internal.common_nn.nllloss->torch.nn.functional.nll_loss(log_softmax_input, target, weight, ignore_index=ignore_index, reduction=reduction)
A:torch.testing._internal.common_nn.ret->torch.sum(smooth_loss)
A:torch.testing._internal.common_nn.(losses, weights)->zip(*losses_and_weights)
A:torch.testing._internal.common_nn.losses_tensor->self._get_input().new_tensor(losses)
A:torch.testing._internal.common_nn.abs_diff->(input - target).abs()
A:torch.testing._internal.common_nn.ge_beta_mask->(abs_diff >= beta).type_as(abs_diff)
A:torch.testing._internal.common_nn.lt_beta_mask->(abs_diff < beta).type_as(abs_diff)
A:torch.testing._internal.common_nn.input_dim->self._get_input().dim()
A:torch.testing._internal.common_nn.n->self._get_input().size(0)
A:torch.testing._internal.common_nn.dim->self._get_input().size(1)
A:torch.testing._internal.common_nn.output[i]->_multilabelmarginloss_reference(input[i], target[i])
A:torch.testing._internal.common_nn.margin_clamp->(margin - input).clamp(min=0).type_as(input)
A:torch.testing._internal.common_nn.target_dim->self._get_target().dim()
A:torch.testing._internal.common_nn.output[x]->_multimarginloss_reference(input[x], target[x], p, margin, weight)
A:torch.testing._internal.common_nn.cos->a.new(a.size(0))
A:torch.testing._internal.common_nn.d_p->torch.pairwise_distance(anchor, positive, p, eps)
A:torch.testing._internal.common_nn.d_n->torch.min(d_n, d_s)
A:torch.testing._internal.common_nn.d_s->torch.pairwise_distance(positive, negative, p, eps)
A:torch.testing._internal.common_nn.input_lengths->torch.as_tensor(input_lengths, dtype=torch.long)
A:torch.testing._internal.common_nn.target_lengths->torch.as_tensor(target_lengths, dtype=torch.long)
A:torch.testing._internal.common_nn.log_probs->log_probs.double().double()
A:torch.testing._internal.common_nn.targets->targets.long().long()
A:torch.testing._internal.common_nn.cum_target_lengths->torch.as_tensor(target_lengths, dtype=torch.long).cumsum(0)
A:torch.testing._internal.common_nn.input_length->input_lengths[i].item()
A:torch.testing._internal.common_nn.target_length->target_lengths[i].item()
A:torch.testing._internal.common_nn.cum_target_length->cum_target_lengths[i].item()
A:torch.testing._internal.common_nn.targets_prime->targets.long().long().new_full((2 * target_length + 1,), blank)
A:torch.testing._internal.common_nn.probs->log_probs[:input_length, i].exp()
A:torch.testing._internal.common_nn.alpha->log_probs.double().double().new_zeros((target_length * 2 + 1,))
A:torch.testing._internal.common_nn.alpha_next->log_probs.double().double().new_zeros((target_length * 2 + 1,)).clone()
A:torch.testing._internal.common_nn.single_batch_input_args->flatten([unsqueeze_inp(input) for input in args[:-1]])
A:torch.testing._internal.common_nn.reduction->get_reduction(criterion)
A:torch.testing._internal.common_nn.regression_test_info->dict(fullname='{}_no_batch_dim_{}'.format(name, reduction), constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction), input_size=(3,), target_size=(3,), reference_fn=single_batch_reference_criterion_fn, test_cpp_api_parity=False)
A:torch.testing._internal.common_nn.classification_test_info->dict(fullname='{}_no_batch_dim_{}'.format(name, reduction), constructor=lambda *args, name=name: getattr(nn, name)(reduction=reduction), input_fn=lambda f=input_fn: f(), target_fn=lambda f=target_fn: f(), reference_fn=single_batch_reference_criterion_fn, test_cpp_api_parity=True, has_parity=classification_cpp_parity.get(name, True))
A:torch.testing._internal.common_nn.output_size->module(input).nelement()
A:torch.testing._internal.common_nn.jacobian_inp->self._jacobian(input, output_size)
A:torch.testing._internal.common_nn.flat_jacobian_input->list(_iter_tensors(jacobian_inp))
A:torch.testing._internal.common_nn.num_param->sum((p.numel() for p in self._get_parameters(module)[0]))
A:torch.testing._internal.common_nn.jacobian_param->torch.zeros(num_param, output_size)
A:torch.testing._internal.common_nn.(param, d_param)->self._get_parameters(module)
A:torch.testing._internal.common_nn.d_out->torch.zeros_like(output)
A:torch.testing._internal.common_nn.flat_d_out->torch.zeros_like(output).view(-1)
A:torch.testing._internal.common_nn.d_input->deepcopy(test_case._backward(module, input, output, grad_output))
A:torch.testing._internal.common_nn.jacobian_x[:, i]->d_x.contiguous().view(-1)
A:torch.testing._internal.common_nn.jacobian_param[:, i]->torch.cat(self._flatten_tensors(d_param), 0)
A:torch.testing._internal.common_nn.(param, _)->self._get_parameters(module)
A:torch.testing._internal.common_nn.jacobian->_get_numerical_jacobian(fw, input, target=p, eps=1e-06)
A:torch.testing._internal.common_nn.jacobian_parameters->bool(self._get_parameters(module)[0])
A:torch.testing._internal.common_nn.analytical->self._analytical_jacobian(module, input, jacobian_input, jacobian_parameters)
A:torch.testing._internal.common_nn.numerical->self._numerical_jacobian(module, input, jacobian_input, jacobian_parameters)
A:torch.testing._internal.common_nn.analytical_t->list(_iter_tensors(analytical))
A:torch.testing._internal.common_nn.numerical_t->list(_iter_tensors(numerical))
A:torch.testing._internal.common_nn.kwargs[name]->tuple()
A:torch.testing._internal.common_nn.self._arg_cache[name]->map_tensor_sizes(self._extra_kwargs[size_name])
A:torch.testing._internal.common_nn.self.jacobian_input->kwargs.get('jacobian_input', True)
A:torch.testing._internal.common_nn.self.should_test_cuda->kwargs.get('test_cuda', True)
A:torch.testing._internal.common_nn.self.should_test_pickle->kwargs.get('pickle', True)
A:torch.testing._internal.common_nn.self.check_gradgrad->kwargs.get('check_gradgrad', True)
A:torch.testing._internal.common_nn.self.FIXME_no_cuda_gradgrad_comparison->kwargs.get('FIXME_no_cuda_gradgrad_comparison', False)
A:torch.testing._internal.common_nn.self.precision->kwargs.get('precision', 0.0002)
A:torch.testing._internal.common_nn.self.check_forward_only->kwargs.get('check_forward_only', False)
A:torch.testing._internal.common_nn.module->self.constructor(*self.constructor_args)
A:torch.testing._internal.common_nn.ref_input->deepcopy(input)
A:torch.testing._internal.common_nn.ref_module->deepcopy(module)
A:torch.testing._internal.common_nn.expected_out->self.reference_fn(*ref_args)
A:torch.testing._internal.common_nn.module_copy->torch.load(f)
A:torch.testing._internal.common_nn.ndim->tensor.dim()
A:torch.testing._internal.common_nn.noncontig->torch.stack([torch.empty_like(tensor), tensor], dim).select(dim, 1).detach()
A:torch.testing._internal.common_nn.grad_output->module(input).new(output.shape).normal_()
A:torch.testing._internal.common_nn.d_param->deepcopy(test_case._get_parameters(module)[1])
A:torch.testing._internal.common_nn.nc_input->self.noncontiguize(input)
A:torch.testing._internal.common_nn.nc_grad_output->self.noncontiguize(grad_output)
A:torch.testing._internal.common_nn.go->deepcopy(grad_output if contig_g else nc_grad_output)
A:torch.testing._internal.common_nn.grad->module(input).data.clone().normal_()
A:torch.testing._internal.common_nn.cpu_input->self._get_input()
A:torch.testing._internal.common_nn.gpu_input_tuple->to_gpu(cpu_input_tuple, type_map=type_map)
A:torch.testing._internal.common_nn.cpu_module->self.constructor(*self.constructor_args)
A:torch.testing._internal.common_nn.gpu_module->self.constructor(*self.constructor_args)
A:torch.testing._internal.common_nn.cpu_param->test_case._get_parameters(cpu_module)
A:torch.testing._internal.common_nn.gpu_param->test_case._get_parameters(gpu_module)
A:torch.testing._internal.common_nn.cpu_output->test_case._forward_criterion(cpu_module, cpu_input, cpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.gpu_output->test_case._forward_criterion(gpu_module, gpu_input, gpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.cpu_gradOutput->torch.randn_like(cpu_output, requires_grad=True)
A:torch.testing._internal.common_nn.gpu_gradOutput->torch.randn_like(cpu_output, requires_grad=True).type_as(gpu_output).detach()
A:torch.testing._internal.common_nn.cpu_gradInput->test_case._backward_criterion(cpu_module, cpu_input, cpu_output, cpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.gpu_gradInput->test_case._backward_criterion(gpu_module, gpu_input, gpu_output, gpu_target, extra_args=extra_args)
A:torch.testing._internal.common_nn.cpu_gradInputs->torch.autograd.grad(cpu_output, cpu_input_tuple + tuple(cpu_module.parameters()), cpu_gradOutput, create_graph=True)
A:torch.testing._internal.common_nn.gpu_gradInputs->torch.autograd.grad(gpu_output, gpu_input_tuple + tuple(gpu_module.parameters()), gpu_gradOutput, create_graph=True)
A:torch.testing._internal.common_nn.cpu_gg->torch.autograd.grad(cpu_output.sum() + sum((x.sum() for x in cpu_gradInputs)), cpu_input_tuple + (cpu_gradOutput,) + tuple(cpu_module.parameters()), retain_graph=True)
A:torch.testing._internal.common_nn.gpu_gg->torch.autograd.grad(gpu_output.sum() + sum((x.sum() for x in gpu_gradInputs)), gpu_input_tuple + (gpu_gradOutput,) + tuple(gpu_module.parameters()), retain_graph=True)
A:torch.testing._internal.common_nn.self.cudnn->kwargs.get('cudnn', False)
A:torch.testing._internal.common_nn.self.check_inplace->kwargs.get('check_inplace', False)
A:torch.testing._internal.common_nn.self.skip_double->kwargs.get('skip_double', False)
A:torch.testing._internal.common_nn.self.skip_half->kwargs.get('skip_half', False)
A:torch.testing._internal.common_nn.self.with_tf32->kwargs.get('with_tf32', True)
A:torch.testing._internal.common_nn.self.tf32_precision->kwargs.get('tf32_precision', 0.001)
A:torch.testing._internal.common_nn.self.test_cpu->kwargs.get('test_cpu', True)
A:torch.testing._internal.common_nn.self.has_sparse_gradients->kwargs.get('has_sparse_gradients', False)
A:torch.testing._internal.common_nn.self.check_batched_grad->kwargs.get('check_batched_grad', True)
A:torch.testing._internal.common_nn.self.gradcheck_fast_mode->kwargs.get('gradcheck_fast_mode', None)
A:torch.testing._internal.common_nn.self.supports_forward_ad->kwargs.get('supports_forward_ad', False)
A:torch.testing._internal.common_nn.self.supports_fwgrad_bwgrad->kwargs.get('supports_fwgrad_bwgrad', False)
A:torch.testing._internal.common_nn.params->tuple((x for x in module.parameters()))
A:torch.testing._internal.common_nn.num_inputs->len(input_tuple)
A:torch.testing._internal.common_nn.test_input_jacobian->torch.is_floating_point(input_tuple[0])
A:torch.testing._internal.common_nn.num_threads->torch.get_num_threads()
A:torch.testing._internal.common_nn.module_ip->self.constructor(*self.constructor_args, inplace=True)
A:torch.testing._internal.common_nn.input_ip->deepcopy(input)
A:torch.testing._internal.common_nn.input_ip_clone->deepcopy(input).clone()
A:torch.testing._internal.common_nn.output_ip->module_ip(input_ip_clone)
A:torch.testing._internal.common_nn.input_tuple->tuple((to_half(t).cuda() for t in input_tuple))
A:torch.testing._internal.common_nn._required_arg_names->TestBase._required_arg_names.union({'target'})
A:torch.testing._internal.common_nn.self.check_half->kwargs.get('check_half', True)
A:torch.testing._internal.common_nn.self.check_bfloat16->kwargs.get('check_bfloat16', False)
A:torch.testing._internal.common_nn.self.check_complex->kwargs.get('check_complex', False)
A:torch.testing._internal.common_nn.cpu_target->self._get_target()
A:torch.testing._internal.common_nn.gpu_input->to_gpu(cpu_input)
A:torch.testing._internal.common_nn.gpu_target->to_gpu(cpu_target)
torch.testing._internal.common_nn.CriterionTest(self,*args,**kwargs)
torch.testing._internal.common_nn.CriterionTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.CriterionTest._get_target(self)
torch.testing._internal.common_nn.CriterionTest.constructor_args(self)
torch.testing._internal.common_nn.CriterionTest.extra_args(self)
torch.testing._internal.common_nn.CriterionTest.test_cuda(self,test_case,dtype,extra_args=None)
torch.testing._internal.common_nn.InputVariableMixin(object)
torch.testing._internal.common_nn.InputVariableMixin._get_input(self)
torch.testing._internal.common_nn.ModuleTest(self,*args,**kwargs)
torch.testing._internal.common_nn.ModuleTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.ModuleTest._do_test(self,test_case:Any,module:nn.Module,input:Any)->Any
torch.testing._internal.common_nn.ModuleTest.noncontiguize(self,obj)
torch.testing._internal.common_nn.ModuleTest.test_cuda(self,test_case)
torch.testing._internal.common_nn.ModuleTest.test_noncontig(self,test_case,module,input)
torch.testing._internal.common_nn.NNTestCase(TestCase)
torch.testing._internal.common_nn.NNTestCase._analytical_jacobian(self,module,input:_TensorOrTensors,jacobian_input=True,jacobian_parameters=True)
torch.testing._internal.common_nn.NNTestCase._backward(self,module:nn.Module,input:_TensorOrTensors,output:torch.Tensor,grad_output:Union[torch.Tensor,Sequence[torch.Tensor]],create_graph:bool=False)
torch.testing._internal.common_nn.NNTestCase._flatten_tensors(self,x)
torch.testing._internal.common_nn.NNTestCase._forward(self,*args,**kwargs)
torch.testing._internal.common_nn.NNTestCase._get_parameters(self,module:nn.Module)->Tuple[List[nn.Parameter], List[nn.Parameter]]
torch.testing._internal.common_nn.NNTestCase._jacobian(self,input,num_out)
torch.testing._internal.common_nn.NNTestCase._numerical_jacobian(self,module,input:_TensorOrTensors,jacobian_input=True,jacobian_parameters=True)
torch.testing._internal.common_nn.NNTestCase._zero_grad_input(self,input)
torch.testing._internal.common_nn.NNTestCase._zero_grad_parameters(self,module:nn.Module)->None
torch.testing._internal.common_nn.NNTestCase.check_jacobian(self,module,input:_TensorOrTensors,jacobian_input=True)
torch.testing._internal.common_nn.NewModuleTest(self,*args,**kwargs)
torch.testing._internal.common_nn.NewModuleTest.__init__(self,*args,**kwargs)
torch.testing._internal.common_nn.NewModuleTest._check_gradients(self,test_case,module,input_tuple)
torch.testing._internal.common_nn.NewModuleTest._do_test(self,test_case,module,input)
torch.testing._internal.common_nn.NewModuleTest._get_target(self)
torch.testing._internal.common_nn.NewModuleTest.constructor_args(self)
torch.testing._internal.common_nn.TestBase(self,constructor,desc='',reference_fn=None,fullname=None,**kwargs)
torch.testing._internal.common_nn.TestBase.__init__(self,constructor,desc='',reference_fn=None,fullname=None,**kwargs)
torch.testing._internal.common_nn.TestBase._get_arg(self,name,unpack)
torch.testing._internal.common_nn.TestBase._get_input(self,unpack=True)
torch.testing._internal.common_nn.TestBase._unpack(self,value)
torch.testing._internal.common_nn.TestBase.constructor_args(self)
torch.testing._internal.common_nn.TestBase.extra_args(self)
torch.testing._internal.common_nn.TestBase.get_name(self)
torch.testing._internal.common_nn._multilabelmarginloss_reference(input,target)
torch.testing._internal.common_nn._multimarginloss_reference(input,target_idx,p,margin,weight)
torch.testing._internal.common_nn._rand_tensor_non_equal(*size)
torch.testing._internal.common_nn.bce_with_logistic_legacy_enum_test()
torch.testing._internal.common_nn.bce_with_logistic_no_reduce_scalar_test()
torch.testing._internal.common_nn.bce_with_logistic_no_reduce_test()
torch.testing._internal.common_nn.bceloss_no_reduce_scalar_test()
torch.testing._internal.common_nn.bceloss_no_reduce_test()
torch.testing._internal.common_nn.bceloss_weights_no_reduce_scalar_test()
torch.testing._internal.common_nn.bceloss_weights_no_reduce_test()
torch.testing._internal.common_nn.cosineembeddingloss_reference(input1,input2,target,margin=0,reduction='mean')
torch.testing._internal.common_nn.cross_entropy_loss_indices_target_reference(input,target,weight=None,ignore_index=-100,reduction='mean',label_smoothing=0.0)
torch.testing._internal.common_nn.cross_entropy_loss_prob_target_reference(input,target,weight=None,reduction='mean',label_smoothing=0.0)
torch.testing._internal.common_nn.cross_entropy_loss_reference(input,target,weight=None,ignore_index=-100,reduction='mean',label_smoothing=0.0)
torch.testing._internal.common_nn.ctcloss_reference(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean')
torch.testing._internal.common_nn.fractional_max_pool2d_no_batch_dim_test(test_case,use_random_samples)
torch.testing._internal.common_nn.fractional_max_pool2d_test(test_case,return_indices=False)
torch.testing._internal.common_nn.fractional_max_pool3d_no_batch_dim_test(test_case,use_random_samples)
torch.testing._internal.common_nn.fractional_max_pool3d_test(test_case,return_indices=False)
torch.testing._internal.common_nn.get_reduction(m)
torch.testing._internal.common_nn.get_weight(m)
torch.testing._internal.common_nn.hingeembeddingloss_margin_no_reduce_test()
torch.testing._internal.common_nn.hingeembeddingloss_no_reduce_test()
torch.testing._internal.common_nn.hingeembeddingloss_reference(input,target,margin=1.0,reduction='mean')
torch.testing._internal.common_nn.huberloss_delta_test()
torch.testing._internal.common_nn.huberloss_reference(input,target,reduction='mean',delta=1.0)
torch.testing._internal.common_nn.kldivloss_log_target_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.kldivloss_no_reduce_log_target_test()
torch.testing._internal.common_nn.kldivloss_no_reduce_scalar_log_target_test()
torch.testing._internal.common_nn.kldivloss_no_reduce_scalar_test()
torch.testing._internal.common_nn.kldivloss_no_reduce_test()
torch.testing._internal.common_nn.kldivloss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.kldivloss_with_log_target_no_reduce_test()
torch.testing._internal.common_nn.kldivloss_with_target_no_reduce_test()
torch.testing._internal.common_nn.l1loss_no_reduce_complex_test()
torch.testing._internal.common_nn.l1loss_no_reduce_scalar_test()
torch.testing._internal.common_nn.l1loss_no_reduce_test()
torch.testing._internal.common_nn.marginrankingloss_reference(input1,input2,target,margin=0,reduction='mean')
torch.testing._internal.common_nn.mseloss_no_reduce_scalar_test()
torch.testing._internal.common_nn.mseloss_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_0d_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_1d_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_index_neg_test()
torch.testing._internal.common_nn.multilabelmarginloss_no_reduce_test()
torch.testing._internal.common_nn.multilabelmarginloss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.multilabelsoftmarginloss_no_reduce_test()
torch.testing._internal.common_nn.multilabelsoftmarginloss_weights_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_1d_input_0d_target_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_1d_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_margin_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_p_no_reduce_test()
torch.testing._internal.common_nn.multimarginloss_reference(input,target,p=1,margin=1,weight=None,reduction='mean')
torch.testing._internal.common_nn.multimarginloss_weights_no_reduce_test()
torch.testing._internal.common_nn.nllloss2d_no_reduce_ignore_index_test()
torch.testing._internal.common_nn.nllloss2d_no_reduce_test()
torch.testing._internal.common_nn.nllloss2d_no_reduce_weights_test()
torch.testing._internal.common_nn.nlllossNd_no_reduce_ignore_index_test()
torch.testing._internal.common_nn.nlllossNd_no_reduce_test()
torch.testing._internal.common_nn.nlllossNd_no_reduce_weights_test()
torch.testing._internal.common_nn.nlllossNd_reference(input,target,weight=None,ignore_index=-100,reduction='mean')
torch.testing._internal.common_nn.nllloss_no_reduce_ignore_index_test()
torch.testing._internal.common_nn.nllloss_no_reduce_test()
torch.testing._internal.common_nn.nllloss_no_reduce_weights_ignore_index_neg_test()
torch.testing._internal.common_nn.nllloss_no_reduce_weights_ignore_index_test()
torch.testing._internal.common_nn.nllloss_no_reduce_weights_test()
torch.testing._internal.common_nn.nllloss_reference(input,target,weight=None,ignore_index=-100,reduction='mean')
torch.testing._internal.common_nn.padding1d_circular(input,pad)
torch.testing._internal.common_nn.padding2d_circular(input,pad)
torch.testing._internal.common_nn.padding3d_circular(input,pad)
torch.testing._internal.common_nn.poissonnllloss_no_reduce_test()
torch.testing._internal.common_nn.single_batch_reference_criterion_fn(*args)
torch.testing._internal.common_nn.single_batch_reference_fn(input,parameters,module)
torch.testing._internal.common_nn.smoothl1loss_beta_test()
torch.testing._internal.common_nn.smoothl1loss_no_reduce_scalar_test()
torch.testing._internal.common_nn.smoothl1loss_no_reduce_test()
torch.testing._internal.common_nn.smoothl1loss_reference(input,target,reduction='mean',beta=1.0)
torch.testing._internal.common_nn.smoothl1loss_zero_beta_test()
torch.testing._internal.common_nn.softmarginloss_no_reduce_test()
torch.testing._internal.common_nn.softmarginloss_reference(input,target,reduction='mean')
torch.testing._internal.common_nn.tripletmarginloss_reference(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,reduction='mean')
torch.testing._internal.common_nn.wrap_functional(fn,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_cuda.py----------------------------------------
A:torch.testing._internal.common_cuda.TEST_CUDA->torch.cuda.is_available()
A:torch.testing._internal.common_cuda.TEST_NUMBA_CUDA->numba.cuda.is_available()
A:torch.testing._internal.common_cuda.arg_names->tuple(params.keys())
A:torch.testing._internal.common_cuda.cond->tf32_is_not_fp32()
A:torch.testing._internal.common_cuda.cuda_version->str(torch.version.cuda)
A:torch.testing._internal.common_cuda.version->_get_torch_cuda_version()
A:torch.testing._internal.common_cuda.TEST_CUSPARSE_GENERIC->_check_cusparse_generic_available()
torch.testing._internal.common_cuda._check_cusparse_generic_available()
torch.testing._internal.common_cuda._get_torch_cuda_version()
torch.testing._internal.common_cuda.initialize_cuda_context_rng()
torch.testing._internal.common_cuda.tf32_is_not_fp32()
torch.testing._internal.common_cuda.tf32_off()
torch.testing._internal.common_cuda.tf32_on(self,tf32_precision=1e-05)
torch.testing._internal.common_cuda.tf32_on_and_off(tf32_precision=1e-05)
torch.testing._internal.common_cuda.with_tf32_off(f)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_modules.py----------------------------------------
A:torch.testing._internal.common_modules.module_cls->getattr(namespace, module_name)
A:torch.testing._internal.common_modules.namespace_name->namespace.__name__.replace('torch.', '').replace('.modules', '')
A:torch.testing._internal.common_modules.test_name->module_info.name.replace('.', '_')
A:torch.testing._internal.common_modules.dtypes->dtypes.intersection(self.allowed_dtypes).intersection(self.allowed_dtypes)
A:torch.testing._internal.common_modules.test_wrapper->decorator(test_wrapper)
A:torch.testing._internal.common_modules.make_input->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_modules.result->torch.einsum('bn,anm,bm->ba', x1, p[0], x2)
A:torch.testing._internal.common_modules.make_weight->partial(make_tensor, device=device, dtype=dtype, requires_grad=False)
A:torch.testing._internal.common_modules.make_target->partial(make_tensor, device=device, dtype=torch.long, requires_grad=False)
A:torch.testing._internal.common_modules.v->kwargs.get(key, default)
A:torch.testing._internal.common_modules.kwargs_to_batchify->get_and_pop('kwargs_to_batchify', None)
A:torch.testing._internal.common_modules.is_criterion->get_and_pop('is_criterion', False)
A:torch.testing._internal.common_modules.kwargs[k]->kwargs.get(key, default).unsqueeze(bdim)
A:torch.testing._internal.common_modules.output->m(*single_batch_input_args, **kwargs)
A:torch.testing._internal.common_modules.reduction->get_reduction(m)
A:torch.testing._internal.common_modules.kwargs['key_padding_mask']->kwargs['key_padding_mask'].unsqueeze(0).unsqueeze(0)
A:torch.testing._internal.common_modules.h->h.unsqueeze(1).unsqueeze(1)
A:torch.testing._internal.common_modules.inp->inp.unsqueeze(batch_dim).unsqueeze(batch_dim)
A:torch.testing._internal.common_modules.lazy->kwargs.get('lazy', False)
A:torch.testing._internal.common_modules.transposed->kwargs.get('transposed', False)
A:torch.testing._internal.common_modules.make_empty->partial(torch.empty, device=device, dtype=torch.long, requires_grad=False)
A:torch.testing._internal.common_modules.products->itertools.product(bool_vals, bool_vals, bool_vals, key_padding_masks, attn_masks)
A:torch.testing._internal.common_modules.is_rnn->kwargs.get('is_rnn', False)
A:torch.testing._internal.common_modules.prod_gen->product(bias, batch_first, bidirectional, proj_sizes)
torch.testing._internal.common_modules.FunctionInput(self,*args,**kwargs)
torch.testing._internal.common_modules.FunctionInput.__init__(self,*args,**kwargs)
torch.testing._internal.common_modules.ModuleInfo(self,module_cls,*,module_inputs_func,skips=(),decorators=None,dtypes=floating_types(),supports_gradgrad=True,gradcheck_nondet_tol=0.0,module_memformat_affects_out=False)
torch.testing._internal.common_modules.ModuleInfo.__init__(self,module_cls,*,module_inputs_func,skips=(),decorators=None,dtypes=floating_types(),supports_gradgrad=True,gradcheck_nondet_tol=0.0,module_memformat_affects_out=False)
torch.testing._internal.common_modules.ModuleInfo.formatted_name(self)
torch.testing._internal.common_modules.ModuleInfo.name(self)
torch.testing._internal.common_modules.ModuleInfo.should_skip(self,cls_name,test_name,device_type,dtype)
torch.testing._internal.common_modules.ModuleInput(self,constructor_input,forward_input=None,desc='',reference_fn=None)
torch.testing._internal.common_modules.ModuleInput.__init__(self,constructor_input,forward_input=None,desc='',reference_fn=None)
torch.testing._internal.common_modules.formatted_module_name(module_cls)
torch.testing._internal.common_modules.generate_regression_criterion_inputs(make_input)
torch.testing._internal.common_modules.module_inputs_torch_nn_AdaptiveAvgPool2d(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_AvgPool1d(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_BatchNorm2d(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_BatchNorm3d(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_Bilinear(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_CELU(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_ConvNd(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_CrossEntropyLoss(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_ELU(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_Embedding(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_GaussianNLLLoss(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_Hardswish(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_L1Loss(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_LSTM(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_LSTMCell(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_Linear(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_MaxPool2d(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_MultiheadAttention(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_NLLLoss(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_RNN_GRU(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_RNN_GRU_Cell(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_ReLU(module_info,device,dtype,requires_grad)
torch.testing._internal.common_modules.module_inputs_torch_nn_Sigmoid(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_Transformer(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_TransformerDecoderLayer(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.module_inputs_torch_nn_TransformerEncoderLayer(module_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_modules.modules(self,module_info_list,allowed_dtypes=None)
torch.testing._internal.common_modules.modules.__init__(self,module_info_list,allowed_dtypes=None)
torch.testing._internal.common_modules.modules._parametrize_test(self,test,generic_cls,device_cls)
torch.testing._internal.common_modules.no_batch_dim_reference_fn(m,p,*args,**kwargs)
torch.testing._internal.common_modules.no_batch_dim_reference_lstm(m,p,*args,**kwargs)
torch.testing._internal.common_modules.no_batch_dim_reference_lstmcell(m,p,*args,**kwargs)
torch.testing._internal.common_modules.no_batch_dim_reference_mha(m,p,*args,**kwargs)
torch.testing._internal.common_modules.no_batch_dim_reference_rnn_gru(m,p,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_quantization.py----------------------------------------
A:torch.testing._internal.common_quantization.output->model(image)
A:torch.testing._internal.common_quantization._default_loss_fn->torch.nn.CrossEntropyLoss()
A:torch.testing._internal.common_quantization.optimizer->torch.optim.SGD(model_with_ddp.parameters(), lr=0.0001)
A:torch.testing._internal.common_quantization.loss->criterion(output, target)
A:torch.testing._internal.common_quantization.(_, predicted)->torch.max(output, 1)
A:torch.testing._internal.common_quantization.maxk->max(topk)
A:torch.testing._internal.common_quantization.batch_size->target.size(0)
A:torch.testing._internal.common_quantization.(_, pred)->model(image).topk(maxk, 1, True, True)
A:torch.testing._internal.common_quantization.pred->pred.t().t()
A:torch.testing._internal.common_quantization.correct->pred.t().t().eq(target.view(1, -1).expand_as(pred))
A:torch.testing._internal.common_quantization.correct_k->correct[:k].view(-1).float().sum(0, keepdim=True)
A:torch.testing._internal.common_quantization.start_time->time.time()
A:torch.testing._internal.common_quantization.(acc1, acc5)->accuracy(output, target, topk=(1, 5))
A:torch.testing._internal.common_quantization.prepared->prepare(model, qconfig_dict, prepare_custom_config_dict=prepare_custom_config_dict, backend_config_dict=backend_config_dict)
A:torch.testing._internal.common_quantization.X_init->torch.randint(X_value_min, X_value_max, (batch_size, in_channels) + input_feature_map_size)
A:torch.testing._internal.common_quantization.X_q->torch.quantize_per_tensor(X, scale=X_scale, zero_point=X_zero_point, dtype=torch.quint8)
A:torch.testing._internal.common_quantization.W_init->torch.randint(W_value_min, W_value_max, (out_channels, in_channels_per_group) + kernel_size)
A:torch.testing._internal.common_quantization.b_init->torch.randint(0, 10, (out_channels,))
A:torch.testing._internal.common_quantization.W_scales_tensor->torch.tensor(W_scale, dtype=torch.float)
A:torch.testing._internal.common_quantization.W_zero_points_tensor->torch.tensor(W_zero_point, dtype=torch.float)
A:torch.testing._internal.common_quantization.W_q->torch.quantize_per_tensor(W, scale=W_scale[0], zero_point=W_zero_point[0], dtype=torch.qint8)
A:torch.testing._internal.common_quantization.skip_if_no_torchvision->unittest.skipIf(not HAS_TORCHVISION, 'no torchvision')
A:torch.testing._internal.common_quantization.tt->torch.from_numpy(np.cumsum(tt, dtype=offset_type))
A:torch.testing._internal.common_quantization.propagate_qconfig_list->get_default_qconfig_propagation_list()
A:torch.testing._internal.common_quantization.float_to_observed_module_class_mapping->prepare_custom_config_dict.get('float_to_observed_custom_module_class', {})
A:torch.testing._internal.common_quantization.model_dict->ref_model.state_dict()
A:torch.testing._internal.common_quantization.b->io.BytesIO()
A:torch.testing._internal.common_quantization.loaded_dict->torch.load(b)
A:torch.testing._internal.common_quantization.ref_out->ref_model(*x)
A:torch.testing._internal.common_quantization.load_out->loaded(*x)
A:torch.testing._internal.common_quantization.loaded->torch.load(b)
A:torch.testing._internal.common_quantization.weight->ref_model.get_weight()
A:torch.testing._internal.common_quantization.bias->ref_model.get_bias()
A:torch.testing._internal.common_quantization.scripted->torch.jit.script(orig_mod)
A:torch.testing._internal.common_quantization.traced->torch.jit.trace(orig_mod, calib_data[0])
A:torch.testing._internal.common_quantization.buffer->io.BytesIO(script_module._save_to_buffer_for_lite_interpreter())
A:torch.testing._internal.common_quantization.loaded_mod->torch.jit.load(buffer)
A:torch.testing._internal.common_quantization.ref_output->orig_mod(*inp)
A:torch.testing._internal.common_quantization.scripted_output->test_mod(*inp)
A:torch.testing._internal.common_quantization.module->module.eval().eval()
A:torch.testing._internal.common_quantization.model->quantize(model, test_only_eval_fn, [self.calib_data])
A:torch.testing._internal.common_quantization.models[debug]->quantize_jit(model, qconfig_dict, test_only_eval_fn, [inputs_copy], inplace=False, debug=debug)
A:torch.testing._internal.common_quantization.outputs[debug]->models[debug](*inputs[0])
A:torch.testing._internal.common_quantization.inputs_copy->copy.deepcopy(inputs)
A:torch.testing._internal.common_quantization.nodes_in_graph->dict()
A:torch.testing._internal.common_quantization.modules->dict(graph_module.named_modules(remove_duplicate=False))
A:torch.testing._internal.common_quantization.n->NodeSpec(node.op, type(modules[node.target]))
A:torch.testing._internal.common_quantization.node_info->' '.join(map(repr, [n.op, n.name, n.target, n.args, n.kwargs]))
A:torch.testing._internal.common_quantization.str_to_print->'\n'.join(node_infos)
A:torch.testing._internal.common_quantization.mod->getattr(gm, node.target)
A:torch.testing._internal.common_quantization.act_type_start_a->_get_underlying_op_type(subgraph_a.start_node, gm_a)
A:torch.testing._internal.common_quantization.act_type_start_b->_get_underlying_op_type(subgraph_b.start_node, gm_b)
A:torch.testing._internal.common_quantization.act_type_end_a->_get_underlying_op_type(subgraph_a.end_node, gm_a)
A:torch.testing._internal.common_quantization.act_type_end_b->_get_underlying_op_type(subgraph_b.end_node, gm_b)
A:torch.testing._internal.common_quantization.(model_name_0, model_name_1)->layer_data.keys()
A:torch.testing._internal.common_quantization.qconfig->torch.quantization.get_default_qconfig(qengine)
A:torch.testing._internal.common_quantization.prepared_copy->copy.deepcopy(prepared)
A:torch.testing._internal.common_quantization.qgraph->convert_fx(prepared)
A:torch.testing._internal.common_quantization.qgraph_reference->convert_fx(prepared_copy, is_reference=True)
A:torch.testing._internal.common_quantization.result->qgraph(*inputs)
A:torch.testing._internal.common_quantization.result_reference->qgraph_reference(*inputs)
A:torch.testing._internal.common_quantization.qgraph_copy->copy.deepcopy(qgraph)
A:torch.testing._internal.common_quantization.qgraph_reference_copy->copy.deepcopy(qgraph_reference)
A:torch.testing._internal.common_quantization.emb_dict->qemb.state_dict()
A:torch.testing._internal.common_quantization.emb_weight->embedding_unpack(emb_dict[key])
A:torch.testing._internal.common_quantization.loaded_weight->embedding_unpack(loaded_dict[key])
A:torch.testing._internal.common_quantization.loaded_qemb->torch.nn.quantized.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, dtype=dtype)
A:torch.testing._internal.common_quantization.float_embedding->torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)
A:torch.testing._internal.common_quantization.float_qparams_observer->torch.quantization.PerChannelMinMaxObserver.with_args(dtype=dtype, qscheme=torch.per_channel_affine_float_qparams, ch_axis=0)
A:torch.testing._internal.common_quantization.float_embedding.qconfig->QConfig(activation=default_dynamic_quant_observer, weight=float_qparams_observer)
A:torch.testing._internal.common_quantization.q_embeddingbag->torch.nn.quantized.Embedding.from_float(float_embedding)
A:torch.testing._internal.common_quantization.script_module->torch.jit.script(model)
A:torch.testing._internal.common_quantization.script_module_result->script_module(input)
A:torch.testing._internal.common_quantization.mobile_module->_load_for_lite_interpreter(buffer)
A:torch.testing._internal.common_quantization.mobile_module_result->mobile_module(input)
A:torch.testing._internal.common_quantization.mobile_module_forward_result->_load_for_lite_interpreter(buffer).forward(input)
A:torch.testing._internal.common_quantization.mobile_module_run_method_result->_load_for_lite_interpreter(buffer).run_method('forward', input)
A:torch.testing._internal.common_quantization.self.fc1->torch.nn.Linear(64, 10).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.x->self.bn3(x)
A:torch.testing._internal.common_quantization.self.qconfig->get_default_qat_qconfig('qnnpack')
A:torch.testing._internal.common_quantization.self.fc2->torch.nn.Linear(10, 10).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.mod->torch.nn.RNNCell(2, 2, nonlinearity='tanh').to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.lstm->torch.nn.LSTM(2, 2).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.(x, hid)->self.lstm(x, hid)
A:torch.testing._internal.common_quantization.self.conv->torch.nn.Conv2d(2, 2, 1, bias=None).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.quant->QuantStub()
A:torch.testing._internal.common_quantization.self.dequant->DeQuantStub()
A:torch.testing._internal.common_quantization.self.bn->torch.nn.BatchNorm1d(10)
A:torch.testing._internal.common_quantization.self.relu->torch.nn.ReLU(inplace=False).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.conv1->torch.nn.Conv2d(inplanes, inplanes, (1, 1), bias=False)
A:torch.testing._internal.common_quantization.self.conv2->torch.nn.Conv2d(inplanes, inplanes, (1, 1), bias=False)
A:torch.testing._internal.common_quantization.self.subm->TwoLayerLinearModel()
A:torch.testing._internal.common_quantization.self.fc->torch.nn.Linear(4, 2)
A:torch.testing._internal.common_quantization.self.fc2.qconfig->torch.quantization.get_default_qconfig('fbgemm')
A:torch.testing._internal.common_quantization.self.hardswish->torch.nn.Hardswish().to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.elu->torch.nn.ELU().to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.layer_norm->torch.nn.LayerNorm(8)
A:torch.testing._internal.common_quantization.self.group_norm->torch.nn.GroupNorm(2, 8)
A:torch.testing._internal.common_quantization.self.instance_norm1d->torch.nn.InstanceNorm1d(8)
A:torch.testing._internal.common_quantization.self.instance_norm2d->torch.nn.InstanceNorm2d(8)
A:torch.testing._internal.common_quantization.self.instance_norm3d->torch.nn.InstanceNorm3d(8)
A:torch.testing._internal.common_quantization.self.sub1->SubModelForFusion()
A:torch.testing._internal.common_quantization.self.sub2->SubModelWithoutFusion()
A:torch.testing._internal.common_quantization.self.fc3->torch.nn.Linear(5, 5).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.sub2.fc1->QuantWrapper(self.sub2.fc1)
A:torch.testing._internal.common_quantization.custom_qconfig->QConfig(activation=default_observer.with_args(**custom_options), weight=default_weight_observer)
A:torch.testing._internal.common_quantization.self.sub2.fc2->QuantWrapper(self.sub2.fc2)
A:torch.testing._internal.common_quantization.self.relu1->torch.nn.ReLU()
A:torch.testing._internal.common_quantization.self.relu2->torch.nn.ReLU()
A:torch.testing._internal.common_quantization.named_children->list(self.named_children())
A:torch.testing._internal.common_quantization.self.weight->torch.rand(3, 3, 3, 3)
A:torch.testing._internal.common_quantization.self.bias->torch.rand(3)
A:torch.testing._internal.common_quantization.self.linear1->FunctionalLinear()
A:torch.testing._internal.common_quantization.self.linear2->FunctionalLinear()
A:torch.testing._internal.common_quantization.self.linear->torch.nn.Linear(12, 1).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.sub->QuantWrapper(InnerModule())
A:torch.testing._internal.common_quantization.self.dropout->torch.nn.Dropout(0.5)
A:torch.testing._internal.common_quantization.self.emb->torch.nn.EmbeddingBag(num_embeddings=10, embedding_dim=12)
A:torch.testing._internal.common_quantization.self.bn1->norm_layer(inplanes)
A:torch.testing._internal.common_quantization.self.bn2->torch.nn.BatchNorm2d(3)
A:torch.testing._internal.common_quantization.self.relu3->torch.nn.ReLU(inplace=True).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.self.conv3->torch.nn.ConvTranspose3d(3, 3, 1)
A:torch.testing._internal.common_quantization.self.bn3->torch.nn.BatchNorm3d(3)
A:torch.testing._internal.common_quantization.self.relu4->torch.nn.ReLU(inplace=True).to(dtype=torch.float)
A:torch.testing._internal.common_quantization.y->self.mycat.cat([x, x, x])
A:torch.testing._internal.common_quantization.self.features->torch.nn.Sequential(*layers)
A:torch.testing._internal.common_quantization.self.classifier->torch.nn.Sequential(*head)
A:torch.testing._internal.common_quantization.self.seq->torch.nn.Sequential()
A:torch.testing._internal.common_quantization.self.mycat->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.myadd->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.myadd_relu->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.z->self.myadd.add(y, y)
A:torch.testing._internal.common_quantization.w->self.myadd_relu.add_relu(z, z)
A:torch.testing._internal.common_quantization.self.downsample->torch.nn.Identity()
A:torch.testing._internal.common_quantization.self.myop->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.avgpool->torch.nn.AdaptiveAvgPool2d((4, 4))
A:torch.testing._internal.common_quantization.out->self.dense_top(sparse_feature, dense)
A:torch.testing._internal.common_quantization.identity->self.downsample(x)
A:torch.testing._internal.common_quantization.self.skip_add->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.cat->torch.nn.quantized.FloatFunctional()
A:torch.testing._internal.common_quantization.self.maxpool->torch.nn.MaxPool2d((4, 4))
A:torch.testing._internal.common_quantization.skip->self.conv2(x)
A:torch.testing._internal.common_quantization.emb->self.emb(indices, offsets)
A:torch.testing._internal.common_quantization.q_x->self.quant(linear_in)
A:torch.testing._internal.common_quantization.fc->self.dequant(fc)
A:torch.testing._internal.common_quantization.features->torch.cat([dense_feature] + [sparse_feature], dim=1)
A:torch.testing._internal.common_quantization.self.dense_mlp->torch.nn.Sequential(nn.Linear(dense_dim, dense_out))
A:torch.testing._internal.common_quantization.self.top_mlp->torch.nn.Sequential(nn.Linear(dense_out + embedding_dim, top_out_in), nn.Linear(top_out_in, top_out_out))
A:torch.testing._internal.common_quantization.dense_feature->self.dense_mlp(dense)
A:torch.testing._internal.common_quantization.self.emb_bag->torch.nn.EmbeddingBag(num_embeddings, embedding_dim, mode='sum')
A:torch.testing._internal.common_quantization.self.model_sparse->EmbBagWrapper(self._NUM_EMBEDDINGS, self._EMBEDDING_DIM)
A:torch.testing._internal.common_quantization.self.dense_top->DenseTopMLP(self._DENSE_DIM, self._DENSE_OUTPUT, self._EMBEDDING_DIM, self._TOP_OUT_IN, self._TOP_OUT_OUT)
A:torch.testing._internal.common_quantization.sparse_feature->self.model_sparse(sparse_indices, sparse_offsets)
torch.testing._internal.common_quantization.ActivationsTestModel(self)
torch.testing._internal.common_quantization.ActivationsTestModel.__init__(self)
torch.testing._internal.common_quantization.ActivationsTestModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedConvBnModel(self)
torch.testing._internal.common_quantization.AnnotatedConvBnModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedConvBnModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedConvBnReLUModel(self,qengine='fbgemm')
torch.testing._internal.common_quantization.AnnotatedConvBnReLUModel.__init__(self,qengine='fbgemm')
torch.testing._internal.common_quantization.AnnotatedConvBnReLUModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedConvBnReLUModel.fuse_model(self)
torch.testing._internal.common_quantization.AnnotatedConvModel(self,qengine)
torch.testing._internal.common_quantization.AnnotatedConvModel.__init__(self,qengine)
torch.testing._internal.common_quantization.AnnotatedConvModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedConvTransposeModel(self,qengine)
torch.testing._internal.common_quantization.AnnotatedConvTransposeModel.__init__(self,qengine)
torch.testing._internal.common_quantization.AnnotatedConvTransposeModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedCustomConfigNestedModel(self)
torch.testing._internal.common_quantization.AnnotatedCustomConfigNestedModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedCustomConfigNestedModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedNestedModel(self,qengine)
torch.testing._internal.common_quantization.AnnotatedNestedModel.__init__(self,qengine)
torch.testing._internal.common_quantization.AnnotatedNestedModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedSingleLayerLinearModel(self,qengine='fbgemm')
torch.testing._internal.common_quantization.AnnotatedSingleLayerLinearModel.__init__(self,qengine='fbgemm')
torch.testing._internal.common_quantization.AnnotatedSingleLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedSkipQuantModel(self,qengine)
torch.testing._internal.common_quantization.AnnotatedSkipQuantModel.__init__(self,qengine)
torch.testing._internal.common_quantization.AnnotatedSkipQuantModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedSkipQuantModel.fuse_modules(self)
torch.testing._internal.common_quantization.AnnotatedSubNestedModel(self)
torch.testing._internal.common_quantization.AnnotatedSubNestedModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedSubNestedModel.forward(self,x)
torch.testing._internal.common_quantization.AnnotatedTwoLayerLinearModel(self)
torch.testing._internal.common_quantization.AnnotatedTwoLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.AnnotatedTwoLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.AverageMeter(self,name,fmt=':f')
torch.testing._internal.common_quantization.AverageMeter.__init__(self,name,fmt=':f')
torch.testing._internal.common_quantization.AverageMeter.__str__(self)
torch.testing._internal.common_quantization.AverageMeter.reset(self)
torch.testing._internal.common_quantization.AverageMeter.update(self,val,n=1)
torch.testing._internal.common_quantization.ConvBNReLU(self)
torch.testing._internal.common_quantization.ConvBNReLU.__init__(self)
torch.testing._internal.common_quantization.ConvBnModel(self)
torch.testing._internal.common_quantization.ConvBnModel.__init__(self)
torch.testing._internal.common_quantization.ConvBnModel.forward(self,x)
torch.testing._internal.common_quantization.ConvBnReLUModel(self)
torch.testing._internal.common_quantization.ConvBnReLUModel.__init__(self)
torch.testing._internal.common_quantization.ConvBnReLUModel.forward(self,x)
torch.testing._internal.common_quantization.ConvModel(self)
torch.testing._internal.common_quantization.ConvModel.__init__(self)
torch.testing._internal.common_quantization.ConvModel.forward(self,x)
torch.testing._internal.common_quantization.ConvReluAddModel(self)
torch.testing._internal.common_quantization.ConvReluAddModel.__init__(self)
torch.testing._internal.common_quantization.ConvReluAddModel.forward(self,x)
torch.testing._internal.common_quantization.ConvReluConvModel(self)
torch.testing._internal.common_quantization.ConvReluConvModel.__init__(self)
torch.testing._internal.common_quantization.ConvReluConvModel.forward(self,x)
torch.testing._internal.common_quantization.ConvReluModel(self)
torch.testing._internal.common_quantization.ConvReluModel.__init__(self)
torch.testing._internal.common_quantization.ConvReluModel.forward(self,x)
torch.testing._internal.common_quantization.ConvTransposeModel(self)
torch.testing._internal.common_quantization.ConvTransposeModel.__init__(self)
torch.testing._internal.common_quantization.ConvTransposeModel.forward(self,x)
torch.testing._internal.common_quantization.DeFusedEmbeddingBagLinear(self)
torch.testing._internal.common_quantization.DeFusedEmbeddingBagLinear.__init__(self)
torch.testing._internal.common_quantization.DeFusedEmbeddingBagLinear.forward(self,input:torch.Tensor)->torch.Tensor
torch.testing._internal.common_quantization.DenseTopMLP(self,dense_dim,dense_out,embedding_dim,top_out_in,top_out_out)
torch.testing._internal.common_quantization.DenseTopMLP.__init__(self,dense_dim,dense_out,embedding_dim,top_out_in,top_out_out)
torch.testing._internal.common_quantization.DenseTopMLP.forward(self,sparse_feature:torch.Tensor,dense:torch.Tensor)->torch.Tensor
torch.testing._internal.common_quantization.DummyObserver(torch.nn.Module)
torch.testing._internal.common_quantization.DummyObserver.calculate_qparams(self)
torch.testing._internal.common_quantization.DummyObserver.forward(self,x)
torch.testing._internal.common_quantization.EmbBagWrapper(self,num_embeddings,embedding_dim)
torch.testing._internal.common_quantization.EmbBagWrapper.__init__(self,num_embeddings,embedding_dim)
torch.testing._internal.common_quantization.EmbBagWrapper.forward(self,indices,offsets)
torch.testing._internal.common_quantization.EmbeddingBagModule(self)
torch.testing._internal.common_quantization.EmbeddingBagModule.__init__(self)
torch.testing._internal.common_quantization.EmbeddingBagModule.forward(self,indices,offsets,per_sample_weights)
torch.testing._internal.common_quantization.EmbeddingModule(self)
torch.testing._internal.common_quantization.EmbeddingModule.__init__(self)
torch.testing._internal.common_quantization.EmbeddingModule.forward(self,indices)
torch.testing._internal.common_quantization.EmbeddingWithStaticLinear(self)
torch.testing._internal.common_quantization.EmbeddingWithStaticLinear.__init__(self)
torch.testing._internal.common_quantization.EmbeddingWithStaticLinear.forward(self,indices,offsets,linear_in)
torch.testing._internal.common_quantization.FunctionalConv2d(self)
torch.testing._internal.common_quantization.FunctionalConv2d.__init__(self)
torch.testing._internal.common_quantization.FunctionalConv2d.forward(self,x)
torch.testing._internal.common_quantization.FunctionalConvReluConvModel(self)
torch.testing._internal.common_quantization.FunctionalConvReluConvModel.__init__(self)
torch.testing._internal.common_quantization.FunctionalConvReluConvModel.forward(self,x)
torch.testing._internal.common_quantization.FunctionalConvReluModel(self)
torch.testing._internal.common_quantization.FunctionalConvReluModel.__init__(self)
torch.testing._internal.common_quantization.FunctionalConvReluModel.forward(self,x)
torch.testing._internal.common_quantization.FunctionalLinear(self)
torch.testing._internal.common_quantization.FunctionalLinear.__init__(self)
torch.testing._internal.common_quantization.FunctionalLinear.forward(self,x)
torch.testing._internal.common_quantization.FunctionalLinearAddModel(self)
torch.testing._internal.common_quantization.FunctionalLinearAddModel.__init__(self)
torch.testing._internal.common_quantization.FunctionalLinearAddModel.forward(self,x)
torch.testing._internal.common_quantization.FunctionalLinearReluLinearModel(self)
torch.testing._internal.common_quantization.FunctionalLinearReluLinearModel.__init__(self)
torch.testing._internal.common_quantization.FunctionalLinearReluLinearModel.forward(self,x)
torch.testing._internal.common_quantization.FunctionalLinearReluModel(self)
torch.testing._internal.common_quantization.FunctionalLinearReluModel.__init__(self)
torch.testing._internal.common_quantization.FunctionalLinearReluModel.forward(self,x)
torch.testing._internal.common_quantization.InnerModule(self)
torch.testing._internal.common_quantization.InnerModule.__init__(self)
torch.testing._internal.common_quantization.InnerModule.forward(self,x)
torch.testing._internal.common_quantization.InnerModule.fuse_modules(self)
torch.testing._internal.common_quantization.LSTMwithHiddenDynamicModel(self,qengine='fbgemm')
torch.testing._internal.common_quantization.LSTMwithHiddenDynamicModel.__init__(self,qengine='fbgemm')
torch.testing._internal.common_quantization.LSTMwithHiddenDynamicModel.forward(self,x,hid)
torch.testing._internal.common_quantization.LinearAddModel(self)
torch.testing._internal.common_quantization.LinearAddModel.__init__(self)
torch.testing._internal.common_quantization.LinearAddModel.forward(self,x)
torch.testing._internal.common_quantization.LinearModelWithSubmodule(self)
torch.testing._internal.common_quantization.LinearModelWithSubmodule.__init__(self)
torch.testing._internal.common_quantization.LinearModelWithSubmodule.forward(self,x)
torch.testing._internal.common_quantization.LinearReluAddModel(self)
torch.testing._internal.common_quantization.LinearReluAddModel.__init__(self)
torch.testing._internal.common_quantization.LinearReluAddModel.forward(self,x)
torch.testing._internal.common_quantization.LinearReluLinearModel(self)
torch.testing._internal.common_quantization.LinearReluLinearModel.__init__(self)
torch.testing._internal.common_quantization.LinearReluLinearModel.forward(self,x)
torch.testing._internal.common_quantization.LinearReluModel(self)
torch.testing._internal.common_quantization.LinearReluModel.__init__(self)
torch.testing._internal.common_quantization.LinearReluModel.forward(self,x)
torch.testing._internal.common_quantization.ManualConvLinearQATModel(self)
torch.testing._internal.common_quantization.ManualConvLinearQATModel.__init__(self)
torch.testing._internal.common_quantization.ManualConvLinearQATModel.forward(self,x)
torch.testing._internal.common_quantization.ManualDropoutQATModel(self,qengine)
torch.testing._internal.common_quantization.ManualDropoutQATModel.__init__(self,qengine)
torch.testing._internal.common_quantization.ManualDropoutQATModel.forward(self,x)
torch.testing._internal.common_quantization.ManualEmbeddingBagLinear(self)
torch.testing._internal.common_quantization.ManualEmbeddingBagLinear.__init__(self)
torch.testing._internal.common_quantization.ManualEmbeddingBagLinear.forward(self,input:torch.Tensor,offsets:Optional[torch.Tensor]=None,per_sample_weights:Optional[torch.Tensor]=None)
torch.testing._internal.common_quantization.ManualLinearDynamicQATModel(self,qconfig=None)
torch.testing._internal.common_quantization.ManualLinearDynamicQATModel.__init__(self,qconfig=None)
torch.testing._internal.common_quantization.ManualLinearDynamicQATModel.forward(self,x)
torch.testing._internal.common_quantization.ManualLinearQATModel(self,qengine)
torch.testing._internal.common_quantization.ManualLinearQATModel.__init__(self,qengine)
torch.testing._internal.common_quantization.ManualLinearQATModel.forward(self,x)
torch.testing._internal.common_quantization.ModelForConvTransposeBNFusion(self)
torch.testing._internal.common_quantization.ModelForConvTransposeBNFusion.__init__(self)
torch.testing._internal.common_quantization.ModelForConvTransposeBNFusion.forward(self,x)
torch.testing._internal.common_quantization.ModelForFusion(self,qconfig)
torch.testing._internal.common_quantization.ModelForFusion.__init__(self,qconfig)
torch.testing._internal.common_quantization.ModelForFusion.forward(self,x)
torch.testing._internal.common_quantization.ModelForFusionWithBias(self)
torch.testing._internal.common_quantization.ModelForFusionWithBias.__init__(self)
torch.testing._internal.common_quantization.ModelForFusionWithBias.forward(self,x)
torch.testing._internal.common_quantization.ModelForLinearBNFusion(self)
torch.testing._internal.common_quantization.ModelForLinearBNFusion.__init__(self)
torch.testing._internal.common_quantization.ModelForLinearBNFusion.forward(self,x)
torch.testing._internal.common_quantization.ModelMultipleOps(self)
torch.testing._internal.common_quantization.ModelMultipleOps.__init__(self)
torch.testing._internal.common_quantization.ModelMultipleOps.forward(self,x)
torch.testing._internal.common_quantization.ModelMultipleOpsNoAvgPool(self)
torch.testing._internal.common_quantization.ModelMultipleOpsNoAvgPool.__init__(self)
torch.testing._internal.common_quantization.ModelMultipleOpsNoAvgPool.forward(self,x)
torch.testing._internal.common_quantization.ModelWithFunctionals(self)
torch.testing._internal.common_quantization.ModelWithFunctionals.__init__(self)
torch.testing._internal.common_quantization.ModelWithFunctionals.forward(self,x)
torch.testing._internal.common_quantization.ModelWithSequentialFusion(self)
torch.testing._internal.common_quantization.ModelWithSequentialFusion.__init__(self)
torch.testing._internal.common_quantization.ModelWithSequentialFusion.forward(self,x)
torch.testing._internal.common_quantization.NestedModel(self)
torch.testing._internal.common_quantization.NestedModel.__init__(self)
torch.testing._internal.common_quantization.NestedModel.forward(self,x)
torch.testing._internal.common_quantization.NodeSpec(self,op,target)
torch.testing._internal.common_quantization.NodeSpec.__eq__(self,other)
torch.testing._internal.common_quantization.NodeSpec.__hash__(self)
torch.testing._internal.common_quantization.NodeSpec.__init__(self,op,target)
torch.testing._internal.common_quantization.NodeSpec.__repr__(self)
torch.testing._internal.common_quantization.NodeSpec.call_function(cls,target)
torch.testing._internal.common_quantization.NodeSpec.call_method(cls,target)
torch.testing._internal.common_quantization.NodeSpec.call_module(cls,target)
torch.testing._internal.common_quantization.NormalizationTestModel(self)
torch.testing._internal.common_quantization.NormalizationTestModel.__init__(self)
torch.testing._internal.common_quantization.NormalizationTestModel.forward(self,x)
torch.testing._internal.common_quantization.QuantStubModel(self)
torch.testing._internal.common_quantization.QuantStubModel.__init__(self)
torch.testing._internal.common_quantization.QuantStubModel.forward(self,x)
torch.testing._internal.common_quantization.QuantSubModel(self)
torch.testing._internal.common_quantization.QuantSubModel.__init__(self)
torch.testing._internal.common_quantization.QuantSubModel.forward(self,x)
torch.testing._internal.common_quantization.QuantizationLiteTestCase(QuantizationTestCase)
torch.testing._internal.common_quantization.QuantizationLiteTestCase._compare_script_and_mobile(self,model:torch.nn.Module,input:torch.Tensor)
torch.testing._internal.common_quantization.QuantizationLiteTestCase._create_quantized_model(self,model_class:Type[torch.nn.Module],**kwargs)
torch.testing._internal.common_quantization.QuantizationLiteTestCase.setUp(self)
torch.testing._internal.common_quantization.QuantizationTestCase(TestCase)
torch.testing._internal.common_quantization.QuantizationTestCase._checkModuleCorrectnessAgainstOrig(self,orig_mod,test_mod,calib_data)
torch.testing._internal.common_quantization.QuantizationTestCase._checkScriptable(self,orig_mod,script_mod,calib_data,check_save_load)
torch.testing._internal.common_quantization.QuantizationTestCase.checkDynamicQuantizedLSTM(self,mod,reference_module_type,dtype)
torch.testing._internal.common_quantization.QuantizationTestCase.checkDynamicQuantizedLinear(self,mod,dtype)
torch.testing._internal.common_quantization.QuantizationTestCase.checkDynamicQuantizedLinearRelu(self,mod,dtype)
torch.testing._internal.common_quantization.QuantizationTestCase.checkDynamicQuantizedModule(self,mod,reference_module_type,dtype)
torch.testing._internal.common_quantization.QuantizationTestCase.checkEmbeddingSerialization(self,qemb,num_embeddings,embedding_dim,indices,offsets,set_qconfig,is_emb_bag,dtype=torch.quint8)
torch.testing._internal.common_quantization.QuantizationTestCase.checkGraphModeOp(self,module,inputs,quantized_op,tracing=False,debug=False,check=True,eval_mode=True,dynamic=False,qconfig=None)
torch.testing._internal.common_quantization.QuantizationTestCase.checkGraphModuleNodes(self,graph_module,expected_node=None,expected_node_occurrence=None,expected_node_list=None)
torch.testing._internal.common_quantization.QuantizationTestCase.checkHasPrepModules(self,module)
torch.testing._internal.common_quantization.QuantizationTestCase.checkLinear(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.checkNoPrepModules(self,module)
torch.testing._internal.common_quantization.QuantizationTestCase.checkNoQconfig(self,module)
torch.testing._internal.common_quantization.QuantizationTestCase.checkObservers(self,module,propagate_qconfig_list=None,prepare_custom_config_dict=None)
torch.testing._internal.common_quantization.QuantizationTestCase.checkQuantDequant(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.checkQuantizedLinear(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.checkScriptable(self,orig_mod,calib_data,check_save_load=False)
torch.testing._internal.common_quantization.QuantizationTestCase.checkWrappedQuantizedLinear(self,mod)
torch.testing._internal.common_quantization.QuantizationTestCase.check_eager_serialization(self,ref_model,loaded_model,x)
torch.testing._internal.common_quantization.QuantizationTestCase.check_weight_bias_api(self,ref_model,weight_keys,bias_keys)
torch.testing._internal.common_quantization.QuantizationTestCase.printGraphModule(self,graph_module,print_str=True)
torch.testing._internal.common_quantization.QuantizationTestCase.setUp(self)
torch.testing._internal.common_quantization.RNNCellDynamicModel(self,mod_type)
torch.testing._internal.common_quantization.RNNCellDynamicModel.__init__(self,mod_type)
torch.testing._internal.common_quantization.RNNCellDynamicModel.forward(self,x)
torch.testing._internal.common_quantization.RNNDynamicModel(self,mod_type)
torch.testing._internal.common_quantization.RNNDynamicModel.__init__(self,mod_type)
torch.testing._internal.common_quantization.RNNDynamicModel.forward(self,x)
torch.testing._internal.common_quantization.ResNetBase(self)
torch.testing._internal.common_quantization.ResNetBase.__init__(self)
torch.testing._internal.common_quantization.ResNetBase.forward(self,x)
torch.testing._internal.common_quantization.ResNetBase.fuse_model(self)
torch.testing._internal.common_quantization.SingleLayerFunctionalConvModel(self)
torch.testing._internal.common_quantization.SingleLayerFunctionalConvModel.__init__(self)
torch.testing._internal.common_quantization.SingleLayerFunctionalConvModel.forward(self,x)
torch.testing._internal.common_quantization.SingleLayerFunctionalLinearModel(self)
torch.testing._internal.common_quantization.SingleLayerFunctionalLinearModel.__init__(self)
torch.testing._internal.common_quantization.SingleLayerFunctionalLinearModel.forward(self,x)
torch.testing._internal.common_quantization.SingleLayerLinearDynamicModel(self,qengine='fbgemm')
torch.testing._internal.common_quantization.SingleLayerLinearDynamicModel.__init__(self,qengine='fbgemm')
torch.testing._internal.common_quantization.SingleLayerLinearDynamicModel.forward(self,x)
torch.testing._internal.common_quantization.SingleLayerLinearModel(self)
torch.testing._internal.common_quantization.SingleLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.SingleLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization.SkipQuantModel(self)
torch.testing._internal.common_quantization.SkipQuantModel.__init__(self)
torch.testing._internal.common_quantization.SkipQuantModel.forward(self,x)
torch.testing._internal.common_quantization.SkipQuantModel.fuse_modules(self)
torch.testing._internal.common_quantization.SparseNNModel(self)
torch.testing._internal.common_quantization.SparseNNModel.__init__(self)
torch.testing._internal.common_quantization.SparseNNModel.forward(self,sparse_indices:torch.Tensor,sparse_offsets:torch.Tensor,dense:torch.Tensor)->torch.Tensor
torch.testing._internal.common_quantization.SubModelForFusion(self)
torch.testing._internal.common_quantization.SubModelForFusion.__init__(self)
torch.testing._internal.common_quantization.SubModelForFusion.forward(self,x)
torch.testing._internal.common_quantization.SubModelWithoutFusion(self)
torch.testing._internal.common_quantization.SubModelWithoutFusion.__init__(self)
torch.testing._internal.common_quantization.SubModelWithoutFusion.forward(self,x)
torch.testing._internal.common_quantization.TwoLayerConvModel(self)
torch.testing._internal.common_quantization.TwoLayerConvModel.__init__(self)
torch.testing._internal.common_quantization.TwoLayerConvModel.forward(self,x)
torch.testing._internal.common_quantization.TwoLayerFunctionalConvModel(self)
torch.testing._internal.common_quantization.TwoLayerFunctionalConvModel.__init__(self)
torch.testing._internal.common_quantization.TwoLayerFunctionalConvModel.forward(self,x)
torch.testing._internal.common_quantization.TwoLayerFunctionalLinearModel(self)
torch.testing._internal.common_quantization.TwoLayerFunctionalLinearModel.__init__(self)
torch.testing._internal.common_quantization.TwoLayerFunctionalLinearModel.forward(self,x)
torch.testing._internal.common_quantization.TwoLayerLinearModel(self)
torch.testing._internal.common_quantization.TwoLayerLinearModel.__init__(self)
torch.testing._internal.common_quantization.TwoLayerLinearModel.forward(self,x)
torch.testing._internal.common_quantization._make_conv_test_input(batch_size,in_channels_per_group,input_feature_map_size,out_channels_per_group,groups,kernel_size,X_scale,X_zero_point,W_scale,W_zero_point,use_bias,use_channelwise)
torch.testing._internal.common_quantization.accuracy(output,target,topk=(1,))
torch.testing._internal.common_quantization.convert_dynamic(module)
torch.testing._internal.common_quantization.ddp_cleanup()
torch.testing._internal.common_quantization.ddp_setup(rank,world_size)
torch.testing._internal.common_quantization.get_script_module(model,tracing,data)
torch.testing._internal.common_quantization.lengths_to_offsets(t,offset_type=np.int64,use_begin_offset=True)
torch.testing._internal.common_quantization.prepare_dynamic(model,qconfig_dict=None)
torch.testing._internal.common_quantization.run_ddp(rank,world_size,prepared)
torch.testing._internal.common_quantization.skipIfNoFBGEMM(fn)
torch.testing._internal.common_quantization.skipIfNoQNNPACK(fn)
torch.testing._internal.common_quantization.test_only_eval_fn(model,calib_data)
torch.testing._internal.common_quantization.test_only_train_fn(model,train_data,loss_fn=_default_loss_fn)
torch.testing._internal.common_quantization.train_one_epoch(model,criterion,optimizer,data_loader,device,ntrain_batches)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_methods_invocations.py----------------------------------------
A:torch.testing._internal.common_methods_invocations._NOTHING->object()
A:torch.testing._internal.common_methods_invocations.shape->str(tuple(arg.shape)).replace('(', '').replace(')', '')
A:torch.testing._internal.common_methods_invocations.self.op->_getattr_qual(torch, alias_name)
A:torch.testing._internal.common_methods_invocations.self.method_variant->getattr(torch.Tensor, alias_name, None)
A:torch.testing._internal.common_methods_invocations.self.inplace_variant->getattr(torch.Tensor, alias_name + '_', None)
A:torch.testing._internal.common_methods_invocations.obj->getattr(obj, path)
A:torch.testing._internal.common_methods_invocations.y->numpy.clip(intermediate, 0, None)
A:torch.testing._internal.common_methods_invocations.NumericsFilter->collections.namedtuple('NumericsFilter', ['condition', 'safe_val'])
A:torch.testing._internal.common_methods_invocations.self.dynamic_dtypes->any(map(lambda dtypes: isinstance(dtypes, opinfo_helper._dynamic_dispatch_dtypes), dtypes_args))
A:torch.testing._internal.common_methods_invocations.self.dtypes->set(dtypes)
A:torch.testing._internal.common_methods_invocations.self.operator_variant->getattr(operator, name, None)
A:torch.testing._internal.common_methods_invocations.self.sample_inputs_func->torch.no_grad()(sample_inputs_func)
A:torch.testing._internal.common_methods_invocations.self.aliases->tuple((AliasInfo(a) for a in aliases))
A:torch.testing._internal.common_methods_invocations.samples->list((SampleInput(make_arg((2, 2), requires_grad=requires_grad), args=(arg,)) for arg in exp_tuple))
A:torch.testing._internal.common_methods_invocations.conj_samples->self.conjugate_sample_inputs(device, dtype, requires_grad, **kwargs)
A:torch.testing._internal.common_methods_invocations.tensor->make_tensor(shape, device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.sample.input->make_tensor(psd_matrix.shape, device, dtype, requires_grad=requires_grad, low=None, high=None)
A:torch.testing._internal.common_methods_invocations.sample.input[0]->conjugate(sample.input[0])
A:torch.testing._internal.common_methods_invocations.samples_list->list(samples)
A:torch.testing._internal.common_methods_invocations.allowed_backward_dtypes->floating_and_complex_types_and(torch.bfloat16, torch.float16)
A:torch.testing._internal.common_methods_invocations.supported->self.supported_dtypes(device_type)
A:torch.testing._internal.common_methods_invocations.generate_args_kwargs->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).get('generate_args_kwargs', lambda *args, **kwargs: (yield (tuple(), {})))
A:torch.testing._internal.common_methods_invocations.t->make_tensor(s, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.sample_input_kwargs->dict(sample_input.kwargs, unbiased=unbiased)
A:torch.testing._internal.common_methods_invocations.dim->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).pop('dim', None)
A:torch.testing._internal.common_methods_invocations.inmask->torch._masked._input_mask(sample_input.input, *sample_input_args, **sample_input_kwargs)
A:torch.testing._internal.common_methods_invocations.orig_count->torch._masked.sum(inmask.new_ones(sample_input.input.shape, dtype=torch.int64), dim, keepdim=True, mask=inmask)
A:torch.testing._internal.common_methods_invocations.make_input->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.kw->dict(device=device, dtype=dtype)
A:torch.testing._internal.common_methods_invocations.make_arg->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.(lu, pivs, _)->torch.linalg.lu_factor_ex(x)
A:torch.testing._internal.common_methods_invocations.(p, l, u)->torch.lu_unpack(lu, pivs)
A:torch.testing._internal.common_methods_invocations.u_diag_abs->x.triu(0).nonzero().transpose(0, 1).diagonal(0, -2, -1).abs()
A:torch.testing._internal.common_methods_invocations.make_arg_fullrank->partial(make_fullrank, dtype=dtype, device=device, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.si1->SampleInput(make_tensor((S,), dtype=torch.float32, device=device), args=(0,))
A:torch.testing._internal.common_methods_invocations.si2->SampleInput(make_tensor((S, S, S), dtype=torch.float32, device=device), args=(0,))
A:torch.testing._internal.common_methods_invocations.input_t->make_arg(input_shape)
A:torch.testing._internal.common_methods_invocations.kwargs->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
A:torch.testing._internal.common_methods_invocations.products->product(rtols, atols, equal_nans)
A:torch.testing._internal.common_methods_invocations.a->make_tensor(s, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.b->make_tensor(s, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.c->make_arg((0, 1, 2, 3))
A:torch.testing._internal.common_methods_invocations.make_arg_without_requires_grad->partial(make_tensor, device=device, dtype=dtype, requires_grad=False)
A:torch.testing._internal.common_methods_invocations.running_mean->make_arg_without_requires_grad(channels, low=0)
A:torch.testing._internal.common_methods_invocations.running_var->make_arg_without_requires_grad(channels, low=0)
A:torch.testing._internal.common_methods_invocations.weight_tensor->torch.tensor(weight, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.new_args->copy.deepcopy(list(args))
A:torch.testing._internal.common_methods_invocations.(op_kwargs, lhs_make_tensor_kwargs, rhs_make_tensor_kwargs)->_resolve_binary_pwise_kwargs(op_info, op_kwargs=op_kwargs, lhs_make_tensor_kwargs=lhs_make_tensor_kwargs, rhs_make_tensor_kwargs=rhs_make_tensor_kwargs)
A:torch.testing._internal.common_methods_invocations.scalar->scalar.item().item()
A:torch.testing._internal.common_methods_invocations.lhs->make_tensor(lhs_shape, device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.rhs->make_tensor(rhs_shape, device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.alpha_val->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).get('alpha', 2 + 3j if dtype.is_complex else 0.6)
A:torch.testing._internal.common_methods_invocations.beta_val->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).get('beta', 1 + 2j if dtype.is_complex else 0.2)
A:torch.testing._internal.common_methods_invocations.args->tuple(args)
A:torch.testing._internal.common_methods_invocations.input1->SampleInput(make_tensor((S, M), device, dtype, low=None, high=None, requires_grad=requires_grad), args=(make_tensor((S,), device, dtype, low=None, high=None, requires_grad=requires_grad), make_tensor((M,), device, dtype, low=None, high=None, requires_grad=requires_grad)))
A:torch.testing._internal.common_methods_invocations.input2->SampleInput(make_tensor((), device, dtype, low=None, high=None, requires_grad=requires_grad), args=(make_tensor((S,), device, dtype, low=None, high=None, requires_grad=requires_grad), make_tensor((M,), device, dtype, low=None, high=None, requires_grad=requires_grad)), broadcasts_input=True)
A:torch.testing._internal.common_methods_invocations.input3->SampleInput(make_tensor((S, M), device, dtype, low=None, high=None, requires_grad=requires_grad), args=(make_tensor((S,), device, dtype, low=None, high=None, requires_grad=requires_grad), make_tensor((M,), device, dtype, low=None, high=None, requires_grad=requires_grad)), kwargs=dict(beta=beta, alpha=alpha))
A:torch.testing._internal.common_methods_invocations.input4->SampleInput(make_tensor((), device, dtype, low=None, high=None, requires_grad=requires_grad), args=(make_tensor((S,), device, dtype, low=None, high=None, requires_grad=requires_grad), make_tensor((M,), device, dtype, low=None, high=None, requires_grad=requires_grad)), kwargs=dict(beta=beta, alpha=alpha), broadcasts_input=True)
A:torch.testing._internal.common_methods_invocations.x->torch.ones(3, 3, dtype=torch.long, device=device, layout=torch.strided)
A:torch.testing._internal.common_methods_invocations.mean->inp.reshape((inp.shape[0], num_groups, -1)).mean(axis=-1, keepdims=True)
A:torch.testing._internal.common_methods_invocations.std->get_value_or_make_tensor(value_or_std_shape)
A:torch.testing._internal.common_methods_invocations.op_kwargs['M']->make_tensor((*batch, m, n), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.it->iter(iterable)
A:torch.testing._internal.common_methods_invocations.chunk->tuple(islice(it, size))
A:torch.testing._internal.common_methods_invocations.tensor1->make_tensor(size, device, dtype, low=-32, high=32, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.tensor2->make_tensor(size, device, dtype, low=0, high=5, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.sample_inputs->list((SampleInput(make_tensor(first_shape, device, dtype, requires_grad=requires_grad), args=(make_tensor(second_shape, device, dtype, requires_grad=requires_grad),), broadcasts_input=broadcasts_input) for (first_shape, second_shape, broadcasts_input) in cases))
A:torch.testing._internal.common_methods_invocations.more_inputs->list((SampleInput(torch.tensor(elements, device=device, dtype=dtype, requires_grad=requires_grad), args=(torch.tensor(elements, device=device, dtype=dtype, requires_grad=requires_grad),)) for elements in more_cases))
A:torch.testing._internal.common_methods_invocations.input->make(shape, low=None, high=0)
A:torch.testing._internal.common_methods_invocations.ii[dim]->slice(0, idx.size(dim) + 1)
A:torch.testing._internal.common_methods_invocations.src->make_arg(src_shape)
A:torch.testing._internal.common_methods_invocations.idx->make_long_input((3, 3), low=0, high=S)
A:torch.testing._internal.common_methods_invocations.bad_src->make_tensor((1, 1), device=device, dtype=torch.float32)
A:torch.testing._internal.common_methods_invocations.bad_idx->make_long_input((3, 3), low=0, high=S).to(torch.int32)
A:torch.testing._internal.common_methods_invocations.out->out.astype(np.float16).astype(np.float16)
A:torch.testing._internal.common_methods_invocations.dst->torch.zeros((3, 5), device=device, dtype=torch.float32)
A:torch.testing._internal.common_methods_invocations.input_tensor->create_tensor(batch_shape + features)
A:torch.testing._internal.common_methods_invocations.bins_tensor->make_arg((bin_ct + 1,))
A:torch.testing._internal.common_methods_invocations.max_val->int(input_tensor.max().item())
A:torch.testing._internal.common_methods_invocations.boundaries->make_arg((S,)).msort()
A:torch.testing._internal.common_methods_invocations.unsorted_tensor->make_arg(size, noncontiguous=noncontiguous)
A:torch.testing._internal.common_methods_invocations.sorter->sorter.reshape(num_splits, -1).reshape(num_splits, -1)
A:torch.testing._internal.common_methods_invocations.(boundary_tensor, sorter)->torch.sort(unsorted_tensor)
A:torch.testing._internal.common_methods_invocations.res->res.to(dtype).to(dtype)
A:torch.testing._internal.common_methods_invocations.dims->range(-3, 3)
A:torch.testing._internal.common_methods_invocations.inputs->list((SampleInput(make_tensor(input_tensor, device, dtype, low=None, high=None, requires_grad=requires_grad), args=args) for (input_tensor, args) in args))
A:torch.testing._internal.common_methods_invocations.keys->set(kwargs.keys())
A:torch.testing._internal.common_methods_invocations.params_generator->params_generator_type_dict[op_info.name]()
A:torch.testing._internal.common_methods_invocations.arg->arg.to(torch.cdouble).to(torch.cdouble)
A:torch.testing._internal.common_methods_invocations.weight->numpy.tile(np.expand_dims(weight, 1), [1] + list(inp.shape[2:]))
A:torch.testing._internal.common_methods_invocations.bias->numpy.tile(np.expand_dims(bias, 1), [1] + list(inp.shape[2:]))
A:torch.testing._internal.common_methods_invocations.create_tensor->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad, low=-2, high=2)
A:torch.testing._internal.common_methods_invocations.input_tensor1->create_tensor(batch_shape + [in_feat1])
A:torch.testing._internal.common_methods_invocations.input_tensor2->create_tensor(batch_shape + [in_feat2])
A:torch.testing._internal.common_methods_invocations.dim_size->create_tensor(batch_shape + features).size(dim)
A:torch.testing._internal.common_methods_invocations.arg_a->make_tensor((S,), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.arg_b->make_tensor((M,), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.make_permutation->partial(torch.randperm, device=device, dtype=torch.int64)
A:torch.testing._internal.common_methods_invocations.make_idx->partial(make_tensor, low=0, dtype=torch.int64, device=device, requires_grad=False)
A:torch.testing._internal.common_methods_invocations.index->torch.zeros(*shape, dtype=torch.long, device=device)
A:torch.testing._internal.common_methods_invocations.y_tensor->make_tensor(y_shape, device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.x_tensor->make_tensor(x_shape, device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.cases->product(shapes, kernel_sizes, dilations, paddings, strides)
A:torch.testing._internal.common_methods_invocations.make_inp->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.(u, s, vh)->torch.linalg.svd(A, full_matrices=False)
A:torch.testing._internal.common_methods_invocations.det->make_tensor((2, 3), device, dtype, requires_grad=requires_grad).det()
A:torch.testing._internal.common_methods_invocations.cond->((det < 0) ^ (sign < 0)).nonzero()
A:torch.testing._internal.common_methods_invocations.d->make_tensor((2, 3), device, dtype, requires_grad=requires_grad).diagonal(0, -2, -1)
A:torch.testing._internal.common_methods_invocations.nd_tensor->partial(make_tensor, (S, S + 1, S + 2), device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.oned_tensor->partial(make_tensor, (31,), device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.SpectralFuncType->Enum('SpectralFuncType', ('OneD', 'TwoD', 'ND'))
A:torch.testing._internal.common_methods_invocations.window->make_tensor(10, low=0.5, high=2.0, dtype=dtype, device=device, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.op->getattr(torch, op_name, None)
A:torch.testing._internal.common_methods_invocations.inplace_op->getattr(torch, inplace_op_name, None)
A:torch.testing._internal.common_methods_invocations.ref->getattr(torch, name, None)
A:torch.testing._internal.common_methods_invocations.ref_inplace->getattr(torch.Tensor, name + '_', None)
A:torch.testing._internal.common_methods_invocations.(foreach_method, foreach_method_inplace, torch_ref_method, torch_ref_inplace)->get_foreach_method_names(name)
A:torch.testing._internal.common_methods_invocations.device->torch.device(device)
A:torch.testing._internal.common_methods_invocations.reflectors->make_input((*batch, m, n))
A:torch.testing._internal.common_methods_invocations.tau->make_input((*batch, min(m, n)))
A:torch.testing._internal.common_methods_invocations.other->make_tensor(other_shape, device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.o->clone_sample(o)
A:torch.testing._internal.common_methods_invocations.make_a->partial(make_fn, dtype=dtype, device=device)
A:torch.testing._internal.common_methods_invocations.make_b->partial(make_tensor, dtype=dtype, device=device)
A:torch.testing._internal.common_methods_invocations.B->make_tensor((1, 3), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.(lu, pivs)->make_tensor(s, device=device, dtype=dtype, requires_grad=requires_grad).lu()
A:torch.testing._internal.common_methods_invocations.lu->lu.contiguous().contiguous()
A:torch.testing._internal.common_methods_invocations.lu_->lu.contiguous().contiguous().clone()
A:torch.testing._internal.common_methods_invocations.b_->make_tensor(s, device=device, dtype=dtype, requires_grad=requires_grad).clone()
A:torch.testing._internal.common_methods_invocations.(lu_data, pivots)->torch.linalg.lu_factor(lu_sample.input)
A:torch.testing._internal.common_methods_invocations.tensor_nd->partial(make_tensor, (S, S, S), device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.tensor_1d->partial(make_tensor, (S,), device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.fweights->make_tensor((num_observations,), device, torch.int, low=1, high=10)
A:torch.testing._internal.common_methods_invocations.aweights->make_tensor((num_observations,), device, torch.float, low=0, high=1, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.(U, _, _)->uniformize(usv)
A:torch.testing._internal.common_methods_invocations.(_, S, Vh)->uniformize(usv)
A:torch.testing._internal.common_methods_invocations.(U, S, Vh)->uniformize(usv)
A:torch.testing._internal.common_methods_invocations.si->SampleInput(t, args=(5,), kwargs={'out': (t, indices)})
A:torch.testing._internal.common_methods_invocations.more_samples->list((SampleInput((make_arg(shape, high=high, low=low) + additive).requires_grad_(b_grad), args=exp) for (shape, low, high, additive, b_grad, exp) in tensor_scalar_inputs))
A:torch.testing._internal.common_methods_invocations.eigvecs->make_tensor((S, S), device=device, dtype=dtype, low=None, high=None)
A:torch.testing._internal.common_methods_invocations.eigvals->make_tensor((S,), device=device, dtype=dtype, low=None, high=None)
A:torch.testing._internal.common_methods_invocations.A->make_tensor((2, 3), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.C->make_tensor((1, 2, 3), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.D->make_tensor((1, 3, 4), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.E->make_tensor((4, 4), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.H->make_tensor((3, 3), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.I->make_tensor((1, 3, 1), device, dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.lb->make_tensor((S, M, S), device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.ub->make_tensor((S, M, S), device, dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.empty_tensor->make_tensor((), device=device, dtype=dtype, low=None, high=None, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.sample0->SampleInput(make_tensor((S, 3), device=device, dtype=dtype, requires_grad=requires_grad), args=(make_tensor((S, 3), device=device, dtype=dtype, requires_grad=requires_grad),))
A:torch.testing._internal.common_methods_invocations.sample1->SampleInput(make_tensor((S, 3, S), device=device, dtype=dtype, requires_grad=requires_grad), args=(make_tensor((S, 3, S), device=device, dtype=dtype, requires_grad=requires_grad),), kwargs={'dim': 1})
A:torch.testing._internal.common_methods_invocations.sample2->SampleInput(make_tensor((S, 3), device=device, dtype=dtype, requires_grad=requires_grad), args=(make_tensor((S, 3), device=device, dtype=dtype, requires_grad=requires_grad),), kwargs={'dim': -1})
A:torch.testing._internal.common_methods_invocations.result->torch.randn(shape).gt(0)
A:torch.testing._internal.common_methods_invocations.zero->torch.tensor(0, dtype=torch.long, device=device)
A:torch.testing._internal.common_methods_invocations.vec_sample->SampleInput(make_tensor((M,), device, dtype, low=None, high=None, requires_grad=requires_grad))
A:torch.testing._internal.common_methods_invocations.input_->make_arg(input_shape)
A:torch.testing._internal.common_methods_invocations.src_shape->make_arg(input_shape).diagonal(*arg_tuple).size()
A:torch.testing._internal.common_methods_invocations.target->make_tensor(s, device=device, dtype=dtype, requires_grad=requires_grad).to(dtype=dtype).detach_().requires_grad_(requires_grad)
A:torch.testing._internal.common_methods_invocations.SCALAR->torch.Size([])
A:torch.testing._internal.common_methods_invocations.VECTOR->torch.Size([3])
A:torch.testing._internal.common_methods_invocations.(input, args)->make_inputs([make_tensor(shape, device, dtype, requires_grad=requires_grad) for shape in shapes])
A:torch.testing._internal.common_methods_invocations.min_val->compute_min_val(n)
A:torch.testing._internal.common_methods_invocations.largesample->SampleInput(large_1d_unique(dtype, device))
A:torch.testing._internal.common_methods_invocations.sample->SampleInput(input, args=(other,))
A:torch.testing._internal.common_methods_invocations.max_index->numpy.random.randint(1, shape[d] * 2)
A:torch.testing._internal.common_methods_invocations.inp->make_input().reshape(*shape_lhs, *shape_rhs).detach()
A:torch.testing._internal.common_methods_invocations.make_tensor_partial->partial(make_tensor, dtype=dtype, device=device, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.mask_t->make_tensor(shape, dtype=torch.bool, device=device, requires_grad=False)
A:torch.testing._internal.common_methods_invocations.zeros->torch.zeros(shape, dtype=dtype, device=device, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.mixed->make_arg(shape).requires_grad_(False)
A:torch.testing._internal.common_methods_invocations.indices->torch.empty((), device=device, dtype=torch.long)
A:torch.testing._internal.common_methods_invocations.offsets->torch.tensor([0, 3], device=device, dtype=torch.long)
A:torch.testing._internal.common_methods_invocations.per_sample_weights->make_per_sample_weight(generate_per_sample_weight, idx)
A:torch.testing._internal.common_methods_invocations.weights->make_input((S, S))
A:torch.testing._internal.common_methods_invocations.offsets_->torch.tensor([0, 3, 6], device=device, dtype=torch.long)
A:torch.testing._internal.common_methods_invocations._make_tensor->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.log_probs->make_log_probs((input_length, batch, num_char))
A:torch.testing._internal.common_methods_invocations.targets->torch.randint(1, num_char, (batch, target_length), dtype=torch.long, device=device)
A:torch.testing._internal.common_methods_invocations.input_lengths->torch.full((batch,), input_length, dtype=torch.long, device=device)
A:torch.testing._internal.common_methods_invocations.target_lengths->torch.randint(10, target_length, (batch,), dtype=torch.long, device=device)
A:torch.testing._internal.common_methods_invocations.make_weight->partial(make_tensor, shape=(num_classes,), device=device, dtype=dtype, requires_grad=False)
A:torch.testing._internal.common_methods_invocations.mask->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).pop('mask')
A:torch.testing._internal.common_methods_invocations.make_var->partial(make_tensor, low=0.1, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.d['margin']->random.uniform(-9, 9)
A:torch.testing._internal.common_methods_invocations.d['delta']->random.uniform(0.001, 9)
A:torch.testing._internal.common_methods_invocations.i1->_make_tensor(s)
A:torch.testing._internal.common_methods_invocations.i2->_make_tensor(s)
A:torch.testing._internal.common_methods_invocations.t1->_make_tensor(s, low=0)
A:torch.testing._internal.common_methods_invocations.t2->_make_tensor(s, low=0)
A:torch.testing._internal.common_methods_invocations.make->partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)
A:torch.testing._internal.common_methods_invocations.close->(t + atol).detach().requires_grad_(requires_grad)
A:torch.testing._internal.common_methods_invocations.close_sample->SampleInput(t, args=(close,), kwargs=dict(rtol=rtol, atol=atol))
A:torch.testing._internal.common_methods_invocations.r_sample->SampleInput(a, args=(b,), kwargs=dict(rtol=rtol, atol=atol))
A:torch.testing._internal.common_methods_invocations.out[mask]->complex(0, 0)
A:torch.testing._internal.common_methods_invocations.output->torch.where(output_mask, output, output.new_zeros([]))
A:torch.testing._internal.common_methods_invocations.num_classes->int(np.amax(a) + 1)
A:torch.testing._internal.common_methods_invocations.one_hot->numpy.zeros((a.size, num_classes), dtype=a.dtype)
A:torch.testing._internal.common_methods_invocations.feature_size->numpy.prod(normalized_shape)
A:torch.testing._internal.common_methods_invocations.inp_view->make_input().reshape(*shape_lhs, *shape_rhs).detach().reshape((inp.shape[0], num_groups, -1))
A:torch.testing._internal.common_methods_invocations.var->conjugate(torch.randn((), dtype=dtype, device=device))
A:torch.testing._internal.common_methods_invocations.Y->Y.reshape(inp.shape).reshape(inp.shape)
A:torch.testing._internal.common_methods_invocations.ret->numpy.searchsorted(sorted_sequence.flatten(), boundary.flatten(), side=side, sorter=sorter)
A:torch.testing._internal.common_methods_invocations.num_splits->numpy.prod(sorted_sequence.shape[:-1])
A:torch.testing._internal.common_methods_invocations.splits->range(0, num_splits)
A:torch.testing._internal.common_methods_invocations.output_mask->torch._masked._output_mask(op, input, *args, **kwargs)
A:torch.testing._internal.common_methods_invocations.keepdim->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).pop('keepdim', False)
A:torch.testing._internal.common_methods_invocations.kwargs['where']->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).pop('mask').cpu().numpy()
A:torch.testing._internal.common_methods_invocations.identity->identity.cpu().cpu()
A:torch.testing._internal.common_methods_invocations.kwargs['initial']->identity.cpu().cpu().numpy()
A:torch.testing._internal.common_methods_invocations.unbiased->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).pop('unbiased')
A:torch.testing._internal.common_methods_invocations.kwargs['ddof']->dict(sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim).pop('correction')
A:torch.testing._internal.common_methods_invocations.g->reference_reduction_numpy(f)
A:torch.testing._internal.common_methods_invocations.numel->torch.tensor(t.shape)[kwargs.get('dim')].prod()
A:torch.testing._internal.common_methods_invocations.l->torch.ones(3, 3, dtype=torch.long, device=device, layout=torch.strided).tril(0).nonzero().transpose(0, 1)
A:torch.testing._internal.common_methods_invocations.u->torch.ones(3, 3, dtype=torch.long, device=device, layout=torch.strided).triu(0).nonzero().transpose(0, 1)
A:torch.testing._internal.common_methods_invocations.non_differentiable->collections.namedtuple('non_differentiable', ['tensor'])
A:torch.testing._internal.common_methods_invocations.v->conjugate(maybe_non_contig(arg)).detach().to(device=device).clone()
A:torch.testing._internal.common_methods_invocations.args_out->tuple((map_arg(arg) for arg in call_args))
torch.testing._internal.common_methods_invocations.AliasInfo(self,alias_name)
torch.testing._internal.common_methods_invocations.AliasInfo.__init__(self,alias_name)
torch.testing._internal.common_methods_invocations.BinaryUfuncInfo(self,name,*,lhs_make_tensor_kwargs=None,rhs_make_tensor_kwargs=None,promotes_int_to_float=False,always_returns_bool=False,**kwargs)
torch.testing._internal.common_methods_invocations.BinaryUfuncInfo.__init__(self,name,*,lhs_make_tensor_kwargs=None,rhs_make_tensor_kwargs=None,promotes_int_to_float=False,always_returns_bool=False,**kwargs)
torch.testing._internal.common_methods_invocations.DecorateInfo(self,decorators,cls_name=None,test_name=None,*,device_type=None,dtypes=None,active_if=True)
torch.testing._internal.common_methods_invocations.DecorateInfo.__init__(self,decorators,cls_name=None,test_name=None,*,device_type=None,dtypes=None,active_if=True)
torch.testing._internal.common_methods_invocations.DecorateInfo.is_active(self,cls_name,test_name,device_type,dtype)
torch.testing._internal.common_methods_invocations.ErrorInput(self,sample_input,*,error_type,error_regex)
torch.testing._internal.common_methods_invocations.ErrorInput.__init__(self,sample_input,*,error_type,error_regex)
torch.testing._internal.common_methods_invocations.ForeachFuncInfo(self,name,dtypes=floating_and_complex_types(),dtypesIfCUDA=floating_and_complex_types_and(torch.half),dtypesIfROCM=None,safe_casts_outputs=True,supports_alpha_param=False,sample_inputs_func=sample_inputs_foreach,**kwargs)
torch.testing._internal.common_methods_invocations.ForeachFuncInfo.__init__(self,name,dtypes=floating_and_complex_types(),dtypesIfCUDA=floating_and_complex_types_and(torch.half),dtypesIfROCM=None,safe_casts_outputs=True,supports_alpha_param=False,sample_inputs_func=sample_inputs_foreach,**kwargs)
torch.testing._internal.common_methods_invocations.MvlGammaInfo(self,variant_test_name,domain,skips,sample_kwargs)
torch.testing._internal.common_methods_invocations.MvlGammaInfo.__init__(self,variant_test_name,domain,skips,sample_kwargs)
torch.testing._internal.common_methods_invocations.OpInfo(self,name,*,ref=None,aliases=None,variant_test_name='',op=None,method_variant=_NOTHING,inplace_variant=_NOTHING,skips=tuple(),decorators=tuple(),sample_inputs_func=None,error_inputs_func=None,dtypes,dtypesIfCPU=None,dtypesIfCUDA=None,dtypesIfROCM=None,backward_dtypes=None,backward_dtypesIfCPU=None,backward_dtypesIfCUDA=None,backward_dtypesIfROCM=None,default_test_dtypes=None,supports_out=True,safe_casts_outputs=False,supports_autograd=True,supports_gradgrad=None,supports_fwgrad_bwgrad=False,supports_inplace_autograd=None,supports_forward_ad=False,gradcheck_wrapper=lambdaop,*args,**kwargs:op(*args,**kwargs),check_batched_grad=None,check_batched_gradgrad=None,check_batched_forward_grad=None,check_inplace_batched_forward_grad=None,gradcheck_nondet_tol=0.0,gradcheck_fast_mode=None,aten_name=None,assert_autodiffed=False,autodiff_nonfusible_nodes=None,autodiff_fusible_nodes=None,supports_sparse=False,supports_scripting=True,supports_sparse_csr=False,test_conjugated_samples=True,test_neg_view=True,assert_jit_shape_analysis=False)
torch.testing._internal.common_methods_invocations.OpInfo.__init__(self,name,*,ref=None,aliases=None,variant_test_name='',op=None,method_variant=_NOTHING,inplace_variant=_NOTHING,skips=tuple(),decorators=tuple(),sample_inputs_func=None,error_inputs_func=None,dtypes,dtypesIfCPU=None,dtypesIfCUDA=None,dtypesIfROCM=None,backward_dtypes=None,backward_dtypesIfCPU=None,backward_dtypesIfCUDA=None,backward_dtypesIfROCM=None,default_test_dtypes=None,supports_out=True,safe_casts_outputs=False,supports_autograd=True,supports_gradgrad=None,supports_fwgrad_bwgrad=False,supports_inplace_autograd=None,supports_forward_ad=False,gradcheck_wrapper=lambdaop,*args,**kwargs:op(*args,**kwargs),check_batched_grad=None,check_batched_gradgrad=None,check_batched_forward_grad=None,check_inplace_batched_forward_grad=None,gradcheck_nondet_tol=0.0,gradcheck_fast_mode=None,aten_name=None,assert_autodiffed=False,autodiff_nonfusible_nodes=None,autodiff_fusible_nodes=None,supports_sparse=False,supports_scripting=True,supports_sparse_csr=False,test_conjugated_samples=True,test_neg_view=True,assert_jit_shape_analysis=False)
torch.testing._internal.common_methods_invocations.OpInfo.conjugate_sample_inputs(self,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.OpInfo.default_test_dtypes(self,device_type)
torch.testing._internal.common_methods_invocations.OpInfo.error_inputs(self,device,**kwargs)
torch.testing._internal.common_methods_invocations.OpInfo.formatted_name(self)
torch.testing._internal.common_methods_invocations.OpInfo.get_decorators(self,test_class,test_name,device,dtype)
torch.testing._internal.common_methods_invocations.OpInfo.get_inplace(self)
torch.testing._internal.common_methods_invocations.OpInfo.get_method(self)
torch.testing._internal.common_methods_invocations.OpInfo.get_op(self)
torch.testing._internal.common_methods_invocations.OpInfo.get_operator_variant(self)
torch.testing._internal.common_methods_invocations.OpInfo.sample_inputs(self,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.OpInfo.supported_backward_dtypes(self,device_type)
torch.testing._internal.common_methods_invocations.OpInfo.supported_dtypes(self,device_type)
torch.testing._internal.common_methods_invocations.OpInfo.supports_complex_autograd(self,device_type)
torch.testing._internal.common_methods_invocations.OpInfo.supports_dtype(self,dtype,device_type)
torch.testing._internal.common_methods_invocations.ReductionOpInfo(self,name,*,identity:Optional[Any]=None,nan_policy:Optional[str]=None,supports_multiple_dims:bool=True,promotes_int_to_float:bool=False,promotes_int_to_int64:bool=False,result_dtype:Optional[torch.dtype]=None,generate_args_kwargs:Callable=lambdat,dim=None,keepdim=False:(yield(tuple(),{})),**kwargs)
torch.testing._internal.common_methods_invocations.ReductionOpInfo.__init__(self,name,*,identity:Optional[Any]=None,nan_policy:Optional[str]=None,supports_multiple_dims:bool=True,promotes_int_to_float:bool=False,promotes_int_to_int64:bool=False,result_dtype:Optional[torch.dtype]=None,generate_args_kwargs:Callable=lambdat,dim=None,keepdim=False:(yield(tuple(),{})),**kwargs)
torch.testing._internal.common_methods_invocations.SampleInput(self,input,*,args=tuple(),kwargs=None,output_process_fn_grad=lambdax:x,broadcasts_input=False,name='')
torch.testing._internal.common_methods_invocations.SampleInput.__init__(self,input,*,args=tuple(),kwargs=None,output_process_fn_grad=lambdax:x,broadcasts_input=False,name='')
torch.testing._internal.common_methods_invocations.SampleInput.__repr__(self)
torch.testing._internal.common_methods_invocations.SampleInput._repr_helper(self,formatter)
torch.testing._internal.common_methods_invocations.SampleInput.noncontiguous(self)
torch.testing._internal.common_methods_invocations.SampleInput.numpy(self)
torch.testing._internal.common_methods_invocations.SampleInput.summary(self)
torch.testing._internal.common_methods_invocations.SampleInput.transform(self,f)
torch.testing._internal.common_methods_invocations.ShapeFuncInfo(self,name,*,ref,dtypes=floating_types(),dtypesIfCUDA=None,dtypesIfROCM=None,sample_inputs_func=None,**kwargs)
torch.testing._internal.common_methods_invocations.ShapeFuncInfo.__init__(self,name,*,ref,dtypes=floating_types(),dtypesIfCUDA=None,dtypesIfROCM=None,sample_inputs_func=None,**kwargs)
torch.testing._internal.common_methods_invocations.SpectralFuncInfo(self,name,*,ref=None,dtypes=floating_and_complex_types(),ndimensional:SpectralFuncType,sample_inputs_func=sample_inputs_spectral_ops,decorators=None,**kwargs)
torch.testing._internal.common_methods_invocations.SpectralFuncInfo.__init__(self,name,*,ref=None,dtypes=floating_and_complex_types(),ndimensional:SpectralFuncType,sample_inputs_func=sample_inputs_spectral_ops,decorators=None,**kwargs)
torch.testing._internal.common_methods_invocations.UnaryUfuncInfo(self,name,*,ref,dtypes=floating_types(),dtypesIfCUDA=None,dtypesIfROCM=None,default_test_dtypes=(torch.uint8,torch.long,torch.half,torch.bfloat16,torch.float32,torch.cfloat),domain=(None,None),handles_large_floats=True,handles_extremals=True,handles_complex_extremals=True,supports_complex_to_float=False,sample_inputs_func=sample_inputs_unary,sample_kwargs=lambdadevice,dtype,input:({},{}),supports_sparse=False,reference_numerics_filter=None,**kwargs)
torch.testing._internal.common_methods_invocations.UnaryUfuncInfo.__init__(self,name,*,ref,dtypes=floating_types(),dtypesIfCUDA=None,dtypesIfROCM=None,default_test_dtypes=(torch.uint8,torch.long,torch.half,torch.bfloat16,torch.float32,torch.cfloat),domain=(None,None),handles_large_floats=True,handles_extremals=True,handles_complex_extremals=True,supports_complex_to_float=False,sample_inputs_func=sample_inputs_unary,sample_kwargs=lambdadevice,dtype,input:({},{}),supports_sparse=False,reference_numerics_filter=None,**kwargs)
torch.testing._internal.common_methods_invocations._TestParamsMaxPool1d(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPool1d.__init__(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPool2d(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPool2d.__init__(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPool3d(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPool3d.__init__(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPoolBase(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPoolBase.__init__(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPoolBase._gen_kwargs(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPoolBase._gen_shape(self)
torch.testing._internal.common_methods_invocations._TestParamsMaxPoolBase.gen_input_params(self)
torch.testing._internal.common_methods_invocations._compare_large_trilu_indices(self,row,col,offset=0,dtype=torch.long,device='cpu')
torch.testing._internal.common_methods_invocations._compare_trilu_indices(self,row,col,offset=0,dtype=torch.long,device='cpu')
torch.testing._internal.common_methods_invocations._fill_indices(idx,dim,dim_size,elems_per_row,m,n,o)
torch.testing._internal.common_methods_invocations._generate_correlation_inputs(device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations._generate_masked_op_mask(input_shape,device,**kwargs)
torch.testing._internal.common_methods_invocations._generate_nan_reduction_inputs(device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations._generate_reduction_inputs(device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations._generate_reduction_kwargs(ndim,supports_multiple_dims=True)
torch.testing._internal.common_methods_invocations._generate_sample_inputs_nn_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations._generate_sample_shape_reduction()
torch.testing._internal.common_methods_invocations._getattr_qual(obj,name,default=_NOTHING)
torch.testing._internal.common_methods_invocations._resolve_binary_pwise_kwargs(op_info,*,op_kwargs=None,lhs_make_tensor_kwargs=None,rhs_make_tensor_kwargs=None)
torch.testing._internal.common_methods_invocations.bernoulli_scalar()
torch.testing._internal.common_methods_invocations.chunk_iter(iterable,size)
torch.testing._internal.common_methods_invocations.clone_sample(sample,**kwargs)
torch.testing._internal.common_methods_invocations.close_to_int(x,eps=0.1)
torch.testing._internal.common_methods_invocations.create_input(call_args,requires_grad=True,non_contiguous=False,call_kwargs=None,dtype=torch.double,device=None)
torch.testing._internal.common_methods_invocations.dont_convert(tuple)
torch.testing._internal.common_methods_invocations.error_inputs_dsplit(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.error_inputs_gather(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.error_inputs_hsplit(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.error_inputs_kthvalue(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.error_inputs_neg(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.error_inputs_scatter_and_scatter_add(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.error_inputs_vsplit(op_info,device,**kwargs)
torch.testing._internal.common_methods_invocations.gather_variable(shape,index_dim,max_indices,duplicate=False,device=torch.device('cpu'))
torch.testing._internal.common_methods_invocations.generate_std_var_kwargs(t:torch.Tensor,**kwargs)
torch.testing._internal.common_methods_invocations.get_foreach_method_names(name)
torch.testing._internal.common_methods_invocations.get_independent_tensor(tensor)
torch.testing._internal.common_methods_invocations.gradcheck_wrapper_hermitian_input(op,input,*args,**kwargs)
torch.testing._internal.common_methods_invocations.gradcheck_wrapper_masked_operation(op,input,*args,**kwargs)
torch.testing._internal.common_methods_invocations.gradcheck_wrapper_triangular_input(op,*args,upper=False,idx=0,**kwargs)
torch.testing._internal.common_methods_invocations.index_variable(shape,max_indices,device=torch.device('cpu'))
torch.testing._internal.common_methods_invocations.mask_not_all_zeros(shape)
torch.testing._internal.common_methods_invocations.np_sinc_with_fp16_as_fp32(x)
torch.testing._internal.common_methods_invocations.np_unary_ufunc_integer_promotion_wrapper(fn)
torch.testing._internal.common_methods_invocations.ref_pairwise_distance(input1,input2)
torch.testing._internal.common_methods_invocations.reference_group_norm(inp:np.ndarray,num_groups:int,weight=None,bias=None,eps=1e-05)
torch.testing._internal.common_methods_invocations.reference_hardsigmoid(x)
torch.testing._internal.common_methods_invocations.reference_layer_norm(inp:np.ndarray,normalized_shape:Tuple[int],weight=None,bias=None,eps=1e-05)
torch.testing._internal.common_methods_invocations.reference_lgamma(x)
torch.testing._internal.common_methods_invocations.reference_logsigmoid(x)
torch.testing._internal.common_methods_invocations.reference_mse_loss(input,target,reduction='mean')
torch.testing._internal.common_methods_invocations.reference_mvlgamma(x,d)
torch.testing._internal.common_methods_invocations.reference_one_hot(a:np.ndarray,num_classes:int=-1)->np.ndarray
torch.testing._internal.common_methods_invocations.reference_polygamma(x,n)
torch.testing._internal.common_methods_invocations.reference_reduction_numpy(f,supports_keepdims=True)
torch.testing._internal.common_methods_invocations.reference_searchsorted(sorted_sequence,boundary,out_int32=False,right=False,side='left',sorter=None)
torch.testing._internal.common_methods_invocations.reference_sgn(x)
torch.testing._internal.common_methods_invocations.reference_sigmoid(x)
torch.testing._internal.common_methods_invocations.reference_sign(x)
torch.testing._internal.common_methods_invocations.reference_softplus(input,beta=1,threshold=20)
torch.testing._internal.common_methods_invocations.reference_std_var(f)
torch.testing._internal.common_methods_invocations.run_additional_tri_tests(self,device)
torch.testing._internal.common_methods_invocations.sample_cumulative_trapezoid(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_T(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adaptive_avg_pool1d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adaptive_avg_pool2d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adaptive_avg_pool3d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adaptive_max_pool1d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adaptive_max_pool2d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adaptive_max_pool3d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_add_sub(op_info,device,dtype,requires_grad,python_scalars=False,alpha=1,op_kwargs=None,lhs_make_tensor_kwargs=None,rhs_make_tensor_kwargs=None,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_addbmm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_addcmul_addcdiv(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_addmm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_addmv(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_addr(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_adjoint(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_allclose(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_aminmax(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_argsort(*args,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_argwhere(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_as_strided(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_atan2(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_atleast1d2d3d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_avgpool1d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_avgpool2d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_avgpool3d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_baddbmm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_batch_norm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_bernoulli(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_bilinear(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_binary_pwise(op_info,device,dtype,requires_grad,*,python_scalars=False,op_kwargs=None,lhs_make_tensor_kwargs=None,rhs_make_tensor_kwargs=None,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_bincount(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_bitwise_shift(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_block_diag(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_bmm(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_broadcast_tensors(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_broadcast_to(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_bucketize(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cartesian_prod(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cat_concat(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cdist(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cholesky_solve(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_chunk(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_clamp(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_clamp_scalar(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_clone(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_column_stack(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_combinations(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_comparison_ops(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_complex(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_contiguous(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conv1d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conv2d(op_info,device,dtype,requires_grad,jit_fail_sample=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conv_transpose1d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conv_transpose2d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conv_transpose3d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conversion(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_conversion_channels_last(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_copysign(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_corrcoef(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cosine_embedding_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cosine_similarity(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cov(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cross(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cross_entropy(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_ctc_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cumprod(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_cumulative_ops(op_info,device,dtype,requires_grad,supports_dtype_kwargs=True,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_diag(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_diagflat(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_diagonal_diag_embed(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_diagonal_scatter(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_diff(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_dist(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_dot_vdot(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_dropout(op_info,device,dtype,requires_grad,*,train=None,valid_input_dim=None,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_dsplit(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_eig(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_einsum(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_embedding(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_embedding_bag(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_entr(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_expand(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_expand_as(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_fftshift(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_fill_(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_flatten(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_flip(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_fliplr_flipud(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_fmod_remainder(op_info,device,dtype,requires_grad,*,autodiffed=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_foreach(self,device,dtype,N,*,noncontiguous=False,same_size=False)
torch.testing._internal.common_methods_invocations.sample_inputs_fractional_max_pool2d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_fractional_max_pool3d(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_full_like(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_gather(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_gaussian_nll_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_gelu(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_getitem(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_glu(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_gradient(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_grid_sample(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_group_norm(opinfo,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_hardswish(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_hinge_embedding_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_histc(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_histogram(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_histogramdd(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_householder_product(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_hsplit(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_hstack_dstack_vstack(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_huber_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_hypot(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_i0_i1(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_igamma_igammac(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_index(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_index_put(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_inner(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_instance_norm(opinfo,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_interpolate(mode,self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_isclose(op_info,device,dtype,requires_grad,python_scalars=False,op_kwargs=None,lhs_make_tensor_kwargs=None,rhs_make_tensor_kwargs=None,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_isin(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_istft(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_kl_div(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_kron(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_kthvalue(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_layer_norm(opinfo,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_leaky_relu(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_legacy_solve(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_lerp(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_like_fns(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_cholesky(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_cholesky_inverse(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_cond(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_det(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_det_singular(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_eig(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_eigh(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_invertible(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_lstsq(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_lu_factor(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_matrix_norm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_matrix_power(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_multi_dot(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_norm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_pinv(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_pinv_hermitian(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_pinv_singular(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_qr_geqrf(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_slogdet(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_solve(op_info,device,dtype,requires_grad=False,vector_rhs_allowed=True,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_solve_triangular(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_svdvals(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linalg_vector_norm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_linear(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_local_response_norm(opinfo,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_logcumsumexp(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_logdet(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_logit(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_logsumexp(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_lu(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_lu_solve(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_lu_unpack(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_fill(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_norm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_normalize(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_reduction(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_scatter(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_select(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_softmax(op_info,device,dtype,requires_grad,with_dtype=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_masked_var(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_matmul(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_matrix_exp(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_max_min_binary(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_max_min_reduction_no_dim(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_max_min_reduction_with_dim(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_max_pool(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_meshgrid(op_info:OpInfo,device:torch.device,dtype:torch.dtype,requires_grad:bool,*,variant:str)->List[SampleInput]
torch.testing._internal.common_methods_invocations.sample_inputs_mm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_mode(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_mse_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_msort(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_multinomial(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_mv(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_mvlgamma(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nan_reduction(supports_multiple_dims)
torch.testing._internal.common_methods_invocations.sample_inputs_narrow(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_new_fns(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_new_full(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nextafter(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nll_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nn_activation_relu(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nn_functional_prelu(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nn_pad(op_info,device,dtype,requires_grad,mode,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nn_unfold(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_nonzero(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_norm(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_norm_fro(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_norm_inf(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_norm_nuc(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_normal_common(self,device,dtype,requires_grad,cases,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_normal_tensor_first(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_normal_tensor_second(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_normalize(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_one_hot(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_ormqr(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_outer(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_pairwise_distance(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_pca_lowrank(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_permute(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_pixel_shuffle(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_pixel_unshuffle(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_poisson_nll_loss(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_polar(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_polygamma(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_pow(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_prod(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_put(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_randint_like(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_ravel(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_rbinops(op_info,device,dtype,requires_grad,supports_dtype_kwargs=True,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_reduction(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_reduction_count_nonzero(*args,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_reduction_quantile(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_renorm(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_repeat_interleave(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_resize_ops(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_roll(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_rot90(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_rsub(op_info,device,dtype,requires_grad,other_scalar,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_scatter(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_scatter_add(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_scatter_reduce(op_info,device,dtype,requires_grad)
torch.testing._internal.common_methods_invocations.sample_inputs_searchsorted(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_select(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_select_scatter(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_singular_matrix_factors(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_slice_scatter(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_softmax_variant(op_info,device,dtype,requires_grad,with_dtype=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_softplus(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_softshrink_hardshrink_hardtanh(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_sort(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_spectral_ops(self,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_split(op_info,device,dtype,requires_grad,*,list_args=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_split_with_sizes(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_squeeze(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_stack(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_std_var(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_stft(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_sum_to_size(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_svd(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_svd_lowrank(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_symeig(op_info,device,dtype,requires_grad=False,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_t(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_take(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_take_along_dim(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_tensor_split(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_tensordot(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_tensorinv(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_tensorsolve(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_threshold(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_to_sparse(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_topk(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_trace(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_transpose_swapdims(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_tril_triu(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_unary(op_info,device,dtype,requires_grad,op_kwargs=None,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_unfold(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_unique(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_unique_consecutive(*args,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_upsample(mode,self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_view_as_complex(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_view_as_real(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_view_as_reshape_as(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_view_reshape(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_vsplit(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_where(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_xlog1py(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_xlogy(self,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_zero_(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_inputs_zeta(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_kwargs_clamp_scalar(device,dtype,input)
torch.testing._internal.common_methods_invocations.sample_movedim_moveaxis(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_repeat_tile(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_trapezoid(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.sample_unsqueeze(op_info,device,dtype,requires_grad,**kwargs)
torch.testing._internal.common_methods_invocations.skips_mvlgamma(skip_redundant=False)
torch.testing._internal.common_methods_invocations.unpack_variables(args)
torch.testing._internal.common_methods_invocations.wrapper_set_seed(op,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/jit_utils.py----------------------------------------
A:torch.testing._internal.jit_utils.RUN_CUDA->torch.cuda.is_available()
A:torch.testing._internal.jit_utils.CUDA_VERSION->torch._C._cuda_getCompiledVersion()
A:torch.testing._internal.jit_utils.torch.jit._recursive.concrete_type_store->torch.jit._recursive.ConcreteTypeStore()
A:torch.testing._internal.jit_utils.execution_plans->list(graph_executor_state.execution_plans.values())
A:torch.testing._internal.jit_utils.num_plans->len(execution_plans)
A:torch.testing._internal.jit_utils.self.stringio->StringIO()
A:torch.testing._internal.jit_utils.se->str(e)
A:torch.testing._internal.jit_utils.archive->zipfile.ZipFile(buffer)
A:torch.testing._internal.jit_utils.files->list(filter(lambda x: x.startswith('archive/code/'), archive.namelist()))
A:torch.testing._internal.jit_utils.code_files_str->filter(lambda x: x.endswith('.py'), files)
A:torch.testing._internal.jit_utils.debug_files_str->filter(lambda f: f.endswith('.debug_pkl'), files)
A:torch.testing._internal.jit_utils.buffer->io.BytesIO()
A:torch.testing._internal.jit_utils.buffer_copy->io.BytesIO().getvalue()
A:torch.testing._internal.jit_utils.(code_files, debug_files)->extract_files(buffer)
A:torch.testing._internal.jit_utils.buffer2->io.BytesIO(buffer_copy)
A:torch.testing._internal.jit_utils.imported->torch.jit.load(buffer, map_location=map_location)
A:torch.testing._internal.jit_utils.saved_module_buffer_2->io.BytesIO()
A:torch.testing._internal.jit_utils.(code_files_2, debug_files_2)->extract_files(saved_module_buffer_2)
A:torch.testing._internal.jit_utils.f->tempfile.NamedTemporaryFile(delete=False)
A:torch.testing._internal.jit_utils.result->getattr(torch._C, '_jit_pass_' + name)(graph)
A:torch.testing._internal.jit_utils.strgraph->str(graph)
A:torch.testing._internal.jit_utils.out_nodes->nodes(graph)
A:torch.testing._internal.jit_utils.g->torch.onnx._optimize_trace(g, operator_export_type=OperatorExportTypes.ONNX)
A:torch.testing._internal.jit_utils.graph->trace.graph()
A:torch.testing._internal.jit_utils.frame->self.get_frame_vars(frames_up)
A:torch.testing._internal.jit_utils.cu->torch.jit.CompilationUnit(script, _frames_up=frames_up)
A:torch.testing._internal.jit_utils.string_frontend->getattr(cu, script.__name__)
A:torch.testing._internal.jit_utils.source->textwrap.dedent(inspect.getsource(script))
A:torch.testing._internal.jit_utils.ge->self.getExportImportCopy(ge)
A:torch.testing._internal.jit_utils.state->model.get_debug_state()
A:torch.testing._internal.jit_utils.plan->get_execution_plan(state)
A:torch.testing._internal.jit_utils.num_bailouts->get_execution_plan(state).code.num_bailouts()
A:torch.testing._internal.jit_utils.bailout_outputs->model(*inputs)
A:torch.testing._internal.jit_utils.extra_profile_runs->any((isinstance(x, torch.Tensor) and x.requires_grad for x in inputs))
A:torch.testing._internal.jit_utils.scripted_fn->torch.jit.script(script, _frames_up=1)
A:torch.testing._internal.jit_utils.recording_inputs->do_input_map(lambda t: Variable(t, requires_grad=True), reference_tensors)
A:torch.testing._internal.jit_utils.script_outputs->scripted_fn(*recording_inputs)
A:torch.testing._internal.jit_utils.opt_script_outputs->scripted_fn(*recording_inputs)
A:torch.testing._internal.jit_utils.python_outputs->python_fn(*inputs)
A:torch.testing._internal.jit_utils.flattened_recording_inputs->flatten_inputs(recording_inputs)
A:torch.testing._internal.jit_utils.outputs->func(*recording_inputs)
A:torch.testing._internal.jit_utils.outputs_ge->ge(*recording_inputs)
A:torch.testing._internal.jit_utils.grads->torch.autograd.grad(l1, flattened_recording_inputs, create_graph=True, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.grads_ge->torch.autograd.grad(l1_ge, flattened_recording_inputs, create_graph=True, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.l1->allSum(outputs)
A:torch.testing._internal.jit_utils.grads2->torch.autograd.grad(l2, flattened_recording_inputs, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.l1_ge->allSum(outputs_ge)
A:torch.testing._internal.jit_utils.grads2_ge->torch.autograd.grad(l2_ge, flattened_recording_inputs, allow_unused=allow_unused)
A:torch.testing._internal.jit_utils.sm->torch.jit.script(nn_module)
A:torch.testing._internal.jit_utils.eager_out->nn_module(*args)
A:torch.testing._internal.jit_utils.script_out->sm(*args)
A:torch.testing._internal.jit_utils.old->torch._C._debug_get_fusion_group_inlining()
A:torch.testing._internal.jit_utils.r->torch.autograd.grad(f, *args)
A:torch.testing._internal.jit_utils.script_path->os.path.join(tmp_dir, 'script.py')
A:torch.testing._internal.jit_utils.spec->importlib.util.spec_from_file_location(fn_name, script_path)
A:torch.testing._internal.jit_utils.module->importlib.util.module_from_spec(spec)
A:torch.testing._internal.jit_utils.fn->getattr(module, fn_name)
torch.testing._internal.jit_utils.JitTestCase(JitCommonTestCase)
torch.testing._internal.jit_utils.JitTestCase._compared_saved_loaded(self,m)
torch.testing._internal.jit_utils.JitTestCase._isHookExceptionOk(self,e)
torch.testing._internal.jit_utils.JitTestCase.assertAllFused(self,graph,except_for=())
torch.testing._internal.jit_utils.JitTestCase.assertExpectedGraph(self,trace,*args,**kwargs)
torch.testing._internal.jit_utils.JitTestCase.assertExpectedONNXGraph(self,g,*args,**kwargs)
torch.testing._internal.jit_utils.JitTestCase.assertGraphContains(self,graph,kind,consider_subgraphs=False)
torch.testing._internal.jit_utils.JitTestCase.assertGraphContainsExactly(self,graph,kind,num_kind_nodes,consider_subgraphs=False)
torch.testing._internal.jit_utils.JitTestCase.assertRaisesRegexWithHighlight(self,exception,regex,highlight)
torch.testing._internal.jit_utils.JitTestCase.capture_stderr(list)
torch.testing._internal.jit_utils.JitTestCase.capture_stderr.__enter__(self)
torch.testing._internal.jit_utils.JitTestCase.capture_stderr.__exit__(self,*args)
torch.testing._internal.jit_utils.JitTestCase.capture_stdout(list)
torch.testing._internal.jit_utils.JitTestCase.capture_stdout.__enter__(self)
torch.testing._internal.jit_utils.JitTestCase.capture_stdout.__exit__(self,*args)
torch.testing._internal.jit_utils.JitTestCase.checkBailouts(self,model,inputs,expected)
torch.testing._internal.jit_utils.JitTestCase.checkModule(self,nn_module,args)
torch.testing._internal.jit_utils.JitTestCase.checkScript(self,script,inputs,name='func',optimize=True,inputs_requires_grad=False,capture_output=False,frames_up=1,profiling=ProfilingMode.PROFILING,atol=None,rtol=None)
torch.testing._internal.jit_utils.JitTestCase.checkScriptRaisesRegex(self,script,inputs,exception,regex,name=None,outputs=None,capture_output=False,frames_up=1,profiling=ProfilingMode.PROFILING)
torch.testing._internal.jit_utils.JitTestCase.checkTrace(self,func,reference_tensors,input_tensors=None,drop=None,allow_unused=False,verbose=False,inputs_require_grads=True,check_tolerance=1e-05,export_import=True,_force_outplace=False)
torch.testing._internal.jit_utils.JitTestCase.clearHooks(self)
torch.testing._internal.jit_utils.JitTestCase.emitFunctionHook(self,func)
torch.testing._internal.jit_utils.JitTestCase.emitModuleHook(self,module)
torch.testing._internal.jit_utils.JitTestCase.getExportImportCopyWithPacking(self,m,also_test_file=True,map_location=None)
torch.testing._internal.jit_utils.JitTestCase.get_frame_vars(self,frames_up)
torch.testing._internal.jit_utils.JitTestCase.run_pass(self,name,trace)
torch.testing._internal.jit_utils.JitTestCase.setHooks(self)
torch.testing._internal.jit_utils.JitTestCase.setUp(self)
torch.testing._internal.jit_utils.JitTestCase.tearDown(self)
torch.testing._internal.jit_utils._AssertRaisesRegexWithHighlightContext(self,test_case,exception,regex,highlight)
torch.testing._internal.jit_utils._AssertRaisesRegexWithHighlightContext.__enter__(self)
torch.testing._internal.jit_utils._AssertRaisesRegexWithHighlightContext.__exit__(self,type,value,traceback)
torch.testing._internal.jit_utils._AssertRaisesRegexWithHighlightContext.__init__(self,test_case,exception,regex,highlight)
torch.testing._internal.jit_utils._get_py3_code(code,fn_name)
torch.testing._internal.jit_utils._inline_everything(fn)
torch.testing._internal.jit_utils._tmp_donotuse_dont_inline_everything(fn)
torch.testing._internal.jit_utils._trace(*args,**kwargs)
torch.testing._internal.jit_utils.attrs_with_prefix(module,prefix)
torch.testing._internal.jit_utils.clear_class_registry()
torch.testing._internal.jit_utils.disable_autodiff_subgraph_inlining(enabled=True)
torch.testing._internal.jit_utils.do_input_map(fn,input)
torch.testing._internal.jit_utils.enable_cpu_fuser(fn)
torch.testing._internal.jit_utils.enable_cpu_fuser_if(cond)
torch.testing._internal.jit_utils.execWrapper(code,glob,loc)
torch.testing._internal.jit_utils.get_execution_plan(graph_executor_state)
torch.testing._internal.jit_utils.get_forward(c)
torch.testing._internal.jit_utils.get_forward_graph(c)
torch.testing._internal.jit_utils.get_module_method(m,module,method)
torch.testing._internal.jit_utils.inline_everything_mode(should_inline)
torch.testing._internal.jit_utils.make_global(*args)
torch.testing._internal.jit_utils.set_fusion_group_inlining(inlining)
torch.testing._internal.jit_utils.warmup_backward(f,*args)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_device_type.py----------------------------------------
A:torch.testing._internal.common_device_type._tls->threading.local()
A:torch.testing._internal.common_device_type.default_dtypes->getattr(generic_test_class, name).dtypes.get('all')
A:torch.testing._internal.common_device_type.self.precision->self._get_precision_override(test, dtype)
A:torch.testing._internal.common_device_type.(self.precision, self.rel_tol)->self._get_tolerance_override(test, dtype)
A:torch.testing._internal.common_device_type.device_arg->cls.get_all_devices()
A:torch.testing._internal.common_device_type.result->test(self, **param_kwargs)
A:torch.testing._internal.common_device_type.self._stop_test_suite->self._should_stop_test_suite()
A:torch.testing._internal.common_device_type.dtypes->dtypes.intersection(self.allowed_dtypes).intersection(self.allowed_dtypes)
A:torch.testing._internal.common_device_type.parametrize_fn->compose_parametrize_fns(dtype_parametrize_fn, parametrize_fn)
A:torch.testing._internal.common_device_type.test_name->'{}{}{}{}'.format(name, test_suffix, device_suffix, _dtype_test_suffix(dtype_kwarg))
A:torch.testing._internal.common_device_type.primary_device_idx->int(cls.get_primary_device().split(':')[1])
A:torch.testing._internal.common_device_type.num_devices->torch.cuda.device_count()
A:torch.testing._internal.common_device_type.prim_device->cls.get_primary_device()
A:torch.testing._internal.common_device_type.t->torch.ones(1).cuda()
A:torch.testing._internal.common_device_type.cls.primary_device->'cuda:{0}'.format(torch.cuda.current_device())
A:torch.testing._internal.common_device_type.device_type_test_bases->filter(lambda x: x.device_type in only_for, device_type_test_bases)
A:torch.testing._internal.common_device_type._TORCH_TEST_DEVICES->os.environ.get('TORCH_TEST_DEVICES', None)
A:torch.testing._internal.common_device_type.mod->runpy.run_path(path, init_globals=globals())
A:torch.testing._internal.common_device_type.empty_class->type(empty_name, generic_test_class.__bases__, {})
A:torch.testing._internal.common_device_type.desired_device_type_test_bases->filter_desired_device_types(desired_device_type_test_bases, env_except_for, env_only_for)
A:torch.testing._internal.common_device_type.env_only_for->split_if_not_empty(os.getenv(PYTORCH_TESTING_DEVICE_ONLY_FOR_KEY, ''))
A:torch.testing._internal.common_device_type.env_except_for->split_if_not_empty(os.getenv(PYTORCH_TESTING_DEVICE_EXCEPT_FOR_KEY, ''))
A:torch.testing._internal.common_device_type.test->getattr(generic_test_class, name)
A:torch.testing._internal.common_device_type.sig->inspect.signature(device_type_test_class.instantiate_test)
A:torch.testing._internal.common_device_type.nontest->getattr(generic_test_class, name)
A:torch.testing._internal.common_device_type.test_wrapper->decorator(test_wrapper)
A:torch.testing._internal.common_device_type.reason->'cuDNN version {0} is available but {1} required'.format(self.cudnn_version, version)
A:torch.testing._internal.common_device_type.tol->namedtuple('tol', ['atol', 'rtol'])
A:torch.testing._internal.common_device_type.d->getattr(fn, 'dtypes', {})
A:torch.testing._internal.common_device_type.version->_get_torch_cuda_version()
A:torch.testing._internal.common_device_type.rocm_version->str(torch.version.hip)
A:torch.testing._internal.common_device_type.rocm_version_tuple->tuple((int(x) for x in rocm_version.split('.')))
torch.testing._internal.common_device_type.CPUTestBase(DeviceTypeTestBase)
torch.testing._internal.common_device_type.CPUTestBase._should_stop_test_suite(self)
torch.testing._internal.common_device_type.CUDATestBase(DeviceTypeTestBase)
torch.testing._internal.common_device_type.CUDATestBase.get_all_devices(cls)
torch.testing._internal.common_device_type.CUDATestBase.get_primary_device(cls)
torch.testing._internal.common_device_type.CUDATestBase.has_cudnn(self)
torch.testing._internal.common_device_type.CUDATestBase.setUpClass(cls)
torch.testing._internal.common_device_type.DeviceTypeTestBase(TestCase)
torch.testing._internal.common_device_type.DeviceTypeTestBase._apply_precision_override_for_test(self,test,param_kwargs)
torch.testing._internal.common_device_type.DeviceTypeTestBase._get_dtypes(cls,test)
torch.testing._internal.common_device_type.DeviceTypeTestBase._get_precision_override(self,test,dtype)
torch.testing._internal.common_device_type.DeviceTypeTestBase._get_tolerance_override(self,test,dtype)
torch.testing._internal.common_device_type.DeviceTypeTestBase.get_all_devices(cls)
torch.testing._internal.common_device_type.DeviceTypeTestBase.get_primary_device(cls)
torch.testing._internal.common_device_type.DeviceTypeTestBase.instantiate_test(cls,name,test,*,generic_cls=None)
torch.testing._internal.common_device_type.DeviceTypeTestBase.precision(self)
torch.testing._internal.common_device_type.DeviceTypeTestBase.precision(self,prec)
torch.testing._internal.common_device_type.DeviceTypeTestBase.rel_tol(self)
torch.testing._internal.common_device_type.DeviceTypeTestBase.rel_tol(self,prec)
torch.testing._internal.common_device_type.DeviceTypeTestBase.run(self,result=None)
torch.testing._internal.common_device_type.MetaTestBase(DeviceTypeTestBase)
torch.testing._internal.common_device_type.MetaTestBase._should_stop_test_suite(self)
torch.testing._internal.common_device_type.OpDTypes(Enum)
torch.testing._internal.common_device_type._dtype_test_suffix(dtypes)
torch.testing._internal.common_device_type._has_sufficient_memory(device,size)
torch.testing._internal.common_device_type._update_param_kwargs(param_kwargs,name,value)
torch.testing._internal.common_device_type.deviceCountAtLeast(self,num_required_devices)
torch.testing._internal.common_device_type.deviceCountAtLeast.__init__(self,num_required_devices)
torch.testing._internal.common_device_type.disableMkldnn(fn)
torch.testing._internal.common_device_type.disablecuDNN(fn)
torch.testing._internal.common_device_type.dtypes(self,*args,device_type='all')
torch.testing._internal.common_device_type.dtypes.__init__(self,*args,device_type='all')
torch.testing._internal.common_device_type.dtypesIfCPU(self,*args)
torch.testing._internal.common_device_type.dtypesIfCPU.__init__(self,*args)
torch.testing._internal.common_device_type.dtypesIfCUDA(self,*args)
torch.testing._internal.common_device_type.dtypesIfCUDA.__init__(self,*args)
torch.testing._internal.common_device_type.expectedAlertNondeterministic(self,caller_name,device_types=None)
torch.testing._internal.common_device_type.expectedAlertNondeterministic.__init__(self,caller_name,device_types=None)
torch.testing._internal.common_device_type.expectedFailure(self,device_type)
torch.testing._internal.common_device_type.expectedFailure.__init__(self,device_type)
torch.testing._internal.common_device_type.expectedFailureCUDA(fn)
torch.testing._internal.common_device_type.expectedFailureMeta(fn)
torch.testing._internal.common_device_type.expectedFailureXLA(fn)
torch.testing._internal.common_device_type.filter_desired_device_types(device_type_test_bases,except_for=None,only_for=None)
torch.testing._internal.common_device_type.get_device_type_test_bases()
torch.testing._internal.common_device_type.has_cusolver()
torch.testing._internal.common_device_type.instantiate_device_type_tests(generic_test_class,scope,except_for=None,only_for=None)
torch.testing._internal.common_device_type.largeTensorTest(size,device=None)
torch.testing._internal.common_device_type.onlyCPU(fn)
torch.testing._internal.common_device_type.onlyCUDA(fn)
torch.testing._internal.common_device_type.onlyNativeDeviceTypes(fn)
torch.testing._internal.common_device_type.onlyOn(self,device_type)
torch.testing._internal.common_device_type.onlyOn.__init__(self,device_type)
torch.testing._internal.common_device_type.ops(self,op_list,*,dtypes:Union[OpDTypes,Sequence[torch.dtype]]=OpDTypes.basic,allowed_dtypes:Optional[Sequence[torch.dtype]]=None)
torch.testing._internal.common_device_type.ops.__init__(self,op_list,*,dtypes:Union[OpDTypes,Sequence[torch.dtype]]=OpDTypes.basic,allowed_dtypes:Optional[Sequence[torch.dtype]]=None)
torch.testing._internal.common_device_type.ops._parametrize_test(self,test,generic_cls,device_cls)
torch.testing._internal.common_device_type.precisionOverride(self,d)
torch.testing._internal.common_device_type.precisionOverride.__init__(self,d)
torch.testing._internal.common_device_type.skipCPUIf(self,dep,reason)
torch.testing._internal.common_device_type.skipCPUIf.__init__(self,dep,reason)
torch.testing._internal.common_device_type.skipCPUIfNoFFT(fn)
torch.testing._internal.common_device_type.skipCPUIfNoLapack(fn)
torch.testing._internal.common_device_type.skipCPUIfNoMkl(fn)
torch.testing._internal.common_device_type.skipCPUIfNoMklSparse(fn)
torch.testing._internal.common_device_type.skipCPUIfNoMkldnn(fn)
torch.testing._internal.common_device_type.skipCUDAIf(self,dep,reason)
torch.testing._internal.common_device_type.skipCUDAIf.__init__(self,dep,reason)
torch.testing._internal.common_device_type.skipCUDAIfCudnnVersionLessThan(version=0)
torch.testing._internal.common_device_type.skipCUDAIfMiopen(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoCudnn(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoCusolver(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoCusparseGeneric(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoMagma(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoMagmaAndNoCusolver(fn)
torch.testing._internal.common_device_type.skipCUDAIfNoMiopen(fn)
torch.testing._internal.common_device_type.skipCUDAIfNotMiopenSuggestNHWC(fn)
torch.testing._internal.common_device_type.skipCUDAIfNotRocm(fn)
torch.testing._internal.common_device_type.skipCUDAIfRocm(fn)
torch.testing._internal.common_device_type.skipCUDAIfRocmVersionLessThan(version=None)
torch.testing._internal.common_device_type.skipCUDAVersionIn(versions:List[Tuple[int,int]]=None)
torch.testing._internal.common_device_type.skipIf(self,dep,reason,device_type=None)
torch.testing._internal.common_device_type.skipIf.__init__(self,dep,reason,device_type=None)
torch.testing._internal.common_device_type.skipMeta(fn)
torch.testing._internal.common_device_type.skipMetaIf(self,dep,reason)
torch.testing._internal.common_device_type.skipMetaIf.__init__(self,dep,reason)
torch.testing._internal.common_device_type.skipXLA(fn)
torch.testing._internal.common_device_type.skipXLAIf(self,dep,reason)
torch.testing._internal.common_device_type.skipXLAIf.__init__(self,dep,reason)
torch.testing._internal.common_device_type.toleranceOverride(self,d)
torch.testing._internal.common_device_type.toleranceOverride.__init__(self,d)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/common_quantized.py----------------------------------------
A:torch.testing._internal.common_quantized.qx->numpy.clip(qx, qmin, qmax).astype(qtype)
A:torch.testing._internal.common_quantized.X->X.clone().detach().clone().detach()
A:torch.testing._internal.common_quantized.min_val->min(min_val, 0.0)
A:torch.testing._internal.common_quantized.max_val->max(max_val, 0.0)
A:torch.testing._internal.common_quantized.scale->numpy.zeros(X.shape[0], dtype=np.float64)
A:torch.testing._internal.common_quantized.zero_point->numpy.zeros(X.shape[0], dtype=np.int64)
A:torch.testing._internal.common_quantized.scale[i]->max(scale[i], np.finfo(np.float32).eps)
A:torch.testing._internal.common_quantized.zero_point[i]->min(qmax, zero_point[i])
A:torch.testing._internal.common_quantized.x_hat->x_hat.dequantize().dequantize()
A:torch.testing._internal.common_quantized.x->x.dequantize().dequantize()
A:torch.testing._internal.common_quantized.noise->(x - x_hat).norm()
A:torch.testing._internal.common_quantized.signal->x.dequantize().dequantize().norm()
A:torch.testing._internal.common_quantized.new_axis_list->list(range(X.dim()))
A:torch.testing._internal.common_quantized.y->X.clone().detach().clone().detach().permute(tuple(new_axis_list))
A:torch.testing._internal.common_quantized.(X, permute_axis_list)->_permute_to_axis_zero(X.to(torch.float32), axis)
A:torch.testing._internal.common_quantized.res->torch.zeros_like(dY)
A:torch.testing._internal.common_quantized.out->torch.zeros_like(dY).permute(tuple(permute_axis_list))
A:torch.testing._internal.common_quantized.Xq->Xq.permute(tuple(permute_axis_list)).permute(tuple(permute_axis_list))
A:torch.testing._internal.common_quantized.Xq[i]->torch.round(X[i] * (1.0 / per_channel_scale[i]) + per_channel_zero_point[i])
torch.testing._internal.common_quantized._calculate_dynamic_per_channel_qparams(X,dtype)
torch.testing._internal.common_quantized._calculate_dynamic_qparams(X,dtype,reduce_range=False)
torch.testing._internal.common_quantized._conv_output_shape(input_size,kernel_size,padding,stride,dilation,output_padding=0)
torch.testing._internal.common_quantized._dequantize(qx,scale,zero_point)
torch.testing._internal.common_quantized._fake_quantize_per_channel_affine_grad_reference(dY,X,per_channel_scale,per_channel_zero_point,axis,quant_min,quant_max)
torch.testing._internal.common_quantized._fake_quantize_per_channel_affine_reference(X,per_channel_scale,per_channel_zero_point,axis,quant_min,quant_max)
torch.testing._internal.common_quantized._permute_to_axis_zero(X,axis)
torch.testing._internal.common_quantized._quantize(x,scale,zero_point,qmin=None,qmax=None,dtype=np.uint8)
torch.testing._internal.common_quantized._requantize(x,multiplier,zero_point,qmin=0,qmax=255,qtype=np.uint8)
torch.testing._internal.common_quantized._snr(x,x_hat)
torch.testing._internal.common_quantized.override_cpu_allocator_for_qnnpack(qengine_is_qnnpack)
torch.testing._internal.common_quantized.override_qengines(qfunction)
torch.testing._internal.common_quantized.override_quantized_engine(qengine)
torch.testing._internal.common_quantized.qengine_is_fbgemm()
torch.testing._internal.common_quantized.qengine_is_qnnpack()
torch.testing._internal.common_quantized.to_tensor(X,device)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/composite_compliance.py----------------------------------------
A:torch.testing._internal.composite_compliance.guard->torch._C._DisableTorchDispatch()
A:torch.testing._internal.composite_compliance.metadata_wrapper_tensor->metadata_accessor(wrapper_tensor)
A:torch.testing._internal.composite_compliance.metadata_elem->metadata_accessor(elem)
A:torch.testing._internal.composite_compliance.r->torch.Tensor._make_wrapper_subclass(cls, elem.size(), dtype=elem.dtype, layout=elem.layout, device=elem.device, requires_grad=elem.requires_grad, strides=elem.stride(), storage_offset=elem.storage_offset())
A:torch.testing._internal.composite_compliance.unwrapped_args->tree_map(unwrap, args)
A:torch.testing._internal.composite_compliance.unwrapped_kwargs->tree_map(unwrap, kwargs)
A:torch.testing._internal.composite_compliance.unwrapped_rs->func(*unwrapped_args, **unwrapped_kwargs)
A:torch.testing._internal.composite_compliance.rs->tree_map(wrap, unwrapped_rs)
A:torch.testing._internal.composite_compliance.result->func(*args, **kwargs)
A:torch.testing._internal.composite_compliance.check->partial(check_metadata_consistency)
A:torch.testing._internal.composite_compliance.args->tree_map(wrap, args)
A:torch.testing._internal.composite_compliance.kwargs->tree_map(wrap, kwargs)
torch.testing._internal.composite_compliance.CompositeCompliantTensor(cls,elem,*args,**kwargs)
torch.testing._internal.composite_compliance.CompositeCompliantTensor.__new__(cls,elem,*args,**kwargs)
torch.testing._internal.composite_compliance.CompositeCompliantTensor.__repr__(self)
torch.testing._internal.composite_compliance.CompositeCompliantTensor.__torch_dispatch__(cls,func,types,args=(),kwargs=None)
torch.testing._internal.composite_compliance._check_composite_compliance(op,args,kwargs)
torch.testing._internal.composite_compliance.check_attr_consistency(wrapper_tensor,metadata_name,metadata_accessor)
torch.testing._internal.composite_compliance.check_metadata_consistency(wrapper_tensor)
torch.testing._internal.composite_compliance.is_inplace_view_fn(func)
torch.testing._internal.composite_compliance.is_view_fn(func)
torch.testing._internal.composite_compliance.no_dispatch()->Iterator[None]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/generated/annotated_fn_args.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/generated/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/pipe_with_ddp_test.py----------------------------------------
A:torch.testing._internal.distributed.pipe_with_ddp_test.fc1->torch.nn.Linear(16, 8, bias=False).cuda(2 * self.rank)
A:torch.testing._internal.distributed.pipe_with_ddp_test.self.fc2->torch.nn.Linear(8, 4, bias=False).cuda(device)
A:torch.testing._internal.distributed.pipe_with_ddp_test.self.fc3->torch.nn.Linear(4, 2, bias=False).cuda(device)
A:torch.testing._internal.distributed.pipe_with_ddp_test.layer2->MyModule(2 * self.rank + 1)
A:torch.testing._internal.distributed.pipe_with_ddp_test.model->DistributedDataParallel(model, find_unused_parameters=find_unused_parameters, static_graph=static_graph)
A:torch.testing._internal.distributed.pipe_with_ddp_test.out->model(model_input).local_value()
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest(RpcAgentTestFixture)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest._run_basic_test(self,backend,checkpoint,find_unused_parameters=False,static_graph=False)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_gloo_ckpt_always(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_gloo_ckpt_except_last(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_gloo_ckpt_never(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_gloo_ckpt_never_find_unused(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_nccl_ckpt_always(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_nccl_ckpt_except_last(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_nccl_ckpt_never(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.test_basic_nccl_ckpt_never_find_unused(self)
torch.testing._internal.distributed.pipe_with_ddp_test.PipeWithDDPTest.world_size(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/distributed_test.py----------------------------------------
A:torch.testing._internal.distributed.distributed_test.self.a->torch.nn.Linear(10, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.b->torch.nn.Linear(10, 1, bias=False)
A:torch.testing._internal.distributed.distributed_test.f->Foo(10)
A:torch.testing._internal.distributed.distributed_test.foo_cpu_tensor->Foo(torch.randn(3, 3))
A:torch.testing._internal.distributed.distributed_test.TestNamedTupleInput_0->namedtuple('NamedTuple', EXPECTED_FIELDS)
A:torch.testing._internal.distributed.distributed_test.skipIfNoTorchVision->sandcastle_skip_if(not HAS_TORCHVISION, 'no torchvision')
A:torch.testing._internal.distributed.distributed_test.INIT_METHOD->os.getenv('INIT_METHOD', 'env://')
A:torch.testing._internal.distributed.distributed_test.self.fc->torch.nn.Linear(10, 50, bias=True)
A:torch.testing._internal.distributed.distributed_test.x->torch.rand((1, 1)).to(device)
A:torch.testing._internal.distributed.distributed_test.self.fc1->torch.nn.Linear(10, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.fc2->torch.nn.Linear(10, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.fc3->torch.nn.Linear(50, 4, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.relu->torch.nn.ReLU()
A:torch.testing._internal.distributed.distributed_test.self.no_grad_param->torch.nn.Parameter(torch.tensor([2, 2]).long(), requires_grad=False)
A:torch.testing._internal.distributed.distributed_test.self.p->torch.nn.Parameter(torch.tensor(1.0))
A:torch.testing._internal.distributed.distributed_test.self.bn->BatchNormNet()
A:torch.testing._internal.distributed.distributed_test.self.c->torch.nn.Linear(5, 5, bias=False)
A:torch.testing._internal.distributed.distributed_test.a->torch.rand(batch, dim, device=self.rank)
A:torch.testing._internal.distributed.distributed_test.b->torch.rand(batch, dim, device=self.rank)
A:torch.testing._internal.distributed.distributed_test.self.module->UnusedParamTwoLinLayerNet()
A:torch.testing._internal.distributed.distributed_test.predictions->cls(test_name).module(x)
A:torch.testing._internal.distributed.distributed_test.loss->model_ddp(inp).sum()
A:torch.testing._internal.distributed.distributed_test.self.embedding->torch.nn.Embedding(num_embeddings=10, embedding_dim=embedding_dim)
A:torch.testing._internal.distributed.distributed_test.self.lin->torch.nn.Linear(100, 1, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.lin1->torch.nn.Linear(10, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.lin2->torch.nn.Linear(10, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.use_second_layer->torch.equal(x, torch.ones(20, 10, device=x.device))
A:torch.testing._internal.distributed.distributed_test.DDP_NET->Net()
A:torch.testing._internal.distributed.distributed_test.BN_NET->BatchNormNet()
A:torch.testing._internal.distributed.distributed_test.BN_NET_NO_AFFINE->BatchNormNet(affine=False)
A:torch.testing._internal.distributed.distributed_test.ONLY_SBN_NET->torch.nn.SyncBatchNorm(2, momentum=0.99)
A:torch.testing._internal.distributed.distributed_test.lockfile->os.path.join(TEMP_DIR, 'lockfile')
A:torch.testing._internal.distributed.distributed_test.barrier_dir->os.path.join(os.environ['TEMP_DIR'], 'barrier')
A:torch.testing._internal.distributed.distributed_test.wait_for->torch.distributed.get_world_size()
A:torch.testing._internal.distributed.distributed_test.pid->str(os.getpid())
A:torch.testing._internal.distributed.distributed_test.barrier_file->os.path.join(barrier_dir, pid)
A:torch.testing._internal.distributed.distributed_test.start_time->time.time()
A:torch.testing._internal.distributed.distributed_test.data->Foo(10).read()
A:torch.testing._internal.distributed.distributed_test.os.environ['MASTER_ADDR']->str(MASTER_ADDR)
A:torch.testing._internal.distributed.distributed_test.os.environ['MASTER_PORT']->str(MASTER_PORT)
A:torch.testing._internal.distributed.distributed_test.self->cls(test_name)
A:torch.testing._internal.distributed.distributed_test.pg_timeout_seconds->CUSTOM_PG_TIMEOUT.get(test_name, default_pg_timeout)
A:torch.testing._internal.distributed.distributed_test.timeout->timedelta(seconds=0.1)
A:torch.testing._internal.distributed.distributed_test.group_id->torch.distributed.ProcessGroupNCCL(store, rank, size, opts)
A:torch.testing._internal.distributed.distributed_test.rank->torch.distributed.get_rank()
A:torch.testing._internal.distributed.distributed_test.group->list(range(0, dist.get_world_size()))
A:torch.testing._internal.distributed.distributed_test.m1_buffers->list(m1.buffers())
A:torch.testing._internal.distributed.distributed_test.m2_buffers->list(m2.buffers())
A:torch.testing._internal.distributed.distributed_test.lines->ddp(inp).sum().getvalue().splitlines()
A:torch.testing._internal.distributed.distributed_test.line->format_line(var)
A:torch.testing._internal.distributed.distributed_test.test_dir->os.path.join(os.environ['TEMP_DIR'], 'test_dir')
A:torch.testing._internal.distributed.distributed_test.num_processes->torch.distributed.get_world_size()
A:torch.testing._internal.distributed.distributed_test.all_ranks->set()
A:torch.testing._internal.distributed.distributed_test.backend_str->BACKEND.lower()
A:torch.testing._internal.distributed.distributed_test.backend->torch.distributed.get_backend(default)
A:torch.testing._internal.distributed.distributed_test.(_, group_id, _)->cls(test_name)._init_full_group_test(timeout=timeout)
A:torch.testing._internal.distributed.distributed_test.local_rank->torch.distributed.get_rank(group_id)
A:torch.testing._internal.distributed.distributed_test.exception_ctx->cls(test_name).assertRaisesRegex(RuntimeError, 'Detected at least one rank that exhausted inputs.')
A:torch.testing._internal.distributed.distributed_test.(group, group_id, rank)->cls(test_name)._init_global_test()
A:torch.testing._internal.distributed.distributed_test.group_rank->torch.distributed.get_rank(group_id)
A:torch.testing._internal.distributed.distributed_test.tensor->tensor.cuda(rank_to_GPU[rank][0]).cuda(rank_to_GPU[rank][0])
A:torch.testing._internal.distributed.distributed_test.(cur_subgroup, subgroups)->torch.distributed.new_subgroups_by_enumeration(ranks_per_subgroup_list=[[0, 2], [1, 3]])
A:torch.testing._internal.distributed.distributed_test.world_size->torch.distributed.get_world_size()
A:torch.testing._internal.distributed.distributed_test.rank_to_GPU->init_multigpu_helper(dist.get_world_size(), BACKEND)
A:torch.testing._internal.distributed.distributed_test.model->NetWithBuffers().cuda(rank)
A:torch.testing._internal.distributed.distributed_test.p.data->torch.ones_like(p.data)
A:torch.testing._internal.distributed.distributed_test.group_nccl->torch.distributed.new_group(ranks=[0, 1], backend='nccl')
A:torch.testing._internal.distributed.distributed_test.param->next(self.parameters())
A:torch.testing._internal.distributed.distributed_test.averager->torch.distributed.algorithms.model_averaging.averagers.PeriodicModelAverager(period=period, warmup_steps=warmup_steps)
A:torch.testing._internal.distributed.distributed_test.param.data->copy.deepcopy(tensor)
A:torch.testing._internal.distributed.distributed_test.send_tensor->_build_tensor(rank + 1)
A:torch.testing._internal.distributed.distributed_test.recv_tensor->_build_tensor(src + 1, value=-1)
A:torch.testing._internal.distributed.distributed_test.recv_op->torch.distributed.P2POp(dist.irecv, recv_tensor, src, tag=src)
A:torch.testing._internal.distributed.distributed_test.send_op->torch.distributed.P2POp(dist.broadcast, send_tensor, 1)
A:torch.testing._internal.distributed.distributed_test.reqs->torch.distributed.batch_isend_irecv(p2p_op_list)
A:torch.testing._internal.distributed.distributed_test.group_gloo->torch.distributed.new_group(timeout=timedelta(seconds=60), backend=dist.Backend.GLOO)
A:torch.testing._internal.distributed.distributed_test.send_op_gloo->torch.distributed.P2POp(dist.isend, send_tensor, 1, group_gloo)
A:torch.testing._internal.distributed.distributed_test.send_op_nccl->torch.distributed.P2POp(dist.isend, send_tensor, 1, group_nccl)
A:torch.testing._internal.distributed.distributed_test.expected_tensor->torch.tensor(inp[self.rank % 2]).to(self.rank).clone()
A:torch.testing._internal.distributed.distributed_test.output_tensor->_build_tensor(send_recv_size, value=-1)
A:torch.testing._internal.distributed.distributed_test.events->get_profiling_event('search_unused_parameters', prof)
A:torch.testing._internal.distributed.distributed_test.profiler_ctx->torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA], record_shapes=True)
A:torch.testing._internal.distributed.distributed_test.event_count->sum((e.count for e in broadcast_events))
A:torch.testing._internal.distributed.distributed_test.autograd_profiler_ctx->torch.autograd.profiler.profile()
A:torch.testing._internal.distributed.distributed_test.torch_profiler_ctx->torch.profiler.profile(activities=[cpu_act, cuda_act])
A:torch.testing._internal.distributed.distributed_test.recv_ranks->list()
A:torch.testing._internal.distributed.distributed_test.irecv_ranks->list()
A:torch.testing._internal.distributed.distributed_test.sender->group_id.allreduce([tensor], opts)._source_rank()
A:torch.testing._internal.distributed.distributed_test.work->torch.distributed.ProcessGroupNCCL(store, rank, size, opts).allreduce([tensor], opts)
A:torch.testing._internal.distributed.distributed_test.recv_ranks_tensor->torch.cat((torch.tensor(recv_ranks), torch.tensor(irecv_ranks)), 0)
A:torch.testing._internal.distributed.distributed_test.opts->AllreduceOptions()
A:torch.testing._internal.distributed.distributed_test.(group, _, rank)->cls(test_name)._init_global_test()
A:torch.testing._internal.distributed.distributed_test.new_port->str(MASTER_PORT + 1)
A:torch.testing._internal.distributed.distributed_test.gen_iterator->torch.distributed.rendezvous('env://', rank, dist.get_world_size())
A:torch.testing._internal.distributed.distributed_test.(store, rank, size)->next(gen_iterator)
A:torch.testing._internal.distributed.distributed_test.store->torch.distributed.PrefixStore(new_port, store)
A:torch.testing._internal.distributed.distributed_test.tensors[i]->tensors[i].cuda(rank_to_GPU[rank][0]).cuda(rank_to_GPU[rank][0])
A:torch.testing._internal.distributed.distributed_test.result->torch.distributed._compute_bucket_assignment_by_size(tensors_sparse, [400])
A:torch.testing._internal.distributed.distributed_test.tests->simple_sparse_reduce_tests(rank, dist.get_world_size(), num_inputs=1)
A:torch.testing._internal.distributed.distributed_test.(master_values, worker_values, expected_values, dtypes)->test_case_func(len(group))
A:torch.testing._internal.distributed.distributed_test.one->torch.ones([1])
A:torch.testing._internal.distributed.distributed_test.size->len(group)
A:torch.testing._internal.distributed.distributed_test.in_tensor->in_tensor.cuda(rank_to_GPU[rank][0]).cuda(rank_to_GPU[rank][0])
A:torch.testing._internal.distributed.distributed_test.out_tensor->out_tensor.cuda(rank_to_GPU[rank][0]).cuda(rank_to_GPU[rank][0])
A:torch.testing._internal.distributed.distributed_test.expected_time->expected_time.cuda(rank_to_GPU[rank][0]).cuda(rank_to_GPU[rank][0])
A:torch.testing._internal.distributed.distributed_test.tensors[0]->torch.tensor(inp[self.rank % 2]).to(self.rank).clone().cuda(device=rank_to_GPU[rank][0])
A:torch.testing._internal.distributed.distributed_test.input_cpu->torch.randn(global_bs, 2)
A:torch.testing._internal.distributed.distributed_test.target->target.cuda(rank).cuda(rank)
A:torch.testing._internal.distributed.distributed_test.output->net(input)
A:torch.testing._internal.distributed.distributed_test.model_DDP->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])
A:torch.testing._internal.distributed.distributed_test.saved_model->torch.load(tmp_file)
A:torch.testing._internal.distributed.distributed_test.model_gpu->NetWithBuffers().cuda(rank).cuda(rank)
A:torch.testing._internal.distributed.distributed_test.local_bs->len(gpu_subset)
A:torch.testing._internal.distributed.distributed_test.(global_bs, input_cpu, target, loss)->cls(test_name)._prepare_dummy_data(local_bs)
A:torch.testing._internal.distributed.distributed_test.ddp_model->torch.nn.parallel.DistributedDataParallel(model, device_ids=[self.rank], find_unused_parameters=find_unused_parameters, gradient_as_bucket_view=True, static_graph=static_graph)
A:torch.testing._internal.distributed.distributed_test.stream->torch.cuda.Stream(self.rank)
A:torch.testing._internal.distributed.distributed_test.net->torch.nn.parallel.DistributedDataParallel(model.cuda(self.rank), device_ids=[self.rank])
A:torch.testing._internal.distributed.distributed_test.batch->torch.tensor([rank]).float().cuda(rank)
A:torch.testing._internal.distributed.distributed_test.avg->grad.clone()
A:torch.testing._internal.distributed.distributed_test.ddp_logging_data->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data()
A:torch.testing._internal.distributed.distributed_test.inp->torch.randn(2, 10, device=rank)
A:torch.testing._internal.distributed.distributed_test.ddp_model_with_optimizer_hook->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(model).cuda(), device_ids=[self.rank], gradient_as_bucket_view=grad_as_bucket_view, static_graph=static_graph)
A:torch.testing._internal.distributed.distributed_test.ddp_model_with_no_hook->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(model).cuda(), device_ids=[self.rank], gradient_as_bucket_view=grad_as_bucket_view, static_graph=static_graph)
A:torch.testing._internal.distributed.distributed_test.hook_params->list(hook_params)
A:torch.testing._internal.distributed.distributed_test.no_hook_params->list(no_hook_params)
A:torch.testing._internal.distributed.distributed_test.optimizer_no_hook->optim_cls(no_hook_params, *functional_optim_args, **functional_optim_kwargs)
A:torch.testing._internal.distributed.distributed_test.opt_hook_init_params->copy.deepcopy(list(ddp_model_with_optimizer_hook.parameters()))
A:torch.testing._internal.distributed.distributed_test.out->ddp(inp).sum()
A:torch.testing._internal.distributed.distributed_test.m->torch.nn.Linear(10, 10).to(self.rank)
A:torch.testing._internal.distributed.distributed_test.net_with_hook->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(m).to(rank), device_ids=[rank], process_group=process_group)
A:torch.testing._internal.distributed.distributed_test.net_without_hook->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(m).to(rank), device_ids=[rank], process_group=process_group)
A:torch.testing._internal.distributed.distributed_test.loss_hook->model_ddp(inp).sum()
A:torch.testing._internal.distributed.distributed_test.avg_hook->grad_hook.clone()
A:torch.testing._internal.distributed.distributed_test.process_group->torch.distributed.new_group(ranks=list((i for i in range(int(self.world_size)))))
A:torch.testing._internal.distributed.distributed_test.powersgd_state->torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group=None, matrix_approximation_rank=1, start_powerSGD_iter=2, warm_start=warm_start)
A:torch.testing._internal.distributed.distributed_test.state->torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=1000)
A:torch.testing._internal.distributed.distributed_test.input->input.cuda(rank).cuda(rank)
A:torch.testing._internal.distributed.distributed_test.(model, ddp_model, input, target)->cls(test_name)._prepare_single_device_module(rank, group_id, devices, devices, global_batch_size, gradient_as_bucket_view)
A:torch.testing._internal.distributed.distributed_test.local_batch_size->len(devices)
A:torch.testing._internal.distributed.distributed_test.fut->torch.distributed.ProcessGroupNCCL(store, rank, size, opts).allreduce([input]).get_future()
A:torch.testing._internal.distributed.distributed_test.res->torch.nn.utils._stateless.functional_call(module, parameters, x)
A:torch.testing._internal.distributed.distributed_test.expected->torch.tensor([world_size, 1], device=self.rank, dtype=torch.int32)
A:torch.testing._internal.distributed.distributed_test.gpus->list(rank_to_GPU[rank])
A:torch.testing._internal.distributed.distributed_test.optimizer->torch.optim.SGD(model.parameters(), lr=0.03)
A:torch.testing._internal.distributed.distributed_test.scaler->GradScaler()
A:torch.testing._internal.distributed.distributed_test.loss_fn->torch.nn.MSELoss()
A:torch.testing._internal.distributed.distributed_test.ddp_model_grad_not_view->cls(test_name)._test_DistributedDataParallel_with_amp(grad_is_view=False)
A:torch.testing._internal.distributed.distributed_test.ddp_model_grad_is_view->cls(test_name)._test_DistributedDataParallel_with_amp(grad_is_view=True)
A:torch.testing._internal.distributed.distributed_test.opt->torch.randn(1, 10, device=param.device)
A:torch.testing._internal.distributed.distributed_test.post_localSGD_net->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(DDP_NET).cuda(), device_ids=[self.rank], gradient_as_bucket_view=grad_is_view)
A:torch.testing._internal.distributed.distributed_test.post_localSGD_opt->torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer(optim=torch.optim.SGD(post_localSGD_net.parameters(), lr=learning_rate), averager=averagers.PeriodicModelAverager(period=period, warmup_steps=warmup_steps))
A:torch.testing._internal.distributed.distributed_test.post_localSGD_output->post_localSGD_net(input)
A:torch.testing._internal.distributed.distributed_test.post_localSGD_loss->loss_fn(post_localSGD_output, target)
A:torch.testing._internal.distributed.distributed_test.bs_offset->int((rank + 3) * rank / 2)
A:torch.testing._internal.distributed.distributed_test.global_bs->int((num_processes + 3) * num_processes / 2)
A:torch.testing._internal.distributed.distributed_test.input_gpu->torch.randn(global_bs, 2, 4, 4, dtype=torch.float).cuda(rank).to(memory_format=memory_format)
A:torch.testing._internal.distributed.distributed_test.target_gpu->torch.randn(global_bs, 2, 4, 4, dtype=torch.float).cuda(rank).to(memory_format=memory_format)
A:torch.testing._internal.distributed.distributed_test.input_var_rank->torch.cat([torch.ones(2, 1, 10 ** (i + 1)) * 0.1 ** (i - 1), torch.ones(2, 1, 10 ** (i + 1)) * 0.3 ** (i - 1)], dim=1)
A:torch.testing._internal.distributed.distributed_test.all_input_var->torch.cat([x.permute(1, 0, 2).contiguous().view(ONLY_SBN_NET.num_features, -1) for x in input_var], dim=1).cuda(rank)
A:torch.testing._internal.distributed.distributed_test.y->torch.rand((1, 1)).to(device).clone()
A:torch.testing._internal.distributed.distributed_test.(batch_size, input, target, loss)->cls(test_name)._prepare_dummy_data(local_bs)
A:torch.testing._internal.distributed.distributed_test.params->list(model_DDP.parameters())
A:torch.testing._internal.distributed.distributed_test.init_bucket_lims->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('initial_bucket_size_limits')
A:torch.testing._internal.distributed.distributed_test.rebuilt_bucket_lims->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('rebuilt_bucket_size_limits')
A:torch.testing._internal.distributed.distributed_test.grad_ready_order->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('prev_iteration_grad_ready_order_indices')
A:torch.testing._internal.distributed.distributed_test.expected_order->list(reversed([str(x) for x in range(3)]))
A:torch.testing._internal.distributed.distributed_test.bucket_indices->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('rebuilt_per_bucket_param_indices')
A:torch.testing._internal.distributed.distributed_test.fwd_host_side_time->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('forward_compute_time_start')
A:torch.testing._internal.distributed.distributed_test.bwd_comp_start_host_side_time->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('backward_compute_time_start')
A:torch.testing._internal.distributed.distributed_test.bwd_comp_end_host_side_time->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('backward_compute_time_end')
A:torch.testing._internal.distributed.distributed_test.bwd_comm_start_host_side_time->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('backward_comm_time_start')
A:torch.testing._internal.distributed.distributed_test.bwd_comm_end_host_side_time->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('backward_comm_time_end')
A:torch.testing._internal.distributed.distributed_test.res50_model->torchvision.models.resnet50()
A:torch.testing._internal.distributed.distributed_test.res50_model_sync->torch.nn.SyncBatchNorm.convert_sync_batchnorm(copy.deepcopy(res50_model), process_group)
A:torch.testing._internal.distributed.distributed_test.input_tensor->torch.tensor(inp[self.rank % 2]).to(self.rank)
A:torch.testing._internal.distributed.distributed_test.input_tensor_copy->torch.tensor(inp[self.rank % 2]).to(self.rank).clone()
A:torch.testing._internal.distributed.distributed_test.bcast_tensor->torch.tensor([random.random() < 0.5 if self.rank == 0 else False for _ in range(tensor_size)]).to(self.rank)
A:torch.testing._internal.distributed.distributed_test.dataset_tiny_size->max(world_size // 2 - 1, 1)
A:torch.testing._internal.distributed.distributed_test.dist_sampler->DistributedSampler(dataset=dataset, drop_last=True)
A:torch.testing._internal.distributed.distributed_test.indices_list->list(iter(dist_sampler_added_samples_tiny))
A:torch.testing._internal.distributed.distributed_test.dist_sampler_added_samples->DistributedSampler(dataset=dataset)
A:torch.testing._internal.distributed.distributed_test.dist_sampler_added_samples_tiny->DistributedSampler(dataset=dataset_tiny)
A:torch.testing._internal.distributed.distributed_test.gather_objects->COLLECTIVES_OBJECT_TEST_LIST.copy()
A:torch.testing._internal.distributed.distributed_test.default->_get_default_group()
A:torch.testing._internal.distributed.distributed_test.subgroup->torch.distributed.new_group(ranks=[0, 1])
A:torch.testing._internal.distributed.distributed_test.my_rank->torch.distributed.get_rank(pg)
A:torch.testing._internal.distributed.distributed_test.net_module_states->list(net.module.state_dict().values())
A:torch.testing._internal.distributed.distributed_test.new_model->torch.nn.Linear(dim, dim, bias=False).cuda(rank)
A:torch.testing._internal.distributed.distributed_test.net.module->copy.deepcopy(new_model)
A:torch.testing._internal.distributed.distributed_test.expected_states->torch.nn.Linear(dim, dim, bias=False).cuda(rank).state_dict().values()
A:torch.testing._internal.distributed.distributed_test.effective_ws->torch.distributed.get_world_size()
A:torch.testing._internal.distributed.distributed_test.profiler_ctx_copy->copy.deepcopy(profiler_ctx)
A:torch.testing._internal.distributed.distributed_test.broadcast_events->get_profiling_event(broadcast_event_name, prof)
A:torch.testing._internal.distributed.distributed_test.local_model->copy.deepcopy(model)
A:torch.testing._internal.distributed.distributed_test.local_iters->sum(rank_to_iter_mapping.values())
A:torch.testing._internal.distributed.distributed_test.local_optim->torch.optim.SGD(local_model.parameters(), lr=learning_rate)
A:torch.testing._internal.distributed.distributed_test.ddp_optim->torch.optim.SGD(model.parameters(), lr=learning_rate * dist.get_world_size())
A:torch.testing._internal.distributed.distributed_test.num_iters_tensor->torch.tensor([num_iters], device=torch.cuda.current_device())
A:torch.testing._internal.distributed.distributed_test.min_num_iters->torch.tensor([num_iters], device=torch.cuda.current_device()).item()
A:torch.testing._internal.distributed.distributed_test.context->suppress()
A:torch.testing._internal.distributed.distributed_test.final_rank_tensor->torch.tensor([net._authoritative_rank], device=self.rank)
A:torch.testing._internal.distributed.distributed_test.model_bn->torch.nn.SyncBatchNorm.convert_sync_batchnorm(copy.deepcopy(model_bn)).cuda(self.rank)
A:torch.testing._internal.distributed.distributed_test.comm_model->ModelWithComm().cuda(self.rank)
A:torch.testing._internal.distributed.distributed_test.model_input->torch.randn(10, 2).cuda(torch.cuda.current_device())
A:torch.testing._internal.distributed.distributed_test.large_model->torch.nn.Sequential(nn.Conv2d(1, 20, 5), nn.ReLU(), nn.Conv2d(20, 32, 5), nn.ReLU(), nn.Conv2d(32, 256, 5), nn.ReLU())
A:torch.testing._internal.distributed.distributed_test.small_model->torch.nn.Linear(dim, dim, bias=False)
A:torch.testing._internal.distributed.distributed_test.bn_net->BatchNormNet()
A:torch.testing._internal.distributed.distributed_test.self.t0->Task()
A:torch.testing._internal.distributed.distributed_test.self.t1->Task()
A:torch.testing._internal.distributed.distributed_test.unjoined_rank_with_unused_params_model->UnusedParamModule(1)
A:torch.testing._internal.distributed.distributed_test.joined_rank_with_unused_params_model->UnusedParamModule(0)
A:torch.testing._internal.distributed.distributed_test.resnet_model->torchvision.models.resnet50()
A:torch.testing._internal.distributed.distributed_test.self.param->torch.nn.Parameter(torch.ones(1, requires_grad=True))
A:torch.testing._internal.distributed.distributed_test.exception_module->ExceptionModule()
A:torch.testing._internal.distributed.distributed_test.proxy_params->list(model.fc2.parameters())
A:torch.testing._internal.distributed.distributed_test.proxy_buffers->list(model.fc2.buffers())
A:torch.testing._internal.distributed.distributed_test.ddp->torch.nn.parallel.DistributedDataParallel(MyModel().cuda(), device_ids=[self.rank])
A:torch.testing._internal.distributed.distributed_test.ddp.module.fc2->torch.nn.Linear(1, 1, bias=False).to(device_id)
A:torch.testing._internal.distributed.distributed_test.self.net1->torch.nn.Linear(10, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.net2->torch.nn.Linear(10, 10)
A:torch.testing._internal.distributed.distributed_test.msg->str(e)
A:torch.testing._internal.distributed.distributed_test.unreduced_params->', '.join(['lin2.weight'])
A:torch.testing._internal.distributed.distributed_test.self.bias->torch.nn.Parameter(torch.zeros(5))
A:torch.testing._internal.distributed.distributed_test.x.t->torch.rand((1, 1)).to(device).t.to(self.rank)
A:torch.testing._internal.distributed.distributed_test._self.lin->torch.nn.Linear(10, 1)
A:torch.testing._internal.distributed.distributed_test.fwd_tensor->validators[expected_type](x)
A:torch.testing._internal.distributed.distributed_test.random_input->torch.randn(20, 10, device=self.rank)
A:torch.testing._internal.distributed.distributed_test.ones_input->torch.ones(batch, dim, device=self.rank)
A:torch.testing._internal.distributed.distributed_test.local_used_map->NetWithBuffers().cuda(rank).reducer._get_local_used_map()
A:torch.testing._internal.distributed.distributed_test.group_to_use->torch.distributed.new_group(backend=dist.get_backend(), timeout=timedelta(seconds=5))
A:torch.testing._internal.distributed.distributed_test.tensors_sparse->cls(test_name)._generate_sparse_tensors_for_bucket_assignment_test()
A:torch.testing._internal.distributed.distributed_test.ctx->cls(test_name).assertRaisesRegex(RuntimeError, expected_err)
A:torch.testing._internal.distributed.distributed_test.(ctx, expected_err)->cls(test_name)._determine_expected_error_verify_model_across_rank(group_to_use)
A:torch.testing._internal.distributed.distributed_test.net.module.lin->torch.nn.Linear(100 if self.rank == 0 else 10, 1)
A:torch.testing._internal.distributed.distributed_test.local_net->copy.deepcopy(net)
A:torch.testing._internal.distributed.distributed_test.(a, b)->local_net(inp)
A:torch.testing._internal.distributed.distributed_test.(a_dist, b_dist)->net(inp)
A:torch.testing._internal.distributed.distributed_test.loss_dist->b_dist.sum()
A:torch.testing._internal.distributed.distributed_test.local_out->local_net(x).sum()
A:torch.testing._internal.distributed.distributed_test.base_model->cls(test_name)._test_different_graph_across_ranks(find_unused_parameters=True)
A:torch.testing._internal.distributed.distributed_test.static_model->cls(test_name)._test_different_graph_across_ranks(static_graph=True)
A:torch.testing._internal.distributed.distributed_test.nccl_pg->torch.distributed.new_group(ranks=list((i for i in range(int(self.world_size)))), timeout=timedelta(seconds=15), backend=dist.Backend.NCCL)
A:torch.testing._internal.distributed.distributed_test.gloo_pg->torch.distributed.new_group(ranks=list((i for i in range(int(self.world_size)))), backend=dist.Backend.GLOO)
A:torch.testing._internal.distributed.distributed_test.rank_str->', '.join([str(i) for i in range(1, int(self.world_size))])
A:torch.testing._internal.distributed.distributed_test.monitored_barrier_timeout_seconds->timedelta(seconds=0.1)
A:torch.testing._internal.distributed.distributed_test.(net_params, _)->torch.nn.parallel.DistributedDataParallel(model.cuda(self.rank), device_ids=[self.rank])._build_params_for_reducer()
A:torch.testing._internal.distributed.distributed_test.param_to_name_mapping->torch.nn.parallel.DistributedDataParallel(model.cuda(self.rank), device_ids=[self.rank])._build_param_to_name_mapping(net_params)
A:torch.testing._internal.distributed.distributed_test.self.embedding_net->EmbeddingNet(0)
A:torch.testing._internal.distributed.distributed_test.self.lin_layer->torch.nn.Linear(4, 10, bias=False)
A:torch.testing._internal.distributed.distributed_test.self.sub_module->SubModule()
A:torch.testing._internal.distributed.distributed_test.e->str(e)
A:torch.testing._internal.distributed.distributed_test.syncbn_model->torch.nn.parallel.DistributedDataParallel(syncbn_model, device_ids=[rank])
A:torch.testing._internal.distributed.distributed_test.local_syncbn_model->copy.deepcopy(syncbn_model)
A:torch.testing._internal.distributed.distributed_test.inp_syncbn->torch.randn(10, 2, 4, 4, device=rank)
A:torch.testing._internal.distributed.distributed_test.all_gather_calls->get_profiling_event('all_gather', prof)
A:torch.testing._internal.distributed.distributed_test.model_static_graph->torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], static_graph=True)
A:torch.testing._internal.distributed.distributed_test.out_static->model_static_graph(inp, output_type=output_type)
A:torch.testing._internal.distributed.distributed_test.loss_static->get_loss(out_static)
A:torch.testing._internal.distributed.distributed_test.o->(out[0] + out[1]).sum()
A:torch.testing._internal.distributed.distributed_test.opt_1->cls(test_name).__init_opt()
A:torch.testing._internal.distributed.distributed_test.opt_2->cls(test_name).__init_opt()
A:torch.testing._internal.distributed.distributed_test.opt_nested->cls(test_name).__init_opt()
A:torch.testing._internal.distributed.distributed_test.(out, opt[0], opt[1], opt[2])->ddp(x, opt_1=opt[0], opt_2=opt[1], opt_nested=opt[2])
A:torch.testing._internal.distributed.distributed_test.self.model->torch.nn.Sequential(nn.Linear(2, 4000, bias=False), *[nn.Linear(4000, 4000, bias=False) for _ in range(10)])
A:torch.testing._internal.distributed.distributed_test.logging_data->torch.nn.parallel.DistributedDataParallel(MyModel().cuda(), device_ids=[self.rank])._get_ddp_logging_data()
A:torch.testing._internal.distributed.distributed_test.model_ddp->torch.nn.parallel.DistributedDataParallel(model, device_ids=[self.rank])
A:torch.testing._internal.distributed.distributed_test.model_ddp_no_hook->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(model), device_ids=[self.rank])
A:torch.testing._internal.distributed.distributed_test.model_no_hook_buffers->list(model_ddp_no_hook.module.buffers())
A:torch.testing._internal.distributed.distributed_test.loss_no_hook->model_ddp_no_hook(inp).sum()
A:torch.testing._internal.distributed.distributed_test.no_sync_bn->torch.nn.parallel.DistributedDataParallel(copy.deepcopy(model_gpu), device_ids=[self.rank])
A:torch.testing._internal.distributed.distributed_test.sync_bn_logged->torch.nn.parallel.DistributedDataParallel(model_DDP, device_ids=[self.rank])._get_ddp_logging_data().get('has_sync_bn', False)
A:torch.testing._internal.distributed.distributed_test.self.l1->torch.nn.Linear(1, 1)
A:torch.testing._internal.distributed.distributed_test.buffer->torch.tensor([0.0], device=device)
A:torch.testing._internal.distributed.distributed_test.module->torch.nn.parallel.DistributedDataParallel(module, device_ids=[device])
A:torch.testing._internal.distributed.distributed_test.weight->torch.tensor([[1.0]], device=device, requires_grad=True)
A:torch.testing._internal.distributed.distributed_test.bias->torch.tensor([0.0], device=device, requires_grad=True)
A:torch.testing._internal.distributed.distributed_test.prev_weight->torch.nn.parallel.DistributedDataParallel(module, device_ids=[device]).module.l1.weight.clone()
A:torch.testing._internal.distributed.distributed_test.prev_buffer->torch.nn.parallel.DistributedDataParallel(module, device_ids=[device]).module.buffer.clone()
torch.testing._internal.distributed.distributed_test.Barrier(object)
torch.testing._internal.distributed.distributed_test.Barrier.init(cls)
torch.testing._internal.distributed.distributed_test.Barrier.sync(cls,wait_for=None,timeout=10)
torch.testing._internal.distributed.distributed_test.BatchNormNet(self,affine=True)
torch.testing._internal.distributed.distributed_test.BatchNormNet.__init__(self,affine=True)
torch.testing._internal.distributed.distributed_test.BatchNormNet.forward(self,x)
torch.testing._internal.distributed.distributed_test.ControlFlowToyModel(self)
torch.testing._internal.distributed.distributed_test.ControlFlowToyModel.__init__(self)
torch.testing._internal.distributed.distributed_test.ControlFlowToyModel.forward(self,x)
torch.testing._internal.distributed.distributed_test.DDPUnevenTestInput(NamedTuple)
torch.testing._internal.distributed.distributed_test.DictOutputModule(self)
torch.testing._internal.distributed.distributed_test.DictOutputModule.__init__(self)
torch.testing._internal.distributed.distributed_test.DictOutputModule.forward(self,x)
torch.testing._internal.distributed.distributed_test.DistributedTest
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._all_reduce_coalesced_max_test_cases(group_size)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._all_reduce_coalesced_min_test_cases(group_size)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._all_reduce_coalesced_product_test_cases(group_size)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._all_reduce_coalesced_sum_test_cases(group_size)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._assert_equal_param(self,param_gpu,param_DDP)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._barrier(self,*args,**kwargs)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._determine_expected_error_verify_model_across_rank(self,group_to_use)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._generate_sparse_tensors_for_bucket_assignment_test(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._init_full_group_test(self,**kwargs)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._init_global_test(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._init_group_test(self,**kwargs)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._model_step(self,model)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._model_step_with_zero_grad(self,model)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._prepare_cpu_module(self,process_group,global_batch_size,gradient_as_bucket_view=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._prepare_dummy_data(self,local_bs)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._prepare_single_device_module(self,rank,process_group,devices,device_ids,global_batch_size,gradient_as_bucket_view=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._run_all_gather_coalesced_and_verify(self,output_tensor_lists,input_tensors,expected_tensors,group_id)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._run_reduction_test(self,tensor,expected_tensor,op,reduction_fn=dist.all_reduce,dst=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._run_uneven_inputs_test(self,test_case,iteration_mapping,find_unused_params)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_DDP_helper(self,model,input_var,target,loss,scale_factor=1.0,memory_format=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_DDP_niter(self,model_base,model_DDP,input,target,loss,local_bs,rank,batch_size,test_save,offset=None,world_size=0,zero_grad=False,memory_format=None,n_iter=5)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_DistributedDataParallel(self,gpu_subset,rank,output_device=None,gradient_as_bucket_view=False,static_graph=False,set_static_graph_twice=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_DistributedDataParallelCPU(self,gradient_as_bucket_view=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_DistributedDataParallel_SyncBatchNorm(self,gpu_subset,rank,local_bs,global_bs,offset,output_device=None,affine=True)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_DistributedDataParallel_with_amp(self,grad_is_view=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_accumulate_gradients_no_sync(self,num_iters=2,ddp_comm_hook=None,gradient_as_bucket_view=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_gather_coalesced_helper(self,group,group_id,rank,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_gather_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_gather_multigpu_helper(self,group,group_id,rank,rank_to_GPU,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_reduce_coalesced_helper(self,group,group_id,rank,op,cuda=False,rank_to_GPU=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_reduce_helper(self,group,group_id,rank,op,master_value,worker_value,expected_value,cuda=False,rank_to_GPU=None,dtype=torch.float,async_op=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_reduce_multigpu_helper(self,group,group_id,rank,rank_to_GPU,op,master_value,worker_value,expected_value,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_to_all_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_to_all_single_equal_split_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_all_to_all_single_unequal_split_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_allgather_object(self,subgroup=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_barrier_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_barrier_timeout(self,group_id,timeout)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_broadcast_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None,with_options=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_broadcast_multigpu_helper(self,group,group_id,rank,rank_to_GPU)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_broadcast_object_list(self,group=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_broadcast_object_list_subgroup(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_compute_bucket_assignment_by_size(self,use_logger)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_buffer_hook_allreduce(self,return_futures)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_hook_parity(self,state,hook)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_hook_with_optimizer_parity(self,grad_as_bucket_view,static_graph,optim_cls,optimize_subset,*functional_optim_args,**functional_optim_kwargs)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_ignore_params_arg(self,static_graph=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_logging_data(self,is_gpu)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_multiple_nested_unused_params_error(self,ignore_sparse)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_new_tensor_in_fwd(self,static_graph)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_ddp_profiling(self,profiler_ctx)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_different_graph_across_ranks(self,find_unused_parameters=False,static_graph=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_gather_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_gather_object(self,pg=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_group_override_backend(self,initializer)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_isend(self,profiler_ctx)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_monitored_barrier_allreduce_hang(self,wait_all_ranks)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_output_unused_in_loss(self,module_cls,gradient_as_bucket_view)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_reduce_helper(self,group,group_id,rank,op,master_value,worker_value,expected_value,cuda=False,rank_to_GPU=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_reduce_multigpu_helper(self,group,group_id,rank,rank_to_GPU,op,master_value,worker_value,expected_value)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_reduce_twice_helper(self,group,group_id,rank,op,master_value,worker_value,expected_value,cuda=False,rank_to_GPU=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_scatter_helper(self,group,group_id,rank,cuda=False,rank_to_GPU=None,dtype=torch.float)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_send_recv(self,profiler_ctx)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_send_recv_any_source(self,profiler_ctx)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_send_recv_nccl(self,profiler_ctx=None)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_send_recv_with_tag(self,profiler_ctx)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_sparse_all_reduce_sum(self,fn)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._test_verify_model_across_rank(self,use_logger)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase._verify_buffers_equal(self,m1,m2)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.call_dist_op(self,profiling_title_postfix,is_async,op,*args,expect_event=True,secondary_op_call=None,profile_cuda=False,tensor_shapes=None,**kwargs)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_Backend_enum_class(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallelCPU(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallelCPU_grad_is_view(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm_2D_Input(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm_Channels_Last(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm_No_Affine(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_non_default_stream(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_requires_grad(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedDataParallel_with_amp_and_grad_is_view(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_DistributedSampler_padding(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_SyncBatchNorm_process_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_accumulate_gradients_no_sync(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_accumulate_gradients_no_sync_allreduce_hook(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_accumulate_gradients_no_sync_allreduce_with_then_hook(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_accumulate_gradients_no_sync_grad_is_view(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_coalesced_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_coalesced_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_coalesced_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_coalesced_simple(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_coalesced_with_empty(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_cuda_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_multigpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_multigpu_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_object_default_pg(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_gather_object_subgroup(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_full_group_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_full_group_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_full_group_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_full_group_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_group_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_group_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_group_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_group_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_max_complex_unsupported(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_coalesced_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_complex_unsupported_ops(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_full_group_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_full_group_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_full_group_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_full_group_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_group_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_group_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_group_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_group_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_multigpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_multigpu_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_result_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_sum_async(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_sum_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_sum_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_sum_cuda_async(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_reduce_sum_cuda_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_cuda_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_full_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_cuda_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_full_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_equal_split_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_cuda_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_full_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_all_to_all_single_unequal_split_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_average_parameters(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_backend_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_backend_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_full_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_group_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_timeout_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_timeout_global(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_barrier_timeout_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_gloo(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_gloo_tags(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_mixed_backend_err(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_nccl(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_no_rank_zero_nccl(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_op_err(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_op_list_err(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_self_nccl(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_batch_isend_irecv_tensor_err(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_broadcast(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_broadcast_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_broadcast_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_broadcast_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_broadcast_multigpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_broadcast_object_list(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_compute_bucket_assignment_by_size_sparse_error_with_logger(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_compute_bucket_assignment_by_size_sparse_error_without_logger(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_broadcast_buffer(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_broadcast_buffer_via_hook(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_buffer_hook_allreduce(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_buffer_hook_allreduce_return_future(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_build_param_to_name_mapping(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_build_param_to_name_mapping_requires_grad(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_comm_hook_logging(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_control_flow_different_across_ranks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_control_flow_same_across_ranks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_create_graph(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_device(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_get_bucket_sizes(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_grad_div_uneven_inputs(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_parity_allreduce(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_parity_allreduce_process_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_parity_post_localSGD(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_parity_powerSGD(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_with_optimizer_parity_adam(self,optimize_subset)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_with_optimizer_parity_adamw(self,grad_as_bucket_view,static_graph,optimize_subset)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_hook_with_optimizer_parity_sgd(self,optimize_subset)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_ignore_params_arg(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_inference(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_join_model_equivalence(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_logging_data_cpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_logging_data_gpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_model_diff_across_ranks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_multiple_nested_unused_params_err_ignore_params(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_multiple_nested_unused_params_error(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_namedtuple(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_new_tensor_in_fwd(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_new_tensor_in_fwd_static_graph(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_profiling_autograd_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_profiling_torch_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_python_error_logged(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_returns_tensor_with_no_grad(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_shared_grad_acc_unused_params(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_static_graph_nested_types(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_sync_bn_training_vs_eval(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_sync_params_and_buffers(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_uneven_input_exception(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_uneven_input_join_disable(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_uneven_inputs(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_uneven_inputs_stop_iteration_sync_bn(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_ddp_unused_params_rebuild_buckets_exception(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_destroy_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_destroy_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_detect_ddp_is_actually_static(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_different_graph_across_ranks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_dump_DDP_relevant_env_vars(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather_checks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather_object(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_gather_object_subgroup(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_get_backend(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_get_future(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_get_rank(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_get_rank_size_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_get_rank_size_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_invalid_static_graph(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_irecv(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_isend(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_isend_autograd_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_isend_torch_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_allreduce_hang(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_allreduce_hang_wait_all_ranks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_failure_order(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_gloo(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_gloo_rank_0_timeout(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_gloo_subgroup(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_monitored_barrier_wait_all_ranks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_nccl_backend_bool_allgather(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_nccl_backend_bool_allreduce(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_nccl_backend_bool_broadcast(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_nccl_backend_bool_reduce(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_nccl_high_priority_stream(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups_by_enumeration(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups_by_enumeration_input_rank_exceeds_world_size(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups_by_enumeration_negative_input_rank(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups_group_size_exceeds_world_size(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups_overlap_not_allowed(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_new_subgroups_world_size_not_divisible_by_group_size(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_output_unused_in_loss_dict_module(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_output_unused_in_loss_tuple_module(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_periodic_model_averager(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_post_localSGD_optimizer_parity(self,grad_is_view=False)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_full_group_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_full_group_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_full_group_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_full_group_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_group_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_group_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_group_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_group_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_max(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_min(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_multigpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_product(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_sum_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_sum_cuda_twice(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_reduce_sum_twice(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_checks(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_cuda_complex(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_full_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_group(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_scatter_object_list(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_any_source(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_any_source_autograd_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_any_source_torch_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_autograd_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_nccl(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_nccl_autograd_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_nccl_torch_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_torch_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_with_tag(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_with_tag_autograd_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_send_recv_with_tag_torch_profiler(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_sparse_all_reduce_sum(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_sparse_all_reduce_sum_cuda(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_stateless_api_with_ddp(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_static_graph_api_cpu(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_sync_bn_logged(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_undefined_grad_parity_unused_parameters(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_verify_model_across_rank_with_logger(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.test_verify_model_across_rank_without_logger(self)
torch.testing._internal.distributed.distributed_test.DistributedTest._DistTestBase.validate_net_equivalence(self,net)
torch.testing._internal.distributed.distributed_test.EmbeddingNet(self,rank)
torch.testing._internal.distributed.distributed_test.EmbeddingNet.__init__(self,rank)
torch.testing._internal.distributed.distributed_test.EmbeddingNet.forward(self,x)
torch.testing._internal.distributed.distributed_test.Foo(self,x)
torch.testing._internal.distributed.distributed_test.Foo.__eq__(self,other)
torch.testing._internal.distributed.distributed_test.Foo.__init__(self,x)
torch.testing._internal.distributed.distributed_test.LargeNet(self)
torch.testing._internal.distributed.distributed_test.LargeNet.__init__(self)
torch.testing._internal.distributed.distributed_test.LargeNet.forward(self,x)
torch.testing._internal.distributed.distributed_test.Net(self)
torch.testing._internal.distributed.distributed_test.Net.__init__(self)
torch.testing._internal.distributed.distributed_test.Net.forward(self,x)
torch.testing._internal.distributed.distributed_test.NetWithBuffers(self)
torch.testing._internal.distributed.distributed_test.NetWithBuffers.__init__(self)
torch.testing._internal.distributed.distributed_test.NetWithBuffers.forward(self,x)
torch.testing._internal.distributed.distributed_test.Task(self)
torch.testing._internal.distributed.distributed_test.Task.__init__(self)
torch.testing._internal.distributed.distributed_test.Task.forward(self,x)
torch.testing._internal.distributed.distributed_test.TestDistBackend(MultiProcessTestCase)
torch.testing._internal.distributed.distributed_test.TestDistBackend._run(cls,rank,test_name,file_name,pipe)
torch.testing._internal.distributed.distributed_test.TestDistBackend.init_method(self)
torch.testing._internal.distributed.distributed_test.TestDistBackend.setUp(self)
torch.testing._internal.distributed.distributed_test.TestDistBackend.setUpClass(cls)
torch.testing._internal.distributed.distributed_test.TestDistBackend.tearDown(self)
torch.testing._internal.distributed.distributed_test.TestDistBackend.world_size(self)
torch.testing._internal.distributed.distributed_test.TestNamedTupleInput_1(NamedTuple)
torch.testing._internal.distributed.distributed_test.TwoLinLayerNet(self)
torch.testing._internal.distributed.distributed_test.TwoLinLayerNet.__init__(self)
torch.testing._internal.distributed.distributed_test.TwoLinLayerNet.forward(self,x)
torch.testing._internal.distributed.distributed_test.UnusedParamTwoLinLayerNet(self)
torch.testing._internal.distributed.distributed_test.UnusedParamTwoLinLayerNet.__init__(self)
torch.testing._internal.distributed.distributed_test.UnusedParamTwoLinLayerNet.forward(self,x)
torch.testing._internal.distributed.distributed_test._FC2(self)
torch.testing._internal.distributed.distributed_test._FC2.__init__(self)
torch.testing._internal.distributed.distributed_test._FC2.forward(self,x)
torch.testing._internal.distributed.distributed_test._build_multidim_tensor(dim,dim_size,value=None,dtype=torch.float)
torch.testing._internal.distributed.distributed_test._build_tensor(size,value=None,dtype=torch.float,device_id=None)
torch.testing._internal.distributed.distributed_test._create_autograd_profiler()
torch.testing._internal.distributed.distributed_test._create_torch_profiler()
torch.testing._internal.distributed.distributed_test._lock()
torch.testing._internal.distributed.distributed_test.get_profiling_event(postfix,profiler)
torch.testing._internal.distributed.distributed_test.get_timeout(test_id)
torch.testing._internal.distributed.distributed_test.require_backend(backends)
torch.testing._internal.distributed.distributed_test.require_backends_available(backends)
torch.testing._internal.distributed.distributed_test.require_world_size(world_size)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc_utils.py----------------------------------------
A:torch.testing._internal.distributed.rpc_utils.use_tcp_init->os.environ.get('RPC_INIT_WITH_TCP', None)
A:torch.testing._internal.distributed.rpc_utils.os.environ['MASTER_PORT']->str(find_free_port())
A:torch.testing._internal.distributed.rpc_utils.class_->type(name, (test_class, mixin, SpawnHelper), dict())
torch.testing._internal.distributed.rpc_utils.SpawnHelper(MultiProcessTestCase)
torch.testing._internal.distributed.rpc_utils.SpawnHelper.setUp(self)
torch.testing._internal.distributed.rpc_utils.SpawnHelper.tearDown(self)
torch.testing._internal.distributed.rpc_utils._check_and_set_tcp_init()
torch.testing._internal.distributed.rpc_utils._check_and_unset_tcp_init()
torch.testing._internal.distributed.rpc_utils.generate_tests(prefix:str,mixin:Type[RpcAgentTestFixture],tests:List[Type[RpcAgentTestFixture]],module_name:str)->Dict[str, Type[RpcAgentTestFixture]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/ddp_under_dist_autograd_test.py----------------------------------------
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.TRAINER_RANKS->list(range(NUM_TRAINERS))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.NONE->enum.auto()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.OUTSIDE->enum.auto()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.INSIDE->enum.auto()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.logger->logging.getLogger(__name__)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.console->logging.StreamHandler()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.formatter->logging.Formatter('%(asctime)s %(filename)s:%(lineno)s %(levelname)s p:%(processName)s t:%(threadName)s: %(message)s')
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.gLogger->init_logger()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.args_tup->tuple([method, rref] + list(args))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.self.em->torch.nn.EmbeddingBag(num_embeddings, embedding_dim, _weight=torch.tensor([init_em] * num_embeddings))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.l->torch.nn.Linear(d_in, d_out, bias=False)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.w->torch.ones((d_out, d_in))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.self.fc->getLinear(d_in, d_out)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.self.relu->torch.nn.ReLU()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.self.fc1->getLinear(D_DENSE, D_DENSE)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.self.fc2->DistributedDataParallel(self.fc2, check_reduction=True, process_group=process_group_for_ddp)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.sparse->_remote_method(RemoteEM.forward, self.remote_em_rref, input.sparse_features)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.dense->self.fc1(input.dense_features)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.x->_remote_method(RemoteNet.forward, self.remote_net_rref, x)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.self.hybrid_module->DistributedDataParallel(self.hybrid_module, check_reduction=True, process_group=self.trainer_group)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.dense_microbatch->torch.split(dense_features, 2)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.sparse_microbatch->torch.split(sparse_features, 2)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.values_microbatch->torch.split(values, 2)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.feature_set->FeatureSet(dense_features=d, sparse_features=s, values=v)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.output->self.hybrid_module.forward(b)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.loss->ddp_layer4(remote_layer3(ddp_layer2(remote_layer1(inputs).cuda(self.rank)).cpu()).cuda(self.rank)).sum()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.grads_dict->torch.distributed.autograd.get_gradients(context_id)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.training_examples->get_training_examples()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.training_examples.dense_features[idx, :]->torch.tensor((x, y))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.examples_per_trainer->int(n / NUM_TRAINERS)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.shutdown_signal->threading.Condition()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.remote_em_rref->torch.distributed.rpc.remote(self.remote_worker_name(), RemoteEM, args=(NUM_EM_ROW, D_SPARSE))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.remote_net_rref->torch.distributed.rpc.remote(self.remote_worker_name(), RemoteNet, args=(D_DENSE + D_SPARSE, D_HID))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.trainer->self.trainer_name(rank)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.num_trainers->len(trainer_rrefs)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.(ddp_grads, non_ddp_grads)->future.wait()
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.net->torch.nn.Linear(2, 3)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.ddp_net->DistributedDataParallel(net)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.model->torch.nn.EmbeddingBag(10, 3, sparse=True)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.ddp_model->DistributedDataParallel(layer2)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.input->torch.LongTensor(10).random_(0, 10)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.offsets->torch.LongTensor([0, 4])
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.remote_layer1->RemoteModule(remote_device='worker0/cpu', module_cls=nn.Linear, args=(10, 7, False))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.layer1->torch.nn.Linear(10, 7, False)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.layer2->torch.nn.Linear(7, 5).cuda(self.rank)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.inputs->torch.rand((10, 10))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.ddp_layer2->DistributedDataParallel(layer2, device_ids=[self.rank])
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.remote_layer3->RemoteModule(remote_device='worker0/cpu', module_cls=nn.Linear, args=(5, 3, False))
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.layer3->torch.nn.Linear(5, 3, False)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.layer4->torch.nn.Linear(3, 1).cuda(self.rank)
A:torch.testing._internal.distributed.ddp_under_dist_autograd_test.ddp_layer4->DistributedDataParallel(layer4, device_ids=[self.rank])
torch.testing._internal.distributed.ddp_under_dist_autograd_test.CommonDdpComparisonTest(RpcAgentTestFixture)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.CommonDdpComparisonTest.get_remote_grads(rref,context_id)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.CommonDdpComparisonTest.trainer_name(self,rank)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.CommonDdpComparisonTest.world_size(self)->int
torch.testing._internal.distributed.ddp_under_dist_autograd_test.CudaDdpComparisonTest(CommonDdpComparisonTest)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.CudaDdpComparisonTest.test_ddp_dist_autograd_local_vs_remote_gpu(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpComparisonTest(CommonDdpComparisonTest)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpComparisonTest._run_test_ddp_comparision(self,simulate_uneven_inputs=False)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpComparisonTest.test_ddp_comparison(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpComparisonTest.test_ddp_comparison_uneven_inputs(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpComparisonTest.test_ddp_dist_autograd_local_vs_remote(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpComparisonTest.test_ddp_dist_autograd_sparse_grads(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpMode(enum.Enum)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest._do_test(self,ddp_mode,simulate_uneven_inputs=False)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest._master_process(self,ddp_mode:DdpMode,simulate_uneven_inputs:bool)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest._remote_worker_process(self,ddp_mode)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest._trainer_process(self,rank:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.do_test_on_master(self,ddp_mode:DdpMode,simulate_uneven_inputs:bool,remote_em_rref:rpc.RRef,remote_net_rref:rpc.RRef)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.remote_worker_name(self)->str
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.test_backward_ddp_inside(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.test_backward_ddp_outside(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.test_backward_ddp_outside_uneven_inputs(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.test_backward_no_ddp(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.trainer_name(self,rank)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.DdpUnderDistAutogradTest.world_size(self)->int
torch.testing._internal.distributed.ddp_under_dist_autograd_test.FeatureSet(NamedTuple)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.HybridModel(self,remote_em_rref:rpc.RRef,remote_net_rref:rpc.RRef,process_group_for_ddp:dist.ProcessGroup=None)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.HybridModel.__init__(self,remote_em_rref:rpc.RRef,remote_net_rref:rpc.RRef,process_group_for_ddp:dist.ProcessGroup=None)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.HybridModel.forward(self,input:FeatureSet)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.RemoteEM(self,num_embeddings:int,embedding_dim:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.RemoteEM.__init__(self,num_embeddings:int,embedding_dim:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.RemoteEM.forward(self,input:torch.Tensor)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.RemoteNet(self,d_in:int,d_out:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.RemoteNet.__init__(self,d_in:int,d_out:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.RemoteNet.forward(self,input:torch.Tensor)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.Trainer(self,remote_em_rref:rpc.RRef,remote_net_rref:rpc.RRef,ddp_mode:DdpMode,rank:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.Trainer.__init__(self,remote_em_rref:rpc.RRef,remote_net_rref:rpc.RRef,ddp_mode:DdpMode,rank:int)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.Trainer.destroy_pg(self)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.Trainer.train_batch(self,mini_batch:FeatureSet,trainer_has_less_inputs:bool,simulate_uneven_inputs:bool)
torch.testing._internal.distributed.ddp_under_dist_autograd_test._call_method(method,rref,*args,**kwargs)
torch.testing._internal.distributed.ddp_under_dist_autograd_test._remote_method(method,rref,*args,**kwargs)
torch.testing._internal.distributed.ddp_under_dist_autograd_test._remote_method_async(method,rref,*args,**kwargs)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.getLinear(d_in,d_out)
torch.testing._internal.distributed.ddp_under_dist_autograd_test.get_training_examples()
torch.testing._internal.distributed.ddp_under_dist_autograd_test.init_logger()
torch.testing._internal.distributed.ddp_under_dist_autograd_test.set_shutdown_signal()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/pipeline/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/_shard/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/_shard/sharded_tensor/_test_st_common.py----------------------------------------
torch.testing._internal.distributed._shard.sharded_tensor._test_st_common._chunk_sharding_specs_list_for_test(sharding_dims,seed=0)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/_shard/sharded_tensor/_test_ops_common.py----------------------------------------
A:torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.sharding_dim_size->local_weight.size(sharded_dim)
A:torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.split_size->get_split_size(sharding_dim_size, gpu_num)
A:torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.chunk_size->get_chunked_dim_size(sharding_dim_size, split_size, idx)
A:torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.tensor->getattr(module, param_name)
torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.clone_module_parameter(module,param_name)
torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.generate_chunk_sharding_specs_for_test(sharding_dim)
torch.testing._internal.distributed._shard.sharded_tensor._test_ops_common.generate_local_weight_sharding_params_for_test(local_weight,sharded_dim,gpu_num,spec,rank)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/_shard/sharded_tensor/__init__.py----------------------------------------
A:torch.testing._internal.distributed._shard.sharded_tensor.__init__.rpc_backend_options->torch.distributed.rpc.TensorPipeRpcBackendOptions()
A:torch.testing._internal.distributed._shard.sharded_tensor.__init__.st1_local_shards->st1.local_shards()
A:torch.testing._internal.distributed._shard.sharded_tensor.__init__.st2_local_shards->st2.local_shards()
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase(MultiProcessTestCase)
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.assert_sharded_tensor_equal(self,st1,st2)
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.destroy_comms(self,destroy_rpc=True)
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.init_comms(self,init_rpc=True,backend='nccl')
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.init_pg(self,backend='nccl')
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.init_rpc(self)
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.setUp(self)->None
torch.testing._internal.distributed._shard.sharded_tensor.__init__.ShardedTensorTestBase.world_size(self)
torch.testing._internal.distributed._shard.sharded_tensor.__init__.with_comms(func=None,init_rpc=True,backend='nccl')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py----------------------------------------
torch.testing._internal.distributed.rpc.tensorpipe_rpc_agent_test_fixture.TensorPipeRpcAgentTestFixture(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.tensorpipe_rpc_agent_test_fixture.TensorPipeRpcAgentTestFixture.get_shutdown_error_regex(self)
torch.testing._internal.distributed.rpc.tensorpipe_rpc_agent_test_fixture.TensorPipeRpcAgentTestFixture.get_timeout_error_regex(self)
torch.testing._internal.distributed.rpc.tensorpipe_rpc_agent_test_fixture.TensorPipeRpcAgentTestFixture.rpc_backend(self)
torch.testing._internal.distributed.rpc.tensorpipe_rpc_agent_test_fixture.TensorPipeRpcAgentTestFixture.rpc_backend_options(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/dist_autograd_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.dist_autograd_test.known_context_ids->set()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.requires_grad_tensor->torch.ones(3, 3, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.grads->torch.distributed.autograd.get_gradients(context_id)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.x->remote_layer.rpc_sync().forward(x)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.grad->grad.to_dense().to_dense()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.tensor->torch.ones(3, 3, requires_grad=i % 2 == 0)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ret->self._verify_backwards(exec_mode, [t6.sum()], context_id, local_grads, t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t1->torch.rand(10, device=self.rank, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.start->time.time()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.context_id_to_raised->set()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.loss->torch.distributed.rpc.remote(dst, DistAutogradTest._slow_add, args=(t1, t2)).to_here().sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.fut->torch.distributed.rpc.rpc_async(worker_name(dst), method, args=args)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.nargs->len(args)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t2->torch.rand(10, device=self.rank, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ctx->torch.distributed.autograd._current_context()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.send_functions->torch.distributed.autograd._current_context()._send_functions()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.recv_functions->torch.distributed.autograd._current_context()._recv_functions()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.dst_rank->self._next_rank()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.worker_ids->torch.distributed.autograd._current_context()._known_worker_ids()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.success->_all_contexts_cleaned_up()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.loss_local->loss_local.sum().sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_ret->torch.add(t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.rref_t1->torch.distributed.rpc.remote(worker_name(self.rank), create_ref_fn, args=())
A:torch.testing._internal.distributed.rpc.dist_autograd_test.rref->torch.distributed.rpc.remote(dst, DistAutogradTest._slow_add, args=(t1, t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_t1->torch.distributed.rpc.remote(worker_name(self.rank), create_ref_fn, args=()).to_here()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.val->torch.distributed.rpc.rpc_sync(worker_name(self._next_rank()), torch.div, args=(val, t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.s1->self._exec_func(exec_mode, torch.stack, (t4, val))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.s2->self._exec_func(exec_mode, torch.stack, (t5, val))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_grads->self._verify_backwards(exec_mode, [loss], context_id, local_grads, t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.forward_ret->self._exec_func(exec_mode, my_script_add, t1, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.callee->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.rref_owner->worker_name((self.rank + 2) % self.world_size)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.dst->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.remote_embedding->torch.distributed.rpc.remote(worker_name(dst), torch.nn.EmbeddingBag, args=(16, 16), kwargs={'mode': 'sum', 'sparse': True})
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_embedding->torch.nn.EmbeddingBag(16, 16, mode='sum', sparse=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.input->torch.rand([1000, 10000], device=self.rank, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.per_sample_weights->torch.rand(8, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.offsets->torch.tensor([0, 4])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_res->local_embedding(input, offsets, per_sample_weights)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.res->torch.distributed.rpc.rpc_sync(dst, torch.add, args=(t1, t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.remote_grad->torch.distributed.rpc.rpc_sync(worker_name(dst), DistAutogradTest._get_grad, args=(remote_embedding, context_id))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.send_event->get_event('SendRpcBackward')
A:torch.testing._internal.distributed.rpc.dist_autograd_test.recv_event->get_event('RecvRpcBackward')
A:torch.testing._internal.distributed.rpc.dist_autograd_test.backward_event->get_event('torch::distributed::autograd::backward')
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t3->self._exec_func(exec_mode, torch.add, t2, t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t4->torch.distributed.rpc.rpc_sync('worker0', torch.mul, args=(t2, t3))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.s->self._exec_func(exec_mode, torch.stack, (t1, t2, t3))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t->torch.rand(10, 10, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.tensor_list->self._exec_func(exec_mode, torch.split, t, 2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.shutdown_error_regex->self.get_shutdown_error_regex()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r1->self._exec_func(exec_mode, torch.add, t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r2->self._exec_func(exec_mode, torch.mul, t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r3->self._exec_func(exec_mode, torch.cos, t1).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r4->self._exec_func(exec_mode, torch.div, t1, t2).sum()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t5->self._exec_func(exec_mode, torch.add, t4.cpu(), t2)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.store->torch.distributed.distributed_c10d._get_default_store()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.context->torch.distributed.autograd._new_context()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.context_id->torch.distributed.autograd._new_context()._context_id()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.embedding->embedding_rref.local_value()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.grad_map->torch.distributed.autograd.get_gradients(context_id)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.debug_info->torch.distributed.autograd._get_debug_info()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.backward_passes->int(debug_info['num_current_backward_passes'])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.res[i + 1]->torch.distributed.rpc.rpc_sync(worker_name(rank), torch.add, args=(res[i], t2))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.num_autograd_context->int(debug_info['num_autograd_contexts'])
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t6->torch.distributed.rpc.rpc_sync('worker0', torch.add, args=(t4, t5))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.MyFunc.static_grad_ptr->grad.to_dense().to_dense()._values().data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.MyFuncSingleGrad.static_grad_ptr->grad.to_dense().to_dense().data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ctx.size->inp1.size()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.a->torch.randn(10, 3, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.b->torch.randn(10, 3, requires_grad=True)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.p_a->grads[a]._values().data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.p_b->grads[b]._values().data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.v->torch.rand(1, 3)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.i->torch.ones(1, 1, dtype=torch.long)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.nv->torch.rand(1, 3).expand(8, 3)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ni->torch.ones(1, 1, dtype=torch.long).expand(1, 8)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.ngrad->torch.sparse.FloatTensor(ni, nv, torch.Size([10, 3]))
A:torch.testing._internal.distributed.rpc.dist_autograd_test.NonContGradFunc.static_grad_ptr->torch.sparse.FloatTensor(ni, nv, torch.Size([10, 3]))._values().data_ptr()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.emb_matrix->MyFunc.apply(a)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.MyFunc.static_grad_indices_ref->grad.to_dense().to_dense()._indices()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.MyFunc.static_grad_values_ref->grad.to_dense().to_dense()._values()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.t7->self._exec_func(exec_mode, torch.add, t6.cpu(), t5)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.self.model->model.to(device)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.remote_compute->torch.distributed.rpc.remote(dst, TensorPipeCudaDistAutogradTest.MyRemoteCompute)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_compute->TensorPipeCudaDistAutogradTest.MyLocalCompute(remote_compute)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.r->random.random()
A:torch.testing._internal.distributed.rpc.dist_autograd_test.result->local_compute(input)
A:torch.testing._internal.distributed.rpc.dist_autograd_test.local_model->torch.nn.Sequential(*local_layers)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_different_dtypes(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_multiple_round_trips(self,t1,t2,t3,t4,t5,local_grads,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_no_grad_on_tensor(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_rref(self,callee,rref_owner,t1,t2,local_grads,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_simple(self,dst,t1,t2,local_grads,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_simple_python_udf(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backward_simple_script_call(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._backwards_nested_python_udf(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._check_rpc_done(self,rank_distance)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._exec_func(self,exec_mode,method,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._exec_func_with_dst(self,dst,exec_mode,method,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._mixed_requires_grad(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._multiple_backward(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._nested_backward_accumulate_grads(self,t1,t2,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._next_rank(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._test_graph(self,fn,exec_mode,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._test_graph_for_py_nested_call(self,exec_mode,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._test_graph_for_py_nested_call_itself(self,exec_mode,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._test_no_graph_with_tensors_not_require_grad(self,exec_mode,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._test_rpc_complex_args(self,exec_mode,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._test_trainer_ps(self,create_ref_fn,trainer_fn,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._verify_backwards(self,exec_mode,tensors,context_id,local_grads,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._verify_backwards_remote(self,tensors,context_id,local_grads,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._verify_graph_for_first_rpc_call(self,send_function,recv_function,t1,t2,ret)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._verify_graph_for_nested_rpc_call(self,ctx)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest._verify_graph_for_rpc_call_exec(self,send_function)
torch.testing._internal.distributed.rpc.dist_autograd_test.CommonDistAutogradTest.context_cleanup_test_helper(self,rpc_args,func,nested=False)
torch.testing._internal.distributed.rpc.dist_autograd_test.CudaDistAutogradTest(CommonDistAutogradTest)
torch.testing._internal.distributed.rpc.dist_autograd_test.CudaDistAutogradTest.test_gpu_simple(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.CudaDistAutogradTest.test_gpu_to_cpu_continuation(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.CudaDistAutogradTest.test_gpu_to_cpu_continuation_gpu_root(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest(CommonDistAutogradTest)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.MyBackwardFunc(Function)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.MyBackwardFunc.backward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.MyBackwardFunc.forward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.TestDebugInfoFunc(Function)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.TestDebugInfoFunc.backward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.TestDebugInfoFunc.forward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._call_remote_embedding(cls,embedding_rref,input,offsets,per_sample_weights)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._complex_python_udf(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._get_grad(cls,embedding_rref,context_id)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._mixed_requires_grad_operaton(cls,t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._nested_python_udf(t1,t2,dst)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._nested_rpc_call_backward_error(t1,t2,dst)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._python_udf_with_backward_error(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._run_test_backward_unused_send_function_in_thread(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._slow_add(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_grad_only_on_return_value(self,exec_mode)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._test_nested_backward_accumulate_grads(t1,t2,dst_rank)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest._workload_thread()
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_async_dist_autograd(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_autograd_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_accumulate_grads(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_autograd_engine_error(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_complex_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_different_dtypes(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_different_tensor_dims(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_invalid_args(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_multiple_output_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_multiple_roots(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_multiple_round_trips(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_no_grad_on_tensor(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_node_failure(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_node_failure_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_python_udf_error(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_rref(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_rref_multi(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_rref_nested(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple_script_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_simple_self(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_unused_send_function(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_unused_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_verify_hooks(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_without_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backward_without_rpc(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_backwards_nested_python_udf(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_clean_context_during_backward(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_nested_rpc(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_no_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_tensor_no_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_context_cleanup_tensor_with_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_debug_info(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_dist_autograd_profiling(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_error_in_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_grad_copy_sparse_indices_extra_ref(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_grad_only_on_return_value(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_grad_only_on_return_value_remote(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_builtin_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_builtin_remote_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_call_itself(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_remote_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_py_nested_remote_call_itself(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_python_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_graph_for_python_remote_call(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_mixed_requires_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_multiple_backward(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_multiple_backward_with_errors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_nested_backward_accumulate_grads(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_nested_context(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_grad_copy(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_grad_copy_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_graph_with_tensors_not_require_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_no_graph_with_tensors_not_require_grad_remote(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_post_hooks(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_remote_complex_args(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_rpc_complex_args(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_thread_local_context_id(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_trainer_ps(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_trainer_ps_torchscript_functions(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.DistAutogradTest.test_worker_ids_recorded(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.ExecMode(Enum)
torch.testing._internal.distributed.rpc.dist_autograd_test.FaultyAgentDistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.dist_autograd_test.FaultyAgentDistAutogradTest.context_cleanup_test_helper(self,rpc_args,func)
torch.testing._internal.distributed.rpc.dist_autograd_test.FaultyAgentDistAutogradTest.test_context_cleanup_tensor_with_grad(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.FaultyAgentDistAutogradTest.test_verify_backend_options(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.SimulateBackwardError(Function)
torch.testing._internal.distributed.rpc.dist_autograd_test.SimulateBackwardError.backward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.SimulateBackwardError.forward(ctx,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest(CommonDistAutogradTest)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_different_dtypes_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_multiple_round_trips_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_no_grad_on_tensor_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_rref_multi_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_rref_nested_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_rref_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_simple_python_udf_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_simple_script_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_simple_self_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backward_simple_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_backwards_nested_python_udf_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_context_cleanup_nested_rpc_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_context_cleanup_tensor_no_grad_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_context_cleanup_tensor_with_grad_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_embedding_bag_with_no_grad_tensors(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_builtin_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_builtin_remote_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_py_nested_call_itself_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_py_nested_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_py_nested_remote_call_itself_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_py_nested_remote_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_python_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_graph_for_python_remote_call_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_mixed_requires_grad_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_multiple_backward_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_nested_backward_accumulate_grads_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_no_graph_with_tensors_not_require_grad_remote_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_no_graph_with_tensors_not_require_grad_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_remote_complex_args_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_rpc_complex_args_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeAgentDistAutogradTest.test_trainer_ps_sparse(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.MyLocalCompute(self,next_stage)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.MyLocalCompute.__init__(self,next_stage)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.MyLocalCompute.forward(self,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.MyRemoteCompute(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.MyRemoteCompute.__init__(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.MyRemoteCompute.forward(self,input)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.test_device_maps_backward_pass(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.test_dist_autograd_sync_streams(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.TensorPipeCudaDistAutogradTest.test_gradients_synchronizations(self)
torch.testing._internal.distributed.rpc.dist_autograd_test.WrapperModule(self,model,device)
torch.testing._internal.distributed.rpc.dist_autograd_test.WrapperModule.__init__(self,model,device)
torch.testing._internal.distributed.rpc.dist_autograd_test.WrapperModule.forward(self,*args)
torch.testing._internal.distributed.rpc.dist_autograd_test.WrapperModule.gradients(self,ctx_id)
torch.testing._internal.distributed.rpc.dist_autograd_test._all_contexts_cleaned_up(timeout_seconds=10)
torch.testing._internal.distributed.rpc.dist_autograd_test._check_rpc_done(rank_distance)
torch.testing._internal.distributed.rpc.dist_autograd_test._compare_owner_value(context_id,rref,grad)
torch.testing._internal.distributed.rpc.dist_autograd_test._run_trainer(rref_t1,t2,ps,rank_diff,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test._run_trainer_torchscript(rref_t1,t2,ps,rank_diff,sparse)
torch.testing._internal.distributed.rpc.dist_autograd_test._set_rpc_done(ctx_id,rank_distance)
torch.testing._internal.distributed.rpc.dist_autograd_test._torch_ones(sizes,requires_grad=False)
torch.testing._internal.distributed.rpc.dist_autograd_test.build_sparse_tensor(coalesce=False,requires_grad=True,dtype=torch.float32)
torch.testing._internal.distributed.rpc.dist_autograd_test.create_tensor()
torch.testing._internal.distributed.rpc.dist_autograd_test.create_torchscript_tensor()->torch.Tensor
torch.testing._internal.distributed.rpc.dist_autograd_test.my_nested_rref_add(dst,rref_t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_py_add(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_py_nested_call(t1,t2,dst,world_size,hops)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_rref_add(rref_t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_scalar_add(a,b)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_script_add(t1,t2)
torch.testing._internal.distributed.rpc.dist_autograd_test.my_script_ref_add(ref_t1:RRef[torch.Tensor],t2:torch.Tensor)->torch.Tensor
torch.testing._internal.distributed.rpc.dist_autograd_test.ret_requires_grad()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/dist_optimizer_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.lock->threading.Lock()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.g_cpu->torch.Generator()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.self.w->torch.rand((3, 3), requires_grad=requires_grad, generator=g_cpu)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_module1->torch.distributed.rpc.remote(owner1, MyModule)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_module2->torch.distributed.rpc.remote(owner2, MyModule, args=(False,))
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_param1->torch.distributed.rpc.remote(owner1, MyModule).remote().get_w()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_param2->torch.distributed.rpc.remote(owner2, MyModule, args=(False,)).remote().get_w()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.dist_optim->DistributedOptimizer(optim_cls, [remote_param1, remote_param2], *args, **kwargs)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.t1->torch.rand((3, 3), requires_grad=True, generator=g_cpu)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.t2->torch.rand((3, 3), requires_grad=True, generator=g_cpu)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.output1->torch.distributed.rpc.remote(owner1, MyModule).rpc_async().forward(t2)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.output2->torch.distributed.rpc.remote(owner2, MyModule, args=(False,)).rpc_async().forward(output1.wait())
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.loss->torch.add(output2.wait(), t1)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.module1->MyModule()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.module2->MyModule(requires_grad=False)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.local_optim->optim_cls(params, *args, **kwargs)
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.old_w1->MyModule().w.clone().detach()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.old_w2->MyModule(requires_grad=False).w.clone().detach()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.old_w1_remote->torch.distributed.rpc.remote(owner1, MyModule).remote().get_w().to_here()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.new_w1->torch.distributed.rpc.remote(owner1, MyModule).rpc_async().get_w().wait()
A:torch.testing._internal.distributed.rpc.dist_optimizer_test.new_w2->torch.distributed.rpc.remote(owner2, MyModule, args=(False,)).rpc_async().get_w().wait()
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest._test_dist_optim_base(self,optim_cls,*args,**kwargs)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest._test_dist_optim_none_grads(self,optim_cls,*args,**kwargs)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim_exception(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim_exception_on_constructor(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.DistOptimizerTest.test_dist_optim_none_grads(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.FailingOptimizer(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.FailingOptimizer.__init__(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.FailingOptimizer.step(self,closure=None)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule(self,requires_grad=True)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule.__init__(self,requires_grad=True)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule.forward(self,t1)
torch.testing._internal.distributed.rpc.dist_optimizer_test.MyModule.get_w(self)
torch.testing._internal.distributed.rpc.dist_optimizer_test.OptimizerFailingOnConstructor(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.OptimizerFailingOnConstructor.__init__(self,params)
torch.testing._internal.distributed.rpc.dist_optimizer_test.OptimizerFailingOnConstructor.step(self,closure=None)
torch.testing._internal.distributed.rpc.dist_optimizer_test._call_method(method,obj_rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.dist_optimizer_test.remote_method(method,obj_rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.dist_optimizer_test.rpc_async_method(method,obj_rref,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py----------------------------------------
A:torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.use_tcp_init->os.environ.get('RPC_INIT_WITH_TCP', None)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture(ABC)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.file_init_method(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.get_shutdown_error_regex(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.get_timeout_error_regex(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.init_method(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.rpc_backend(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.rpc_backend_options(self)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.setup_fault_injection(self,faulty_messages,messages_to_delay)
torch.testing._internal.distributed.rpc.rpc_agent_test_fixture.RpcAgentTestFixture.world_size(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/rpc_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.rpc_test.t->torch.zeros((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.VALUE_FUTURE->concurrent.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.DONE_FUTURE->concurrent.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.TensorClass->namedtuple('TensorClass', ['tensors'])
A:torch.testing._internal.distributed.rpc.rpc_test.(pickled_python_udf, tensors)->torch.distributed.rpc.internal._internal_rpc_pickler.serialize(PythonUDF(my_tensor_function, (torch.ones(2, 2), torch.ones(2, 2)), None))
A:torch.testing._internal.distributed.rpc.rpc_test.python_udf->torch.distributed.rpc.internal._internal_rpc_pickler.deserialize(obj[0], obj[1])
A:torch.testing._internal.distributed.rpc.rpc_test.result->rref_api(timeout=timeout).my_instance_method(torch.ones(2, 2))
A:torch.testing._internal.distributed.rpc.rpc_test.tensor->torch.zeros((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.a->torch.ones((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.current_dst->worker_name(dst)
A:torch.testing._internal.distributed.rpc.rpc_test.rref->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),))
A:torch.testing._internal.distributed.rpc.rpc_test.ret_rref->torch.distributed.rpc.remote(worker_name(dst_rank), check_rref_confirmed, args=(rref,))
A:torch.testing._internal.distributed.rpc.rpc_test.out->torch.distributed.rpc.remote(out_relay, TensorPipeAgentCudaRpcTest._rref_relay, args=(rref_out,)).to_here()
A:torch.testing._internal.distributed.rpc.rpc_test.fut->torch.distributed.rpc.rpc_async(dst, async_cuda_nested_add, args=(nested_dst, a, b, c))
A:torch.testing._internal.distributed.rpc.rpc_test.fut2->torch.distributed.rpc.rpc_async(dst_worker_cuda_1, udf_with_torch_ops, args=(1,))
A:torch.testing._internal.distributed.rpc.rpc_test.fut1->torch.distributed.rpc.rpc_async(dst_worker_cuda_0, udf_with_torch_ops, args=(0,))
A:torch.testing._internal.distributed.rpc.rpc_test.x->torch.randn(200, 1, 28, 28).to(local_device)
A:torch.testing._internal.distributed.rpc.rpc_test.y->torch.ones(2).to(0)
A:torch.testing._internal.distributed.rpc.rpc_test.lock->Lock()
A:torch.testing._internal.distributed.rpc.rpc_test.ret_future->torch.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.original_stream->torch.cuda.current_stream(device)
A:torch.testing._internal.distributed.rpc.rpc_test.new_stream->torch.cuda.Stream(device)
A:torch.testing._internal.distributed.rpc.rpc_test.self.lock->Lock()
A:torch.testing._internal.distributed.rpc.rpc_test.self.event->torch.cuda.Event(enable_timing=True)
A:torch.testing._internal.distributed.rpc.rpc_test.ret_fut->torch.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.self.eb->torch.nn.EmbeddingBag(10, 10, sparse=sparse)
A:torch.testing._internal.distributed.rpc.rpc_test.self->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).local_value()
A:torch.testing._internal.distributed.rpc.rpc_test.self_worker_info->torch.distributed.rpc.get_worker_info()
A:torch.testing._internal.distributed.rpc.rpc_test.ret->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).rpc_sync().sum()
A:torch.testing._internal.distributed.rpc.rpc_test.tik->time.time()
A:torch.testing._internal.distributed.rpc.rpc_test.tok->time.time()
A:torch.testing._internal.distributed.rpc.rpc_test.rref_a->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(n, n), 2))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_b->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(n, n), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_c->torch.distributed.rpc.remote(worker_name(dst_rank), my_rref_function, args=(rref_a, rref_b))
A:torch.testing._internal.distributed.rpc.rpc_test.c->torch.ones((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.rref_of_rrefs->torch.distributed.rpc.remote(worker_name(dst_rank1), f, args=(worker_name(dst_rank2),))
A:torch.testing._internal.distributed.rpc.rpc_test.rrefs->torch.distributed.rpc.remote(worker_name(dst_rank1), f, args=(worker_name(dst_rank2),)).to_here()
A:torch.testing._internal.distributed.rpc.rpc_test.ps_rref->RRef(MyParameterServer(self.world_size - 1))
A:torch.testing._internal.distributed.rpc.rpc_test.future->Future(devices=['cuda:0'])
A:torch.testing._internal.distributed.rpc.rpc_test.stream->torch.cuda.Stream()
A:torch.testing._internal.distributed.rpc.rpc_test.another_stream->torch.cuda.Stream('cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.add_tensor->torch.ones((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.expected_tensor->(tensor + add_tensor).coalesce()
A:torch.testing._internal.distributed.rpc.rpc_test.peer_worker_info->torch.distributed.rpc.get_worker_info(worker_name(peer_rank))
A:torch.testing._internal.distributed.rpc.rpc_test.unknown_worker_id->torch.distributed.rpc.get_worker_info('WorkerUnknown')
A:torch.testing._internal.distributed.rpc.rpc_test.worker_infos->torch.distributed.rpc.api._get_current_rpc_agent().get_worker_infos()
A:torch.testing._internal.distributed.rpc.rpc_test.expected_worker_ids->set(range(self.world_size))
A:torch.testing._internal.distributed.rpc.rpc_test.self_worker_name->worker_name(self.rank)
A:torch.testing._internal.distributed.rpc.rpc_test.dst->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.proxy_rpc_sync->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).rpc_sync()
A:torch.testing._internal.distributed.rpc.rpc_test.proxy_rpc_async->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).rpc_async()
A:torch.testing._internal.distributed.rpc.rpc_test.proxy_remote->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).remote()
A:torch.testing._internal.distributed.rpc.rpc_test.expected->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).rpc_sync().forward(rref_input, True)
A:torch.testing._internal.distributed.rpc.rpc_test.backend->torch.distributed.rpc.backend_registry.register_backend(backend_name, _stub_construct_rpc_backend_options_handler, _stub_init_rpc_backend_handler)
A:torch.testing._internal.distributed.rpc.rpc_test.(store, _, _)->next(torch.distributed.rendezvous(self.init_method, rank=self.rank, world_size=self.world_size))
A:torch.testing._internal.distributed.rpc.rpc_test.self.lin->torch.nn.Linear(3, 4)
A:torch.testing._internal.distributed.rpc.rpc_test.model->torch.distributed.rpc.remote('w0', torch.nn.Linear, (2048, 20000)).remote().to(remote_device)
A:torch.testing._internal.distributed.rpc.rpc_test.info->torch.distributed.rpc.api._get_current_rpc_agent().get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.workder_info->torch.distributed.rpc.get_worker_info(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.rpc_test.results->torch.distributed.rpc.api._all_gather(info.id)
A:torch.testing._internal.distributed.rpc.rpc_test.expected_error->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).local_value().get_timeout_error_regex()
A:torch.testing._internal.distributed.rpc.rpc_test.names->sorted(names)
A:torch.testing._internal.distributed.rpc.rpc_test.all_worker_info->torch.distributed.rpc._get_current_rpc_agent().get_worker_infos()
A:torch.testing._internal.distributed.rpc.rpc_test.th->threading.Thread(target=self._test_barrier_helper, args=(info, names, True))
A:torch.testing._internal.distributed.rpc.rpc_test.value->concurrent.futures.Future().result()
A:torch.testing._internal.distributed.rpc.rpc_test.rpc_profiling_key->_build_rpc_profiling_key(RPCExecMode.ASYNC, udf_with_torch_ops.__qualname__, worker_name(self.rank), worker_name(dst))
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker->worker_name(next_rank)
A:torch.testing._internal.distributed.rpc.rpc_test.res->torch.distributed.rpc.rpc_sync(dst, torch.add, (torch.ones(i, i), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.event_cpu_mem_usages->set((event.cpu_memory_usage for event in function_events))
A:torch.testing._internal.distributed.rpc.rpc_test.trace->json.load(f)
A:torch.testing._internal.distributed.rpc.rpc_test.event_exists->any([expected_event_name in event_name for event_name in event_names])
A:torch.testing._internal.distributed.rpc.rpc_test.remote_event_name_set->set(EXPECTED_REMOTE_EVENTS)
A:torch.testing._internal.distributed.rpc.rpc_test.rpc_event->get_function_event(events, torch._jit_internal._qualified_name(my_script_func))
A:torch.testing._internal.distributed.rpc.rpc_test.dst1->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.dst2->worker_name((self.rank + 2) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.key_prefix->_build_rpc_profiling_key(RPCExecMode.ASYNC, slow_async_add.__qualname__, worker_name(self.rank), dst1)
A:torch.testing._internal.distributed.rpc.rpc_test.nested_rpc_key_prefix->_build_rpc_profiling_key(RPCExecMode.ASYNC, slow_add.__qualname__, dst1, dst2)
A:torch.testing._internal.distributed.rpc.rpc_test.remote_children->get_cpu_children(record_function_remote_event)
A:torch.testing._internal.distributed.rpc.rpc_test.local_children->get_cpu_children(local_record_function_event)
A:torch.testing._internal.distributed.rpc.rpc_test.local_name->convert_remote_to_local(evt.name)
A:torch.testing._internal.distributed.rpc.rpc_test.scope_event->get_function_event(events, 'foo')
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_name->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.foo_event_ix->next((i for (i, event) in enumerate(events) if 'foo' in event.name))
A:torch.testing._internal.distributed.rpc.rpc_test.rpc_event_idx->next((i for (i, event) in enumerate(events) if rpc_exec_mode.value in event.name))
A:torch.testing._internal.distributed.rpc.rpc_test.top_level_event_names->sorted(top_level_event_names)
A:torch.testing._internal.distributed.rpc.rpc_test.expected_top_level_event_names->sorted(expected_top_level_event_names)
A:torch.testing._internal.distributed.rpc.rpc_test.outer_profile_rref->torch.distributed.rpc.remote(dst_worker_name, rpc._server_process_global_profile)
A:torch.testing._internal.distributed.rpc.rpc_test.inner_profile_rref->torch.distributed.rpc.remote(dst_worker_name, rpc._server_process_global_profile)
A:torch.testing._internal.distributed.rpc.rpc_test.inner_events->torch.distributed.rpc.rpc_sync(dst_worker_name, get_events_from_profile, (inner_profile_rref,))
A:torch.testing._internal.distributed.rpc.rpc_test.outer_events->torch.distributed.rpc.rpc_sync(dst_worker_name, get_events_from_profile, (outer_profile_rref,))
A:torch.testing._internal.distributed.rpc.rpc_test.key->_build_rpc_profiling_key(RPCExecMode.ASYNC, torch._jit_internal._qualified_name(my_script_func), 'worker1', 'worker0')
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_info->torch.distributed.rpc.get_worker_info(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.rpc_test.b->torch.ones((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.m->MyEmbeddingBagModel(sparse=sparse)
A:torch.testing._internal.distributed.rpc.rpc_test.stderr_lines->err.getvalue()
A:torch.testing._internal.distributed.rpc.rpc_test.msg->str(e)
A:torch.testing._internal.distributed.rpc.rpc_test.local_rref->RRef(35)
A:torch.testing._internal.distributed.rpc.rpc_test.rref_list->torch.distributed.rpc.rpc_sync(worker_name(dst_rank), get_rref_list, args=([1, 2, 3],))
A:torch.testing._internal.distributed.rpc.rpc_test.expected_type->type(torch.ones(2))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_type->rref_type.wait().wait()
A:torch.testing._internal.distributed.rpc.rpc_test.other_a->torch.distributed.rpc.remote(worker_name(other_rank), torch.add, args=(torch.ones(1), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.other_b->torch.distributed.rpc.remote(worker_name(other_rank), torch.add, args=(torch.ones(1), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.rref1->RRef(self.rank)
A:torch.testing._internal.distributed.rpc.rpc_test.rref2->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(2, 2), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.rref3->torch.distributed.rpc.remote(worker_name(dst_rank), torch.add, args=(torch.ones(2, 2), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_info->_rref_context_get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.agent_info->torch.distributed.rpc.api._get_current_rpc_agent().get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.autograd_info->torch.distributed.autograd._get_debug_info()
A:torch.testing._internal.distributed.rpc.rpc_test.error_str->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).local_value().get_shutdown_error_regex()
A:torch.testing._internal.distributed.rpc.rpc_test.dist_initialized->torch.distributed.is_initialized()
A:torch.testing._internal.distributed.rpc.rpc_test.set_timeout->torch.distributed.rpc.get_rpc_timeout()
A:torch.testing._internal.distributed.rpc.rpc_test.test_pickler->TestPickler()
A:torch.testing._internal.distributed.rpc.rpc_test.RpcTest.timed_out_rpc_event->Event()
A:torch.testing._internal.distributed.rpc.rpc_test.fut3->torch.distributed.rpc.rpc_async(dst, torch.add, args=(torch.ones(2, 2), 1)).then(callback)
A:torch.testing._internal.distributed.rpc.rpc_test.a.rref->torch.distributed.rpc.remote(dst_worker_name, torch.add, args=(torch.ones(n, n), 2))
A:torch.testing._internal.distributed.rpc.rpc_test.t_view->torch.zeros((100,), device='cuda:0').narrow(1, 2, 2)
A:torch.testing._internal.distributed.rpc.rpc_test.t_cont->torch.zeros((100,), device='cuda:0').narrow(1, 2, 2).contiguous()
A:torch.testing._internal.distributed.rpc.rpc_test.t_ret->torch.distributed.rpc.rpc_sync(worker_name(next_rank), non_cont_test, args=(t_view, t_cont))
A:torch.testing._internal.distributed.rpc.rpc_test.set_by_cb->concurrent.futures.Future()
A:torch.testing._internal.distributed.rpc.rpc_test.cb_fut->torch.distributed.rpc.rpc_async(dst, async_cuda_nested_add, args=(nested_dst, a, b, c)).then(my_function)
A:torch.testing._internal.distributed.rpc.rpc_test.fut0->torch.distributed.rpc.rpc_async(dst, torch.add, args=(torch.ones(2, 2), 1))
A:torch.testing._internal.distributed.rpc.rpc_test.fut_then->torch.distributed.rpc.rpc_async(dst, async_cuda_nested_add, args=(nested_dst, a, b, c)).then(lambda _: True)
A:torch.testing._internal.distributed.rpc.rpc_test.rpc_backend_options->torch.distributed.rpc.TensorPipeRpcBackendOptions(init_method=self.rpc_backend_options.init_method, num_worker_threads=self.rpc_backend_options.num_worker_threads, rpc_timeout=timeout)
A:torch.testing._internal.distributed.rpc.rpc_test.t1->torch.rand(10, 10, requires_grad=True)
A:torch.testing._internal.distributed.rpc.rpc_test.t2->torch.distributed.rpc.rpc_sync(dst, torch.add, args=(t1, t1))
A:torch.testing._internal.distributed.rpc.rpc_test.loss_fn->torch.nn.MSELoss()
A:torch.testing._internal.distributed.rpc.rpc_test.outputs->m(torch.rand(10, 10).long())
A:torch.testing._internal.distributed.rpc.rpc_test.gradient->gradient.to_dense().double().to_dense().double()
A:torch.testing._internal.distributed.rpc.rpc_test.ps_gradient->ps_gradient.to_dense().double().to_dense().double()
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_cuda_0->worker_name(dst_cuda_0)
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_cuda_1->worker_name(dst_cuda_1)
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_b->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.dst_worker_c->worker_name((self.rank + 2) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.slow_rref->torch.distributed.rpc.remote(dst, MyClass, args=(torch.ones(2, 2), True))
A:torch.testing._internal.distributed.rpc.rpc_test.default_timeout->torch.distributed.rpc.get_rpc_timeout()
A:torch.testing._internal.distributed.rpc.rpc_test.timeout->torch.distributed.rpc.get_rpc_timeout()
A:torch.testing._internal.distributed.rpc.rpc_test.expected_err->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).local_value().get_timeout_error_regex()
A:torch.testing._internal.distributed.rpc.rpc_test.rref_api->getattr(slow_rref, rref_proxy_api)
A:torch.testing._internal.distributed.rpc.rpc_test.self.net->torch.nn.Sequential(nn.Conv2d(1, 16, 3, 1), nn.ReLU(), nn.Conv2d(16, 32, 3, 1), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(1), nn.Linear(4608, 128), nn.ReLU(), nn.Linear(128, 10)).to(device)
A:torch.testing._internal.distributed.rpc.rpc_test.rets->torch.distributed.rpc.rpc_sync(dst, TensorPipeAgentCudaRpcTest._gpu_add_wrong_gpus, args=(x, y))
A:torch.testing._internal.distributed.rpc.rpc_test.s0->torch.cuda.current_stream(x.device)
A:torch.testing._internal.distributed.rpc.rpc_test.s1->torch.cuda.Stream(device=x.device)
A:torch.testing._internal.distributed.rpc.rpc_test.nested_dst->worker_name((self.rank + 2) % self.world_size)
A:torch.testing._internal.distributed.rpc.rpc_test.actual->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).remote().forward(rref_x, True).to_here()
A:torch.testing._internal.distributed.rpc.rpc_test.input_src->worker_name(0)
A:torch.testing._internal.distributed.rpc.rpc_test.rref_x->RRef(torch.randn(200, 1, 28, 28).to(local_device))
A:torch.testing._internal.distributed.rpc.rpc_test.model_dst->worker_name(1)
A:torch.testing._internal.distributed.rpc.rpc_test.out_relay->worker_name(2)
A:torch.testing._internal.distributed.rpc.rpc_test.rref_input->RRef(torch.randn(200, 1, 28, 28).to(local_device))
A:torch.testing._internal.distributed.rpc.rpc_test.rref_out->torch.distributed.rpc.remote(dst, TensorWrapper, args=(torch.zeros(42, device='cuda:0'),)).remote().forward(rref_input, True)
A:torch.testing._internal.distributed.rpc.rpc_test.data->torch.rand(2048, 2048).to(local_device)
A:torch.testing._internal.distributed.rpc.rpc_test.output->torch.distributed.rpc.remote('w0', torch.nn.Linear, (2048, 20000)).remote().to(remote_device).rpc_sync().forward(data)
A:torch.testing._internal.distributed.rpc.rpc_test.v0->torch.distributed.rpc.RRef(output).remote().sum().to_here().item()
A:torch.testing._internal.distributed.rpc.rpc_test.v1->torch.distributed.rpc.remote('w0', torch.nn.Linear, (2048, 20000)).remote().to(remote_device).rpc_sync().forward(data).sum().item()
A:torch.testing._internal.distributed.rpc.rpc_test.options->torch.distributed.rpc.TensorPipeRpcBackendOptions(init_method=self.rpc_backend_options.init_method, num_worker_threads=self.rpc_backend_options.num_worker_threads, device_maps={dst: {0: 1}}, devices=[0])
A:torch.testing._internal.distributed.rpc.rpc_test.tensor0->torch.zeros((100,), device='cuda:0')
A:torch.testing._internal.distributed.rpc.rpc_test.tensor1->torch.zeros((100,), device='cuda:1')
A:torch.testing._internal.distributed.rpc.rpc_test.parent_future->Future(devices=['cuda:1'])
A:torch.testing._internal.distributed.rpc.rpc_test.t0->torch.distributed.rpc.rpc_async(dst, async_cuda_nested_add, args=(nested_dst, a, b, c)).value()
A:torch.testing._internal.distributed.rpc.rpc_test.child_future->Future(devices=['cuda:1']).then(cb)
A:torch.testing._internal.distributed.rpc.rpc_test.tensor_list[0]->torch.ones((100,), device='cuda:0')
torch.testing._internal.distributed.rpc.rpc_test.AsyncExecutionClass
torch.testing._internal.distributed.rpc.rpc_test.AsyncExecutionClass.bound_async_add(self,to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.AsyncExecutionClass.class_async_add(cls,to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.AsyncExecutionClass.static_async_add(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.CudaRpcTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.rpc_test.CudaRpcTest.test_profiler_remote_cuda(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest._test_remote_message_delay_timeout(self,func,args,dst=None)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest._test_remote_message_dropped_pickle(self,dst=None)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest._test_remote_message_dropped_timeout(self,func,args,dst=None)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_builtin_remote_message_dropped_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_builtin_remote_message_dropped_timeout_to_self(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_check_failed_messages(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_custom_faulty_messages(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_custom_messages_to_delay(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_no_faulty_messages(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_remote_message_builtin_delay_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_remote_message_builtin_delay_timeout_to_self(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_remote_message_dropped_pickle(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_remote_message_dropped_pickle_to_self(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_remote_message_script_delay_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_remote_message_script_delay_timeout_to_self(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_rpc_builtin_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_rpc_script_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_rref_to_here_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_udf_remote_message_delay_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_udf_remote_message_delay_timeout_to_self(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_udf_remote_message_dropped_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_udf_remote_message_dropped_timeout_to_self(self)
torch.testing._internal.distributed.rpc.rpc_test.FaultyAgentRpcTest.test_verify_backend_options(self)
torch.testing._internal.distributed.rpc.rpc_test.FooBackendOptions(self,init_method)
torch.testing._internal.distributed.rpc.rpc_test.FooBackendOptions.__init__(self,init_method)
torch.testing._internal.distributed.rpc.rpc_test.MyClass(self,a,delay=False)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.__init__(self,a,delay=False)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.get_value(self)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.increment_value(self,increment)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_class_method(cls,d,e)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_instance_method(self,b)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_slow_method(self,my_tensor_arg)
torch.testing._internal.distributed.rpc.rpc_test.MyClass.my_static_method(f)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST(self,device)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.__getstate__(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.__init__(self,device)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.forward(self,x,is_rref=False)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_builtin_remote_ret_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_builtin_remote_self_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_multi_builtin_remote_ret_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_multi_py_udf_remote_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_multi_rpc_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_my_parameter_server_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_nested_remote_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_nested_rpc_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_nested_rref_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_nested_rref_stress_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_py_rpc_rref_args_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_py_rref_args_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_py_rref_args_user_share_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_py_sparse_tensors_in_container(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_self_py_udf_remote_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_self_remote_rref_as_remote_arg_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_self_remote_rref_as_rpc_arg_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_self_remote_rref_as_self_remote_arg_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_self_remote_rref_as_self_rpc_arg_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_send_to_rank_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_stress_heavy_rpc_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_wait_all_workers_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_wait_all_workers_twice_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyConvNetForMNIST.test_world_size_one_sparse(self)
torch.testing._internal.distributed.rpc.rpc_test.MyEmbeddingBagModel(self,sparse)
torch.testing._internal.distributed.rpc.rpc_test.MyEmbeddingBagModel.__init__(self,sparse)
torch.testing._internal.distributed.rpc.rpc_test.MyEmbeddingBagModel.forward(self,x)
torch.testing._internal.distributed.rpc.rpc_test.MyParameterServer(self,trainers)
torch.testing._internal.distributed.rpc.rpc_test.MyParameterServer.__init__(self,trainers)
torch.testing._internal.distributed.rpc.rpc_test.MyParameterServer.average(rref,riteration,tensor)
torch.testing._internal.distributed.rpc.rpc_test.MyParameterServer.get_gradient(rref)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass(self)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.__getstate__(self)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.__init__(self)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.__setstate__(self,obj)
torch.testing._internal.distributed.rpc.rpc_test.MyPickleClass.set(self,val)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest(RpcAgentTestFixture,RpcTestCommon)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._assert_top_level_events(self,process_global_events,expected_top_level_event_names)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._create_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._gpu_tensor_list_arg(tensor_list)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._identity(x)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._multi_args_fn(n,sparse=False)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._multi_kwargs_fn(n,sparse=False)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._profiler_test_with_rpc(self,rpc_exec_mode,func,args,use_record_function=False,dst=None)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._return_gpu_tensor()
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._return_gpu_tensor_list()
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_rpc_profiling_async_function(self,device='cpu')
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_remote_events_profiled(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_async_rpc_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_async_rpc_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_autograd_context(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_remote_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_remote_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_script_async_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_script_remote_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_script_sync_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_sync_rpc_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_test_profiler_with_sync_rpc_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._run_uneven_workload(self,f,x,num_repeat=30)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._slow_add(x,y)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._sum(x)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_async_function(self,fn,mode=RPCExecMode.SYNC)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_async_function_multi(self,fn,mode=RPCExecMode.SYNC)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_async_function_raise(self,mode)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_async_function_wrong_return_type(self,mode)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_barrier_helper(self,info,names,multi_threaded=False)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_future_cb(self,func)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_return_future(self,mode)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_leak(self,_mock_delete_all_user_and_unforked_owner_rrefs,ignore_leak)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_proxy_class(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_proxy_tensor(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_type(self,blocking)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_type_owner(self,blocking)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_rref_type_with_error(self,blocking)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._test_test_async_class_rref_proxy(self,mode=RPCExecMode.SYNC)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest._trainer_func(self,rref,sparse)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.check_profiling_info(self,self_worker_name,dst_worker_name,func,rpc_event,rpc_exec_mode)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.return_callee_id()
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.run_profiling_workload(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_add_done_callback(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_add_with_id(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_all_gather(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_all_gather_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_class_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_class_method_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_class_rref_proxy(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_class_rref_proxy_async(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_class_rref_proxy_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_chained(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_chained_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_multi_chained(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_multi_chained_async(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_multi_chained_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_multi_fanout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_multi_fanout_async(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_multi_fanout_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_nested(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_nested_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_raise(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_raise_async(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_raise_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_simple(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_with_future_ctor(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_with_future_ctor_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_wrong_return_type(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_wrong_return_type_async(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_function_wrong_return_type_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_record_function_cbs_jit_call(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_record_function_double_end_callbacks(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_static_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_async_static_method_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_build_rpc_profiling_key(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_builtin_remote_ret(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_builtin_remote_self(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_call_method_on_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_chain(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_in_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_multi(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_none(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_simple(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_with_error(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_with_ret(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_wrong_arg_num(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_callback_wrong_arg_type(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_cannot_infer_backend_from_options(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_deadlock(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_debug_info(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_default_timeout_used(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_disable_gil_profiling(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_dist_init_decorator(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_duplicate_name(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_duplicate_name_2(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_expected_src(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_function_not_on_callee(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_future_done(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_future_done_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_future_in_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_future_nested_callback(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_future_wait_twice(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_get_worker_infos(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_graceful_shutdown_with_uneven_workload(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_handle_send_exceptions(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_ignore_rref_leak(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_init_pg_then_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_init_rpc_then_pg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_init_rpc_twice(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_int_callee(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_invalid_names(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_rref_no_fork(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_shutdown(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_shutdown_with_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_local_value_not_on_owner(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_mark_future_twice(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_builtin_remote_ret(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_layer_nested_async_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_py_udf_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_multi_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_my_parameter_server(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nested_rref_stress(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_non_cont_tensors(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_non_garbage_collected_user_rref_due_to_local_circular_dependency(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_nonzero(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_owner_equality(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_owner_rref_backward(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_pass_local_rrefs(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_pg_init_no_rpc_init(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_pickle_future(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_export_trace(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_remote_events_profiled(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_remote_events_profiled_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_rpc_key_names(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_rpc_memory(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_rpc_record_shapes(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_async_rpc_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_async_rpc_builtin_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_async_rpc_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_async_rpc_udf_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_autograd_context(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_autograd_context_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_remote_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_remote_builtin_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_remote_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_remote_udf_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_script_async_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_script_async_rpc_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_script_remote_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_script_remote_rpc_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_script_sync_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_script_sync_rpc_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_sync_rpc_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_sync_rpc_builtin_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_sync_rpc_udf(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_profiler_with_sync_rpc_udf_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_built_in(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_constructor(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_instance_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_class_static_method(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_function_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_multi_async_call(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_nested_pickle(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_no_return_result(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_raise_in_user_func(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_raise_in_user_func_escaped_str(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_rpc_rref_args(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_rref_args(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_rref_args_user_share(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_tensors(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_tensors_in_container(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_tensors_multi_async_call(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_udf_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_py_user_defined(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_register_rpc_backend_and_set_and_start_rpc_backend(self,mock_rpc_agent,mock_dist_autograd_init)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_reinit(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_remote_same_worker(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_remote_throw(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_remote_with_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_return_future(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_return_future_async(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_return_future_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_return_local_rrefs(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_barrier_all(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_barrier_multithreaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_barrier_partial_subset(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_barrier_subset(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_profiling_async_function(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_profiling_async_function_single_threaded(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_profiling_remote_record_function(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_return_rref(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rpc_timeouts(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_context_debug_info(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_forward_chain(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_get_future(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_leak(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_proxy_class(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_proxy_class_self(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_proxy_non_exist(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_proxy_reuse(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_proxy_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_proxy_tensor_self(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_py_pickle_not_supported(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_str(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_non_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_owner_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_owner_non_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_slow_init(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_with_error_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_rref_type_with_error_non_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_scalar_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_add(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_py_udf_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_remote_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_rpc_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_self_remote_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_self_remote_rref_as_self_rpc_arg(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_send_to_rank(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_server_process_global_profiler(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_set_and_get_default_rpc_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_shutdown_errors(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_shutdown_followed_by_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_stress_heavy_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_stress_heavy_rpc_torchscript(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_stress_light_rpc(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_use_rpc_pickler(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_use_rref_after_shutdown(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_user_rref_backward(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_user_rrefs_confirmed(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_user_rrefs_confirmed_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_exit_early_builtin(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_exit_early_python(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_exit_early_script_function(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_multiple_call(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_raise_in_body(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_raise_in_user_func(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_with_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_with_partial_exception(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_workers_dense(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_workers_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wait_all_workers_twice_dense(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_worker_id(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_world_size_one(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.test_wrong_types(self)
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.timed_out_rpc()
torch.testing._internal.distributed.rpc.rpc_test.RpcTest.validate_profiling_workload(self,dst,prof)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._builtin_remote_ret(self,x,y,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._builtin_remote_self(self,x,y,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._multi_rpc(self,sparse)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._my_parameter_server(self,sparse)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._nested_remote(self,f,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._nested_rpc(self,f,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._nested_rref(self,f,expected1,expected2)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._nested_rref_stress(self,f,expected1,expected2)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._py_rpc_rref_args(self,a,b,c,x,y,z,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._py_rref_args(self,a,b,x,y,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._py_rref_args_user_share(self,a,b,c,x,y,z,expected)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._run_func_in_mode(self,to,fn,mode,args=None,kwargs=None)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._self_py_udf_remote(self,worker_info,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._self_remote_rref_as_remote_arg(self,dst,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._self_remote_rref_as_rpc_arg(self,dst,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._stress_test_rpc(self,f,repeat=1000,args=())
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._test_cuda_future_extraction(self,wrapper,unwrapper,sparse_tensor)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._test_multi_remote_call(self,fn,sparse,args_fn=lambdax,y:(),kwargs_fn=lambdax,y:{})
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._wait_all_workers(self,f,x)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._wait_all_workers_twice(self,f,x)
torch.testing._internal.distributed.rpc.rpc_test.RpcTestCommon._world_size_one(self,a,b)
torch.testing._internal.distributed.rpc.rpc_test.SlowPickleClass(self,t)
torch.testing._internal.distributed.rpc.rpc_test.SlowPickleClass.__getstate__(self)
torch.testing._internal.distributed.rpc.rpc_test.SlowPickleClass.__init__(self,t)
torch.testing._internal.distributed.rpc.rpc_test.SlowPickleClass.__setstate__(self,obj)
torch.testing._internal.distributed.rpc.rpc_test.StubRpcAgent(self,world_size)
torch.testing._internal.distributed.rpc.rpc_test.StubRpcAgent.__init__(self,world_size)
torch.testing._internal.distributed.rpc.rpc_test.StubRpcAgent.get_worker_infos(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest(RpcAgentTestFixture,RpcTestCommon)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._add_to_gpu(x,y)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._gpu_add(x,y)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._gpu_add_given_devices(x,y,x_to,y_to,z_to)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._gpu_add_multi_gpu(x,y)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._gpu_add_return_to_gpu(x,y)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._gpu_add_wrong_gpus(x,y)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._nested_slow_add_on_user_stream(dst,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._return_tensor_view(i)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._rref_relay(rref)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._slow_add_on_user_stream(x,y)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_custom_stream(self,fn,device_map)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_device_maps(self,options,errMsg)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_device_maps_gpu(self,x_from,y_from,z_to,device_map,dst=None,fn=None)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_device_maps_missing_config(self,mode)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_device_maps_missing_config_response(self,mode)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_device_maps_multi_gpu(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_device_maps_return_to_gpu(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_owner_rref_forward_synchronization(self,local_device,remote_device)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_rref_as_arg_synchronization(self,local_device,remote_device,devicesOptions=None)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_rref_forward_synchronization(self,local_device,remote_device)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_rref_synchronization(self,local_device,remote_device)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_stream_multi_async(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_stream_nested_multi_async(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_stream_nested_sync(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest._test_stream_sync(self,dst)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_async_execution_nested_with_cuda_future(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_async_execution_with_cuda_future(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_callback_changes_devices(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_can_extract_cuda_sparse_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_can_extract_cuda_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_can_extract_custom_class_with_cuda_sparse_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_can_extract_custom_class_with_cuda_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_can_extract_list_with_cuda_sparse_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_can_extract_list_with_cuda_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_device_as_device(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_device_as_int(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_device_as_str(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_device_not_cuda(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_modify_tensor_inplace(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_replace_tensor(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_cuda_future_value_on_bad_device(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_custom_stream(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_custom_stream_multi(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_custom_stream_nested(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_custom_stream_nested_multi(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_cpu(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_cpu_to_gpu_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_cpu_to_gpu_non_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_default_to_non_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_1(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_2(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_3(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_4(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_5(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_6(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_7(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_8(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_1(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_2(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_3(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_4(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_5(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_6(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_7(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_mixed_self_8(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_non_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_non_default_to_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_to_cpu_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_map_gpu_to_cpu_non_default(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_gpu(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_in_options(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_invalid_max_local_device(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_invalid_max_remote_device(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_invalid_min_device(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_many_to_one(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config_loop(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config_not_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config_remote_response(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config_response(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_missing_config_response_loop(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_multi_gpu(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_multi_gpu_self(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_one_to_many(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_remote(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_return_to_gpu(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_return_to_gpu_self(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_maps_wrong_worker_name(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_device_mismatch(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_devices_option_mismatch(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_devices_option_mismatch_reverse(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_owner_rref_forward_synchronization1(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_owner_rref_forward_synchronization2(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_owner_rref_forward_synchronization3(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_owner_rref_forward_synchronization4(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_as_arg_synchronization1(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_as_arg_synchronization2(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_as_arg_synchronization3(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_as_arg_synchronization4(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_as_arg_synchronization5(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_forward_synchronization1(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_forward_synchronization2(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_forward_synchronization3(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_forward_synchronization4(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_to_here_synchronization1(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_to_here_synchronization2(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_to_here_synchronization3(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_to_here_synchronization4(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_rref_with_unpickleable_attributes(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentCudaRpcTest.test_tensor_view_as_return_value(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest(RpcAgentTestFixture,RpcTestCommon)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest._test_rref_get_type_timeout(self,blocking)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest._test_rref_proxy_timeout(self,rref_proxy_api)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_infer_backend_from_options(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_mismatched_type_for_options(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_op_with_invalid_args(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_rref_get_type_timeout_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_rref_get_type_timeout_non_blocking(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_rref_proxy_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_set_and_get_num_worker_threads(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_tensorpipe_options_throw_on_timedelta_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorPipeAgentRpcTest.test_tensorpipe_set_default_timeout(self)
torch.testing._internal.distributed.rpc.rpc_test.TensorWrapper(self,t)
torch.testing._internal.distributed.rpc.rpc_test.TensorWrapper.__init__(self,t)
torch.testing._internal.distributed.rpc.rpc_test.TensorWrapper.increase(self,v)
torch.testing._internal.distributed.rpc.rpc_test.TensorWrapper.sum(self)
torch.testing._internal.distributed.rpc.rpc_test._call_method_on_rref(method,rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.rpc_test._increment_count()
torch.testing._internal.distributed.rpc.rpc_test._reset_count()
torch.testing._internal.distributed.rpc.rpc_test._stub_construct_rpc_backend_options_handler(**kwargs)
torch.testing._internal.distributed.rpc.rpc_test._stub_init_rpc_backend_handler(store,name,rank,world_size,rpc_backend_options)
torch.testing._internal.distributed.rpc.rpc_test.add_rref_to_value(rref,value)
torch.testing._internal.distributed.rpc.rpc_test.add_use_future_cb(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.add_use_future_nested_cb(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.add_use_future_set_result(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.async_add(to,x,y)
torch.testing._internal.distributed.rpc.rpc_test.async_add_chained(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.async_add_chained_multi(to,x,num,step)
torch.testing._internal.distributed.rpc.rpc_test.async_add_multi_fanout(to,x,num,step)
torch.testing._internal.distributed.rpc.rpc_test.async_add_nested(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.async_add_with_future_ctor(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.async_cuda_nested_add(to,x,y,z)
torch.testing._internal.distributed.rpc.rpc_test.async_cuda_sleep_and_set_to_one(t)
torch.testing._internal.distributed.rpc.rpc_test.async_raise_func()
torch.testing._internal.distributed.rpc.rpc_test.async_wrong_type()
torch.testing._internal.distributed.rpc.rpc_test.build_complex_tensors()
torch.testing._internal.distributed.rpc.rpc_test.build_sparse_tensor(coalesce=False)
torch.testing._internal.distributed.rpc.rpc_test.check_rref_confirmed(rref)
torch.testing._internal.distributed.rpc.rpc_test.clear_global_rref()
torch.testing._internal.distributed.rpc.rpc_test.delayed_add(a,b,seconds=0.05)
torch.testing._internal.distributed.rpc.rpc_test.fail_on_fut(fut)
torch.testing._internal.distributed.rpc.rpc_test.foo_add()
torch.testing._internal.distributed.rpc.rpc_test.get_events_from_profile(profile_rref)
torch.testing._internal.distributed.rpc.rpc_test.get_rref_debug_info()
torch.testing._internal.distributed.rpc.rpc_test.get_rref_list(values)
torch.testing._internal.distributed.rpc.rpc_test.heavy_rpc(tensor)
torch.testing._internal.distributed.rpc.rpc_test.heavy_rpc_sparse(tensor)
torch.testing._internal.distributed.rpc.rpc_test.heavy_rpc_torchscript(tensor)
torch.testing._internal.distributed.rpc.rpc_test.light_rpc()
torch.testing._internal.distributed.rpc.rpc_test.multi_layer_nested_async_rpc(dst,world_size,ttl)
torch.testing._internal.distributed.rpc.rpc_test.my_complex_tensor_function(list_input,tensor_class_input,dict_input)
torch.testing._internal.distributed.rpc.rpc_test.my_container_sum(a)
torch.testing._internal.distributed.rpc.rpc_test.my_function(a,b,c)
torch.testing._internal.distributed.rpc.rpc_test.my_rref_function(rref_a,rref_b)
torch.testing._internal.distributed.rpc.rpc_test.my_script_func(tensor)
torch.testing._internal.distributed.rpc.rpc_test.my_sleep_func(seconds=1)
torch.testing._internal.distributed.rpc.rpc_test.my_tensor_function(a,b)
torch.testing._internal.distributed.rpc.rpc_test.nested_remote(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_remote_sparse(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_rpc(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_rpc_sparse(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_rref(dst)
torch.testing._internal.distributed.rpc.rpc_test.nested_rref_sparse(dst)
torch.testing._internal.distributed.rpc.rpc_test.no_result()
torch.testing._internal.distributed.rpc.rpc_test.non_cont_test(t_view,t_cont)
torch.testing._internal.distributed.rpc.rpc_test.raise_func()
torch.testing._internal.distributed.rpc.rpc_test.raise_func_escape()
torch.testing._internal.distributed.rpc.rpc_test.raise_func_script(expected_err:str)->torch.Tensor
torch.testing._internal.distributed.rpc.rpc_test.raise_or_inc(value)
torch.testing._internal.distributed.rpc.rpc_test.return_future()
torch.testing._internal.distributed.rpc.rpc_test.rpc_return_rref(dst)
torch.testing._internal.distributed.rpc.rpc_test.rref_forward_chain(dst,world_size,rref,ttl)
torch.testing._internal.distributed.rpc.rpc_test.run_nested_pickle(pickle_cls_instance,tensor)
torch.testing._internal.distributed.rpc.rpc_test.set_and_check_done(value)
torch.testing._internal.distributed.rpc.rpc_test.set_global_rref(rref)
torch.testing._internal.distributed.rpc.rpc_test.set_value(value)
torch.testing._internal.distributed.rpc.rpc_test.slow_add(x,y,device='cpu')
torch.testing._internal.distributed.rpc.rpc_test.slow_async_add(to,x,y,device='cpu')
torch.testing._internal.distributed.rpc.rpc_test.udf_with_torch_ops(device=-1,use_record_function=False)
torch.testing._internal.distributed.rpc.rpc_test.wait_for_value_future()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py----------------------------------------
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture(self,*args,**kwargs)
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture.__init__(self,*args,**kwargs)
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture.get_shutdown_error_regex(self)
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture.get_timeout_error_regex(self)
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture.rpc_backend(self)
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture.rpc_backend_options(self)
torch.testing._internal.distributed.rpc.faulty_rpc_agent_test_fixture.FaultyRpcAgentTestFixture.setup_fault_injection(self,faulty_messages,messages_to_delay)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/jit/dist_autograd_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.fut->torch.jit._fork(remote_add, t1, t2, dst)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.t1->torch.ones((2, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.t2->torch.ones((2, 3), requires_grad=True)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.t3->torch.add(t1, t2)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.grads->torch.distributed.autograd.get_gradients(context_id)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.dst_worker_name->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.loss->fork_add(t1, t2, dst_worker_name).sum()
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.res->fork_add(t1, t2, dst_worker_name)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.res1_fut->torch.distributed.rpc.rpc_async(dst_worker_name, local_add, (t1, t1))
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.res1->torch.distributed.rpc.rpc_async(dst_worker_name, local_add, (t1, t1)).wait()
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.loss1->torch.distributed.rpc.rpc_async(dst_worker_name, local_add, (t1, t1)).wait().sum()
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.res2_fut->torch.distributed.rpc.rpc_async(dst_worker_name, local_add, (t2, t2))
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.res2->torch.distributed.rpc.rpc_async(dst_worker_name, local_add, (t2, t2)).wait()
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.loss2->torch.distributed.rpc.rpc_async(dst_worker_name, local_add, (t2, t2)).wait().sum()
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.(loss0, loss1)->forward_script(context_id, dst_worker_name, t1, t2)
A:torch.testing._internal.distributed.rpc.jit.dist_autograd_test.(grad0, grad1)->torch.distributed.autograd.get_gradients(context_id)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest.test_dist_backward(self)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest.test_get_gradients(self)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest.test_jit_fork_within_context(self)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.JitDistAutogradTest.test_restore_context_after_swtich_to_jit_thread(self)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.fork_add(t1,t2,dst:str)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.local_add(t1,t2)
torch.testing._internal.distributed.rpc.jit.dist_autograd_test.remote_add(t1,t2,dst:str)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py----------------------------------------
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.fut->rpc_async_call_with_timeout_future_ret(dst_worker_name, args, kwargs, 0)
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.ret->rpc_async_call_with_timeout(dst_worker_name, args, kwargs, 0)
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.dst_worker_name->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.expected_error->self.get_timeout_error_regex()
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.result->rpc_async_call_with_timeout_future_ret(dst_worker_name, args, kwargs, 0).wait()
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.dst_worker->'worker{}'.format(dst_rank)
A:torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rref->torch.distributed.rpc.remote(dst_worker, torch.add, args=(torch.tensor(1), torch.tensor(1)))
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest.test_remote_timeout_to_here_in_jit(self)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest.test_rref_timeout_pickle_in_jit(self)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest.test_rref_timeout_pickle_script_func(self)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest.test_rref_to_here_timeout_in_jit(self)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest.test_timeout_in_python(self)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.JitFaultyAgentRpcTest.test_timeout_in_torchscript_function(self)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rpc_async_call_future_ret(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor])
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rpc_async_call_with_timeout(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor],timeout:float)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rpc_async_call_with_timeout_future_ret(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor],timeout:float)
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rpc_async_with_rref_arg(dst_worker_name:str,args:Tuple[RRef[Tensor]])->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rref_to_here(rref_var:RRef[Tensor])->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.rref_to_here_with_timeout(rref_var:RRef[Tensor],timeout:float)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.script_rpc_async_call(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor])
torch.testing._internal.distributed.rpc.jit.rpc_test_faulty.two_args_two_kwargs(first_arg,second_arg,first_kwarg=torch.tensor([3,3]),second_kwarg=torch.tensor([4,4]))


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/jit/rpc_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.jit.rpc_test.dst_worker_name->worker_name(dst_rank)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_var->rpc_return_rref(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.res->torch.distributed.rpc.rpc_sync(dst_worker_name, two_args_two_kwargs, args, kwargs)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref->torch.distributed.rpc.remote(worker_name((self.rank + 1) % self.world_size), async_wrong_type)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.retret->torch.distributed.rpc.rpc_sync(dst_worker_name, rref_local_value, (rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.ret->torch.distributed.rpc.rpc_sync(dst1, async_add, args=(dst2, torch.ones(2, 2), torch.ones(2, 2)))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.ret_rref->torch.distributed.rpc.remote(worker_name(dst_rank), script_check_rref_confirmed, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.dst->worker_name(n % self.world_size)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.list_rref->torch.distributed.rpc.remote(dst, list_create)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.fut1->torch.distributed.rpc.rpc_async(dst_worker_name, script_add_ones, (t,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.fut2->torch.distributed.rpc.rpc_async(dst_worker_name, script_add_ones, (t,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.fut->torch.jit._fork(sleep, sleep_interval)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.x->torch.jit._wait(fut)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.value->torch.jit._wait(fut)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.res_tensor->torch.ones(2, 2)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.local_ret->one_arg(torch.ones(2, 2))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref1->torch.distributed.rpc.rpc_sync(worker_name(dst_rank), return_rref, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref2->torch.distributed.rpc.remote(worker_name(dst_rank), rref_to_here, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref3->torch.distributed.rpc.remote(worker_name(dst_rank), return_rref, args=(rref,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.module_with_rrefs->MyScriptModuleWithRRefs(worker_name(dst_rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.ret_fut->torch.distributed.rpc.rpc_async(worker_name(dst_rank), two_args_two_kwargs, args=inputs)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.expected_res->torch.add(input_0, input_1)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.fut_res->future_return_to_python(dst_rank, inputs)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.input_0->torch.ones(2, 2)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.self.a->torch.ones(rank)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_script_class->torch.distributed.rpc.RRef(MyScriptClass(self.rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_script_module->torch.distributed.rpc.RRef(MyScriptModule(self.rank))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.use_rref_on_owner_script->torch.jit.script(use_rref_on_owner)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rref_res->torch.distributed.rpc.remote(dst_worker_name, two_args_two_kwargs, args, kwargs)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.module->ref_script_module.to_here()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.f->io.BytesIO()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.m->torch.jit.load(f)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.local_rref->torch.distributed.rpc.remote(worker_name(self.rank), one_arg, args=(torch.ones(2, 2),))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.my_local_script_module->MyScriptModule(self.rank)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.remote_ref->torch.distributed.rpc.remote(worker_name(dst_rank), construct_my_script_module, args=(self.rank,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.dst_name->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.created_script_module->torch.distributed.rpc.rpc_sync(dst_name, MyScriptModule, args=(self.rank,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.rank_ones_tensor->local_script_module()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.remote_script_module->torch.distributed.rpc.remote(dst_name, MyScriptModule, args=(self.rank,))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.remote_end_is_script->torch.distributed.rpc.rpc_sync(remote_script_module.owner(), rref_isinstance, args=(remote_script_module, torch.jit.ScriptModule))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.remote_forward_output->torch.distributed.rpc.remote(dst_name, MyScriptModule, args=(self.rank,)).rpc_sync().forward()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.remote_func_output->torch.distributed.rpc.remote(dst_name, MyScriptModule, args=(self.rank,)).rpc_sync().custom_func()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.local_script_module->torch.distributed.rpc.remote(dst_name, MyScriptModule, args=(self.rank,)).to_here()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.local_script_func_output->torch.distributed.rpc.remote(dst_name, MyScriptModule, args=(self.rank,)).to_here().custom_func()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.m1->MyScriptModuleWithRRefs(dst_name)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.m2->MyScriptModuleWithRRefs(dst_name)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.out1->torch.distributed.rpc.rpc_sync(dst_name, load_script_module_with_pickled_rref, args=(f.getvalue(),))
A:torch.testing._internal.distributed.rpc.jit.rpc_test.out2->m2()
A:torch.testing._internal.distributed.rpc.jit.rpc_test.future->torch.distributed.rpc.rpc_async(worker_name((self.rank + 1) % self.world_size), script_fork_wait_throw, args=(torch.ones(2),)).then(callback)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.future_then->torch.distributed.rpc.rpc_async(worker_name((self.rank + 1) % self.world_size), script_fork_wait_throw, args=(torch.ones(2),)).then(callback).then(lambda _: True)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.prof_key->_build_rpc_profiling_key(RPCExecMode.ASYNC, torch._jit_internal._qualified_name(one_arg), 'worker0', 'worker1')
A:torch.testing._internal.distributed.rpc.jit.rpc_test.function_event->get_function_event(events, 'foo')
A:torch.testing._internal.distributed.rpc.jit.rpc_test.qual_name->torch._jit_internal._qualified_name(two_args_two_kwargs)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.profiled_name->_build_rpc_profiling_key(RPCExecMode.ASYNC_JIT, qual_name, worker_name(self.rank), dst_worker_name)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.expected_key->_build_rpc_profiling_key(RPCExecMode.ASYNC_JIT, torch._jit_internal._qualified_name(script_add_ones), worker_name(self.rank), dst_worker_name)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.sleep_event->get_function_event(function_events, 'foo')
A:torch.testing._internal.distributed.rpc.jit.rpc_test.dst1->worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.rpc.jit.rpc_test.dst2->worker_name((self.rank + 2) % self.world_size)
torch.testing._internal.distributed.rpc.jit.rpc_test.FutureTypingTest
torch.testing._internal.distributed.rpc.jit.rpc_test.FutureTypingTest.test_future_passed_between_python_and_jit(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.FutureTypingTest.test_future_python_annotation(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_all_kwargs_are_populated_by_defaults(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_args_and_kwargs_contain_different_types(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_args_kwargs_are_neither_passed(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_call_python_function_remotely_from_script_not_supported(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_call_script_function_that_not_exists_remotely_from_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_call_script_function_that_raises_remotely_from_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_kwargs_not_passed(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_less_than_needed_args_are_specified(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_more_than_needed_args_are_specified(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_no_kwargs_are_populated_by_defaults(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_some_kwargs_are_populated_by_defaults(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcOpTest.test_unexepected_kwarg_is_specified(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest(RRefAPITest,RRefTypingTest,LocalRRefTest,JitRpcOpTest,FutureTypingTest,RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_add_done_callback(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_function_remote(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_function_remote_multi(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_function_simple(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_function_wrong_decorator_order(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_function_wrong_return_type(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_function_wrong_return_type_remote(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_script_throw(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_async_script_udf(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_call_fork_in_jit_with_profiling(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_call_rpc_with_profiling(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_callback_chain(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_callback_simple(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_callback_with_exception(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_create_script_module_on_remote(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_load_script_module_with_pickled_rref(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_record_function_jit_end_callbacks_with_fork(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_record_function_on_caller_rpc_async(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_remote_script_module(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_remote_script_throw(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_remote_script_udf(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rpc_async_jit_profiled(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rpc_torchscript_record_function(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_rref_jit_pickle_not_supported(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_torchscript_function(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_torchscript_function_exception(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.JitRpcTest.test_torchscript_functions_not_supported(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_create_local_script_class_rref_in_py(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_create_local_script_module_rref_in_py(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_return_local_script_class_rref_in_py_and_use_in_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.LocalRRefTest.test_return_local_script_module_rref_in_py_and_use_in_script(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyModuleInterface(torch.nn.Module)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyModuleInterface.forward(self)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptClass(self,a:int)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptClass.__init__(self,a:int)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptClass.get_value(self)->int
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule(self,rank)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule.__init__(self,rank)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule.custom_func(self)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModule.forward(self)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModuleWithRRefs(self,dst_worker)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModuleWithRRefs.__init__(self,dst_worker)
torch.testing._internal.distributed.rpc.jit.rpc_test.MyScriptModuleWithRRefs.forward(self)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest._create_rref(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest.test_local_rref_local_value(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest.test_rref_is_owner(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest.test_rref_list_mutate(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest.test_rref_local_value(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest.test_user_rrefs_confirmed(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefAPITest.test_user_rrefs_confirmed_remote(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefTypingTest
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefTypingTest.test_my_script_module_with_rrefs(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefTypingTest.test_rref_as_arg_and_return(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.RRefTypingTest.test_rref_python_annotation(self)
torch.testing._internal.distributed.rpc.jit.rpc_test.assorted_types_args_kwargs(tensor_arg:Tensor,str_arg:str,int_arg:int,tensor_kwarg:Tensor=torch.tensor([2,2]),str_kwarg:str='str_kwarg',int_kwarg:int=2)
torch.testing._internal.distributed.rpc.jit.rpc_test.async_add(to:str,x:Tensor,y:Tensor)->Future[Tensor]
torch.testing._internal.distributed.rpc.jit.rpc_test.async_wrong_type()->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.call_fork_with_profiling(handle:Tensor)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.call_rpc_torchscript_with_record_function(dst_worker_name:str,block:str)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.call_rpc_with_profiling(handle:Tensor,dst_worker_name:str)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.construct_my_script_module(rank:int)->MyModuleInterface
torch.testing._internal.distributed.rpc.jit.rpc_test.list_create()->List[int]
torch.testing._internal.distributed.rpc.jit.rpc_test.load_script_module_with_pickled_rref(pickled_script_module)
torch.testing._internal.distributed.rpc.jit.rpc_test.my_script_module_init(rank:int)->MyModuleInterface
torch.testing._internal.distributed.rpc.jit.rpc_test.no_arg()
torch.testing._internal.distributed.rpc.jit.rpc_test.one_arg(value)
torch.testing._internal.distributed.rpc.jit.rpc_test.owner_create_rref_my_script_class(a)
torch.testing._internal.distributed.rpc.jit.rpc_test.owner_create_rref_my_script_module(a)
torch.testing._internal.distributed.rpc.jit.rpc_test.python_function()
torch.testing._internal.distributed.rpc.jit.rpc_test.raise_script()
torch.testing._internal.distributed.rpc.jit.rpc_test.record_function_on_caller_rpc_async(dst_worker_name:str,block:str)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.return_rref(rref_var:RRef[Tensor])->RRef[Tensor]
torch.testing._internal.distributed.rpc.jit.rpc_test.return_value(value:int)->int
torch.testing._internal.distributed.rpc.jit.rpc_test.rpc_return_rref(dst)
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_isinstance(rref,cls_to_check)
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_list_mutate(rref:RRef[List[int]])->None
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_local_value(rref:RRef[Tensor])->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_python_annotation(rref_var:RRef[Tensor])->RRef[Tensor]
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_script_annotation(rref_var:RRef[Tensor])->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.rref_to_here(rref_var:RRef[Tensor])->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.run_ref_script_module(ref_script_module:RRef[MyModuleInterface],t:Tensor)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.save_rref(rref_var:RRef[Tensor],fname:str)->None
torch.testing._internal.distributed.rpc.jit.rpc_test.script_add(x:Tensor,y:Tensor)->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.script_add_ones(x)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_add_ones_with_record_function(x,block:str)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_check_rref_confirmed(rref:RRef[Tensor])->bool
torch.testing._internal.distributed.rpc.jit.rpc_test.script_fork_wait_throw(invalue)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_fork_wait_udf(tensor)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_raise_func(value)
torch.testing._internal.distributed.rpc.jit.rpc_test.script_rpc_async_call(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor])
torch.testing._internal.distributed.rpc.jit.rpc_test.script_rpc_remote_call(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor])
torch.testing._internal.distributed.rpc.jit.rpc_test.script_rpc_sync_call(dst_worker_name:str,args:Tuple[Tensor,Tensor],kwargs:Dict[str,Tensor])
torch.testing._internal.distributed.rpc.jit.rpc_test.script_rref_get_value_my_script_class(rref:RRef[MyScriptClass])->int
torch.testing._internal.distributed.rpc.jit.rpc_test.script_rref_run_forward_my_script_module(rref:RRef[MyModuleInterface])->Tensor
torch.testing._internal.distributed.rpc.jit.rpc_test.sleep(t)
torch.testing._internal.distributed.rpc.jit.rpc_test.two_args_two_kwargs(first_arg,second_arg,first_kwarg=torch.tensor([3,3]),second_kwarg=torch.tensor([4,4]))


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/jit/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/examples/parameter_server_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.self.model->torch.nn.Linear(in_features, out_features)
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.self.lock->threading.Lock()
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.self.future_model->torch.futures.Future()
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.self.optimizer->torch.optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.p.grad->torch.zeros_like(p)
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.self->torch.distributed.rpc.RRef(BatchUpdateParameterServer(len(trainers))).local_value()
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.self.loss_fn->torch.nn.L1Loss()
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.inputs->torch.randn(batch_size, in_features)
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.labels->torch.zeros(batch_size, out_features)
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.m->torch.distributed.rpc.rpc_sync(self.ps_rref.owner(), BatchUpdateParameterServer.update_and_fetch_model, args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]))
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.trainer->Trainer(ps_rref)
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.start->perf_counter()
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.ps_rref->torch.distributed.rpc.RRef(BatchUpdateParameterServer(len(trainers)))
A:torch.testing._internal.distributed.rpc.examples.parameter_server_test.stop->perf_counter()
torch.testing._internal.distributed.rpc.examples.parameter_server_test.BatchUpdateParameterServer(self,batch_update_size)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.BatchUpdateParameterServer.__init__(self,batch_update_size)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.BatchUpdateParameterServer.get_model(self)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.BatchUpdateParameterServer.update_and_fetch_model(ps_rref,grads)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.ParameterServerTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.ParameterServerTest.test_batch_updating_parameter_server(self)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.Trainer(self,ps_rref)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.Trainer.__init__(self,ps_rref)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.Trainer.get_next_batch(self)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.Trainer.train(self)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.run_ps(trainers)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.run_trainer(ps_rref)
torch.testing._internal.distributed.rpc.examples.parameter_server_test.timed_log(text)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/examples/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py----------------------------------------
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.affine1->torch.nn.Linear(4, 128)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.dropout->torch.nn.Dropout(p=0.6)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.affine2->torch.nn.Linear(128, 2)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.x->torch.nn.functional.relu(x)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.action_scores->self.affine2(x)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.state->torch.randn(self.state_dim)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.env->DummyEnv()
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.action->Categorical(probs).sample()
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.(state, reward, done, _)->self.env.step(action)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.agent_rref->RRef(self)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.policy->Policy()
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.optimizer->torch.optim.Adam(self.policy.parameters(), lr=0.01)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.self.eps->numpy.finfo(np.float32).eps.item()
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.ob_info->torch.distributed.rpc.get_worker_info(worker_name(ob_rank))
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.probs->self.policy(state.unsqueeze(0))
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.m->Categorical(probs)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.min_reward->min([sum(self.rewards[ob_id]) for ob_id in self.rewards])
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.returns->torch.tensor(returns)
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.policy_loss->torch.cat(policy_loss).sum()
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.last_reward->Agent(self.world_size).finish_episode()
A:torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.agent->Agent(self.world_size)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Agent(self,world_size)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Agent.__init__(self,world_size)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Agent.finish_episode(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Agent.report_reward(self,ob_id,reward)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Agent.run_episode(self,n_steps=0)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Agent.select_action(self,ob_id,state)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.DummyEnv(self,state_dim=4,num_iters=10,reward_threshold=475.0)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.DummyEnv.__init__(self,state_dim=4,num_iters=10,reward_threshold=475.0)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.DummyEnv.reset(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.DummyEnv.seed(self,manual_seed)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.DummyEnv.step(self,action)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Observer(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Observer.__init__(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Observer.run_episode(self,agent_rref,n_steps)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Policy(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Policy.__init__(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.Policy.forward(self,x)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.ReinforcementLearningRpcTest(RpcAgentTestFixture)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.ReinforcementLearningRpcTest.test_rl_rpc(self)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test._call_method(method,rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test._remote_method(method,rref,*args,**kwargs)
torch.testing._internal.distributed.rpc.examples.reinforcement_learning_rpc_test.run_agent(agent,n_steps)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/nn/api/remote_module_test.py----------------------------------------
A:torch.testing._internal.distributed.nn.api.remote_module_test._PARAM_VAL->torch.nn.Parameter(torch.ones(1))
A:torch.testing._internal.distributed.nn.api.remote_module_test.module->MyModule(first_arg, first_kwarg=first_kwarg)
A:torch.testing._internal.distributed.nn.api.remote_module_test.scripted_module->torch.jit.script(module)
A:torch.testing._internal.distributed.nn.api.remote_module_test.modes->ModuleCreationMode.__members__.values()
A:torch.testing._internal.distributed.nn.api.remote_module_test.kwargs->dict(word=t2)
A:torch.testing._internal.distributed.nn.api.remote_module_test.remote_module->_RemoteModule(remote_device, create_scripted_module, args, kwargs, _module_interface_cls=MyModuleInterface)
A:torch.testing._internal.distributed.nn.api.remote_module_test.scripted_remote_module->next(self._create_remote_module_iter('{}/cuda:0'.format(dst_worker_name), modes=[ModuleCreationMode.MODULE_CTOR_WITH_INTERFACE]))
A:torch.testing._internal.distributed.nn.api.remote_module_test.dst_worker_name->torch.testing._internal.dist_utils.worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.nn.api.remote_module_test.remote_device->'{}/cpu'.format(dst_worker_name)
A:torch.testing._internal.distributed.nn.api.remote_module_test.ret_fut->_RemoteModule(remote_device, create_scripted_module, args, kwargs, _module_interface_cls=MyModuleInterface).forward_async(*args, **kwargs)
A:torch.testing._internal.distributed.nn.api.remote_module_test.ret->run_forward(scripted_remote_module)
A:torch.testing._internal.distributed.nn.api.remote_module_test.param_rrefs->_RemoteModule(remote_device, create_scripted_module, args, kwargs, _module_interface_cls=MyModuleInterface).remote_parameters()
A:torch.testing._internal.distributed.nn.api.remote_module_test.rref->_RemoteModule(remote_device, create_scripted_module, args, kwargs, _module_interface_cls=MyModuleInterface).get_module_rref()
A:torch.testing._internal.distributed.nn.api.remote_module_test.ret1->torch.distributed.rpc.rpc_sync(dst_worker1_name, remote_forward, (remote_module, args))
A:torch.testing._internal.distributed.nn.api.remote_module_test.ret2->torch.distributed.rpc.rpc_sync(dst_worker2_name, remote_forward, (remote_module2, args))
A:torch.testing._internal.distributed.nn.api.remote_module_test.fn->torch.rand((3, 3), requires_grad=False)
A:torch.testing._internal.distributed.nn.api.remote_module_test.attrs->torch.distributed.rpc.rpc_sync(dst_worker2_name, remote_module_attributes, (remote_module,))
A:torch.testing._internal.distributed.nn.api.remote_module_test.dst_worker1_name->torch.testing._internal.dist_utils.worker_name((self.rank + 1) % self.world_size)
A:torch.testing._internal.distributed.nn.api.remote_module_test.dst_worker2_name->torch.testing._internal.dist_utils.worker_name((self.rank + 2) % self.world_size)
A:torch.testing._internal.distributed.nn.api.remote_module_test.expected_unpickled_attrs->list(_REMOTE_MODULE_PICKLED_ATTRIBUTES)
A:torch.testing._internal.distributed.nn.api.remote_module_test.remote_module2->torch.distributed.rpc.rpc_sync(dst_worker2_name, RemoteModule.init_from_module_rref, (dst_worker2_name, remote_module.get_module_rref()))
A:torch.testing._internal.distributed.nn.api.remote_module_test.device->torch.distributed.rpc.rpc_sync(dst_worker_name, remote_device, (remote_module.module_rref,))
A:torch.testing._internal.distributed.nn.api.remote_module_test.t1->torch.ones(1)
torch.testing._internal.distributed.nn.api.remote_module_test.BadModule(self,first_arg,first_kwarg=-1)
torch.testing._internal.distributed.nn.api.remote_module_test.BadModule.__init__(self,first_arg,first_kwarg=-1)
torch.testing._internal.distributed.nn.api.remote_module_test.CommonRemoteModuleTest(RpcAgentTestFixture)
torch.testing._internal.distributed.nn.api.remote_module_test.CommonRemoteModuleTest._create_remote_module_iter(remote_device,modes=None)
torch.testing._internal.distributed.nn.api.remote_module_test.CommonRemoteModuleTest.world_size(self)
torch.testing._internal.distributed.nn.api.remote_module_test.CudaRemoteModuleTest(CommonRemoteModuleTest)
torch.testing._internal.distributed.nn.api.remote_module_test.CudaRemoteModuleTest.test_input_moved_to_cuda_device(self)
torch.testing._internal.distributed.nn.api.remote_module_test.CudaRemoteModuleTest.test_input_moved_to_cuda_device_script(self)
torch.testing._internal.distributed.nn.api.remote_module_test.CudaRemoteModuleTest.test_invalid_devices(self)
torch.testing._internal.distributed.nn.api.remote_module_test.CudaRemoteModuleTest.test_valid_device(self)
torch.testing._internal.distributed.nn.api.remote_module_test.ModuleCreationMode(enum.Enum)
torch.testing._internal.distributed.nn.api.remote_module_test.MyModule(self,first_arg,first_kwarg=-1)
torch.testing._internal.distributed.nn.api.remote_module_test.MyModule.__init__(self,first_arg,first_kwarg=-1)
torch.testing._internal.distributed.nn.api.remote_module_test.MyModule.forward(self,tensor:Tensor,number:int,word:str='default')->Tuple[str, int, Tensor]
torch.testing._internal.distributed.nn.api.remote_module_test.MyModuleInterface
torch.testing._internal.distributed.nn.api.remote_module_test.MyModuleInterface.forward(self,tensor:Tensor,number:int,word:str='default')->Tuple[str, int, Tensor]
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest(CommonRemoteModuleTest)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_bad_module(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_forward_async(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_forward_async_script(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_forward_sync(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_forward_sync_script(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_forward_with_kwargs(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_get_module_rref(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_remote_module_py_pickle_not_supported(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_remote_module_py_pickle_not_supported_script(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_remote_parameters(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_send_remote_module_with_a_new_attribute_not_pickled_over_the_wire(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_train_eval(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteModuleTest.test_unsupported_methods(self)
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteMyModuleInterface
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteMyModuleInterface.forward(self,tensor:Tensor,number:int,word:str='default')->Tuple[str, int, Tensor]
torch.testing._internal.distributed.nn.api.remote_module_test.RemoteMyModuleInterface.forward_async(self,tensor:Tensor,number:int,word:str='default')->Future[Tuple[str, int, Tensor]]
torch.testing._internal.distributed.nn.api.remote_module_test.ThreeWorkersRemoteModuleTest(CommonRemoteModuleTest)
torch.testing._internal.distributed.nn.api.remote_module_test.ThreeWorkersRemoteModuleTest.test_create_remote_module_from_module_rref(self)
torch.testing._internal.distributed.nn.api.remote_module_test.ThreeWorkersRemoteModuleTest.test_send_remote_module_over_the_wire(self)
torch.testing._internal.distributed.nn.api.remote_module_test.ThreeWorkersRemoteModuleTest.test_send_remote_module_over_the_wire_script_not_supported(self)
torch.testing._internal.distributed.nn.api.remote_module_test.ThreeWorkersRemoteModuleTest.world_size(self)
torch.testing._internal.distributed.nn.api.remote_module_test.create_scripted_module(first_arg,first_kwarg=-1)
torch.testing._internal.distributed.nn.api.remote_module_test.get_remote_training_arg(module_rref)
torch.testing._internal.distributed.nn.api.remote_module_test.remote_device(module_rref)
torch.testing._internal.distributed.nn.api.remote_module_test.remote_forward(remote_module,args)
torch.testing._internal.distributed.nn.api.remote_module_test.remote_forward_async(remote_module,args)
torch.testing._internal.distributed.nn.api.remote_module_test.remote_module_attributes(remote_module)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/distributed/nn/api/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/test_module/no_future_div.py----------------------------------------
torch.testing._internal.test_module.no_future_div.div_float_nofuture()
torch.testing._internal.test_module.no_future_div.div_int_nofuture()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/test_module/future_div.py----------------------------------------
torch.testing._internal.test_module.future_div.div_float_future()
torch.testing._internal.test_module.future_div.div_int_future()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/test_module/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/codegen/random_topo_test.py----------------------------------------
A:torch.testing._internal.codegen.random_topo_test.max_dim->len(tensor_shape)
A:torch.testing._internal.codegen.random_topo_test.num_b_dims->numpy.random.randint(0, max_dim + 1)
A:torch.testing._internal.codegen.random_topo_test.trim_head->numpy.random.randint(0, min(num_b_dims + 1, max_dim))
A:torch.testing._internal.codegen.random_topo_test.num_tensor->numpy.random.randint(1, max_tensor_num)
A:torch.testing._internal.codegen.random_topo_test.num_const->numpy.random.randint(0, num_tensor + 1)
A:torch.testing._internal.codegen.random_topo_test.const_list->numpy.random.random(num_const)
A:torch.testing._internal.codegen.random_topo_test.candidate->list(range(num_tensor))
A:torch.testing._internal.codegen.random_topo_test.u_op_size->len(unary_operations)
A:torch.testing._internal.codegen.random_topo_test.b_op_size->len(binary_operations)
A:torch.testing._internal.codegen.random_topo_test.num_operations->numpy.random.randint(num_sets - 1, num_sets * GRAPH_FACTOR)
A:torch.testing._internal.codegen.random_topo_test.index->numpy.random.randint(0, len(candidate))
A:torch.testing._internal.codegen.random_topo_test.op_index->numpy.random.randint(0, u_op_size + b_op_size)
A:torch.testing._internal.codegen.random_topo_test.out_tensor->binary_operations[op_index - u_op_size](tensor_list[lh_index], tensor_list[rh_index])
A:torch.testing._internal.codegen.random_topo_test.op_2_index->numpy.random.randint(0, len(tensor_list) + num_const)
A:torch.testing._internal.codegen.random_topo_test.cand_index->numpy.random.randint(0, len(candidate))
A:torch.testing._internal.codegen.random_topo_test.candidate[index]->len(tensor_list)
A:torch.testing._internal.codegen.random_topo_test.lh_root->get_root(lh_index, d_map)
A:torch.testing._internal.codegen.random_topo_test.rh_root->get_root(rh_index, d_map)
A:torch.testing._internal.codegen.random_topo_test.d_map[rh_root]->len(tensor_list)
A:torch.testing._internal.codegen.random_topo_test.d_map[lh_root]->len(tensor_list)
A:torch.testing._internal.codegen.random_topo_test.out_list->numpy.random.choice(range(num_tensor, len(tensor_list)), np.random.randint(0, len(tensor_list) - num_tensor), False)
A:torch.testing._internal.codegen.random_topo_test.seed_tensor->torch.tensor(np.random.randint(0, seed))
A:torch.testing._internal.codegen.random_topo_test.tensor_dim->numpy.random.randint(1, max_tensor_dim)
A:torch.testing._internal.codegen.random_topo_test.size_i->min(size_i, 128 + size_i % 128)
A:torch.testing._internal.codegen.random_topo_test.num_broadcasted_tensors->numpy.random.randint(0, 1)
A:torch.testing._internal.codegen.random_topo_test.broadcasted_tensors_indices->numpy.random.choice(torch.arange(num_tensor), num_broadcasted_tensors, replace=False)
A:torch.testing._internal.codegen.random_topo_test.compatible_shape->get_broadcast_compatible_shape(tensor_shape)
A:torch.testing._internal.codegen.random_topo_test.repro_str->'python {0}'.format(__file__)
A:torch.testing._internal.codegen.random_topo_test.(seed_tensor, tensor_list)->prepareInputTensorsToRandomTopoTest(seed, args.max_num_tensor, args.max_tensor_dim, args.max_tensor_size, args.debug_tensor, 'cuda' if not args.cpu else 'cpu', torch.float32 if not args.fp16 else torch.float16)
A:torch.testing._internal.codegen.random_topo_test.o->random_topology_test(seed_tensor, *tensor_list)
A:torch.testing._internal.codegen.random_topo_test.traced_model->torch.jit.trace(random_topology_test, (seed_tensor, *tensor_list))
A:torch.testing._internal.codegen.random_topo_test.jit_o->traced_model(seed_tensor, *tensor_list)
A:torch.testing._internal.codegen.random_topo_test.validate_o->zip(o, jit_o)
A:torch.testing._internal.codegen.random_topo_test.parser->argparse.ArgumentParser()
A:torch.testing._internal.codegen.random_topo_test.group->argparse.ArgumentParser().add_mutually_exclusive_group()
A:torch.testing._internal.codegen.random_topo_test.args->parse_args()
torch.testing._internal.codegen.random_topo_test.WrongResultException(Exception)
torch.testing._internal.codegen.random_topo_test.get_broadcast_compatible_shape(tensor_shape)
torch.testing._internal.codegen.random_topo_test.parse_args()
torch.testing._internal.codegen.random_topo_test.prepareInputTensorsToRandomTopoTest(seed,max_tensor_num,max_tensor_dim,max_tensor_size,debug_tensor,device,dtype)
torch.testing._internal.codegen.random_topo_test.random_topology_test(seed,*inp_tensor_list)
torch.testing._internal.codegen.random_topo_test.reproString(current_seed,args)
torch.testing._internal.codegen.random_topo_test.runDefaultTestWithSeed(seed)
torch.testing._internal.codegen.random_topo_test.runTest(seed,args)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/codegen/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/data/network1.py----------------------------------------
A:torch.testing._internal.data.network1.self.linear->torch.nn.Linear(10, 20)
torch.testing._internal.data.network1.Net(self)
torch.testing._internal.data.network1.Net.__init__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/data/network2.py----------------------------------------
A:torch.testing._internal.data.network2.self.linear->torch.nn.Linear(10, 20)
A:torch.testing._internal.data.network2.self.relu->torch.nn.ReLU()
torch.testing._internal.data.network2.Net(self)
torch.testing._internal.data.network2.Net.__init__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/testing/_internal/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/contrib/_tensorboard_vis.py----------------------------------------
A:torch.contrib._tensorboard_vis.pb_graph->visualize(graph_executor)
A:torch.contrib._tensorboard_vis.evt->tensorflow.core.util.event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
A:torch.contrib._tensorboard_vis.input_node->visualize(graph_executor).node.add(op='input', name=name_prefix + 'input')
A:torch.contrib._tensorboard_vis.return_node->visualize(graph_executor).node.add(op='output', name=name_prefix + 'output')
A:torch.contrib._tensorboard_vis.input_kinds->visualize(graph_executor).node.add(op='INPUT_KIND', name=subgraph_name)
A:torch.contrib._tensorboard_vis.input_kinds.attr['inputs'].s->repr(arg_spec).encode('ascii')
A:torch.contrib._tensorboard_vis.(op, name)->name_for(node)
A:torch.contrib._tensorboard_vis.ge->next(executors_it)
A:torch.contrib._tensorboard_vis.pb_node->visualize(graph_executor).node.add(op=op, name=name)
torch.contrib._tensorboard_vis.dump_tensorboard_summary(graph_executor,logdir)
torch.contrib._tensorboard_vis.visualize(graph,name_prefix='',pb_graph=None,executors_it=None)
torch.contrib._tensorboard_vis.visualize_graph_executor(state,name_prefix,pb_graph,inline_graph)
torch.contrib._tensorboard_vis.visualize_rec(graph,value_map,name_prefix,pb_graph,executors_it=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/contrib/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/_mappings.py----------------------------------------
A:torch.ao.sparsity._mappings._static_sparse_quantized_mapping->dict({torch.nn.Linear: torch.ao.nn.sparse.quantized.Linear})
A:torch.ao.sparsity._mappings._dynamic_sparse_quantized_mapping->dict({torch.nn.Linear: torch.ao.nn.sparse.quantized.dynamic.Linear})
torch.ao.sparsity._mappings.get_dynamic_sparse_quantized_mapping()
torch.ao.sparsity._mappings.get_static_sparse_quantized_mapping()
torch.ao.sparsity.get_dynamic_sparse_quantized_mapping()
torch.ao.sparsity.get_static_sparse_quantized_mapping()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/sparsifier/base_sparsifier.py----------------------------------------
A:torch.ao.sparsity.sparsifier.base_sparsifier.self.defaults->dict()
A:torch.ao.sparsity.sparsifier.base_sparsifier.module_groups->copy.deepcopy(state_dict['module_groups'])
A:torch.ao.sparsity.sparsifier.base_sparsifier.layer->fqn_to_module(self.model, fqn)
A:torch.ao.sparsity.sparsifier.base_sparsifier.p->FakeSparsity(torch.ones(layer.weight.shape))
A:torch.ao.sparsity.sparsifier.base_sparsifier.mask->config.get('mask', torch.ones_like(module.weight))
A:torch.ao.sparsity.sparsifier.base_sparsifier.module->fqn_to_module(model, module_fqn)
A:torch.ao.sparsity.sparsifier.base_sparsifier.local_args->copy.deepcopy(self.defaults)
A:torch.ao.sparsity.sparsifier.base_sparsifier.module_fqn->module_to_fqn(model, module)
A:torch.ao.sparsity.sparsifier.base_sparsifier.module_from_fqn->fqn_to_module(model, module_fqn)
A:torch.ao.sparsity.sparsifier.base_sparsifier.param->config.get('parametrization', FakeSparsity)
A:torch.ao.sparsity.sparsifier.base_sparsifier.sparse_params->dict()
A:torch.ao.sparsity.sparsifier.base_sparsifier.params->params_to_keep_per_layer.get(config['fqn'], None)
torch.ao.sparsity.BaseSparsifier(self,defaults)
torch.ao.sparsity.BaseSparsifier.__getstate__(self)
torch.ao.sparsity.BaseSparsifier.__repr__(self)
torch.ao.sparsity.BaseSparsifier.__setstate__(self,state)
torch.ao.sparsity.BaseSparsifier._prepare(self,*args,**kwargs)
torch.ao.sparsity.BaseSparsifier.convert(self)
torch.ao.sparsity.BaseSparsifier.load_state_dict(self,state_dict,strict=True)
torch.ao.sparsity.BaseSparsifier.prepare(self,model,config)
torch.ao.sparsity.BaseSparsifier.squash_mask(self,params_to_keep:Optional[Tuple[str,...]]=None,params_to_keep_per_layer:Optional[Dict[str,Tuple[str,...]]]=None,*args,**kwargs)
torch.ao.sparsity.BaseSparsifier.state_dict(self)
torch.ao.sparsity.BaseSparsifier.step(self,use_path=True)
torch.ao.sparsity.BaseSparsifier.update_mask(self,layer,**kwargs)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier(self,defaults)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.__getstate__(self)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.__init__(self,defaults)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.__repr__(self)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.__setstate__(self,state)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier._prepare(self,*args,**kwargs)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.convert(self)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.load_state_dict(self,state_dict,strict=True)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.prepare(self,model,config)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.squash_mask(self,params_to_keep:Optional[Tuple[str,...]]=None,params_to_keep_per_layer:Optional[Dict[str,Tuple[str,...]]]=None,*args,**kwargs)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.state_dict(self)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.step(self,use_path=True)
torch.ao.sparsity.sparsifier.base_sparsifier.BaseSparsifier.update_mask(self,layer,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/sparsifier/utils.py----------------------------------------
A:torch.ao.sparsity.sparsifier.utils.child_path->module_to_fqn(child, layer, prefix=new_name)
A:torch.ao.sparsity.sparsifier.utils.path->path.split('.').split('.')
A:torch.ao.sparsity.sparsifier.utils.model->getattr(model, name, None)
torch.ao.sparsity.FakeSparsity(self,mask)
torch.ao.sparsity.FakeSparsity.forward(self,x)
torch.ao.sparsity.FakeSparsity.state_dict(self,*args,**kwargs)
torch.ao.sparsity.fqn_to_module(model,path)
torch.ao.sparsity.module_to_fqn(model,layer,prefix='')
torch.ao.sparsity.sparsifier.utils.FakeSparsity(self,mask)
torch.ao.sparsity.sparsifier.utils.FakeSparsity.__init__(self,mask)
torch.ao.sparsity.sparsifier.utils.FakeSparsity.forward(self,x)
torch.ao.sparsity.sparsifier.utils.FakeSparsity.state_dict(self,*args,**kwargs)
torch.ao.sparsity.sparsifier.utils.fqn_to_module(model,path)
torch.ao.sparsity.sparsifier.utils.module_to_fqn(model,layer,prefix='')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/sparsifier/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/sparsifier/weight_norm_sparsifier.py----------------------------------------
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.zeros_per_block->reduce(lambda x, y: x * y, sparse_block_shape)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.values_per_block->reduce(lambda x, y: x * y, sparse_block_shape)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.mask.data->torch.zeros_like(mask)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.ww_reshaped->ww.reshape(1, *ww.shape)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.ww_pool->torch.nn.functional.avg_pool2d(ww_reshaped, kernel_size=sparse_block_shape, stride=sparse_block_shape, ceil_mode=True)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.ww_pool_flat->torch.nn.functional.avg_pool2d(ww_reshaped, kernel_size=sparse_block_shape, stride=sparse_block_shape, ceil_mode=True).flatten()
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.(_, sorted_idx)->torch.sort(w_flat)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.threshold_idx->int(round(sparsity_level * len(sorted_idx)))
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.(rows, cols)->_flat_idx_to_2d(sorted_idx, submask.shape)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.new_mask->torch.ones(ww.shape, device=layer.weight.device)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.w->torch.abs(subweight)
A:torch.ao.sparsity.sparsifier.weight_norm_sparsifier.w_flat->torch.abs(subweight).flatten()
torch.ao.sparsity.WeightNormSparsifier(self,sparsity_level:float=0.5,sparse_block_shape:Tuple[int,int]=(1,4),zeros_per_block:int=None)
torch.ao.sparsity.WeightNormSparsifier._update_block(self,submask:torch.Tensor,subweight:torch.Tensor,zeros_per_block:int,values_per_block:int)
torch.ao.sparsity.WeightNormSparsifier.update_mask(self,layer,sparsity_level,sparse_block_shape,zeros_per_block,**kwargs)
torch.ao.sparsity.sparsifier.weight_norm_sparsifier.WeightNormSparsifier(self,sparsity_level:float=0.5,sparse_block_shape:Tuple[int,int]=(1,4),zeros_per_block:int=None)
torch.ao.sparsity.sparsifier.weight_norm_sparsifier.WeightNormSparsifier.__init__(self,sparsity_level:float=0.5,sparse_block_shape:Tuple[int,int]=(1,4),zeros_per_block:int=None)
torch.ao.sparsity.sparsifier.weight_norm_sparsifier.WeightNormSparsifier._update_block(self,submask:torch.Tensor,subweight:torch.Tensor,zeros_per_block:int,values_per_block:int)
torch.ao.sparsity.sparsifier.weight_norm_sparsifier.WeightNormSparsifier.update_mask(self,layer,sparsity_level,sparse_block_shape,zeros_per_block,**kwargs)
torch.ao.sparsity.sparsifier.weight_norm_sparsifier._flat_idx_to_2d(idx,shape)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/experimental/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/experimental/pruner/base_pruner.py----------------------------------------
A:torch.ao.sparsity.experimental.pruner.base_pruner.module->fqn_to_module(self.model, config['fqn'])
A:torch.ao.sparsity.experimental.pruner.base_pruner.param->config.get('parametrization', ZeroesParametrization)
A:torch.ao.sparsity.experimental.pruner.base_pruner.local_args->copy.deepcopy(self.defaults)
A:torch.ao.sparsity.experimental.pruner.base_pruner.module_fqn->module_to_fqn(model, module)
torch.ao.sparsity.BasePruner(self,defaults,also_prune_bias=True)
torch.ao.sparsity.BasePruner._prepare(self,use_path=False,*args,**kwargs)
torch.ao.sparsity.BasePruner.get_module_pruned_outputs(self,module)
torch.ao.sparsity.BasePruner.prepare(self,model,config)
torch.ao.sparsity.BasePruner.squash_mask(self,use_path=False,*args,**kwargs)
torch.ao.sparsity.BasePruner.step(self,use_path=False)
torch.ao.sparsity.BasePruner.update_mask(self,layer,**kwargs)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner(self,defaults,also_prune_bias=True)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner.__init__(self,defaults,also_prune_bias=True)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner._prepare(self,use_path=False,*args,**kwargs)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner.get_module_pruned_outputs(self,module)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner.prepare(self,model,config)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner.squash_mask(self,use_path=False,*args,**kwargs)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner.step(self,use_path=False)
torch.ao.sparsity.experimental.pruner.base_pruner.BasePruner.update_mask(self,layer,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/experimental/pruner/parametrization.py----------------------------------------
A:torch.ao.sparsity.experimental.pruner.parametrization.self.original_outputs->set(range(original_outputs.item()))
A:torch.ao.sparsity.experimental.pruner.parametrization.self.pruned_outputs->set()
A:torch.ao.sparsity.experimental.pruner.parametrization.valid_columns->list(max_outputs - pruned_outputs)
A:torch.ao.sparsity.experimental.pruner.parametrization.sizes->list(output.shape)
A:torch.ao.sparsity.experimental.pruner.parametrization.sizes[1]->len(max_outputs)
A:torch.ao.sparsity.experimental.pruner.parametrization.reconstructed_tensor->torch.zeros(sizes, dtype=output.dtype, device=output.device, layout=output.layout)
A:torch.ao.sparsity.experimental.pruner.parametrization.bias->bias.reshape(idx).reshape(idx)
torch.ao.sparsity.ActivationReconstruction(self,parametrization)
torch.ao.sparsity.BiasHook(self,parametrization,prune_bias)
torch.ao.sparsity.PruningParametrization(self,original_outputs)
torch.ao.sparsity.PruningParametrization.forward(self,x)
torch.ao.sparsity.ZeroesParametrization(self,original_outputs)
torch.ao.sparsity.ZeroesParametrization.forward(self,x)
torch.ao.sparsity.experimental.pruner.parametrization.ActivationReconstruction(self,parametrization)
torch.ao.sparsity.experimental.pruner.parametrization.ActivationReconstruction.__init__(self,parametrization)
torch.ao.sparsity.experimental.pruner.parametrization.BiasHook(self,parametrization,prune_bias)
torch.ao.sparsity.experimental.pruner.parametrization.BiasHook.__init__(self,parametrization,prune_bias)
torch.ao.sparsity.experimental.pruner.parametrization.PruningParametrization(self,original_outputs)
torch.ao.sparsity.experimental.pruner.parametrization.PruningParametrization.__init__(self,original_outputs)
torch.ao.sparsity.experimental.pruner.parametrization.PruningParametrization.forward(self,x)
torch.ao.sparsity.experimental.pruner.parametrization.ZeroesParametrization(self,original_outputs)
torch.ao.sparsity.experimental.pruner.parametrization.ZeroesParametrization.__init__(self,original_outputs)
torch.ao.sparsity.experimental.pruner.parametrization.ZeroesParametrization.forward(self,x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/experimental/pruner/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/scheduler/lambda_scheduler.py----------------------------------------
A:torch.ao.sparsity.scheduler.lambda_scheduler.self.sl_lambdas->list(sl_lambda)
torch.ao.sparsity.LambdaSL(self,sparsifier,sl_lambda,last_epoch=-1,verbose=False)
torch.ao.sparsity.LambdaSL.get_sl(self)
torch.ao.sparsity.scheduler.lambda_scheduler.LambdaSL(self,sparsifier,sl_lambda,last_epoch=-1,verbose=False)
torch.ao.sparsity.scheduler.lambda_scheduler.LambdaSL.__init__(self,sparsifier,sl_lambda,last_epoch=-1,verbose=False)
torch.ao.sparsity.scheduler.lambda_scheduler.LambdaSL.get_sl(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/scheduler/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/sparsity/scheduler/base_scheduler.py----------------------------------------
A:torch.ao.sparsity.scheduler.base_scheduler.instance_ref->weakref.ref(method.__self__)
A:torch.ao.sparsity.scheduler.base_scheduler.instance->instance_ref()
A:torch.ao.sparsity.scheduler.base_scheduler.wrapped->func.__get__(instance, cls)
A:torch.ao.sparsity.scheduler.base_scheduler.self.sparsifier.step->with_counter(self.sparsifier.step)
A:torch.ao.sparsity.scheduler.base_scheduler.values->self.get_sl()
torch.ao.sparsity.BaseScheduler(self,sparsifier,last_epoch=-1,verbose=False)
torch.ao.sparsity.BaseScheduler.__repr__(self)
torch.ao.sparsity.BaseScheduler.get_last_sl(self)
torch.ao.sparsity.BaseScheduler.get_sl(self)
torch.ao.sparsity.BaseScheduler.load_state_dict(self,state_dict)
torch.ao.sparsity.BaseScheduler.print_sl(self,is_verbose,group,sl,epoch=None)
torch.ao.sparsity.BaseScheduler.state_dict(self)
torch.ao.sparsity.BaseScheduler.step(self,epoch=None)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler(self,sparsifier,last_epoch=-1,verbose=False)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.__init__(self,sparsifier,last_epoch=-1,verbose=False)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.__repr__(self)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.get_last_sl(self)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.get_sl(self)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.load_state_dict(self,state_dict)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.print_sl(self,is_verbose,group,sl,epoch=None)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.state_dict(self)
torch.ao.sparsity.scheduler.base_scheduler.BaseScheduler.step(self,epoch=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/_numeric_suite.py----------------------------------------
A:torch.ao.ns._numeric_suite.split_str->key.split('.')
A:torch.ao.ns._numeric_suite.match_string->''.join(key_str.split('.')[0:-2])
A:torch.ao.ns._numeric_suite.pattern1->''.join(s2.split('.')[0:-1])
A:torch.ao.ns._numeric_suite.pattern2->''.join(s2.split('.')[0:-2])
A:torch.ao.ns._numeric_suite.match_key->_find_match(sorted(float_dict, reverse=True), key, 'stats')
A:torch.ao.ns._numeric_suite.module_name->'.'.join(split_str[:-3])
A:torch.ao.ns._numeric_suite.self.dequant->torch.nn.quantized.DeQuantize()
A:torch.ao.ns._numeric_suite.self.logger->logger_cls()
A:torch.ao.ns._numeric_suite.xl->_convert_tuple_to_list(x)
A:torch.ao.ns._numeric_suite.output->self.orig_module.add_relu(x, y)
A:torch.ao.ns._numeric_suite.xl_float->_dequantize_tensor_list(xl)
A:torch.ao.ns._numeric_suite.shadow_output->self.shadow_module.add_relu(x, y)
A:torch.ao.ns._numeric_suite.x->x.dequantize().dequantize()
A:torch.ao.ns._numeric_suite.y->y.dequantize().dequantize()
A:torch.ao.ns._numeric_suite.reassign[name]->Shadow(mod, float_mod, logger_cls)
A:torch.ao.ns._numeric_suite.ob_dict->get_logger_dict(q_model)
A:torch.ao.ns._numeric_suite.float_dict->get_logger_dict(float_module)
A:torch.ao.ns._numeric_suite.quantized_dict->get_logger_dict(q_module)
A:torch.ao.ns._numeric_suite.allow_list->get_default_compare_output_module_list()
A:torch.ao.ns._numeric_suite.qconfig_debug->torch.ao.quantization.QConfig(activation=logger_cls, weight=None)
A:torch.ao.ns._numeric_suite.act_compare_dict->get_matching_activations(float_model, q_model)
torch.ao.ns._numeric_suite.Logger(self)
torch.ao.ns._numeric_suite.Logger.__init__(self)
torch.ao.ns._numeric_suite.Logger.forward(self,x)
torch.ao.ns._numeric_suite.OutputLogger(self)
torch.ao.ns._numeric_suite.OutputLogger.__init__(self)
torch.ao.ns._numeric_suite.OutputLogger.forward(self,x)
torch.ao.ns._numeric_suite.Shadow(self,q_module,float_module,logger_cls)
torch.ao.ns._numeric_suite.Shadow.__init__(self,q_module,float_module,logger_cls)
torch.ao.ns._numeric_suite.Shadow.add(self,x:torch.Tensor,y:torch.Tensor)->torch.Tensor
torch.ao.ns._numeric_suite.Shadow.add_relu(self,x:torch.Tensor,y:torch.Tensor)->torch.Tensor
torch.ao.ns._numeric_suite.Shadow.add_scalar(self,x:torch.Tensor,y:float)->torch.Tensor
torch.ao.ns._numeric_suite.Shadow.cat(self,x:List[torch.Tensor],dim:int=0)->torch.Tensor
torch.ao.ns._numeric_suite.Shadow.forward(self,*x)->torch.Tensor
torch.ao.ns._numeric_suite.Shadow.mul(self,x:torch.Tensor,y:torch.Tensor)->torch.Tensor
torch.ao.ns._numeric_suite.Shadow.mul_scalar(self,x:torch.Tensor,y:float)->torch.Tensor
torch.ao.ns._numeric_suite.ShadowLogger(self)
torch.ao.ns._numeric_suite.ShadowLogger.__init__(self)
torch.ao.ns._numeric_suite.ShadowLogger.forward(self,x,y)
torch.ao.ns._numeric_suite._convert_tuple_to_list(t:Any)->Any
torch.ao.ns._numeric_suite._dequantize_tensor_list(t:Any)->Any
torch.ao.ns._numeric_suite._find_match(str_list:Union[Dict[str,Any],List[str]],key_str:str,postfix:str)->Optional[str]
torch.ao.ns._numeric_suite._get_logger_dict_helper(mod:nn.Module,target_dict:Dict[str,Any],prefix:str='')->None
torch.ao.ns._numeric_suite._is_identical_module_type(mod1,mod2)
torch.ao.ns._numeric_suite.compare_model_outputs(float_model:nn.Module,q_model:nn.Module,*data,logger_cls=OutputLogger,allow_list=None)->Dict[str, Dict[str, torch.Tensor]]
torch.ao.ns._numeric_suite.compare_model_stub(float_model:nn.Module,q_model:nn.Module,module_swap_list:Set[type],*data,logger_cls=ShadowLogger)->Dict[str, Dict]
torch.ao.ns._numeric_suite.compare_weights(float_dict:Dict[str,Any],quantized_dict:Dict[str,Any])->Dict[str, Dict[str, torch.Tensor]]
torch.ao.ns._numeric_suite.get_logger_dict(mod:nn.Module,prefix:str='')->Dict[str, Dict]
torch.ao.ns._numeric_suite.get_matching_activations(float_module:nn.Module,q_module:nn.Module)->Dict[str, Dict[str, torch.Tensor]]
torch.ao.ns._numeric_suite.prepare_model_outputs(float_module:nn.Module,q_module:nn.Module,logger_cls=OutputLogger,allow_list=None)->None
torch.ao.ns._numeric_suite.prepare_model_with_stubs(float_module:nn.Module,q_module:nn.Module,module_swap_list:Set[type],logger_cls:Callable)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/_numeric_suite_fx.py----------------------------------------
A:torch.ao.ns._numeric_suite_fx.extracted_weight->extract_weight_from_node(node, model, op_to_type_to_weight_extraction_fn)
A:torch.ao.ns._numeric_suite_fx.matched_subgraph_pairs->get_matching_subgraph_pairs(gm_a, gm_b, base_name_to_sets_of_related_ops, unmatchable_types_map)
A:torch.ao.ns._numeric_suite_fx.results->rekey_logger_info_on_node_name_of_model(results, model_name_to_use_for_layer_names)
A:torch.ao.ns._numeric_suite_fx.base_name_to_sets_of_related_ops->get_base_name_to_sets_of_related_ops()
A:torch.ao.ns._numeric_suite_fx.type_a_related_to_b->get_type_a_related_to_b(base_name_to_sets_of_related_ops)
A:torch.ao.ns._numeric_suite_fx.tracer_a->NSTracer(skipped_module_names, skipped_module_classes)
A:torch.ao.ns._numeric_suite_fx.tracer_b->NSTracer(skipped_module_names, skipped_module_classes)
A:torch.ao.ns._numeric_suite_fx.gm_a->GraphModule(model_a, tracer_a.trace(model_a))
A:torch.ao.ns._numeric_suite_fx.gm_b->GraphModule(model_b, tracer_b.trace(model_b))
A:torch.ao.ns._numeric_suite_fx.model->add_loggers_to_model(model, node_to_instrument_inputs_to_ref_name, node_to_instrument_outputs_to_ref_name, logger_cls, model_name)
A:torch.ao.ns._numeric_suite_fx.ref_node_type_a->get_target_type_str(subgraph_a.base_op_node, gm_a)
A:torch.ao.ns._numeric_suite_fx.ref_node_type_b->get_target_type_str(subgraph_b.base_op_node, gm_b)
A:torch.ao.ns._numeric_suite_fx.new_model_a->_add_loggers_one_model(name_a, gm_a, nodes_and_names_to_instrument_inputs_a, nodes_and_names_to_instrument_outputs_a, logger_cls)
A:torch.ao.ns._numeric_suite_fx.new_model_b->_add_loggers_one_model(name_b, gm_b, nodes_and_names_to_instrument_inputs_b, nodes_and_names_to_instrument_outputs_b, logger_cls)
A:torch.ao.ns._numeric_suite_fx.gm_a_shadows_b->create_a_shadows_b(name_a, gm_a, name_b, gm_b, matched_subgraph_pairs, logger_cls, should_log_inputs=should_log_inputs, node_type_to_io_type_map=node_type_to_io_type_map)
A:torch.ao.ns._numeric_suite_fx.comparison_result->comparison_fn(value_1, value_2)
torch.ao.ns._numeric_suite_fx.NSTracer(quantize_fx.QuantizationTracer)
torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module(self,m:torch.nn.Module,module_qualified_name:str)->bool
torch.ao.ns._numeric_suite_fx.OutputLogger(self,ref_node_name:str,prev_node_name:str,model_name:str,ref_name:str,prev_node_target_type:str,ref_node_target_type:str,results_type:str,index_within_arg:int,index_of_arg:int,fqn:Optional[str])
torch.ao.ns._numeric_suite_fx.OutputLogger.__init__(self,ref_node_name:str,prev_node_name:str,model_name:str,ref_name:str,prev_node_target_type:str,ref_node_target_type:str,results_type:str,index_within_arg:int,index_of_arg:int,fqn:Optional[str])
torch.ao.ns._numeric_suite_fx.OutputLogger.__repr__(self)
torch.ao.ns._numeric_suite_fx.OutputLogger.forward(self,x)
torch.ao.ns._numeric_suite_fx._add_loggers_impl(name_a:str,gm_a:GraphModule,name_b:str,gm_b:GraphModule,logger_cls:Callable,should_log_inputs:bool,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None)->Tuple[nn.Module, nn.Module]
torch.ao.ns._numeric_suite_fx._add_loggers_one_model(model_name:str,model:GraphModule,nodes_and_names_to_instrument_inputs:List[Tuple[Node,str,str]],nodes_and_names_to_instrument_outputs:List[Tuple[Node,str,str]],logger_cls:Callable)->nn.Module
torch.ao.ns._numeric_suite_fx._add_shadow_loggers_impl(name_a:str,gm_a:GraphModule,name_b:str,gm_b:GraphModule,logger_cls:Callable,should_log_inputs:bool,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,node_type_to_io_type_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None)->nn.Module
torch.ao.ns._numeric_suite_fx._extract_logger_info_one_model(model:nn.Module,results:NSResultsType,logger_cls:Callable)->None
torch.ao.ns._numeric_suite_fx._extract_weights_impl(model_name_a:str,gm_a:GraphModule,model_name_b:str,gm_b:GraphModule,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None,op_to_type_to_weight_extraction_fn:Optional[Dict[str,Dict[Callable,Callable]]]=None)->NSResultsType
torch.ao.ns._numeric_suite_fx._extract_weights_one_model(model_name:str,model:GraphModule,nodes_and_names_to_instrument:List[Tuple[Node,str]],results:NSResultsType,op_to_type_to_weight_extraction_fn:Optional[Dict[str,Dict[Callable,Callable]]]=None)->None
torch.ao.ns._numeric_suite_fx.add_loggers(name_a:str,model_a:nn.Module,name_b:str,model_b:nn.Module,logger_cls:Callable,should_log_inputs:bool=False,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None)->Tuple[nn.Module, nn.Module]
torch.ao.ns._numeric_suite_fx.add_shadow_loggers(name_a:str,model_a:nn.Module,name_b:str,model_b:nn.Module,logger_cls:Callable,should_log_inputs:bool=False,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,node_type_to_io_type_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None)->nn.Module
torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison(results:NSResultsType,model_name_1:str,model_name_2:str,comparison_fn:Callable[[torch.Tensor,torch.Tensor],torch.Tensor],comparison_name:str)->None
torch.ao.ns._numeric_suite_fx.extract_logger_info(model_a:nn.Module,model_b:nn.Module,logger_cls:Callable,model_name_to_use_for_layer_names:str)->NSResultsType
torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info(model_a_shadows_b:nn.Module,logger_cls:Callable,model_name_to_use_for_layer_names:str)->NSResultsType
torch.ao.ns._numeric_suite_fx.extract_weights(model_name_a:str,model_a:nn.Module,model_name_b:str,model_b:nn.Module,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None,op_to_type_to_weight_extraction_fn:Optional[Dict[str,Dict[Callable,Callable]]]=None)->NSResultsType


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/_numeric_suite_dbr.py----------------------------------------
A:torch.ao.ns._numeric_suite_dbr.(model_name_a, results_a)->_extract_logger_info_one_model(model_a)
A:torch.ao.ns._numeric_suite_dbr.(model_name_b, results_b)->_extract_logger_info_one_model(model_b)
torch.ao.ns._numeric_suite_dbr._extract_logger_info_one_model(model:torch.nn.Module)->Tuple[str, Any]
torch.ao.ns._numeric_suite_dbr._turn_on_loggers(name:str,model:torch.nn.Module)->None
torch.ao.ns._numeric_suite_dbr.add_loggers(name_a:str,model_a:torch.nn.Module,name_b:str,model_b:torch.nn.Module)->Tuple[torch.nn.Module, torch.nn.Module]
torch.ao.ns._numeric_suite_dbr.extract_logger_info(model_a:torch.nn.Module,model_b:torch.nn.Module,model_name_to_use_for_layer_names:str)->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/utils.py----------------------------------------
A:torch.ao.ns.fx.utils.FP32->enum.auto()
A:torch.ao.ns.fx.utils.INT8->enum.auto()
A:torch.ao.ns.fx.utils.FP16->enum.auto()
A:torch.ao.ns.fx.utils.UNKNOWN->enum.auto()
A:torch.ao.ns.fx.utils.FP32_OR_INT8->enum.auto()
A:torch.ao.ns.fx.utils.mod->getattr_from_fqn(gm, node.target)
A:torch.ao.ns.fx.utils.(_prev_node_input_type, prev_node_output_type)->get_node_first_input_and_output_type(prev_node, gm, logger_cls, node_type_to_io_type_map)
A:torch.ao.ns.fx.utils.is_known_fp32_input_module->any((isinstance(mod, target_type) for target_type in MODS_IO_TYPE_FP32))
A:torch.ao.ns.fx.utils.is_known_int8_input_module->any((isinstance(mod, target_type) for target_type in MODS_IO_TYPE_INT8))
A:torch.ao.ns.fx.utils.is_known_fp32_or_int8_input_module->any((isinstance(module_obj, target_type) for target_type in MODS_IO_TYPE_FP32_OR_INT8))
A:torch.ao.ns.fx.utils.scale_obj->getattr_from_fqn(gm, scale_node.target)
A:torch.ao.ns.fx.utils.zp_obj->getattr_from_fqn(gm, zp_node.target)
A:torch.ao.ns.fx.utils.module_obj->getattr_from_fqn(gm, prev_node.target)
A:torch.ao.ns.fx.utils.node_obj->getattr_from_fqn(gm, node.target)
A:torch.ao.ns.fx.utils.target_type->torch.typename(target_mod)
A:torch.ao.ns.fx.utils.target_mod->getattr_from_fqn(gm, node.target)
A:torch.ao.ns.fx.utils.a0->a0.dequantize().dequantize()
A:torch.ao.ns.fx.utils.a1->a1.dequantize().dequantize()
A:torch.ao.ns.fx.utils.Ps->torch.norm(x)
A:torch.ao.ns.fx.utils.Pn->torch.norm(x - y)
A:torch.ao.ns.fx.utils.x->x.reshape(1, -1).reshape(1, -1)
A:torch.ao.ns.fx.utils.y->y.reshape(1, -1).reshape(1, -1)
torch.ao.ns.fx.utils.NodeInputOrOutputType(enum.Enum)
torch.ao.ns.fx.utils.compute_cosine_similarity(x:torch.Tensor,y:torch.Tensor)->torch.Tensor
torch.ao.ns.fx.utils.compute_normalized_l2_error(x:torch.Tensor,y:torch.Tensor)->torch.Tensor
torch.ao.ns.fx.utils.compute_sqnr(x:torch.Tensor,y:torch.Tensor)->torch.Tensor
torch.ao.ns.fx.utils.get_arg_indices_of_inputs_to_log(node:Node)->List[int]
torch.ao.ns.fx.utils.get_node_first_input_and_output_type(node:Node,gm:GraphModule,logger_cls:Callable,node_type_to_io_type_map:Dict[str,Set[NSNodeTargetType]])->Tuple[NodeInputOrOutputType, NodeInputOrOutputType]
torch.ao.ns.fx.utils.get_node_input_qparams(node:Node,gm:GraphModule,node_type_to_io_type_map:Dict[str,Set[NSNodeTargetType]])->Optional[Tuple[Union[torch.Tensor, float], Union[torch.Tensor, int]]]
torch.ao.ns.fx.utils.get_number_of_non_param_args(node:Node,gm:GraphModule)->int
torch.ao.ns.fx.utils.get_target_type_str(node:Node,gm:GraphModule)->str
torch.ao.ns.fx.utils.maybe_add_missing_fqns(results:NSResultsType)->None
torch.ao.ns.fx.utils.maybe_dequantize_first_two_tensor_args_and_handle_tuples(f)
torch.ao.ns.fx.utils.rekey_logger_info_on_node_name_of_model(results:NSResultsType,model_name:str)->NSResultsType
torch.ao.ns.fx.utils.return_first_non_observer_node(node:Node,gm:GraphModule)->Node


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/mappings.py----------------------------------------
A:torch.ao.ns.fx.mappings.base_name->str(counter)
A:torch.ao.ns.fx.mappings.base_name_to_sets_of_related_ops[str(counter)]->set([op])
torch.ao.ns.fx.mappings.add_op_to_sets_of_related_ops(base_name_to_sets_of_related_ops:Dict[str,Set[NSNodeTargetType]],op:NSNodeTargetType,related_op:Optional[NSNodeTargetType])->None
torch.ao.ns.fx.mappings.get_base_name_for_op(base_name_to_sets_of_related_ops:Dict[str,Set[NSNodeTargetType]],op:NSNodeTargetType)->Optional[str]
torch.ao.ns.fx.mappings.get_base_name_to_sets_of_related_ops()->Dict[str, Set[NSNodeTargetType]]
torch.ao.ns.fx.mappings.get_node_type_to_io_type_map()->Dict[str, Set[NSNodeTargetType]]
torch.ao.ns.fx.mappings.get_unmatchable_types_map()->Dict[str, Set[NSNodeTargetType]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/graph_matcher.py----------------------------------------
A:torch.ao.ns.fx.graph_matcher.cur_end_node->self.stack.pop()
A:torch.ao.ns.fx.graph_matcher.is_match->end_node_matches_reversed_fusion(cur_end_node, _reverse_fusion_ops, self.gm, self.seen_nodes)
A:torch.ao.ns.fx.graph_matcher.maybe_obs->getattr_from_fqn(self.gm, cur_end_node.target)
A:torch.ao.ns.fx.graph_matcher.target_mod->getattr_from_fqn(self.gm, node.target)
A:torch.ao.ns.fx.graph_matcher.EQUAL->enum.auto()
A:torch.ao.ns.fx.graph_matcher.EQUAL_BUT_UKNOWN->enum.auto()
A:torch.ao.ns.fx.graph_matcher.RELATED_BUT_NOT_EQUAL->enum.auto()
A:torch.ao.ns.fx.graph_matcher.NOT_RELATED->enum.auto()
A:torch.ao.ns.fx.graph_matcher.mod_a->getattr_from_fqn(gm_a, node_a.target)
A:torch.ao.ns.fx.graph_matcher.mod_b->getattr_from_fqn(gm_b, node_b.target)
A:torch.ao.ns.fx.graph_matcher.target_type->_get_node_target_type(subgraph_a.base_op_node, gm_a)
A:torch.ao.ns.fx.graph_matcher.mod->getattr_from_fqn(gm, node.target)
A:torch.ao.ns.fx.graph_matcher.unmatchable_types_map->get_unmatchable_types_map()
A:torch.ao.ns.fx.graph_matcher.graph_a_iterator->_NSGraphMatchableSubgraphsIterator(gm_a, non_matchable_functions, non_matchable_modules, non_matchable_methods)
A:torch.ao.ns.fx.graph_matcher.graph_b_iterator->_NSGraphMatchableSubgraphsIterator(gm_b, non_matchable_functions, non_matchable_modules, non_matchable_methods)
A:torch.ao.ns.fx.graph_matcher.results->collections.OrderedDict(reversed(list(results.items())))
A:torch.ao.ns.fx.graph_matcher.base_name_to_sets_of_related_ops->get_base_name_to_sets_of_related_ops()
A:torch.ao.ns.fx.graph_matcher.type_a_related_to_b->get_type_a_related_to_b(base_name_to_sets_of_related_ops)
A:torch.ao.ns.fx.graph_matcher.cur_subgraph_a->next(graph_a_iterator)
A:torch.ao.ns.fx.graph_matcher.cur_subgraph_b->next(graph_b_iterator)
A:torch.ao.ns.fx.graph_matcher.type_start_a->_get_node_target_type(cur_subgraph_a.start_node, gm_a)
A:torch.ao.ns.fx.graph_matcher.type_start_b->_get_node_target_type(cur_subgraph_b.start_node, gm_b)
A:torch.ao.ns.fx.graph_matcher.subgraph_relationship->_get_subgraph_relationship_type(cur_subgraph_a, cur_subgraph_b, gm_a, gm_b, type_a_related_to_b)
A:torch.ao.ns.fx.graph_matcher.key_name_a->_get_name_for_subgraph(cur_subgraph_a, gm_a, base_name_to_sets_of_related_ops, existing_names_a)
A:torch.ao.ns.fx.graph_matcher.key_name_b->_get_name_for_subgraph(cur_subgraph_b, gm_b, base_name_to_sets_of_related_ops, existing_names_b)
torch.ao.ns.fx.graph_matcher.GraphMatchingException(Exception)
torch.ao.ns.fx.graph_matcher.SubgraphTypeRelationship(enum.Enum)
torch.ao.ns.fx.graph_matcher._NSGraphMatchableSubgraphsIterator(self,gm:GraphModule,non_matchable_functions:Set[NSNodeTargetType],non_matchable_modules:Set[NSNodeTargetType],non_matchable_methods:Set[NSNodeTargetType])
torch.ao.ns.fx.graph_matcher._NSGraphMatchableSubgraphsIterator.__init__(self,gm:GraphModule,non_matchable_functions:Set[NSNodeTargetType],non_matchable_modules:Set[NSNodeTargetType],non_matchable_methods:Set[NSNodeTargetType])
torch.ao.ns.fx.graph_matcher._NSGraphMatchableSubgraphsIterator.__iter__(self)
torch.ao.ns.fx.graph_matcher._NSGraphMatchableSubgraphsIterator.__next__(self)->NSSubgraph
torch.ao.ns.fx.graph_matcher._NSGraphMatchableSubgraphsIterator._is_matchable(self,node:Node)->bool
torch.ao.ns.fx.graph_matcher._NSGraphMatchableSubgraphsIterator._recursively_add_node_arg_to_stack(self,arg:Any)->None
torch.ao.ns.fx.graph_matcher._get_name_for_subgraph(subgraph_a:NSSubgraph,gm_a:GraphModule,base_name_to_sets_of_related_ops:Dict[str,Set[NSNodeTargetType]],existing_names:Set[str])->str
torch.ao.ns.fx.graph_matcher._get_node_target_type(node:Node,gm:GraphModule)->Optional[NSNodeTargetType]
torch.ao.ns.fx.graph_matcher._get_output_nodes(g:Graph)->List[Node]
torch.ao.ns.fx.graph_matcher._get_subgraph_relationship_type(subgraph_a:NSSubgraph,subgraph_b:NSSubgraph,gm_a:GraphModule,gm_b:GraphModule,type_a_related_to_b:Set[Tuple[NSNodeTargetType,NSNodeTargetType]])->SubgraphTypeRelationship
torch.ao.ns.fx.graph_matcher.get_matching_subgraph_pairs(gm_a:GraphModule,gm_b:GraphModule,base_name_to_sets_of_related_ops:Optional[Dict[str,Set[NSNodeTargetType]]]=None,unmatchable_types_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None)->Dict[str, Tuple[NSSubgraph, NSSubgraph]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/graph_passes.py----------------------------------------
A:torch.ao.ns.fx.graph_passes.module->getattr_from_fqn(gm, node.target)
A:torch.ao.ns.fx.graph_passes.logger_node_name->get_new_attr_name_with_prefix(node.name + logger_node_name_suffix)(gm)
A:torch.ao.ns.fx.graph_passes.target_type->get_target_type_str(node, gm)
A:torch.ao.ns.fx.graph_passes.logger_obj->logger_cls(ref_node_name, node.name, model_name, ref_name, target_type, ref_node_target_type, results_type, index_within_arg, index_of_arg, fqn)
A:torch.ao.ns.fx.graph_passes.logger_node->node.graph.create_node('call_module', logger_node_name, (node,), {})
A:torch.ao.ns.fx.graph_passes.new_graph->Graph()
A:torch.ao.ns.fx.graph_passes.modules->dict(gm_b.named_modules())
A:torch.ao.ns.fx.graph_passes.fqn->_maybe_get_fqn(node, gm)
A:torch.ao.ns.fx.graph_passes.arg_indices_to_log->get_arg_indices_of_inputs_to_log(node)
A:torch.ao.ns.fx.graph_passes.env[node_arg.name]->_insert_logger_after_node(prev_node, gm, logger_cls, '_ns_logger_', node.name, model_name, ref_name, ref_node_type, NSSingleResultValuesType.NODE_INPUT.value, index_within_arg=0, index_of_arg=node_arg_idx, fqn=fqn)
A:torch.ao.ns.fx.graph_passes.env[prev_node.name]->_insert_logger_after_node(prev_node, gm, logger_cls, '_ns_logger_', node.name, model_name, ref_name, ref_node_type, NSSingleResultValuesType.NODE_INPUT.value, index_within_arg=arg_idx, index_of_arg=node_arg_idx, fqn=fqn)
A:torch.ao.ns.fx.graph_passes.env[node.name]->Graph().node_copy(node, load_arg)
A:torch.ao.ns.fx.graph_passes.new_gm->GraphModule(gm, new_graph)
A:torch.ao.ns.fx.graph_passes.scale_node_name->get_new_attr_name_with_prefix(node_a.name + '_input_scale_')(gm_b)
A:torch.ao.ns.fx.graph_passes.scale_node->Graph().create_node('get_attr', scale_node_name, (), {}, scale_node_name)
A:torch.ao.ns.fx.graph_passes.zero_point_node_name->get_new_attr_name_with_prefix(node_a.name + '_input_zero_point_')(gm_b)
A:torch.ao.ns.fx.graph_passes.zero_point_node->Graph().create_node('get_attr', zero_point_node_name, (), {}, zero_point_node_name)
A:torch.ao.ns.fx.graph_passes.(node_input_type_a, _node_output_type_a)->get_node_first_input_and_output_type(node_a, gm_a, logger_cls, node_type_to_io_type_map)
A:torch.ao.ns.fx.graph_passes.(node_input_type_c, _node_output_type_c)->get_node_first_input_and_output_type(node_c, gm_b, logger_cls, node_type_to_io_type_map)
A:torch.ao.ns.fx.graph_passes.node_a_input_qparams->get_node_input_qparams(subgraph_a.start_node, gm_a, node_type_to_io_type_map)
A:torch.ao.ns.fx.graph_passes.new_dtype_cast_name->get_new_attr_name_with_prefix(node_name_prefix)(gm_b)
A:torch.ao.ns.fx.graph_passes.dtype_cast_mod->dtype_cast_mod_cls()
A:torch.ao.ns.fx.graph_passes.new_dtype_cast_node->Graph().create_node('call_module', new_dtype_cast_name, (prev_node_c_inner,), {}, new_dtype_cast_name)
A:torch.ao.ns.fx.graph_passes.node_a_copy_name->get_new_attr_name_with_prefix(node_a.name + '_shadow_copy_')(gm_b)
A:torch.ao.ns.fx.graph_passes.node_a_obj->node_a_obj.detach().detach()
A:torch.ao.ns.fx.graph_passes.node_a_copy->Graph().create_node(node_a.op, node_a.target, (arg_copy, node_a.args[1]), {}, node_a_copy_name)
A:torch.ao.ns.fx.graph_passes.arg_copy->_copy_node_from_a_to_c(node_a.args[0], gm_a, gm_b, graph_c)
A:torch.ao.ns.fx.graph_passes.cur_node_c->_insert_copy_of_node_a_after_input_node_c(prev_node_c, None, cur_node_a, gm_a, gm_b, node_name_prefix)
A:torch.ao.ns.fx.graph_passes.arg_a->return_first_non_observer_node(node_a_arg, gm_a)
A:torch.ao.ns.fx.graph_passes.node_a_arg_copy->_copy_node_from_a_to_c(arg_a, gm_a, gm_b, graph_c)
A:torch.ao.ns.fx.graph_passes.kwarg_a->return_first_non_observer_node(node_a_kwarg, gm_a)
A:torch.ao.ns.fx.graph_passes.node_a_kwarg_copy->_copy_node_from_a_to_c(kwarg_a, gm_a, gm_b, graph_c)
A:torch.ao.ns.fx.graph_passes.node_a_shadows_c_name->get_new_attr_name_with_prefix(node_name_prefix)(gm_b)
A:torch.ao.ns.fx.graph_passes.new_mod_copy_name->get_new_attr_name_with_prefix(node_name_prefix)(gm_b)
A:torch.ao.ns.fx.graph_passes.mod_a->getattr_from_fqn(gm_a, node_a.target)
A:torch.ao.ns.fx.graph_passes.node_a_shadows_c->_insert_copy_of_subgraph_a_after_input_node_c(dtype_cast_node, node_c_second_non_param_arg, subgraph_a, gm_a, gm_b, node_c.name + '_shadow_copy_')
A:torch.ao.ns.fx.graph_passes.node_type_to_io_type_map->get_node_type_to_io_type_map()
A:torch.ao.ns.fx.graph_passes.graph_c->Graph()
A:torch.ao.ns.fx.graph_passes.ref_node_type_a->get_target_type_str(subgraph_a.base_op_node, gm_a)
A:torch.ao.ns.fx.graph_passes.ref_node_type_b->get_target_type_str(subgraph_b.base_op_node, gm_b)
A:torch.ao.ns.fx.graph_passes.(node_input_type_a, node_output_type_a)->get_node_first_input_and_output_type(subgraph_a.start_node, gm_a, logger_cls, node_type_to_io_type_map)
A:torch.ao.ns.fx.graph_passes.(node_input_type_b, node_output_type_b)->get_node_first_input_and_output_type(node_b, gm_b, logger_cls, node_type_to_io_type_map)
A:torch.ao.ns.fx.graph_passes.env_c[node_b.name]->Graph().node_copy(node_b, load_arg)
A:torch.ao.ns.fx.graph_passes.fqn_base_a->_maybe_get_fqn(subgraph_a.base_op_node, gm_a)
A:torch.ao.ns.fx.graph_passes.fqn_base_b->_maybe_get_fqn(subgraph_b.base_op_node, gm_b)
A:torch.ao.ns.fx.graph_passes.env_c[prev_node_c.name]->_insert_logger_after_node(prev_node_c, gm_b, logger_cls, '_ns_logger_b_inp_', node_b.name, name_b, ref_name, ref_node_type_b, NSSingleResultValuesType.NODE_INPUT.value, index_within_arg=arg_idx, index_of_arg=0, fqn=fqn_base_b)
A:torch.ao.ns.fx.graph_passes.dtype_cast_node->_insert_logger_after_node(dtype_cast_node, gm_b, logger_cls, '_ns_logger_a_inp_', ref_node_name, name_a, ref_name, ref_node_type_a, NSSingleResultValuesType.NODE_INPUT.value, index_within_arg=0, index_of_arg=0, fqn=fqn_base_a)
A:torch.ao.ns.fx.graph_passes.dtype_cast_logger->_insert_logger_after_node(dtype_cast_node_inner, gm_b, logger_cls, '_ns_logger_a_inp_', ref_node_name, name_a, ref_name, ref_node_type_a, NSSingleResultValuesType.NODE_INPUT.value, index_within_arg=dtype_cast_idx, index_of_arg=0, fqn=fqn_base_a)
A:torch.ao.ns.fx.graph_passes.num_non_param_args_node_a->get_number_of_non_param_args(subgraph_a.start_node, gm_a)
A:torch.ao.ns.fx.graph_passes.input_logger_mod->getattr(gm_b, input_logger_inner.name)
A:torch.ao.ns.fx.graph_passes.env_c[node_a_shadows_c.name]->_insert_logger_after_node(env_c[node_a_shadows_c.name], gm_b, logger_cls, '_ns_logger_a_', node_a_shadows_c.name, name_a, ref_name, ref_node_type_a, NSSingleResultValuesType.NODE_OUTPUT.value, index_within_arg=0, index_of_arg=0, fqn=fqn_base_a)
A:torch.ao.ns.fx.graph_passes.gm_c->GraphModule(gm_b, graph_c)
torch.ao.ns.fx.graph_passes._copy_node_from_a_to_c(node_a:Node,gm_a:GraphModule,gm_b:GraphModule,graph_c:Graph)->Node
torch.ao.ns.fx.graph_passes._insert_copy_of_node_a_after_input_node_c(input_node_c:Union[Node,List[Node]],input_node_c_2:Optional[Union[Node,List[Node]]],node_a:Node,gm_a:GraphModule,gm_b:GraphModule,node_name_prefix:str)->Node
torch.ao.ns.fx.graph_passes._insert_copy_of_subgraph_a_after_input_node_c(input_node_c:Union[Node,List[Node]],input_node_c_2:Optional[Union[Node,List[Node]]],subgraph_a:NSSubgraph,gm_a:GraphModule,gm_b:GraphModule,node_name_prefix:str)->Node
torch.ao.ns.fx.graph_passes._insert_dtype_cast_after_node(node_a:Node,node_c:Node,prev_node_c:Union[Node,List[Node]],gm_a:GraphModule,gm_b:GraphModule,graph_c:Graph,node_name_prefix:str,logger_cls:Callable,node_type_to_io_type_map:Dict[str,Set[NSNodeTargetType]])->Union[Node, List[Node]]
torch.ao.ns.fx.graph_passes._insert_logger_after_node(node:Node,gm:GraphModule,logger_cls:Callable,logger_node_name_suffix:str,ref_node_name:str,model_name:str,ref_name:str,ref_node_target_type:str,results_type:str,index_within_arg:int,index_of_arg:int,fqn:Optional[str])->Node
torch.ao.ns.fx.graph_passes._insert_quantize_per_tensor_node(prev_node_c:Node,node_a:Node,gm_b:GraphModule,graph_c:Graph,scale:Union[torch.Tensor,float],zero_point:Union[torch.Tensor,int],dtype_cast_name:str)->Node
torch.ao.ns.fx.graph_passes._maybe_get_fqn(node:Node,gm:GraphModule)->Optional[str]
torch.ao.ns.fx.graph_passes.add_loggers_to_model(gm:GraphModule,node_to_instrument_inputs_to_ref_node_name:Dict[Node,Tuple[str,str]],node_to_instrument_outputs_to_ref_node_name:Dict[Node,Tuple[str,str]],logger_cls:Callable,model_name:str)->GraphModule
torch.ao.ns.fx.graph_passes.create_a_shadows_b(name_a:str,gm_a:GraphModule,name_b:str,gm_b:GraphModule,matched_subgraph_pairs:Dict[str,Tuple[NSSubgraph,NSSubgraph]],logger_cls:Callable,should_log_inputs:bool,node_type_to_io_type_map:Optional[Dict[str,Set[NSNodeTargetType]]]=None)->GraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/ns_types.py----------------------------------------
A:torch.ao.ns.fx.ns_types.NSSubgraph->NamedTuple('NSSubgraph', [('start_node', Node), ('end_node', Node), ('base_op_node', Node)])
torch.ao.ns.fx.ns_types.NSSingleResultValuesType(str,enum.Enum)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/weight_utils.py----------------------------------------
A:torch.ao.ns.fx.weight_utils.param_value->getattr_from_fqn(gm, node.target)._flat_weights[idx].detach()
A:torch.ao.ns.fx.weight_utils.weight_node->return_first_non_observer_node(weight_arg_node, gm)
A:torch.ao.ns.fx.weight_utils.weight->weight_extraction_fn(mod)
A:torch.ao.ns.fx.weight_utils.qconv_state_obj->getattr_from_fqn(gm, qconv_state_node.target)
A:torch.ao.ns.fx.weight_utils.packed_weight->getattr_from_fqn(gm, packed_weight_node.target)
A:torch.ao.ns.fx.weight_utils.((weight, _bias), _name)->getattr_from_fqn(gm, packed_weight_node.target).__getstate__()
A:torch.ao.ns.fx.weight_utils.op_to_type_to_weight_extraction_fn->get_op_to_type_to_weight_extraction_fn()
A:torch.ao.ns.fx.weight_utils.ref_node_type->get_target_type_str(node, gm)
A:torch.ao.ns.fx.weight_utils.mod->getattr_from_fqn(gm, node.target)
torch.ao.ns.fx.weight_utils.extract_weight_from_node(node:Node,gm:GraphModule,op_to_type_to_weight_extraction_fn:Optional[Dict[str,Dict[Callable,Callable]]]=None)->Optional[NSSingleResultType]
torch.ao.ns.fx.weight_utils.get_conv_fun_weight(node:Node,gm:GraphModule)->torch.Tensor
torch.ao.ns.fx.weight_utils.get_conv_mod_weight(mod:nn.Module)->torch.Tensor
torch.ao.ns.fx.weight_utils.get_linear_fun_weight(node:Node,gm:GraphModule)->torch.Tensor
torch.ao.ns.fx.weight_utils.get_linear_mod_weight(mod:nn.Module)->torch.Tensor
torch.ao.ns.fx.weight_utils.get_lstm_mod_weights(mod:nn.Module)->List[torch.Tensor]
torch.ao.ns.fx.weight_utils.get_lstm_weight(mod:nn.Module)->List[torch.Tensor]
torch.ao.ns.fx.weight_utils.get_op_to_type_to_weight_extraction_fn()->Dict[str, Dict[Callable, Callable]]
torch.ao.ns.fx.weight_utils.get_qconv_fun_weight(node:Node,gm:GraphModule)->torch.Tensor
torch.ao.ns.fx.weight_utils.get_qlinear_fun_weight(node:Node,gm:GraphModule)->torch.Tensor
torch.ao.ns.fx.weight_utils.get_qlstm_weight(mod:nn.Module)->List[torch.Tensor]
torch.ao.ns.fx.weight_utils.mod_0_weight_detach(mod:nn.Module)->torch.Tensor
torch.ao.ns.fx.weight_utils.mod_weight_bias_0(mod:nn.Module)->torch.Tensor
torch.ao.ns.fx.weight_utils.mod_weight_detach(mod:nn.Module)->torch.Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/ns/fx/pattern_utils.py----------------------------------------
A:torch.ao.ns.fx.pattern_utils.s_list->list(s)
A:torch.ao.ns.fx.pattern_utils.all_quant_patterns->get_default_quant_patterns()
A:torch.ao.ns.fx.pattern_utils.fusion_el_is_mod->isinstance(cur_fusion_el, type)
A:torch.ao.ns.fx.pattern_utils.target_mod->getattr_from_fqn(gm, cur_node.target)
A:torch.ao.ns.fx.pattern_utils.fusion_el_is_meth_without_args->isinstance(cur_fusion_el, str)
torch.ao.ns.fx.pattern_utils.end_node_matches_reversed_fusion(end_node:Node,reversed_fusion:NSFusionType,gm:GraphModule,seen_nodes:Set[Node])->bool
torch.ao.ns.fx.pattern_utils.get_reversed_fusions()->List[Tuple[NSFusionType, int]]
torch.ao.ns.fx.pattern_utils.get_type_a_related_to_b(base_name_to_sets_of_related_ops:Dict[str,Set[NSNodeTargetType]])->Set[Tuple[NSNodeTargetType, NSNodeTargetType]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_quantize_dbr.py----------------------------------------
A:torch.ao.quantization._quantize_dbr.flattened_qconfig_dict->get_flattened_qconfig_dict(qconfig_dict)
A:torch.ao.quantization._quantize_dbr.non_traceable_module_class->prepare_custom_config_dict.get('non_traceable_module_class', [])
A:torch.ao.quantization._quantize_dbr.model->add_auto_convert(model)
A:torch.ao.quantization._quantize_dbr.module_fusion_fqns->get_module_fusion_fqns(model)
A:torch.ao.quantization._quantize_dbr.static_mappings->get_default_static_quant_module_mappings()
A:torch.ao.quantization._quantize_dbr.dynamic_mappings->get_default_dynamic_quant_module_mappings()
torch.ao.quantization._quantize_dbr.convert(model:torch.nn.Module)->torch.nn.Module
torch.ao.quantization._quantize_dbr.prepare(model,qconfig_dict,example_inputs,inplace=False,allow_list=None,observer_non_leaf_module_list=None,prepare_custom_config_dict=None,fuse_modules=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_equalize.py----------------------------------------
A:torch.ao.quantization._equalize._all_supported_types->_supported_types.union(_supported_intrinsic_types)
A:torch.ao.quantization._equalize.module.weight->torch.nn.Parameter(weight)
A:torch.ao.quantization._equalize.module[0].weight->torch.nn.Parameter(weight)
A:torch.ao.quantization._equalize.module.bias->torch.nn.Parameter(bias)
A:torch.ao.quantization._equalize.module[0].bias->torch.nn.Parameter(bias)
A:torch.ao.quantization._equalize.(input, _)->input.min(axis, keepdim)
A:torch.ao.quantization._equalize.axis_list->list(range(size_of_tensor_dim))
A:torch.ao.quantization._equalize.mins->min_over_ndim(input, axis_list)
A:torch.ao.quantization._equalize.maxs->max_over_ndim(input, axis_list)
A:torch.ao.quantization._equalize.weight1->get_module_weight(module1)
A:torch.ao.quantization._equalize.weight2->get_module_weight(module2)
A:torch.ao.quantization._equalize.bias->get_module_bias(module1)
A:torch.ao.quantization._equalize.weight1_range->channel_range(weight1, output_axis)
A:torch.ao.quantization._equalize.weight2_range->channel_range(weight2, input_axis)
A:torch.ao.quantization._equalize.scaling_factors->torch.reshape(scaling_factors, size2)
A:torch.ao.quantization._equalize.inverse_scaling_factors->torch.reshape(inverse_scaling_factors, size1)
A:torch.ao.quantization._equalize.size1[output_axis]->get_module_weight(module1).size(output_axis)
A:torch.ao.quantization._equalize.size2[input_axis]->get_module_weight(module2).size(input_axis)
A:torch.ao.quantization._equalize.model->copy.deepcopy(model)
A:torch.ao.quantization._equalize.previous_name_to_module[pair[0]]->copy.deepcopy(name_to_module[pair[0]])
A:torch.ao.quantization._equalize.previous_name_to_module[pair[1]]->copy.deepcopy(name_to_module[pair[1]])
A:torch.ao.quantization._equalize.summed_norms->torch.tensor(0.0)
A:torch.ao.quantization._equalize.curr_weight->get_module_weight(curr_modules[name])
A:torch.ao.quantization._equalize.prev_weight->get_module_weight(prev_modules[name])
A:torch.ao.quantization._equalize.difference->get_module_weight(curr_modules[name]).sub(prev_weight)
torch.ao.quantization._equalize.channel_range(input,axis=0)
torch.ao.quantization._equalize.converged(curr_modules,prev_modules,threshold=0.0001)
torch.ao.quantization._equalize.cross_layer_equalization(module1,module2,output_axis=0,input_axis=1)
torch.ao.quantization._equalize.equalize(model,paired_modules_list,threshold=0.0001,inplace=True)
torch.ao.quantization._equalize.get_module_bias(module)
torch.ao.quantization._equalize.get_module_weight(module)
torch.ao.quantization._equalize.max_over_ndim(input,axis_list,keepdim=False)
torch.ao.quantization._equalize.min_over_ndim(input,axis_list,keepdim=False)
torch.ao.quantization._equalize.set_module_bias(module,bias)->None
torch.ao.quantization._equalize.set_module_weight(module,weight)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/quantization_mappings.py----------------------------------------
A:torch.ao.quantization.quantization_mappings.no_observers->set([nn.quantizable.LSTM, nn.quantizable.MultiheadAttention])
A:torch.ao.quantization.quantization_mappings.mapping->copy.deepcopy(DEFAULT_QAT_MODULE_MAPPINGS)
A:torch.ao.quantization.quantization_mappings.all_mappings->get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping)
A:torch.ao.quantization.quantization_mappings.static_quant_module_class->get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping).get(float_module_class, None)
A:torch.ao.quantization.quantization_mappings.dynamic_quant_module_class->get_combined_dict(DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS, additional_dynamic_quant_mapping).get(float_module_class, None)
A:torch.ao.quantization.quantization_mappings.quantized_op->DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS.get(float_op, None)
torch.ao.quantization._get_special_act_post_process(module:torch.nn.Module)->Optional[Callable]
torch.ao.quantization._has_special_act_post_process(module:torch.nn.Module)->bool
torch.ao.quantization.get_default_compare_output_module_list()->Set[Callable]
torch.ao.quantization.get_default_dynamic_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_default_dynamic_sparse_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_default_float_to_quantized_operator_mappings()->Dict[Union[Callable, str], Callable]
torch.ao.quantization.get_default_qat_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_default_qconfig_propagation_list()->Set[Callable]
torch.ao.quantization.get_default_static_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_default_static_sparse_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_dynamic_quant_module_class(float_module_class:Callable,additional_dynamic_quant_mapping:Optional[Dict[Callable,Any]]=None)->Any
torch.ao.quantization.get_embedding_qat_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_embedding_static_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.get_quantized_operator(float_op:Union[Callable,str])->Callable
torch.ao.quantization.get_static_quant_module_class(float_module_class:Callable,additional_static_quant_mapping:Optional[Dict[Callable,Any]]=None,is_reference:bool=False)->Any
torch.ao.quantization.no_observer_set()->Set[Any]
torch.ao.quantization.quantization_mappings._get_special_act_post_process(module:torch.nn.Module)->Optional[Callable]
torch.ao.quantization.quantization_mappings._has_special_act_post_process(module:torch.nn.Module)->bool
torch.ao.quantization.quantization_mappings.get_default_compare_output_module_list()->Set[Callable]
torch.ao.quantization.quantization_mappings.get_default_dynamic_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_default_dynamic_sparse_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_default_float_to_quantized_operator_mappings()->Dict[Union[Callable, str], Callable]
torch.ao.quantization.quantization_mappings.get_default_qat_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_default_qconfig_propagation_list()->Set[Callable]
torch.ao.quantization.quantization_mappings.get_default_static_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_default_static_sparse_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_dynamic_quant_module_class(float_module_class:Callable,additional_dynamic_quant_mapping:Optional[Dict[Callable,Any]]=None)->Any
torch.ao.quantization.quantization_mappings.get_embedding_qat_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_embedding_static_quant_module_mappings()->Dict[Callable, Any]
torch.ao.quantization.quantization_mappings.get_quantized_operator(float_op:Union[Callable,str])->Callable
torch.ao.quantization.quantization_mappings.get_static_quant_module_class(float_module_class:Callable,additional_static_quant_mapping:Optional[Dict[Callable,Any]]=None,is_reference:bool=False)->Any
torch.ao.quantization.quantization_mappings.no_observer_set()->Set[Any]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/utils.py----------------------------------------
A:torch.ao.quantization.utils.d->default_dict.copy()
A:torch.ao.quantization.utils.(scale, zero_point)->observer_or_fake_quant.calculate_qparams()
A:torch.ao.quantization.utils.quant_type->get_quant_type(qconfig)
A:torch.ao.quantization.utils.quant_type_str->quant_type_to_str(quant_type)
A:torch.ao.quantization.utils.class_mapping->custom_module_class_mapping.get(quant_type_str, {})
A:torch.ao.quantization.utils.activation->qconfig.activation()
A:torch.ao.quantization.utils.weight->qconfig.weight()
A:torch.ao.quantization.utils.(activation_dtype, weight_dtype, activation_compute_dtype)->get_qconfig_dtypes(qconfig)
A:torch.ao.quantization.utils.r->target.rsplit('.', 1)
torch.ao.quantization.utils._parent_name(target)
torch.ao.quantization.utils.activation_dtype(qconfig)
torch.ao.quantization.utils.activation_is_int8_quantized(qconfig)
torch.ao.quantization.utils.activation_is_statically_quantized(qconfig)
torch.ao.quantization.utils.calculate_qmin_qmax(quant_min:int,quant_max:int,has_customized_qrange:bool,dtype:torch.dtype,reduce_range:bool)->Tuple[int, int]
torch.ao.quantization.utils.check_min_max_valid(min_val:torch.Tensor,max_val:torch.Tensor)->bool
torch.ao.quantization.utils.check_node(node,modules)
torch.ao.quantization.utils.get_combined_dict(default_dict,additional_dict)
torch.ao.quantization.utils.get_qconfig_dtypes(qconfig)
torch.ao.quantization.utils.get_qparam_dict(observer_or_fake_quant)
torch.ao.quantization.utils.get_quant_type(qconfig)
torch.ao.quantization.utils.get_swapped_custom_module_class(custom_module,custom_module_class_mapping,qconfig)
torch.ao.quantization.utils.getattr_from_fqn(obj:Any,fqn:str)->Any
torch.ao.quantization.utils.is_per_channel(qscheme)
torch.ao.quantization.utils.is_per_tensor(qscheme)
torch.ao.quantization.utils.op_is_int8_dynamically_quantized(qconfig)->bool
torch.ao.quantization.utils.weight_dtype(qconfig)
torch.ao.quantization.utils.weight_is_quantized(qconfig)
torch.ao.quantization.utils.weight_is_statically_quantized(qconfig)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fuse_modules.py----------------------------------------
A:torch.ao.quantization.fuse_modules.tokens->submodule_key.split('.')
A:torch.ao.quantization.fuse_modules.cur_mod->getattr(cur_mod, s)
A:torch.ao.quantization.fuse_modules.types->tuple((type(m) for m in mod_list))
A:torch.ao.quantization.fuse_modules.fuser_method->get_fuser_method(types, additional_fuser_method_mapping)
A:torch.ao.quantization.fuse_modules.fused->fuser_method(is_qat, *mod_list)
A:torch.ao.quantization.fuse_modules.identity->torch.nn.Identity()
A:torch.ao.quantization.fuse_modules.additional_fuser_method_mapping->fuse_custom_config_dict.get('additional_fuser_method_mapping', {})
A:torch.ao.quantization.fuse_modules.new_mod_list->fuser_func(mod_list, is_qat, additional_fuser_method_mapping)
A:torch.ao.quantization.fuse_modules.model->copy.deepcopy(model)
torch.ao.quantization.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules,fuse_custom_config_dict=None)
torch.ao.quantization.fuse_modules._fuse_modules(model,modules_to_fuse,is_qat,inplace=False,fuser_func=fuse_known_modules,fuse_custom_config_dict=None)
torch.ao.quantization.fuse_modules._fuse_modules_helper(model,modules_to_fuse,is_qat,fuser_func=fuse_known_modules,fuse_custom_config_dict=None)
torch.ao.quantization.fuse_modules._get_module(model,submodule_key)
torch.ao.quantization.fuse_modules._set_module(model,submodule_key,module)
torch.ao.quantization.fuse_modules.fuse_known_modules(mod_list,is_qat,additional_fuser_method_mapping=None)
torch.ao.quantization.fuse_modules.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules,fuse_custom_config_dict=None)
torch.ao.quantization.fuse_modules.fuse_modules_qat(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules,fuse_custom_config_dict=None)
torch.ao.quantization.fuse_modules_qat(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules,fuse_custom_config_dict=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/observer.py----------------------------------------
A:torch.ao.quantization.observer.result->_PartialWrapper(p=self.p)
A:torch.ao.quantization.observer.r->_PartialWrapper(partial(cls_or_self))
A:torch.ao.quantization.observer.with_args->classmethod(_with_args)
A:torch.ao.quantization.observer.with_callable_args->classmethod(_with_callable_args)
A:torch.ao.quantization.observer.factory_kwargs->torch.nn.factory_kwargs(factory_kwargs)
A:torch.ao.quantization.observer.(self.quant_min, self.quant_max)->calculate_qmin_qmax(quant_min, quant_max, self.has_customized_qrange, self.dtype, self.reduce_range)
A:torch.ao.quantization.observer.version->local_metadata.get('version', None)
A:torch.ao.quantization.observer.eps->torch.tensor([torch.finfo(torch.float32).eps])
A:torch.ao.quantization.observer.min_val_neg->torch.min(min_val, torch.zeros_like(min_val))
A:torch.ao.quantization.observer.max_val_pos->torch.max(-min_val_neg, max_val_pos)
A:torch.ao.quantization.observer.scale->torch.tensor([float(scale)], dtype=scale.dtype, device=device)
A:torch.ao.quantization.observer.zero_point->torch.tensor([float(zero_point)], dtype=zero_point.dtype, device=device)
A:torch.ao.quantization.observer.x->x_orig.detach()
A:torch.ao.quantization.observer.(min_val_cur, max_val_cur)->torch.aminmax(y, dim=1)
A:torch.ao.quantization.observer.min_val->torch.min(min_val_cur, min_val)
A:torch.ao.quantization.observer.max_val->torch.max(max_val_cur, max_val)
A:torch.ao.quantization.observer.(min_val, max_val)->torch.aminmax(x)
A:torch.ao.quantization.observer.x_dim->x_orig.detach().size()
A:torch.ao.quantization.observer.y->torch.flatten(y, start_dim=1)
A:torch.ao.quantization.observer.self.min_val->torch.tensor([])
A:torch.ao.quantization.observer.self.max_val->torch.tensor([])
A:torch.ao.quantization.observer.src_bin->torch.arange(self.bins, device=self.histogram.device)
A:torch.ao.quantization.observer.dst_bin_of_begin->torch.clamp(torch.div(src_bin_begin, dst_bin_width, rounding_mode='floor'), 0, self.dst_nbins - 1)
A:torch.ao.quantization.observer.dst_bin_of_end->torch.clamp(torch.div(src_bin_end, dst_bin_width, rounding_mode='floor'), 0, self.dst_nbins - 1)
A:torch.ao.quantization.observer.norm->self._compute_quantization_error(next_start_bin, next_end_bin)
A:torch.ao.quantization.observer.total->torch.sum(self.histogram).item()
A:torch.ao.quantization.observer.cSum->torch.cumsum(self.histogram, dim=0)
A:torch.ao.quantization.observer.norm_min->float('inf')
A:torch.ao.quantization.observer.downsample_rate->int(torch.ceil((combined_max - combined_min) / (self.bins * hist_bin_width)).item())
A:torch.ao.quantization.observer.start_idx->int(torch.round((self.min_val - combined_min) / hist_bin_width).item())
A:torch.ao.quantization.observer.upsampled_histogram->new_hist.repeat_interleave(upsample_rate)
A:torch.ao.quantization.observer.histogram_with_output_range->torch.zeros(Nbins * downsample_rate, device=orig_hist.device)
A:torch.ao.quantization.observer.shifted_integral_histogram->torch.zeros(Nbins, device=orig_hist.device)
A:torch.ao.quantization.observer.(new_min, new_max)->self._non_linear_param_search()
A:torch.ao.quantization.observer.combined_min->torch.min(new_min, min_val)
A:torch.ao.quantization.observer.combined_max->torch.max(new_max, max_val)
A:torch.ao.quantization.observer.(combined_min, combined_max, downsample_rate, start_idx)->self._adjust_min_max(combined_min, combined_max, self.upsample_rate)
A:torch.ao.quantization.observer.combined_histogram->self._combine_histograms(combined_histogram, self.histogram, self.upsample_rate, downsample_rate, start_idx, self.bins)
A:torch.ao.quantization.observer.state_dict[min_val_name]->torch.tensor(float('inf'))
A:torch.ao.quantization.observer.state_dict[max_val_name]->torch.tensor(float('-inf'))
A:torch.ao.quantization.observer.name->re.sub('\\.___torch_mangle_\\d+', '', suffix)
A:torch.ao.quantization.observer.od->OrderedDict()
A:torch.ao.quantization.observer.default_observer->MinMaxObserver.with_args(quant_min=0, quant_max=127)
A:torch.ao.quantization.observer.default_weight_observer->MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)
A:torch.ao.quantization.observer.default_histogram_observer->HistogramObserver.with_args(quant_min=0, quant_max=127)
A:torch.ao.quantization.observer.default_per_channel_weight_observer->PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
A:torch.ao.quantization.observer.default_dynamic_quant_observer->PlaceholderObserver.with_args(dtype=torch.float, compute_dtype=torch.quint8)
A:torch.ao.quantization.observer.default_float_qparams_observer->PerChannelMinMaxObserver.with_args(dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams, ch_axis=0)
A:torch.ao.quantization.observer.default_float_qparams_observer_4bit->PerChannelMinMaxObserver.with_args(dtype=torch.quint4x2, qscheme=torch.per_channel_affine_float_qparams, ch_axis=0)
A:torch.ao.quantization.observer.default_symmetric_fixed_qparams_observer->FixedQParamsObserver.with_args(scale=2.0 / 256.0, zero_point=128, dtype=torch.quint8, quant_min=0, quant_max=255)
A:torch.ao.quantization.observer.default_affine_fixed_qparams_observer->FixedQParamsObserver.with_args(scale=1.0 / 256.0, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255)
torch.ao.quantization.FixedQParamsObserver(self,scale,zero_point,dtype=torch.quint8,qscheme=torch.per_tensor_affine,quant_min=0,quant_max=255)
torch.ao.quantization.FixedQParamsObserver.calculate_qparams(self)
torch.ao.quantization.FixedQParamsObserver.forward(self,X)
torch.ao.quantization.HistogramObserver(self,bins:int=2048,upsample_rate:int=128,dtype:torch.dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.HistogramObserver._adjust_min_max(self,combined_min:torch.Tensor,combined_max:torch.Tensor,upsample_rate:int)->Tuple[torch.Tensor, torch.Tensor, int, int]
torch.ao.quantization.HistogramObserver._combine_histograms(self,orig_hist:torch.Tensor,new_hist:torch.Tensor,upsample_rate:int,downsample_rate:int,start_idx:int,Nbins:int)->torch.Tensor
torch.ao.quantization.HistogramObserver._compute_quantization_error(self,next_start_bin:int,next_end_bin:int)
torch.ao.quantization.HistogramObserver._get_norm(self,delta_begin:torch.Tensor,delta_end:torch.Tensor,density:torch.Tensor)->torch.Tensor
torch.ao.quantization.HistogramObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.HistogramObserver._non_linear_param_search(self)->Tuple[torch.Tensor, torch.Tensor]
torch.ao.quantization.HistogramObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.quantization.HistogramObserver.calculate_qparams(self)
torch.ao.quantization.HistogramObserver.forward(self,x_orig:torch.Tensor)->torch.Tensor
torch.ao.quantization.MinMaxObserver(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None,memoryless=False)
torch.ao.quantization.MinMaxObserver.calculate_qparams(self)
torch.ao.quantization.MinMaxObserver.extra_repr(self)
torch.ao.quantization.MinMaxObserver.forward(self,x_orig)
torch.ao.quantization.MinMaxObserver.reset_min_max_vals(self)
torch.ao.quantization.MovingAverageMinMaxObserver(self,averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,**kwargs)
torch.ao.quantization.MovingAverageMinMaxObserver.forward(self,x_orig)
torch.ao.quantization.MovingAveragePerChannelMinMaxObserver(self,averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False,quant_min=None,quant_max=None,**kwargs)
torch.ao.quantization.MovingAveragePerChannelMinMaxObserver.forward(self,x_orig)
torch.ao.quantization.NoopObserver(self,dtype=torch.float16,custom_op_name='')
torch.ao.quantization.NoopObserver.calculate_qparams(self)
torch.ao.quantization.NoopObserver.forward(self,x)
torch.ao.quantization.ObserverBase(self,dtype)
torch.ao.quantization.ObserverBase.calculate_qparams(self,**kwargs)
torch.ao.quantization.ObserverBase.forward(self,x)
torch.ao.quantization.PerChannelMinMaxObserver(self,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None,memoryless=False)
torch.ao.quantization.PerChannelMinMaxObserver._forward(self,x_orig)
torch.ao.quantization.PerChannelMinMaxObserver._load_from_state_dict(self,state_dict:Union[Dict[str,torch.Tensor],Dict[str,torch.Tensor]],prefix:str,local_metadata:Dict[str,torch.Tensor],strict:bool,missing_keys:List[str],unexpected_keys:List[str],error_msgs:List[str])
torch.ao.quantization.PerChannelMinMaxObserver._load_from_state_dict_script(self,state_dict:Union[Dict[str,torch.Tensor],Dict[str,torch.Tensor]],prefix:str,local_metadata:Dict[str,torch.Tensor],strict:bool,missing_keys:List[str],unexpected_keys:List[str],error_msgs:List[str])
torch.ao.quantization.PerChannelMinMaxObserver.calculate_qparams(self)
torch.ao.quantization.PerChannelMinMaxObserver.extra_repr(self)
torch.ao.quantization.PerChannelMinMaxObserver.forward(self,x_orig)
torch.ao.quantization.PerChannelMinMaxObserver.reset_min_max_vals(self)
torch.ao.quantization.PlaceholderObserver(self,dtype=torch.float32,custom_op_name='',compute_dtype=None)
torch.ao.quantization.PlaceholderObserver.calculate_qparams(self)
torch.ao.quantization.PlaceholderObserver.forward(self,x)
torch.ao.quantization.RecordingObserver(self,**kwargs)
torch.ao.quantization.RecordingObserver.calculate_qparams(self)
torch.ao.quantization.RecordingObserver.forward(self,x)
torch.ao.quantization.RecordingObserver.get_tensor_value(self)
torch.ao.quantization.ReuseInputObserver(self)
torch.ao.quantization.ReuseInputObserver.calculate_qparams(self)
torch.ao.quantization.ReuseInputObserver.forward(self,x)
torch.ao.quantization._ObserverBase(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization._ObserverBase._calculate_qparams(self,min_val:torch.Tensor,max_val:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
torch.ao.quantization._ObserverBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization._ObserverBase._validate_qmin_qmax(self,quant_min:int,quant_max:int)->None
torch.ao.quantization._ObserverBase.reset_min_max_vals(self)
torch.ao.quantization._PartialWrapper(self,p)
torch.ao.quantization._PartialWrapper.__repr__(self)
torch.ao.quantization._PartialWrapper.with_args(self,**kwargs)
torch.ao.quantization._PartialWrapper.with_callable_args(self,**kwargs)
torch.ao.quantization._is_activation_post_process(module)
torch.ao.quantization._is_observer_script_module(mod,obs_type_name)
torch.ao.quantization._is_per_channel_script_obs_instance(module)
torch.ao.quantization._with_args(cls_or_self,**kwargs)
torch.ao.quantization._with_callable_args(cls_or_self,**kwargs)
torch.ao.quantization.get_observer_state_dict(mod)
torch.ao.quantization.load_observer_state_dict(mod,obs_dict)
torch.ao.quantization.observer.FixedQParamsObserver(self,scale,zero_point,dtype=torch.quint8,qscheme=torch.per_tensor_affine,quant_min=0,quant_max=255)
torch.ao.quantization.observer.FixedQParamsObserver.__init__(self,scale,zero_point,dtype=torch.quint8,qscheme=torch.per_tensor_affine,quant_min=0,quant_max=255)
torch.ao.quantization.observer.FixedQParamsObserver.calculate_qparams(self)
torch.ao.quantization.observer.FixedQParamsObserver.forward(self,X)
torch.ao.quantization.observer.HistogramObserver(self,bins:int=2048,upsample_rate:int=128,dtype:torch.dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.observer.HistogramObserver.__init__(self,bins:int=2048,upsample_rate:int=128,dtype:torch.dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.observer.HistogramObserver._adjust_min_max(self,combined_min:torch.Tensor,combined_max:torch.Tensor,upsample_rate:int)->Tuple[torch.Tensor, torch.Tensor, int, int]
torch.ao.quantization.observer.HistogramObserver._combine_histograms(self,orig_hist:torch.Tensor,new_hist:torch.Tensor,upsample_rate:int,downsample_rate:int,start_idx:int,Nbins:int)->torch.Tensor
torch.ao.quantization.observer.HistogramObserver._compute_quantization_error(self,next_start_bin:int,next_end_bin:int)
torch.ao.quantization.observer.HistogramObserver._get_norm(self,delta_begin:torch.Tensor,delta_end:torch.Tensor,density:torch.Tensor)->torch.Tensor
torch.ao.quantization.observer.HistogramObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.observer.HistogramObserver._non_linear_param_search(self)->Tuple[torch.Tensor, torch.Tensor]
torch.ao.quantization.observer.HistogramObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.quantization.observer.HistogramObserver.calculate_qparams(self)
torch.ao.quantization.observer.HistogramObserver.forward(self,x_orig:torch.Tensor)->torch.Tensor
torch.ao.quantization.observer.MinMaxObserver(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None,memoryless=False)
torch.ao.quantization.observer.MinMaxObserver.__init__(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None,memoryless=False)
torch.ao.quantization.observer.MinMaxObserver.calculate_qparams(self)
torch.ao.quantization.observer.MinMaxObserver.extra_repr(self)
torch.ao.quantization.observer.MinMaxObserver.forward(self,x_orig)
torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals(self)
torch.ao.quantization.observer.MovingAverageMinMaxObserver(self,averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,**kwargs)
torch.ao.quantization.observer.MovingAverageMinMaxObserver.__init__(self,averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,**kwargs)
torch.ao.quantization.observer.MovingAverageMinMaxObserver.forward(self,x_orig)
torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver(self,averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False,quant_min=None,quant_max=None,**kwargs)
torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.__init__(self,averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False,quant_min=None,quant_max=None,**kwargs)
torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.forward(self,x_orig)
torch.ao.quantization.observer.NoopObserver(self,dtype=torch.float16,custom_op_name='')
torch.ao.quantization.observer.NoopObserver.__init__(self,dtype=torch.float16,custom_op_name='')
torch.ao.quantization.observer.NoopObserver.calculate_qparams(self)
torch.ao.quantization.observer.NoopObserver.forward(self,x)
torch.ao.quantization.observer.ObserverBase(self,dtype)
torch.ao.quantization.observer.ObserverBase.__init__(self,dtype)
torch.ao.quantization.observer.ObserverBase.calculate_qparams(self,**kwargs)
torch.ao.quantization.observer.ObserverBase.forward(self,x)
torch.ao.quantization.observer.PerChannelMinMaxObserver(self,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None,memoryless=False)
torch.ao.quantization.observer.PerChannelMinMaxObserver.__init__(self,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None,memoryless=False)
torch.ao.quantization.observer.PerChannelMinMaxObserver._forward(self,x_orig)
torch.ao.quantization.observer.PerChannelMinMaxObserver._load_from_state_dict(self,state_dict:Union[Dict[str,torch.Tensor],Dict[str,torch.Tensor]],prefix:str,local_metadata:Dict[str,torch.Tensor],strict:bool,missing_keys:List[str],unexpected_keys:List[str],error_msgs:List[str])
torch.ao.quantization.observer.PerChannelMinMaxObserver._load_from_state_dict_script(self,state_dict:Union[Dict[str,torch.Tensor],Dict[str,torch.Tensor]],prefix:str,local_metadata:Dict[str,torch.Tensor],strict:bool,missing_keys:List[str],unexpected_keys:List[str],error_msgs:List[str])
torch.ao.quantization.observer.PerChannelMinMaxObserver.calculate_qparams(self)
torch.ao.quantization.observer.PerChannelMinMaxObserver.extra_repr(self)
torch.ao.quantization.observer.PerChannelMinMaxObserver.forward(self,x_orig)
torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals(self)
torch.ao.quantization.observer.PlaceholderObserver(self,dtype=torch.float32,custom_op_name='',compute_dtype=None)
torch.ao.quantization.observer.PlaceholderObserver.__init__(self,dtype=torch.float32,custom_op_name='',compute_dtype=None)
torch.ao.quantization.observer.PlaceholderObserver.calculate_qparams(self)
torch.ao.quantization.observer.PlaceholderObserver.forward(self,x)
torch.ao.quantization.observer.RecordingObserver(self,**kwargs)
torch.ao.quantization.observer.RecordingObserver.__init__(self,**kwargs)
torch.ao.quantization.observer.RecordingObserver.calculate_qparams(self)
torch.ao.quantization.observer.RecordingObserver.forward(self,x)
torch.ao.quantization.observer.RecordingObserver.get_tensor_value(self)
torch.ao.quantization.observer.ReuseInputObserver(self)
torch.ao.quantization.observer.ReuseInputObserver.__init__(self)
torch.ao.quantization.observer.ReuseInputObserver.calculate_qparams(self)
torch.ao.quantization.observer.ReuseInputObserver.forward(self,x)
torch.ao.quantization.observer._ObserverBase(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.observer._ObserverBase.__init__(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.observer._ObserverBase._calculate_qparams(self,min_val:torch.Tensor,max_val:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
torch.ao.quantization.observer._ObserverBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.observer._ObserverBase._validate_qmin_qmax(self,quant_min:int,quant_max:int)->None
torch.ao.quantization.observer._ObserverBase.reset_min_max_vals(self)
torch.ao.quantization.observer._PartialWrapper(self,p)
torch.ao.quantization.observer._PartialWrapper.__init__(self,p)
torch.ao.quantization.observer._PartialWrapper.__repr__(self)
torch.ao.quantization.observer._PartialWrapper.with_args(self,**kwargs)
torch.ao.quantization.observer._PartialWrapper.with_callable_args(self,**kwargs)
torch.ao.quantization.observer._is_activation_post_process(module)
torch.ao.quantization.observer._is_observer_script_module(mod,obs_type_name)
torch.ao.quantization.observer._is_per_channel_script_obs_instance(module)
torch.ao.quantization.observer._with_args(cls_or_self,**kwargs)
torch.ao.quantization.observer._with_callable_args(cls_or_self,**kwargs)
torch.ao.quantization.observer.get_observer_state_dict(mod)
torch.ao.quantization.observer.load_observer_state_dict(mod,obs_dict)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/quantize_jit.py----------------------------------------
A:torch.ao.quantization.quantize_jit.model_c->torch._C._jit_pass_quant_finalize(model_c, quant_type, preserved_attrs)
A:torch.ao.quantization.quantize_jit.model->convert_jit(model, True, debug)
A:torch.ao.quantization.quantize_jit.scripted_qconfig_dict->script_qconfig_dict(qconfig_dict)
A:torch.ao.quantization.quantize_jit.is_xpu->all((p.device.type == 'xpu' for p in model.parameters()))
torch.ao.quantization._check_forward_method(model)
torch.ao.quantization._check_is_script_module(model)
torch.ao.quantization._convert_jit(model,inplace=False,debug=False,quant_type=QuantType.STATIC,preserved_attrs=None)
torch.ao.quantization._prepare_jit(model,qconfig_dict,inplace=False,quant_type=QuantType.STATIC)
torch.ao.quantization._quantize_jit(model,qconfig_dict,run_fn=None,run_args=None,inplace=False,debug=False,quant_type=QuantType.STATIC)
torch.ao.quantization.convert_dynamic_jit(model,inplace=False,debug=False,preserved_attrs=None)
torch.ao.quantization.convert_jit(model,inplace=False,debug=False,preserved_attrs=None)
torch.ao.quantization.fuse_conv_bn_jit(model,inplace=False)
torch.ao.quantization.prepare_dynamic_jit(model,qconfig_dict,inplace=False)
torch.ao.quantization.prepare_jit(model,qconfig_dict,inplace=False)
torch.ao.quantization.quantize_dynamic_jit(model,qconfig_dict,inplace=False,debug=False)
torch.ao.quantization.quantize_jit(model,qconfig_dict,run_fn,run_args,inplace=False,debug=False)
torch.ao.quantization.quantize_jit._check_forward_method(model)
torch.ao.quantization.quantize_jit._check_is_script_module(model)
torch.ao.quantization.quantize_jit._convert_jit(model,inplace=False,debug=False,quant_type=QuantType.STATIC,preserved_attrs=None)
torch.ao.quantization.quantize_jit._prepare_jit(model,qconfig_dict,inplace=False,quant_type=QuantType.STATIC)
torch.ao.quantization.quantize_jit._quantize_jit(model,qconfig_dict,run_fn=None,run_args=None,inplace=False,debug=False,quant_type=QuantType.STATIC)
torch.ao.quantization.quantize_jit.convert_dynamic_jit(model,inplace=False,debug=False,preserved_attrs=None)
torch.ao.quantization.quantize_jit.convert_jit(model,inplace=False,debug=False,preserved_attrs=None)
torch.ao.quantization.quantize_jit.fuse_conv_bn_jit(model,inplace=False)
torch.ao.quantization.quantize_jit.prepare_dynamic_jit(model,qconfig_dict,inplace=False)
torch.ao.quantization.quantize_jit.prepare_jit(model,qconfig_dict,inplace=False)
torch.ao.quantization.quantize_jit.quantize_dynamic_jit(model,qconfig_dict,inplace=False,debug=False)
torch.ao.quantization.quantize_jit.quantize_jit(model,qconfig_dict,run_fn,run_args,inplace=False,debug=False)
torch.ao.quantization.quantize_jit.script_qconfig(qconfig)
torch.ao.quantization.quantize_jit.script_qconfig_dict(qconfig_dict)
torch.ao.quantization.script_qconfig(qconfig)
torch.ao.quantization.script_qconfig_dict(qconfig_dict)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/qconfig.py----------------------------------------
A:torch.ao.quantization.qconfig.default_qconfig->QConfig(activation=default_observer, weight=default_weight_observer)
A:torch.ao.quantization.qconfig.default_debug_qconfig->QConfig(weight=default_weight_observer, activation=default_debug_observer)
A:torch.ao.quantization.qconfig.default_per_channel_qconfig->QConfig(activation=default_observer, weight=default_per_channel_weight_observer)
A:torch.ao.quantization.qconfig.default_dynamic_qconfig->QConfig(activation=default_dynamic_quant_observer, weight=default_weight_observer)
A:torch.ao.quantization.qconfig.float16_dynamic_qconfig->QConfig(activation=PlaceholderObserver.with_args(dtype=torch.float32), weight=PlaceholderObserver.with_args(dtype=torch.float16))
A:torch.ao.quantization.qconfig.float16_static_qconfig->QConfig(activation=PlaceholderObserver.with_args(dtype=torch.float16), weight=PlaceholderObserver.with_args(dtype=torch.float16))
A:torch.ao.quantization.qconfig.per_channel_dynamic_qconfig->QConfig(activation=default_dynamic_quant_observer, weight=default_per_channel_weight_observer)
A:torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig->QConfig(activation=default_placeholder_observer, weight=default_float_qparams_observer)
A:torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig_4bit->QConfig(activation=default_placeholder_observer, weight=default_float_qparams_observer_4bit)
A:torch.ao.quantization.qconfig.default_qat_qconfig->QConfig(activation=default_fake_quant, weight=default_weight_fake_quant)
A:torch.ao.quantization.qconfig.default_dynamic_qat_qconfig->QConfig(activation=default_dynamic_fake_quant, weight=default_weight_fake_quant)
A:torch.ao.quantization.qconfig.default_weight_only_qconfig->QConfig(activation=torch.nn.Identity, weight=default_weight_fake_quant)
A:torch.ao.quantization.qconfig.default_activation_only_qconfig->QConfig(activation=default_fake_quant, weight=torch.nn.Identity)
A:torch.ao.quantization.qconfig.default_qat_qconfig_v2->QConfig(activation=default_fused_act_fake_quant, weight=default_fused_wt_fake_quant)
A:torch.ao.quantization.qconfig.default_reuse_input_qconfig->QConfig(activation=default_reuse_input_observer, weight=NoopObserver)
A:torch.ao.quantization.qconfig.qconfig->get_default_qat_qconfig(backend, version=version)
A:torch.ao.quantization.qconfig.default_embedding_qat_qconfig->QConfig(activation=NoopObserver.with_args(dtype=torch.float32), weight=default_embedding_fake_quant)
A:torch.ao.quantization.qconfig.default_embedding_qat_qconfig_4bit->QConfig(activation=NoopObserver.with_args(dtype=torch.float32), weight=default_embedding_fake_quant_4bit)
A:torch.ao.quantization.qconfig.example_observer->get_default_qat_qconfig(backend, version=version).weight()
A:torch.ao.quantization.qconfig.check->original_constructor.with_args(factory_kwargs=None)
A:torch.ao.quantization.qconfig.activation->configure_constructor_to_put_obs_on_module_device(qconfig.activation)
A:torch.ao.quantization.qconfig.weight->configure_constructor_to_put_obs_on_module_device(qconfig.weight)
A:torch.ao.quantization.qconfig.activation_same->partial_equals(q1.activation.p, q2.activation.p)
A:torch.ao.quantization.qconfig.weight_same->partial_equals(q1.weight.p, q2.weight.p)
A:torch.ao.quantization.qconfig.act->get_default_qat_qconfig(backend, version=version).activation()
torch.ao.quantization.QConfig(cls,activation,weight)
torch.ao.quantization.QConfigDynamic(cls,activation=torch.nn.Identity,weight=torch.nn.Identity)
torch.ao.quantization.activation_is_memoryless(qconfig:QConfig)
torch.ao.quantization.add_module_to_qconfig_obs_ctr(qconfig:QConfigAny,module:Optional[nn.Module])->Any
torch.ao.quantization.assert_valid_qconfig(qconfig:Optional[QConfig],mod:torch.nn.Module)->None
torch.ao.quantization.get_default_qat_qconfig(backend='fbgemm',version=1)
torch.ao.quantization.get_default_qat_qconfig_dict(backend='fbgemm',version=1)
torch.ao.quantization.get_default_qconfig(backend='fbgemm')
torch.ao.quantization.get_default_qconfig_dict(backend='fbgemm',version=0)
torch.ao.quantization.is_reuse_input_qconfig(qconfig:Optional[QConfig])
torch.ao.quantization.qconfig.QConfig(cls,activation,weight)
torch.ao.quantization.qconfig.QConfig.__new__(cls,activation,weight)
torch.ao.quantization.qconfig.QConfigDynamic(cls,activation=torch.nn.Identity,weight=torch.nn.Identity)
torch.ao.quantization.qconfig.QConfigDynamic.__new__(cls,activation=torch.nn.Identity,weight=torch.nn.Identity)
torch.ao.quantization.qconfig.activation_is_memoryless(qconfig:QConfig)
torch.ao.quantization.qconfig.add_module_to_qconfig_obs_ctr(qconfig:QConfigAny,module:Optional[nn.Module])->Any
torch.ao.quantization.qconfig.assert_valid_qconfig(qconfig:Optional[QConfig],mod:torch.nn.Module)->None
torch.ao.quantization.qconfig.get_default_qat_qconfig(backend='fbgemm',version=1)
torch.ao.quantization.qconfig.get_default_qat_qconfig_dict(backend='fbgemm',version=1)
torch.ao.quantization.qconfig.get_default_qconfig(backend='fbgemm')
torch.ao.quantization.qconfig.get_default_qconfig_dict(backend='fbgemm',version=0)
torch.ao.quantization.qconfig.is_reuse_input_qconfig(qconfig:Optional[QConfig])
torch.ao.quantization.qconfig.qconfig_equals(q1:QConfigAny,q2:QConfigAny)
torch.ao.quantization.qconfig_equals(q1:QConfigAny,q2:QConfigAny)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_learnable_fake_quantize.py----------------------------------------
A:torch.ao.quantization._learnable_fake_quantize.self.scale->Parameter(torch.tensor([scale] * channel_len))
A:torch.ao.quantization._learnable_fake_quantize.self.zero_point->Parameter(torch.tensor([zero_point] * channel_len))
A:torch.ao.quantization._learnable_fake_quantize.self.activation_post_process->observer(**observer_kwargs)
A:torch.ao.quantization._learnable_fake_quantize.bitrange->torch.tensor(quant_max - quant_min + 1).double()
A:torch.ao.quantization._learnable_fake_quantize.self.bitwidth->int(torch.log2(bitrange).item())
A:torch.ao.quantization._learnable_fake_quantize.self.static_enabled[0]->int(enabled)
A:torch.ao.quantization._learnable_fake_quantize.self.learning_enabled[0]->int(enabled)
A:torch.ao.quantization._learnable_fake_quantize.self.fake_quant_enabled[0]->int(enabled)
A:torch.ao.quantization._learnable_fake_quantize.scale->self.scale.detach()
A:torch.ao.quantization._learnable_fake_quantize.zero_point->self.zero_point.detach().round().clamp(self.quant_min, self.quant_max).long()
A:torch.ao.quantization._learnable_fake_quantize.(_scale, _zero_point)->self.activation_post_process.calculate_qparams()
A:torch.ao.quantization._learnable_fake_quantize._scale->_scale.to(self.scale.device).to(self.scale.device)
A:torch.ao.quantization._learnable_fake_quantize._zero_point->_zero_point.to(self.zero_point.device).to(self.zero_point.device)
A:torch.ao.quantization._learnable_fake_quantize.X->torch._fake_quantize_learnable_per_tensor_affine(X, self.scale, self.zero_point, self.quant_min, self.quant_max, grad_factor)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize(self,observer,quant_min=0,quant_max=255,scale=1.0,zero_point=0.0,channel_len=-1,use_grad_scaling=False,**observer_kwargs)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.calculate_qparams(self)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.enable_observer(self,enabled=True)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.enable_param_learning(self)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.enable_static_estimate(self)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.enable_static_observation(self)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.forward(self,X)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.observe_quant_params(self)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.toggle_fake_quant(self,enabled=True)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.toggle_observer_update(self,enabled=True)
torch.ao.quantization._learnable_fake__LearnableFakeQuantize.toggle_qparam_learning(self,enabled=True)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize(self,observer,quant_min=0,quant_max=255,scale=1.0,zero_point=0.0,channel_len=-1,use_grad_scaling=False,**observer_kwargs)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.__init__(self,observer,quant_min=0,quant_max=255,scale=1.0,zero_point=0.0,channel_len=-1,use_grad_scaling=False,**observer_kwargs)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.calculate_qparams(self)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.enable_observer(self,enabled=True)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.enable_param_learning(self)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.enable_static_estimate(self)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.enable_static_observation(self)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.forward(self,X)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.observe_quant_params(self)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.toggle_fake_quant(self,enabled=True)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.toggle_observer_update(self,enabled=True)
torch.ao.quantization._learnable_fake_quantize._LearnableFakeQuantize.toggle_qparam_learning(self,enabled=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_quantize_fx_do_not_use.py----------------------------------------
A:torch.ao.quantization._quantize_fx_do_not_use.quantized->_convert_do_not_use(graph_module, is_reference, convert_custom_config_dict, False, _remove_qconfig_flag=_remove_qconfig, backend_config_dict=backend_config_dict)
A:torch.ao.quantization._quantize_fx_do_not_use.preserved_attributes->convert_custom_config_dict.get('preserved_attributes', [])
torch.ao.quantization._quantize_fx_do_not_use._convert_fx_do_not_use(graph_module:GraphModule,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None,_remove_qconfig:bool=True,backend_config_dict:Optional[Dict[str,Any]]=None)->torch.nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_correct_bias.py----------------------------------------
A:torch.ao.quantization._correct_bias.split_name->name.rsplit('.', 1)
A:torch.ao.quantization._correct_bias.param->getattr(module, attr, None)
A:torch.ao.quantization._correct_bias.x->x.dequantize().dequantize()
A:torch.ao.quantization._correct_bias.quantized_submodule->get_module(quantized_model, uncorrected_module)
A:torch.ao.quantization._correct_bias.bias->get_param(quantized_submodule, 'bias')
A:torch.ao.quantization._correct_bias.ob_dict->torch.ao.ns._numeric_suite.get_logger_dict(quantized_model)
A:torch.ao.quantization._correct_bias.(parent_name, _)->parent_child_names(uncorrected_module)
A:torch.ao.quantization._correct_bias.dims->list(range(quantization_error.dim()))
A:torch.ao.quantization._correct_bias.expected_error->torch.mean(quantization_error, dims)
torch.ao.quantization._correct_bias.MeanShadowLogger(self)
torch.ao.quantization._correct_bias.MeanShadowLogger.__init__(self)
torch.ao.quantization._correct_bias.MeanShadowLogger.clear(self)
torch.ao.quantization._correct_bias.MeanShadowLogger.forward(self,x,y)
torch.ao.quantization._correct_bias.bias_correction(float_model,quantized_model,img_data,target_modules=_supported_modules_quantized,neval_batches=None)
torch.ao.quantization._correct_bias.get_module(model,name)
torch.ao.quantization._correct_bias.get_param(module,attr)
torch.ao.quantization._correct_bias.parent_child_names(name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fake_quantize.py----------------------------------------
A:torch.ao.quantization.fake_quantize.with_args->classmethod(_with_args)
A:torch.ao.quantization.fake_quantize.self.activation_post_process->observer(**observer_kwargs)
A:torch.ao.quantization.fake_quantize.self.is_per_channel->_is_per_channel(self.qscheme)
A:torch.ao.quantization.fake_quantize.(_scale, _zero_point)->self.calculate_qparams()
A:torch.ao.quantization.fake_quantize.X->torch.fake_quantize_per_tensor_affine(X, self.scale, self.zero_point, self.quant_min, self.quant_max)
A:torch.ao.quantization.fake_quantize.self.is_symmetric_quant->_is_symmetric_quant(self.activation_post_process.qscheme)
A:torch.ao.quantization.fake_quantize.default_fake_quant->FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
A:torch.ao.quantization.fake_quantize.default_weight_fake_quant->FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False)
A:torch.ao.quantization.fake_quantize.default_dynamic_fake_quant->FakeQuantize.with_args(observer=MinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, memoryless=True)
A:torch.ao.quantization.fake_quantize.default_symmetric_fixed_qparams_fake_quant->FixedQParamsFakeQuantize.with_args(observer=default_symmetric_fixed_qparams_observer)
A:torch.ao.quantization.fake_quantize.default_affine_fixed_qparams_fake_quant->FixedQParamsFakeQuantize.with_args(observer=default_affine_fixed_qparams_observer)
A:torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant->FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0)
A:torch.ao.quantization.fake_quantize.default_embedding_fake_quant->FakeQuantize.with_args(observer=PerChannelMinMaxObserver, qscheme=torch.per_channel_affine_float_qparams, dtype=torch.quint8, quant_min=0, quant_max=255, ch_axis=0, memoryless=True)
A:torch.ao.quantization.fake_quantize.default_embedding_fake_quant_4bit->FakeQuantize.with_args(observer=PerChannelMinMaxObserver, qscheme=torch.per_channel_affine_float_qparams, ch_axis=0, dtype=torch.quint4x2, memoryless=True)
A:torch.ao.quantization.fake_quantize.default_histogram_fake_quant->FakeQuantize.with_args(observer=HistogramObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
A:torch.ao.quantization.fake_quantize.default_fused_act_fake_quant->FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8)
A:torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant->FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)
A:torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant->FusedMovingAvgObsFakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
A:torch.ao.quantization.fake_quantize.name->re.sub('\\.___torch_mangle_\\d+', '', suffix)
torch.ao.quantization.fake_FakeQuantize(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.ao.quantization.fake_FakeQuantize._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.fake_FakeQuantize._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.quantization.fake_FakeQuantize.calculate_qparams(self)
torch.ao.quantization.fake_FakeQuantize.extra_repr(self)
torch.ao.quantization.fake_FakeQuantize.forward(self,X)
torch.ao.quantization.fake_FakeQuantizeBase(self)
torch.ao.quantization.fake_FakeQuantizeBase.calculate_qparams(self,**kwargs)
torch.ao.quantization.fake_FakeQuantizeBase.disable_fake_quant(self)
torch.ao.quantization.fake_FakeQuantizeBase.disable_observer(self)
torch.ao.quantization.fake_FakeQuantizeBase.enable_fake_quant(self,enabled:bool=True)->None
torch.ao.quantization.fake_FakeQuantizeBase.enable_observer(self,enabled:bool=True)->None
torch.ao.quantization.fake_FakeQuantizeBase.forward(self,x)
torch.ao.quantization.fake_FixedQParamsFakeQuantize(self,observer)
torch.ao.quantization.fake_FixedQParamsFakeQuantize.calculate_qparams(self)
torch.ao.quantization.fake_FixedQParamsFakeQuantize.extra_repr(self)
torch.ao.quantization.fake_FusedMovingAvgObsFakeQuantize(self,observer:Any=MovingAverageMinMaxObserver,quant_min:int=0,quant_max:int=255,**observer_kwargs:Any)
torch.ao.quantization.fake_FusedMovingAvgObsFakeQuantize.calculate_qparams(self)->Tuple[torch.Tensor, torch.Tensor]
torch.ao.quantization.fake_FusedMovingAvgObsFakeQuantize.extra_repr(self)->str
torch.ao.quantization.fake_FusedMovingAvgObsFakeQuantize.forward(self,X:torch.Tensor)->torch.Tensor
torch.ao.quantization.fake__is_fake_quant_script_module(mod)
torch.ao.quantization.fake__is_float_qparams(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake__is_per_channel(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake__is_per_tensor(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake__is_symmetric_quant(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake_disable_fake_quant(mod)
torch.ao.quantization.fake_disable_observer(mod)
torch.ao.quantization.fake_enable_fake_quant(mod)
torch.ao.quantization.fake_enable_observer(mod)
torch.ao.quantization.fake_quantize.FakeQuantize(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.ao.quantization.fake_quantize.FakeQuantize.__init__(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.ao.quantization.fake_quantize.FakeQuantize._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.fake_quantize.FakeQuantize._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.quantization.fake_quantize.FakeQuantize.calculate_qparams(self)
torch.ao.quantization.fake_quantize.FakeQuantize.extra_repr(self)
torch.ao.quantization.fake_quantize.FakeQuantize.forward(self,X)
torch.ao.quantization.fake_quantize.FakeQuantizeBase(self)
torch.ao.quantization.fake_quantize.FakeQuantizeBase.__init__(self)
torch.ao.quantization.fake_quantize.FakeQuantizeBase.calculate_qparams(self,**kwargs)
torch.ao.quantization.fake_quantize.FakeQuantizeBase.disable_fake_quant(self)
torch.ao.quantization.fake_quantize.FakeQuantizeBase.disable_observer(self)
torch.ao.quantization.fake_quantize.FakeQuantizeBase.enable_fake_quant(self,enabled:bool=True)->None
torch.ao.quantization.fake_quantize.FakeQuantizeBase.enable_observer(self,enabled:bool=True)->None
torch.ao.quantization.fake_quantize.FakeQuantizeBase.forward(self,x)
torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize(self,observer)
torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.__init__(self,observer)
torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.calculate_qparams(self)
torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.extra_repr(self)
torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize(self,observer:Any=MovingAverageMinMaxObserver,quant_min:int=0,quant_max:int=255,**observer_kwargs:Any)
torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.__init__(self,observer:Any=MovingAverageMinMaxObserver,quant_min:int=0,quant_max:int=255,**observer_kwargs:Any)
torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.calculate_qparams(self)->Tuple[torch.Tensor, torch.Tensor]
torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.extra_repr(self)->str
torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.forward(self,X:torch.Tensor)->torch.Tensor
torch.ao.quantization.fake_quantize._is_fake_quant_script_module(mod)
torch.ao.quantization.fake_quantize._is_float_qparams(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake_quantize._is_per_channel(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake_quantize._is_per_tensor(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake_quantize._is_symmetric_quant(qscheme:'torch.qscheme')->bool
torch.ao.quantization.fake_quantize.disable_fake_quant(mod)
torch.ao.quantization.fake_quantize.disable_observer(mod)
torch.ao.quantization.fake_quantize.enable_fake_quant(mod)
torch.ao.quantization.fake_quantize.enable_observer(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/qconfig_dict_utils.py----------------------------------------
A:torch.ao.quantization.qconfig_dict_utils.(parent, _)->_parent_name(module_name)
A:torch.ao.quantization.qconfig_dict_utils.module_type_qconfig->get_object_type_qconfig(qconfig_dict, module_type, global_qconfig)
A:torch.ao.quantization.qconfig_dict_utils.module_name_regex_qconfig->get_module_name_regex_qconfig(qconfig_dict, module_name, module_type_qconfig)
A:torch.ao.quantization.qconfig_dict_utils.module_name_qconfig->get_module_name_qconfig(qconfig_dict, module_name, module_name_regex_qconfig)
A:torch.ao.quantization.qconfig_dict_utils.flattened->dict()
A:torch.ao.quantization.qconfig_dict_utils.qconfig_dict[key]->OrderedDict(qconfig_dict.get(key, []))
A:torch.ao.quantization.qconfig_dict_utils.all_qat_mappings->get_combined_dict(get_default_qat_module_mappings(), additional_qat_module_mapping)
A:torch.ao.quantization.qconfig_dict_utils.object_type_dict->qconfig_dict.get('object_type', None)
A:torch.ao.quantization.qconfig_dict_utils.new_object_type_dict->qconfig_dict.get('object_type', None).copy()
torch.ao.quantization.qconfig_dict_utils.convert_dict_to_ordered_dict(qconfig_dict:Any)->Dict[str, Dict[Any, Any]]
torch.ao.quantization.qconfig_dict_utils.get_flattened_qconfig_dict(qconfig_dict)
torch.ao.quantization.qconfig_dict_utils.get_module_name_qconfig(qconfig_dict,module_name,fallback_qconfig)
torch.ao.quantization.qconfig_dict_utils.get_module_name_regex_qconfig(qconfig_dict,module_name,fallback_qconfig)
torch.ao.quantization.qconfig_dict_utils.get_object_type_qconfig(qconfig_dict:Any,object_type:Union[Callable,str],fallback_qconfig:QConfigAny)->QConfigAny
torch.ao.quantization.qconfig_dict_utils.maybe_adjust_qconfig_for_module_type_or_name(qconfig_dict,module_type,module_name,global_qconfig)
torch.ao.quantization.qconfig_dict_utils.update_qconfig_for_qat(qconfig_dict:Any,additional_qat_module_mapping:Dict[Callable,Callable])->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fuser_method_mappings.py----------------------------------------
A:torch.ao.quantization.fuser_method_mappings.fused_module_class->fused_module_class_map.get(type(conv), None)
A:torch.ao.quantization.fuser_method_mappings.fused_module->map_to_fused_module_eval.get(type(conv), None)
A:torch.ao.quantization.fuser_method_mappings.fused_conv->torch.nn.utils.fusion.fuse_conv_bn_eval(conv, bn)
A:torch.ao.quantization.fuser_method_mappings.additional_fuser_method_mapping->dict()
A:torch.ao.quantization.fuser_method_mappings.all_mappings->get_combined_dict(DEFAULT_OP_LIST_TO_FUSER_METHOD, additional_fuser_method_mapping)
A:torch.ao.quantization.fuser_method_mappings.fuser_method->fuser_method_mapping.get(op_pattern, None)
torch.ao.quantization.fuse_conv_bn(is_qat,conv,bn)
torch.ao.quantization.fuse_conv_bn_relu(is_qat,conv,bn,relu)
torch.ao.quantization.fuse_convtranspose_bn(is_qat,convt,bn)
torch.ao.quantization.fuse_linear_bn(is_qat,linear,bn)
torch.ao.quantization.fuser_method_mappings.fuse_conv_bn(is_qat,conv,bn)
torch.ao.quantization.fuser_method_mappings.fuse_conv_bn_relu(is_qat,conv,bn,relu)
torch.ao.quantization.fuser_method_mappings.fuse_convtranspose_bn(is_qat,convt,bn)
torch.ao.quantization.fuser_method_mappings.fuse_linear_bn(is_qat,linear,bn)
torch.ao.quantization.fuser_method_mappings.get_fuser_method(op_list,additional_fuser_method_mapping=None)
torch.ao.quantization.fuser_method_mappings.get_fuser_method_new(op_pattern:Pattern,fuser_method_mapping:Optional[Dict[Pattern,Union[nn.Sequential,Callable]]]=None)
torch.ao.quantization.fuser_method_mappings.reverse2(f)
torch.ao.quantization.fuser_method_mappings.reverse3(f)
torch.ao.quantization.fuser_method_mappings.reverse_sequential_wrapper2(sequential)
torch.ao.quantization.fuser_method_mappings.sequential_wrapper2(sequential)
torch.ao.quantization.get_fuser_method(op_list,additional_fuser_method_mapping=None)
torch.ao.quantization.get_fuser_method_new(op_pattern:Pattern,fuser_method_mapping:Optional[Dict[Pattern,Union[nn.Sequential,Callable]]]=None)
torch.ao.quantization.reverse2(f)
torch.ao.quantization.reverse3(f)
torch.ao.quantization.reverse_sequential_wrapper2(sequential)
torch.ao.quantization.sequential_wrapper2(sequential)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/quant_type.py----------------------------------------
torch.ao.quantization.QuantType(enum.IntEnum)
torch.ao.quantization.quant_type.QuantType(enum.IntEnum)
torch.ao.quantization.quant_type.quant_type_to_str(quant_type)
torch.ao.quantization.quant_type_to_str(quant_type)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/__init__.py----------------------------------------
torch.ao.quantization.__init__.default_eval_fn(model,calib_data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/stubs.py----------------------------------------
A:torch.ao.quantization.stubs.X->self.module(X)
torch.ao.quantization.DeQuantStub(self)
torch.ao.quantization.DeQuantStub.forward(self,x)
torch.ao.quantization.QuantStub(self,qconfig=None)
torch.ao.quantization.QuantStub.forward(self,x)
torch.ao.quantization.QuantWrapper(self,module)
torch.ao.quantization.QuantWrapper.forward(self,X)
torch.ao.quantization.stubs.DeQuantStub(self)
torch.ao.quantization.stubs.DeQuantStub.__init__(self)
torch.ao.quantization.stubs.DeQuantStub.forward(self,x)
torch.ao.quantization.stubs.QuantStub(self,qconfig=None)
torch.ao.quantization.stubs.QuantStub.__init__(self,qconfig=None)
torch.ao.quantization.stubs.QuantStub.forward(self,x)
torch.ao.quantization.stubs.QuantWrapper(self,module)
torch.ao.quantization.stubs.QuantWrapper.__init__(self,module)
torch.ao.quantization.stubs.QuantWrapper.forward(self,X)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/quantize.py----------------------------------------
A:torch.ao.quantization.quantize.module_qconfig->getattr(module, 'qconfig', module_qconfig)
A:torch.ao.quantization.quantize.qconfig_with_device_check->add_module_to_qconfig_obs_ctr(module_qconfig, module)
A:torch.ao.quantization.quantize.handle->copy.deepcopy(module).register_forward_hook(_observer_forward_hook)
A:torch.ao.quantization.quantize.qconfig_propagation_list->get_default_qconfig_propagation_list()
A:torch.ao.quantization.quantize.devices->get_unique_devices_(mod)
A:torch.ao.quantization.quantize.child.activation_post_process->get_activation_post_process(child.qconfig, device)
A:torch.ao.quantization.quantize.special_act_post_process->_get_special_act_post_process(child)
A:torch.ao.quantization.quantize.observed_child->custom_module_class_mapping[type(child)].from_float(child)
A:torch.ao.quantization.quantize.module._modules[name]->add_quant_dequant(child)
A:torch.ao.quantization.quantize.custom_module_class_mapping->convert_custom_config_dict.get('observed_to_quantized_custom_module_class', {})
A:torch.ao.quantization.quantize.model->copy.deepcopy(model)
A:torch.ao.quantization.quantize.handle_ids_to_remove->set()
A:torch.ao.quantization.quantize.mapping->get_default_static_quant_module_mappings()
A:torch.ao.quantization.quantize.qconfig_spec->dict(zip(qconfig_spec, itertools.repeat(default_qconfig)))
A:torch.ao.quantization.quantize.module->copy.deepcopy(module)
A:torch.ao.quantization.quantize.reassign[name]->swap_module(mod, mapping, custom_module_class_mapping)
A:torch.ao.quantization.quantize.new_mod->mapping[type(mod)].from_float(mod)
torch.ao.quantization._convert(module,mapping=None,inplace=False,convert_custom_config_dict=None)
torch.ao.quantization._observer_forward_hook(self,input,output)
torch.ao.quantization._observer_forward_pre_hook(self,input)
torch.ao.quantization._propagate_qconfig_helper(module,qconfig_dict,qconfig_parent=None,prefix='')
torch.ao.quantization._remove_activation_post_process(module)
torch.ao.quantization._remove_qconfig(module)
torch.ao.quantization.add_observer_(module,qconfig_propagation_list=None,non_leaf_module_list=None,device=None,custom_module_class_mapping=None)
torch.ao.quantization.add_quant_dequant(module)
torch.ao.quantization.convert(module,mapping=None,inplace=False,remove_qconfig=True,convert_custom_config_dict=None)
torch.ao.quantization.get_observer_dict(mod,target_dict,prefix='')
torch.ao.quantization.get_unique_devices_(module)
torch.ao.quantization.is_activation_post_process(module)
torch.ao.quantization.prepare(model,inplace=False,allow_list=None,observer_non_leaf_module_list=None,prepare_custom_config_dict=None)
torch.ao.quantization.prepare_qat(model,mapping=None,inplace=False)
torch.ao.quantization.propagate_qconfig_(module,qconfig_dict=None)
torch.ao.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)
torch.ao.quantization.quantize._convert(module,mapping=None,inplace=False,convert_custom_config_dict=None)
torch.ao.quantization.quantize._observer_forward_hook(self,input,output)
torch.ao.quantization.quantize._observer_forward_pre_hook(self,input)
torch.ao.quantization.quantize._propagate_qconfig_helper(module,qconfig_dict,qconfig_parent=None,prefix='')
torch.ao.quantization.quantize._remove_activation_post_process(module)
torch.ao.quantization.quantize._remove_qconfig(module)
torch.ao.quantization.quantize.add_observer_(module,qconfig_propagation_list=None,non_leaf_module_list=None,device=None,custom_module_class_mapping=None)
torch.ao.quantization.quantize.add_quant_dequant(module)
torch.ao.quantization.quantize.convert(module,mapping=None,inplace=False,remove_qconfig=True,convert_custom_config_dict=None)
torch.ao.quantization.quantize.get_observer_dict(mod,target_dict,prefix='')
torch.ao.quantization.quantize.get_unique_devices_(module)
torch.ao.quantization.quantize.is_activation_post_process(module)
torch.ao.quantization.quantize.prepare(model,inplace=False,allow_list=None,observer_non_leaf_module_list=None,prepare_custom_config_dict=None)
torch.ao.quantization.quantize.prepare_qat(model,mapping=None,inplace=False)
torch.ao.quantization.quantize.propagate_qconfig_(module,qconfig_dict=None)
torch.ao.quantization.quantize.quantize(model,run_fn,run_args,mapping=None,inplace=False)
torch.ao.quantization.quantize.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)
torch.ao.quantization.quantize.quantize_qat(model,run_fn,run_args,inplace=False)
torch.ao.quantization.quantize.register_activation_post_process_hook(module,pre_hook=False)
torch.ao.quantization.quantize.swap_module(mod,mapping,custom_module_class_mapping)
torch.ao.quantization.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)
torch.ao.quantization.quantize_qat(model,run_fn,run_args,inplace=False)
torch.ao.quantization.register_activation_post_process_hook(module,pre_hook=False)
torch.ao.quantization.swap_module(mod,mapping,custom_module_class_mapping)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/quantize_fx.py----------------------------------------
A:torch.ao.quantization.quantize_fx.model._modules[name]->torch.nn.quantized.FXFloatFunctional()
A:torch.ao.quantization.quantize_fx.fuser->Fuser()
A:torch.ao.quantization.quantize_fx.self.scope.module_type->type(current_module)
A:torch.ao.quantization.quantize_fx.self.scope->Scope('', None)
A:torch.ao.quantization.quantize_fx.module_qualified_name->self.path_of_module(m)
A:torch.ao.quantization.quantize_fx.node->super().create_node(kind, target, args, kwargs, name, type_expr)
A:torch.ao.quantization.quantize_fx.skipped_module_names->prepare_custom_config_dict.get('non_traceable_module_name', [])
A:torch.ao.quantization.quantize_fx.skipped_module_classes->prepare_custom_config_dict.get('non_traceable_module_class', [])
A:torch.ao.quantization.quantize_fx.standalone_module_name_configs->prepare_custom_config_dict.get('standalone_module_name', [])
A:torch.ao.quantization.quantize_fx.standalone_module_class_configs->prepare_custom_config_dict.get('standalone_module_class', [])
A:torch.ao.quantization.quantize_fx.float_custom_module_classes->get_custom_module_class_keys(prepare_custom_config_dict, 'float_to_observed_custom_module_class')
A:torch.ao.quantization.quantize_fx.preserved_attributes->convert_custom_config_dict.get('preserved_attributes', [])
A:torch.ao.quantization.quantize_fx.tracer->QuantizationTracer(skipped_module_names, skipped_module_classes)
A:torch.ao.quantization.quantize_fx.graph_module->torch.fx.symbolic_trace(model)
A:torch.ao.quantization.quantize_fx.prepared->prepare(graph_module, qconfig_dict, is_qat, tracer.node_name_to_scope, prepare_custom_config_dict=prepare_custom_config_dict, equalization_qconfig_dict=equalization_qconfig_dict, backend_config_dict=backend_config_dict, is_standalone_module=is_standalone_module)
A:torch.ao.quantization.quantize_fx.quantized->convert(graph_module, is_reference, convert_custom_config_dict, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, convert_qconfig_dict=qconfig_dict)
torch.ao.quantization.quantize_fx.QuantizationTracer(self,skipped_module_names:List[str],skipped_module_classes:List[Callable])
torch.ao.quantization.quantize_fx.QuantizationTracer.__init__(self,skipped_module_names:List[str],skipped_module_classes:List[Callable])
torch.ao.quantization.quantize_fx.QuantizationTracer.call_module(self,m:torch.nn.Module,forward:Callable[...,Any],args:Tuple[Any,...],kwargs:Dict[str,Any])->Any
torch.ao.quantization.quantize_fx.QuantizationTracer.create_node(self,kind:str,target:Target,args:Tuple[Argument,...],kwargs:Dict[str,Argument],name:Optional[str]=None,type_expr:Optional[Any]=None)->Node
torch.ao.quantization.quantize_fx.QuantizationTracer.is_leaf_module(self,m:torch.nn.Module,module_qualified_name:str)->bool
torch.ao.quantization.quantize_fx.Scope(self,module_path:str,module_type:Any)
torch.ao.quantization.quantize_fx.Scope.__init__(self,module_path:str,module_type:Any)
torch.ao.quantization.quantize_fx.ScopeContextManager(self,scope:Scope,current_module:torch.nn.Module,current_module_path:str)
torch.ao.quantization.quantize_fx.ScopeContextManager.__enter__(self)
torch.ao.quantization.quantize_fx.ScopeContextManager.__exit__(self,*args)
torch.ao.quantization.quantize_fx.ScopeContextManager.__init__(self,scope:Scope,current_module:torch.nn.Module,current_module_path:str)
torch.ao.quantization.quantize_fx._check_is_graph_module(model:torch.nn.Module)->None
torch.ao.quantization.quantize_fx._convert_fx(graph_module:GraphModule,is_reference:bool,convert_custom_config_dict:Optional[Dict[str,Any]]=None,is_standalone_module:bool=False,_remove_qconfig:bool=True,qconfig_dict:Dict[str,Any]=None)->torch.nn.Module
torch.ao.quantization.quantize_fx._convert_standalone_module_fx(graph_module:GraphModule,is_reference:bool=False,convert_custom_config_dict:Optional[Dict[str,Any]]=None)->torch.nn.Module
torch.ao.quantization.quantize_fx._fuse_fx(graph_module:GraphModule,is_qat:bool,fuse_custom_config_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None)->GraphModule
torch.ao.quantization.quantize_fx._prepare_fx(model:torch.nn.Module,qconfig_dict:Any,is_qat:bool,prepare_custom_config_dict:Optional[Dict[str,Any]]=None,equalization_qconfig_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None,is_standalone_module:bool=False)->ObservedGraphModule
torch.ao.quantization.quantize_fx._prepare_standalone_module_fx(model:torch.nn.Module,qconfig_dict:Any,is_qat:bool,prepare_custom_config_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None)->GraphModule
torch.ao.quantization.quantize_fx._swap_ff_with_fxff(model:torch.nn.Module)->None
torch.ao.quantization.quantize_fx.convert_fx(graph_module:GraphModule,is_reference:bool=False,convert_custom_config_dict:Optional[Dict[str,Any]]=None,_remove_qconfig:bool=True,qconfig_dict:Dict[str,Any]=None)->torch.nn.Module
torch.ao.quantization.quantize_fx.fuse_fx(model:torch.nn.Module,fuse_custom_config_dict:Optional[Dict[str,Any]]=None)->GraphModule
torch.ao.quantization.quantize_fx.prepare_fx(model:torch.nn.Module,qconfig_dict:Any,prepare_custom_config_dict:Optional[Dict[str,Any]]=None,equalization_qconfig_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None)->ObservedGraphModule
torch.ao.quantization.quantize_fx.prepare_qat_fx(model:torch.nn.Module,qconfig_dict:Any,prepare_custom_config_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None)->ObservedGraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/model_utils.py----------------------------------------
A:torch.ao.quantization._dbr.model_utils.weight->getattr(module, weight_name)
A:torch.ao.quantization._dbr.model_utils.bias->getattr(module, seen_op_info.packable_tensor_idx_to_name[2])
A:torch.ao.quantization._dbr.model_utils.(scale, zp)->observer.calculate_qparams()
A:torch.ao.quantization._dbr.model_utils.qweight->torch.quantize_per_tensor(weight, scale, zp, torch.qint8)
A:torch.ao.quantization._dbr.model_utils.packed_params->toq.linear_prepack(qweight, bias)
A:torch.ao.quantization._dbr.model_utils.param_name->seen_op_info.packable_tensor_idx_to_name.get(idx, None)
A:torch.ao.quantization._dbr.model_utils.weight_name->get_tensor_param_name(1, 'weight')
A:torch.ao.quantization._dbr.model_utils.bias_name->get_tensor_param_name(2, 'bias')
A:torch.ao.quantization._dbr.model_utils.qstate.idx_to_op_convert_info[seen_op_info.idx]->qstate.calculate_op_convert_info(seen_op_info)
torch.ao.quantization._dbr.model_utils.attach_op_convert_info_to_model(module:torch.nn.Module)->None
torch.ao.quantization._dbr.model_utils.attach_output_convert_info_to_model(module:torch.nn.Module)->None
torch.ao.quantization._dbr.model_utils.attach_scale_zp_values_to_model(module:torch.nn.Module)->None
torch.ao.quantization._dbr.model_utils.pack_weights_for_functionals(module:torch.nn.Module)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/utils.py----------------------------------------
A:torch.ao.quantization._dbr.utils.wrapper->ObserverWrapper(child)
A:torch.ao.quantization._dbr.utils.is_module->isinstance(op_type, type(torch.nn.Module))
A:torch.ao.quantization._dbr.utils.info->get_weight_argument_info(op)
A:torch.ao.quantization._dbr.utils.arg_name_in_root->get_param_name(module, param_name)
A:torch.ao.quantization._dbr.utils.new_arg->iterate_and_apply(args[idx], flattened_tensor_infos, func, flattened_tensor_infos_idx)
A:torch.ao.quantization._dbr.utils.cached_hook_type->getattr(cur_module, '_auto_quant_module_hook_type', None)
A:torch.ao.quantization._dbr.utils.x_copy->x.clone().detach()
A:torch.ao.quantization._dbr.utils.packable_arg_idxs->get_packable_arg_idxs(seen_op_info.type)
A:torch.ao.quantization._dbr.utils.qconfig->maybe_adjust_qconfig_for_module_type_or_name(qconfig_dict, cur_op_type, cur_fqn, global_qconfig)
torch.ao.quantization._dbr.utils.FuncOutputDTypeType(enum.Enum)
torch.ao.quantization._dbr.utils.FuncOutputObsType(enum.Enum)
torch.ao.quantization._dbr.utils.HookType(enum.Enum)
torch.ao.quantization._dbr.utils.ObserverWrapper(self,child)
torch.ao.quantization._dbr.utils.ObserverWrapper.__init__(self,child)
torch.ao.quantization._dbr.utils.QTensorInfo
torch.ao.quantization._dbr.utils.SeenOpInfo
torch.ao.quantization._dbr.utils.SeenOpInfo.__repr__(self)->str
torch.ao.quantization._dbr.utils._raise_obs_not_found_error(func)
torch.ao.quantization._dbr.utils._raise_obs_op_mismatch(func,prev_op)
torch.ao.quantization._dbr.utils.clone_detach_tensor_without_dispatch(x:torch.Tensor)->torch.Tensor
torch.ao.quantization._dbr.utils.converted_func_needs_scale_zp(seen_op_info:SeenOpInfo)->bool
torch.ao.quantization._dbr.utils.get_cur_qconfig(qconfig_dict:Dict[str,Any],cur_fqn:str,cur_op_type:Callable)->Optional[QConfigAny]
torch.ao.quantization._dbr.utils.get_func_output_dtype_type(seen_op_info:SeenOpInfo)->FuncOutputDTypeType
torch.ao.quantization._dbr.utils.get_func_output_obs_type(seen_op_info:SeenOpInfo)->FuncOutputObsType
torch.ao.quantization._dbr.utils.get_input_args_quant_dequant_info(seen_op_info:SeenOpInfo,tensor_id_to_scale_zp:Dict[int,Tuple[torch.Tensor,torch.Tensor]])->Tuple[List[Optional[Tuple[float, int]]], List[bool], bool]
torch.ao.quantization._dbr.utils.get_input_observed_arg_idxs(op_type:Callable,op_type_is_module:bool)->Optional[List[int]]
torch.ao.quantization._dbr.utils.get_module_hook_type(parent_module:Optional[torch.nn.Module],cur_module:torch.nn.Module)->HookType
torch.ao.quantization._dbr.utils.get_op_packing_only_uses_module_attributes(op:Callable,args:Tuple[Any,...],kwargs:Dict[str,Any],module:torch.nn.Module)->bool
torch.ao.quantization._dbr.utils.get_packable_arg_idxs(op:Callable)->Optional[List[int]]
torch.ao.quantization._dbr.utils.get_packable_nontensor_arg_idxs(op:Callable)->Optional[List[int]]
torch.ao.quantization._dbr.utils.get_packable_tensor_arg_idxs(op:Callable)->Optional[List[int]]
torch.ao.quantization._dbr.utils.get_packable_tensor_kwarg_names(op:Callable)->Optional[List[str]]
torch.ao.quantization._dbr.utils.get_param_name(module:torch.nn.Module,arg:Any)->Optional[str]
torch.ao.quantization._dbr.utils.get_producer_of_seen_op_info(idx_to_seen_op_info:Dict[int,SeenOpInfo],cur_seen_op_info:SeenOpInfo)->Optional[SeenOpInfo]
torch.ao.quantization._dbr.utils.get_quantized_op(seen_op_info:SeenOpInfo)->Optional[Callable]
torch.ao.quantization._dbr.utils.get_torch_function_hook_type(parent_module:Optional[torch.nn.Module],func:Callable)->HookType
torch.ao.quantization._dbr.utils.get_users_of_seen_op_info(idx_to_seen_op_info:Dict[int,SeenOpInfo],cur_seen_op_info:SeenOpInfo)->List[SeenOpInfo]
torch.ao.quantization._dbr.utils.get_weight_arg_idx(op:Callable)->Optional[int]
torch.ao.quantization._dbr.utils.get_weight_argument_info(op:Callable)->Optional[Tuple[int, str]]
torch.ao.quantization._dbr.utils.is_leaf(m:torch.nn.Module,prepare_custom_config_dict:Optional[Dict[str,Any]])->bool
torch.ao.quantization._dbr.utils.iterate_and_apply(args:Any,flattened_tensor_infos:List[Optional[QTensorInfo]],func:Callable,flattened_tensor_infos_idx=None)->Any
torch.ao.quantization._dbr.utils.op_needs_quantization(op:Callable)->bool
torch.ao.quantization._dbr.utils.trace_with_inputs(model:torch.nn.Module,example_args:Tuple[Any])->None
torch.ao.quantization._dbr.utils.unwrap_observers_from_placeholders(module:torch.nn.Module)->None
torch.ao.quantization._dbr.utils.wrap_observers_in_placeholders(module:torch.nn.Module)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/quantization_state.py----------------------------------------
A:torch.ao.quantization._dbr.quantization_state.self.tensor_id_to_observer->torch.nn.ModuleDict()
A:torch.ao.quantization._dbr.quantization_state.self.idx_to_op_convert_info[seen_op_info.idx]->self.calculate_op_convert_info(seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.seen_op_info->self._get_cur_seen_op_info()
A:torch.ao.quantization._dbr.quantization_state.outputs->outputs.dequantize().dequantize()
A:torch.ao.quantization._dbr.quantization_state.args->iterate_and_apply(args, seen_op_info.input_tensor_infos, _maybe_observe)
A:torch.ao.quantization._dbr.quantization_state.func_output_obs_type->get_func_output_obs_type(seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.output->obs(output)
A:torch.ao.quantization._dbr.quantization_state.output_clone->clone_detach_tensor_without_dispatch(output)
A:torch.ao.quantization._dbr.quantization_state.(maybe_new_op, arg_quant_infos, arg_dequant_infos, packed_param_name, additional_kwargs, any_arg_quant_or_dequant_needed, any_arg_kwarg_modification_needed)->self.get_op_convert_info(op)
A:torch.ao.quantization._dbr.quantization_state.arg->arg.dequantize().dequantize()
A:torch.ao.quantization._dbr.quantization_state.packable_arg_idxs->get_packable_arg_idxs(orig_op)
A:torch.ao.quantization._dbr.quantization_state.packed_param->getattr(root_module, packed_param_name)
A:torch.ao.quantization._dbr.quantization_state.maybe_new_op->get_quantized_op(seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.(arg_quant_infos, arg_dequant_infos, any_arg_quant_or_dequant_needed)->get_input_args_quant_dequant_info(seen_op_info, self.tensor_id_to_scale_zp)
A:torch.ao.quantization._dbr.quantization_state.packed_param_name->self._get_packed_param_name(seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.needs_scale_zp->converted_func_needs_scale_zp(seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.any_arg_kwarg_modification_needed->bool(any_arg_quant_or_dequant_needed or packed_param_name is not None or len(additional_kwargs))
A:torch.ao.quantization._dbr.quantization_state.output._qtensor_info->QTensorInfo(qtensor_id[0], output.dtype, dtype_to_use)
A:torch.ao.quantization._dbr.quantization_state.arg._qtensor_info->QTensorInfo(qtensor_id[0], arg.dtype, arg.dtype)
A:torch.ao.quantization._dbr.quantization_state.op_packing_only_uses_module_attributes->get_op_packing_only_uses_module_attributes(op, args, kwargs, root_module)
A:torch.ao.quantization._dbr.quantization_state.packable_tensor_arg_idxs->get_packable_tensor_arg_idxs(op)
A:torch.ao.quantization._dbr.quantization_state.param_name->get_param_name(root_module, arg)
A:torch.ao.quantization._dbr.quantization_state.packable_nontensor_arg_idxs->get_packable_nontensor_arg_idxs(op)
A:torch.ao.quantization._dbr.quantization_state.packable_tensor_kwarg_names->get_packable_tensor_kwarg_names(op)
A:torch.ao.quantization._dbr.quantization_state.kwarg_name_on_module->get_param_name(root_module, kwarg)
A:torch.ao.quantization._dbr.quantization_state.op_type_is_module->isinstance(op, torch.nn.Module)
A:torch.ao.quantization._dbr.quantization_state.qconfig->get_cur_qconfig(self.qconfig_dict, seen_op_info.fqn, seen_op_info.type)
A:torch.ao.quantization._dbr.quantization_state.self.idx_to_seen_op_infos[self.idx]->SeenOpInfo(self.idx, op_type, op_type_is_module, fqn, arg_tensor_infos, [], packable_tensor_idx_to_name, packable_nontensor_idx_to_arg, packable_tensor_kwarg_name_to_name, op_packing_only_uses_module_attributes, qconfig)
A:torch.ao.quantization._dbr.quantization_state.func_output_dtype_type->get_func_output_dtype_type(seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.input_observed_arg_idxs->get_input_observed_arg_idxs(seen_op_info.type, seen_op_info.type_is_module)
A:torch.ao.quantization._dbr.quantization_state.weight_arg_idx->get_weight_arg_idx(seen_op_info.type)
A:torch.ao.quantization._dbr.quantization_state.self.tensor_id_to_observer[str(output_tensor_id)]->get_cur_qconfig(self.qconfig_dict, seen_op_info.fqn, seen_op_info.type).activation()
A:torch.ao.quantization._dbr.quantization_state.prev_op->get_producer_of_seen_op_info(self.idx_to_seen_op_infos, seen_op_info)
A:torch.ao.quantization._dbr.quantization_state.first_input_mod->getattr(root_module, fqn_last_part)
A:torch.ao.quantization._dbr.quantization_state.first_input_obs->get_cur_qconfig(self.qconfig_dict, seen_op_info.fqn, seen_op_info.type).activation()
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState(self,qconfig_dict:Dict[str,Any],fqn:str,input_dtypes:Any=None,output_dtypes:Any=None)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.__init__(self,qconfig_dict:Dict[str,Any],fqn:str,input_dtypes:Any=None,output_dtypes:Any=None)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._first_call_assign_qtensor_infos_to_mod_outputs(self,outputs:Any,qtensor_id:List[int])->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._first_call_assign_qtensor_infos_to_mod_outputs_tensor(self,output:torch.Tensor,qtensor_id:List[int])->torch.Tensor
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._first_call_op_prepare_after_hook_adjust_subgraphs(self,op:Callable,output:Any,args:Tuple[Any,...],first_call:bool,qtensor_id:List[int],seen_op_info:SeenOpInfo)->None
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._first_call_op_prepare_before_hook_create_subgraphs(self,op:Callable,args:Tuple[Any,...],kwargs:Dict[str,Any],first_call:bool,qtensor_id:List[int],fqn:str,root_module:torch.nn.Module)->Tuple[Tuple[Any, ...], Dict[str, Any]]
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._first_call_op_prepare_before_hook_create_subgraphs_tensor(self,op:Callable,arg:Any,arg_tensor_infos:List[Optional[QTensorInfo]],qtensor_id:List[int])->None
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._get_cur_seen_op_info(self)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._get_packed_param_name(self,seen_op_info:SeenOpInfo)->Optional[str]
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._maybe_insert_input_observers(self,seen_op_info:SeenOpInfo)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._maybe_insert_output_observers(self,seen_op_info:SeenOpInfo,root_module:torch.nn.Module)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState._maybe_mod_outputs_dtype_transform(self,outputs:Any)->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.add_seen_op_type_without_op_hooks(self,op_type:Callable)->None
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.calculate_op_convert_info(self,seen_op_info:SeenOpInfo)->OpConvertInfo
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.cur_op_needs_hooks(self,cur_op:Callable)->bool
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.extra_repr(self)->str
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.forward(self,x)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.get_cur_output_inf_dtype(self)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.get_extra_state(self)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.get_op_convert_info(self,op:Callable)->OpConvertInfo
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.get_output_dtypes(self)->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.get_output_qtensor_infos(self)->List[Optional[QTensorInfo]]
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.has_at_least_one_seen_op_info(self)->bool
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.insert_observers(self,root_module:torch.nn.Module)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.mark_cur_op_complete(self,cur_op:Callable)->None
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.op_convert_after_hook(self,op:Callable,output,global_op_idx:List[int])->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.op_convert_before_hook(self,op:Callable,args:Tuple[Any,...],kwargs:Dict[str,Any],root_module:torch.nn.Module)->Tuple[Callable, Tuple[Any, ...], Dict[str, Any]]
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.op_prepare_after_hook(self,op:Callable,output:Any,args:Tuple[Any,...],first_call:bool,qtensor_id:List[int],global_op_idx:List[int])->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.op_prepare_before_hook(self,op:Callable,args:Tuple[Any,...],kwargs:Dict[str,Any],first_call:bool,qtensor_id:List[int],fqn:str,root_module:torch.nn.Module)->Tuple[Tuple[Any, ...], Dict[str, Any]]
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.outputs_convert_hook(self,outputs:Any)->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.outputs_prepare_hook(self,outputs:Any,first_call:bool,qtensor_id:List[int])->Any
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.reset_to_new_call(self)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.set_extra_state(self,state)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.set_needs_dtype_transform_on_outputs(self)
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.validate_cur_op(self,cur_op:Callable)->None
torch.ao.quantization._dbr.quantization_state.AutoQuantizationState.validate_is_at_last_seen_idx(self)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/mappings.py----------------------------------------
A:torch.ao.quantization._dbr.mappings.functions_supported_by_quantization->set([torch.Tensor.add, torch.Tensor.add_, torch.Tensor.mul, torch.add, torch.mul, torch.cat, torch.nn.functional.adaptive_avg_pool2d, F.hardsigmoid, torch.flatten, toq.add, toq.mul, toq.cat, F.conv2d, toq.conv2d, F.dropout, torch.relu, F.relu, F.linear, toq.linear])
A:torch.ao.quantization._dbr.mappings.module_types_supported_by_quantization->set()
A:torch.ao.quantization._dbr.mappings.module_types_supported_by_quantization_preserves_dtype->set([nn.Identity, nn.Dropout])
A:torch.ao.quantization._dbr.mappings.functions_supported_by_quantization_preserves_dtype->set([F.dropout])
A:torch.ao.quantization._dbr.mappings.add_and_mul_ops->set([torch.add, torch.Tensor.add, torch.Tensor.add_, torch.mul, torch.Tensor.mul])
A:torch.ao.quantization._dbr.mappings.a_related_to_b->set()
A:torch.ao.quantization._dbr.mappings.cur_op->type(cur_op)
torch.ao.quantization._dbr.mappings.ops_are_related(cur_op:Callable,expected_op_type:Callable,type_is_module:bool)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/auto_trace_rewriter.py----------------------------------------
A:torch.ao.quantization._dbr.auto_trace_rewriter.quant->super().create_node('call_function', torch.quantize_per_tensor, (args[idx], scale.item(), zp.item(), torch.quint8), {}, None, None)
A:torch.ao.quantization._dbr.auto_trace_rewriter.args->self._maybe_update_outputs(args, output_qtensor_infos, output_dtypes)
A:torch.ao.quantization._dbr.auto_trace_rewriter.(new_target, arg_quant_infos, arg_dequant_infos, packed_param_name, additional_kwargs, _, _)->qstate.get_op_convert_info(target)
A:torch.ao.quantization._dbr.auto_trace_rewriter.additional_kwargs[k]->additional_kwargs[k].item().item()
A:torch.ao.quantization._dbr.auto_trace_rewriter.packable_arg_idxs->get_packable_arg_idxs(old_target)
A:torch.ao.quantization._dbr.auto_trace_rewriter.packed_param_node->super().create_node('get_attr', packed_param_name, (), {}, None, None)
A:torch.ao.quantization._dbr.auto_trace_rewriter.dtype_to_use->qstate.get_cur_output_inf_dtype()
A:torch.ao.quantization._dbr.auto_trace_rewriter.module_instance->getattr(self.root, target)
A:torch.ao.quantization._dbr.auto_trace_rewriter.(_, arg_quant_infos, arg_dequant_infos, _packed_param_name, additional_kwargs, _, _)->qstate.get_op_convert_info(module_instance)
A:torch.ao.quantization._dbr.auto_trace_rewriter.output_qtensor_infos->qstate.get_output_qtensor_infos()
A:torch.ao.quantization._dbr.auto_trace_rewriter.output_dtypes->qstate.get_output_dtypes()
A:torch.ao.quantization._dbr.auto_trace_rewriter.out->super().create_node(kind, target, args, kwargs, name, type_expr)
A:torch.ao.quantization._dbr.auto_trace_rewriter.copied->copy.copy(mod)
A:torch.ao.quantization._dbr.auto_trace_rewriter.graph->AllModuleTracer().trace(copied)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer(self,autowrap_modules:Tuple[ModuleType]=(math,),autowrap_functions:Tuple[Callable,...]=(),param_shapes_constant:bool=False)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer.__init__(self,autowrap_modules:Tuple[ModuleType]=(math,),autowrap_functions:Tuple[Callable,...]=(),param_shapes_constant:bool=False)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer._maybe_update_args_with_dequants(self,args)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer._maybe_update_args_with_quants(self,args,arg_quant_infos,target)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer._maybe_update_outputs(self,outputs,output_qtensor_infos,output_dtypes)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer.call_module(self,m:torch.nn.Module,forward:Callable[...,Any],args:Tuple[Any,...],kwargs:Dict[str,Any])->Any
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer.create_node(self,kind,target,args,kwargs,name=None,type_expr=None)
torch.ao.quantization._dbr.auto_trace_rewriter.AllModuleTracer.is_leaf_module(self,m,module_qualified_name)->bool
torch.ao.quantization._dbr.auto_trace_rewriter.rewrite_for_scripting(mod:torch.nn.Module)->torch.nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/fusion.py----------------------------------------
A:torch.ao.quantization._dbr.fusion.next_seen_op_infos->get_users_of_seen_op_info(qstate.idx_to_seen_op_infos, cur_seen_op_info)
torch.ao.quantization._dbr.fusion.get_module_fusion_fqns(module:torch.nn.Module)->List[List[str]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/auto_trace.py----------------------------------------
A:torch.ao.quantization._dbr.auto_trace.logger->logging.getLogger('auto_trace')
A:torch.ao.quantization._dbr.auto_trace.hook_type->get_module_hook_type(parent_module, cur_module)
A:torch.ao.quantization._dbr.auto_trace.qstate->getattr(parent_module, '_auto_quant_state', None)
A:torch.ao.quantization._dbr.auto_trace.(args, kwargs)->parent_qstate.op_prepare_before_hook(cur_module, args, kwargs, first_call, qtensor_id, fqn, cur_module)
A:torch.ao.quantization._dbr.auto_trace.output->map_aggregate(output, unwrap_proxy)
A:torch.ao.quantization._dbr.auto_trace.new_args->map_aggregate(args, convert_to_dispatch_proxy)
A:torch.ao.quantization._dbr.auto_trace.new_kwargs->map_aggregate(kwargs, convert_to_dispatch_proxy)
A:torch.ao.quantization._dbr.auto_trace.fqn->module_id_to_fqn.get(id(self), None)
A:torch.ao.quantization._dbr.auto_trace.parent_qstate_fc->getattr(parent_module, '_auto_quant_state', None)
A:torch.ao.quantization._dbr.auto_trace.named_modules->list(self.named_modules())
A:torch.ao.quantization._dbr.auto_trace.leaves->set()
A:torch.ao.quantization._dbr.auto_trace.v._auto_quant_state->AutoQuantizationState(qconfig_dict, fqn)
A:torch.ao.quantization._dbr.auto_trace.(func, args, kwargs)->getattr(parent_module, '_auto_quant_state', None).op_convert_before_hook(func, args, kwargs, parent_module)
A:torch.ao.quantization._dbr.auto_trace.args->tuple(new_args)
A:torch.ao.quantization._dbr.auto_trace.fqn_for_logging->module_id_to_fqn.get(id(self), None)
A:torch.ao.quantization._dbr.auto_trace.(_, args, kwargs)->getattr(parent_module, '_auto_quant_state', None).op_convert_before_hook(cur_module, args, kwargs, cur_module)
A:torch.ao.quantization._dbr.auto_trace.dequant->arg.dequantize().as_subclass(QuantizationConvertTensorProxy)
A:torch.ao.quantization._dbr.auto_trace.input->module(input)
torch.ao.quantization._dbr.auto_trace._nn_sequential_patched_forward(cls,input)
torch.ao.quantization._dbr.auto_trace.add_auto_convert(module:torch.nn.Module)->torch.nn.Module
torch.ao.quantization._dbr.auto_trace.add_auto_observation(model:torch.nn.Module,qconfig_dict:Dict[str,Any],example_inputs:Tuple[Any],input_dtypes:Any=(torch.float,),output_dtypes:Any=(torch.float,),prepare_custom_config_dict:Dict[str,Any]=None)->torch.nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/module_swap_utils.py----------------------------------------
A:torch.ao.quantization._dbr.module_swap_utils.qconfig->getattr(mod, 'qconfig', None)
A:torch.ao.quantization._dbr.module_swap_utils.activation_int8_quantized->activation_is_int8_quantized(qconfig)
A:torch.ao.quantization._dbr.module_swap_utils.op_int8_dynamically_quantized->op_is_int8_dynamically_quantized(qconfig)
A:torch.ao.quantization._dbr.module_swap_utils.reassign[name]->swap_module(mod, dynamic_mappings, {})
torch.ao.quantization._dbr.module_swap_utils._swap_child_modules(module:torch.nn.Module,static_mappings:Dict[Callable,Any],dynamic_mappings:Dict[Callable,Any])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/qconfig_dict_utils.py----------------------------------------
A:torch.ao.quantization._dbr.qconfig_dict_utils.replacement_type->TYPE_TO_REPLACEMENT_TYPE.get(target_type, None)
torch.ao.quantization._dbr.qconfig_dict_utils.normalize_object_types(qconfig_dict:Dict[str,Any])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/_dbr/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/_equalize.py----------------------------------------
A:torch.ao.quantization.fx._equalize.new_shape[axis]->input.size(axis)
A:torch.ao.quantization.fx._equalize.self.input_obs->PerChannelMinMaxObserver(ch_axis=1, dtype=dtype, qscheme=qscheme, quant_min=quant_min, quant_max=quant_max, factory_kwargs=factory_kwargs)
A:torch.ao.quantization.fx._equalize.self.equalization_scale->torch.tensor(1)
A:torch.ao.quantization.fx._equalize.self.equalization_shape[1]->x_orig.size(1)
A:torch.ao.quantization.fx._equalize.(min_inputs, max_inputs)->input_obs.get_input_minmax()
A:torch.ao.quantization.fx._equalize.equalization_scale_reshaped->reshape_scale(equalization_scale, 1, weight)
A:torch.ao.quantization.fx._equalize.min_input_scaled->torch.min(torch.mul(min_inputs, equalization_scale_reshaped))
A:torch.ao.quantization.fx._equalize.max_input_scaled->torch.max(torch.mul(max_inputs, equalization_scale_reshaped))
A:torch.ao.quantization.fx._equalize.with_args->classmethod(_with_args)
A:torch.ao.quantization.fx._equalize.self.weight_col_obs->PerChannelMinMaxObserver(ch_axis=1, dtype=dtype, qscheme=qscheme, quant_min=quant_min, quant_max=quant_max, factory_kwargs=factory_kwargs)
A:torch.ao.quantization.fx._equalize.(min_weights, max_weights)->weight_obs.get_weight_col_minmax()
A:torch.ao.quantization.fx._equalize.equalization_scale->calculate_equalization_scale(input_eq_obs, weight_eq_obs)
A:torch.ao.quantization.fx._equalize.self->super(EqualizationQConfig, cls).__new__(cls, input_activation, weight)
A:torch.ao.quantization.fx._equalize.input_equalization_observer->_InputEqualizationObserver.with_args(dtype=torch.quint8, qscheme=torch.per_tensor_symmetric)
A:torch.ao.quantization.fx._equalize.weight_equalization_observer->_WeightEqualizationObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
A:torch.ao.quantization.fx._equalize.default_equalization_qconfig->EqualizationQConfig(input_activation=input_equalization_observer, weight=weight_equalization_observer)
A:torch.ao.quantization.fx._equalize.weight_eq_obs->update_obs_for_equalization(model, modules).get(node.name)
A:torch.ao.quantization.fx._equalize.weight_node->maybe_get_weight_eq_obs_node(op_node, modules)
A:torch.ao.quantization.fx._equalize.maybe_relu_node->maybe_get_next_module(node, modules, target_functional_type=F.relu)
A:torch.ao.quantization.fx._equalize.maybe_eq_obs_node->maybe_get_next_module(maybe_obs_node, modules, _InputEqualizationObserver)
A:torch.ao.quantization.fx._equalize.next_inp_eq_obs->maybe_get_next_input_eq_obs(node, modules)
A:torch.ao.quantization.fx._equalize.(min_input_scaled, max_input_scaled)->input_eq_obs.calculate_scaled_minmax()
A:torch.ao.quantization.fx._equalize.scaled_weight->torch.mul(scaled_weight, next_equalization_scale_reshaped)
A:torch.ao.quantization.fx._equalize.op_module.weight->torch.nn.Parameter(scaled_weight)
A:torch.ao.quantization.fx._equalize.next_equalization_scale_reshaped->reshape_scale(next_equalization_scale, 0, bias)
A:torch.ao.quantization.fx._equalize.scaled_bias->torch.mul(bias, next_equalization_scale_reshaped)
A:torch.ao.quantization.fx._equalize.op_module.bias->torch.nn.Parameter(scaled_bias)
A:torch.ao.quantization.fx._equalize.weight_eq_obs_node->maybe_get_weight_eq_obs_node(node, modules)
A:torch.ao.quantization.fx._equalize.(weight_parent_name, weight_name)->_parent_name(weight_node.target)
A:torch.ao.quantization.fx._equalize.weight->getattr(modules[weight_parent_name], weight_name)
A:torch.ao.quantization.fx._equalize.(bias_parent_name, bias_name)->_parent_name(bias_node.target)
A:torch.ao.quantization.fx._equalize.bias->getattr(modules[bias_parent_name], bias_name)
A:torch.ao.quantization.fx._equalize.orig_users->list(node.users.keys())
A:torch.ao.quantization.fx._equalize.(op_node, weight_eq_obs)->get_op_node_and_weight_eq_obs(node, model, modules)
A:torch.ao.quantization.fx._equalize.get_new_eq_scale_name->get_new_attr_name_with_prefix(prev_node.name + '_equalization_scale')
A:torch.ao.quantization.fx._equalize.name->get_new_eq_scale_name(modules)
A:torch.ao.quantization.fx._equalize.eq_scale_node->model.graph.create_node('get_attr', name)
A:torch.ao.quantization.fx._equalize.mul_node->model.graph.create_node('call_function', torch.mul, inputs)
A:torch.ao.quantization.fx._equalize.maybe_next_equalization_scale->maybe_get_next_equalization_scale(node, modules)
A:torch.ao.quantization.fx._equalize.modules->dict(model.named_modules(remove_duplicate=False))
A:torch.ao.quantization.fx._equalize.weight_eq_obs_dict->update_obs_for_equalization(model, modules)
A:torch.ao.quantization.fx._equalize.unmatchable_types_map->get_unmatchable_types_map()
A:torch.ao.quantization.fx._equalize.(model_a_ns, model_b_ns)->torch.ao.ns._numeric_suite_fx.add_loggers('fp32', model_a, 'int8', model_b, ns.OutputLogger, unmatchable_types_map=unmatchable_types_map)
A:torch.ao.quantization.fx._equalize.activation_comparison_dict->torch.ao.ns._numeric_suite_fx.extract_logger_info(model_a_ns, model_b_ns, ns.OutputLogger, 'int8')
A:torch.ao.quantization.fx._equalize.layer_sqnr_sorted->sorted(layer_sqnr_dict.items(), key=lambda item: item[1])
A:torch.ao.quantization.fx._equalize.module_to_qconfig_list->list(map(lambda item: (item[0], default_equalization_qconfig), layers_to_equalize))
torch.ao.quantization.fx._equalize.EqualizationQConfig(cls,input_activation=torch.nn.Identity,weight=torch.nn.Identity)
torch.ao.quantization.fx._equalize.EqualizationQConfig.__new__(cls,input_activation=torch.nn.Identity,weight=torch.nn.Identity)
torch.ao.quantization.fx._equalize._InputEqualizationObserver(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.fx._equalize._InputEqualizationObserver.__init__(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.fx._equalize._InputEqualizationObserver.calculate_scaled_minmax(self)
torch.ao.quantization.fx._equalize._InputEqualizationObserver.forward(self,x_orig)
torch.ao.quantization.fx._equalize._InputEqualizationObserver.get_input_minmax(self)
torch.ao.quantization.fx._equalize._InputEqualizationObserver.set_equalization_scale(self,equalization_scale)
torch.ao.quantization.fx._equalize._WeightEqualizationObserver(self,dtype=torch.qint8,qscheme=torch.per_tensor_affine,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.fx._equalize._WeightEqualizationObserver.__init__(self,dtype=torch.qint8,qscheme=torch.per_tensor_affine,quant_min=None,quant_max=None,factory_kwargs=None)
torch.ao.quantization.fx._equalize._WeightEqualizationObserver.forward(self,w_orig)
torch.ao.quantization.fx._equalize._WeightEqualizationObserver.get_weight_col_minmax(self)
torch.ao.quantization.fx._equalize._WeightEqualizationObserver.set_equalization_scale(self,equalization_scale)
torch.ao.quantization.fx._equalize._convert_equalization_ref(model:GraphModule)
torch.ao.quantization.fx._equalize.calculate_equalization_scale(input_obs:_InputEqualizationObserver,weight_obs:_WeightEqualizationObserver)->torch.Tensor
torch.ao.quantization.fx._equalize.clear_weight_quant_obs_node(op_node:Node,modules:Dict[str,nn.Module])->None
torch.ao.quantization.fx._equalize.convert_eq_obs(model:GraphModule,modules:Dict[str,nn.Module],weight_eq_obs_dict:Dict[str,_WeightEqualizationObserver])->None
torch.ao.quantization.fx._equalize.fused_module_supports_equalization(module)->bool
torch.ao.quantization.fx._equalize.get_equalization_qconfig_dict(layer_sqnr_dict:Dict[str,float],num_layers_to_equalize:int)->Any
torch.ao.quantization.fx._equalize.get_layer_sqnr_dict(model_a:nn.Module,model_b:nn.Module,x:torch.Tensor)->Dict[str, float]
torch.ao.quantization.fx._equalize.get_op_node_and_weight_eq_obs(input_eq_obs_node:Node,model:GraphModule,modules:Dict[str,nn.Module])->Tuple[Optional[Node], Optional[_WeightEqualizationObserver]]
torch.ao.quantization.fx._equalize.is_equalization_observer(observer:nn.Module)->bool
torch.ao.quantization.fx._equalize.maybe_get_next_equalization_scale(node:Node,modules:Dict[str,nn.Module])->Optional[torch.Tensor]
torch.ao.quantization.fx._equalize.maybe_get_next_input_eq_obs(node:Node,modules:Dict[str,nn.Module])->Optional[_InputEqualizationObserver]
torch.ao.quantization.fx._equalize.maybe_get_weight_eq_obs_node(op_node:Node,modules:Dict[str,nn.Module])->Optional[Node]
torch.ao.quantization.fx._equalize.nn_module_supports_equalization(module)->bool
torch.ao.quantization.fx._equalize.node_supports_equalization(node:Node,modules)->bool
torch.ao.quantization.fx._equalize.remove_node(model:GraphModule,node:Node,prev_node:Node)
torch.ao.quantization.fx._equalize.reshape_scale(scale:torch.Tensor,axis:int,input:torch.Tensor)->torch.Tensor
torch.ao.quantization.fx._equalize.scale_input_observer(node:Node,modules:Dict[str,nn.Module])->None
torch.ao.quantization.fx._equalize.scale_weight_functional(op_node:Node,model:GraphModule,modules:Dict[str,nn.Module],equalization_scale:torch.Tensor,next_equalization_scale:Optional[torch.Tensor])->None
torch.ao.quantization.fx._equalize.scale_weight_node(node:Node,modules:Dict[str,nn.Module],equalization_scale:torch.Tensor,next_equalization_scale:Optional[torch.Tensor])->None
torch.ao.quantization.fx._equalize.update_obs_for_equalization(model:GraphModule,modules:Dict[str,nn.Module])->Dict[str, _WeightEqualizationObserver]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/quantized_fusion_patterns_and_replacements.py----------------------------------------
A:torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.x->torch.flatten(x)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements._get_all_patterns_and_replacements()
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.flatten_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.flatten_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.get_fbgemm_patterns_and_replacements()
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.get_qnnpack_patterns_and_replacements()
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.hardtanh_inplace_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.hardtanh_inplace_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.hardtanh_non_inplace_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.hardtanh_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.hardtanh_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.max_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.max_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.mean_method_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.mean_method_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.mean_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.mean_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.min_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.min_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu6_inplace_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu6_non_inplace_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu6_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_inplace_method_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_inplace_method_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_inplace_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_method_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_method_replacement(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_non_inplace_pattern(x,scale,zero_point)
torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements.relu_replacement(x,scale,zero_point)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/quantization_types.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/qconfig_utils.py----------------------------------------
A:torch.ao.quantization.fx.qconfig_utils.qconfig_module_name_object_type_order->qconfig_dict.get('module_name_object_type_order', {})
A:torch.ao.quantization.fx.qconfig_utils.object_type_dict->qconfig_dict.get('object_type', None)
A:torch.ao.quantization.fx.qconfig_utils.modules->dict(model.named_modules())
A:torch.ao.quantization.fx.qconfig_utils.ops->list(maybe_fused_module._modules.values())
A:torch.ao.quantization.fx.qconfig_utils.fused_qconfig->qconfig_dict.get('object_type', None).get(type(ops[0]), None)
A:torch.ao.quantization.fx.qconfig_utils.global_qconfig->qconfig_dict.get('', None)
A:torch.ao.quantization.fx.qconfig_utils.qconfig_map->dict()
A:torch.ao.quantization.fx.qconfig_utils.(module_name, _)->_parent_name(node.target)
A:torch.ao.quantization.fx.qconfig_utils.qconfig->maybe_adjust_qconfig_for_module_name_object_type_order(qconfig_dict, parent_name, module_type, cur_object_type_idx, qconfig)
A:torch.ao.quantization.fx.qconfig_utils.qconfig_with_device_check->add_module_to_qconfig_obs_ctr(qconfig, modules.get(node.target, None))
A:torch.ao.quantization.fx.qconfig_utils.function_qconfig->get_object_type_qconfig(qconfig_dict, node.target, global_qconfig)
A:torch.ao.quantization.fx.qconfig_utils.(parent_name, _)->_parent_name(module_path)
A:torch.ao.quantization.fx.qconfig_utils.prepare_keys->prepare_qconfig_dict.keys()
A:torch.ao.quantization.fx.qconfig_utils.convert_keys->convert_qconfig_dict.keys()
A:torch.ao.quantization.fx.qconfig_utils.standalone_module_name_configs->custom_config_dict.get('standalone_module_name', [])
A:torch.ao.quantization.fx.qconfig_utils.standalone_module_class_configs->custom_config_dict.get('standalone_module_class', [])
A:torch.ao.quantization.fx.qconfig_utils.config->name_config_map.get(module_name, config)
torch.ao.quantization.fx.qconfig_utils.check_is_valid_config_dict(config_dict:Any,allowed_keys:Set[str],dict_name:str)->None
torch.ao.quantization.fx.qconfig_utils.check_is_valid_convert_custom_config_dict(convert_custom_config_dict:Optional[Dict[str,Any]]=None)->None
torch.ao.quantization.fx.qconfig_utils.check_is_valid_fuse_custom_config_dict(fuse_custom_config_dict:Optional[Dict[str,Any]]=None)->None
torch.ao.quantization.fx.qconfig_utils.check_is_valid_prepare_custom_config_dict(prepare_custom_config_dict:Optional[Dict[str,Any]]=None)->None
torch.ao.quantization.fx.qconfig_utils.check_is_valid_qconfig_dict(qconfig_dict:Any)->None
torch.ao.quantization.fx.qconfig_utils.compare_prepare_convert_qconfig_dict(prepare_qconfig_dict:Dict[str,Dict[Any,Any]],convert_qconfig_dict:Dict[str,Dict[Any,Any]])->None
torch.ao.quantization.fx.qconfig_utils.generate_qconfig_map(root:torch.nn.Module,modules:Dict[str,torch.nn.Module],input_graph:Graph,qconfig_dict:Any,node_name_to_scope:Dict[str,Tuple[str,type]])->Dict[str, QConfigAny]
torch.ao.quantization.fx.qconfig_utils.get_standalone_module_configs(module_name:str,module_type:Callable,custom_config_dict:Dict[str,Any])
torch.ao.quantization.fx.qconfig_utils.maybe_adjust_qconfig_for_module_name_object_type_order(qconfig_dict:Any,cur_module_path:str,cur_object_type:Callable,cur_object_type_idx:int,fallback_qconfig:QConfigAny)->QConfigAny
torch.ao.quantization.fx.qconfig_utils.update_qconfig_for_fusion(model:GraphModule,qconfig_dict:Any)->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/utils.py----------------------------------------
A:torch.ao.quantization.fx.utils.built_in_func_re->re.compile('<built-in function (.*)>')
A:torch.ao.quantization.fx.utils.built_in_meth_re->re.compile('<built-in method (.*) of type.*>')
A:torch.ao.quantization.fx.utils.max_lens[s]->len(s)
A:torch.ao.quantization.fx.utils.name->name.replace('activation_post_process', 'obs').replace('activation_post_process', 'obs')
A:torch.ao.quantization.fx.utils.op->str(n.op)
A:torch.ao.quantization.fx.utils.target->target.replace('activation_post_process', 'obs').replace('activation_post_process', 'obs')
A:torch.ao.quantization.fx.utils.built_in_func->re.compile('<built-in function (.*)>').search(target)
A:torch.ao.quantization.fx.utils.built_in_meth->re.compile('<built-in method (.*) of type.*>').search(target)
A:torch.ao.quantization.fx.utils.args->args.replace('activation_post_process', 'obs').replace('activation_post_process', 'obs')
A:torch.ao.quantization.fx.utils.kwargs->str(n.kwargs)
A:torch.ao.quantization.fx.utils.max_lens[k]->max(max_lens[k], len(v))
A:torch.ao.quantization.fx.utils.(scale, zero_point)->activation_post_process.calculate_qparams()
A:torch.ao.quantization.fx.utils.scale->float(scale)
A:torch.ao.quantization.fx.utils.zero_point->int(zero_point)
A:torch.ao.quantization.fx.utils.ch_axis->int(activation_post_process.ch_axis)
A:torch.ao.quantization.fx.utils.users->list(obs_node.users)
A:torch.ao.quantization.fx.utils.(node_type, quantize_op, qparams)->get_quantize_node_info(obs_module)
A:torch.ao.quantization.fx.utils.qparam_node->create_getattr_from_value(root_module, graph, module_path + prefix + key, value)
A:torch.ao.quantization.fx.utils.custom_module_mapping->custom_config_dict.get(custom_config_dict_key, {})
A:torch.ao.quantization.fx.utils.quant_mode_custom_module_config->custom_config_dict.get(custom_config_dict_key, {}).get(quant_mode, {})
A:torch.ao.quantization.fx.utils.quant_mode_custom_module_classes->set(quant_mode_custom_module_config.keys())
A:torch.ao.quantization.fx.utils.prepack_op->prepack_ops.get(conv_op, None)
A:torch.ao.quantization.fx.utils.qconv->qconv_op[has_relu].get(conv_op)
A:torch.ao.quantization.fx.utils.prefix->prefix.replace('.', '_').replace('.', '_')
A:torch.ao.quantization.fx.utils.attr_name->get_new_attr_name(module)
A:torch.ao.quantization.fx.utils.node->frontier.pop()
A:torch.ao.quantization.fx.utils.graph->Graph()
A:torch.ao.quantization.fx.utils.env[producer_node]->Graph().node_copy(producer_node, load_arg)
A:torch.ao.quantization.fx.utils.graph_module->GraphModule(root, graph)
A:torch.ao.quantization.fx.utils.get_new_attr_name->get_new_attr_name_with_prefix(prefix)
A:torch.ao.quantization.fx.utils.device->assert_and_get_unique_device(module)
A:torch.ao.quantization.fx.utils.attr_node->Graph().create_node('get_attr', attr_name)
A:torch.ao.quantization.fx.utils.scale_node->create_getattr_from_value(root_module, quantized_graph, module_path + '_scale_', scale)
A:torch.ao.quantization.fx.utils.zero_point_node->create_getattr_from_value(root_module, quantized_graph, module_path + '_zero_point_', zero_point)
A:torch.ao.quantization.fx.utils.result->all_node_args_have_no_tensors(node.args[0], modules, cache)
A:torch.ao.quantization.fx.utils.this_list_el_args_have_no_tensors->all_node_args_have_no_tensors(list_el, modules, cache)
A:torch.ao.quantization.fx.utils.this_arg_args_have_no_tensors->all_node_args_have_no_tensors(arg, modules, cache)
A:torch.ao.quantization.fx.utils.new_node->quantized_graph.create_node(*create_node_args)
torch.ao.quantization.fx.utils.all_node_args_have_no_tensors(node:Node,modules:Dict[str,torch.nn.Module],cache:Dict[Node,bool])->bool
torch.ao.quantization.fx.utils.assert_and_get_unique_device(module:torch.nn.Module)->Any
torch.ao.quantization.fx.utils.collect_producer_nodes(node:Node)->Optional[List[Node]]
torch.ao.quantization.fx.utils.create_getattr_from_value(module:torch.nn.Module,graph:Graph,prefix:str,value:Any)->Node
torch.ao.quantization.fx.utils.create_node_from_old_node_preserve_meta(quantized_graph:Graph,create_node_args:Tuple[Any,...],old_node:Node)->Node
torch.ao.quantization.fx.utils.create_qparam_nodes(node_name:str,scale:Any,zero_point:Any,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]])->Tuple[Node, Node]
torch.ao.quantization.fx.utils.get_custom_module_class_keys(custom_config_dict,custom_config_dict_key)->List[Any]
torch.ao.quantization.fx.utils.get_linear_prepack_op_for_dtype(dtype)
torch.ao.quantization.fx.utils.get_new_attr_name_with_prefix(prefix:str)->Callable
torch.ao.quantization.fx.utils.get_per_tensor_qparams(activation_post_process)
torch.ao.quantization.fx.utils.get_qconv_op(conv_op:Callable,has_relu:bool)->Callable
torch.ao.quantization.fx.utils.get_qconv_prepack_op(conv_op:Callable)->Callable
torch.ao.quantization.fx.utils.get_quantize_node_info(activation_post_process:Callable)->Tuple[str, Union[Callable, str], Dict[str, Any]]
torch.ao.quantization.fx.utils.graph_module_from_producer_nodes(root:GraphModule,producer_nodes:List[Node])->GraphModule
torch.ao.quantization.fx.utils.graph_pretty_str(g,shorten=True)->str
torch.ao.quantization.fx.utils.is_get_tensor_info_node(node:Node)->bool
torch.ao.quantization.fx.utils.maybe_get_next_module(node:Node,modules:Dict[str,nn.Module],target_module_type:Optional[Type[nn.Module]]=None,target_functional_type:Any=None)->Optional[Node]
torch.ao.quantization.fx.utils.node_bool_tensor_arg_indexes(node:Node)->List[int]
torch.ao.quantization.fx.utils.node_return_type_is_int(node:Node)->bool
torch.ao.quantization.fx.utils.quantize_node(in_node:Node,obs_module:torch.nn.Module,obs_node:Node,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],is_input:bool)->Node


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/subgraph_rewriter_FORKED_DO_NOT_USE.py----------------------------------------
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.self.pattern_anchor->next(iter(reversed(pattern.nodes)))
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.match_found->any((self._match_nodes(pn.all_input_nodes[0], gn_) for gn_ in gn.all_input_nodes))
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.mod_match->mod.get_submodule(target)
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.gm_submod->try_get_submodule(gm, node.target)
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.replacement_submod->try_get_submodule(replacement, node.target)
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.new_submod->copy.deepcopy(getattr(replacement, node.target))
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.matcher->_SubgraphMatcher(pattern_graph)
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.original_graph_node->match_changed_node.get(match.nodes_map[pattern_node], match.nodes_map[pattern_node])
A:torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.copied_output->original_graph.graph_copy(replacement_graph, val_map)
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.Match(NamedTuple)
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE._SubgraphMatcher(self,pattern:Graph)
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE._SubgraphMatcher.__init__(self,pattern:Graph)
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE._SubgraphMatcher._match_nodes(self,pn:Node,gn:Node)->bool
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE._SubgraphMatcher.matches_subgraph_from_anchor(self,anchor:Node)->bool
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE._replace_submodules(gm:GraphModule,replacement:torch.nn.Module)->None
torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE.replace_pattern(gm:GraphModule,pattern:Callable,replacement:Callable)->List[Match]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/prepare.py----------------------------------------
A:torch.ao.quantization.fx.prepare.is_weight->node_arg_is_weight(node, arg)
A:torch.ao.quantization.fx.prepare.is_bias->node_arg_is_bias(node, arg)
A:torch.ao.quantization.fx.prepare.input_activation_dtype->dtype_config.get('input_activation_dtype', None)
A:torch.ao.quantization.fx.prepare.weight_dtype->dtype_config.get('weight_dtype', None)
A:torch.ao.quantization.fx.prepare.bias_dtype->dtype_config.get('bias_dtype', None)
A:torch.ao.quantization.fx.prepare.output_dtype->dtype_config.get('output_dtype', None)
A:torch.ao.quantization.fx.prepare.pattern_to_dtype_configs->get_pattern_to_dtype_configs(backend_config_dict)
A:torch.ao.quantization.fx.prepare.standalone_module_name->str(node.target)
A:torch.ao.quantization.fx.prepare.standalone_module_type->type(modules[standalone_module_name])
A:torch.ao.quantization.fx.prepare.(sm_qconfig_dict, sm_prepare_config_dict, sm_backend_config_dict)->prepare_get_standalone_module_configs(root_node, modules, prepare_custom_config_dict, qconfig, backend_config_dict)
A:torch.ao.quantization.fx.prepare.model_device->assert_and_get_unique_device(model)
A:torch.ao.quantization.fx.prepare.get_new_observer_name->get_new_attr_name_with_prefix(prefix)
A:torch.ao.quantization.fx.prepare.observer_name->get_new_observer_name(model)
A:torch.ao.quantization.fx.prepare.new_obs->insert_observer(node, node, observer, model, modules, graph)
A:torch.ao.quantization.fx.prepare.args_have_no_tensors->all_node_args_have_no_tensors(node, modules, cache_for_no_tensor_check)
A:torch.ao.quantization.fx.prepare.(act_dtype, weight_dtype, act_compute_dtype)->get_qconfig_dtypes(qconfig)
A:torch.ao.quantization.fx.prepare.new_inner_arg->maybe_insert_input_observer_for_arg_or_kwarg(node, inner_arg, qconfig, model, modules, graph, node_name_to_target_dtype, qhandler, prepare_custom_config_dict, backend_config_dict)
A:torch.ao.quantization.fx.prepare.is_reuse_input_qconfig_->is_reuse_input_qconfig(qconfig)
A:torch.ao.quantization.fx.prepare.arg_as_output_target_dtype->get_arg_target_dtype_as_output(arg, modules, node_name_to_target_dtype)
A:torch.ao.quantization.fx.prepare.arg_as_input_target_dtype->get_arg_target_dtype_as_input_to_node(arg, node, modules, node_name_to_target_dtype)
A:torch.ao.quantization.fx.prepare.(_sm_qconfig_dict, sm_prepare_config_dict, _sm_backend_config_dict)->prepare_get_standalone_module_configs(node, modules, prepare_custom_config_dict, qconfig, backend_config_dict)
A:torch.ao.quantization.fx.prepare.sm_input_quantized_idxs->sm_prepare_config_dict.get('input_quantized_idxs', [])
A:torch.ao.quantization.fx.prepare.new_obs_mod->act_post_process_ctr()
A:torch.ao.quantization.fx.prepare.new_obs_node->insert_observer(arg, node, new_obs_mod, model, modules, graph)
A:torch.ao.quantization.fx.prepare.new_arg->maybe_insert_input_observer_for_arg_or_kwarg(node, arg, qconfig, model, modules, graph, node_name_to_target_dtype, qhandler, prepare_custom_config_dict, backend_config_dict)
A:torch.ao.quantization.fx.prepare.new_kwarg->maybe_insert_input_observer_for_arg_or_kwarg(node, kwarg, qconfig, model, modules, graph, node_name_to_target_dtype, qhandler, prepare_custom_config_dict, backend_config_dict)
A:torch.ao.quantization.fx.prepare.node.args->tuple(new_args)
A:torch.ao.quantization.fx.prepare.new_eq_obs_mod->act_eq_process_ctr()
A:torch.ao.quantization.fx.prepare.new_eq_obs_node->insert_observer(arg, node, new_eq_obs_mod, model, modules, graph)
A:torch.ao.quantization.fx.prepare.(root_node, matched_nodes, pattern, qhandler, qconfig)->find_matches(model.graph, modules, patterns, qconfig_map, standalone_module_names, standalone_module_classes, custom_module_classes).get(node.name, (None, None, None, None, None))
A:torch.ao.quantization.fx.prepare.act_post_process_ctr->qhandler.get_activation_ctr(qconfig, matched_pattern, is_qat)
A:torch.ao.quantization.fx.prepare.observer->act_post_process_ctr()
A:torch.ao.quantization.fx.prepare.this_node_dtype->get_arg_target_dtype_as_output(maybe_node, modules, node_name_to_target_dtype)
A:torch.ao.quantization.fx.prepare.qconfig->generate_qconfig_map(model, modules, model.graph, qconfig_dict, node_name_to_scope).get(maybe_node.name)
A:torch.ao.quantization.fx.prepare.observer_mod->generate_qconfig_map(model, modules, model.graph, qconfig_dict, node_name_to_scope).get(maybe_node.name).activation()
A:torch.ao.quantization.fx.prepare.observer_node->insert_observer(maybe_node, maybe_node, observer_mod, model, modules, graph)
A:torch.ao.quantization.fx.prepare.results_dict[k]->_recursive_maybe_replace_node_with_obs(inner_v, target_dtype, node_name_to_target_dtype, qconfig_map, model, modules, graph)
A:torch.ao.quantization.fx.prepare.graph_output_node.args->tuple(new_args)
A:torch.ao.quantization.fx.prepare.bool_arg_idxs->node_bool_tensor_arg_indexes(node)
A:torch.ao.quantization.fx.prepare.(parent_name, name)->_parent_name(root_node.target)
A:torch.ao.quantization.fx.prepare.items->list(node.users.items())
A:torch.ao.quantization.fx.prepare.custom_module_class_mapping->prepare_custom_config_dict.get('float_to_observed_custom_module_class', {})
A:torch.ao.quantization.fx.prepare.observed_custom_module_class->get_swapped_custom_module_class(custom_module, custom_module_class_mapping, qconfig)
A:torch.ao.quantization.fx.prepare.observed_custom_module->get_swapped_custom_module_class(custom_module, custom_module_class_mapping, qconfig).from_float(custom_module)
A:torch.ao.quantization.fx.prepare.modules->dict(model.named_modules(remove_duplicate=False))
A:torch.ao.quantization.fx.prepare.node_name_to_target_dtype[node.name]->get_target_activation_dtype_for_node(node, qconfig, inputs_seen_counter, outputs_seen_counter, input_quantized_idxs, output_quantized_idxs, qhandler, modules, cache_for_no_tensor_check)
A:torch.ao.quantization.fx.prepare.nodes_before_observation->list(model.graph.nodes)
A:torch.ao.quantization.fx.prepare.equalization_qconfig->equalization_config_map.get(node.name, None)
A:torch.ao.quantization.fx.prepare.is_supported_by_backend->is_pattern_dtype_config_supported_by_backend(pattern, matched_nodes, node_name_to_target_dtype, backend_config_dict)
A:torch.ao.quantization.fx.prepare.maybe_output_obs_node->maybe_insert_output_observer_for_node(node, model, modules, graph, matches, node_name_to_target_dtype, pattern, qhandler, is_qat)
A:torch.ao.quantization.fx.prepare.orig_users->list(node.users.keys())
A:torch.ao.quantization.fx.prepare.observed_standalone_module->ObservedStandaloneGraphModule(observed_standalone_module, observed_standalone_module.graph, preserved_attributes)
A:torch.ao.quantization.fx.prepare.preserved_attributes->set(prepare_custom_config_dict.get('preserved_attributes', []))
A:torch.ao.quantization.fx.prepare.additional_quant_patterns->prepare_custom_config_dict.get('additional_quant_pattern', {})
A:torch.ao.quantization.fx.prepare.quant_patterns->get_default_quant_patterns()
A:torch.ao.quantization.fx.prepare.patterns->get_pattern_to_quantize_handlers(backend_config_dict)
A:torch.ao.quantization.fx.prepare.pattern_to_input_type_to_index->get_pattern_to_input_type_to_index(backend_config_dict)
A:torch.ao.quantization.fx.prepare.qconfig_dict->update_qconfig_for_qat(qconfig_dict, additional_qat_module_mapping)
A:torch.ao.quantization.fx.prepare.equalization_qconfig_dict->update_qconfig_for_fusion(model, equalization_qconfig_dict)
A:torch.ao.quantization.fx.prepare.flattened_qconfig_dict->get_flattened_qconfig_dict(qconfig_dict)
A:torch.ao.quantization.fx.prepare.additional_qat_module_mapping->prepare_custom_config_dict.get('additional_qat_module_mapping', {})
A:torch.ao.quantization.fx.prepare.module_to_qat_module->get_module_to_qat_module(backend_config_dict)
A:torch.ao.quantization.fx.prepare.equalization_qconfig_map->generate_qconfig_map(model, modules, model.graph, equalization_qconfig_dict, node_name_to_scope)
A:torch.ao.quantization.fx.prepare.qconfig_map->generate_qconfig_map(model, modules, model.graph, qconfig_dict, node_name_to_scope)
A:torch.ao.quantization.fx.prepare.standalone_module_name_configs->prepare_custom_config_dict.get('standalone_module_name', [])
A:torch.ao.quantization.fx.prepare.standalone_module_class_configs->prepare_custom_config_dict.get('standalone_module_class', [])
A:torch.ao.quantization.fx.prepare.custom_module_classes->get_custom_module_class_keys(prepare_custom_config_dict, 'float_to_observed_custom_module_class')
A:torch.ao.quantization.fx.prepare.matches->find_matches(model.graph, modules, patterns, qconfig_map, standalone_module_names, standalone_module_classes, custom_module_classes)
A:torch.ao.quantization.fx.prepare.result_node->insert_observers_for_model(model, modules, matches, qconfig_map, model.graph, prepare_custom_config_dict, equalization_qconfig_map, input_quantized_idxs, output_quantized_idxs, backend_config_dict, observed_node_names, is_qat)
A:torch.ao.quantization.fx.prepare.model->ObservedGraphModule(model, model.graph, preserved_attributes)
A:torch.ao.quantization.fx.prepare.model._standalone_module_input_quantized_idxs->torch.tensor(input_quantized_idxs)
A:torch.ao.quantization.fx.prepare.model._standalone_module_output_quantized_idxs->torch.tensor(output_quantized_idxs)
torch.ao.quantization.fx.prepare(model:GraphModule,qconfig_dict:Any,is_qat:bool,node_name_to_scope:Dict[str,Tuple[str,type]],prepare_custom_config_dict:Optional[Dict[str,Any]]=None,equalization_qconfig_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None,is_standalone_module:bool=False)->ObservedGraphModule
torch.ao.quantization.fx.prepare.get_arg_target_dtype_as_input_to_node(arg:Node,node:Node,modules:Dict[str,torch.nn.Module],node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]])->Optional[torch.dtype]
torch.ao.quantization.fx.prepare.get_arg_target_dtype_as_output(arg:Node,modules:Dict[str,torch.nn.Module],node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]])->Optional[torch.dtype]
torch.ao.quantization.fx.prepare.get_target_activation_dtype_for_node(node:Node,qconfig:QConfigAny,inputs_seen_counter:int,outputs_seen_counter:int,input_quantized_idxs:List[int],output_quantized_idxs:List[int],qhandler:Optional[QuantizeHandler],modules:Dict[str,torch.nn.Module],cache_for_no_tensor_check:Dict[Node,bool])->Dict[str, Optional[torch.dtype]]
torch.ao.quantization.fx.prepare.insert_observer(node:Node,observed_op:Node,observer:ObserverBase,model:torch.nn.Module,modules:Dict[str,torch.nn.Module],graph:Graph)->Node
torch.ao.quantization.fx.prepare.insert_observers_for_model(model:GraphModule,modules:Dict[str,torch.nn.Module],matches:Dict[str,MatchResult],qconfig_map:Dict[str,QConfigAny],graph:Graph,prepare_custom_config_dict:Dict[str,Any],equalization_config_map:Dict[str,Any],input_quantized_idxs:List[int],output_quantized_idxs:List[int],backend_config_dict:Optional[Dict[str,Any]],observed_node_names:Set[str],is_qat:bool)->Optional[Node]
torch.ao.quantization.fx.prepare.is_activation_post_process_node(node:Node,modules:Dict[str,torch.nn.Module])->bool
torch.ao.quantization.fx.prepare.is_input_arg_dtype_supported_by_backend(arg:Argument,node:Node,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],dtype_config:Dict[str,torch.dtype])->bool
torch.ao.quantization.fx.prepare.is_output_dtype_supported_by_backend(node:Node,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],dtype_config:Dict[str,torch.dtype])->bool
torch.ao.quantization.fx.prepare.is_pattern_dtype_config_supported_by_backend(pattern:Optional[Pattern],matched_nodes:Optional[List[Node]],node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],backend_config_dict:Optional[Dict[str,Any]])->bool
torch.ao.quantization.fx.prepare.maybe_insert_input_equalization_observers_for_node(node:Node,equalization_qconfig:Any,model:torch.nn.Module,modules:Dict[str,torch.nn.Module],graph:Graph,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],is_branch:bool)->None
torch.ao.quantization.fx.prepare.maybe_insert_input_observer_for_arg_or_kwarg(node:Union[Node,Any],arg:Argument,qconfig:QConfigAny,model:torch.nn.Module,modules:Dict[str,torch.nn.Module],graph:Graph,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],qhandler:Optional[QuantizeHandler],prepare_custom_config_dict:Dict[str,Any],backend_config_dict:Optional[Dict[str,Any]])->Argument
torch.ao.quantization.fx.prepare.maybe_insert_input_observers_for_node(node:Node,qconfig:QConfigAny,model:torch.nn.Module,modules:Dict[str,torch.nn.Module],graph:Graph,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],qhandler:Optional[QuantizeHandler],prepare_custom_config_dict:Dict[str,Any],backend_config_dict:Optional[Dict[str,Any]])->None
torch.ao.quantization.fx.prepare.maybe_insert_observers_before_graph_output(graph_output_node:Node,output_quantized_idxs:List[int],node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],qconfig_map:Dict[str,QConfigAny],model:torch.nn.Module,modules:Dict[str,torch.nn.Module],graph:Graph)->None
torch.ao.quantization.fx.prepare.maybe_insert_output_observer_for_node(node:Node,model:torch.nn.Module,modules:Dict[str,torch.nn.Module],graph:Graph,matches:Dict[str,MatchResult],node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],matched_pattern:Any,qhandler:Optional[QuantizeHandler],is_qat:bool)->Optional[Node]
torch.ao.quantization.fx.prepare.maybe_make_input_output_share_observers(node:Node,model:torch.nn.Module,modules:Dict[str,torch.nn.Module])->bool
torch.ao.quantization.fx.prepare.maybe_propagate_dtype_for_node(node:Node,target_dtype:torch.dtype,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],matches:Dict[str,MatchResult])->None
torch.ao.quantization.fx.prepare.node_arg_is_bias(node:Node,arg:Any)->bool
torch.ao.quantization.fx.prepare.node_arg_is_weight(node:Node,arg:Any)->bool
torch.ao.quantization.fx.prepare.prepare(model:GraphModule,qconfig_dict:Any,is_qat:bool,node_name_to_scope:Dict[str,Tuple[str,type]],prepare_custom_config_dict:Optional[Dict[str,Any]]=None,equalization_qconfig_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None,is_standalone_module:bool=False)->ObservedGraphModule
torch.ao.quantization.fx.prepare.prepare_get_standalone_module_configs(node:Node,modules:Dict[str,torch.nn.Module],prepare_custom_config_dict:Dict[str,Any],parent_qconfig:QConfigAny,parent_backend_config_dict:Optional[Dict[str,Any]])->Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]
torch.ao.quantization.fx.prepare.propagate_dtypes_for_known_nodes(graph:Graph,node_name_to_target_dtype:Dict[str,Dict[str,Optional[torch.dtype]]],matches:Dict[str,MatchResult])->None
torch.ao.quantization.fx.prepare.qat_swap_modules(root:torch.nn.Module,module_to_qat_module:Dict[Callable,Callable])->None
torch.ao.quantization.fx.prepare.remove_output_observer(node:Node,model:torch.nn.Module,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.prepare.run_prepare_fx_on_standalone_modules(model:torch.nn.Module,is_qat:bool,modules:Dict[str,torch.nn.Module],matches:Any,prepare_custom_config_dict:Dict[str,Any],backend_config_dict:Optional[Dict[str,Any]])->None
torch.ao.quantization.fx.prepare.save_state(observed:GraphModule,qconfig_map:Dict[str,QConfigAny],node_name_to_scope:Dict[str,Tuple[str,type]],patterns:Dict[Pattern,QuantizeHandler],prepare_custom_config_dict:Dict[str,Any],equalization_qconfig_map:Dict[str,Any],qconfig_dict:Dict[str,Dict[Any,Any]],is_qat:bool,observed_node_names:Set[str])->None
torch.ao.quantization.fx.prepare.swap_custom_module_to_observed(node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],prepare_custom_config_dict:Dict[str,Any])
torch.ao.quantization.fx.prepare_get_standalone_module_configs(node:Node,modules:Dict[str,torch.nn.Module],prepare_custom_config_dict:Dict[str,Any],parent_qconfig:QConfigAny,parent_backend_config_dict:Optional[Dict[str,Any]])->Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/_lower_to_native_backend.py----------------------------------------
A:torch.ao.quantization.fx._lower_to_native_backend.modules->dict(model.named_modules(remove_duplicate=False))
A:torch.ao.quantization.fx._lower_to_native_backend.nodes->list(model.graph.nodes)
A:torch.ao.quantization.fx._lower_to_native_backend.output_scale->getattr(model, scale_node.target)
A:torch.ao.quantization.fx._lower_to_native_backend.output_zero_point->getattr(model, zero_point_node.target)
A:torch.ao.quantization.fx._lower_to_native_backend.q_module->q_class.from_reference(ref_module, output_scale, output_zero_point)
A:torch.ao.quantization.fx._lower_to_native_backend.(parent_name, module_name)->_parent_name(ref_node.target)
A:torch.ao.quantization.fx._lower_to_native_backend.(is_call_function, is_call_method, is_call_module)->check_node(ref_node, modules)
A:torch.ao.quantization.fx._lower_to_native_backend.model->_lower_weighted_ref_module(model)
torch.ao.quantization.fx._lower_to_native_backend._lower_to_native_backend(model:QuantizedGraphModule)->QuantizedGraphModule
torch.ao.quantization.fx._lower_to_native_backend._lower_weighted_ref_module(model:QuantizedGraphModule)->QuantizedGraphModule
torch.ao.quantization.fx._lower_to_native_backend.special_pattern_replacement(model:QuantizedGraphModule)->QuantizedGraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/fuse.py----------------------------------------
A:torch.ao.quantization.fx.fuse.self.modules->dict(input_root.named_modules())
A:torch.ao.quantization.fx.fuse.additional_fusion_patterns->fuse_custom_config_dict.get('additional_fusion_pattern', {})
A:torch.ao.quantization.fx.fuse.fusion_pattern_to_fuse_handler_cls->get_fusion_pattern_to_fuse_handler_cls(backend_config_dict)
A:torch.ao.quantization.fx.fuse.fuser_method_mapping->get_fuser_method_mapping(backend_config_dict)
A:torch.ao.quantization.fx.fuse.fusion_pairs->self._find_matches(input_root, input_graph, fusion_pattern_to_fuse_handler_cls)
A:torch.ao.quantization.fx.fuse.self.fused_graph->Graph()
A:torch.ao.quantization.fx.fuse.(maybe_last_node, pattern, matched_node_pattern, obj)->self._find_matches(input_root, input_graph, fusion_pattern_to_fuse_handler_cls).get(node.name, (None, None, None, None))
A:torch.ao.quantization.fx.fuse.root_node->get_root_node(matched_node_pattern)
A:torch.ao.quantization.fx.fuse.env[node.name]->self.fused_graph.node_copy(node, load_arg)
A:torch.ao.quantization.fx.fuse.preserved_attributes->set(fuse_custom_config_dict.get('preserved_attributes', []))
A:torch.ao.quantization.fx.fuse.model->FusedGraphModule(input_root, self.fused_graph, preserved_attributes)
A:torch.ao.quantization.fx.fuse.modules->dict(root.named_modules())
torch.ao.quantization.fx.Fuser
torch.ao.quantization.fx.Fuser._find_matches(self,root:GraphModule,graph:Graph,patterns:Dict[Pattern,Callable])->Dict[str, Tuple[Node, Pattern, NodePattern, FuseHandler]]
torch.ao.quantization.fx.Fuser.fuse(self,model:GraphModule,is_qat:bool,fuse_custom_config_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None)->GraphModule
torch.ao.quantization.fx.fuse.Fuser
torch.ao.quantization.fx.fuse.Fuser._find_matches(self,root:GraphModule,graph:Graph,patterns:Dict[Pattern,Callable])->Dict[str, Tuple[Node, Pattern, NodePattern, FuseHandler]]
torch.ao.quantization.fx.fuse.Fuser.fuse(self,model:GraphModule,is_qat:bool,fuse_custom_config_dict:Optional[Dict[str,Any]]=None,backend_config_dict:Optional[Dict[str,Any]]=None)->GraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/quantization_patterns.py----------------------------------------
A:torch.ao.quantization.fx.quantization_patterns.self.num_tensor_args->len(node.args)
A:torch.ao.quantization.fx.quantization_patterns.weight_qparams->get_qparam_dict(weight_post_process)
A:torch.ao.quantization.fx.quantization_patterns.float_module_name->float_module._get_name()
A:torch.ao.quantization.fx.quantization_patterns.dtypes->get_qconfig_dtypes(qconfig)
A:torch.ao.quantization.fx.quantization_patterns.act_dtype->activation_dtype(qconfig)
A:torch.ao.quantization.fx.quantization_patterns.args->load_arg(quantized=torch.float)(self.linear_node.args)
A:torch.ao.quantization.fx.quantization_patterns.kwargs->load_arg(quantized=torch.float)(self.linear_node.kwargs)
A:torch.ao.quantization.fx.quantization_patterns.op_out->create_node_from_old_node_preserve_meta(quantized_graph, ('call_function', torch.nn.functional.relu, tuple(relu_args), relu_kwargs), self.relu_node)
A:torch.ao.quantization.fx.quantization_patterns.activation_post_process->self._maybe_get_last_node_only_observer(modules)
A:torch.ao.quantization.fx.quantization_patterns.(scale, zero_point)->self._maybe_get_last_node_only_observer(modules).calculate_qparams()
A:torch.ao.quantization.fx.quantization_patterns.scale->float(scale)
A:torch.ao.quantization.fx.quantization_patterns.zero_point->int(zero_point)
A:torch.ao.quantization.fx.quantization_patterns.(scale_arg, zero_point_arg)->create_qparam_nodes(node.name, scale, zero_point, modules, quantized_graph, node_name_to_scope)
A:torch.ao.quantization.fx.quantization_patterns.op->create_node_from_old_node_preserve_meta(quantized_graph, ('call_function', qlinear_op, qlinear_args, kwargs), self.linear_node)
A:torch.ao.quantization.fx.quantization_patterns.relu_kwargs->load_arg(quantized=torch.float)(self.relu_node.kwargs)
A:torch.ao.quantization.fx.quantization_patterns.conv_out->quantized_graph.node_copy(self.conv_node, load_arg(quantized=torch.float))
A:torch.ao.quantization.fx.quantization_patterns.activation_int8_quantized->activation_is_int8_quantized(qconfig)
A:torch.ao.quantization.fx.quantization_patterns.output_activation_post_process->self._maybe_get_last_node_only_observer(modules)
A:torch.ao.quantization.fx.quantization_patterns.dtype->activation_dtype(qconfig)
A:torch.ao.quantization.fx.quantization_patterns.activation->load_arg(quantized=dtype)(self.linear_node.args[0])
A:torch.ao.quantization.fx.quantization_patterns.float_conv->self.conv.to_float()
A:torch.ao.quantization.fx.quantization_patterns.(parent_name, name)->_parent_name(self.linear_node.target)
A:torch.ao.quantization.fx.quantization_patterns.weight_post_process->qconfig.weight()
A:torch.ao.quantization.fx.quantization_patterns.qconv_cls->get_static_quant_module_class(type(float_conv), is_reference=is_reference)
A:torch.ao.quantization.fx.quantization_patterns.ref_conv->get_static_quant_module_class(type(float_conv), is_reference=is_reference).from_float(float_conv, weight_qparams)
A:torch.ao.quantization.fx.quantization_patterns.additional_static_quant_mapping->convert_custom_config_dict.get('static', {})
A:torch.ao.quantization.fx.quantization_patterns.quantized->get_static_quant_module_class(type(emb)).from_float(emb)
A:torch.ao.quantization.fx.quantization_patterns.weight->load_arg(quantized=torch.qint8)(self.conv_node.args[1])
A:torch.ao.quantization.fx.quantization_patterns.other_args->load_arg(quantized=torch.float)(self.linear_node.args[2:])
A:torch.ao.quantization.fx.quantization_patterns.prepack_op->get_linear_prepack_op_for_dtype(weight_dtype)
A:torch.ao.quantization.fx.quantization_patterns.packed_weight->quantized_graph.create_node('call_function', prepack_op, prepack_args, {})
A:torch.ao.quantization.fx.quantization_patterns.qconv_op->get_qconv_op(self.conv, self.relu_node is not None)
A:torch.ao.quantization.fx.quantization_patterns.conv_input->load_arg(quantized=torch.quint8)(self.conv_node.args[0])
A:torch.ao.quantization.fx.quantization_patterns.(scale, zero_point, _)->get_per_tensor_qparams(activation_post_process)
A:torch.ao.quantization.fx.quantization_patterns.(scale_node, zero_point_node)->create_qparam_nodes(self.linear_node.name, scale, zero_point, modules, quantized_graph, node_name_to_scope)
A:torch.ao.quantization.fx.quantization_patterns.activation_statically_quantized->activation_is_statically_quantized(qconfig)
A:torch.ao.quantization.fx.quantization_patterns.float_linear->float_linear.to_float().to_float()
A:torch.ao.quantization.fx.quantization_patterns.qlinear_cls->get_static_quant_module_class(type(float_linear), is_reference=is_reference)
A:torch.ao.quantization.fx.quantization_patterns.ref_linear->get_static_quant_module_class(type(float_linear), is_reference=is_reference).from_float(float_linear, weight_qparams)
A:torch.ao.quantization.fx.quantization_patterns.qlinear->get_dynamic_quant_module_class(type(self.linear), additional_dynamic_quant_mapping)
A:torch.ao.quantization.fx.quantization_patterns.additional_dynamic_quant_mapping->convert_custom_config_dict.get('dynamic', {})
A:torch.ao.quantization.fx.quantization_patterns.weight_quantized->weight_is_statically_quantized(qconfig)
A:torch.ao.quantization.fx.quantization_patterns.linear_weight->load_arg(quantized=dtype)(self.linear_node.args[1])
A:torch.ao.quantization.fx.quantization_patterns.bias->load_arg(quantized=torch.float)(self.linear_node.kwargs).pop('bias', None)
A:torch.ao.quantization.fx.quantization_patterns.linear_input->load_arg(quantized=torch.float)(self.linear_node.args[0])
A:torch.ao.quantization.fx.quantization_patterns.qbn_cls->get_static_quant_module_class(type(self.bn), additional_static_quant_mapping)
A:torch.ao.quantization.fx.quantization_patterns.qemb->get_static_quant_module_class(type(emb))
A:torch.ao.quantization.fx.quantization_patterns.qmodule_cls->get_dynamic_quant_module_class(type(module))
A:torch.ao.quantization.fx.quantization_patterns.qmodule->get_dynamic_quant_module_class(type(module)).from_float(module)
A:torch.ao.quantization.fx.quantization_patterns.self.op->type(modules[str(node.target)])
A:torch.ao.quantization.fx.quantization_patterns.quantized_module_cls->get_static_quant_module_class(type(module), additional_static_quant_mapping)
A:torch.ao.quantization.fx.quantization_patterns.quantized_module->get_static_quant_module_class(type(module), additional_static_quant_mapping).from_float(module)
A:torch.ao.quantization.fx.quantization_patterns.quantized_op->get_quantized_operator(node.target)
A:torch.ao.quantization.fx.quantization_patterns.(is_call_function, is_call_method, is_call_module)->check_node(node, modules)
A:torch.ao.quantization.fx.quantization_patterns.custom_module_class_mapping->convert_custom_config_dict.get('observed_to_quantized_custom_module_class', None)
A:torch.ao.quantization.fx.quantization_patterns.quantized_custom_module_class->get_swapped_custom_module_class(observed_custom_module, custom_module_class_mapping, qconfig)
A:torch.ao.quantization.fx.quantization_patterns.quantized_custom_module->get_swapped_custom_module_class(observed_custom_module, custom_module_class_mapping, qconfig).from_observed(observed_custom_module)
A:torch.ao.quantization.fx.quantization_patterns.input_quantized_idxs->observed_standalone_module._standalone_module_input_quantized_idxs.tolist()
A:torch.ao.quantization.fx.quantization_patterns.quantized_standalone_module->convert(observed_standalone_module, is_reference=is_reference)
torch.ao.quantization.fx.quantization_patterns.BatchNormQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.BatchNormQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.BatchNormQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler.input_output_observed(self)
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler.is_general_tensor_value_op(self)->bool
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler.is_output_quantized(self,qconfig)
torch.ao.quantization.fx.quantization_patterns.BinaryOpQuantizeHandler.should_insert_observer_for_output(self,qconfig:Any,model_is_training:bool)->bool
torch.ao.quantization.fx.quantization_patterns.CatQuantizeHandler(QuantizeHandler)
torch.ao.quantization.fx.quantization_patterns.CatQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.CatQuantizeHandler.is_general_tensor_value_op(self)->bool
torch.ao.quantization.fx.quantization_patterns.ConvReLUQuantizeHandlerNew(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.ConvReLUQuantizeHandlerNew.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.ConvReLUQuantizeHandlerNew.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.ConvReLUQuantizeHandlerNew.is_output_quantized(self,qconfig)
torch.ao.quantization.fx.quantization_patterns.ConvReluQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.ConvReluQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.ConvReluQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.CopyNodeQuantizeHandler(QuantizeHandler)
torch.ao.quantization.fx.quantization_patterns.CopyNodeQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.CopyNodeQuantizeHandler.is_general_tensor_value_op(self)->bool
torch.ao.quantization.fx.quantization_patterns.CopyNodeQuantizeHandler.should_mark_output_quantized_from_input_quantized_status(self,qconfig:QConfigAny)->bool
torch.ao.quantization.fx.quantization_patterns.CustomModuleQuantizeHandler(QuantizeHandler)
torch.ao.quantization.fx.quantization_patterns.CustomModuleQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.DefaultNodeQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.DefaultNodeQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.DefaultNodeQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.DefaultNodeQuantizeHandler.is_output_quantized(self,qconfig)
torch.ao.quantization.fx.quantization_patterns.EmbeddingQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.EmbeddingQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.EmbeddingQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.EmbeddingQuantizeHandler.input_output_observed(self)->bool
torch.ao.quantization.fx.quantization_patterns.FixedQParamsOpQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.FixedQParamsOpQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.FixedQParamsOpQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.FixedQParamsOpQuantizeHandler.get_activation_ctr(self,qconfig,pattern,is_training)->Optional[Callable]
torch.ao.quantization.fx.quantization_patterns.FixedQParamsOpQuantizeHandler.should_mark_output_quantized_from_input_quantized_status(self,qconfig:QConfigAny)->bool
torch.ao.quantization.fx.quantization_patterns.GeneralTensorShapeOpQuantizeHandler(QuantizeHandler)
torch.ao.quantization.fx.quantization_patterns.GeneralTensorShapeOpQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.GeneralTensorShapeOpQuantizeHandler.is_general_tensor_shape_op(self)->bool
torch.ao.quantization.fx.quantization_patterns.GeneralTensorShapeOpQuantizeHandler.should_mark_output_quantized_from_input_quantized_status(self,qconfig:QConfigAny)->bool
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandlerNew(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandlerNew.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandlerNew.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.LinearReLUQuantizeHandlerNew.is_output_quantized(self,qconfig)
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler._maybe_get_last_node_only_observer(self,modules:Dict[str,torch.nn.Module])->Optional[torch.nn.Module]
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.get_activation_ctr(self,qconfig:Any,pattern:Pattern,is_training:bool)->Optional[Callable]
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.input_output_observed(self)->bool
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.is_general_tensor_shape_op(self)->bool
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.is_general_tensor_value_op(self)->bool
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.is_output_quantized(self,qconfig)
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.should_insert_observer_for_output(self,qconfig:Any,model_is_training:bool)->bool
torch.ao.quantization.fx.quantization_patterns.QuantizeHandler.should_mark_output_quantized_from_input_quantized_status(self,qconfig:QConfigAny)->bool
torch.ao.quantization.fx.quantization_patterns.RNNDynamicQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.RNNDynamicQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.quantization_patterns.RNNDynamicQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns.RNNDynamicQuantizeHandler.input_output_observed(self)->bool
torch.ao.quantization.fx.quantization_patterns.StandaloneModuleQuantizeHandler(QuantizeHandler)
torch.ao.quantization.fx.quantization_patterns.StandaloneModuleQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node
torch.ao.quantization.fx.quantization_patterns._load_weight_qparams(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.fx.quantization_patterns._save_weight_qparams(self,destination,prefix,keep_vars)
torch.ao.quantization.fx.quantization_patterns._to_reference(float_module,weight_qparams)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/graph_module.py----------------------------------------
A:torch.ao.quantization.fx.graph_module.fake_mod->torch.nn.Module()
A:torch.ao.quantization.fx.graph_module.fake_mod.__dict__->copy.deepcopy(self.__dict__)
A:torch.ao.quantization.fx.graph_module.self.preserved_attr_names->set(['_activation_post_process_map', '_activation_post_process_indexes', '_patterns', '_qconfig_map', '_prepare_custom_config_dict', '_equalization_qconfig_map', '_node_name_to_scope', '_qconfig_dict', '_is_qat', '_observed_node_names']).union(preserved_attr_names)
A:torch.ao.quantization.fx.graph_module.preserved_attr_names->preserved_attr_names.union(set(['_standalone_module_input_quantized_idxs', '_standalone_module_output_quantized_idxs'])).union(set(['_standalone_module_input_quantized_idxs', '_standalone_module_output_quantized_idxs']))
A:torch.ao.quantization.fx.graph_module.packed_weight->getattr(self, attr_name)
torch.ao.quantization.fx.graph_module.FusedGraphModule(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.FusedGraphModule.__deepcopy__(self,memo)
torch.ao.quantization.fx.graph_module.FusedGraphModule.__init__(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.ObservedGraphModule(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.ObservedGraphModule.__deepcopy__(self,memo)
torch.ao.quantization.fx.graph_module.ObservedGraphModule.__init__(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.ObservedStandaloneGraphModule(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.ObservedStandaloneGraphModule.__deepcopy__(self,memo)
torch.ao.quantization.fx.graph_module.ObservedStandaloneGraphModule.__init__(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.QuantizedGraphModule(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.QuantizedGraphModule.__deepcopy__(self,memo)
torch.ao.quantization.fx.graph_module.QuantizedGraphModule.__init__(self,root:Union[torch.nn.Module,Dict[str,Any]],graph:Graph,preserved_attr_names:Set[str])
torch.ao.quantization.fx.graph_module.QuantizedGraphModule._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.quantization.fx.graph_module._save_packed_weight(self,destination,prefix,keep_vars)
torch.ao.quantization.fx.graph_module.is_observed_module(module:Any)->bool
torch.ao.quantization.fx.graph_module.is_observed_standalone_module(module:Any)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/lower_to_qnnpack.py----------------------------------------
torch.ao.quantization.fx.lower_to_qnnpack.lower_to_qnnpack(model:QuantizedGraphModule)->QuantizedGraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/common_quantization_patterns.py----------------------------------------
A:torch.ao.quantization.fx.common_quantization_patterns.self.op->type(modules[str(node.target)])
A:torch.ao.quantization.fx.common_quantization_patterns.additional_static_quant_mapping->convert_custom_config_dict.get('static', {})
A:torch.ao.quantization.fx.common_quantization_patterns.dtypes->get_qconfig_dtypes(qconfig)
A:torch.ao.quantization.fx.common_quantization_patterns.act_dtype->activation_dtype(qconfig)
A:torch.ao.quantization.fx.common_quantization_patterns.op_out->quantized_graph.node_copy(node, load_arg(quantized=torch.float))
A:torch.ao.quantization.fx.common_quantization_patterns.activation_post_process->self._maybe_get_last_node_only_observer(modules)
A:torch.ao.quantization.fx.common_quantization_patterns.args->load_arg(quantized=torch.float)(node.args)
A:torch.ao.quantization.fx.common_quantization_patterns.kwargs->load_arg(quantized=torch.float)(node.kwargs)
torch.ao.quantization.fx.common_quantization_patterns.CommonQuantizeHandler(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.common_quantization_patterns.CommonQuantizeHandler.__init__(self,node:Node,modules:Dict[str,torch.nn.Module])
torch.ao.quantization.fx.common_quantization_patterns.CommonQuantizeHandler.convert(self,node:Node,qconfig:QConfigAny,modules:Dict[str,torch.nn.Module],quantized_graph:Graph,node_name_to_scope:Dict[str,Tuple[str,type]],load_arg:Callable,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None)->Node


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/_convert_do_not_use.py----------------------------------------
A:torch.ao.quantization.fx._convert_do_not_use.dequantize_node->graph.call_method('dequantize', (node,))
A:torch.ao.quantization.fx._convert_do_not_use.(patterns, node_name_to_scope, prepare_custom_config_dict, observed_node_names)->restore_state(model)
A:torch.ao.quantization.fx._convert_do_not_use.modules->dict(model.named_modules(remove_duplicate=False))
A:torch.ao.quantization.fx._convert_do_not_use.custom_module_classes->get_custom_module_class_keys(convert_custom_config_dict, 'observed_to_quantized_custom_module_class')
A:torch.ao.quantization.fx._convert_do_not_use.weight_eq_obs_dict->update_obs_for_equalization(model, modules)
A:torch.ao.quantization.fx._convert_do_not_use.(node_type, quantize_op, qparams)->get_quantize_node_info(observer_module)
A:torch.ao.quantization.fx._convert_do_not_use.qparam_node->create_getattr_from_value(root_module, graph, key, value)
A:torch.ao.quantization.fx._convert_do_not_use.quantized_node->graph.create_node(node_type, quantize_op, tuple(inputs), {})
A:torch.ao.quantization.fx._convert_do_not_use.dequantized_node->graph.call_method('dequantize', args=(quantized_node,))
A:torch.ao.quantization.fx._convert_do_not_use.quantized_reference_module_mapping->get_quantized_reference_module_mapping(backend_config_dict)
A:torch.ao.quantization.fx._convert_do_not_use.weighted_module_classes->tuple(quantized_reference_module_mapping.keys())
A:torch.ao.quantization.fx._convert_do_not_use.sm_input_quantized_idxs->observed_standalone_module._standalone_module_input_quantized_idxs.tolist()
A:torch.ao.quantization.fx._convert_do_not_use.args->list(node.args)
A:torch.ao.quantization.fx._convert_do_not_use.sm_output_quantized_idxs->observed_standalone_module._standalone_module_output_quantized_idxs.tolist()
A:torch.ao.quantization.fx._convert_do_not_use.quantized_standalone_module->convert(observed_standalone_module, is_reference=True, backend_config_dict=backend_config_dict)
A:torch.ao.quantization.fx._convert_do_not_use.(parent_name, name)->_parent_name(node.target)
A:torch.ao.quantization.fx._convert_do_not_use.is_activation_quantized->activation_is_int8_quantized(qconfig)
A:torch.ao.quantization.fx._convert_do_not_use.is_weight_quantized->weight_is_statically_quantized(qconfig)
A:torch.ao.quantization.fx._convert_do_not_use.float_module->original_module.to_float()
A:torch.ao.quantization.fx._convert_do_not_use.weight_post_process->qconfig.weight()
A:torch.ao.quantization.fx._convert_do_not_use.weight_qparams->get_qparam_dict(weight_post_process)
A:torch.ao.quantization.fx._convert_do_not_use.ref_qmodule_cls->get_quantized_reference_module_mapping(backend_config_dict).get(type(float_module), None)
A:torch.ao.quantization.fx._convert_do_not_use.ref_qmodule->get_quantized_reference_module_mapping(backend_config_dict).get(type(float_module), None).from_float(float_module, weight_qparams)
A:torch.ao.quantization.fx._convert_do_not_use.preserved_attributes->set(convert_custom_config_dict.get('preserved_attributes', []))
A:torch.ao.quantization.fx._convert_do_not_use.model->QuantizedGraphModule(model, model.graph, preserved_attributes)
torch.ao.quantization.fx._convert_do_not_use._convert_do_not_use(model:GraphModule,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None,is_standalone_module:bool=False,_remove_qconfig_flag:bool=True,backend_config_dict:Optional[Dict[str,Any]]=None)->torch.nn.Module
torch.ao.quantization.fx._convert_do_not_use.insert_dequantize_node(node:Node,graph:Graph)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/match_utils.py----------------------------------------
torch.ao.quantization.fx.match_utils.MatchAllNode
torch.ao.quantization.fx.match_utils.find_matches(graph:Graph,modules:Dict[str,torch.nn.Module],patterns:Dict[Pattern,QuantizeHandler],qconfig_map:Dict[str,QConfigAny],standalone_module_names:List[str]=None,standalone_module_classes:List[Callable]=None,custom_module_classes:List[Any]=None)->Dict[str, MatchResult]
torch.ao.quantization.fx.match_utils.is_match(modules,node,pattern,max_uses=sys.maxsize)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/lower_to_fbgemm.py----------------------------------------
torch.ao.quantization.fx.lower_to_fbgemm.lower_to_fbgemm(model:QuantizedGraphModule)->QuantizedGraphModule


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/convert.py----------------------------------------
A:torch.ao.quantization.fx.convert.weight_observer_nodes->collect_producer_nodes(node_arg)
A:torch.ao.quantization.fx.convert.weight_observer_module->graph_module_from_producer_nodes(observed, weight_observer_nodes)
A:torch.ao.quantization.fx.convert.packed_weights->dict()
A:torch.ao.quantization.fx.convert.folded_nodes->dict()
A:torch.ao.quantization.fx.convert.nodes_to_fold->collect_producer_nodes(node)
A:torch.ao.quantization.fx.convert.prepacking_module->graph_module_from_producer_nodes(quantized, nodes_to_fold)
A:torch.ao.quantization.fx.convert.packed_weight->prepacking_module()
A:torch.ao.quantization.fx.convert.folded_graph->Graph()
A:torch.ao.quantization.fx.convert.prepack_node->dict().get(node.name, None)
A:torch.ao.quantization.fx.convert.get_new_packed_weight_name->get_new_attr_name_with_prefix(module_path + '_packed_weight_')
A:torch.ao.quantization.fx.convert.packed_weight_name->get_new_packed_weight_name(quantized_root)
A:torch.ao.quantization.fx.convert.env[node.name]->Graph().node_copy(node, load_arg)
A:torch.ao.quantization.fx.convert.quantized->is_output_quantized(node, obj, qconfig, modules)
A:torch.ao.quantization.fx.convert.users->list(node.users)
A:torch.ao.quantization.fx.convert.orig_args->list(node.args)
A:torch.ao.quantization.fx.convert.new_node->is_output_quantized(node, obj, qconfig, modules).graph.create_node('call_method', 'dequantize', node.args, {})
A:torch.ao.quantization.fx.convert.unique_dq->is_output_quantized(node, obj, qconfig, modules).graph.create_node('call_method', 'dequantize', users[0].args, {})
A:torch.ao.quantization.fx.convert.(patterns, node_name_to_scope, prepare_custom_config_dict, _)->restore_state(model)
A:torch.ao.quantization.fx.convert.modules->dict(model.named_modules(remove_duplicate=False))
A:torch.ao.quantization.fx.convert.modules_copy->copy.deepcopy(modules)
A:torch.ao.quantization.fx.convert.additional_qat_module_mapping->prepare_custom_config_dict.get('additional_qat_module_mapping', {})
A:torch.ao.quantization.fx.convert.convert_qconfig_dict->update_qconfig_for_fusion(model, convert_qconfig_dict)
A:torch.ao.quantization.fx.convert.convert_qconfig_map->generate_qconfig_map(model, modules_copy, model.graph, convert_qconfig_dict, node_name_to_scope)
A:torch.ao.quantization.fx.convert.custom_module_classes->get_custom_module_class_keys(convert_custom_config_dict, 'observed_to_quantized_custom_module_class')
A:torch.ao.quantization.fx.convert.matches->find_matches(model.graph, modules, patterns, qconfig_map, custom_module_classes=custom_module_classes)
A:torch.ao.quantization.fx.convert.weight_eq_obs_dict->update_obs_for_equalization(model, modules)
A:torch.ao.quantization.fx.convert.quantized_graph->Graph()
A:torch.ao.quantization.fx.convert.env[node.name][torch.float]->Graph().node_copy(node, load_non_quantized)
A:torch.ao.quantization.fx.convert.env[node.name][observer_dtype]->quantize_node(load_non_quantized(prev_node), observer_module, node, modules, quantized_graph, node_name_to_scope, is_input=True)
A:torch.ao.quantization.fx.convert.env[node.name][dtype]->quantize_node(load_non_quantized(node.args[0]), observer_module, node, modules, quantized_graph, node_name_to_scope, is_input=True)
A:torch.ao.quantization.fx.convert.graph_output->map_arg(node.args[0], load_non_quantized)
A:torch.ao.quantization.fx.convert.(root_node, matched, matched_pattern, obj, qconfig)->find_matches(model.graph, modules, patterns, qconfig_map, custom_module_classes=custom_module_classes).get(node.name, (None, None, None, None, None))
A:torch.ao.quantization.fx.convert.result->Graph().node_copy(node, load_non_quantized)
A:torch.ao.quantization.fx.convert.float_mod->modules[node.target].to_float()
A:torch.ao.quantization.fx.convert.new_float_node->remove_extra_dequantize(model).graph.create_node('call_module', node.name, node.args, node.kwargs)
A:torch.ao.quantization.fx.convert.out_quant_idxs->modules[node.target]._standalone_module_output_quantized_idxs.tolist()
A:torch.ao.quantization.fx.convert.env[node.name][torch.quint8]->Graph().node_copy(node, load_non_quantized)
A:torch.ao.quantization.fx.convert.env[node.name][None]->Graph().node_copy(node, load_x)
A:torch.ao.quantization.fx.convert.act_post_process_removed_graph->Graph()
A:torch.ao.quantization.fx.convert.remove_env[node.name]->Graph().node_copy(node, load_arg_remove)
A:torch.ao.quantization.fx.convert.preserved_attributes->set(convert_custom_config_dict.get('preserved_attributes', []))
A:torch.ao.quantization.fx.convert.model->remove_extra_dequantize(model)
torch.ao.quantization.fx.convert(model:GraphModule,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None,is_standalone_module:bool=False,_remove_qconfig_flag:bool=True,convert_qconfig_dict:Dict[str,Any]=None)->torch.nn.Module
torch.ao.quantization.fx.convert.convert(model:GraphModule,is_reference:bool=False,convert_custom_config_dict:Dict[str,Any]=None,is_standalone_module:bool=False,_remove_qconfig_flag:bool=True,convert_qconfig_dict:Dict[str,Any]=None)->torch.nn.Module
torch.ao.quantization.fx.convert.duplicate_dequantize_node(quantized:QuantizedGraphModule)->QuantizedGraphModule
torch.ao.quantization.fx.convert.fold_weight(quantized:QuantizedGraphModule,node_name_to_scope:Dict[str,Tuple[str,type]])->QuantizedGraphModule
torch.ao.quantization.fx.convert.remove_extra_dequantize(quantized:QuantizedGraphModule)->QuantizedGraphModule
torch.ao.quantization.fx.convert.remove_quant_dequant_pairs(quantized:QuantizedGraphModule)->QuantizedGraphModule
torch.ao.quantization.fx.convert.restore_state(observed:torch.nn.Module)->Tuple[Dict[Pattern, QuantizeHandler], Dict[str, Tuple[str, type]], Dict[str, Any], Set[str]]
torch.ao.quantization.fx.convert.run_weight_observers(observed:GraphModule)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/pattern_utils.py----------------------------------------
A:torch.ao.quantization.fx.pattern_utils.DEFAULT_FUSION_PATTERNS->OrderedDict()
A:torch.ao.quantization.fx.pattern_utils.DEFAULT_QUANTIZATION_PATTERNS->OrderedDict()
A:torch.ao.quantization.fx.pattern_utils.DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP->dict()
A:torch.ao.quantization.fx.pattern_utils.DEFAULT_OUTPUT_OBSERVER_MAP->dict()
A:torch.ao.quantization.fx.pattern_utils.DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP[pattern]->fake_quantize.FixedQParamsFakeQuantize.with_args(observer=fixed_qparams_observer)
torch.ao.quantization.fx.pattern_utils.get_default_fusion_patterns()->Dict[Pattern, QuantizeHandler]
torch.ao.quantization.fx.pattern_utils.get_default_output_activation_post_process_map(is_training)->Dict[Pattern, ObserverBase]
torch.ao.quantization.fx.pattern_utils.get_default_quant_patterns()->Dict[Pattern, QuantizeHandler]
torch.ao.quantization.fx.pattern_utils.register_fusion_pattern(pattern)
torch.ao.quantization.fx.pattern_utils.register_quant_pattern(pattern,fixed_qparams_observer=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/fusion_patterns.py----------------------------------------
A:torch.ao.quantization.fx.fusion_patterns.additional_fuser_method_mapping->fuse_custom_config_dict.get('additional_fuser_method_mapping', {})
A:torch.ao.quantization.fx.fusion_patterns.relu->torch.nn.ReLU()
A:torch.ao.quantization.fx.fusion_patterns.matched_modules->get_modules(matched_node_pattern, [])
A:torch.ao.quantization.fx.fusion_patterns.matched_module_types->get_matched_types(matched_modules)
A:torch.ao.quantization.fx.fusion_patterns.(module_parent_name, module_name)->_parent_name(root_node.target)
A:torch.ao.quantization.fx.fusion_patterns.fuser_method->get_fuser_method_new(matched_module_types, fuser_method_mapping)
A:torch.ao.quantization.fx.fusion_patterns.fused_module->fuser_method(is_qat, *matched_modules)
torch.ao.quantization.fx.fusion_patterns.DefaultFuseHandler(self,quantizer:QuantizerCls,node:Node)
torch.ao.quantization.fx.fusion_patterns.DefaultFuseHandler.__init__(self,quantizer:QuantizerCls,node:Node)
torch.ao.quantization.fx.fusion_patterns.DefaultFuseHandler.fuse(self,quantizer:QuantizerCls,load_arg:Callable,root_node:Node,matched_node_pattern:NodePattern,fuse_custom_config_dict:Dict[str,Any],fuser_method_mapping:Optional[Dict[Pattern,Union[torch.nn.Sequential,Callable]]],is_qat:bool)->Node
torch.ao.quantization.fx.fusion_patterns.FuseHandler(self,quantizer:QuantizerCls,node:Node)
torch.ao.quantization.fx.fusion_patterns.FuseHandler.__init__(self,quantizer:QuantizerCls,node:Node)
torch.ao.quantization.fx.fusion_patterns.FuseHandler.fuse(self,quantizer:QuantizerCls,load_arg:Callable,root_node:Node,matched_node_pattern:NodePattern,fuse_custom_config_dict:Dict[str,Any],fuser_method_mapping:Optional[Dict[Pattern,Union[torch.nn.Sequential,Callable]]],is_qat:bool)->Node


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/backend_config/utils.py----------------------------------------
A:torch.ao.quantization.fx.backend_config.utils.pattern_to_quantize_handlers->dict()
A:torch.ao.quantization.fx.backend_config.utils.pattern_to_quantize_handlers[pattern]->get_quantize_handler_cls(observation_type, dtype_configs)
A:torch.ao.quantization.fx.backend_config.utils.input_type_to_index->config.get('input_type_to_index', {})
A:torch.ao.quantization.fx.backend_config.utils.fusion_pattern_to_fuse_handlers->dict()
A:torch.ao.quantization.fx.backend_config.utils.fusion_pattern_to_fuse_handlers[pattern]->get_fuse_handler_cls()
torch.ao.quantization.fx.backend_config.utils.get_fuser_method_mapping(backend_config_dict:Dict[str,Any])->Dict[Pattern, Union[nn.Sequential, Callable]]
torch.ao.quantization.fx.backend_config.utils.get_fusion_pattern_to_fuse_handler_cls(backend_config_dict:Dict[str,Any])->Dict[Pattern, Callable]
torch.ao.quantization.fx.backend_config.utils.get_module_to_qat_module(backend_config_dict:Dict[str,Any])->Dict[Callable, Callable]
torch.ao.quantization.fx.backend_config.utils.get_pattern_to_dtype_configs(backend_config_dict:Dict[str,Any])->Dict[Pattern, List[Dict[str, torch.dtype]]]
torch.ao.quantization.fx.backend_config.utils.get_pattern_to_input_type_to_index(backend_config_dict:Dict[str,Any])->Dict[Pattern, Dict[str, int]]
torch.ao.quantization.fx.backend_config.utils.get_pattern_to_quantize_handlers(backend_config_dict:Dict[str,Any])->Dict[Pattern, QuantizerCls]
torch.ao.quantization.fx.backend_config.utils.get_quantized_reference_module_mapping(backend_config_dict:Dict[str,Any])->Dict[Callable, Callable]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/backend_config/observation_type.py----------------------------------------
torch.ao.quantization.fx.backend_config.observation_type.ObservationType(Enum)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/backend_config/quantize_handler.py----------------------------------------
torch.ao.quantization.fx.backend_config.quantize_handler.get_quantize_handler_cls(observation_type,dtype_configs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/backend_config/fuse_handler.py----------------------------------------
torch.ao.quantization.fx.backend_config.fuse_handler.get_fuse_handler_cls()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/backend_config/tensorrt.py----------------------------------------
torch.ao.quantization.fx.backend_config.tensorrt.get_tensorrt_backend_config_dict()
torch.ao.quantization.fx.get_tensorrt_backend_config_dict()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/quantization/fx/backend_config/__init__.py----------------------------------------
torch.ao.quantization.fx.backend_config.__init__.validate_backend_config_dict(backend_config_dict)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/sparse/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/sparse/quantized/utils.py----------------------------------------
A:torch.ao.nn.sparse.quantized.utils.rlock->threading.RLock()
torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern(self,row_block_size=1,col_block_size=4)
torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern.__enter__(self)
torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern.__exit__(self,exc_type,exc_value,backtrace)
torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern.__init__(self,row_block_size=1,col_block_size=4)
torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern.block_size()
torch.ao.nn.sparse.quantized.utils.is_valid_linear_block_sparse_pattern(row_block_size,col_block_size)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/sparse/quantized/linear.py----------------------------------------
A:torch.ao.nn.sparse.quantized.linear.wq->torch._empty_affine_quantized([1, 1], scale=1.0, zero_point=0, dtype=torch.qint8)
A:torch.ao.nn.sparse.quantized.linear.self._packed_params->LinearPackedParams(dtype)
A:torch.ao.nn.sparse.quantized.linear.destination[prefix + '_packed_params']->self._weight_bias()
A:torch.ao.nn.sparse.quantized.linear.version->local_metadata.get('version', None)
A:torch.ao.nn.sparse.quantized.linear.self.dtype->state_dict.pop(prefix + 'dtype')
A:torch.ao.nn.sparse.quantized.linear.(weight, bias, row_block_size, col_block_size)->state_dict.pop(prefix + '_packed_params')
A:torch.ao.nn.sparse.quantized.linear.(qweight, bias, row_block_size, col_block_size)->self._weight_bias()
A:torch.ao.nn.sparse.quantized.linear.bias->torch.zeros(self.out_features, dtype=torch.float)
A:torch.ao.nn.sparse.quantized.linear.qweight->_quantize_weight(weight.float(), weight_post_process)
A:torch.ao.nn.sparse.quantized.linear.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.ao.nn.sparse.quantized.linear.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.ao.nn.sparse.quantized.linear.self.scale->float(state_dict[prefix + 'scale'])
A:torch.ao.nn.sparse.quantized.linear.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.ao.nn.sparse.quantized.linear.op_type->int(state_dict[prefix + 'op_type'])
A:torch.ao.nn.sparse.quantized.linear.sparse_block_shape->mod.sparse_params.get('sparse_block_shape', None)
A:torch.ao.nn.sparse.quantized.linear.weight_post_process->mod.qconfig.weight()
A:torch.ao.nn.sparse.quantized.linear.(act_scale, act_zp)->activation_post_process.calculate_qparams()
A:torch.ao.nn.sparse.quantized.linear.(w_sc, w_zp)->mod.qconfig.weight().calculate_qparams()
A:torch.ao.nn.sparse.quantized.linear.qlinear->cls(mod.in_features, mod.out_features, row_block_size, col_block_size, dtype=dtype)
A:torch.ao.nn.sparse.quantized.linear.qlinear.scale->float(act_scale)
A:torch.ao.nn.sparse.quantized.linear.qlinear.zero_point->int(act_zp)
torch.ao.nn.sparse.quantized.Linear(self,in_features,out_features,row_block_size,col_block_size,bias=True,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.Linear.__repr__(self)
torch.ao.nn.sparse.quantized.Linear._get_name(cls)
torch.ao.nn.sparse.quantized.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.nn.sparse.quantized.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.nn.sparse.quantized.Linear._weight_bias(self)
torch.ao.nn.sparse.quantized.Linear.bias(self)
torch.ao.nn.sparse.quantized.Linear.extra_repr(self)
torch.ao.nn.sparse.quantized.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.ao.nn.sparse.quantized.Linear.from_float(cls,mod)
torch.ao.nn.sparse.quantized.Linear.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor],row_block_size:Optional[int],col_block_size:Optional[int])->None
torch.ao.nn.sparse.quantized.Linear.weight(self)
torch.ao.nn.sparse.quantized.LinearPackedParams(self,row_block_size=1,col_block_size=4,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.LinearPackedParams.__getstate__(self)
torch.ao.nn.sparse.quantized.LinearPackedParams.__repr__(self)
torch.ao.nn.sparse.quantized.LinearPackedParams.__setstate__(self,state)
torch.ao.nn.sparse.quantized.LinearPackedParams._get_name(self)
torch.ao.nn.sparse.quantized.LinearPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.nn.sparse.quantized.LinearPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.nn.sparse.quantized.LinearPackedParams._weight_bias(self)
torch.ao.nn.sparse.quantized.LinearPackedParams.forward(self,x)
torch.ao.nn.sparse.quantized.LinearPackedParams.set_weight_bias(self,weight:torch.Tensor,bias:Optional[torch.Tensor],row_block_size:Optional[int],col_block_size:Optional[int])->None
torch.ao.nn.sparse.quantized.linear.Linear(self,in_features,out_features,row_block_size,col_block_size,bias=True,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.linear.Linear.__init__(self,in_features,out_features,row_block_size,col_block_size,bias=True,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.linear.Linear.__repr__(self)
torch.ao.nn.sparse.quantized.linear.Linear._get_name(cls)
torch.ao.nn.sparse.quantized.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.nn.sparse.quantized.linear.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.nn.sparse.quantized.linear.Linear._weight_bias(self)
torch.ao.nn.sparse.quantized.linear.Linear.bias(self)
torch.ao.nn.sparse.quantized.linear.Linear.extra_repr(self)
torch.ao.nn.sparse.quantized.linear.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.ao.nn.sparse.quantized.linear.Linear.from_float(cls,mod)
torch.ao.nn.sparse.quantized.linear.Linear.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor],row_block_size:Optional[int],col_block_size:Optional[int])->None
torch.ao.nn.sparse.quantized.linear.Linear.weight(self)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams(self,row_block_size=1,col_block_size=4,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams.__getstate__(self)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams.__init__(self,row_block_size=1,col_block_size=4,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams.__repr__(self)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams.__setstate__(self,state)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams._get_name(self)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams._weight_bias(self)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams.forward(self,x)
torch.ao.nn.sparse.quantized.linear.LinearPackedParams.set_weight_bias(self,weight:torch.Tensor,bias:Optional[torch.Tensor],row_block_size:Optional[int],col_block_size:Optional[int])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/sparse/quantized/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/sparse/quantized/dynamic/linear.py----------------------------------------
A:torch.ao.nn.sparse.quantized.dynamic.linear.bias->state_dict.pop(prefix + 'bias')
A:torch.ao.nn.sparse.quantized.dynamic.linear.qweight->_quantize_weight(weight.float(), weight_observer)
A:torch.ao.nn.sparse.quantized.dynamic.linear.self._packed_params->torch.ao.nn.sparse.quantized.linear.LinearPackedParams(dtype)
A:torch.ao.nn.sparse.quantized.dynamic.linear.op_type->int(state_dict[prefix + 'op_type'])
A:torch.ao.nn.sparse.quantized.dynamic.linear.version->local_metadata.get('version', None)
A:torch.ao.nn.sparse.quantized.dynamic.linear.weight->state_dict.pop(prefix + 'weight')
A:torch.ao.nn.sparse.quantized.dynamic.linear.weight_observer->torch.ao.quantization.qconfig.default_dynamic_qconfig.weight()
A:torch.ao.nn.sparse.quantized.dynamic.linear.(w_sc, w_zp)->torch.ao.quantization.qconfig.default_dynamic_qconfig.weight().calculate_qparams()
A:torch.ao.nn.sparse.quantized.dynamic.linear.(row_block_size, col_block_size)->torch.ao.nn.sparse.quantized.utils.LinearBlockSparsePattern.block_size()
A:torch.ao.nn.sparse.quantized.dynamic.linear.qlinear->cls(mod.in_features, mod.out_features, row_block_size, col_block_size, dtype=dtype)
torch.ao.nn.sparse.quantized.dynamic.Linear(self,in_features,out_features,row_block_size,col_block_size,bias=True,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.dynamic.Linear.__repr__(self)
torch.ao.nn.sparse.quantized.dynamic.Linear._get_name(self)
torch.ao.nn.sparse.quantized.dynamic.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.nn.sparse.quantized.dynamic.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.nn.sparse.quantized.dynamic.Linear._weight_bias(self)
torch.ao.nn.sparse.quantized.dynamic.Linear.bias(self)
torch.ao.nn.sparse.quantized.dynamic.Linear.extra_repr(self)
torch.ao.nn.sparse.quantized.dynamic.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.ao.nn.sparse.quantized.dynamic.Linear.from_float(cls,mod)
torch.ao.nn.sparse.quantized.dynamic.Linear.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor],row_block_size:Optional[int],col_block_size:Optional[int])->None
torch.ao.nn.sparse.quantized.dynamic.Linear.weight(self)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear(self,in_features,out_features,row_block_size,col_block_size,bias=True,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.__init__(self,in_features,out_features,row_block_size,col_block_size,bias=True,dtype=torch.qint8)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.__repr__(self)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear._get_name(self)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear._weight_bias(self)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.bias(self)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.extra_repr(self)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.from_float(cls,mod)
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor],row_block_size:Optional[int],col_block_size:Optional[int])->None
torch.ao.nn.sparse.quantized.dynamic.linear.Linear.weight(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/ao/nn/sparse/quantized/dynamic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/frontend.py----------------------------------------
A:torch.jit.frontend._identifier_chars->set(string.ascii_lowercase + string.ascii_uppercase + string.digits)
A:torch.jit.frontend.self.error_report->torch._C.ErrorReport(self.source_range)
A:torch.jit.frontend.node_type->type(offending_node)
A:torch.jit.frontend.range_len->len(node_start_tokens.get(node_type, ' '))
A:torch.jit.frontend.source_range->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_range(offending_node.lineno, offending_node.col_offset, offending_node.col_offset + range_len)
A:torch.jit.frontend.feature_name->pretty_node_names.get(node_type, node_type.__name__)
A:torch.jit.frontend.msg->"{} {}aren't supported".format(feature_name, reason + ' ' if reason else '')
A:torch.jit.frontend.props->inspect.getmembers(cls, predicate=lambda m: isinstance(m, property))
A:torch.jit.frontend.unused_properties->getattr(cls, '__jit_unused_properties__', [])
A:torch.jit.frontend.getter->get_jit_def(prop[1].fget, f'__{prop[0]}_getter', self_name=self_name)
A:torch.jit.frontend.methods->inspect.getmembers(cls, predicate=lambda m: (inspect.ismethod(m) or inspect.isfunction(m)) and (not is_static_fn(cls, m.__name__)) and (m.__name__ in cls.__dict__))
A:torch.jit.frontend.properties->get_class_properties(cls, self_name)
A:torch.jit.frontend.(sourcelines, file_lineno, filename)->get_source_lines_and_file(cls, torch._C.ErrorReport.call_stack())
A:torch.jit.frontend.source->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).source.encode('utf-8')
A:torch.jit.frontend.dedent_src->dedent(source)
A:torch.jit.frontend.py_ast->ast.parse(dedent_src)
A:torch.jit.frontend.ctx->make_source_context(source, filename, file_lineno, leading_whitespace_len, False)
A:torch.jit.frontend.assigns->get_class_assigns(ctx, class_ast)
A:torch.jit.frontend.parsed_def->parse_def(fn)
A:torch.jit.frontend.type_line->torch.jit.annotations.get_type_line(parsed_def.source)
A:torch.jit.frontend.unused_fn_def->ast.parse('def unused_fn(self: Any):\n\traise RuntimeError("Cannot call @unused methods")')
A:torch.jit.frontend.type_trace_db->torch.jit._script._get_type_trace_db()
A:torch.jit.frontend.qualname->get_qualified_name(fn)
A:torch.jit.frontend.pdt_arg_types->torch.jit._script._get_type_trace_db().get_args_types(qualname)
A:torch.jit.frontend.method->getattr(self, 'build_' + node.__class__.__name__, None)
A:torch.jit.frontend.r->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_range(expr.lineno, expr.col_offset, expr.col_offset + 1)
A:torch.jit.frontend.param_list->build_param_list(ctx, py_def.args, self_name, pdt_arg_types)
A:torch.jit.frontend.return_type->build_expr(ctx, py_def.returns)
A:torch.jit.frontend.decl->torch._C.merge_type_from_type_comment(decl, type_comment_decl, is_method)
A:torch.jit.frontend.type_comment_decl->torch._C.parse_type_comment(type_line)
A:torch.jit.frontend.ctx_range->build_expr(ctx, arg).range()
A:torch.jit.frontend.annotation_expr->EmptyTypeAnnotation(r)
A:torch.jit.frontend.InputType->namedtuple('InputType', ['name', 'ann'])
A:torch.jit.frontend.OutputType->namedtuple('OutputType', ['name', 'ann'])
A:torch.jit.frontend.(var_decl_type, var_ann)->var_ann.split(':')
A:torch.jit.frontend.(inputs, outputs)->process_ins_outs(stmt.items[0].context_expr.keywords)
A:torch.jit.frontend.(return_ann, return_stmt)->build_return_ann_stmt(outputs)
A:torch.jit.frontend.assign_str_lhs->build_args(outputs)
A:torch.jit.frontend.signature->inspect.signature(fn)
A:torch.jit.frontend.rhs->build_expr(ctx, expr.right)
A:torch.jit.frontend.sr->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_range(stmt.lineno, start, end)
A:torch.jit.frontend.lhs->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.the_type->build_expr(ctx, stmt.annotation)
A:torch.jit.frontend.expr->build_expr(ctx, stmt.exc)
A:torch.jit.frontend.test->build_expr(ctx, stmt.test)
A:torch.jit.frontend.op->type(op_)
A:torch.jit.frontend.assign_ast->build_ignore_context_manager(ctx, stmt)
A:torch.jit.frontend.base->build_expr(ctx, expr.value)
A:torch.jit.frontend.name_range->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_raw_range(start_pos, end_pos)
A:torch.jit.frontend.func->build_expr(ctx, expr.func)
A:torch.jit.frontend.stararg_expr->build_expr(ctx, expr.starargs)
A:torch.jit.frontend.kw_expr->build_expr(ctx, kw.value)
A:torch.jit.frontend.err_range->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_raw_range(sub_exprs[0].range().end, sub_exprs[1].range().start)
A:torch.jit.frontend.op_token->ExprBuilder.cmpop_map.get(op)
A:torch.jit.frontend.sub_expr->build_expr(ctx, expr.operand)
A:torch.jit.frontend.in_expr->BinOp('in', lhs, rhs)
A:torch.jit.frontend.cmp_expr->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.result->BinOp('and', result, cmp_expr)
A:torch.jit.frontend.sub_type->type(expr.slice)
A:torch.jit.frontend.tup->TupleLiteral(r, [])
A:torch.jit.frontend.range->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_range(expr.lineno, expr.col_offset, expr.col_offset + 1)
A:torch.jit.frontend.value->str(expr.s)
A:torch.jit.frontend.error_range->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).make_range(expr.lineno, expr.col_offset, expr.col_offset + len(str(value)))
A:torch.jit.frontend.elt_expr->build_expr(ctx, stmt.elt)
A:torch.jit.frontend.target_expr->build_expr(ctx, stmt.generators[0].target)
A:torch.jit.frontend.iter_expr->build_expr(ctx, stmt.generators[0].iter)
A:torch.jit.frontend.key_expr->build_expr(ctx, stmt.key)
A:torch.jit.frontend.value_expr->build_expr(ctx, stmt.value)
A:torch.jit.frontend.build_expr->ExprBuilder()
A:torch.jit.frontend.build_stmt->StmtBuilder()
A:torch.jit.frontend.build_withitem->WithItemBuilder()
A:torch.jit.frontend.new_pos->make_source_context(source, filename, file_lineno, leading_whitespace_len, False).source[:pos].rindex(substr)
torch.jit.frontend.Builder(self,ctx,node)
torch.jit.frontend.Builder.__call__(self,ctx,node)
torch.jit.frontend.ExprBuilder(Builder)
torch.jit.frontend.ExprBuilder.build_Attribute(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BinOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BoolOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Call(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Compare(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Constant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Dict(ctx,expr)
torch.jit.frontend.ExprBuilder.build_DictComp(ctx,stmt)
torch.jit.frontend.ExprBuilder.build_Ellipsis(ctx,expr)
torch.jit.frontend.ExprBuilder.build_IfExp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_JoinedStr(ctx,expr)
torch.jit.frontend.ExprBuilder.build_List(ctx,expr)
torch.jit.frontend.ExprBuilder.build_ListComp(ctx,stmt)
torch.jit.frontend.ExprBuilder.build_Name(ctx,expr)
torch.jit.frontend.ExprBuilder.build_NameConstant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Num(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Starred(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Str(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Subscript(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Tuple(ctx,expr)
torch.jit.frontend.ExprBuilder.build_UnaryOp(ctx,expr)
torch.jit.frontend.FrontendError(self,source_range,msg)
torch.jit.frontend.FrontendError.__init__(self,source_range,msg)
torch.jit.frontend.FrontendError.__str__(self)
torch.jit.frontend.FrontendTypeError(FrontendError)
torch.jit.frontend.NotSupportedError(FrontendError)
torch.jit.frontend.StmtBuilder(Builder)
torch.jit.frontend.StmtBuilder.build_AnnAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assert(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_AugAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Break(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Continue(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Delete(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Expr(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_For(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_If(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Pass(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Print(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Raise(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Return(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_While(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_With(ctx,stmt)
torch.jit.frontend.UnsupportedNodeError(self,ctx,offending_node,reason='')
torch.jit.frontend.UnsupportedNodeError.__init__(self,ctx,offending_node,reason='')
torch.jit.frontend.WithItemBuilder(Builder)
torch.jit.frontend.WithItemBuilder.build_withitem(ctx,item)
torch.jit.frontend.build_class_def(ctx,py_def,methods,properties,self_name,assigns)
torch.jit.frontend.build_def(ctx,py_def,type_line,def_name,self_name=None,pdt_arg_types=None)
torch.jit.frontend.build_ignore_context_manager(ctx,stmt)
torch.jit.frontend.build_param(ctx,py_arg,self_name,kwarg_only,pdt_arg_type=None)
torch.jit.frontend.build_param_list(ctx,py_args,self_name,pdt_arg_types=None)
torch.jit.frontend.build_stmts(ctx,stmts)
torch.jit.frontend.build_withitems(ctx,items)
torch.jit.frontend.find_before(ctx,pos,substr,offsets=(0,0))
torch.jit.frontend.get_class_assigns(ctx,cls_ast)
torch.jit.frontend.get_class_properties(cls,self_name)
torch.jit.frontend.get_default_args(fn)
torch.jit.frontend.get_default_args_for_class(cls)
torch.jit.frontend.get_jit_class_def(cls,self_name)
torch.jit.frontend.get_jit_def(fn,def_name,self_name=None,is_classmethod=False)
torch.jit.frontend.is_reserved_name(name)
torch.jit.frontend.is_torch_jit_ignore_context_manager(stmt)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_trace.py----------------------------------------
A:torch.jit._trace.frame->inspect.currentframe()
A:torch.jit._trace.state_dict->make_module(mod, _module_class, _compilation_unit).state_dict(keep_vars=True)
A:torch.jit._trace.filtered_dict->type(state_dict)()
A:torch.jit._trace.filtered_dict[k]->a.detach().clone(memory_format=None if a.is_mkldnn else torch.preserve_format).requires_grad_(a.requires_grad).detach()
A:torch.jit._trace.(in_vars, in_desc)->_flatten(args)
A:torch.jit._trace.module_state->list(_unique_state_dict(self, keep_vars=True).values())
A:torch.jit._trace.trace_inputs->_unflatten(in_args, in_desc)
A:torch.jit._trace.(out_vars, _)->_flatten(out)
A:torch.jit._trace.(graph, out)->torch._C._create_graph_by_tracing(wrapper, in_vars + module_state, _create_interpreter_name_lookup_fn(), self.strict, self._force_outplace)
A:torch.jit._trace.v->a.detach().clone(memory_format=None if a.is_mkldnn else torch.preserve_format).requires_grad_(a.requires_grad)
A:torch.jit._trace.v.grad->clone_input(v.grad)
A:torch.jit._trace._JIT_TIME->os.environ.get('PYTORCH_JIT_TIME', False)
A:torch.jit._trace._JIT_DISABLE->os.environ.get('PYTORCH_JIT_DISABLE', False)
A:torch.jit._trace._JIT_STATS->os.environ.get('PYTORCH_JIT_STATS', False)
A:torch.jit._trace.stream->torch.cuda.current_stream()
A:torch.jit._trace.start->torch.cuda.Event(enable_timing=True)
A:torch.jit._trace.end->torch.cuda.Event(enable_timing=True)
A:torch.jit._trace.is_module->isinstance(model, Module)
A:torch.jit._trace.saved_args->_clone_inputs(args)
A:torch.jit._trace.saved_state->copy.deepcopy(model.state_dict())
A:torch.jit._trace.(in_vars, _)->_flatten((args, params))
A:torch.jit._trace.out->model(*args)
A:torch.jit._trace.loss->loss_fn(*out)
A:torch.jit._trace.grads->torch.autograd.grad([loss], in_vars)
A:torch.jit._trace.(uncompiled_outs, uncompiled_grads)->run_fwd_bwd(args, force_trace=True)
A:torch.jit._trace.(compiled_outs, compiled_grads)->run_fwd_bwd(args, assert_compiled=True)
A:torch.jit._trace.copied_dict[name]->_clone_inputs(data)
A:torch.jit._trace.check_mod->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, strict=strict, _force_outplace=force_outplace, _module_class=_module_class)
A:torch.jit._trace.check_mod_func->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, strict=strict, _force_outplace=force_outplace, _module_class=_module_class)._c._get_method(traced_func.name)
A:torch.jit._trace.mod_canonicalized->torch._C._jit_pass_canonicalize(traced_func.graph)
A:torch.jit._trace.mod_str->re.sub('___torch_mangle_[0-9]+\\.', '', mod_str)
A:torch.jit._trace.check_canonicalized->torch._C._jit_pass_canonicalize(check_mod_func.graph)
A:torch.jit._trace.check_str->re.sub('___torch_mangle_[0-9]+\\.', '', check_str)
A:torch.jit._trace.graph_diff->difflib.ndiff(mod_str.splitlines(True), check_str.splitlines(True))
A:torch.jit._trace.node_diff->difflib.ndiff(str(n_mod).splitlines(True), str(n_check).splitlines(True))
A:torch.jit._trace.mod_stack->n_mod.sourceRange()
A:torch.jit._trace.check_stack->n_check.sourceRange()
A:torch.jit._trace.mod_tensor_val->n_mod.t('value')
A:torch.jit._trace.check_tensor_val->n_check.t('value')
A:torch.jit._trace.compare_stack->n_mod.sourceRange()
A:torch.jit._trace.outs->ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
A:torch.jit._trace.(graph_diff_errors, tensor_compare_errors)->graph_diagnostic_info()
A:torch.jit._trace.orig->orig.to_dense().to_dense()
A:torch.jit._trace.ref->ref.to_dense().to_dense()
A:torch.jit._trace.traced_outs->run_mod_and_filter_tensor_outputs(traced_func, inputs, 'trace')
A:torch.jit._trace.fn_outs->run_mod_and_filter_tensor_outputs(func, inputs, 'Python function')
A:torch.jit._trace.check_outs->run_mod_and_filter_tensor_outputs(check_mod_func, inputs, 'repeated trace')
A:torch.jit._trace.diag_info->graph_diagnostic_info()
A:torch.jit._trace.example_inputs->make_tuple(example_inputs)
A:torch.jit._trace.var_lookup_fn->_create_interpreter_name_lookup_fn(0)
A:torch.jit._trace.name->_qualified_name(func)
A:torch.jit._trace.traced->torch._C._create_function_from_trace(name, func, example_inputs, var_lookup_fn, strict, _force_outplace, get_callable_argument_names(func))
A:torch.jit._trace.module->make_module(mod, _module_class, _compilation_unit)
A:torch.jit._trace.forward_method->getattr(mod, method_name)
A:torch.jit._trace.argument_names->get_callable_argument_names(func)
A:torch.jit._trace.func->getattr(mod, method_name)
A:torch.jit._trace.check_trace_method->make_module(mod, _module_class, _compilation_unit)._c._get_method(method_name)
A:torch.jit._trace.id_set->set()
A:torch.jit._trace.QualnameWrapper._jit_override_qualname->torch._jit_internal._qualified_name(type(orig))
A:torch.jit._trace.tmp_module->QualnameWrapper()
A:torch.jit._trace.tmp_module._modules[name]->make_module(submodule, TracedModule, _compilation_unit=None)
A:torch.jit._trace.script_module->torch.jit._recursive.create_script_module(tmp_module, lambda module: (), share_types=False, is_tracing=True)
A:torch.jit._trace.forward->_CachedForward()
A:torch.jit._trace.compiled_fn->script(wrapper.__original_fn)
torch.jit.ONNXTracedModule(self,inner,strict=True,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit.ONNXTracedModule.forward(self,*args:torch.Tensor)
torch.jit.TopLevelTracedModule(TracedModule)
torch.jit.TopLevelTracedModule._reconstruct(self,cpp_module)
torch.jit.TracedModule(self,orig,id_set=None,_compilation_unit=None)
torch.jit.TracedModule.__getattr__(self,attr)
torch.jit.TracedModule.__setattr__(self,attr,value)
torch.jit.TracedModule._get_name(self)
torch.jit.TracedModule.extra_repr(self)
torch.jit.TracedModule.forward(self,*args,**kwargs)
torch.jit.TracerWarning(Warning)
torch.jit.TracerWarning.ignore_lib_warnings()
torch.jit.TracingCheckError(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit._get_trace_graph(f,args=(),kwargs=None,strict=True,_force_outplace=False,return_inputs=False,_return_inputs_states=False)
torch.jit._script_if_tracing(fn)
torch.jit._trace.ONNXTracedModule(self,inner,strict=True,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit._trace.ONNXTracedModule.__init__(self,inner,strict=True,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit._trace.ONNXTracedModule.forward(self,*args:torch.Tensor)
torch.jit._trace.TopLevelTracedModule(TracedModule)
torch.jit._trace.TopLevelTracedModule._reconstruct(self,cpp_module)
torch.jit._trace.TracedModule(self,orig,id_set=None,_compilation_unit=None)
torch.jit._trace.TracedModule.__getattr__(self,attr)
torch.jit._trace.TracedModule.__init__(self,orig,id_set=None,_compilation_unit=None)
torch.jit._trace.TracedModule.__setattr__(self,attr,value)
torch.jit._trace.TracedModule._get_name(self)
torch.jit._trace.TracedModule.extra_repr(self)
torch.jit._trace.TracedModule.forward(self,*args,**kwargs)
torch.jit._trace.TracerWarning(Warning)
torch.jit._trace.TracerWarning.ignore_lib_warnings()
torch.jit._trace.TracingCheckError(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit._trace.TracingCheckError.__init__(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit._trace._check_trace(check_inputs,func,traced_func,check_tolerance,strict,force_outplace,is_trace_module,_module_class)
torch.jit._trace._clone_inputs(args)
torch.jit._trace._create_interpreter_name_lookup_fn(frames_up=1)
torch.jit._trace._get_trace_graph(f,args=(),kwargs=None,strict=True,_force_outplace=False,return_inputs=False,_return_inputs_states=False)
torch.jit._trace._script_if_tracing(fn)
torch.jit._trace._time(trace_name,name,time=True)
torch.jit._trace._unique_state_dict(module,keep_vars=False)
torch.jit._trace._verify_equal(xs,ys)
torch.jit._trace.indent(s)
torch.jit._trace.is_tracing()
torch.jit._trace.make_module(mod,_module_class,_compilation_unit)
torch.jit._trace.make_tuple(example_inputs)
torch.jit._trace.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,strict=True,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit._trace.trace_module(mod,inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,strict=True,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit._trace.verify(model,args,loss_fn=torch.sum,devices=None)
torch.jit._trace.wrap_check_inputs(check_inputs)
torch.jit._unique_state_dict(module,keep_vars=False)
torch.jit.is_tracing()
torch.jit.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,strict=True,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit.trace_module(mod,inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,strict=True,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/annotations.py----------------------------------------
A:torch.jit.annotations.signature->parse_type_line(type_line, rcb, loc)
A:torch.jit.annotations.source->dedent(''.join(get_source_lines_and_file(fn)[0]))
A:torch.jit.annotations.type_line->get_type_line(source)
A:torch.jit.annotations.fn->inspect.unwrap(fn)
A:torch.jit.annotations.py_ast->ast.parse(source)
A:torch.jit.annotations.(arg_ann_str, ret_ann_str)->split_type_line(type_line)
A:torch.jit.annotations.arg_ann->eval(arg_ann_str, {}, EvalEnv(rcb))
A:torch.jit.annotations.ret_ann->eval(ret_ann_str, {}, EvalEnv(rcb))
A:torch.jit.annotations.lines->dedent(''.join(get_source_lines_and_file(fn)[0])).split('\n')
A:torch.jit.annotations.type_lines->list(filter(lambda line: not type_pattern.search(line[1]), type_lines))
A:torch.jit.annotations.type_pattern->re.compile('# type:\\ ignore(\\[[a-zA-Z-]+\\])?$')
A:torch.jit.annotations.wrong_type_pattern->re.compile('#[\t ]*type[\t ]*(?!: ignore(\\[.*\\])?$):')
A:torch.jit.annotations.wrong_type_lines->list(filter(lambda line: wrong_type_pattern.search(line[1]), lines))
A:torch.jit.annotations.types->map(get_parameter_type, parameter_type_lines)
A:torch.jit.annotations.parameter_types->', '.join(types)
A:torch.jit.annotations.start_offset->len('# type:')
A:torch.jit.annotations.arrow_pos->get_type_line(source).index('->')
A:torch.jit.annotations.sig->inspect.signature(fn)
A:torch.jit.annotations.return_type->ann_to_type(sig.return_annotation, loc)
A:torch.jit.annotations.elem_type->try_ann_to_type(ann.__args__[0], loc)
A:torch.jit.annotations.key->try_ann_to_type(ann.__args__[0], loc)
A:torch.jit.annotations.value->try_ann_to_type(ann.__args__[1], loc)
A:torch.jit.annotations.valid_type->try_ann_to_type(contained, loc)
A:torch.jit.annotations.maybe_type->try_ann_to_type(a, loc)
A:torch.jit.annotations.scripted_class->torch.jit._script._recursive_compile_class(ann, loc)
A:torch.jit.annotations.name->_qualified_name(ann)
A:torch.jit.annotations.maybe_script_class->_get_script_class(ann)
A:torch.jit.annotations.the_type->try_ann_to_type(ann, loc)
torch.jit.annotations.EvalEnv(self,rcb)
torch.jit.annotations.EvalEnv.__getitem__(self,name)
torch.jit.annotations.EvalEnv.__init__(self,rcb)
torch.jit.annotations.Module(self,name,members)
torch.jit.annotations.Module.__getattr__(self,name)
torch.jit.annotations.Module.__init__(self,name,members)
torch.jit.annotations.ann_to_type(ann,loc)
torch.jit.annotations.check_fn(fn,loc)
torch.jit.annotations.get_enum_value_type(e:Type[enum.Enum],loc)
torch.jit.annotations.get_param_names(fn,n_args)
torch.jit.annotations.get_signature(fn,rcb,loc,is_method)
torch.jit.annotations.get_type_line(source)
torch.jit.annotations.is_function_or_method(the_callable)
torch.jit.annotations.is_tensor(ann)
torch.jit.annotations.is_vararg(the_callable)
torch.jit.annotations.parse_type_line(type_line,rcb,loc)
torch.jit.annotations.split_type_line(type_line)
torch.jit.annotations.try_ann_to_type(ann,loc)
torch.jit.annotations.try_real_annotations(fn,loc)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_script.py----------------------------------------
A:torch.jit._script.type_trace_db->JitTypeTraceStore()
A:torch.jit._script.Attribute->collections.namedtuple('Attribute', ['value', 'type'])
A:torch.jit._script.r->self._python_modules.items()
A:torch.jit._script.cls._constants_set->type(module)._constants_set.union(base_constants)
A:torch.jit._script.base_constants->getattr(base, '_constants_set', set())
A:torch.jit._script.original_init->getattr(cls, '__init__', lambda self: None)
A:torch.jit._script.num_methods->len(cls._methods)
A:torch.jit._script.cls->type(module)
A:torch.jit._script.self.__dict__['_actual_script_module']->torch.jit._recursive.create_script_module(self, make_stubs, share_types=not added_methods_in_init)
A:torch.jit._script._rcb->torch._jit_internal.createResolutionCallbackFromClosure(impl_fn)
A:torch.jit._script.ast->get_jit_class_def(obj, obj.__name__)
A:torch.jit._script.cu->torch._C.CompilationUnit()
A:torch.jit._script.cpp_module->torch._C._import_ir_module_from_package(cu, importer.zip_reader, importer.storage_context, validate_map_location(importer.last_map_location), script_module_id)
A:torch.jit._script.self_method->getattr(self, method_name)
A:torch.jit._script.forward->_CachedForward()
A:torch.jit._script.rcb->torch._jit_internal.createResolutionCallbackForClassMethods(obj)
A:torch.jit._script.self._methods[ast.name().name]->ScriptMethodStub(rcb, ast, None)
A:torch.jit._script.script_module_id->exporter.get_unique_id()
A:torch.jit._script.script_module->RecursiveScriptModule(cpp_module)
A:torch.jit._script.script_module._parameters->OrderedDictWrapper(torch._C.ParameterDict(script_module._c))
A:torch.jit._script.script_module._buffers->OrderedDictWrapper(torch._C.BufferDict(script_module._c))
A:torch.jit._script.script_module._modules->OrderedModuleDict(script_module._c, script_module._modules)
A:torch.jit._script.self._concrete_type->torch._C.ConcreteModuleType.from_jit_type(self._c._type())
A:torch.jit._script.modules[name]->wrap_cpp_module(cpp_module)
A:torch.jit._script.self._modules->OrderedModuleDict(self._c, modules)
A:torch.jit._script.self._parameters->OrderedDictWrapper(torch._C.ParameterDict(self._c))
A:torch.jit._script.self._buffers->OrderedDictWrapper(torch._C.BufferDict(self._c))
A:torch.jit._script.script_method->self._c._get_method(attr)
A:torch.jit._script.obj_id->id(obj)
A:torch.jit._script.sub_module[k]->call_prepare_scriptable_func_impl(v, memo)
A:torch.jit._script.new_obj_dict[name]->call_prepare_scriptable_func_impl(sub_module, memo)
A:torch.jit._script.monkeytype_config->JitTypeTraceConfig(type_trace_db)
A:torch.jit._script.obj->call_prepare_scriptable_func(obj)
A:torch.jit._script.qualified_name->_qualified_name(obj)
A:torch.jit._script.maybe_already_compiled_fn->_try_get_jit_cached_function(obj)
A:torch.jit._script.fn->torch._C._jit_script_compile_overload(qual_name, overload_decl, impl_ast, _rcb, implementation_defaults, overload_signature)
A:torch.jit._script.overload_decl->get_jit_def(overload_fn, overload_fn.__name__).decl()
A:torch.jit._script.overload_signature->torch.jit.annotations.get_signature(overload_fn, None, None, inspect.ismethod(overload_fn))
A:torch.jit._script.impl_ast->get_jit_def(impl_fn, impl_fn.__name__)
A:torch.jit._script.overload_defaults->get_default_args(overload_fn)
A:torch.jit._script.implementation_defaults->get_default_args(impl_fn)
A:torch.jit._script.existing_compiled_fns->_try_get_jit_cached_overloads(obj)
A:torch.jit._script.qual_name->_qualified_name(obj)
A:torch.jit._script.uncompiled_overloads->torch._jit_internal._get_fn_overloads(qual_name)
A:torch.jit._script.mangled_classname->torch._C._jit_script_interface_compile(qualified_name, ast, rcb, is_module_interface)
A:torch.jit._script._qual_name->_qualified_name(obj)
A:torch.jit._script.error_stack->torch._C.CallStack(_qual_name, loc)
A:torch.jit._script.max_length->max(len(cell), max_length)
A:torch.jit._script.cell->rows.get(line)
A:torch.jit._script.(header, rows)->col.materialize()
A:torch.jit._script.self.profile->torch._classes.classes.profiling._ScriptProfile()
A:torch.jit._script.source_ref->source_stats.source()
A:torch.jit._script.source_lines->source_stats.source().text().splitlines()
A:torch.jit._script.dedent->min([len(line) - len(line.lstrip(' ')) for line in source_lines])
A:torch.jit._script.start_line->source_stats.source().starting_lineno()
A:torch.jit._script.source_range->range(start_line, end_line)
A:torch.jit._script.lineno->_ScriptProfileColumn('Line #')
A:torch.jit._script.hits->_ScriptProfileColumn('Hits')
A:torch.jit._script.time_ns->_ScriptProfileColumn('Time (ns)')
A:torch.jit._script.line_contents->_ScriptProfileColumn('Line Contents', 0, 1)
A:torch.jit._script.stats->source_stats.line_map()
A:torch.jit._script.stat->source_stats.line_map().get(line)
A:torch.jit._script.table->_ScriptProfileTable([lineno, hits, time_ns, line_contents], list(source_range))
torch.jit.ScriptWarning(Warning)
torch.jit._ScriptProfile(self)
torch.jit._ScriptProfile.disable(self)
torch.jit._ScriptProfile.dump(self)
torch.jit._ScriptProfile.dump_string(self)->str
torch.jit._ScriptProfile.enable(self)
torch.jit._ScriptProfileColumn(self,header:str,alignment:int=4,offset:int=0)
torch.jit._ScriptProfileColumn.add_row(self,lineno:int,value:Any)
torch.jit._ScriptProfileColumn.materialize(self)
torch.jit._ScriptProfileTable(self,cols:List[_ScriptProfileColumn],source_range:List[int])
torch.jit._ScriptProfileTable.dump_string(self)
torch.jit._script.ConstMap(self,const_mapping)
torch.jit._script.ConstMap.__getattr__(self,attr)
torch.jit._script.ConstMap.__init__(self,const_mapping)
torch.jit._script.OrderedDictWrapper(self,_c)
torch.jit._script.OrderedDictWrapper.__contains__(self,k)
torch.jit._script.OrderedDictWrapper.__delitem__(self,k)
torch.jit._script.OrderedDictWrapper.__getitem__(self,k)
torch.jit._script.OrderedDictWrapper.__init__(self,_c)
torch.jit._script.OrderedDictWrapper.__len__(self)
torch.jit._script.OrderedDictWrapper.__setitem__(self,k,v)
torch.jit._script.OrderedDictWrapper.items(self)
torch.jit._script.OrderedDictWrapper.keys(self)
torch.jit._script.OrderedDictWrapper.values(self)
torch.jit._script.OrderedModuleDict(self,module,python_dict)
torch.jit._script.OrderedModuleDict.__contains__(self,k)
torch.jit._script.OrderedModuleDict.__getitem__(self,k)
torch.jit._script.OrderedModuleDict.__init__(self,module,python_dict)
torch.jit._script.OrderedModuleDict.__setitem__(self,k,v)
torch.jit._script.OrderedModuleDict.items(self)
torch.jit._script.ScriptMeta(cls,name,bases,attrs)
torch.jit._script.ScriptMeta.__init__(cls,name,bases,attrs)
torch.jit._script.ScriptWarning(Warning)
torch.jit._script._CachedForward(object)
torch.jit._script._CachedForward.__get__(self,obj,cls)
torch.jit._script._ScriptProfile(self)
torch.jit._script._ScriptProfile.__init__(self)
torch.jit._script._ScriptProfile.disable(self)
torch.jit._script._ScriptProfile.dump(self)
torch.jit._script._ScriptProfile.dump_string(self)->str
torch.jit._script._ScriptProfile.enable(self)
torch.jit._script._ScriptProfileColumn(self,header:str,alignment:int=4,offset:int=0)
torch.jit._script._ScriptProfileColumn.__init__(self,header:str,alignment:int=4,offset:int=0)
torch.jit._script._ScriptProfileColumn.add_row(self,lineno:int,value:Any)
torch.jit._script._ScriptProfileColumn.materialize(self)
torch.jit._script._ScriptProfileTable(self,cols:List[_ScriptProfileColumn],source_range:List[int])
torch.jit._script._ScriptProfileTable.__init__(self,cols:List[_ScriptProfileColumn],source_range:List[int])
torch.jit._script._ScriptProfileTable.dump_string(self)
torch.jit._script._check_directly_compile_overloaded(obj)
torch.jit._script._check_overload_defaults(impl_defaults,overload_defaults,loc)
torch.jit._script._compile_function_with_overload(overload_fn,qual_name,impl_fn)
torch.jit._script._get_function_from_type(cls,name)
torch.jit._script._get_overloads(obj)
torch.jit._script._get_type_trace_db()
torch.jit._script._is_new_style_class(cls)
torch.jit._script._recursive_compile_class(obj,loc)
torch.jit._script._reduce(cls)
torch.jit._script._unwrap_optional(x)
torch.jit._script.call_prepare_scriptable_func(obj)
torch.jit._script.call_prepare_scriptable_func_impl(obj,memo)
torch.jit._script.create_script_dict(obj)
torch.jit._script.create_script_list(obj,type_hint=None)
torch.jit._script.interface(obj)
torch.jit._script.pad(s:str,padding:int,offset:int=0,char:str='')
torch.jit._script.script(obj,optimize=None,_frames_up=0,_rcb=None,example_inputs:Union[List[Tuple],Dict[Callable,List[Tuple]],None]=None)
torch.jit._script.script_method(fn)
torch.jit._script.unpackage_script_module(importer:PackageImporter,script_module_id:str)->torch.nn.Module
torch.jit._unwrap_optional(x)
torch.jit.interface(obj)
torch.jit.script(obj,optimize=None,_frames_up=0,_rcb=None,example_inputs:Union[List[Tuple],Dict[Callable,List[Tuple]],None]=None)
torch.jit.script_method(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_async.py----------------------------------------
torch.jit._async.fork(func,*args,**kwargs)
torch.jit._async.wait(future)
torch.jit.fork(func,*args,**kwargs)
torch.jit.wait(future)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_recursive.py----------------------------------------
A:torch.jit._recursive.ScriptMethodStub->collections.namedtuple('ScriptMethodStub', ('resolution_callback', 'def_', 'original_method'))
A:torch.jit._recursive.PropertyStub->collections.namedtuple('PropertyStub', ('resolution_callback', 'def_'))
A:torch.jit._recursive.script_class->torch._C._jit_script_class_compile(qualified_name, ast, defaults, rcb)
A:torch.jit._recursive.ast->get_jit_def(func, name, self_name='RecursiveScriptModule')
A:torch.jit._recursive.defaults->torch.jit.frontend.get_default_args_for_class(obj)
A:torch.jit._recursive.rcb->torch._jit_internal.createResolutionCallbackFromClosure(fn)
A:torch.jit._recursive.func->getattr(nn_module, method_name)
A:torch.jit._recursive.item->getattr(orig_class, name, None)
A:torch.jit._recursive.user_annotated_ignored_attributes->getattr(nn_module, '__jit_ignored_attributes__', list())
A:torch.jit._recursive.properties->get_properties_names(type(module))
A:torch.jit._recursive.user_annoted_ignored_properties->set()
A:torch.jit._recursive.constants->', '.join((torch.typename(typ) for typ in _constant_types))
A:torch.jit._recursive.concrete_type_builder->infer_concrete_type_builder(nn_module, share_types)
A:torch.jit._recursive.class_annotations->getattr(nn_module, '__annotations__', {})
A:torch.jit._recursive.ignored_properties->jit_ignored_properties(nn_module)
A:torch.jit._recursive.ann_to_type->torch.jit.annotations.ann_to_type(item.type, fake_range())
A:torch.jit._recursive.attr_type->torch._C._jit_try_infer_type(item)
A:torch.jit._recursive.added_names->set()
A:torch.jit._recursive.(attr_type, _)->infer_type(name, item)
A:torch.jit._recursive.sub_concrete_type->get_module_concrete_type(item, share_types)
A:torch.jit._recursive.constants_set->set(getattr(nn_module, '__constants__', ()))
A:torch.jit._recursive.value->getattr(nn_module, name)
A:torch.jit._recursive.overloads->getattr(nn_module, '__overloads__', {})
A:torch.jit._recursive.isoverloadpacket->isinstance(value, torch._ops.OpOverloadPacket)
A:torch.jit._recursive.scripted_fn->torch.jit.script(value)
A:torch.jit._recursive.hint->'(This function exists as an attribute on the Python module, but we failed to compile it to a TorchScript function. \nThe error stack is reproduced here:\n{}'.format(e)
A:torch.jit._recursive.builtin_symbol_name->_find_builtin(value)
A:torch.jit._recursive.(attr_type, inferred)->infer_type(name, value)
A:torch.jit._recursive.self.methods_compiled->set()
A:torch.jit._recursive.nn_module_type->type(nn_module)
A:torch.jit._recursive.concrete_type->get_module_concrete_type(nn_module, share_types)
A:torch.jit._recursive.concrete_type_store->ConcreteTypeStore()
A:torch.jit._recursive.qualified_class_name->torch._jit_internal._qualified_name(type(obj))
A:torch.jit._recursive.class_ty->torch.jit._state._python_cu.get_class(qualified_class_name)
A:torch.jit._recursive.cpp_object->torch._C._create_object_with_type(class_ty)
A:torch.jit._recursive.cpp_module->torch._C._create_module_with_type(concrete_type.jit_type)
A:torch.jit._recursive.method_stubs->stubs_fn(nn_module)
A:torch.jit._recursive.property_stubs->get_property_stubs(nn_module)
A:torch.jit._recursive.(hook_stubs, pre_hook_stubs)->get_hook_stubs(nn_module)
A:torch.jit._recursive.orig_value->getattr(nn_module, name)
A:torch.jit._recursive.scripted->create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)
A:torch.jit._recursive.bound_method->unbound_function.__get__(script_module)
A:torch.jit._recursive.script_module->torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
A:torch.jit._recursive.keys->repr(list(nn_module.keys()))
A:torch.jit._recursive.script_method->torch._C._create_module_with_type(concrete_type.jit_type)._get_method(name)
A:torch.jit._recursive.wrapped_script_method->functools.wraps(method_stub.original_method)(script_method)
A:torch.jit._recursive.fget->torch._C._create_module_with_type(concrete_type.jit_type)._get_method(property_stub.def_.getter_name().name)
A:torch.jit._recursive.setter_name->property_stub.def_.setter_name()
A:torch.jit._recursive.script_module.__dict__[property_name]->property(property_name, fget, fset)
A:torch.jit._recursive.script_attr->getattr(script_model, attr, None)
A:torch.jit._recursive.default_attr->getattr(torch.jit.RecursiveScriptModule, attr, None)
A:torch.jit._recursive.method_overloads->torch._jit_internal._get_overloaded_methods(item, mod.__class__)
A:torch.jit._recursive.overloads[item]->list(zip(names, method_overloads))
A:torch.jit._recursive.signature->torch.jit.annotations.get_signature(func, None, fake_range(), inspect.ismethod(func))
A:torch.jit._recursive.qual_name->torch._jit_internal._qualified_name(func)
A:torch.jit._recursive.orig_ast->get_jit_def(orig_fn, orig_fn.__name__, self_name='RecursiveScriptModule')
A:torch.jit._recursive.over_ast->get_jit_def(overload_fn, overload_fn.__name__, self_name='RecursiveScriptModule')
A:torch.jit._recursive.new_ast->torch._C._replace_overloaded_method_decl(over_ast.decl(), orig_ast, overload_name)
A:torch.jit._recursive._rcb->torch._jit_internal.createResolutionCallbackFromClosure(orig_fn)
A:torch.jit._recursive.forward_func->getattr(nn_module.forward, '__func__', None)
A:torch.jit._recursive.module_forward->getattr(torch.nn.Module, 'forward', None)
A:torch.jit._recursive.overload_name_mappings->dict(getattr(nn_module, '__overloads__', {}))
A:torch.jit._recursive.overload_info->get_overload_annotations(nn_module, ignored_properties)
A:torch.jit._recursive.overload_stubs->make_stubs_for_overloads(overload_info)
A:torch.jit._recursive.filtered_methods->filter(ignore_overloaded, methods)
A:torch.jit._recursive.module_ty->type(nn_module)
A:torch.jit._recursive.properties_asts->get_class_properties(module_ty, self_name='RecursiveScriptModule')
A:torch.jit._recursive.rcbs[name]->torch._jit_internal.createResolutionCallbackFromClosure(item.fget)
A:torch.jit._recursive.script_module._concrete_type->torch._C.ConcreteModuleType.from_jit_type(script_module._c._type())
A:torch.jit._recursive.stub->make_stub(fn, fn.__name__)
A:torch.jit._recursive.method->types.MethodType(unbound_method, script_module)
torch.jit._recursive.ConcreteTypeStore(self)
torch.jit._recursive.ConcreteTypeStore.__init__(self)
torch.jit._recursive.ConcreteTypeStore.get_or_create_concrete_type(self,nn_module)
torch.jit._recursive.SourceContext(self,source,filename,file_lineno,leading_whitespace_len)
torch.jit._recursive.SourceContext.__init__(self,source,filename,file_lineno,leading_whitespace_len)
torch.jit._recursive._check_no_signature(func)
torch.jit._recursive._compile_and_register_class(obj,rcb,qualified_name)
torch.jit._recursive._get_valid_constant(attr,v,owner_type)
torch.jit._recursive.add_python_attr_to_scripted_model(script_model,orig,attr)
torch.jit._recursive.check_module_initialized(mod)
torch.jit._recursive.compile_unbound_method(concrete_type,fn)
torch.jit._recursive.create_hooks_from_stubs(concrete_type,hook_stubs,pre_hook_stubs)
torch.jit._recursive.create_methods_and_properties_from_stubs(concrete_type,method_stubs,property_stubs)
torch.jit._recursive.create_script_class(obj)
torch.jit._recursive.create_script_module(nn_module,stubs_fn,share_types=True,is_tracing=False)
torch.jit._recursive.create_script_module_impl(nn_module,concrete_type,stubs_fn)
torch.jit._recursive.get_hook_stubs(nn_module)
torch.jit._recursive.get_module_concrete_type(nn_module,share_types=True)
torch.jit._recursive.get_overload_annotations(mod,jit_ignored_properties)
torch.jit._recursive.get_overload_name_mapping(overload_info)
torch.jit._recursive.get_property_stubs(nn_module)
torch.jit._recursive.infer_concrete_type_builder(nn_module,share_types=True)
torch.jit._recursive.infer_methods_to_compile(nn_module)
torch.jit._recursive.interface_script(mod_interface,nn_module)
torch.jit._recursive.jit_ignored_properties(module)
torch.jit._recursive.lazy_bind(concrete_type,unbound_method)
torch.jit._recursive.make_stub(func,name)
torch.jit._recursive.make_stub_from_method(nn_module,method_name)
torch.jit._recursive.make_stubs_for_overloads(overload_info)
torch.jit._recursive.make_stubs_from_exported_methods(mod)
torch.jit._recursive.script_model_defines_attr(script_model,attr)
torch.jit._recursive.try_compile_fn(fn,loc)
torch.jit._recursive.wrap_cpp_class(cpp_class)
torch.jit._recursive.wrap_cpp_module(cpp_module)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/supported_ops.py----------------------------------------
A:torch.jit.supported_ops.v->'\n{}{}'.format(' ' * indent, v)
A:torch.jit.supported_ops.qualified_name->'{}.{}'.format(mod, name)
A:torch.jit.supported_ops.schema_str->_emit_schema(mod.__name__, fn.__name__, schema)
A:torch.jit.supported_ops.schemas->torch._C._jit_get_schemas_for_operator(op_name)
A:torch.jit.supported_ops.attr->getattr(mod, elem)
A:torch.jit.supported_ops.attr_module->inspect.getmodule(attr)
A:torch.jit.supported_ops.scripted->torch.jit.script(attr)
A:torch.jit.supported_ops.builtin->_find_builtin(fn)
A:torch.jit.supported_ops.mod->inspect.getmodule(fn)
A:torch.jit.supported_ops.builtins->filter(lambda fn: _is_math_fn(fn[0]), _get_builtins_helper())
A:torch.jit.supported_ops.builtins_list->list(builtins)
A:torch.jit.supported_ops.op_name->'aten::{}'.format(fn)
A:torch.jit.supported_ops.table_row->'":any:`{}`", "{}"'.format(fn, schemaless_op_explanations[fn])
A:torch.jit.supported_ops.schematized_ops_str->textwrap.indent(schematized_ops_str, '\t')
A:torch.jit.supported_ops.schemaless_ops_str->textwrap.indent(schemaless_ops_str, '\t')
A:torch.jit.supported_ops.magic_methods_rows_str->textwrap.indent(magic_methods_rows_str, '\t')
A:torch.jit.supported_ops.section->'{}\n{}\n{}'.format(header, '~' * len(header), emit_block(items))
A:torch.jit.supported_ops.(header, items)->fn()
A:torch.jit.supported_ops.link_target->header.replace('`', '').replace('-', '').lower().replace(' ', '-')
A:torch.jit.supported_ops.__doc__->_list_supported_ops()
torch.jit.supported_ops._emit_arg(indent,i,arg)
torch.jit.supported_ops._emit_args(indent,arguments)
torch.jit.supported_ops._emit_ret(ret)
torch.jit.supported_ops._emit_rets(returns)
torch.jit.supported_ops._emit_schema(mod,name,schema,arg_start=0,padding=4)
torch.jit.supported_ops._emit_type(type)
torch.jit.supported_ops._get_builtins_helper()
torch.jit.supported_ops._get_global_builtins()
torch.jit.supported_ops._get_math_builtins()
torch.jit.supported_ops._get_nn_functional_ops()
torch.jit.supported_ops._get_tensor_ops()
torch.jit.supported_ops._get_torchscript_builtins()
torch.jit.supported_ops._hidden(name)
torch.jit.supported_ops._is_math_fn(fn)
torch.jit.supported_ops._list_supported_ops()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_state.py----------------------------------------
A:torch.jit._state.self.enabled->self.parse_env('PYTORCH_JIT', True, '> Using PyTorch JIT', '> PyTorch JIT DISABLED')
A:torch.jit._state.value->os.environ.get(name)
A:torch.jit._state._enabled->EnabledProxy()
A:torch.jit._state._python_cu->torch._C.CompilationUnit()
A:torch.jit._state.override->getattr(python_class, '_jit_override_qualname', None)
A:torch.jit._state.python_class->_get_python_class(override)
A:torch.jit._state.qual_names->_jit_function_overload_caching.get(key, None)
A:torch.jit._state.qual_name->_jit_caching_layer.get(key, None)
torch.jit._state.EnabledProxy(self)
torch.jit._state.EnabledProxy.__bool__(self)
torch.jit._state.EnabledProxy.__init__(self)
torch.jit._state.EnabledProxy.parse_env(self,name,default,true_message,false_message)
torch.jit._state._add_script_class(python_class,script_class)
torch.jit._state._clear_class_state()
torch.jit._state._get_python_class(qualified_name)
torch.jit._state._get_script_class(python_class)
torch.jit._state._set_jit_function_cache(key,value)
torch.jit._state._set_jit_overload_cache(key,compiled_fns)
torch.jit._state._try_get_jit_cached_function(key)
torch.jit._state._try_get_jit_cached_overloads(key)
torch.jit._state.disable()
torch.jit._state.enable()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_pickle.py----------------------------------------
torch.jit._pickle.build_boollist(data)
torch.jit._pickle.build_doublelist(data)
torch.jit._pickle.build_intlist(data)
torch.jit._pickle.build_tensor_from_id(data)
torch.jit._pickle.build_tensorlist(data)
torch.jit._pickle.restore_type_tag(value,type_str)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_check.py----------------------------------------
A:torch.jit._check.source_lines->'\n'.join([l for l in source_lines.split('\n') if not is_useless_comment(l)])
A:torch.jit._check.line->line.strip().strip()
A:torch.jit._check.init_ast->ast.parse(textwrap.dedent(source_lines))
A:torch.jit._check.self.class_level_annotations->list(nn_module.__annotations__.keys())
torch.jit._check.AttributeTypeIsSupportedChecker(ast.NodeVisitor)
torch.jit._check.AttributeTypeIsSupportedChecker._is_empty_container(self,node:ast.AST,ann_type:str)->bool
torch.jit._check.AttributeTypeIsSupportedChecker.check(self,nn_module:torch.nn.Module)->None
torch.jit._check.AttributeTypeIsSupportedChecker.visit_AnnAssign(self,node)
torch.jit._check.AttributeTypeIsSupportedChecker.visit_Assign(self,node)
torch.jit._check.AttributeTypeIsSupportedChecker.visit_Call(self,node)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_freeze.py----------------------------------------
A:torch.jit._freeze.out->RecursiveScriptModule(torch._C._freeze_module(mod._c, preserved_attrs))
A:torch.jit._freeze.mod->freeze(mod.eval(), preserved_attrs=other_methods)
torch.jit._freeze.freeze(mod,preserved_attrs:Optional[List[str]]=None,optimize_numerics:bool=True)
torch.jit._freeze.optimize_for_inference(mod:ScriptModule,other_methods:Optional[List[str]]=None)->ScriptModule
torch.jit._freeze.run_frozen_optimizations(mod,optimize_numerics:bool=True,preserved_methods:Optional[List[str]]=None)
torch.jit.freeze(mod,preserved_attrs:Optional[List[str]]=None,optimize_numerics:bool=True)
torch.jit.optimize_for_inference(mod:ScriptModule,other_methods:Optional[List[str]]=None)->ScriptModule
torch.jit.run_frozen_optimizations(mod,optimize_numerics:bool=True,preserved_methods:Optional[List[str]]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/quantized.py----------------------------------------
A:torch.jit.quantized.(self.weight, self.col_offsets, self.scale, self.zero_point)->torch.fbgemm_linear_quantize_weight(other.weight.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.self.weight->torch.fbgemm_pack_gemm_matrix_fp16(other.weight.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.self.col_offsets->torch.nn.Parameter(self.col_offsets, requires_grad=False)
A:torch.jit.quantized.self.bias->torch.nn.Parameter(other.bias.clone(memory_format=torch.contiguous_format).float(), requires_grad=False)
A:torch.jit.quantized.out->torch.fbgemm_linear_fp16_weight_fp32_activation(input.float(), self.packed_weight, self.bias)
A:torch.jit.quantized.repr->'in_features={in_features}, out_features={out_features}, '.format(**self.__dict__)
A:torch.jit.quantized.(weight_ih, col_offsets_ih, self.scale_ih, self.zero_point_ih)->torch.fbgemm_linear_quantize_weight(other.weight_ih.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.(weight_hh, col_offsets_hh, self.scale_hh, self.zero_point_hh)->torch.fbgemm_linear_quantize_weight(other.weight_hh.clone(memory_format=torch.contiguous_format).float())
A:torch.jit.quantized.packed_ih->torch.ops.quantized.linear_prepack_fp16(weight_ih.float(), bias_ih)
A:torch.jit.quantized.packed_hh->torch.ops.quantized.linear_prepack_fp16(weight_hh.float(), bias_hh)
A:torch.jit.quantized.self.bias_ih->torch.nn.Parameter(other.bias_ih.clone(memory_format=torch.contiguous_format).float(), requires_grad=False)
A:torch.jit.quantized.self.bias_hh->torch.nn.Parameter(other.bias_hh.clone(memory_format=torch.contiguous_format).float(), requires_grad=False)
A:torch.jit.quantized.hx->self.permute_hidden(hx, sorted_indices)
A:torch.jit.quantized.ret->torch._VF.quantized_rnn_relu_cell(input, hx, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh, self.packed_ih, self.packed_hh, self.col_offsets_ih, self.col_offsets_hh, self.scale_ih, self.scale_hh, self.zero_point_ih, self.zero_point_hh)
A:torch.jit.quantized.zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.jit.quantized.weight_name->'weight_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.jit.quantized.bias_name->'bias_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.jit.quantized.weight->getattr(other, weight_name)
A:torch.jit.quantized.bias->getattr(other, bias_name)
A:torch.jit.quantized.(weight_ih, bias_ih)->get_weight_bias('ih')
A:torch.jit.quantized.(weight_hh, bias_hh)->get_weight_bias('hh')
A:torch.jit.quantized.cell_params->torch.ops.quantized.make_quantized_cell_params_fp16(packed_ih, packed_hh)
A:torch.jit.quantized.mini_batch->int(batch_sizes[0])
A:torch.jit.quantized.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.jit.quantized.result->torch.quantized_gru(input, batch_sizes, hx, self.all_weights, self.bias, self.num_layers, float(self.dropout), self.training, self.bidirectional)
A:torch.jit.quantized.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.jit.quantized.max_batch_size->int(max_batch_size)
A:torch.jit.quantized.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.jit.quantized.new_mod->quantize_rnn_modules(mod, dtype)
torch.jit.quantized.QuantizedGRU(QuantizedRNNBase)
torch.jit.quantized.QuantizedGRU.forward(self,input,hx=None)
torch.jit.quantized.QuantizedGRU.forward_impl(self,input:Tensor,hx:Optional[Tensor],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.jit.quantized.QuantizedGRU.forward_packed(self,input:PackedSequence,hx:Optional[Tensor]=None)->Tuple[PackedSequence, Tensor]
torch.jit.quantized.QuantizedGRU.forward_tensor(self,input:Tensor,hx:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.jit.quantized.QuantizedGRUCell(self,other)
torch.jit.quantized.QuantizedGRUCell.__init__(self,other)
torch.jit.quantized.QuantizedGRUCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.jit.quantized.QuantizedLSTM(self,other,dtype)
torch.jit.quantized.QuantizedLSTM.__init__(self,other,dtype)
torch.jit.quantized.QuantizedLSTM.check_forward_args(self,input:Tensor,hidden:Tuple[Tensor,Tensor],batch_sizes:Optional[Tensor])->None
torch.jit.quantized.QuantizedLSTM.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.forward_impl(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.jit.quantized.QuantizedLSTM.forward_packed(self,input:PackedSequence,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[PackedSequence, Tuple[Tensor, Tensor]]
torch.jit.quantized.QuantizedLSTM.forward_tensor(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.jit.quantized.QuantizedLSTM.permute_hidden(self,hx:Tuple[Tensor,Tensor],permutation:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.jit.quantized.QuantizedLSTMCell(self,other)
torch.jit.quantized.QuantizedLSTMCell.__init__(self,other)
torch.jit.quantized.QuantizedLSTMCell.forward(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.jit.quantized.QuantizedLinear(self,other)
torch.jit.quantized.QuantizedLinear.__init__(self,other)
torch.jit.quantized.QuantizedLinear._pack(self)
torch.jit.quantized.QuantizedLinear._unpack(self)
torch.jit.quantized.QuantizedLinear.extra_repr(self)
torch.jit.quantized.QuantizedLinear.forward(self,input)
torch.jit.quantized.QuantizedLinearFP16(self,other)
torch.jit.quantized.QuantizedLinearFP16.__init__(self,other)
torch.jit.quantized.QuantizedLinearFP16._pack(self)
torch.jit.quantized.QuantizedLinearFP16._unpack(self)
torch.jit.quantized.QuantizedLinearFP16.extra_repr(self)
torch.jit.quantized.QuantizedLinearFP16.forward(self,input)
torch.jit.quantized.QuantizedRNNBase(self,other,dtype=torch.int8)
torch.jit.quantized.QuantizedRNNBase.__init__(self,other,dtype=torch.int8)
torch.jit.quantized.QuantizedRNNBase.check_forward_args(self,input:Tensor,hidden:Tensor,batch_sizes:Optional[Tensor])->None
torch.jit.quantized.QuantizedRNNBase.check_hidden_size(self,hx:Tensor,expected_hidden_size:Tuple[int,int,int],msg:str='Expectedhiddensize{},got{}')->None
torch.jit.quantized.QuantizedRNNBase.check_input(self,input:Tensor,batch_sizes:Optional[Tensor])->None
torch.jit.quantized.QuantizedRNNBase.get_expected_hidden_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.jit.quantized.QuantizedRNNBase.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])->Tensor
torch.jit.quantized.QuantizedRNNCell(self,other)
torch.jit.quantized.QuantizedRNNCell.__init__(self,other)
torch.jit.quantized.QuantizedRNNCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.jit.quantized.QuantizedRNNCellBase(self,other)
torch.jit.quantized.QuantizedRNNCellBase.__init__(self,other)
torch.jit.quantized.QuantizedRNNCellBase._pack(self)
torch.jit.quantized.QuantizedRNNCellBase._unpack(self)
torch.jit.quantized.QuantizedRNNCellBase.check_forward_hidden(self,input:Tensor,hx:Tensor,hidden_label:str='')->None
torch.jit.quantized.QuantizedRNNCellBase.check_forward_input(self,input)
torch.jit.quantized.QuantizedRNNCellBase.extra_repr(self)
torch.jit.quantized.apply_permutation(tensor:Tensor,permutation:Tensor,dim:int=1)->Tensor
torch.jit.quantized.quantize_linear_modules(module,dtype=torch.int8)
torch.jit.quantized.quantize_rnn_cell_modules(module)
torch.jit.quantized.quantize_rnn_modules(module,dtype=torch.int8)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/unsupported_tensor_ops.py----------------------------------------
A:torch.jit.unsupported_tensor_ops.tensor_attrs->set(filter(lambda x: x[0] != '_', dir(torch.Tensor)))
A:torch.jit.unsupported_tensor_ops.tensor->torch.tensor([2])
A:torch.jit.unsupported_tensor_ops.funcs_template->dedent('\n    def func(x):\n        return x.{op}()\n    ')
A:torch.jit.unsupported_tensor_ops.deprecated_apis->set(['volatile', 'resize', 'reinforce', 'new', 'name', 'map2_', 'has_names', 'grad_fn', 'resize_as'])
A:torch.jit.unsupported_tensor_ops.sorted_tensor_attrs->sorted(list(tensor_attrs), key=lambda x: x.lower())
A:torch.jit.unsupported_tensor_ops.funcs_str->dedent('\n    def func(x):\n        return x.{op}()\n    ').format(op=attr)
A:torch.jit.unsupported_tensor_ops.cu->torch.jit.CompilationUnit(funcs_str)
A:torch.jit.unsupported_tensor_ops.attr_repr->repr(getattr(tensor, attr))
A:torch.jit.unsupported_tensor_ops.(methods, properties)->_gen_unsupported_methods_properties()
A:torch.jit.unsupported_tensor_ops.__doc__->_list_unsupported_tensor_ops()
torch.jit.unsupported_tensor_ops._gen_unsupported_methods_properties()
torch.jit.unsupported_tensor_ops._list_unsupported_tensor_ops()
torch.jit.unsupported_tensor_ops.execWrapper(code,glob,loc)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_serialization.py----------------------------------------
A:torch.jit._serialization.ret->m.save_to_buffer(_extra_files=_extra_files)
A:torch.jit._serialization.map_location->torch.device(map_location)
A:torch.jit._serialization.cu->torch._C.CompilationUnit()
A:torch.jit._serialization.cpp_module->torch._C.import_ir_module_from_buffer(cu, f.read(), map_location, _extra_files)
torch.jit._serialization.load(f,map_location=None,_extra_files=None)
torch.jit._serialization.save(m,f,_extra_files=None)
torch.jit._serialization.validate_map_location(map_location=None)
torch.jit.load(f,map_location=None,_extra_files=None)
torch.jit.save(m,f,_extra_files=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/__init__.py----------------------------------------
torch.jit.__init__._hide_source_ranges()->Iterator[None]
torch.jit.__init__.annotate(the_type,the_value)
torch.jit.__init__.export_opnames(m)
torch.jit.__init__.isinstance(obj,target_type)
torch.jit.__init__.script_if_tracing(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_monkeytype_config.py----------------------------------------
A:torch.jit._monkeytype_config.parent_modules->cls.__module__.split('.')
A:torch.jit._monkeytype_config.root_module->sys.modules.get(parent_modules[0])
A:torch.jit._monkeytype_config.type_to_string->str(type)
A:torch.jit._monkeytype_config.elem_type->get_type(elem_type)
A:torch.jit._monkeytype_config.qualified_name->get_qualified_name(t.func)
A:torch.jit._monkeytype_config.all_args->self.analyze(qualified_name)
A:torch.jit._monkeytype_config.types->list(types)
A:torch.jit._monkeytype_config.type_length->len(types)
A:torch.jit._monkeytype_config.all_args[arg]->get_type(types[0])
A:torch.jit._monkeytype_config.filename->pathlib.Path(code.co_filename).resolve()
torch.jit._monkeytype_config.get_optional_of_element_type(types)
torch.jit._monkeytype_config.get_qualified_name(func)
torch.jit._monkeytype_config.get_type(type)
torch.jit._monkeytype_config.is_torch_native_class(cls)
torch.jit._monkeytype_config.jit_code_filter(code:CodeType)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/generate_bytecode.py----------------------------------------
A:torch.jit.generate_bytecode.content->listify(content)
A:torch.jit.generate_bytecode.upgraders_graph_map->_generate_upgraders_graph()
A:torch.jit.generate_bytecode.bytecode_table->_compile_graph_to_code_table(upgrader_name, upgrader_graph)
torch.jit.generate_bytecode.format_bytecode(table)
torch.jit.generate_bytecode.generate_upgraders_bytecode()->List


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_builtins.py----------------------------------------
A:torch.jit._builtins._functional_registered_ops->_gen_torch_functional_registered_ops()
A:torch.jit._builtins.v->getattr(mod, name)
torch.jit._builtins._find_builtin(fn)
torch.jit._builtins._gen_torch_functional_registered_ops()
torch.jit._builtins._get_builtin_table()
torch.jit._builtins._is_special_functional_bound_op(fn)
torch.jit._builtins._register_builtin(fn,op)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_logging.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_fuser.py----------------------------------------
A:torch.jit._fuser.stored_flag->torch._C._get_graph_executor_optimize()
A:torch.jit._fuser.old_cpu_fuse->torch._C._jit_can_fuse_on_cpu()
A:torch.jit._fuser.old_gpu_fuse->torch._C._jit_can_fuse_on_gpu()
A:torch.jit._fuser.old_texpr_fuser_state->torch._C._jit_texpr_fuser_enabled()
A:torch.jit._fuser.old_nvfuser_state->torch._C._jit_nvfuser_enabled()
A:torch.jit._fuser.old_profiling_executor->torch._C._jit_set_profiling_executor(True)
A:torch.jit._fuser.old_profiling_mode->torch._C._jit_set_profiling_mode(True)
A:torch.jit._fuser.dbs->parent.get_debug_state()
A:torch.jit._fuser.eps->list(dbs.execution_plans.values())
A:torch.jit._fuser.graph->eps[0].graph.copy()
A:torch.jit._fuser.fw_states->eps[0].code.differentiable_op_executor_states()
A:torch.jit._fuser.fw_execution_plans->list(state.execution_plans.values())
torch.jit._fuser._get_differentiable_graph_node(node,diff_node)
torch.jit._fuser._graph_for(self,*args,**kwargs)
torch.jit._fuser._script_method_graph_for(self,parent,*args,**kwargs)
torch.jit._fuser.fuser(name)
torch.jit._fuser.optimized_execution(should_optimize)
torch.jit._fuser.set_fusion_strategy(strategy:List[Tuple[str,int]])
torch.jit.fuser(name)
torch.jit.optimized_execution(should_optimize)
torch.jit.set_fusion_strategy(strategy:List[Tuple[str,int]])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_passes/_property_propagation.py----------------------------------------
A:torch.jit._passes._property_propagation.graph_inputs->list(graph.inputs())
torch.jit._passes._property_propagation.apply_input_props_using_example(graph:Graph,example_input:List[Any])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/_passes/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/jit/mobile/__init__.py----------------------------------------
A:torch.jit.mobile.__init__.map_location->validate_map_location(map_location)
A:torch.jit.mobile.__init__.cpp_module->torch._C._load_for_lite_interpreter_from_buffer(f.read(), map_location)
torch.jit.mobile.__init__.LiteScriptModule(self,cpp_module)
torch.jit.mobile.__init__.LiteScriptModule.__init__(self,cpp_module)
torch.jit.mobile.__init__.LiteScriptModule.find_method(self,method_name)
torch.jit.mobile.__init__.LiteScriptModule.forward(self,*input)
torch.jit.mobile.__init__.LiteScriptModule.run_method(self,method_name,*input)
torch.jit.mobile.__init__._backport_for_mobile(f_input,f_output,to_version)
torch.jit.mobile.__init__._backport_for_mobile_to_buffer(f_input,to_version)
torch.jit.mobile.__init__._export_operator_list(module:LiteScriptModule)
torch.jit.mobile.__init__._get_mobile_model_contained_types(f_input)->int
torch.jit.mobile.__init__._get_model_bytecode_version(f_input)->int
torch.jit.mobile.__init__._get_model_ops_and_info(f_input)
torch.jit.mobile.__init__._load_for_lite_interpreter(f,map_location=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset8.py----------------------------------------
A:torch.onnx.symbolic_opset8.vars()[block_listed_op]->_block_list_in_opset(block_listed_op)
A:torch.onnx.symbolic_opset8.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset8.align_corners->torch.onnx.symbolic_helper._maybe_get_const(align_corners, 'b')
A:torch.onnx.symbolic_opset8.output_size->torch.onnx.symbolic_helper._maybe_get_const(output_size, 'is')
A:torch.onnx.symbolic_opset8.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset8.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset8.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset8.(scales, mode)->torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g, input, size, scale_factor, mode, align_corners)
A:torch.onnx.symbolic_opset8.arg0_type->args[0].type().scalarType()
A:torch.onnx.symbolic_opset8.args->tuple((_cast_Float(g, arg, False) for arg in args))
A:torch.onnx.symbolic_opset8.other->torch.onnx.symbolic_helper._if_scalar_type_as(g, other, input)
A:torch.onnx.symbolic_opset8.(_, input, other)->_try_cast_integer_to_float(g, input, other)
A:torch.onnx.symbolic_opset8.(old_type, self, other)->_try_cast_integer_to_float(g, self, other)
A:torch.onnx.symbolic_opset8.self_rank->torch.onnx.symbolic_helper._get_tensor_rank(self)
A:torch.onnx.symbolic_opset8.weight->g.op('Unsqueeze', weight, axes_i=list(range(1, self_rank - 1)))
A:torch.onnx.symbolic_opset8.(old_type, self, weight)->_try_cast_integer_to_float(g, self, weight)
A:torch.onnx.symbolic_opset8.ty->torch.onnx.symbolic_helper._try_get_scalar_type(self, other).lower()
A:torch.onnx.symbolic_opset8.C->g.constant(0, [1], ty)
A:torch.onnx.symbolic_opset8.(old_type, self, other, C)->_try_cast_integer_to_float(g, self, other, C)
A:torch.onnx.symbolic_opset8.(old_type, self, mat1, mat2)->_try_cast_integer_to_float(g, self, mat1, mat2)
A:torch.onnx.symbolic_opset8.start_dim_i->torch.onnx.symbolic_helper._get_const(start_dim, 'i', 'start_dim')
A:torch.onnx.symbolic_opset8.end_dim_i->torch.onnx.symbolic_helper._get_const(end_dim, 'i', 'end_dim')
A:torch.onnx.symbolic_opset8.dim->input.type().dim()
A:torch.onnx.symbolic_opset8.(old_type, input)->_try_cast_integer_to_float(g, input)
A:torch.onnx.symbolic_opset8.result->g.op('ConstantFill', sizes, dtype_i=sym_help.cast_pytorch_to_onnx['Float'], input_as_shape_i=1, value_f=const_value)
A:torch.onnx.symbolic_opset8.shape->g.op('Shape', input)
A:torch.onnx.symbolic_opset8.const_value->torch.onnx.symbolic_helper._maybe_get_const(value, 't')
A:torch.onnx.symbolic_opset8.tmp->zeros(g, sizes, dtype, layout, device)
A:torch.onnx.symbolic_opset8.dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset8.repeats->g.op('Constant', value_t=torch.LongTensor(repeats))
A:torch.onnx.symbolic_opset8.repeat_size_len->len(const_repeats)
A:torch.onnx.symbolic_opset8.const_repeats->torch.onnx.symbolic_helper._maybe_get_const(repeats, 'is')
A:torch.onnx.symbolic_opset8.sizes->torch.onnx.symbolic_opset9.view(g, self, g.op('Constant', value_t=torch.tensor([1] * diff_dims + sizes))).type().sizes()
A:torch.onnx.symbolic_opset8.self->torch.onnx.symbolic_opset9.view(g, self, g.op('Constant', value_t=torch.tensor([1] * diff_dims + sizes)))
torch.onnx.symbolic_opset8.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor,antialias)
torch.onnx.symbolic_opset8._cast_to_type(g,input,to_type)
torch.onnx.symbolic_opset8._comparison_operator(g,input,other,op_name)
torch.onnx.symbolic_opset8._constant_fill(g,sizes,dtype,const_value)
torch.onnx.symbolic_opset8._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset8._try_cast_integer_to_float(g,*args)
torch.onnx.symbolic_opset8.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic_opset8.bmm(g,self,other)
torch.onnx.symbolic_opset8.empty(g,sizes,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.empty_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset8.full(g,sizes,value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.full_like(g,input,fill_value,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.gt(g,input,other)
torch.onnx.symbolic_opset8.lt(g,input,other)
torch.onnx.symbolic_opset8.matmul(g,self,other)
torch.onnx.symbolic_opset8.mm(g,self,other)
torch.onnx.symbolic_opset8.ones(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.ones_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.prelu(g,self,weight)
torch.onnx.symbolic_opset8.repeat(g,self,repeats)
torch.onnx.symbolic_opset8.zeros(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.zeros_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/utils.py----------------------------------------
A:torch.onnx.utils.output_type->identity_node.output().output().type()
A:torch.onnx.utils.input->g.insertConstant(val)
A:torch.onnx.utils.lc->g.create('prim::ListConstruct', inputs).insertBefore(node).output().setType(ListType.ofTensors())
A:torch.onnx.utils.graph->_optimize_graph(graph, operator_export_type, _disable_torch_constant_prop=_disable_torch_constant_prop, fixed_batch_size=fixed_batch_size, params_dict=params_dict, dynamic_axes=dynamic_axes, input_names=input_names, module=module)
A:torch.onnx.utils.do_constant_folding->_resolve_args_by_export_type('do_constant_folding', do_constant_folding, operator_export_type)
A:torch.onnx.utils.sig->inspect.signature(model.forward)
A:torch.onnx.utils.ordered_list_keys->list(sig.parameters.keys())
A:torch.onnx.utils.n_nonkeyword->len(args)
A:torch.onnx.utils.args->list((const_if_tensor(arg) for arg in raw_args))
A:torch.onnx.utils.(trace_graph, torch_out, inputs_states)->torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)
A:torch.onnx.utils.trace_graph->_optimize_graph(trace_graph, operator_export_type, params_dict={})
A:torch.onnx.utils.orig_state_dict_keys->_unique_state_dict(model).keys()
A:torch.onnx.utils.(in_vars, _)->torch.jit._flatten(args_params)
A:torch.onnx.utils.freezed_m->torch._C._freeze_module(model._c, preserveParameters=True)
A:torch.onnx.utils.(module, params)->torch._C._jit_onnx_list_model_parameters(freezed_m)
A:torch.onnx.utils.param_count_list->_get_param_count_list(graph, args)
A:torch.onnx.utils.(in_vars, in_desc)->torch.jit._flatten(tuple(args))
A:torch.onnx.utils.(graph, torch_out)->_trace_and_get_graph_from_model(model, args)
A:torch.onnx.utils.state_dict->_unique_state_dict(model)
A:torch.onnx.utils.params->list(state_dict.values())
A:torch.onnx.utils.graph_inputs->list(graph.inputs())
A:torch.onnx.utils.param_names->list(state_dict.keys())
A:torch.onnx.utils._params_dict->dict(zip(param_names, params))
A:torch.onnx.utils.input_args->copy.deepcopy(args)
A:torch.onnx.utils.example_outputs->_get_example_outputs(model, args)
A:torch.onnx.utils.(graph, params, torch_out, module)->_create_jit_graph(model, args)
A:torch.onnx.utils.params_dict->torch._C._jit_pass_filter_non_tensor_arguments(params_dict)
A:torch.onnx.utils.(out_vars, desc)->torch.jit._flatten(tuple(example_outputs))
A:torch.onnx.utils.(flatten_args, _)->torch._C._jit_flatten(args)
A:torch.onnx.utils.(output_tensors, out_desc)->torch._C._jit_flatten(tuple(output_wrapped))
A:torch.onnx.utils.val_keep_init_as_ip->_decide_keep_init_as_input(keep_initializers_as_inputs, operator_export_type, opset_version)
A:torch.onnx.utils.val_add_node_names->_decide_add_node_names(add_node_names, operator_export_type)
A:torch.onnx.utils.val_do_constant_folding->_decide_constant_folding(do_constant_folding, operator_export_type, training)
A:torch.onnx.utils.(graph, params_dict, torch_out)->_model_to_graph(model, args, verbose, input_names, output_names, operator_export_type, val_do_constant_folding, fixed_batch_size=fixed_batch_size, training=training, dynamic_axes=dynamic_axes)
A:torch.onnx.utils.unsupported_ops->list()
A:torch.onnx.utils.trace_module_map->__setup_trace_module_map()
A:torch.onnx.utils.export_modules_as_functions->_setup_trace_module_map(model, export_modules_as_functions)
A:torch.onnx.utils.model_file_location->str()
A:torch.onnx.utils.node_attr_to_name->torch._C._jit_pass_onnx_function_extraction(graph, export_modules_as_functions, list(params_dict.keys()))
A:torch.onnx.utils.(proto, export_map, val_use_external_data_format)->_optimize_graph(graph, operator_export_type, _disable_torch_constant_prop=_disable_torch_constant_prop, fixed_batch_size=fixed_batch_size, params_dict=params_dict, dynamic_axes=dynamic_axes, input_names=input_names, module=module)._export_onnx({}, opset_version, dynamic_axes, False, operator_export_type, not verbose, val_keep_init_as_ip, custom_opsets, val_add_node_names, model_file_location, node_attr_to_name)
A:torch.onnx.utils.model_proto_file->os.path.join(f, ONNX_ARCHIVE_MODEL_PROTO_NAME)
A:torch.onnx.utils.weight_proto_file->os.path.join(f, k)
A:torch.onnx.utils.old_name->v.debugName()
A:torch.onnx.utils.params[new_name]->list(state_dict.values()).pop(old_name)
A:torch.onnx.utils.output_node_set->set()
A:torch.onnx.utils.identity_node->_optimize_graph(graph, operator_export_type, _disable_torch_constant_prop=_disable_torch_constant_prop, fixed_batch_size=fixed_batch_size, params_dict=params_dict, dynamic_axes=dynamic_axes, input_names=input_names, module=module).create('onnx::Identity')
A:torch.onnx.utils.node->_optimize_graph(graph, operator_export_type, _disable_torch_constant_prop=_disable_torch_constant_prop, fixed_batch_size=fixed_batch_size, params_dict=params_dict, dynamic_axes=dynamic_axes, input_names=input_names, module=module).create('onnx::Identity').output()
A:torch.onnx.utils.attr_pattern->re.compile('^(.+)_([ifstgz])$')
A:torch.onnx.utils.m->re.compile('^(.+)_([ifstgz])$').match(key)
A:torch.onnx.utils.value->_scalar(value)
A:torch.onnx.utils.aten->dict(((k, v) for (k, v) in kwargs.items() if v is not None)).pop('aten', False)
A:torch.onnx.utils.n->b.addNode(ns_opname, list(args))
A:torch.onnx.utils.outputs->b.addNode(ns_opname, list(args)).outputsSize()
A:torch.onnx.utils.kwargs->dict(((k, v) for (k, v) in kwargs.items() if v is not None))
A:torch.onnx.utils.new_output->block.registerOutput(value)
A:torch.onnx.utils.ns_op_name->b.addNode(ns_opname, list(args)).kind()
A:torch.onnx.utils.(ns, op_name)->get_ns_op_name_from_custom_op(symbolic_name)
A:torch.onnx.utils.is_exportable_aten_op->torch.onnx.symbolic_registry.is_registered_op(op_name, '', opset_version)
A:torch.onnx.utils.symbolic_fn->_find_symbolic_in_registry(domain, op_name, opset_version, operator_export_type)
A:torch.onnx.utils.vals->b.addNode(ns_opname, list(args)).output().toIValue()
A:torch.onnx.utils.input_flag->inputs[0].node()['value'].tolist()
A:torch.onnx.utils.env->torch._C._jit_pass_onnx_block(current_b, block, operator_export_type, env, is_sub_block)
A:torch.onnx.utils.if_output_list->list(n.outputs())
A:torch.onnx.utils.current_b_list->list(current_b.outputs())
A:torch.onnx.utils.new_op_outputs->torch._C._jit_pass_fixup_onnx_controlflow_node(new_node, opset_version)
A:torch.onnx.utils.new_block->new_node.addBlock()
A:torch.onnx.utils.type->type.lower().lower()
A:torch.onnx.utils.tensor->torch.DoubleTensor(*dims)
A:torch.onnx.utils.sel->self.kindOf(k)
A:torch.onnx.utils.valid_names->set((input_names or []) + (output_names or []))
torch.onnx.utils._add_attribute(node,key,value,aten)
torch.onnx.utils._add_block(node)
torch.onnx.utils._add_input_to_block(block)
torch.onnx.utils._add_output_to_block(block,value)
torch.onnx.utils._apply_friendly_debug_names(graph,params)
torch.onnx.utils._block_op(b,opname,*args,**kwargs)
torch.onnx.utils._create_jit_graph(model,args)
torch.onnx.utils._decide_add_node_names(add_node_names,operator_export_type)
torch.onnx.utils._decide_constant_folding(do_constant_folding,operator_export_type,training)
torch.onnx.utils._decide_input_format(model,args)
torch.onnx.utils._decide_keep_init_as_input(keep_initializers_as_inputs,operator_export_type,opset_version)
torch.onnx.utils._export(model,args,f,export_params=True,verbose=False,training=None,input_names=None,output_names=None,operator_export_type=None,export_type=ExportTypes.PROTOBUF_FILE,opset_version=None,do_constant_folding=True,dynamic_axes=None,keep_initializers_as_inputs=None,fixed_batch_size=False,custom_opsets=None,add_node_names=True,onnx_shape_inference=True,export_modules_as_functions=False)
torch.onnx.utils._find_symbolic_in_registry(domain,op_name,opset_version,operator_export_type)
torch.onnx.utils._get_example_outputs(model,args)
torch.onnx.utils._get_named_param_dict(graph,params)
torch.onnx.utils._get_param_count_list(method_graph,args_params)
torch.onnx.utils._graph_at(g,opname,*args,**kwargs)
torch.onnx.utils._graph_constant(g,value,dims,type,*args,**kwargs)
torch.onnx.utils._graph_op(g,opname,*raw_args,**kwargs)
torch.onnx.utils._is_constant_tensor_list(node)
torch.onnx.utils._is_onnx_list(value)
torch.onnx.utils._model_to_graph(model,args,verbose=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,do_constant_folding=True,_disable_torch_constant_prop=False,fixed_batch_size=False,training=None,dynamic_axes=None)
torch.onnx.utils._newNode(g,opname,outputs,*args,**kwargs)
torch.onnx.utils._node_getitem(self,k)
torch.onnx.utils._optimize_graph(graph,operator_export_type,_disable_torch_constant_prop=False,fixed_batch_size=False,params_dict=None,dynamic_axes=None,input_names=None,module=None)
torch.onnx.utils._reset_trace_module_map()
torch.onnx.utils._resolve_args_by_export_type(arg_name,arg_value,operator_export_type)
torch.onnx.utils._run_symbolic_function(g,block,n,inputs,env,operator_export_type=OperatorExportTypes.ONNX)
torch.onnx.utils._run_symbolic_method(g,op_name,symbolic_fn,args)
torch.onnx.utils._scalar(x)
torch.onnx.utils._set_input_and_output_names(graph,input_names,output_names)
torch.onnx.utils._setup_trace_module_map(model,export_modules_as_functions)
torch.onnx.utils._split_tensor_list_constants(g,block)
torch.onnx.utils._trace(func,args,operator_export_type,return_outs=False)
torch.onnx.utils._trace_and_get_graph_from_model(model,args)
torch.onnx.utils._validate_dynamic_axes(dynamic_axes,model,input_names,output_names)
torch.onnx.utils.disable_apex_o2_state_dict_hook(model)
torch.onnx.utils.export(model,args,f,export_params=True,verbose=False,training=None,input_names=None,output_names=None,operator_export_type=None,opset_version=None,do_constant_folding=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,export_modules_as_functions=False)
torch.onnx.utils.export_to_pretty_string(model,args,export_params=True,verbose=False,training=None,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,export_type=ExportTypes.PROTOBUF_FILE,google_printer=False,opset_version=None,keep_initializers_as_inputs=None,custom_opsets=None,add_node_names=True,do_constant_folding=True,dynamic_axes=None)
torch.onnx.utils.exporter_context(model,mode)
torch.onnx.utils.get_ns_op_name_from_custom_op(symbolic_name)
torch.onnx.utils.is_in_onnx_export()
torch.onnx.utils.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)
torch.onnx.utils.select_model_mode_for_export(model,mode)
torch.onnx.utils.unconvertible_ops(model,args,training=TrainingMode.EVAL,opset_version=None)
torch.onnx.utils.unregister_custom_op_symbolic(symbolic_name,opset_version)
torch.onnx.utils.warn_on_static_input_change(input_states)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_helper.py----------------------------------------
A:torch.onnx.symbolic_helper.value_t->_maybe_get_const(value, 't')
A:torch.onnx.symbolic_helper.list_node->list_value.node()
A:torch.onnx.symbolic_helper.sig->inspect.signature(fn)
A:torch.onnx.symbolic_helper.scalar_type->tensor.type().scalarType()
A:torch.onnx.symbolic_helper.ty->tensor.type().scalarType().lower()
A:torch.onnx.symbolic_helper.element_type->str(x.type().getElementType())
A:torch.onnx.symbolic_helper.sizes->_get_tensor_sizes(x)
A:torch.onnx.symbolic_helper.index_const->_maybe_get_scalar(index)
A:torch.onnx.symbolic_helper.index_dim->_get_tensor_rank(index)
A:torch.onnx.symbolic_helper.index->g.op('Cast', index, to_i=cast_pytorch_to_onnx['Long'])
A:torch.onnx.symbolic_helper.index_scalar_type->g.op('Cast', index, to_i=cast_pytorch_to_onnx['Long']).type().scalarType()
A:torch.onnx.symbolic_helper.type->scalar_type_to_pytorch_type.index(torch.get_default_dtype())
A:torch.onnx.symbolic_helper.shape_->g.op('Shape', input)
A:torch.onnx.symbolic_helper.dim_size_->g.op('Gather', shape_, g.op('Constant', value_t=torch.tensor([dim], dtype=torch.int64)))
A:torch.onnx.symbolic_helper.k->_reshape_helper(g, k, g.op('Constant', value_t=torch.tensor([1])))
A:torch.onnx.symbolic_helper.axes->g.op('Constant', value_t=torch.tensor(axes_i, dtype=torch.long))
A:torch.onnx.symbolic_helper.keepdims_i->_maybe_get_const(keepdims_i, 'i')
A:torch.onnx.symbolic_helper.axes_i->g.op('Constant', value_t=torch.tensor(axes_i, dtype=torch.long))
A:torch.onnx.symbolic_helper.output_size->g.op('Concat', input_size_beg, output_size, axis_i=0)
A:torch.onnx.symbolic_helper.offsets->g.op('Constant', value_t=torch.ones(2, dtype=torch.float32))
A:torch.onnx.symbolic_helper.dividend->g.op('Cast', output_size, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.divisor->g.op('Cast', divisor, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.scale_dims->g.op('Div', dividend, divisor)
A:torch.onnx.symbolic_helper.scales->_interpolate_get_scales(g, scale_factor, rank)
A:torch.onnx.symbolic_helper.scales_list->g.op('Constant', value_t=torch.tensor(_maybe_get_const(scales[0], 'fs')))
A:torch.onnx.symbolic_helper.scale_factor_rank->_get_tensor_rank(scale_factor)
A:torch.onnx.symbolic_helper.scale_factor->_interpolate_size_to_scales(g, input, size, dim)
A:torch.onnx.symbolic_helper.mode->_maybe_get_const(mode, 's')
A:torch.onnx.symbolic_helper.align_corners->_maybe_get_const(align_corners, 'b')
A:torch.onnx.symbolic_helper.dim->input.type().dim()
A:torch.onnx.symbolic_helper.size->g.op('Concat', input_size, size, axis_i=0)
A:torch.onnx.symbolic_helper.(scales, align_corners)->_get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_helper.input_size->g.op('Shape', input)
A:torch.onnx.symbolic_helper.input_size_beg->_slice_helper(g, input_size, axes=[0], ends=[2], starts=[0])
A:torch.onnx.symbolic_helper.empty_roi->g.op('Constant', value_t=torch.tensor([], dtype=torch.float32))
A:torch.onnx.symbolic_helper.empty_scales->g.op('Constant', value_t=torch.tensor([], dtype=torch.float32))
A:torch.onnx.symbolic_helper.rank->_get_tensor_rank(self)
A:torch.onnx.symbolic_helper.repeats->g.op('Constant', value_t=torch.tensor([1] * reps))
A:torch.onnx.symbolic_helper.full_shape->g.op('Shape', self)
A:torch.onnx.symbolic_helper.self_dim->self.type().dim()
A:torch.onnx.symbolic_helper.dim_value->_parse_arg(dim, 'i')
A:torch.onnx.symbolic_helper.unsqueezed_index->_unsqueeze_helper(g, index, [i for i in range(self_dim) if i != dim_value])
A:torch.onnx.symbolic_helper.expanded_index_shape->scatter(g, g.op('Shape', self), 0, _unsqueeze_helper(g, dim, [0]), g.op('Shape', index))
A:torch.onnx.symbolic_helper.expanded_index->expand(g, unsqueezed_index, expanded_index_shape, None)
A:torch.onnx.symbolic_helper.shape->g.op('Constant', value_t=torch.LongTensor(shape))
A:torch.onnx.symbolic_helper.batch_size->_get_tensor_dim_size(input, 0)
A:torch.onnx.symbolic_helper.channel_size->_get_tensor_dim_size(input, 1)
A:torch.onnx.symbolic_helper.weight_value->torch.tensor([1.0] * channel_size).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_helper.weight->g.op('Constant', value_t=weight_value)
A:torch.onnx.symbolic_helper.bias_value->torch.tensor([0.0] * channel_size).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_helper.bias->g.op('Constant', value_t=bias_value)
A:torch.onnx.symbolic_helper.reshape_in->_reshape_helper(g, input, g.op('Constant', value_t=torch.tensor([batch_size, channel_size, -1], dtype=torch.int64)))
A:torch.onnx.symbolic_helper.trans_in->g.op('Transpose', reshape_in, perm_i=[0, 2, 1])
A:torch.onnx.symbolic_helper.(running_var, running_mean)->_var_mean(g, trans_in, g.op('Constant', value_t=torch.tensor([0, 1], dtype=torch.int64)), False, False)
A:torch.onnx.symbolic_helper.padding->tuple(tuple_fn(padding))
A:torch.onnx.symbolic_helper.slice1->_slice_helper(g, input_size, axes=[0], starts=[0], ends=[start_dim])
A:torch.onnx.symbolic_helper.slice3->_slice_helper(g, input_size, axes=[0], starts=[end_dim + 1], ends=[dim])
A:torch.onnx.symbolic_helper.final_shape->g.op('Concat', *slices, axis_i=0)
A:torch.onnx.symbolic_helper.n->g.op('prim::Constant')
A:torch.onnx.symbolic_helper._constant_folding_opset_versions->list(range(9, _onnx_main_opset + 1))
A:torch.onnx.symbolic_helper.INT8->enum.auto()
A:torch.onnx.symbolic_helper.SHORT->enum.auto()
A:torch.onnx.symbolic_helper.INT->enum.auto()
A:torch.onnx.symbolic_helper.INT64->enum.auto()
A:torch.onnx.symbolic_helper.HALF->enum.auto()
A:torch.onnx.symbolic_helper.FLOAT->enum.auto()
A:torch.onnx.symbolic_helper.DOUBLE->enum.auto()
A:torch.onnx.symbolic_helper.COMPLEX32->enum.auto()
A:torch.onnx.symbolic_helper.COMPLEX64->enum.auto()
A:torch.onnx.symbolic_helper.COMPLEX128->enum.auto()
A:torch.onnx.symbolic_helper.BOOL->enum.auto()
A:torch.onnx.symbolic_helper.QINT8->enum.auto()
A:torch.onnx.symbolic_helper.QUINT8->enum.auto()
A:torch.onnx.symbolic_helper.QINT32->enum.auto()
A:torch.onnx.symbolic_helper.BFLOAT16->enum.auto()
torch.onnx.symbolic_helper.ScalarType(enum.IntEnum)
torch.onnx.symbolic_helper.__interpolate_helper(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor)
torch.onnx.symbolic_helper._arange_cast_helper(g,end,start=None,step=None,dtype=None)
torch.onnx.symbolic_helper._arange_helper(g,*args)
torch.onnx.symbolic_helper._avgpool_helper(tuple_fn,padding,kernel_size,stride,divisor_override,name)
torch.onnx.symbolic_helper._batchnorm_helper(g,input,weight,bias,running_mean,running_var)
torch.onnx.symbolic_helper._block_list_in_opset(name)
torch.onnx.symbolic_helper._cast_func_template(to_i,g,input,non_blocking)
torch.onnx.symbolic_helper._flatten_helper(g,input,start_dim,end_dim,dim)
torch.onnx.symbolic_helper._generate_wrapped_number(g,scalar)
torch.onnx.symbolic_helper._get_const(value,desc,arg_name)
torch.onnx.symbolic_helper._get_interpolate_attributes(g,mode,args)
torch.onnx.symbolic_helper._get_tensor_dim_size(x,dim)
torch.onnx.symbolic_helper._get_tensor_rank(x)
torch.onnx.symbolic_helper._get_tensor_sizes(x,allow_nonstatic=True)
torch.onnx.symbolic_helper._handle_reduce_dim_none(g,self,op_name)
torch.onnx.symbolic_helper._if_scalar_type_as(g,self,tensor)
torch.onnx.symbolic_helper._index_fill_reshape_helper(g,self,dim,index)
torch.onnx.symbolic_helper._interpolate_get_scales(g,scale_factor,dim)
torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g,input,size,scale_factor,mode,align_corners)
torch.onnx.symbolic_helper._interpolate_get_scales_if_available(g,scales)
torch.onnx.symbolic_helper._interpolate_helper(name,dim,interpolate_mode)
torch.onnx.symbolic_helper._interpolate_size_to_scales(g,input,output_size,dim)
torch.onnx.symbolic_helper._interpolate_warning(interpolate_mode)
torch.onnx.symbolic_helper._is_constant(value)
torch.onnx.symbolic_helper._is_fp(value)
torch.onnx.symbolic_helper._is_list(x)
torch.onnx.symbolic_helper._is_none(x)
torch.onnx.symbolic_helper._is_packed_list(list_value)
torch.onnx.symbolic_helper._is_scalar_list(x)
torch.onnx.symbolic_helper._is_split_static(split_size_or_sizes,_outputs)
torch.onnx.symbolic_helper._is_tensor(x)
torch.onnx.symbolic_helper._is_tensor_list(x)
torch.onnx.symbolic_helper._is_value(x)
torch.onnx.symbolic_helper._lt_helper(g,input,other)
torch.onnx.symbolic_helper._maybe_get_const(value,desc)
torch.onnx.symbolic_helper._maybe_get_scalar(value)
torch.onnx.symbolic_helper._onnx_opset_unsupported(op_name,current_opset,supported_opset)
torch.onnx.symbolic_helper._onnx_opset_unsupported_detailed(op_name,current_opset,supported_opset,reason)
torch.onnx.symbolic_helper._onnx_unsupported(op_name)
torch.onnx.symbolic_helper._optional_input_placeholder_tensor(g)
torch.onnx.symbolic_helper._parse_arg(value,desc,arg_name=None,node_name=None)
torch.onnx.symbolic_helper._reducesum_helper(g,input,axes_i=None,keepdims_i=1,noop_with_empty_axes_i=0)
torch.onnx.symbolic_helper._repeat_interleave_split_helper(g,self,reps,dim)
torch.onnx.symbolic_helper._reshape_helper(g,input,shape,allowzero=0)
torch.onnx.symbolic_helper._scalar(x)
torch.onnx.symbolic_helper._scatter_helper(g,self,dim,index,src)
torch.onnx.symbolic_helper._select_helper(g,self,dim,index,apply_reshape=True)
torch.onnx.symbolic_helper._set_onnx_shape_inference(onnx_shape_inference)
torch.onnx.symbolic_helper._set_operator_export_type(operator_export_type)
torch.onnx.symbolic_helper._set_opset_version(opset_version)
torch.onnx.symbolic_helper._set_training_mode(training_mode)
torch.onnx.symbolic_helper._size_helper(g,self,dim)
torch.onnx.symbolic_helper._slice_helper(g,input,axes,starts,ends,steps=None,dynamic_slice=False)
torch.onnx.symbolic_helper._sort_helper(g,input,dim,decending=True,out=None)
torch.onnx.symbolic_helper._squeeze_helper(g,input,axes_i)
torch.onnx.symbolic_helper._topk_helper(g,input,k,dim,largest=True,sorted=False,out=None)
torch.onnx.symbolic_helper._try_get_scalar_type(*args)
torch.onnx.symbolic_helper._unbind_helper(g,self,dim,_outputs)
torch.onnx.symbolic_helper._unimplemented(op,msg)
torch.onnx.symbolic_helper._unpack_list(list_value)
torch.onnx.symbolic_helper._unsqueeze_helper(g,input,axes_i)
torch.onnx.symbolic_helper.check_training_mode(op_train_mode,op_name)
torch.onnx.symbolic_helper.parse_args(*arg_descriptors)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_registry.py----------------------------------------
A:torch.onnx.symbolic_registry.module->importlib.import_module('torch.onnx.symbolic_opset{}'.format(opset_version))
A:torch.onnx.symbolic_registry.version_ops->get_ops_in_version(iter_version)
A:torch.onnx.symbolic_registry.supported_version->get_op_supported_version(opname, domain, version)
torch.onnx.symbolic_registry.get_op_supported_version(opname,domain,version)
torch.onnx.symbolic_registry.get_ops_in_version(version)
torch.onnx.symbolic_registry.get_registered_op(opname,domain,version)
torch.onnx.symbolic_registry.is_registered_op(opname,domain,version)
torch.onnx.symbolic_registry.is_registered_version(domain,version)
torch.onnx.symbolic_registry.register_op(opname,op,domain,version)
torch.onnx.symbolic_registry.register_ops_helper(domain,version,iter_version)
torch.onnx.symbolic_registry.register_ops_in_version(domain,version)
torch.onnx.symbolic_registry.register_version(domain,version)
torch.onnx.symbolic_registry.unregister_op(opname,domain,version)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset11.py----------------------------------------
A:torch.onnx.symbolic_opset11.dtype->_get_arange_dtype(args[2])
A:torch.onnx.symbolic_opset11.min_val->g.op('Constant', value_t=torch.tensor(0, dtype=sym_help.scalar_type_to_pytorch_type[dtype]))
A:torch.onnx.symbolic_opset11.max_val->g.op('Constant', value_t=torch.tensor(6, dtype=sym_help.scalar_type_to_pytorch_type[dtype]))
A:torch.onnx.symbolic_opset11.min->unused(g)
A:torch.onnx.symbolic_opset11.max->g.op('Cast', max, to_i=sym_help.cast_pytorch_to_onnx[dtype])
A:torch.onnx.symbolic_opset11.relu->g.op('Relu', input)
A:torch.onnx.symbolic_opset11.indices_list->torch.onnx.symbolic_helper._unpack_list(indices_list_value)
A:torch.onnx.symbolic_opset11.accumulate->torch.onnx.symbolic_helper._parse_arg(accumulate, 'b')
A:torch.onnx.symbolic_opset11.indices_list[idx_]->g.op('NonZero', indices_list[idx_])
A:torch.onnx.symbolic_opset11.index->g.op('Constant', value_t=torch.tensor([i + 1], dtype=torch.long))
A:torch.onnx.symbolic_opset11.broadcast_index_shape->g.op('Shape', index)
A:torch.onnx.symbolic_opset11.rank->g.op('Constant', value_t=torch.tensor(rank, dtype=torch.int64))
A:torch.onnx.symbolic_opset11.sub_data_shape->torch.onnx.symbolic_helper._slice_helper(g, g.op('Shape', self), axes=[0], starts=[len(indices_list)], ends=[maxsize])
A:torch.onnx.symbolic_opset11.values_shape->g.op('Concat', broadcast_index_shape, sub_data_shape, axis_i=0)
A:torch.onnx.symbolic_opset11.values->g.op('Cast', values, to_i=sym_help.cast_pytorch_to_onnx[dtype])
A:torch.onnx.symbolic_opset11.zeros->g.op('ConstantOfShape', g.op('Shape', self), value_t=torch.tensor([0], dtype=dtype))
A:torch.onnx.symbolic_opset11.result->mul(g, scale, g.op('RandomNormalLike', loc))
A:torch.onnx.symbolic_opset11.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset11.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset11.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset11.upsample_bicubic2d->_interpolate('upsample_bicubic2d', 4, 'cubic')
A:torch.onnx.symbolic_opset11.src_type->g.op('Cast', src, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()]).type().scalarType()
A:torch.onnx.symbolic_opset11.src->g.op('Cast', src, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()])
A:torch.onnx.symbolic_opset11.dim_tensor->g.op('Constant', value_t=torch.tensor(dim, dtype=torch.int))
A:torch.onnx.symbolic_opset11.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset11.cast->g.op('Cast', self, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset11.csum->g.op('CumSum', cast, dim_tensor)
A:torch.onnx.symbolic_opset11.source->torch.onnx.symbolic_helper._slice_helper(g, source, axes=torch.LongTensor([0]), starts=torch.LongTensor([0]), ends=size(g, index, torch.LongTensor([0])), dynamic_slice=True)
A:torch.onnx.symbolic_opset11.sz_0->size(g, self, g.op('Constant', value_t=torch.LongTensor([0])))
A:torch.onnx.symbolic_opset11.tensor_list->g.op('SequenceErase', tensor_list, i)
A:torch.onnx.symbolic_opset11.tensor_list_node->g.op('Cast', other, to_i=sym_help.cast_pytorch_to_onnx['Float']).node()
A:torch.onnx.symbolic_opset11.tensors->torch.onnx.symbolic_helper._unpack_list(other)
A:torch.onnx.symbolic_opset11.l->g.op('SequenceInsert', l, t)
A:torch.onnx.symbolic_opset11.dim->torch.onnx.symbolic_helper._get_tensor_rank(input)
A:torch.onnx.symbolic_opset11.(u, indices, inverse_indices, counts)->g.op('Unique', self, axis_i=dim, sorted_i=sorted, outputs=4)
A:torch.onnx.symbolic_opset11.padding->torch.onnx.symbolic_helper._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
A:torch.onnx.symbolic_opset11.input->g.op('Pad', input, g.op('Constant', value_t=torch.tensor(((0,) * 2 + padding) * 2)), mode_s='constant')
A:torch.onnx.symbolic_opset11.output->g.op('Transpose', output, perm_i=[0, 1, 2, 4, 3, 5])
A:torch.onnx.symbolic_opset11.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset11.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset11.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset11.split_out->g.op('SplitToSequence', self, split_size_or_sizes, axis_i=dim)
A:torch.onnx.symbolic_opset11.start->g.op('Constant', value_t=torch.tensor([0], dtype=torch.long))
A:torch.onnx.symbolic_opset11.axis->g.op('Constant', value_t=torch.tensor([dim], dtype=torch.long))
A:torch.onnx.symbolic_opset11.end->g.op('Mul', chunk_dim, index)
A:torch.onnx.symbolic_opset11.pad->g.op('Constant', value_t=torch.LongTensor([0, 0, padding_h, padding_w] * 2))
A:torch.onnx.symbolic_opset11.pad_len->torch.onnx.symbolic_opset9.size(g, pad, g.op('Constant', value_t=torch.tensor([0])))
A:torch.onnx.symbolic_opset11.extension->g.op('Sub', g.op('Mul', rank, g.op('Constant', value_t=torch.tensor(2, dtype=torch.int64))), pad_len)
A:torch.onnx.symbolic_opset11.paddings->_prepare_onnx_paddings(g, input, padding)
A:torch.onnx.symbolic_opset11.padding_c->g.op('Cast', paddings, to_i=sym_help.cast_pytorch_to_onnx['Long'])
A:torch.onnx.symbolic_opset11.value->torch.onnx.symbolic_helper._if_scalar_type_as(g, value, self)
A:torch.onnx.symbolic_opset11.(type, end, start, step)->torch.onnx.symbolic_helper._arange_cast_helper(g, start=args[0], end=args[1], dtype=dtype)
A:torch.onnx.symbolic_opset11.start_default->g.op('Constant', value_t=torch.tensor(0, dtype=sym_help.scalar_type_to_pytorch_type[type]))
A:torch.onnx.symbolic_opset11.delta_default->g.op('Constant', value_t=torch.tensor(1, dtype=sym_help.scalar_type_to_pytorch_type[type]))
A:torch.onnx.symbolic_opset11.arange_tensor->g.op('Range', start, end, delta_default)
A:torch.onnx.symbolic_opset11.like_shape->g.op('Shape', like)
A:torch.onnx.symbolic_opset11.stop->g.op('Gather', like_shape, g.op('Constant', value_t=torch.tensor(dim)), axis_i=0)
A:torch.onnx.symbolic_opset11.input_rank->torch.onnx.symbolic_helper._get_tensor_rank(self)
A:torch.onnx.symbolic_opset11.dim_size->g.op('Gather', g.op('Shape', self), dim, axis_i=0)
A:torch.onnx.symbolic_opset11.dim_constant->g.op('Constant', value_t=torch.tensor([dim]))
A:torch.onnx.symbolic_opset11.size->torch.onnx.symbolic_helper._size_helper(g, self, dim_constant)
A:torch.onnx.symbolic_opset11.const_one->g.op('Constant', value_t=torch.ones(1, dtype=torch.int64))
A:torch.onnx.symbolic_opset11.cond->_add_input_to_block(loop_block)
A:torch.onnx.symbolic_opset11.if_node_outputs->g.op('If', cond)
A:torch.onnx.symbolic_opset11.if_node->g.op('If', cond).node()
A:torch.onnx.symbolic_opset11.if_block->torch.onnx.utils._add_block(if_node)
A:torch.onnx.symbolic_opset11.squeeze_->torch.onnx.symbolic_helper._squeeze_helper(if_block, self, [dim])
A:torch.onnx.symbolic_opset11.else_block->torch.onnx.utils._add_block(if_node)
A:torch.onnx.symbolic_opset11.identity_->torch.onnx.utils._add_block(if_node).op('Identity', self)
A:torch.onnx.symbolic_opset11.indices->torch.onnx.symbolic_helper._unpack_list(index)
A:torch.onnx.symbolic_opset11.dim_value->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset11.(expanded_index_shape, expanded_index)->torch.onnx.symbolic_helper._index_fill_reshape_helper(g, self, dim, index)
A:torch.onnx.symbolic_opset11.expanded_value->expand(g, value, expanded_index_shape, None)
A:torch.onnx.symbolic_opset11.other->g.op('Cast', other, to_i=sym_help.cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_opset11.two->g.op('Constant', value_t=torch.tensor(2, dtype=torch.float32))
A:torch.onnx.symbolic_opset11.two_pow->g.op('Cast', two_pow, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()])
A:torch.onnx.symbolic_opset11.rshift->g.op('Div', self, two_pow)
A:torch.onnx.symbolic_opset11.lshift->g.op('Mul', self, two_pow)
A:torch.onnx.symbolic_opset11.blocks_d->g.op('Sub', blocks_d, g.op('Constant', value_t=torch.tensor(dilation_d * (kernel_size_d - 1))))
A:torch.onnx.symbolic_opset11.blocks_d_indices->torch.onnx.symbolic_helper._unsqueeze_helper(g, blocks_d_indices, [0])
A:torch.onnx.symbolic_opset11.kernel_grid->g.op('Constant', value_t=kernel_grid.unsqueeze(0))
A:torch.onnx.symbolic_opset11.kernel_mask->torch.onnx.symbolic_helper._reshape_helper(g, kernel_grid, g.op('Constant', value_t=torch.tensor([-1, 1])))
A:torch.onnx.symbolic_opset11.block_mask->g.op('Add', blocks_d_indices, kernel_mask)
A:torch.onnx.symbolic_opset11.batch_dim->size(g, input, g.op('Constant', value_t=torch.tensor(0)))
A:torch.onnx.symbolic_opset11.channel_dim->size(g, input, g.op('Constant', value_t=torch.tensor(1)))
A:torch.onnx.symbolic_opset11.channel_unfolded->g.op('Mul', channel_dim, g.op('Constant', value_t=torch.tensor(kernel_h * kernel_w)))
A:torch.onnx.symbolic_opset11.input_h->size(g, input, g.op('Constant', value_t=torch.tensor(2)))
A:torch.onnx.symbolic_opset11.input_w->size(g, input, g.op('Constant', value_t=torch.tensor(3)))
A:torch.onnx.symbolic_opset11.blocks_row_indices->_get_im2col_indices_along_dim(g, input_h, kernel_h, dilation_h, padding_h, stride_h)
A:torch.onnx.symbolic_opset11.blocks_col_indices->_get_im2col_indices_along_dim(g, input_w, kernel_w, dilation_w, padding_w, stride_w)
A:torch.onnx.symbolic_opset11.output_shape->_get_im2col_output_shape(g, input, kernel_h, kernel_w)
A:torch.onnx.symbolic_opset11.padded_input->_get_im2col_padded_input(g, input, padding_h, padding_w)
A:torch.onnx.symbolic_opset11.loop_condition->g.op('Cast', loop_condition, to_i=9)
A:torch.onnx.symbolic_opset11.zero->g.op('Constant', value_t=torch.tensor([0]))
A:torch.onnx.symbolic_opset11.indices_len->torch.onnx.symbolic_helper._unsqueeze_helper(g, sym_help._size_helper(g, indices, g.op('Constant', value_t=torch.tensor(0))), [0])
A:torch.onnx.symbolic_opset11.offsets->g.op('Concat', *offsets, axis_i=0)
A:torch.onnx.symbolic_opset11.offsets_starts->torch.onnx.symbolic_helper._slice_helper(g, offsets, axes=[0], starts=[0], ends=[maxsize], steps=[1])
A:torch.onnx.symbolic_opset11.offsets_ends->torch.onnx.symbolic_helper._slice_helper(g, offsets, axes=[0], starts=[1], ends=[maxsize], steps=[1])
A:torch.onnx.symbolic_opset11.loop_len->torch.onnx.symbolic_helper._size_helper(g, offsets_ends, g.op('Constant', value_t=torch.tensor(0)))
A:torch.onnx.symbolic_opset11.loop->g.op('Loop', loop_len, loop_condition)
A:torch.onnx.symbolic_opset11.loop_block->_add_block(loop.node())
A:torch.onnx.symbolic_opset11.block_input_iter->_add_input_to_block(loop_block)
A:torch.onnx.symbolic_opset11.indices_start->torch.onnx.symbolic_helper._unsqueeze_helper(loop_block, indices_start, [0])
A:torch.onnx.symbolic_opset11.indices_end->torch.onnx.symbolic_helper._unsqueeze_helper(loop_block, indices_end, [0])
A:torch.onnx.symbolic_opset11.indices_row->_add_block(loop.node()).op('Slice', indices, indices_start, indices_end, zero)
A:torch.onnx.symbolic_opset11.embeddings->_add_block(loop.node()).op('ReduceMax', embeddings, axes_i=[0], keepdims_i=0)
A:torch.onnx.symbolic_opset11.per_sample_weights_row->torch.onnx.symbolic_helper._unsqueeze_helper(loop_block, per_sample_weights_row, [1])
A:torch.onnx.symbolic_opset11.cond_out->_add_block(loop.node()).op('Cast', loop_condition, to_i=9)
A:torch.onnx.symbolic_opset11.input_shape->g.op('Shape', self)
A:torch.onnx.symbolic_opset11.input_shape_dim->g.op('Gather', input_shape, axis, axis_i=0)
A:torch.onnx.symbolic_opset11.chunk_size->g.op('Div', g.op('Add', dim_size, chunk_size_s), chunks)
A:torch.onnx.symbolic_opset11.chunk_size_minus_1->g.op('Constant', value_t=torch.tensor([chunks - 1], dtype=torch.long))
A:torch.onnx.symbolic_opset11.input_shape_dim_shift->g.op('Add', input_shape_dim, chunk_size_minus_1)
A:torch.onnx.symbolic_opset11.chunk_dim->g.op('Div', input_shape_dim_shift, chunk_size)
A:torch.onnx.symbolic_opset11.chunk_size_s->g.op('Sub', chunks, g.op('Constant', value_t=torch.tensor([1], dtype=torch.long)))
A:torch.onnx.symbolic_opset11.chunk_vec->g.op('Concat', *chunk_vec, axis_i=0)
torch.onnx.symbolic_opset11.Delete(g,tensor_list,dim)
torch.onnx.symbolic_opset11.__getitem_(g,self,i)
torch.onnx.symbolic_opset11.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor,antialias)
torch.onnx.symbolic_opset11.__lshift_(g,self,other)
torch.onnx.symbolic_opset11.__rshift_(g,self,other)
torch.onnx.symbolic_opset11._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset11._dim_arange(g,like,dim)
torch.onnx.symbolic_opset11._get_im2col_indices_along_dim(g,input_d,kernel_size_d,dilation_d,padding_d,stride_d)
torch.onnx.symbolic_opset11._get_im2col_output_shape(g,input,kernel_h,kernel_w)
torch.onnx.symbolic_opset11._get_im2col_padded_input(g,input,padding_h,padding_w)
torch.onnx.symbolic_opset11._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset11._len(g,self)
torch.onnx.symbolic_opset11._prepare_onnx_paddings(g,input,pad)
torch.onnx.symbolic_opset11._set_item(g,tensor_list,i,v)
torch.onnx.symbolic_opset11._unique2(g,self,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset11.add(g,self,other,alpha=None)
torch.onnx.symbolic_opset11.append(g,self,tensor)
torch.onnx.symbolic_opset11.arange(g,*args)
torch.onnx.symbolic_opset11.cat(g,tensor_list,dim)
torch.onnx.symbolic_opset11.chunk(g,self,chunks,dim)
torch.onnx.symbolic_opset11.clamp(g,self,min,max)
torch.onnx.symbolic_opset11.clamp_max(g,self,max)
torch.onnx.symbolic_opset11.clamp_min(g,self,min)
torch.onnx.symbolic_opset11.constant_pad_nd(g,input,padding,value=None)
torch.onnx.symbolic_opset11.cumsum(g,self,dim,dtype=None)
torch.onnx.symbolic_opset11.embedding_bag(g,embedding_matrix,indices,offsets,scale_grad_by_freq,mode,sparse,per_sample_weights,include_last_offset,padding_idx)
torch.onnx.symbolic_opset11.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset11.gather(g,self,dim,index,sparse_grad=False)
torch.onnx.symbolic_opset11.hardtanh(g,self,min_val,max_val)
torch.onnx.symbolic_opset11.im2col(g,input,kernel_size,dilation,padding,stride)
torch.onnx.symbolic_opset11.index(g,self,index)
torch.onnx.symbolic_opset11.index_copy(g,self,dim,index,source)
torch.onnx.symbolic_opset11.index_fill(g,self,dim,index,value)
torch.onnx.symbolic_opset11.index_put(g,self,indices_list_value,values,accumulate=False)
torch.onnx.symbolic_opset11.insert(g,self,pos,tensor)
torch.onnx.symbolic_opset11.linalg_det(g,self)
torch.onnx.symbolic_opset11.logdet(g,input)
torch.onnx.symbolic_opset11.masked_scatter(g,self,mask,source)
torch.onnx.symbolic_opset11.masked_select(g,self,mask)
torch.onnx.symbolic_opset11.mm(g,self,other)
torch.onnx.symbolic_opset11.narrow(g,input,dim,start,length)
torch.onnx.symbolic_opset11.normal(g,loc,scale,seed)
torch.onnx.symbolic_opset11.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic_opset11.pop(g,tensor_list,dim)
torch.onnx.symbolic_opset11.prim_ConstantChunk(g,self,chunks,dim)
torch.onnx.symbolic_opset11.reflection_pad(g,input,padding)
torch.onnx.symbolic_opset11.relu6(g,input)
torch.onnx.symbolic_opset11.remainder(g,input,other)
torch.onnx.symbolic_opset11.replication_pad(g,input,padding)
torch.onnx.symbolic_opset11.round(g,self)
torch.onnx.symbolic_opset11.scatter(g,self,dim,index,src)
torch.onnx.symbolic_opset11.select(g,self,dim,index)
torch.onnx.symbolic_opset11.size(g,self,dim=None)
torch.onnx.symbolic_opset11.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset11.split(g,self,split_size_or_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset11.split_with_sizes(g,self,split_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset11.squeeze(g,self,dim=None)
torch.onnx.symbolic_opset11.stack(g,tensor_list,dim)
torch.onnx.symbolic_opset11.topk(g,self,k,dim,largest,sorted,out=None)
torch.onnx.symbolic_opset11.unbind(g,self,dim=0,_outputs=None)
torch.onnx.symbolic_opset11.unique_dim(g,self,dim,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset11.unsqueeze(g,self,dim)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset15.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_caffe2.py----------------------------------------
A:torch.onnx.symbolic_caffe2.module->importlib.import_module('torch.onnx.symbolic_caffe2')
A:torch.onnx.symbolic_caffe2.quant_version_ops->getmembers(sym_registry._symbolic_versions['caffe2'])
A:torch.onnx.symbolic_caffe2.output->g.op('_caffe2::Int8Sigmoid', input, **kwargs)
A:torch.onnx.symbolic_caffe2.output_size->torch.onnx.symbolic_helper._parse_arg(output_size, 'is')
A:torch.onnx.symbolic_caffe2.input->nchw2nhwc(g, input)
A:torch.onnx.symbolic_caffe2.start->torch.onnx.symbolic_helper._parse_arg(start, 'i')
A:torch.onnx.symbolic_caffe2.end->torch.onnx.symbolic_helper._parse_arg(end, 'i')
A:torch.onnx.symbolic_caffe2.dim->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_caffe2.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
torch.onnx.symbolic_caffe2._empty_affine_quantized(g,input,shape,scale,zero_point,dtype,pin_memory,memory_format,layout)
torch.onnx.symbolic_caffe2._permute_helper(g,input,axes)
torch.onnx.symbolic_caffe2.add(g,input_a,input_b,scale,zero_point)
torch.onnx.symbolic_caffe2.avg_pool2d(g,input,kernel_size,stride,padding,ceil_mode,count_include_pad,divisor_override=None)
torch.onnx.symbolic_caffe2.cat(g,tensor_list,dim,scale=None,zero_point=None)
torch.onnx.symbolic_caffe2.conv2d(g,input,weight,bias,stride,padding,dilation,groups,scale,zero_point)
torch.onnx.symbolic_caffe2.conv2d_relu(g,input,weight,bias,stride,padding,dilation,groups,scale,zero_point)
torch.onnx.symbolic_caffe2.conv_prepack(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_caffe2.dequantize(g,input)
torch.onnx.symbolic_caffe2.linear(g,input,weight,bias,scale,zero_point)
torch.onnx.symbolic_caffe2.linear_prepack(g,weight,bias)
torch.onnx.symbolic_caffe2.max_pool2d(g,input,kernel_size,stride,padding,dilation,ceil_mode)
torch.onnx.symbolic_caffe2.nchw2nhwc(g,input)
torch.onnx.symbolic_caffe2.nhwc2nchw(g,input)
torch.onnx.symbolic_caffe2.quantize_per_tensor(g,input,scale,zero_point,dtype)
torch.onnx.symbolic_caffe2.register_quantized_ops(domain,version)
torch.onnx.symbolic_caffe2.relu(g,input)
torch.onnx.symbolic_caffe2.reshape(g,input,shape)
torch.onnx.symbolic_caffe2.sigmoid(g,input)
torch.onnx.symbolic_caffe2.slice(g,input,dim,start,end,step)
torch.onnx.symbolic_caffe2.upsample_nearest2d(g,input,output_size,align_corners=None,scales_h=None,scales_w=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset13.py----------------------------------------
A:torch.onnx.symbolic_opset13.softmax->g.op('Cast', softmax, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset13.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset13.return_op->g.op('Cast', return_op, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset13.dim_val->torch.onnx.symbolic_helper._maybe_get_const(dim, 'is')
A:torch.onnx.symbolic_opset13.sqr->g.op('Mul', self, self)
A:torch.onnx.symbolic_opset13.sumsqr->torch.onnx.symbolic_helper._reducesum_helper(g, sqr, dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset13.split_out->g.op('SplitToSequence', self, split_size_or_sizes, axis_i=dim)
A:torch.onnx.symbolic_opset13.start->g.op('Constant', value_t=torch.tensor([0], dtype=torch.long))
A:torch.onnx.symbolic_opset13.axis->g.op('Constant', value_t=torch.tensor([dim], dtype=torch.long))
A:torch.onnx.symbolic_opset13.end->g.op('Add', start, split_sizes[i])
A:torch.onnx.symbolic_opset13.split_size->torch.onnx.symbolic_helper._get_const(split_size_or_sizes, 'i', 'split_size')
A:torch.onnx.symbolic_opset13.size->torch.onnx.symbolic_helper._get_tensor_dim_size(self, dim)
A:torch.onnx.symbolic_opset13.splits->g.op('Constant', value_t=torch.tensor(splits, dtype=torch.long))
A:torch.onnx.symbolic_opset13.outputs->g.op('Split', self, splits, axis_i=dim, outputs=_outputs)
A:torch.onnx.symbolic_opset13.condition->nonzero(g, condition)
A:torch.onnx.symbolic_opset13.zero_point->g.op('Cast', zero_point, to_i=sym_help.cast_pytorch_to_onnx['Char'])
A:torch.onnx.symbolic_opset13.self->g.op('Transpose', self, perm_i=axes + [dim1, dim2])
A:torch.onnx.symbolic_opset13.keepdim->torch.onnx.symbolic_helper._get_const(keepdim, 'i', 'keepdim')
A:torch.onnx.symbolic_opset13.symbolic->_reduce_op_symbolic(onnx_op)
A:torch.onnx.symbolic_opset13.dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset13.sum->_reduce_with_dtype('ReduceSum', 'sum')
A:torch.onnx.symbolic_opset13.input->torch.onnx.symbolic_helper._reshape_helper(g, self, g.op('Constant', value_t=torch.tensor([-1])))
A:torch.onnx.symbolic_opset13.dim->torch.onnx.symbolic_helper._maybe_get_scalar(dim)
A:torch.onnx.symbolic_opset13.repeats_dim->torch.onnx.symbolic_helper._get_tensor_rank(repeats)
A:torch.onnx.symbolic_opset13.repeats_sizes->torch.onnx.symbolic_helper._get_tensor_sizes(repeats)
A:torch.onnx.symbolic_opset13.input_sizes->torch.onnx.symbolic_helper._get_tensor_sizes(input)
A:torch.onnx.symbolic_opset13.output_sizes->torch.onnx.symbolic_helper._get_tensor_sizes(input).copy()
A:torch.onnx.symbolic_opset13.reps->unsqueeze(g, reps, 0)
A:torch.onnx.symbolic_opset13.repeats->where(g, repeat_cond, g.op('Expand', repeats, reps), repeats)
A:torch.onnx.symbolic_opset13.repeat_dim->torch.onnx.symbolic_helper._size_helper(g, repeats, g.op('Constant', value_t=torch.LongTensor([0])))
A:torch.onnx.symbolic_opset13.repeat_cond->g.op('Equal', repeat_dim, g.op('Constant', value_t=torch.LongTensor([1])))
A:torch.onnx.symbolic_opset13.reps_like->g.op('ConstantOfShape', g.op('Shape', repeats), value_t=torch.tensor([1], dtype=torch.long))
A:torch.onnx.symbolic_opset13.r_splits->split(g, repeats, reps_like, 0)
A:torch.onnx.symbolic_opset13.i_splits->split(g, input, reps_like, dim)
A:torch.onnx.symbolic_opset13.loop_condition->g.op('Cast', loop_condition, to_i=9)
A:torch.onnx.symbolic_opset13.final_splits->_add_block(loop.node()).op('SequenceInsert', final_splits, i_split)
A:torch.onnx.symbolic_opset13.loop->g.op('Loop', loop_len, loop_condition, final_splits)
A:torch.onnx.symbolic_opset13.loop_block->_add_block(loop.node())
A:torch.onnx.symbolic_opset13.block_input_iter->_add_input_to_block(loop_block)
A:torch.onnx.symbolic_opset13.cond->_add_input_to_block(loop_block)
A:torch.onnx.symbolic_opset13.r_split->_add_block(loop.node()).op('SequenceAt', r_splits, block_input_iter)
A:torch.onnx.symbolic_opset13.i_split->torch.onnx.symbolic_helper._reshape_helper(loop_block, i_split, g.op('Constant', value_t=torch.LongTensor(output_sizes)))
A:torch.onnx.symbolic_opset13.r_concat->_add_block(loop.node()).op('Concat', *r_concat, axis_i=0)
A:torch.onnx.symbolic_opset13.cond_out->_add_block(loop.node()).op('Cast', loop_condition, to_i=9)
A:torch.onnx.symbolic_opset13.loop_out->g.op('ConcatFromSequence', loop_out, axis_i=dim)
A:torch.onnx.symbolic_opset13.dim1_size->size(g, self, dim=g.op('Constant', value_t=torch.LongTensor([dim1])))
A:torch.onnx.symbolic_opset13.dim2_size->size(g, self, dim=g.op('Constant', value_t=torch.LongTensor([dim2])))
A:torch.onnx.symbolic_opset13.mask_shape->g.op('Concat', dim1_size, dim2_size, axis_i=0)
A:torch.onnx.symbolic_opset13.mask->g.op('EyeLike', mask, k_i=offset)
A:torch.onnx.symbolic_opset13.rank->torch.onnx.symbolic_helper._get_tensor_rank(self)
A:torch.onnx.symbolic_opset13.axes->list(range(rank))
A:torch.onnx.symbolic_opset13.result->torch.onnx.symbolic_helper._reducesum_helper(g, result, axes_i=[-1], keepdims_i=0)
A:torch.onnx.symbolic_opset13.offset_op->g.op('Constant', value_t=torch.LongTensor([offset]))
A:torch.onnx.symbolic_opset13.diag_size->g.op('Concat', diag_size, axis_i=0)
A:torch.onnx.symbolic_opset13.select_window_ones_fill->ones(g, diag_size, 4, None, None)
A:torch.onnx.symbolic_opset13.select_window->g.op('Add', select_window, g.op('Constant', value_t=torch.LongTensor([abs(offset) - 1])))
A:torch.onnx.symbolic_opset13.gather_shape->g.op('Concat', *gather_shape, axis_i=0)
A:torch.onnx.symbolic_opset13.gather_indices->zeros(g, gather_shape, 4, None, None)
A:torch.onnx.symbolic_opset13.overrun_cond->g.op('Not', g.op('Equal', diag_size, g.op('Constant', value_t=torch.tensor(0, dtype=torch.int64))))
A:torch.onnx.symbolic_opset13.if_op->g.op('If', overrun_cond)
A:torch.onnx.symbolic_opset13.if_node->g.op('If', overrun_cond).node()
A:torch.onnx.symbolic_opset13.if_block->_add_block(if_node)
A:torch.onnx.symbolic_opset13.gather_indices_if_block->torch.onnx.symbolic_helper._unsqueeze_helper(if_block, gather_indices_if_block, [rank - 1])
A:torch.onnx.symbolic_opset13.final_non_overrun_->_add_block(if_node).op('GatherND', result, gather_indices_if_block, batch_dims_i=rank - 2)
A:torch.onnx.symbolic_opset13.else_block->_add_block(if_node)
A:torch.onnx.symbolic_opset13.final_overrun_->zeros(else_block, gather_shape, 6, None, None)
torch.onnx.symbolic_opset13._reduce_op_symbolic(onnx_op_name)
torch.onnx.symbolic_opset13._reduce_with_dtype(onnx_op,name)
torch.onnx.symbolic_opset13.diagonal(g,self,offset,dim1,dim2)
torch.onnx.symbolic_opset13.fake_quantize_per_channel_affine(g,inputs,scale,zero_point,axis,quant_min=-128,quant_max=127)
torch.onnx.symbolic_opset13.frobenius_norm(g,self,dim=None,keepdim=False)
torch.onnx.symbolic_opset13.log_softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset13.nonzero_numpy(g,input,_outputs=None)
torch.onnx.symbolic_opset13.repeat_interleave(g,self,repeats,dim=None,output_size=None)
torch.onnx.symbolic_opset13.softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset13.split(g,self,split_size_or_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset13.split_with_sizes(g,self,split_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset13.unbind(g,self,dim=0,_outputs=None)
torch.onnx.symbolic_opset13.unsafe_chunk(g,self,chunks,dim,_outputs=None)
torch.onnx.symbolic_opset13.unsafe_split(g,self,split_size_or_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset13.unsafe_split_with_sizes(g,self,split_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset13.where(g,condition,self=None,other=None,_outputs=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/operators.py----------------------------------------
torch.onnx.operators.reshape_from_tensor_shape(x,shape)
torch.onnx.operators.shape_as_tensor(x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset9.py----------------------------------------
A:torch.onnx.symbolic_opset9.n->g.op('ConvTranspose' if transposed else 'Conv', *args, **kwargs)
A:torch.onnx.symbolic_opset9.shape->torch.onnx.symbolic_helper._slice_helper(g, result, axes=[dims[i]], starts=[0], ends=[-shifts[i]])
A:torch.onnx.symbolic_opset9.out->reshape_as(g, out, index)
A:torch.onnx.symbolic_opset9.scalar_type->torch.get_default_dtype()
A:torch.onnx.symbolic_opset9.div->g.op('Ceil', true_divide(g, sub, step))
A:torch.onnx.symbolic_opset9.zero->g.op('Constant', value_t=torch.tensor(0, dtype=torch.int64))
A:torch.onnx.symbolic_opset9.negative->g.op('Xor', sym_help._lt_helper(g, self, zero), sym_help._lt_helper(g, other, zero))
A:torch.onnx.symbolic_opset9.mod->g.op('Sub', self, g.op('Mul', div, other))
A:torch.onnx.symbolic_opset9.fixup_mask->g.op('And', negative, g.op('Not', g.op('Equal', mod, zero)))
A:torch.onnx.symbolic_opset9.one->g.op('Constant', value_t=torch.tensor(correction, dtype=torch.float))
A:torch.onnx.symbolic_opset9.fixup->g.op('Mul', fixup_mask, one)
A:torch.onnx.symbolic_opset9.self->g.op('Transpose', self, perm_i=adv_idx_permute)
A:torch.onnx.symbolic_opset9.other->expand_as(g, other, new_shape)
A:torch.onnx.symbolic_opset9.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
A:torch.onnx.symbolic_opset9.C->g.op('Constant', value_t=torch.tensor([1]))
A:torch.onnx.symbolic_opset9.self_dtype->torch.onnx.symbolic_helper._try_get_scalar_type(self)
A:torch.onnx.symbolic_opset9.mat1_dtype->torch.onnx.symbolic_helper._try_get_scalar_type(mat1)
A:torch.onnx.symbolic_opset9.mat2_dtype->torch.onnx.symbolic_helper._try_get_scalar_type(mat2)
A:torch.onnx.symbolic_opset9.mat1_rank->torch.onnx.symbolic_helper._get_tensor_rank(mat1)
A:torch.onnx.symbolic_opset9.mat2_rank->torch.onnx.symbolic_helper._get_tensor_rank(mat2)
A:torch.onnx.symbolic_opset9.dtype->torch.onnx.symbolic_helper.scalar_type_to_onnx.index(sym_help.cast_pytorch_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.res1->g.op('Mul', res1, alpha)
A:torch.onnx.symbolic_opset9.alpha->g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64))
A:torch.onnx.symbolic_opset9.beta->g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64))
A:torch.onnx.symbolic_opset9.res2->g.op('Mul', res2, beta)
A:torch.onnx.symbolic_opset9.overloads->fn(g, *args)
A:torch.onnx.symbolic_opset9.symbolic->_reduce_op_symbolic(onnx_op, allow_multi_dim_support=allow_multi_dim_support)
A:torch.onnx.symbolic_opset9.sum->torch.onnx.symbolic_helper._reducesum_helper(g, exp, axes_i=[dim])
A:torch.onnx.symbolic_opset9.mean->g.op('ReduceMean', input, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.prod->_reduce_with_dtype('ReduceProd', 'prod', allow_multi_dim_support=False)
A:torch.onnx.symbolic_opset9.size->select(g, sizes, g.op('Constant', value_t=torch.tensor([0])), g.op('Constant', value_t=torch.tensor(i)))
A:torch.onnx.symbolic_opset9.ones->ones_like(g, size, dtype)
A:torch.onnx.symbolic_opset9.neg_ones->mul(g, ones, g.op('Constant', value_t=torch.tensor(-1)))
A:torch.onnx.symbolic_opset9.self_t->self_t.to(torch.double).to(torch.double)
A:torch.onnx.symbolic_opset9.rank->torch.onnx.symbolic_helper._get_tensor_rank(input)
A:torch.onnx.symbolic_opset9.dim->torch.onnx.symbolic_helper._maybe_get_const(dim, 'i')
A:torch.onnx.symbolic_opset9.axes->list(range(rank))
A:torch.onnx.symbolic_opset9.dim_size->torch.onnx.symbolic_helper._unsqueeze_helper(g, n, [0])
A:torch.onnx.symbolic_opset9.split_size->torch.onnx.symbolic_helper._get_const(split_size_or_sizes, 'i', 'split_size')
A:torch.onnx.symbolic_opset9.outputs->g.op('Split', self, split_i=[1] * _outputs, axis_i=dim, outputs=_outputs)
A:torch.onnx.symbolic_opset9.index->torch.onnx.symbolic_helper._unsqueeze_helper(g, index, [sym_help._get_tensor_rank(index)])
A:torch.onnx.symbolic_opset9.slice_node->torch.onnx.symbolic_helper._slice_helper(g, self, axes=[dim], starts=[index], ends=[end_index])
A:torch.onnx.symbolic_opset9.squeeze_dim->torch.onnx.symbolic_helper._get_const(dim, 'i', 'dim')
A:torch.onnx.symbolic_opset9.self_rank->torch.onnx.symbolic_helper._get_tensor_rank(self)
A:torch.onnx.symbolic_opset9.weight->t(g, weight)
A:torch.onnx.symbolic_opset9.relu->g.op('Relu', input)
A:torch.onnx.symbolic_opset9.sz_0->size(g, self, g.op('Constant', value_t=torch.LongTensor([0])))
A:torch.onnx.symbolic_opset9.negative_slope->torch.onnx.symbolic_helper._get_const(negative_slope, 't', 'negative_slope')
A:torch.onnx.symbolic_opset9.(first, second)->g.op('Split', input, axis_i=dim, outputs=2)
A:torch.onnx.symbolic_opset9.input_dim->torch.onnx.symbolic_helper._get_tensor_rank(input)
A:torch.onnx.symbolic_opset9.input->g.op('Not', args[0])
A:torch.onnx.symbolic_opset9.softmax->g.op('Cast', softmax, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset9.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset9.exp->g.op('Exp', input)
A:torch.onnx.symbolic_opset9.beta_const->torch.onnx.symbolic_helper._maybe_get_const(beta, 'f')
A:torch.onnx.symbolic_opset9.sizes->torch.onnx.symbolic_helper._maybe_get_const(sizes, 'is')
A:torch.onnx.symbolic_opset9.padding->_convert_padding_node(padding)
A:torch.onnx.symbolic_opset9.padding_ceil->get_pool_ceil_padding(input, kernel_size, stride, padding)
A:torch.onnx.symbolic_opset9.(r, indices)->g.op('MaxPool', input, outputs=2, **kwargs)
A:torch.onnx.symbolic_opset9.(_, flattened_indices)->g.op('MaxPool', input, outputs=2, kernel_shape_i=[1 for _ in range(ndims)], strides_i=[1 for _ in range(ndims)])
A:torch.onnx.symbolic_opset9.s->torch.onnx.symbolic_helper._slice_helper(g, flattened_indices, axes=[2 + i for i in range(ndims)], starts=tuple_fn(0), ends=tuple_fn(1))
A:torch.onnx.symbolic_opset9.indices->torch.onnx.symbolic_helper._unpack_list(index)
A:torch.onnx.symbolic_opset9.r->g.op('MaxPool', input, outputs=1, **kwargs)
A:torch.onnx.symbolic_opset9.max_pool1d->_max_pool('max_pool1d', _single, 1, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool2d->_max_pool('max_pool2d', _pair, 2, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool3d->_max_pool('max_pool3d', _triple, 3, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool1d_with_indices->_max_pool('max_pool1d_with_indices', _single, 1, return_indices=True)
A:torch.onnx.symbolic_opset9.max_pool2d_with_indices->_max_pool('max_pool2d_with_indices', _pair, 2, return_indices=True)
A:torch.onnx.symbolic_opset9.max_pool3d_with_indices->_max_pool('max_pool3d_with_indices', _triple, 3, return_indices=True)
A:torch.onnx.symbolic_opset9.output->g.op('Cast', square(g, sin(g, output)), to_i=sym_help.scalar_type_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset9.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset9.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset9.output_size->_parse_arg(output_size, 'is')
A:torch.onnx.symbolic_opset9.adaptive_avg_pool1d->_adaptive_pool('adaptive_avg_pool1d', 'AveragePool', _single)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool2d->_adaptive_pool('adaptive_avg_pool2d', 'AveragePool', _pair)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool3d->_adaptive_pool('adaptive_avg_pool3d', 'AveragePool', _triple)
A:torch.onnx.symbolic_opset9.adaptive_max_pool1d->_adaptive_pool('adaptive_max_pool1d', 'MaxPool', _single, max_pool1d_with_indices)
A:torch.onnx.symbolic_opset9.adaptive_max_pool2d->_adaptive_pool('adaptive_max_pool2d', 'MaxPool', _pair, max_pool2d_with_indices)
A:torch.onnx.symbolic_opset9.adaptive_max_pool3d->_adaptive_pool('adaptive_max_pool3d', 'MaxPool', _triple, max_pool3d_with_indices)
A:torch.onnx.symbolic_opset9.input_list->list()
A:torch.onnx.symbolic_opset9.value->torch.onnx.symbolic_helper._maybe_get_scalar(value)
A:torch.onnx.symbolic_opset9.paddings->_prepare_onnx_paddings(sym_help._get_tensor_rank(input), padding)
A:torch.onnx.symbolic_opset9.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset9.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset9.scales->torch.onnx.symbolic_helper._interpolate_size_to_scales(g, input, output_size, dim)
A:torch.onnx.symbolic_opset9.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset9.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset9.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset9.(scales, mode)->torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g, input, size, scale_factor, mode, align_corners)
A:torch.onnx.symbolic_opset9.from_cast_func->wrap_logical_op_with_cast_to(input.type().scalarType())(fn)
A:torch.onnx.symbolic_opset9.two->g.op('Constant', value_t=torch.tensor(2, dtype=torch.float32))
A:torch.onnx.symbolic_opset9.two_pow->g.op('Cast', two_pow, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()])
A:torch.onnx.symbolic_opset9.rshift->g.op('Div', self, two_pow)
A:torch.onnx.symbolic_opset9.lshift->g.op('Mul', self, two_pow)
A:torch.onnx.symbolic_opset9.condition->torch.onnx.symbolic_opset9.nonzero(g, condition)
A:torch.onnx.symbolic_opset9.return_op->g.op('Transpose', return_op, perm_i=axes)
A:torch.onnx.symbolic_opset9.weight_size->torch.onnx.symbolic_helper._get_tensor_sizes(weight)
A:torch.onnx.symbolic_opset9.(weight, bias, running_mean, running_var)->torch.onnx.symbolic_helper._batchnorm_helper(g, input, weight, bias, running_mean, running_var)
A:torch.onnx.symbolic_opset9.two_cst->torch.onnx.symbolic_helper._generate_wrapped_number(g, 2.0)
A:torch.onnx.symbolic_opset9.eps_cst->torch.onnx.symbolic_helper._generate_wrapped_number(g, eps)
A:torch.onnx.symbolic_opset9.numerator->sub(g, input, mean)
A:torch.onnx.symbolic_opset9.variance->g.op('ReduceMean', pow(g, numerator, two_cst), axes_i=axes)
A:torch.onnx.symbolic_opset9.denominator->sqrt(g, add(g, variance, eps_cst))
A:torch.onnx.symbolic_opset9.layer_norm->add(g, layer_norm, bias)
A:torch.onnx.symbolic_opset9.channel_size->torch.onnx.symbolic_helper._get_tensor_dim_size(input, 1)
A:torch.onnx.symbolic_opset9.weight_value->torch.tensor([1.0]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_opset9.bias_value->torch.tensor([0.0]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_opset9.bias->g.op('Constant', value_t=bias_value)
A:torch.onnx.symbolic_opset9.input_size->torch.onnx.symbolic_helper._get_tensor_sizes(input)
A:torch.onnx.symbolic_opset9.input_size_reshape->torch.onnx.symbolic_helper._get_tensor_sizes(input).copy()
A:torch.onnx.symbolic_opset9.weight_->g.op('Constant', value_t=torch.tensor([1.0] * num_groups).type('torch.' + input.type().scalarType() + 'Tensor'))
A:torch.onnx.symbolic_opset9.bias_->g.op('Constant', value_t=torch.tensor([0.0] * num_groups).type('torch.' + input.type().scalarType() + 'Tensor'))
A:torch.onnx.symbolic_opset9.running_mean_->repeat(g, running_mean, g.op('Constant', value_t=torch.tensor([n], dtype=torch.int64)))
A:torch.onnx.symbolic_opset9.running_var_->repeat(g, running_var, g.op('Constant', value_t=torch.tensor([n], dtype=torch.int64)))
A:torch.onnx.symbolic_opset9.input_reshaped->torch.onnx.symbolic_helper._reshape_helper(g, input, g.op('Constant', value_t=torch.LongTensor(shape)))
A:torch.onnx.symbolic_opset9.low_indices->range(0, sizedim, step)
A:torch.onnx.symbolic_opset9.hi_indices->range(size, sizedim + 1, step)
A:torch.onnx.symbolic_opset9.ndim->len(sizes)
A:torch.onnx.symbolic_opset9.perm->list(range(0, ndim))
A:torch.onnx.symbolic_opset9.indices_list->torch.onnx.symbolic_helper._unpack_list(indices_list_value)
A:torch.onnx.symbolic_opset9.accumulate->torch.onnx.symbolic_helper._parse_arg(accumulate, 'b')
A:torch.onnx.symbolic_opset9.dim_value->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset9.(expanded_index_shape, expanded_index)->torch.onnx.symbolic_helper._index_fill_reshape_helper(g, self, dim, index)
A:torch.onnx.symbolic_opset9.expanded_value->expand(g, value, expanded_index_shape, None)
A:torch.onnx.symbolic_opset9.other_dtype->torch.onnx.symbolic_helper._try_get_scalar_type(other)
A:torch.onnx.symbolic_opset9.f_dtypeself_dtype->g.op('Transpose', self, perm_i=adv_idx_permute).type().scalarType()
A:torch.onnx.symbolic_opset9.exponent->g.op('Cast', exponent, to_i=sym_help.cast_pytorch_to_onnx[f_dtype])
A:torch.onnx.symbolic_opset9.pow->g.op('Pow', self, exponent)
A:torch.onnx.symbolic_opset9.min->g.op('ReduceMin', self, axes_i=[dim], keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.max->g.op('ReduceMax', self, axes_i=[dim], keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.keepdim->_parse_arg(keepdim, 'i')
A:torch.onnx.symbolic_opset9.(r, _)->g.op('Dropout', input, ratio_f=p, outputs=2)
A:torch.onnx.symbolic_opset9.feature_dropout->_unsupported_dropout('feature_dropout')
A:torch.onnx.symbolic_opset9.alpha_dropout->_unsupported_dropout('alpha_dropout')
A:torch.onnx.symbolic_opset9.feature_alpha_dropout->_unsupported_dropout('feature_alpha_dropout')
A:torch.onnx.symbolic_opset9.f->_reduce_op_symbolic('ReduceL2')
A:torch.onnx.symbolic_opset9.conv->conv1d(g, input, weight, bias, [1], [pad], [1], 1)
A:torch.onnx.symbolic_opset9.name->'_cast_{}'.format(k)
A:torch.onnx.symbolic_opset9.globals()[name]->parse_args('v', 'i')(partial(sym_help._cast_func_template, v))
A:torch.onnx.symbolic_opset9.scalar->g.op('Cast', scalar, to_i=sym_help.scalar_type_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.shape_reference->g.op('Constant', value_t=torch.LongTensor([1]))
A:torch.onnx.symbolic_opset9.t->g.op('Cast', t, to_i=sym_help.scalar_type_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.data->g.op('Transpose', data, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.sizes_->torch.onnx.symbolic_helper._maybe_get_const(sizes, 'is')
A:torch.onnx.symbolic_opset9.const_value->torch.onnx.symbolic_helper._maybe_get_const(value, 't')
A:torch.onnx.symbolic_opset9.tmp->zeros_like(g, input, dtype, layout, device)
A:torch.onnx.symbolic_opset9.fill_value->g.op('Cast', fill_value, to_i=sym_help.scalar_type_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.tensor->zeros(g, shape, dtype, layout, device)
A:torch.onnx.symbolic_opset9.step->div(g, sub(g, end, start), sub(g, steps, g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64))))
A:torch.onnx.symbolic_opset9.start_unsqueezed->torch.onnx.symbolic_helper._unsqueeze_helper(g, start, [0])
A:torch.onnx.symbolic_opset9.end_unsqueezed->torch.onnx.symbolic_helper._unsqueeze_helper(g, end, [0])
A:torch.onnx.symbolic_opset9.dim_unsqueezed->torch.onnx.symbolic_helper._unsqueeze_helper(g, dim, [0])
A:torch.onnx.symbolic_opset9.hs->hardsigmoid(g, self)
A:torch.onnx.symbolic_opset9.lambd_op->g.op('Constant', value_t=torch.FloatTensor([lambd]))
A:torch.onnx.symbolic_opset9.cond->logical_or(g, gt(g, self, lambd_op), lt(g, self, neg(g, lambd_op)))
A:torch.onnx.symbolic_opset9.gt_cond->gt(g, self, lambd_op)
A:torch.onnx.symbolic_opset9.gt_out->g.op('Where', gt_cond, sub(g, self, lambd_op), g.op('Constant', value_t=torch.FloatTensor([0])))
A:torch.onnx.symbolic_opset9.lt_cond->lt(g, self, neg(g, lambd_op))
A:torch.onnx.symbolic_opset9.lt_out->g.op('Where', lt_cond, add(g, self, lambd_op), g.op('Constant', value_t=torch.FloatTensor([0])))
A:torch.onnx.symbolic_opset9.self_sizes->torch.onnx.symbolic_helper._get_tensor_sizes(self)
A:torch.onnx.symbolic_opset9.tval->tval.item().item()
A:torch.onnx.symbolic_opset9.shape_->ones_like(g, repeats, dtype)
A:torch.onnx.symbolic_opset9.repeats_dim->torch.onnx.symbolic_helper._get_tensor_rank(repeats)
A:torch.onnx.symbolic_opset9.repeats_sizes->torch.onnx.symbolic_helper._get_tensor_sizes(repeats)
A:torch.onnx.symbolic_opset9.input_sizes->torch.onnx.symbolic_helper._get_tensor_sizes(input)
A:torch.onnx.symbolic_opset9.input_sizes_temp->torch.onnx.symbolic_helper._get_tensor_sizes(input).copy()
A:torch.onnx.symbolic_opset9.repeats->expand(g, repeats, g.op('Constant', value_t=torch.tensor([reps])), None)
A:torch.onnx.symbolic_opset9.final_splits->list()
A:torch.onnx.symbolic_opset9.r_splits->torch.onnx.symbolic_helper._repeat_interleave_split_helper(g, repeats, reps, 0)
A:torch.onnx.symbolic_opset9.i_splits->torch.onnx.symbolic_helper._repeat_interleave_split_helper(g, input, reps, dim)
A:torch.onnx.symbolic_opset9.i_split->torch.onnx.symbolic_helper._reshape_helper(g, i_split, g.op('Constant', value_t=torch.LongTensor(input_sizes)), allowzero=0)
A:torch.onnx.symbolic_opset9.r_concat->g.op('Concat', *r_concat, axis_i=0)
A:torch.onnx.symbolic_opset9.dims->torch.onnx.symbolic_helper._get_tensor_sizes(self)
A:torch.onnx.symbolic_opset9.after_view->torch.onnx.symbolic_helper._reshape_helper(g, self, g.op('Constant', value_t=torch.tensor([-1, output_channel, upscale_factor, upscale_factor, dims[2], dims[3]])), allowzero=0)
A:torch.onnx.symbolic_opset9.after_transpose->g.op('Transpose', after_view, perm_i=[0, 1, 4, 2, 5, 3])
A:torch.onnx.symbolic_opset9.variantToOnnxActivationMap->dict(zip([act_fun.lower() for act_fun in onnxActivations], onnxActivations))
A:torch.onnx.symbolic_opset9.hidden_size->torch.onnx.symbolic_helper._get_tensor_dim_size(w_hh, 1)
A:torch.onnx.symbolic_opset9.bias_concat->unused(g)
A:torch.onnx.symbolic_opset9.(weight_ih, weight_hh, bias_concat)->transform_weights(i)
A:torch.onnx.symbolic_opset9.(weight_ih, weight_hh)->transform_weights_no_bias(i)
A:torch.onnx.symbolic_opset9.(weight_ih_f, weight_hh_f, bias_f)->transform_weights(2 * i)
A:torch.onnx.symbolic_opset9.(weight_ih_b, weight_hh_b, bias_b)->transform_weights(2 * i + 1)
A:torch.onnx.symbolic_opset9.(weight_ih_f, weight_hh_f)->transform_weights_no_bias(2 * i)
A:torch.onnx.symbolic_opset9.(weight_ih_b, weight_hh_b)->transform_weights_no_bias(2 * i + 1)
A:torch.onnx.symbolic_opset9.weight_ih->g.op('Concat', weight_ih_f, weight_ih_b, axis_i=0)
A:torch.onnx.symbolic_opset9.weight_hh->g.op('Concat', weight_hh_f, weight_hh_b, axis_i=0)
A:torch.onnx.symbolic_opset9.(prev_output, h_out)->g.op('GRU', *inputs, outputs=2, hidden_size_i=hidden_size, linear_before_reset_i=1, **extra_kwargs)
A:torch.onnx.symbolic_opset9.(prev_output, h_out, c_out)->g.op('LSTM', *inputs, outputs=3, hidden_size_i=hidden_size, **extra_kwargs)
A:torch.onnx.symbolic_opset9.prev_output->g.op('Transpose', prev_output, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.hidden->torch.onnx.symbolic_helper._unpack_list(hidden)
A:torch.onnx.symbolic_opset9.(_, h_outs, c_outs)->_generic_rnn(g, 'LSTM', input, hidden, weight, has_biases, num_layers=1, dropout=0, train=0, bidirectional=False, batch_first=False)
A:torch.onnx.symbolic_opset9.gru->_one_hidden_rnn('GRU')
A:torch.onnx.symbolic_opset9.rnn_tanh->_one_hidden_rnn('RNN_TANH')
A:torch.onnx.symbolic_opset9.rnn_relu->_one_hidden_rnn('RNN_RELU')
A:torch.onnx.symbolic_opset9.like_shape->g.op('Shape', like)
A:torch.onnx.symbolic_opset9.stop->g.op('Gather', like_shape, g.op('Constant', value_t=torch.tensor(dim)), axis_i=0)
A:torch.onnx.symbolic_opset9.lengths->_cast_Int(g, lengths, False)
A:torch.onnx.symbolic_opset9.(data, lengths)->g.op('prim::PadPacked', data, batch_sizes, outputs=2)
A:torch.onnx.symbolic_opset9.shape_const->g.op('ConstantOfShape', shapes, value_t=torch.tensor([0], dtype=sym_help.scalar_type_to_pytorch_type[6]))
A:torch.onnx.symbolic_opset9.p->g.op('Sigmoid', input)
A:torch.onnx.symbolic_opset9.input_sum->torch.onnx.symbolic_helper._reducesum_helper(g, input, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.flattened->torch.onnx.symbolic_helper._reshape_helper(g, input, g.op('Constant', value_t=torch.tensor([-1])))
A:torch.onnx.symbolic_opset9.src_type->g.op('Cast', src, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()]).type().scalarType()
A:torch.onnx.symbolic_opset9.src->g.op('Cast', src, to_i=sym_help.cast_pytorch_to_onnx[self.type().scalarType()])
A:torch.onnx.symbolic_opset9.to_add->torch.onnx.symbolic_helper._scatter_helper(g, to_add, dim, index, src)
A:torch.onnx.symbolic_opset9.values->g.op('Constant', value_t=torch.LongTensor([0, 1]))
A:torch.onnx.symbolic_opset9.num_classes->g.op('Cast', num_classes, to_i=sym_help.cast_pytorch_to_onnx['Long'])
A:torch.onnx.symbolic_opset9.depth->size(g, self, g.op('Constant', value_t=torch.LongTensor([dim])))
A:torch.onnx.symbolic_opset9.mul->g.op('Mul', var, num_elements)
A:torch.onnx.symbolic_opset9.num_elements->g.op('Cast', num_elements, to_i=sym_help.cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_opset9.t_mean->g.op('ReduceMean', input, axes_i=dim, keepdims_i=1)
A:torch.onnx.symbolic_opset9.redudced_dims->g.op('Gather', redudced_dims, g.op('Constant', value_t=torch.tensor(dim)), axis_i=0)
A:torch.onnx.symbolic_opset9.sub_v->g.op('Sub', input, t_mean)
A:torch.onnx.symbolic_opset9.sqr_sub->g.op('Mul', sub_v, sub_v)
A:torch.onnx.symbolic_opset9.var->g.op('Div', mul, g.op('Sub', num_elements, one))
A:torch.onnx.symbolic_opset9.(var, _)->var_mean(g, input, *args)
A:torch.onnx.symbolic_opset9.(var, mean)->var_mean(g, input, *args)
A:torch.onnx.symbolic_opset9.range_tensor->_float_step_convert(g.op('Sub', end, start))
A:torch.onnx.symbolic_opset9.(dtype, end, start, step)->torch.onnx.symbolic_helper._arange_cast_helper(g, start=args[0], end=args[1], dtype=dtype)
A:torch.onnx.symbolic_opset9.end->torch.onnx.symbolic_helper._unsqueeze_helper(g, end, [0])
A:torch.onnx.symbolic_opset9.arange_tensor->g.op('Add', sym_help._squeeze_helper(g, nonzero(g, ones(g, range_tensor, dtype, *args[3:])), [1]), start)
A:torch.onnx.symbolic_opset9.start->torch.onnx.symbolic_helper._unsqueeze_helper(g, start, [0])
A:torch.onnx.symbolic_opset9.end_epsilon->g.op('Add', step, end)
A:torch.onnx.symbolic_opset9.mask->_cast_Bool(g, mask, False)
A:torch.onnx.symbolic_opset9.adv_idx_count->len(adv_idx_indices)
A:torch.onnx.symbolic_opset9.shape_tensor->_shape_as_tensor(g, self)
A:torch.onnx.symbolic_opset9.adv_index->g.op('Mul', indices[adv_idx_indices[i]], multiplier)
A:torch.onnx.symbolic_opset9.cum_adv_index->g.op('Add', cum_adv_index, adv_index)
A:torch.onnx.symbolic_opset9.multiplier->g.op('Mul', multiplier, dim_tensor_list[adv_idx_indices[i]])
A:torch.onnx.symbolic_opset9.cum_adv_index_shape_tensor->_shape_as_tensor(g, cum_adv_index)
A:torch.onnx.symbolic_opset9.folded_adv_idx_shape->g.op('Concat', *folded_adv_idx_shape_list, axis_i=0)
A:torch.onnx.symbolic_opset9.final_shape->g.op('Concat', cum_adv_index_shape_tensor, *[dim_tensor_list[i] for i in range(rank) if i not in adv_idx_indices], axis_i=0)
A:torch.onnx.symbolic_opset9.sqr->g.op('Mul', self, self)
A:torch.onnx.symbolic_opset9.sumsqr->torch.onnx.symbolic_helper._reducesum_helper(g, sqr, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.log_input->log(g, input)
A:torch.onnx.symbolic_opset9.batch_mul->matmul(g, batch1, batch2)
A:torch.onnx.symbolic_opset9.mul_a->mul(g, batch_mul, g.op('Cast', alpha, to_i=sym_help.cast_pytorch_to_onnx[dtype]))
A:torch.onnx.symbolic_opset9.mul_b->mul(g, self, g.op('Cast', beta, to_i=sym_help.cast_pytorch_to_onnx[dtype]))
A:torch.onnx.symbolic_opset9.out_shape->g.op('Concat', *tensors_shape, axis_i=0)
A:torch.onnx.symbolic_opset9.t_reshaped->_reshape_from_tensor(g, t, g.op('Concat', *shape_i, axis_i=0))
A:torch.onnx.symbolic_opset9.quo->g.op('Mul', div, other)
A:torch.onnx.symbolic_opset9.erf->g.op('Erf', g.op('Div', self, torch.tensor(_sqrt2, dtype=torch.double)))
A:torch.onnx.symbolic_opset9.erf_plusone->add(g, erf, g.op('Constant', value_t=torch.tensor(1, dtype=torch.double)))
A:torch.onnx.symbolic_opset9.input_rank->torch.onnx.symbolic_helper._get_tensor_rank(input)
A:torch.onnx.symbolic_opset9.norm_reshaped->g.op('InstanceNormalization', input_reshaped, weight_, bias_, epsilon_f=eps)
A:torch.onnx.symbolic_opset9.norm->torch.onnx.symbolic_helper._reshape_helper(g, norm_reshaped, g.op('Shape', input))
A:torch.onnx.symbolic_opset9.norm_v->norm(g, weight_v, 2, axes, 1)
A:torch.onnx.symbolic_opset9.self_flattened->torch.onnx.symbolic_helper._reshape_helper(g, self, g.op('Constant', value_t=torch.tensor([-1], dtype=torch.int64)))
A:torch.onnx.symbolic_opset9.diff_->sub(g, log_, input)
A:torch.onnx.symbolic_opset9.exp_->exp(g, target)
A:torch.onnx.symbolic_opset9.log_->log(g, target)
A:torch.onnx.symbolic_opset9.output_pos->mul(g, target, diff_)
A:torch.onnx.symbolic_opset9.zeros_->zeros_like(g, output_pos)
A:torch.onnx.symbolic_opset9.mask_->gt(g, target, g.op('Constant', value_t=torch.tensor(0)))
A:torch.onnx.symbolic_opset9.self_1d->torch.onnx.symbolic_helper._reshape_helper(g, self, g.op('Constant', value_t=torch.tensor([-1], dtype=torch.int64)))
A:torch.onnx.symbolic_opset9.ind->g.op('Add', ind, g.op('Constant', torch.tensor([offset])))
A:torch.onnx.symbolic_opset9.tmp_ind->g.op('Mul', tmp_ind, g.op('Constant', value_t=torch.tensor([stride])))
A:torch.onnx.symbolic_opset9.sub->g.op('Sub', hi, lo)
A:torch.onnx.symbolic_opset9.n_array->arange(g, window_length, 4, None, None, None)
A:torch.onnx.symbolic_opset9.window_length->sub(g, window_length, g.op('Constant', value_t=torch.tensor(1, dtype=torch.int)))
A:torch.onnx.symbolic_opset9.self_dim_rank->torch.onnx.symbolic_helper._get_tensor_rank(self)
A:torch.onnx.symbolic_opset9.other_dim_rank->torch.onnx.symbolic_helper._get_tensor_rank(other)
A:torch.onnx.symbolic_opset9.other_dim_size->torch.onnx.symbolic_helper._get_tensor_dim_size(other, dim)
A:torch.onnx.symbolic_opset9.self_dim_size->torch.onnx.symbolic_helper._get_tensor_dim_size(self, dim)
A:torch.onnx.symbolic_opset9.new_shape_axes->list(range(self_dim_rank))
A:torch.onnx.symbolic_opset9.new_shape->torch.onnx.symbolic_helper._slice_helper(g, self, axes=new_shape_axes, starts=new_shape_starts, ends=new_shape_ends)
A:torch.onnx.symbolic_opset9.result->g.op('Concat', *shapes, axis_i=dims[i])
A:torch.onnx.symbolic_opset9.all_tensors->torch.onnx.symbolic_helper._unpack_list(self)
A:torch.onnx.symbolic_opset9.t_with_final_shape->add(g, t_with_final_shape, t)
torch.onnx.symbolic_opset9.__and_(g,input,other)
torch.onnx.symbolic_opset9.__derive_index(g,index,start,step)
torch.onnx.symbolic_opset9.__getitem_(g,self,i)
torch.onnx.symbolic_opset9.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor,antialias)
torch.onnx.symbolic_opset9.__is_(g,self,other)
torch.onnx.symbolic_opset9.__isnot_(g,self,other)
torch.onnx.symbolic_opset9.__lshift_(g,self,other)
torch.onnx.symbolic_opset9.__not_(g,self)
torch.onnx.symbolic_opset9.__or_(g,input,other)
torch.onnx.symbolic_opset9.__range_length(g,lo,hi,step)
torch.onnx.symbolic_opset9.__rshift_(g,self,other)
torch.onnx.symbolic_opset9.__xor_(g,input,other)
torch.onnx.symbolic_opset9._adaptive_pool(name,type,tuple_fn,fn=None)
torch.onnx.symbolic_opset9._all(g,*args)
torch.onnx.symbolic_opset9._any(g,*args)
torch.onnx.symbolic_opset9._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset9._convert_padding_node(padding)
torch.onnx.symbolic_opset9._convolution(g,input,weight,bias,stride,padding,dilation,transposed,output_padding,groups,benchmark,deterministic,cudnn_enabled,allow_tf32)
torch.onnx.symbolic_opset9._dim_arange(g,like,dim)
torch.onnx.symbolic_opset9._div_rounding_mode(g,self,other,rounding_mode)
torch.onnx.symbolic_opset9._floor_divide(g,self,other)
torch.onnx.symbolic_opset9._generic_rnn(g,variant,input,initial_states,all_weights,has_biases,num_layers,dropout,train,bidirectional,batch_first=None,batch_sizes=None)
torch.onnx.symbolic_opset9._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset9._kl_div_log_target_impl(g,input,target)
torch.onnx.symbolic_opset9._kl_div_non_log_target_impl(g,input,target)
torch.onnx.symbolic_opset9._len(g,self)
torch.onnx.symbolic_opset9._list(g,self)
torch.onnx.symbolic_opset9._lstm_full(g,input,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional,batch_first)
torch.onnx.symbolic_opset9._lstm_packed(g,input,batch_sizes,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional)
torch.onnx.symbolic_opset9._max_pool(name,tuple_fn,ndims,return_indices)
torch.onnx.symbolic_opset9._maybe_cast_reduce_op_input(g,self)
torch.onnx.symbolic_opset9._one_hidden_rnn(kind)
torch.onnx.symbolic_opset9._pack_padded_sequence(g,input,lengths,batch_first)
torch.onnx.symbolic_opset9._pad_packed_sequence(g,data,batch_sizes,batch_first,padding_value,total_length)
torch.onnx.symbolic_opset9._prepare_onnx_paddings(dim,pad)
torch.onnx.symbolic_opset9._reduce_op_symbolic(onnx_op_name,allow_multi_dim_support=True)
torch.onnx.symbolic_opset9._reduce_with_dtype(onnx_op,name,allow_multi_dim_support=True)
torch.onnx.symbolic_opset9._reshape_from_tensor(g,input,shape)
torch.onnx.symbolic_opset9._sample_dirichlet(g,self,generator)
torch.onnx.symbolic_opset9._shape_as_tensor(g,input)
torch.onnx.symbolic_opset9._slice(g,input,axes,starts,ends)
torch.onnx.symbolic_opset9._standard_gamma(g,self,generator)
torch.onnx.symbolic_opset9._trunc_divide(g,self,other)
torch.onnx.symbolic_opset9._unique(g,input,sorted,return_inverse)
torch.onnx.symbolic_opset9._unique2(g,input,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset9._unsupported_dropout(name)
torch.onnx.symbolic_opset9._var_mean(g,input,dim,correction,keepdim)
torch.onnx.symbolic_opset9._weight_norm(g,weight_v,weight_g,dim)
torch.onnx.symbolic_opset9.abs(g,self)
torch.onnx.symbolic_opset9.acos(g,self)
torch.onnx.symbolic_opset9.add(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic_opset9.alias(g,self)
torch.onnx.symbolic_opset9.arange(g,*args)
torch.onnx.symbolic_opset9.argmax(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.argmin(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.as_strided(g,self,sizes,strides,offset=None)
torch.onnx.symbolic_opset9.as_tensor(g,data,dtype=None,device=None)
torch.onnx.symbolic_opset9.asin(g,self)
torch.onnx.symbolic_opset9.atan(g,self)
torch.onnx.symbolic_opset9.baddbmm(g,self,batch1,batch2,beta,alpha)
torch.onnx.symbolic_opset9.batch_norm(g,input,weight,bias,running_mean,running_var,training,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.bernoulli(g,input,generator=None,out=None)
torch.onnx.symbolic_opset9.bitwise_not(g,inp)
torch.onnx.symbolic_opset9.bmm(g,self,other)
torch.onnx.symbolic_opset9.broadcast_tensors(g,self)
torch.onnx.symbolic_opset9.cat(g,tensor_list,dim)
torch.onnx.symbolic_opset9.ceil(g,input)
torch.onnx.symbolic_opset9.clamp(g,self,min,max)
torch.onnx.symbolic_opset9.clamp_max(g,self,max)
torch.onnx.symbolic_opset9.clamp_min(g,self,min)
torch.onnx.symbolic_opset9.clone(g,input,unused_memory_format)
torch.onnx.symbolic_opset9.constant_pad_nd(g,input,padding,value)
torch.onnx.symbolic_opset9.contiguous(g,input,memory_format)
torch.onnx.symbolic_opset9.conv1d(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_opset9.conv2d(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_opset9.conv3d(g,input,weight,bias,stride,padding,dilation,groups)
torch.onnx.symbolic_opset9.conv_tbc(g,input,weight,bias,pad)
torch.onnx.symbolic_opset9.conv_transpose1d(g,input,weight,bias,stride,padding,output_padding,groups,dilation)
torch.onnx.symbolic_opset9.conv_transpose2d(g,input,weight,bias,stride,padding,output_padding,groups,dilation)
torch.onnx.symbolic_opset9.conv_transpose3d(g,input,weight,bias,stride,padding,output_padding,groups,dilation)
torch.onnx.symbolic_opset9.cos(g,self)
torch.onnx.symbolic_opset9.cosine_similarity(g,x1,x2,dim,eps)
torch.onnx.symbolic_opset9.cumsum(g,input,dim,dtype)
torch.onnx.symbolic_opset9.detach(g,input)
torch.onnx.symbolic_opset9.dim(g,self)
torch.onnx.symbolic_opset9.div(g,self,other,*args)
torch.onnx.symbolic_opset9.dot(g,self,other)
torch.onnx.symbolic_opset9.dropout(g,input,p,train)
torch.onnx.symbolic_opset9.elu(g,input,alpha,scale,input_scale)
torch.onnx.symbolic_opset9.embedding(g,weight,indices,padding_idx,scale_grad_by_freq,sparse)
torch.onnx.symbolic_opset9.embedding_bag(g,embedding_matrix,indices,offsets,scale_grad_by_freq,mode,sparse,per_sample_weights,include_last_offset,padding_idx)
torch.onnx.symbolic_opset9.empty(g,sizes,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.empty_like(g,input,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.eq(g,self,other)
torch.onnx.symbolic_opset9.erf(g,input)
torch.onnx.symbolic_opset9.exp(g,self)
torch.onnx.symbolic_opset9.expand(g,self,size,implicit)
torch.onnx.symbolic_opset9.expand_as(g,self,other)
torch.onnx.symbolic_opset9.eye(g,*args)
torch.onnx.symbolic_opset9.fill(g,self,value)
torch.onnx.symbolic_opset9.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset9.floor(g,input)
torch.onnx.symbolic_opset9.floor_divide(g,self,other)
torch.onnx.symbolic_opset9.floordiv(g,self,other)
torch.onnx.symbolic_opset9.frobenius_norm(g,self,dim=None,keepdim=False)
torch.onnx.symbolic_opset9.full(g,sizes,value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.full_like(g,input,fill_value,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.gather(g,self,dim,index,sparse_grad=False)
torch.onnx.symbolic_opset9.ge(g,input,other)
torch.onnx.symbolic_opset9.gelu(g,self)
torch.onnx.symbolic_opset9.get_pool_ceil_padding(input,kernel_size,stride,padding)
torch.onnx.symbolic_opset9.glu(g,input,dim)
torch.onnx.symbolic_opset9.group_norm(g,input,num_groups,weight,bias,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.gt(g,input,other)
torch.onnx.symbolic_opset9.gt_impl(g,input,other)
torch.onnx.symbolic_opset9.hann_window(g,window_length,periodic=True,dtype=None,layout=None,device=None,pin_memory=None,requires_grad=False)
torch.onnx.symbolic_opset9.hardshrink(g,self,lambd)
torch.onnx.symbolic_opset9.hardsigmoid(g,self)
torch.onnx.symbolic_opset9.hardswish(g,self)
torch.onnx.symbolic_opset9.hardtanh(g,self,min_val,max_val)
torch.onnx.symbolic_opset9.index(g,self,index)
torch.onnx.symbolic_opset9.index_add(g,self,dim,index,other,alpha=None)
torch.onnx.symbolic_opset9.index_copy(g,self,dim,index,source)
torch.onnx.symbolic_opset9.index_fill(g,self,dim,index,value)
torch.onnx.symbolic_opset9.index_put(g,self,indices_list_value,values,accumulate)
torch.onnx.symbolic_opset9.index_select(g,self,dim,index)
torch.onnx.symbolic_opset9.instance_norm(g,input,weight,bias,running_mean,running_var,use_input_stats,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.is_floating_point(g,self)
torch.onnx.symbolic_opset9.isnan(g,input)
torch.onnx.symbolic_opset9.item(g,self)
torch.onnx.symbolic_opset9.kl_div(g,input,target,reduction,log_target)
torch.onnx.symbolic_opset9.layer_norm(g,input,normalized_shape,weight,bias,eps,cudnn_enable)
torch.onnx.symbolic_opset9.le(g,input,other)
torch.onnx.symbolic_opset9.leaky_relu(g,input,negative_slope,inplace=False)
torch.onnx.symbolic_opset9.linear(g,input,weight,bias)
torch.onnx.symbolic_opset9.linspace(g,start,end,steps,dtype,layout,device,pin_memory)
torch.onnx.symbolic_opset9.log(g,self)
torch.onnx.symbolic_opset9.log10(g,self)
torch.onnx.symbolic_opset9.log1p(g,self)
torch.onnx.symbolic_opset9.log2(g,self)
torch.onnx.symbolic_opset9.log_sigmoid(g,input)
torch.onnx.symbolic_opset9.log_softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset9.logical_and(g,input,other)
torch.onnx.symbolic_opset9.logical_or(g,input,other)
torch.onnx.symbolic_opset9.logical_xor(g,input,other)
torch.onnx.symbolic_opset9.logsumexp(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.lstm(g,*args)
torch.onnx.symbolic_opset9.lstm_cell(g,self,hidden,w_ih,w_hh,b_ih,b_hh)
torch.onnx.symbolic_opset9.lt(g,input,other)
torch.onnx.symbolic_opset9.lt_impl(g,input,other)
torch.onnx.symbolic_opset9.masked_fill(g,self,mask,value)
torch.onnx.symbolic_opset9.matmul(g,self,other)
torch.onnx.symbolic_opset9.max(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset9.meshgrid(g,tensor_list,indexing:Optional[str]=None)
torch.onnx.symbolic_opset9.min(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset9.mish(g,input)
torch.onnx.symbolic_opset9.mm(g,self,other)
torch.onnx.symbolic_opset9.mul(g,self,other)
torch.onnx.symbolic_opset9.multinomial(g,input,num_samples,replacement=False,generator=None)
torch.onnx.symbolic_opset9.mv(g,self,vec)
torch.onnx.symbolic_opset9.narrow(g,input,dim,start,length)
torch.onnx.symbolic_opset9.ne(g,self,other)
torch.onnx.symbolic_opset9.neg(g,self)
torch.onnx.symbolic_opset9.new_empty(g,self,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.new_full(g,self,size,fill_value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.new_ones(g,self,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.new_zeros(g,self,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.nonzero(g,input)
torch.onnx.symbolic_opset9.nonzero_numpy(g,input,_outputs=None)
torch.onnx.symbolic_opset9.norm(g,self,p,dim,keepdim)
torch.onnx.symbolic_opset9.numel(g,self)
torch.onnx.symbolic_opset9.one_hot(g,self,num_classes)
torch.onnx.symbolic_opset9.ones(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.ones_like(g,input,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.overload_by_arg_count(fn)
torch.onnx.symbolic_opset9.permute(g,self,dims)
torch.onnx.symbolic_opset9.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic_opset9.pow(g,self,exponent)
torch.onnx.symbolic_opset9.prelu(g,self,weight)
torch.onnx.symbolic_opset9.prim_ConstantChunk(g,self,chunks,dim)
torch.onnx.symbolic_opset9.prim_ConstantSplit(g,self,split_size,dim)
torch.onnx.symbolic_opset9.prim_data(g,self)
torch.onnx.symbolic_opset9.prim_dtype(g,self)
torch.onnx.symbolic_opset9.prim_max(g,self,other)
torch.onnx.symbolic_opset9.prim_min(g,self,other=None)
torch.onnx.symbolic_opset9.prim_shape(g,self)
torch.onnx.symbolic_opset9.prim_tolist(g,input,dim_val,elem_ty_val)
torch.onnx.symbolic_opset9.prim_unchecked_cast(g,self)
torch.onnx.symbolic_opset9.rand(g,shapes,dtype,*options)
torch.onnx.symbolic_opset9.rand_like(g,self,dtype,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.randn(g,shapes,dtype,*options)
torch.onnx.symbolic_opset9.randn_like(g,self,dtype,layout=None,device=None,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.reciprocal(g,self)
torch.onnx.symbolic_opset9.reflection_pad(g,input,padding)
torch.onnx.symbolic_opset9.relu(g,input)
torch.onnx.symbolic_opset9.relu6(g,input)
torch.onnx.symbolic_opset9.remainder(g,input,other)
torch.onnx.symbolic_opset9.repeat(g,self,repeats)
torch.onnx.symbolic_opset9.repeat_interleave(g,self,repeats,dim=None,output_size=None)
torch.onnx.symbolic_opset9.replication_pad(g,input,padding)
torch.onnx.symbolic_opset9.reshape(g,self,shape)
torch.onnx.symbolic_opset9.reshape_as(g,self,other)
torch.onnx.symbolic_opset9.roll(g,self,shifts,dims)
torch.onnx.symbolic_opset9.rrelu(g,input,lower,upper,training,generator)
torch.onnx.symbolic_opset9.rsqrt(g,self)
torch.onnx.symbolic_opset9.rsub(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.scalar_tensor(g,scalar,dtype,*options)
torch.onnx.symbolic_opset9.scatter(g,self,dim,index,src)
torch.onnx.symbolic_opset9.scatter_add(g,self,dim,index,src)
torch.onnx.symbolic_opset9.select(g,self,dim,index)
torch.onnx.symbolic_opset9.selu(g,input)
torch.onnx.symbolic_opset9.sigmoid(g,self)
torch.onnx.symbolic_opset9.sign(g,self)
torch.onnx.symbolic_opset9.silu(g,input)
torch.onnx.symbolic_opset9.sin(g,self)
torch.onnx.symbolic_opset9.size(g,self,dim=None)
torch.onnx.symbolic_opset9.slice(g,self,*args)
torch.onnx.symbolic_opset9.softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset9.softplus(g,self,beta,threshold)
torch.onnx.symbolic_opset9.softshrink(g,self,lambd)
torch.onnx.symbolic_opset9.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset9.split(g,self,split_size_or_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset9.split_with_sizes(g,self,split_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset9.sqrt(g,self)
torch.onnx.symbolic_opset9.square(g,self)
torch.onnx.symbolic_opset9.squeeze(g,self,dim=None)
torch.onnx.symbolic_opset9.stack(g,tensor_list,dim)
torch.onnx.symbolic_opset9.std(g,input,*args)
torch.onnx.symbolic_opset9.std_mean(g,input,*args)
torch.onnx.symbolic_opset9.sub(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.t(g,self)
torch.onnx.symbolic_opset9.take(g,self,index)
torch.onnx.symbolic_opset9.tan(g,self)
torch.onnx.symbolic_opset9.tanh(g,self)
torch.onnx.symbolic_opset9.tanhshrink(g,self)
torch.onnx.symbolic_opset9.tensor(g,data,dtype=None,device=None,requires_grad=False)
torch.onnx.symbolic_opset9.threshold(g,self,threshold,value)
torch.onnx.symbolic_opset9.to(g,self,*args)
torch.onnx.symbolic_opset9.topk(g,self,k,dim,largest,sorted,out=None)
torch.onnx.symbolic_opset9.transpose(g,self,dim0,dim1)
torch.onnx.symbolic_opset9.true_divide(g,self,other)
torch.onnx.symbolic_opset9.type_as(g,self,other)
torch.onnx.symbolic_opset9.unbind(g,self,dim=0,_outputs=None)
torch.onnx.symbolic_opset9.unfold(g,input,dimension,size,step)
torch.onnx.symbolic_opset9.unsafe_chunk(g,self,chunks,dim,_outputs=None)
torch.onnx.symbolic_opset9.unsafe_split(g,self,split_size_or_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset9.unsafe_split_with_sizes(g,self,split_sizes,dim,_outputs=None)
torch.onnx.symbolic_opset9.unsqueeze(g,self,dim)
torch.onnx.symbolic_opset9.unused(g)
torch.onnx.symbolic_opset9.var(g,input,*args)
torch.onnx.symbolic_opset9.var_mean(g,input,*args)
torch.onnx.symbolic_opset9.view(g,self,size)
torch.onnx.symbolic_opset9.view_as(g,self,other)
torch.onnx.symbolic_opset9.where(g,condition,self=None,other=None,_outputs=None)
torch.onnx.symbolic_opset9.wrap_logical_op_with_cast_to(to_type)
torch.onnx.symbolic_opset9.wrap_logical_op_with_cast_to_and_from(to_type)
torch.onnx.symbolic_opset9.wrap_logical_op_with_negation(func)
torch.onnx.symbolic_opset9.zeros(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.zeros_like(g,input,dtype=None,layout=None,device=None,pin_memory=False,memory_format=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/__init__.py----------------------------------------
A:torch.onnx.__init__.result->torch.onnx.utils._export(*args, **kwargs)
torch.onnx.__init__.CheckerError(Exception)
torch.onnx.__init__.ExportTypes
torch.onnx.__init__._export(*args,**kwargs)
torch.onnx.__init__._optimize_trace(graph,operator_export_type)
torch.onnx.__init__._run_symbolic_function(*args,**kwargs)
torch.onnx.__init__._run_symbolic_method(*args,**kwargs)
torch.onnx.__init__.export(model,args,f,export_params=True,verbose=False,training=TrainingMode.EVAL,input_names=None,output_names=None,operator_export_type=None,opset_version=None,do_constant_folding=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,export_modules_as_functions=False)
torch.onnx.__init__.export_to_pretty_string(*args,**kwargs)->str
torch.onnx.__init__.is_in_onnx_export()
torch.onnx.__init__.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)
torch.onnx.__init__.select_model_mode_for_export(model,mode)
torch.onnx.__init__.unregister_custom_op_symbolic(symbolic_name,opset_version)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset10.py----------------------------------------
A:torch.onnx.symbolic_opset10.out->torch.onnx.symbolic_opset9.true_divide(g, self, other)
A:torch.onnx.symbolic_opset10.div->g.op('Div', self, other)
A:torch.onnx.symbolic_opset10.zero->g.op('Constant', value_t=torch.tensor(0, dtype=torch.int64))
A:torch.onnx.symbolic_opset10.negative->g.op('Xor', g.op('Less', self, zero), g.op('Less', other, zero))
A:torch.onnx.symbolic_opset10.mod->g.op('Mod', self, other, fmod_i=0)
A:torch.onnx.symbolic_opset10.fixup_mask->g.op('And', negative, g.op('Not', g.op('Equal', mod, zero)))
A:torch.onnx.symbolic_opset10.one->g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64))
A:torch.onnx.symbolic_opset10.fixup->g.op('Sub', div, one)
A:torch.onnx.symbolic_opset10.kwargs['dilations_i']->tuple_fn(dilation)
A:torch.onnx.symbolic_opset10.(r, indices)->g.op('MaxPool', input, outputs=2, **kwargs)
A:torch.onnx.symbolic_opset10.(_, flattened_indices)->g.op('MaxPool', input, outputs=2, kernel_shape_i=[1 for _ in range(ndims)], strides_i=[1 for _ in range(ndims)])
A:torch.onnx.symbolic_opset10.s->torch.onnx.symbolic_helper._slice_helper(g, flattened_indices, axes=[2 + i for i in range(ndims)], starts=tuple_fn(0), ends=tuple_fn(1))
A:torch.onnx.symbolic_opset10.indices->sub(g, indices, s)
A:torch.onnx.symbolic_opset10.r->g.op('MaxPool', input, outputs=1, **kwargs)
A:torch.onnx.symbolic_opset10.max_pool1d->_max_pool('max_pool1d', _single, 1, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool2d->_max_pool('max_pool2d', _pair, 2, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool3d->_max_pool('max_pool3d', _triple, 3, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool1d_with_indices->_max_pool('max_pool1d_with_indices', _single, 1, return_indices=True)
A:torch.onnx.symbolic_opset10.max_pool2d_with_indices->_max_pool('max_pool2d_with_indices', _pair, 2, return_indices=True)
A:torch.onnx.symbolic_opset10.max_pool3d_with_indices->_max_pool('max_pool3d_with_indices', _triple, 3, return_indices=True)
A:torch.onnx.symbolic_opset10.padding->torch.onnx.symbolic_helper._avgpool_helper(tuple_fn, padding, kernel_size, stride, divisor_override, name)
A:torch.onnx.symbolic_opset10.input->g.op('Pad', input, pads_i=((0,) * 2 + padding) * 2, mode_s='constant', value_f=0.0)
A:torch.onnx.symbolic_opset10.output->g.op('Concat', *list_, axis_i=0)
A:torch.onnx.symbolic_opset10.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset10.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset10.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset10.(scales, align_corners)->torch.onnx.symbolic_helper._get_interpolate_attributes(g, interpolate_mode, args)
A:torch.onnx.symbolic_opset10.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset10.scales->torch.onnx.symbolic_helper._interpolate_size_to_scales(g, input, output_size, dim)
A:torch.onnx.symbolic_opset10.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset10.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset10.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset10.(scales, mode)->torch.onnx.symbolic_helper._interpolate_get_scales_and_mode(g, input, size, scale_factor, mode, align_corners)
A:torch.onnx.symbolic_opset10.starts->g.op('Constant', value_t=torch.tensor(starts))
A:torch.onnx.symbolic_opset10.ends->g.op('Constant', value_t=torch.tensor(ends))
A:torch.onnx.symbolic_opset10.axes->g.op('Constant', value_t=torch.tensor(axes))
A:torch.onnx.symbolic_opset10.steps->g.op('Constant', value_t=torch.tensor(steps))
A:torch.onnx.symbolic_opset10.step->torch.onnx.symbolic_helper._parse_arg(step, 'i')
A:torch.onnx.symbolic_opset10.start->g.op('Constant', value_t=torch.tensor(0))
A:torch.onnx.symbolic_opset10.end->g.op('Constant', value_t=torch.tensor(9223372036854775807))
A:torch.onnx.symbolic_opset10.offsets_dim_0->torch.onnx.symbolic_helper._get_tensor_dim_size(offsets, 0)
A:torch.onnx.symbolic_opset10.offsets_extended->g.op('Concat', *offsets_extended, axis_i=0)
A:torch.onnx.symbolic_opset10.start_->torch.onnx.symbolic_helper._unsqueeze_helper(g, select(g, offsets_extended, torch.tensor(0), torch.tensor(i)), [0])
A:torch.onnx.symbolic_opset10.end_->torch.onnx.symbolic_helper._unsqueeze_helper(g, select(g, offsets_extended, torch.tensor(0), torch.tensor(i + 1)), [0])
A:torch.onnx.symbolic_opset10.axes_->g.op('Constant', value_t=torch.tensor([0]))
A:torch.onnx.symbolic_opset10.indices_row->g.op('Slice', indices, start_, end_, axes_)
A:torch.onnx.symbolic_opset10.embeddings->torch.onnx.symbolic_helper._unsqueeze_helper(g, embeddings, [0])
A:torch.onnx.symbolic_opset10.per_sample_weights_row->torch.onnx.symbolic_helper._unsqueeze_helper(g, per_sample_weights_row, [1])
A:torch.onnx.symbolic_opset10.zero_point->torch.tensor(zero_point, dtype=zero_point_dtype)
A:torch.onnx.symbolic_opset10.inf_node->isinf(g, input)
A:torch.onnx.symbolic_opset10.nan_node->isnan(g, input)
torch.onnx.symbolic_opset10.__interpolate(g,input,size,scale_factor,mode,align_corners,recompute_scale_factor,antialias)
torch.onnx.symbolic_opset10._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset10._div_rounding_mode(g,self,other,rounding_mode)
torch.onnx.symbolic_opset10._floor_divide(g,self,other)
torch.onnx.symbolic_opset10._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset10._max_pool(name,tuple_fn,ndims,return_indices)
torch.onnx.symbolic_opset10._slice(g,input,axes,starts,ends,steps=None,dynamic_slice=False)
torch.onnx.symbolic_opset10.div(g,self,other,*args)
torch.onnx.symbolic_opset10.embedding_bag(g,embedding_matrix,indices,offsets,scale_grad_by_freq,mode,sparse,per_sample_weights,include_last_offset,padding_idx)
torch.onnx.symbolic_opset10.fake_quantize_per_tensor_affine(g,inputs,scale,zero_point,quant_min=-128,quant_max=127)
torch.onnx.symbolic_opset10.flip(g,input,dims)
torch.onnx.symbolic_opset10.fmod(g,input,other)
torch.onnx.symbolic_opset10.isfinite(g,input)
torch.onnx.symbolic_opset10.isinf(g,input)
torch.onnx.symbolic_opset10.slice(g,self,*args)
torch.onnx.symbolic_opset10.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset10.topk(g,self,k,dim,largest,sorted,out=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset14.py----------------------------------------
A:torch.onnx.symbolic_opset14.k->g.op('Constant', value_t=torch.tensor(diagonal, dtype=torch.int64))
A:torch.onnx.symbolic_opset14.(weight, bias, running_mean, running_var)->torch.onnx.symbolic_helper._batchnorm_helper(g, input, weight, bias, running_mean, running_var)
A:torch.onnx.symbolic_opset14.out->g.op('BatchNormalization', input, weight, bias, running_mean, running_var, epsilon_f=eps, momentum_f=1 - momentum, training_mode_i=0 if not training else 1, outputs=1 if not training else 3)
torch.onnx.symbolic_opset14.batch_norm(g,input,weight,bias,running_mean,running_var,training,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset14.hardswish(g,self)
torch.onnx.symbolic_opset14.reshape(g,self,shape)
torch.onnx.symbolic_opset14.tril(g,self,diagonal,out=None)
torch.onnx.symbolic_opset14.triu(g,self,diagonal,out=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset7.py----------------------------------------
A:torch.onnx.symbolic_opset7.vars()[block_listed_op]->_block_list_in_opset(block_listed_op)
torch.onnx.symbolic_opset7.max(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset7.min(g,self,dim_or_y=None,keepdim=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/onnx/symbolic_opset12.py----------------------------------------
A:torch.onnx.symbolic_opset12.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
A:torch.onnx.symbolic_opset12.other->g.op('Cast', other, to_i=sym_help.cast_pytorch_to_onnx[input.type().scalarType()])
A:torch.onnx.symbolic_opset12.p->g.op('Constant', value_t=torch.tensor([1]))
A:torch.onnx.symbolic_opset12.t->g.op('Constant', value_t=torch.tensor(True))
A:torch.onnx.symbolic_opset12.(r, _)->g.op('Dropout', input, p, t, outputs=2)
A:torch.onnx.symbolic_opset12.reduction->torch.onnx.symbolic_helper._maybe_get_const(reduction, 'i')
A:torch.onnx.symbolic_opset12.ignore_index->torch.onnx.symbolic_helper._maybe_get_const(ignore_index, 'i')
A:torch.onnx.symbolic_opset12.nllloss->g.op('NegativeLogLikelihoodLoss', self, target, weight, reduction_s=reduction, ignore_index_i=ignore_index)
A:torch.onnx.symbolic_opset12.label_smoothing->torch.onnx.symbolic_helper._maybe_get_const(label_smoothing, 'f')
A:torch.onnx.symbolic_opset12.celoss->g.op('SoftmaxCrossEntropyLoss', self, target, weight, reduction_s=reduction, ignore_index_i=ignore_index)
A:torch.onnx.symbolic_opset12.sig_x->sigmoid(g, input)
A:torch.onnx.symbolic_opset12.log_sig_x->log(g, sig_x)
A:torch.onnx.symbolic_opset12.sub_1_x->sub(g, p, sig_x)
A:torch.onnx.symbolic_opset12.sub_1_y->sub(g, p, target)
A:torch.onnx.symbolic_opset12.log_1_x->log(g, sub_1_x)
A:torch.onnx.symbolic_opset12.output->einsum(g, 'ij,jk->ik', g.op('prim::ListConstruct', *[output_a, output_b]))
A:torch.onnx.symbolic_opset12.alpha->torch.onnx.symbolic_helper._maybe_get_const(alpha, 'f')
A:torch.onnx.symbolic_opset12.self->g.op('Cast', self, to_i=sym_help.cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_opset12.out->g.op('Celu', self, alpha_f=alpha)
A:torch.onnx.symbolic_opset12.flattened->torch.onnx.symbolic_helper._reshape_helper(g, input, g.op('Constant', value_t=torch.tensor([-1])))
A:torch.onnx.symbolic_opset12.dim->_parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset12.keepdim->_parse_arg(keepdim, 'i')
A:torch.onnx.symbolic_opset12.const_size->torch.onnx.symbolic_helper._maybe_get_const(size, 'i')
A:torch.onnx.symbolic_opset12.const_step->torch.onnx.symbolic_helper._maybe_get_const(step, 'i')
A:torch.onnx.symbolic_opset12.sizedim->torch.onnx.symbolic_helper._get_tensor_dim_size(input, dimension)
A:torch.onnx.symbolic_opset12.low_start->g.op('Constant', value_t=torch.tensor(0))
A:torch.onnx.symbolic_opset12.low_end->g.op('Constant', value_t=torch.tensor(sizedim))
A:torch.onnx.symbolic_opset12.hi_end->g.op('Constant', value_t=torch.tensor(sizedim + 1))
A:torch.onnx.symbolic_opset12.low_indices->g.op('Range', low_start, low_end, step)
A:torch.onnx.symbolic_opset12.hi_indices->g.op('Range', size, hi_end, step)
A:torch.onnx.symbolic_opset12.low_size->torch.onnx.symbolic_helper._size_helper(g, low_indices, g.op('Constant', value_t=torch.tensor(0)))
A:torch.onnx.symbolic_opset12.hi_size->torch.onnx.symbolic_helper._size_helper(g, hi_indices, g.op('Constant', value_t=torch.tensor(0)))
A:torch.onnx.symbolic_opset12.ndim->torch.onnx.symbolic_helper._get_tensor_rank(input)
A:torch.onnx.symbolic_opset12.perm->list(range(0, ndim))
A:torch.onnx.symbolic_opset12.loop_condition->g.op('Cast', loop_condition, to_i=9)
A:torch.onnx.symbolic_opset12.loop_len->g.op('Min', low_size, hi_size)
A:torch.onnx.symbolic_opset12.loop->g.op('Loop', loop_len, loop_condition)
A:torch.onnx.symbolic_opset12.loop_block->_add_block(loop.node())
A:torch.onnx.symbolic_opset12.block_input_iter->_add_input_to_block(loop_block)
A:torch.onnx.symbolic_opset12.cond->_add_input_to_block(loop_block)
A:torch.onnx.symbolic_opset12.starts->torch.onnx.symbolic_helper._unsqueeze_helper(loop_block, starts, [0])
A:torch.onnx.symbolic_opset12.ends->torch.onnx.symbolic_helper._unsqueeze_helper(loop_block, ends, [0])
A:torch.onnx.symbolic_opset12.axes->_add_block(loop.node()).op('Constant', value_t=torch.tensor([2]))
A:torch.onnx.symbolic_opset12.stack->_add_block(loop.node()).op('Slice', input, starts, ends, axes)
A:torch.onnx.symbolic_opset12.unsqueeze->torch.onnx.symbolic_helper._unsqueeze_helper(loop_block, loop_block.op('Transpose', stack, perm_i=perm), [dimension])
A:torch.onnx.symbolic_opset12.concat->_add_block(loop.node()).op('Concat', *unsqueeze_list, axis_i=0)
A:torch.onnx.symbolic_opset12.cond_out->_add_block(loop.node()).op('Cast', loop_condition, to_i=9)
A:torch.onnx.symbolic_opset12.loop_output->g.op('Loop', loop_len, loop_condition).node().output()
A:torch.onnx.symbolic_opset12.transpose->g.op('Transpose', loop_output, perm_i=perm)
A:torch.onnx.symbolic_opset12.squeeze->torch.onnx.symbolic_helper._squeeze_helper(g, transpose, [0])
A:torch.onnx.symbolic_opset12.dim_count_a->torch.onnx.symbolic_helper._get_tensor_rank(input_a)
A:torch.onnx.symbolic_opset12.dim_count_b->torch.onnx.symbolic_helper._get_tensor_rank(input_b)
A:torch.onnx.symbolic_opset12.new_input_a->permute(g, input_a, left_dims_a + dims_a)
A:torch.onnx.symbolic_opset12.new_input_b->permute(g, input_b, dims_b + left_dims_b)
A:torch.onnx.symbolic_opset12.input_shape->g.op('Shape', output_b)
A:torch.onnx.symbolic_opset12.left_sizes_a->torch.onnx.symbolic_helper._slice_helper(g, input_shape, axes=[0], starts=[0], ends=[len(left_dims_a)])
A:torch.onnx.symbolic_opset12.output_a->_reshape_from_tensor(g, new_input_a, shape_sizes)
A:torch.onnx.symbolic_opset12.slices->torch.onnx.symbolic_helper._slice_helper(g, input_shape, axes=[0], starts=[-1], ends=[maxsize])
A:torch.onnx.symbolic_opset12.left_sizes_b->torch.onnx.symbolic_helper._slice_helper(g, input_shape, axes=[0], starts=[len(dims_b)], ends=[maxsize])
A:torch.onnx.symbolic_opset12.output_b->_reshape_from_tensor(g, new_input_b, shape_sizes)
torch.onnx.symbolic_opset12.argmax(g,input,dim,keepdim)
torch.onnx.symbolic_opset12.argmin(g,input,dim,keepdim)
torch.onnx.symbolic_opset12.binary_cross_entropy_with_logits(g,input,target,weight,pos_weight,reduction)
torch.onnx.symbolic_opset12.celu(g,self,alpha)
torch.onnx.symbolic_opset12.cross_entropy_loss(g,self,target,weight,reduction,ignore_index,label_smoothing)
torch.onnx.symbolic_opset12.dropout(g,input,p,train)
torch.onnx.symbolic_opset12.einsum(g,equation,tensor_list)
torch.onnx.symbolic_opset12.einsum_helper(g,equation,tensors)
torch.onnx.symbolic_opset12.ge(g,input,other)
torch.onnx.symbolic_opset12.le(g,input,other)
torch.onnx.symbolic_opset12.nll_loss(g,self,target,weight,reduction,ignore_index)
torch.onnx.symbolic_opset12.nll_loss2d(g,self,target,weight,reduction,ignore_index)
torch.onnx.symbolic_opset12.nll_loss_nd(g,self,target,weight,reduction,ignore_index)
torch.onnx.symbolic_opset12.outer(g,input,other)
torch.onnx.symbolic_opset12.pow(g,self,exponent)
torch.onnx.symbolic_opset12.tensordot(g,input_a,input_b,dims_a,dims_b,out=None)
torch.onnx.symbolic_opset12.unfold(g,input,dimension,size,step)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rendezvous.py----------------------------------------
A:torch.distributed.rendezvous.result->urlparse(url)
A:torch.distributed.rendezvous.url->urlunparse(result)
A:torch.distributed.rendezvous.path->os.path.normpath(path)
A:torch.distributed.rendezvous.query->dict((pair.split('=') for pair in filter(None, result.query.split('&'))))
A:torch.distributed.rendezvous.rank->int(_get_env_or_raise('RANK'))
A:torch.distributed.rendezvous.world_size->int(_get_env_or_raise('WORLD_SIZE'))
A:torch.distributed.rendezvous.store->_create_c10d_store(master_addr, master_port, rank, world_size, timeout)
A:torch.distributed.rendezvous.tcp_store->TCPStore(hostname, port, world_size, False, timeout)
A:torch.distributed.rendezvous.env_val->os.environ.get(env_var, None)
A:torch.distributed.rendezvous.master_addr->_get_env_or_raise('MASTER_ADDR')
A:torch.distributed.rendezvous.master_port->int(_get_env_or_raise('MASTER_PORT'))
torch.distributed.rendezvous._create_c10d_store(hostname,port,rank,world_size,timeout)->Store
torch.distributed.rendezvous._env_rendezvous_handler(url:str,timeout:timedelta=default_pg_timeout,**kwargs)
torch.distributed.rendezvous._file_rendezvous_handler(url:str,**kwargs)
torch.distributed.rendezvous._rendezvous_error(msg)
torch.distributed.rendezvous._tcp_rendezvous_handler(url:str,timeout:timedelta=default_pg_timeout,**kwargs)
torch.distributed.rendezvous._torchelastic_use_agent_store()->bool
torch.distributed.rendezvous.register_rendezvous_handler(scheme,handler)
torch.distributed.rendezvous.rendezvous(url:str,rank:int=-1,world_size:int=-1,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/run.py----------------------------------------
A:torch.distributed.run.log->get_logger()
A:torch.distributed.run.parser->get_args_parser()
A:torch.distributed.run.arr->nnodes.split(':')
A:torch.distributed.run.min_nodesmax_nodes->int(arr[0])
A:torch.distributed.run.min_nodes->int(arr[0])
A:torch.distributed.run.max_nodes->int(arr[1])
A:torch.distributed.run.num_proc->os.cpu_count()
A:torch.distributed.run.(min_nodes, max_nodes)->parse_min_max_nnodes(args.nnodes)
A:torch.distributed.run.nproc_per_node->determine_local_world_size(args.nproc_per_node)
A:torch.distributed.run.os.environ['OMP_NUM_THREADS']->str(omp_num_threads)
A:torch.distributed.run.rdzv_configs->_parse_rendezvous_config(args.rdzv_conf)
A:torch.distributed.run.rdzv_endpoint->get_rdzv_endpoint(args)
A:torch.distributed.run.config->LaunchConfig(min_nodes=min_nodes, max_nodes=max_nodes, nproc_per_node=nproc_per_node, run_id=args.rdzv_id, role=args.role, rdzv_endpoint=rdzv_endpoint, rdzv_backend=args.rdzv_backend, rdzv_configs=rdzv_configs, max_restarts=args.max_restarts, monitor_interval=args.monitor_interval, start_method=args.start_method, redirects=Std.from_str(args.redirects), tee=Std.from_str(args.tee), log_dir=args.log_dir)
A:torch.distributed.run.use_env->get_use_env(args)
A:torch.distributed.run.cmd->os.getenv('PYTHON_EXEC', sys.executable)
A:torch.distributed.run.args.rdzv_id->str(uuid.uuid4())
A:torch.distributed.run.(config, cmd, cmd_args)->config_from_args(args)
A:torch.distributed.run.args->parse_args(args)
torch.distributed.run.config_from_args(args)->Tuple[LaunchConfig, Union[Callable, str], List[str]]
torch.distributed.run.determine_local_world_size(nproc_per_node:str)
torch.distributed.run.get_args_parser()->ArgumentParser
torch.distributed.run.get_rdzv_endpoint(args)
torch.distributed.run.get_use_env(args)->bool
torch.distributed.run.main(args=None)
torch.distributed.run.parse_args(args)
torch.distributed.run.parse_min_max_nnodes(nnodes:str)
torch.distributed.run.run(args)
torch.distributed.run.run_script_path(training_script:str,*training_script_args:str)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/distributed_c10d.py----------------------------------------
A:torch.distributed.distributed_c10d.logger->logging.getLogger(__name__)
A:torch.distributed.distributed_c10d.value->name.lower()
A:torch.distributed.distributed_c10d.reduce_op->_reduce_op()
A:torch.distributed.distributed_c10d.NON_GROUP_MEMBER->object()
A:torch.distributed.distributed_c10d.store_key->'{}:{}'.format(STORE_BASED_BARRIER_PREFIX, _group_count)
A:torch.distributed.distributed_c10d.world_size->get_world_size()
A:torch.distributed.distributed_c10d.worker_count->PrefixStore(prefix, store).add(store_key, 0)
A:torch.distributed.distributed_c10d.start->time.time()
A:torch.distributed.distributed_c10d.log_time->time.time()
A:torch.distributed.distributed_c10d.default_pg->_get_default_group()
A:torch.distributed.distributed_c10d.backend->Backend(backend)
A:torch.distributed.distributed_c10d.pg->_new_process_group_helper(group_world_size, group_rank, ranks, backend, default_store, pg_options=pg_options, timeout=timeout)
A:torch.distributed.distributed_c10d.pg_store->_pg_map.get(pg, None)
A:torch.distributed.distributed_c10d.rendezvous_iterator->rendezvous(init_method, rank, world_size, timeout=timeout)
A:torch.distributed.distributed_c10d.(store, rank, world_size)->next(rendezvous_iterator)
A:torch.distributed.distributed_c10d.store->PrefixStore(prefix, store)
A:torch.distributed.distributed_c10d.group_name->str(_group_count)
A:torch.distributed.distributed_c10d.global_rank->_get_default_group().rank()
A:torch.distributed.distributed_c10d.prefix_store->PrefixStore(group_name, store)
A:torch.distributed.distributed_c10d.pg_options->torch._C._distributed_c10d.ProcessGroupNCCL.Options()
A:torch.distributed.distributed_c10d.group_dst_rank->_get_group_rank(group, dst)
A:torch.distributed.distributed_c10d.group_src_rank->_get_group_rank(group, src)
A:torch.distributed.distributed_c10d.work->group.barrier(opts=opts)
A:torch.distributed.distributed_c10d.src_rank->group.barrier(opts=opts)._source_rank()
A:torch.distributed.distributed_c10d.ret->op(tensor, peer, curr_group, tag)
A:torch.distributed.distributed_c10d.opts->BarrierOptions()
A:torch.distributed.distributed_c10d.tensor->tensor.cpu().cpu()
A:torch.distributed.distributed_c10d.f->io.BytesIO()
A:torch.distributed.distributed_c10d.byte_storage->torch.ByteStorage.from_buffer(f.getvalue())
A:torch.distributed.distributed_c10d.byte_tensor->torch.ByteTensor(byte_storage)
A:torch.distributed.distributed_c10d.local_size->local_size.to(current_device).to(current_device)
A:torch.distributed.distributed_c10d.(input_tensor, local_size)->_object_to_tensor(obj)
A:torch.distributed.distributed_c10d.current_device->torch.device('cuda', torch.cuda.current_device())
A:torch.distributed.distributed_c10d.is_nccl_backend->_check_for_nccl_backend(group)
A:torch.distributed.distributed_c10d.input_tensor->input_tensor.to(current_device).to(current_device)
A:torch.distributed.distributed_c10d.group_size->torch.cuda.device_count()
A:torch.distributed.distributed_c10d.object_sizes_tensor->object_sizes_tensor.to(current_device).to(current_device)
A:torch.distributed.distributed_c10d.max_object_size->int(max(object_size_list).item())
A:torch.distributed.distributed_c10d.coalesced_output_tensor->torch.empty(max_object_size * group_size, dtype=torch.uint8, device=current_device)
A:torch.distributed.distributed_c10d.object_list[i]->_tensor_to_object(obj_view, obj_size)
A:torch.distributed.distributed_c10d.my_rank->get_rank()
A:torch.distributed.distributed_c10d.object_gather_list[i]->_tensor_to_object(tensor, tensor_size)
A:torch.distributed.distributed_c10d.(tensor_list, size_list)->zip(*[_object_to_tensor(obj) for obj in object_list])
A:torch.distributed.distributed_c10d.object_tensor->object_tensor.to(current_device).to(current_device)
A:torch.distributed.distributed_c10d.obj_view->obj_view.cpu().cpu()
A:torch.distributed.distributed_c10d.(tensor_list, tensor_sizes)->zip(*[_object_to_tensor(obj) for obj in scatter_object_input_list])
A:torch.distributed.distributed_c10d.max_tensor_size->torch.tensor([0], dtype=torch.long)
A:torch.distributed.distributed_c10d.output_tensor->torch.empty(max_tensor_size.item(), dtype=torch.uint8)
A:torch.distributed.distributed_c10d.obj_tensor_size->torch.tensor([0], dtype=torch.long)
A:torch.distributed.distributed_c10d.scatter_object_output_list[0]->_tensor_to_object(output_tensor, obj_tensor_size)
A:torch.distributed.distributed_c10d.input->torch.view_as_real(input)
A:torch.distributed.distributed_c10d.output->torch.view_as_real(output)
A:torch.distributed.distributed_c10d.helper_pg->ProcessGroupGloo(store, rank, world_size, timeout=timeout)
A:torch.distributed.distributed_c10d.wrapped_pg->_ProcessGroupWrapper(wrapped_pg, helper_pg)
A:torch.distributed.distributed_c10d.global_world_size->_get_default_group().size()
A:torch.distributed.distributed_c10d.ranks->list(range(global_world_size))
A:torch.distributed.distributed_c10d.group_world_size->len(ranks)
A:torch.distributed.distributed_c10d.group_rank->list(range(global_world_size)).index(global_rank)
A:torch.distributed.distributed_c10d.ranks_in_subgroup->list(range(start_rank, end_rank))
A:torch.distributed.distributed_c10d.subgroup->new_group(ranks=ranks, timeout=timeout, backend=backend, pg_options=pg_options)
A:torch.distributed.distributed_c10d.rank->get_rank()
torch.distributed.Backend(cls,name:str)
torch.distributed.Backend.register_backend(cls,name,func)
torch.distributed.GroupMember(object)
torch.distributed.P2POp(self,op,tensor,peer,group=None,tag=0)
torch.distributed._all_gather_base(output_tensor,input_tensor,group=None,async_op=False)
torch.distributed._batch_p2p_manager(backend)
torch.distributed._check_for_nccl_backend(group)
torch.distributed._check_op(op)
torch.distributed._check_p2p_op_list(p2p_op_list)
torch.distributed._check_single_tensor(param,param_name)
torch.distributed._check_tensor_list(param,param_name)
torch.distributed._create_process_group_wrapper(wrapped_pg:ProcessGroup,store_prefix:str,store:Store,rank:int,world_size:int,timeout:timedelta=default_pg_timeout)
torch.distributed._get_default_group()
torch.distributed._get_default_store()
torch.distributed._get_global_rank(group,group_rank)
torch.distributed._get_group_rank(group:ProcessGroup,rank)
torch.distributed._get_group_size(group)
torch.distributed._new_process_group_helper(world_size,rank,group_ranks,backend,store,pg_options=None,group_name=None,timeout=default_pg_timeout)
torch.distributed._object_to_tensor(obj)
torch.distributed._rank_not_in_group(group:ProcessGroup)
torch.distributed._reduce_op(self)
torch.distributed._reduce_op.__getattribute__(self,key)
torch.distributed._reduce_scatter_base(output,input,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed._store_based_barrier(rank,store,timeout)
torch.distributed._tensor_to_object(tensor,tensor_size)
torch.distributed._update_default_pg(pg)
torch.distributed._validate_output_list_for_rank(my_rank,dst,gather_list)
torch.distributed._warn_not_in_group(op_name)
torch.distributed.all_gather(tensor_list,tensor,group=None,async_op=False)
torch.distributed.all_gather_coalesced(output_tensor_lists,input_tensor_list,group=None,async_op=False)
torch.distributed.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=None,async_op=False)
torch.distributed.all_gather_object(object_list,obj,group=None)
torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.all_reduce_coalesced(tensors,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.all_to_all(output_tensor_list,input_tensor_list,group=None,async_op=False)
torch.distributed.all_to_all_single(output,input,output_split_sizes=None,input_split_sizes=None,group=None,async_op=False)
torch.distributed.barrier(group=GroupMember.WORLD,async_op=False,device_ids=None)
torch.distributed.batch_isend_irecv(p2p_op_list)
torch.distributed.broadcast(tensor,src,group=None,async_op=False)
torch.distributed.broadcast_multigpu(tensor_list,src,group=None,async_op=False,src_tensor=0)
torch.distributed.broadcast_object_list(object_list,src=0,group=None,device=None)
torch.distributed.destroy_process_group(group=None)
torch.distributed.distributed_c10d.Backend(cls,name:str)
torch.distributed.distributed_c10d.Backend.__new__(cls,name:str)
torch.distributed.distributed_c10d.Backend.register_backend(cls,name,func)
torch.distributed.distributed_c10d.GroupMember(object)
torch.distributed.distributed_c10d.P2POp(self,op,tensor,peer,group=None,tag=0)
torch.distributed.distributed_c10d.P2POp.__init__(self,op,tensor,peer,group=None,tag=0)
torch.distributed.distributed_c10d._all_gather_base(output_tensor,input_tensor,group=None,async_op=False)
torch.distributed.distributed_c10d._batch_p2p_manager(backend)
torch.distributed.distributed_c10d._check_for_nccl_backend(group)
torch.distributed.distributed_c10d._check_op(op)
torch.distributed.distributed_c10d._check_p2p_op_list(p2p_op_list)
torch.distributed.distributed_c10d._check_single_tensor(param,param_name)
torch.distributed.distributed_c10d._check_tensor_list(param,param_name)
torch.distributed.distributed_c10d._create_process_group_wrapper(wrapped_pg:ProcessGroup,store_prefix:str,store:Store,rank:int,world_size:int,timeout:timedelta=default_pg_timeout)
torch.distributed.distributed_c10d._get_default_group()
torch.distributed.distributed_c10d._get_default_store()
torch.distributed.distributed_c10d._get_global_rank(group,group_rank)
torch.distributed.distributed_c10d._get_group_rank(group:ProcessGroup,rank)
torch.distributed.distributed_c10d._get_group_size(group)
torch.distributed.distributed_c10d._new_process_group_helper(world_size,rank,group_ranks,backend,store,pg_options=None,group_name=None,timeout=default_pg_timeout)
torch.distributed.distributed_c10d._object_to_tensor(obj)
torch.distributed.distributed_c10d._rank_not_in_group(group:ProcessGroup)
torch.distributed.distributed_c10d._reduce_op(self)
torch.distributed.distributed_c10d._reduce_op.__getattribute__(self,key)
torch.distributed.distributed_c10d._reduce_op.__init__(self)
torch.distributed.distributed_c10d._reduce_scatter_base(output,input,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d._store_based_barrier(rank,store,timeout)
torch.distributed.distributed_c10d._tensor_to_object(tensor,tensor_size)
torch.distributed.distributed_c10d._update_default_pg(pg)
torch.distributed.distributed_c10d._validate_output_list_for_rank(my_rank,dst,gather_list)
torch.distributed.distributed_c10d._warn_not_in_group(op_name)
torch.distributed.distributed_c10d.all_gather(tensor_list,tensor,group=None,async_op=False)
torch.distributed.distributed_c10d.all_gather_coalesced(output_tensor_lists,input_tensor_list,group=None,async_op=False)
torch.distributed.distributed_c10d.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=None,async_op=False)
torch.distributed.distributed_c10d.all_gather_object(object_list,obj,group=None)
torch.distributed.distributed_c10d.all_reduce(tensor,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d.all_reduce_coalesced(tensors,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d.all_to_all(output_tensor_list,input_tensor_list,group=None,async_op=False)
torch.distributed.distributed_c10d.all_to_all_single(output,input,output_split_sizes=None,input_split_sizes=None,group=None,async_op=False)
torch.distributed.distributed_c10d.barrier(group=GroupMember.WORLD,async_op=False,device_ids=None)
torch.distributed.distributed_c10d.batch_isend_irecv(p2p_op_list)
torch.distributed.distributed_c10d.broadcast(tensor,src,group=None,async_op=False)
torch.distributed.distributed_c10d.broadcast_multigpu(tensor_list,src,group=None,async_op=False,src_tensor=0)
torch.distributed.distributed_c10d.broadcast_object_list(object_list,src=0,group=None,device=None)
torch.distributed.distributed_c10d.destroy_process_group(group=None)
torch.distributed.distributed_c10d.gather(tensor,gather_list=None,dst=0,group=None,async_op=False)
torch.distributed.distributed_c10d.gather_object(obj,object_gather_list=None,dst=0,group=None)
torch.distributed.distributed_c10d.get_backend(group=None)
torch.distributed.distributed_c10d.get_rank(group=None)
torch.distributed.distributed_c10d.get_world_size(group=None)
torch.distributed.distributed_c10d.group(object)
torch.distributed.distributed_c10d.init_process_group(backend,init_method=None,timeout=default_pg_timeout,world_size=-1,rank=-1,store=None,group_name='',pg_options=None)
torch.distributed.distributed_c10d.irecv(tensor,src=None,group=None,tag=0)
torch.distributed.distributed_c10d.is_gloo_available()
torch.distributed.distributed_c10d.is_initialized()
torch.distributed.distributed_c10d.is_mpi_available()
torch.distributed.distributed_c10d.is_nccl_available()
torch.distributed.distributed_c10d.is_torchelastic_launched()
torch.distributed.distributed_c10d.isend(tensor,dst,group=None,tag=0)
torch.distributed.distributed_c10d.monitored_barrier(group=GroupMember.WORLD,timeout=None,wait_all_ranks=False)
torch.distributed.distributed_c10d.new_group(ranks=None,timeout=default_pg_timeout,backend=None,pg_options=None)
torch.distributed.distributed_c10d.new_subgroups(group_size=None,group=None,timeout=default_pg_timeout,backend=None,pg_options=None)
torch.distributed.distributed_c10d.new_subgroups_by_enumeration(ranks_per_subgroup_list,timeout=default_pg_timeout,backend=None,pg_options=None)
torch.distributed.distributed_c10d.recv(tensor,src=None,group=None,tag=0)
torch.distributed.distributed_c10d.reduce(tensor,dst,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=None,async_op=False,dst_tensor=0)
torch.distributed.distributed_c10d.reduce_scatter(output,input_list,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d.reduce_scatter_multigpu(output_tensor_list,input_tensor_lists,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.distributed_c10d.scatter(tensor,scatter_list=None,src=0,group=None,async_op=False)
torch.distributed.distributed_c10d.scatter_object_list(scatter_object_output_list,scatter_object_input_list,src=0,group=None)
torch.distributed.distributed_c10d.send(tensor,dst,group=None,tag=0)
torch.distributed.distributed_c10d.supports_complex(reduceOp:ReduceOp)->bool
torch.distributed.gather(tensor,gather_list=None,dst=0,group=None,async_op=False)
torch.distributed.gather_object(obj,object_gather_list=None,dst=0,group=None)
torch.distributed.get_backend(group=None)
torch.distributed.get_rank(group=None)
torch.distributed.get_world_size(group=None)
torch.distributed.group(object)
torch.distributed.init_process_group(backend,init_method=None,timeout=default_pg_timeout,world_size=-1,rank=-1,store=None,group_name='',pg_options=None)
torch.distributed.irecv(tensor,src=None,group=None,tag=0)
torch.distributed.is_gloo_available()
torch.distributed.is_initialized()
torch.distributed.is_mpi_available()
torch.distributed.is_nccl_available()
torch.distributed.is_torchelastic_launched()
torch.distributed.isend(tensor,dst,group=None,tag=0)
torch.distributed.monitored_barrier(group=GroupMember.WORLD,timeout=None,wait_all_ranks=False)
torch.distributed.new_group(ranks=None,timeout=default_pg_timeout,backend=None,pg_options=None)
torch.distributed.new_subgroups(group_size=None,group=None,timeout=default_pg_timeout,backend=None,pg_options=None)
torch.distributed.new_subgroups_by_enumeration(ranks_per_subgroup_list,timeout=default_pg_timeout,backend=None,pg_options=None)
torch.distributed.recv(tensor,src=None,group=None,tag=0)
torch.distributed.reduce(tensor,dst,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=None,async_op=False,dst_tensor=0)
torch.distributed.reduce_scatter(output,input_list,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.reduce_scatter_multigpu(output_tensor_list,input_tensor_lists,op=ReduceOp.SUM,group=None,async_op=False)
torch.distributed.scatter(tensor,scatter_list=None,src=0,group=None,async_op=False)
torch.distributed.scatter_object_list(scatter_object_output_list,scatter_object_input_list,src=0,group=None)
torch.distributed.send(tensor,dst,group=None,tag=0)
torch.distributed.supports_complex(reduceOp:ReduceOp)->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/argparse_util.py----------------------------------------
A:torch.distributed.argparse_util.default->bool(int(os.environ.get(env_name, '1' if default else '0')))
torch.distributed.argparse_util.check_env(self,dest,default=False,**kwargs)
torch.distributed.argparse_util.check_env.__init__(self,dest,default=False,**kwargs)
torch.distributed.argparse_util.env(self,dest,default=None,required=False,**kwargs)
torch.distributed.argparse_util.env.__init__(self,dest,default=None,required=False,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/remote_device.py----------------------------------------
A:torch.distributed.remote_device.fields->self._worker_name.split(':')
A:torch.distributed.remote_device.self._device->torch.device(self._device)
A:torch.distributed.remote_device.self._rank->int(fields[1])
torch.distributed._remote_device(self,remote_device:Union[str,torch.device])
torch.distributed._remote_device.__eq__(self,other)
torch.distributed._remote_device.__repr__(self)
torch.distributed._remote_device._is_valid_local_device(device)
torch.distributed._remote_device.device(self)->torch.device
torch.distributed._remote_device.rank(self)->Optional[int]
torch.distributed._remote_device.worker_name(self)->Optional[str]
torch.distributed.remote_device._remote_device(self,remote_device:Union[str,torch.device])
torch.distributed.remote_device._remote_device.__eq__(self,other)
torch.distributed.remote_device._remote_device.__init__(self,remote_device:Union[str,torch.device])
torch.distributed.remote_device._remote_device.__repr__(self)
torch.distributed.remote_device._remote_device._is_valid_local_device(device)
torch.distributed.remote_device._remote_device.device(self)->torch.device
torch.distributed.remote_device._remote_device.rank(self)->Optional[int]
torch.distributed.remote_device._remote_device.worker_name(self)->Optional[str]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/launch.py----------------------------------------
A:torch.distributed.launch.logger->logging.getLogger(__name__)
A:torch.distributed.launch.parser->get_args_parser()
A:torch.distributed.launch.args->parse_args(args)
torch.distributed.launch.launch(args)
torch.distributed.launch.main(args=None)
torch.distributed.launch.parse_args(args)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/__init__.py----------------------------------------
torch.distributed.__init__.is_available()->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/constants.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/fsdp/fully_sharded_data_parallel.py----------------------------------------
A:torch.distributed.fsdp.fully_sharded_data_parallel.BACKWARD_PRE->auto()
A:torch.distributed.fsdp.fully_sharded_data_parallel.BACKWARD_POST->auto()
A:torch.distributed.fsdp.fully_sharded_data_parallel.IDLE->auto()
A:torch.distributed.fsdp.fully_sharded_data_parallel.FORWARD->auto()
A:torch.distributed.fsdp.fully_sharded_data_parallel.self.rank->self.process_group.rank()
A:torch.distributed.fsdp.fully_sharded_data_parallel.self.world_size->self.process_group.size()
A:torch.distributed.fsdp.fully_sharded_data_parallel.self.compute_device->_get_default_cuda_device(module)
A:torch.distributed.fsdp.fully_sharded_data_parallel.cpu_device->torch.device('cpu')
A:torch.distributed.fsdp.fully_sharded_data_parallel.p.data->p.data.to(self.compute_device, non_blocking=True)
A:torch.distributed.fsdp.fully_sharded_data_parallel.memo->set()
A:torch.distributed.fsdp.fully_sharded_data_parallel.buf->buf.to(device=device or self.compute_device).to(device=device or self.compute_device)
A:torch.distributed.fsdp.fully_sharded_data_parallel.p._orig_size->p.size()
A:torch.distributed.fsdp.fully_sharded_data_parallel.orig_storage->p.storage()
A:torch.distributed.fsdp.fully_sharded_data_parallel.(local_shard, num_padded)->self._get_shard(p)
A:torch.distributed.fsdp.fully_sharded_data_parallel.chunks->list(grad_flatten.chunk(self.world_size))
A:torch.distributed.fsdp.fully_sharded_data_parallel.chunk->chunks[0].new_empty(0)
A:torch.distributed.fsdp.fully_sharded_data_parallel.shard->torch.nn.functional.pad(shard, [0, num_to_pad])
A:torch.distributed.fsdp.fully_sharded_data_parallel.p._cpu_grad->torch.zeros_like(p, device=torch.device('cpu')).pin_memory()
A:torch.distributed.fsdp.fully_sharded_data_parallel.p._full_param_padded->torch.zeros(p.numel() * self.world_size, device=self.compute_device, dtype=p.dtype)
A:torch.distributed.fsdp.fully_sharded_data_parallel.self._streams['all_gather']->torch.cuda.Stream()
A:torch.distributed.fsdp.fully_sharded_data_parallel.self._streams['post_backward']->torch.cuda.Stream()
A:torch.distributed.fsdp.fully_sharded_data_parallel.outputs->_apply_to_tensors(_register_hook, outputs)
A:torch.distributed.fsdp.fully_sharded_data_parallel.self._my_fsdp_idx_in_graph->len(self._fsdp_graph_order)
A:torch.distributed.fsdp.fully_sharded_data_parallel.p_tmp->p.expand_as(p)
A:torch.distributed.fsdp.fully_sharded_data_parallel.handle->grad_acc.register_hook(functools.partial(self._post_backward_hook, p))
A:torch.distributed.fsdp.fully_sharded_data_parallel.grad_flatten->torch.flatten(param.grad)
A:torch.distributed.fsdp.fully_sharded_data_parallel.input_flattened->torch.nn.functional.pad(grad_flatten, [0, num_pad])
A:torch.distributed.fsdp.fully_sharded_data_parallel.output->torch.zeros_like(chunks[0])
A:torch.distributed.fsdp.fully_sharded_data_parallel.p_full_size->p._full_param_padded.size()
A:torch.distributed.fsdp.fully_sharded_data_parallel.current_stream->torch.cuda.current_stream()
torch.distributed.fsdp.CPUOffload
torch.distributed.fsdp.FullyShardedDataParallel(self,module:nn.Module,process_group:Optional[ProcessGroup]=None,cpu_offload:Optional[CPUOffload]=None,fsdp_auto_wrap_policy:Optional[Callable]=None,backward_prefetch:Optional[BackwardPrefetch]=None)
torch.distributed.fsdp.FullyShardedDataParallel.__getattr__(self,name:str)->Any
torch.distributed.fsdp.FullyShardedDataParallel.__getitem__(self,key:int)->Any
torch.distributed.fsdp.FullyShardedDataParallel._assert_state(self,state:Union[TrainingState_,List[TrainingState_]])->None
torch.distributed.fsdp.FullyShardedDataParallel._cast_buffers(self,device:Optional[torch.device]=None,memo:Optional[Set]=None)->None
torch.distributed.fsdp.FullyShardedDataParallel._check_wrapped(cls,begin_module,check_fn,err_fn)
torch.distributed.fsdp.FullyShardedDataParallel._free_full_params(self,params:Optional[List[Parameter]]=None)->None
torch.distributed.fsdp.FullyShardedDataParallel._get_gradient_predivide_factor(self,world_size:int)->float
torch.distributed.fsdp.FullyShardedDataParallel._get_shard(self,tensor:torch.Tensor)->Tuple[torch.Tensor, int]
torch.distributed.fsdp.FullyShardedDataParallel._init_param_attributes(self,p:Parameter)->None
torch.distributed.fsdp.FullyShardedDataParallel._lazy_init(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._need_prefetch_post_backward_hook(self)->bool
torch.distributed.fsdp.FullyShardedDataParallel._need_prefetch_pre_backward_hook(self)->bool
torch.distributed.fsdp.FullyShardedDataParallel._offload_to_cpu(self,p)
torch.distributed.fsdp.FullyShardedDataParallel._post_backward_hook(self,param:Parameter,*unused:Any)->None
torch.distributed.fsdp.FullyShardedDataParallel._prep_grads_for_backward(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._queue_wait_for_post_backward(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._rebuild_full_params(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._register_post_backward_hooks(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._register_pre_backward_hooks(self,outputs:Any)->Any
torch.distributed.fsdp.FullyShardedDataParallel._reset_lazy_init(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._set_is_root(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._setup_streams(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._shard_parameters(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._use_param_local_shard(self,params:Optional[List[Parameter]]=None)->None
torch.distributed.fsdp.FullyShardedDataParallel._wait_for_post_backward(self)->None
torch.distributed.fsdp.FullyShardedDataParallel._wait_for_previous_optim_step(self)->None
torch.distributed.fsdp.FullyShardedDataParallel.forward(self,*args:Any,**kwargs:Any)->Any
torch.distributed.fsdp.FullyShardedDataParallel.module(self)->FlattenParamsWrapper
torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch(Enum)
torch.distributed.fsdp.fully_sharded_data_parallel.CPUOffload
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel(self,module:nn.Module,process_group:Optional[ProcessGroup]=None,cpu_offload:Optional[CPUOffload]=None,fsdp_auto_wrap_policy:Optional[Callable]=None,backward_prefetch:Optional[BackwardPrefetch]=None)
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel.__getattr__(self,name:str)->Any
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel.__getitem__(self,key:int)->Any
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel.__init__(self,module:nn.Module,process_group:Optional[ProcessGroup]=None,cpu_offload:Optional[CPUOffload]=None,fsdp_auto_wrap_policy:Optional[Callable]=None,backward_prefetch:Optional[BackwardPrefetch]=None)
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._assert_state(self,state:Union[TrainingState_,List[TrainingState_]])->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._cast_buffers(self,device:Optional[torch.device]=None,memo:Optional[Set]=None)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._check_wrapped(cls,begin_module,check_fn,err_fn)
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._free_full_params(self,params:Optional[List[Parameter]]=None)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._get_gradient_predivide_factor(self,world_size:int)->float
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._get_shard(self,tensor:torch.Tensor)->Tuple[torch.Tensor, int]
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._init_param_attributes(self,p:Parameter)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._lazy_init(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._need_prefetch_post_backward_hook(self)->bool
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._need_prefetch_pre_backward_hook(self)->bool
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._offload_to_cpu(self,p)
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._post_backward_hook(self,param:Parameter,*unused:Any)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._prep_grads_for_backward(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._queue_wait_for_post_backward(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._rebuild_full_params(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._register_post_backward_hooks(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._register_pre_backward_hooks(self,outputs:Any)->Any
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._reset_lazy_init(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._set_is_root(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._setup_streams(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._shard_parameters(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._use_param_local_shard(self,params:Optional[List[Parameter]]=None)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._wait_for_post_backward(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel._wait_for_previous_optim_step(self)->None
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel.forward(self,*args:Any,**kwargs:Any)->Any
torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel.module(self)->FlattenParamsWrapper
torch.distributed.fsdp.fully_sharded_data_parallel.TrainingState_(Enum)
torch.distributed.fsdp.fully_sharded_data_parallel._alloc_storage(data:torch.Tensor,size:torch.Size)->None
torch.distributed.fsdp.fully_sharded_data_parallel._free_storage(data:torch.Tensor)->None
torch.distributed.fsdp.fully_sharded_data_parallel._get_default_cuda_device(module:nn.Module)->torch.device


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/fsdp/utils.py----------------------------------------
torch.distributed.fsdp.utils._apply_to_tensors(fn:Callable,container:Union[torch.Tensor,Dict,List,Tuple,Set])->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/fsdp/flatten_params_wrapper.py----------------------------------------
A:torch.distributed.fsdp.flatten_params_wrapper.unique_param_list->set(param_list)
A:torch.distributed.fsdp.flatten_params_wrapper.self.param_set->set()
A:torch.distributed.fsdp.flatten_params_wrapper.params->self._init_flatten_params()
A:torch.distributed.fsdp.flatten_params_wrapper.self.flat_param->torch.nn.Parameter(torch.cat([p.detach().reshape(-1) if isinstance(p, nn.Parameter) else p.reshape(-1) for p in params], 0), requires_grad=params[0].requires_grad)
A:torch.distributed.fsdp.flatten_params_wrapper.ps->self._get_param_views()
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper(self,module:nn.Module,param_list:List[nn.Parameter])
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper.__getattr__(self,name:str)->Any
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper.__getitem__(self,key:int)->Any
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper.__init__(self,module:nn.Module,param_list:List[nn.Parameter])
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper._flatten_params(self)->None
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper._get_param_views(self,external_data:Optional[Tensor]=None)->Iterator[Tensor]
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper._init_flatten_params(self)->List[nn.Parameter]
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper._unflatten_params(self)->None
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper._unflatten_params_as_views(self)->None
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper.forward(self,*inputs:Any,**kwinputs:Any)->Any
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper.metadata(self)->Tuple[List[str], List[torch.Size], List[int]]
torch.distributed.fsdp.flatten_params_wrapper.FlattenParamsWrapper.module(self)->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/fsdp/wrap.py----------------------------------------
A:torch.distributed.fsdp.wrap.num_params->sum([p.numel() for p in module.parameters()])
A:torch.distributed.fsdp.wrap.(wrapped_child, num_wrapped_params)->_recursive_wrap(module=child, auto_wrap_policy=auto_wrap_policy, wrapper_cls=wrapper_cls, **kwargs)
A:torch.distributed.fsdp.wrap.ConfigAutoWrap.wrapper_cls->cast(Callable, kwargs['wrapper_cls'])
torch.distributed.fsdp.wrap.ConfigAutoWrap(self,**kwargs:Dict[str,Any])
torch.distributed.fsdp.wrap.ConfigAutoWrap.__enter__(self)->None
torch.distributed.fsdp.wrap.ConfigAutoWrap.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)->None
torch.distributed.fsdp.wrap.ConfigAutoWrap.__init__(self,**kwargs:Dict[str,Any])
torch.distributed.fsdp.wrap.ConfigAutoWrap.disable_autowrap_context()->None
torch.distributed.fsdp.wrap.ConfigAutoWrap.enable_autowrap_context(kwargs:Any)->None
torch.distributed.fsdp.wrap._recursive_wrap(module:nn.Module,auto_wrap_policy:Callable,wrapper_cls:Callable,only_wrap_children:bool=False,**kwargs:Any)->Tuple[nn.Module, int]
torch.distributed.fsdp.wrap._wrap(module:nn.Module,wrapper_cls:Callable,**kwargs)->nn.Module
torch.distributed.fsdp.wrap.default_auto_wrap_policy(module:nn.Module,recurse:bool,unwrapped_params:int,min_num_params:int=int(100000000.0),force_leaf_modules:Optional[Set[Type[nn.Module]]]=None,exclude_wrap_modules:Optional[Set[Type[nn.Module]]]=None)->bool
torch.distributed.fsdp.wrap.enable_wrap(*,wrapper_cls:Any,**wrapper_kwargs:Any)->Generator[None, None, None]
torch.distributed.fsdp.wrap.wrap(module:nn.Module,**wrap_overrides:Any)->nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/fsdp/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_sharding_spec/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/worker.py----------------------------------------
A:torch.distributed.pipeline.sync.worker.self._grad_enabled->torch.is_grad_enabled()
A:torch.distributed.pipeline.sync.worker.task->Queue().get()
A:torch.distributed.pipeline.sync.worker.batch->Queue().get().compute()
A:torch.distributed.pipeline.sync.worker.exc_info->cast(ExcInfo, sys.exc_info())
A:torch.distributed.pipeline.sync.worker.device->normalize_device(device)
A:torch.distributed.pipeline.sync.worker.in_queue->Queue()
A:torch.distributed.pipeline.sync.worker.out_queue->Queue()
A:torch.distributed.pipeline.sync.worker.t->Thread(target=worker, args=(in_queue, out_queue, device), daemon=True)
A:torch.distributed.pipeline.sync.worker.(in_queues, out_queues)->create_workers(devices)
torch.distributed.pipeline.sync.worker.Task(self,stream:AbstractStream,*,compute:Callable[[],Batch],finalize:Optional[Callable[[Batch],None]])
torch.distributed.pipeline.sync.worker.Task.__init__(self,stream:AbstractStream,*,compute:Callable[[],Batch],finalize:Optional[Callable[[Batch],None]])
torch.distributed.pipeline.sync.worker.Task.compute(self)->Batch
torch.distributed.pipeline.sync.worker.Task.finalize(self,batch:Batch)->None
torch.distributed.pipeline.sync.worker.create_workers(devices:List[torch.device])->Tuple[List[InQueue], List[OutQueue]]
torch.distributed.pipeline.sync.worker.spawn_workers(devices:List[torch.device])->Generator[Tuple[List[InQueue], List[OutQueue]], None, None]
torch.distributed.pipeline.sync.worker.worker(in_queue:InQueue,out_queue:OutQueue,device:torch.device)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/copy.py----------------------------------------
A:torch.distributed.pipeline.sync.copy.output_stream->current_stream(get_device(next_stream))
A:torch.distributed.pipeline.sync.copy.y->x.to(get_device(prev_stream), non_blocking=True)
A:torch.distributed.pipeline.sync.copy.input_stream->current_stream(get_device(prev_stream))
torch.distributed.pipeline.sync.copy.Context
torch.distributed.pipeline.sync.copy.Copy(torch.autograd.Function)
torch.distributed.pipeline.sync.copy.Copy.backward(ctx:Context,*grad_output:Tensor)->Tuple[Optional[Tensor], ...]
torch.distributed.pipeline.sync.copy.Copy.forward(ctx:Context,prev_stream:AbstractStream,next_stream:AbstractStream,*input)->Tensors
torch.distributed.pipeline.sync.copy.Wait(torch.autograd.Function)
torch.distributed.pipeline.sync.copy.Wait.backward(ctx:Context,*grad_input:Tensor)->Tuple[Optional[Tensor], ...]
torch.distributed.pipeline.sync.copy.Wait.forward(ctx:Context,prev_stream:AbstractStream,next_stream:AbstractStream,*input)->Tensors


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/stream.py----------------------------------------
A:torch.distributed.pipeline.sync.stream.CPUStream->CPUStreamType()
A:torch.distributed.pipeline.sync.stream.tensor->tensor.new_empty([0]).set_(tensor.storage()).new_empty([0]).set_(tensor.storage())
torch.distributed.pipeline.sync.stream.CPUStreamType
torch.distributed.pipeline.sync.stream.as_cuda(stream:AbstractStream)->torch.cuda.Stream
torch.distributed.pipeline.sync.stream.current_stream(device:torch.device)->AbstractStream
torch.distributed.pipeline.sync.stream.default_stream(device:torch.device)->AbstractStream
torch.distributed.pipeline.sync.stream.get_device(stream:AbstractStream)->torch.device
torch.distributed.pipeline.sync.stream.is_cuda(stream:AbstractStream)->bool
torch.distributed.pipeline.sync.stream.new_stream(device:torch.device)->AbstractStream
torch.distributed.pipeline.sync.stream.record_stream(tensor:torch.Tensor,stream:AbstractStream)->None
torch.distributed.pipeline.sync.stream.use_device(device:torch.device)->Generator[None, None, None]
torch.distributed.pipeline.sync.stream.use_stream(stream:AbstractStream)->Generator[None, None, None]
torch.distributed.pipeline.sync.stream.wait_stream(source:AbstractStream,target:AbstractStream)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/utils.py----------------------------------------
torch.distributed.pipeline.sync.utils.partition_model(module:nn.Sequential,balance:List[int],devices:List[int]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/checkpoint.py----------------------------------------
A:torch.distributed.pipeline.sync.checkpoint.batch->Checkpointing(function, batch).checkpoint()
A:torch.distributed.pipeline.sync.checkpoint.chk->Checkpointing(function, batch)
A:torch.distributed.pipeline.sync.checkpoint.inputs->tuple(self.batch)
A:torch.distributed.pipeline.sync.checkpoint.phony->Recompute.apply(phony, self.recomputed, self.rng_states, self.function, input_atomic, *inputs)
A:torch.distributed.pipeline.sync.checkpoint.output->ctx.function(*inputs_leaf)
A:torch.distributed.pipeline.sync.checkpoint.tensor_idx->Checkpointing(function, batch).checkpoint().find_tensor_idx()
A:torch.distributed.pipeline.sync.checkpoint.(batch[tensor_idx], phony)->fork(batch[tensor_idx])
A:torch.distributed.pipeline.sync.checkpoint.batch[tensor_idx]->join(batch[tensor_idx], phony)
A:torch.distributed.pipeline.sync.checkpoint.thread_local->ThreadLocal()
A:torch.distributed.pipeline.sync.checkpoint.cpu_rng_state->torch.get_rng_state()
A:torch.distributed.pipeline.sync.checkpoint.gpu_rng_state->torch.cuda.get_rng_state(device)
A:torch.distributed.pipeline.sync.checkpoint.(cpu_rng_state, gpu_rng_state)->rng_states.pop()
A:torch.distributed.pipeline.sync.checkpoint.(output, input_leaf)->ctx.recomputed.pop()
A:torch.distributed.pipeline.sync.checkpoint.tensors->tuple([x for x in outputs if torch.is_tensor(x) and x.requires_grad])
A:torch.distributed.pipeline.sync.checkpoint.inputs_leaf->tuple((x.detach().requires_grad_(x.requires_grad) if torch.is_tensor(x) else x for x in inputs))
torch.distributed.pipeline.sync.checkpoint.Checkpoint(torch.autograd.Function)
torch.distributed.pipeline.sync.checkpoint.Checkpoint.backward(ctx:Context,*grad_output:Tensor)->Tuple[Optional[Tensor], ...]
torch.distributed.pipeline.sync.checkpoint.Checkpoint.forward(ctx:Context,phony:Tensor,recomputed:Deque[Recomputed],rng_states:Deque[RNGStates],function:Function,input_atomic:bool,*inputs)
torch.distributed.pipeline.sync.checkpoint.Checkpointing(self,function:Function,batch:Batch)
torch.distributed.pipeline.sync.checkpoint.Checkpointing.__init__(self,function:Function,batch:Batch)
torch.distributed.pipeline.sync.checkpoint.Checkpointing.checkpoint(self)->Batch
torch.distributed.pipeline.sync.checkpoint.Checkpointing.recompute(self,batch:Batch)->None
torch.distributed.pipeline.sync.checkpoint.Context
torch.distributed.pipeline.sync.checkpoint.Context.save_for_backward(self,*tensors:Tensor)->None
torch.distributed.pipeline.sync.checkpoint.Function(self,input:TensorOrTensors)
torch.distributed.pipeline.sync.checkpoint.Function.__call__(self,input:TensorOrTensors)
torch.distributed.pipeline.sync.checkpoint.Recompute(torch.autograd.Function)
torch.distributed.pipeline.sync.checkpoint.Recompute.backward(ctx:Context,*grad_output:Tensor)->Tuple[None, ...]
torch.distributed.pipeline.sync.checkpoint.Recompute.forward(ctx:Context,phony:Tensor,recomputed:Deque[Recomputed],rng_states:Deque[RNGStates],function:Function,input_atomic:bool,*inputs)->Tensor
torch.distributed.pipeline.sync.checkpoint.ThreadLocal(self)
torch.distributed.pipeline.sync.checkpoint.ThreadLocal.__init__(self)
torch.distributed.pipeline.sync.checkpoint.checkpoint(function:Function,input)
torch.distributed.pipeline.sync.checkpoint.enable_checkpointing()->Generator[None, None, None]
torch.distributed.pipeline.sync.checkpoint.enable_recomputing()->Generator[None, None, None]
torch.distributed.pipeline.sync.checkpoint.is_checkpointing()->bool
torch.distributed.pipeline.sync.checkpoint.is_recomputing()->bool
torch.distributed.pipeline.sync.checkpoint.restore_rng_states(device:torch.device,rng_states:Deque[RNGStates])->Generator[None, None, None]
torch.distributed.pipeline.sync.checkpoint.save_rng_states(device:torch.device,rng_states:Deque[RNGStates])->None
torch.distributed.pipeline.sync.is_checkpointing()->bool
torch.distributed.pipeline.sync.is_recomputing()->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/pipeline.py----------------------------------------
A:torch.distributed.pipeline.sync.pipeline.fork_from_idx->fork_from.find_tensor_idx()
A:torch.distributed.pipeline.sync.pipeline.join_to_idx->join_to.find_tensor_idx()
A:torch.distributed.pipeline.sync.pipeline.(fork_from[fork_from_idx], phony)->fork(fork_from[fork_from_idx])
A:torch.distributed.pipeline.sync.pipeline.join_to[join_to_idx]->join(join_to[join_to_idx], phony)
A:torch.distributed.pipeline.sync.pipeline.batch[:]->tuple([x.detach() if torch.is_tensor(x) and (not x.is_floating_point()) else x for x in batch])
A:torch.distributed.pipeline.sync.pipeline.(self.in_queues, self.out_queues)->create_workers(devices)
A:torch.distributed.pipeline.sync.pipeline.m->len(batches)
A:torch.distributed.pipeline.sync.pipeline.n->len(partitions)
A:torch.distributed.pipeline.sync.pipeline.chk->Checkpointing(function, batch)
A:torch.distributed.pipeline.sync.pipeline.task->Task(streams[j], compute=compute, finalize=None)
A:torch.distributed.pipeline.sync.pipeline.(ok, payload)->self.out_queues[j].get()
A:torch.distributed.pipeline.sync.pipeline.exc_info->cast(ExcInfo, payload)
A:torch.distributed.pipeline.sync.pipeline.(task, batch)->cast(Tuple[Task, Batch], payload)
torch.distributed.pipeline.sync.pipeline.Pipeline(self,partitions:List[nn.Sequential],devices:List[torch.device],copy_streams:List[List[AbstractStream]],skip_layout:SkipLayout,checkpoint_stop:int)
torch.distributed.pipeline.sync.pipeline.Pipeline.__init__(self,partitions:List[nn.Sequential],devices:List[torch.device],copy_streams:List[List[AbstractStream]],skip_layout:SkipLayout,checkpoint_stop:int)
torch.distributed.pipeline.sync.pipeline.Pipeline.compute(self,batches:List[Batch],schedule:List[Tuple[int,int]],skip_trackers:List[SkipTrackerThroughPotals])->None
torch.distributed.pipeline.sync.pipeline.Pipeline.fence(self,batches:List[Batch],schedule:List[Tuple[int,int]],skip_trackers:List[SkipTrackerThroughPotals])->None
torch.distributed.pipeline.sync.pipeline.Pipeline.run(self,batches:List[Batch])->None
torch.distributed.pipeline.sync.pipeline._clock_cycles(m:int,n:int)->Iterable[List[Tuple[int, int]]]
torch.distributed.pipeline.sync.pipeline._copy(batch:Batch,prev_stream:AbstractStream,next_stream:AbstractStream)->None
torch.distributed.pipeline.sync.pipeline._depend(fork_from:Batch,join_to:Batch)->None
torch.distributed.pipeline.sync.pipeline._wait(batch:Batch,prev_stream:AbstractStream,next_stream:AbstractStream)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/batchnorm.py----------------------------------------
A:torch.distributed.pipeline.sync.batchnorm.TModule->TypeVar('TModule', bound=nn.Module)
A:torch.distributed.pipeline.sync.batchnorm.tracked_enough->self._track(input)
A:torch.distributed.pipeline.sync.batchnorm.module_output->DeferredBatchNorm(module.num_features, module.eps, module.momentum, module.affine, chunks)
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm(self,num_features:int,eps:float=1e-05,momentum:Optional[float]=0.1,affine:bool=True,chunks:int=1)
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm.__init__(self,num_features:int,eps:float=1e-05,momentum:Optional[float]=0.1,affine:bool=True,chunks:int=1)
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm._check_input_dim(self,input:Tensor)->None
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm._commit(self)->None
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm._track(self,input:Tensor)->bool
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm.convert_deferred_batch_norm(cls,module:TModule,chunks:int=1)->TModule
torch.distributed.pipeline.sync.batchnorm.DeferredBatchNorm.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/microbatch.py----------------------------------------
A:torch.distributed.pipeline.sync.microbatch.self.atomic->torch.is_tensor(values)
A:torch.distributed.pipeline.sync.microbatch.tensors->tuple((b.tensor for b in outputs))
A:torch.distributed.pipeline.sync.microbatch.num_chunks->len(tensors)
A:torch.distributed.pipeline.sync.microbatch.output->tuple(output_buf)
A:torch.distributed.pipeline.sync.microbatch.output_type->type(outputs[0][i])
torch.distributed.pipeline.sync.NoChunk(self,inp:Tensor)
torch.distributed.pipeline.sync.NoChunk.tensor(self)
torch.distributed.pipeline.sync.microbatch.Batch(self,values:Union[List[Any],Tensor])
torch.distributed.pipeline.sync.microbatch.Batch.__getitem__(self,index:int)
torch.distributed.pipeline.sync.microbatch.Batch.__init__(self,values:Union[List[Any],Tensor])
torch.distributed.pipeline.sync.microbatch.Batch.__iter__(self)
torch.distributed.pipeline.sync.microbatch.Batch.__len__(self)->int
torch.distributed.pipeline.sync.microbatch.Batch.__repr__(self)->str
torch.distributed.pipeline.sync.microbatch.Batch.__setitem__(self,index:Union[int,slice],value)->None
torch.distributed.pipeline.sync.microbatch.Batch._setitem_by_index(self,index:int,value)->None
torch.distributed.pipeline.sync.microbatch.Batch._setitem_by_slice(self,index:slice,value)->None
torch.distributed.pipeline.sync.microbatch.Batch.call(self,function:Function)->'Batch'
torch.distributed.pipeline.sync.microbatch.Batch.find_tensor_idx(self)
torch.distributed.pipeline.sync.microbatch.Batch.get_device(self)
torch.distributed.pipeline.sync.microbatch.Batch.tensor(self)->Tensor
torch.distributed.pipeline.sync.microbatch.Batch.values(self)
torch.distributed.pipeline.sync.microbatch.NoChunk(self,inp:Tensor)
torch.distributed.pipeline.sync.microbatch.NoChunk.__init__(self,inp:Tensor)
torch.distributed.pipeline.sync.microbatch.NoChunk.tensor(self)
torch.distributed.pipeline.sync.microbatch.check(first_device,*inputs)->None
torch.distributed.pipeline.sync.microbatch.gather(outputs:List[Batch])
torch.distributed.pipeline.sync.microbatch.scatter(*inputs,chunks:int)->List[Batch]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/pipe.py----------------------------------------
A:torch.distributed.pipeline.sync.pipe.named_children->list(module.named_children())
A:torch.distributed.pipeline.sync.pipe.num_parameters->len(list(module.parameters()))
A:torch.distributed.pipeline.sync.pipe.num_child_parameters->sum((len(list(child.parameters())) for child in module.children()))
A:torch.distributed.pipeline.sync.pipe.inputs->module(inputs)
A:torch.distributed.pipeline.sync.pipe.self._device->torch.device(device)
A:torch.distributed.pipeline.sync.pipe.device->_retrieve_device(module)
A:torch.distributed.pipeline.sync.pipe.partitions->cast(List[nn.Sequential], nn.ModuleList(partitions))
A:torch.distributed.pipeline.sync.pipe.MOVING_DENIED->TypeError('denied to move parameters and buffers, because Pipe should manage device placement')
A:torch.distributed.pipeline.sync.pipe.chunks->int(chunks)
A:torch.distributed.pipeline.sync.pipe.checkpoint->str(checkpoint)
A:torch.distributed.pipeline.sync.pipe.module->batchnorm.DeferredBatchNorm.convert_deferred_batch_norm(module, chunks)
A:torch.distributed.pipeline.sync.pipe.(self.partitions, self.devices)->_split_module(module)
A:torch.distributed.pipeline.sync.pipe.self._skip_layout->inspect_skip_layout(self.partitions)
A:torch.distributed.pipeline.sync.pipe.copy_streams->self._ensure_copy_streams()
A:torch.distributed.pipeline.sync.pipe.self.pipeline->Pipeline(self.partitions, self.devices, copy_streams, self._skip_layout, checkpoint_stop)
A:torch.distributed.pipeline.sync.pipe.shift->len(partition)
A:torch.distributed.pipeline.sync.pipe.batches->microbatch.scatter(*inputs, chunks=self.chunks)
A:torch.distributed.pipeline.sync.pipe.output->microbatch.gather(batches)
torch.distributed.pipeline.sync.Pipe(self,module:nn.Sequential,chunks:int=1,checkpoint:str='except_last',deferred_batch_norm:bool=False)
torch.distributed.pipeline.sync.Pipe.__getitem__(self,index:int)->nn.Module
torch.distributed.pipeline.sync.Pipe.__iter__(self)->Iterable[nn.Module]
torch.distributed.pipeline.sync.Pipe.__len__(self)->int
torch.distributed.pipeline.sync.Pipe._ensure_copy_streams(self)->List[List[AbstractStream]]
torch.distributed.pipeline.sync.Pipe.cpu(self)->'Pipe'
torch.distributed.pipeline.sync.Pipe.cuda(self,device:Optional[Device]=None)->'Pipe'
torch.distributed.pipeline.sync.Pipe.forward(self,*inputs)->RRef
torch.distributed.pipeline.sync.Pipe.to(self,*args:Any,**kwargs:Any)->'Pipe'
torch.distributed.pipeline.sync.PipeSequential(nn.Sequential)
torch.distributed.pipeline.sync.PipeSequential.forward(self,*inputs)
torch.distributed.pipeline.sync.WithDevice(self,module:nn.Module,device:torch.device)
torch.distributed.pipeline.sync.WithDevice.device(self)
torch.distributed.pipeline.sync.WithDevice.forward(self,*args,**kwargs)
torch.distributed.pipeline.sync.WithDevice.module(self)
torch.distributed.pipeline.sync.pipe.BalanceError(ValueError)
torch.distributed.pipeline.sync.pipe.Pipe(self,module:nn.Sequential,chunks:int=1,checkpoint:str='except_last',deferred_batch_norm:bool=False)
torch.distributed.pipeline.sync.pipe.Pipe.__getitem__(self,index:int)->nn.Module
torch.distributed.pipeline.sync.pipe.Pipe.__init__(self,module:nn.Sequential,chunks:int=1,checkpoint:str='except_last',deferred_batch_norm:bool=False)
torch.distributed.pipeline.sync.pipe.Pipe.__iter__(self)->Iterable[nn.Module]
torch.distributed.pipeline.sync.pipe.Pipe.__len__(self)->int
torch.distributed.pipeline.sync.pipe.Pipe._ensure_copy_streams(self)->List[List[AbstractStream]]
torch.distributed.pipeline.sync.pipe.Pipe.cpu(self)->'Pipe'
torch.distributed.pipeline.sync.pipe.Pipe.cuda(self,device:Optional[Device]=None)->'Pipe'
torch.distributed.pipeline.sync.pipe.Pipe.forward(self,*inputs)->RRef
torch.distributed.pipeline.sync.pipe.Pipe.to(self,*args:Any,**kwargs:Any)->'Pipe'
torch.distributed.pipeline.sync.pipe.PipeSequential(nn.Sequential)
torch.distributed.pipeline.sync.pipe.PipeSequential.forward(self,*inputs)
torch.distributed.pipeline.sync.pipe.WithDevice(self,module:nn.Module,device:torch.device)
torch.distributed.pipeline.sync.pipe.WithDevice.__init__(self,module:nn.Module,device:torch.device)
torch.distributed.pipeline.sync.pipe.WithDevice.device(self)
torch.distributed.pipeline.sync.pipe.WithDevice.forward(self,*args,**kwargs)
torch.distributed.pipeline.sync.pipe.WithDevice.module(self)
torch.distributed.pipeline.sync.pipe._assemble_partition(modules:List[nn.Module])
torch.distributed.pipeline.sync.pipe._recommend_auto_balance(message:str)->str
torch.distributed.pipeline.sync.pipe._retrieve_device(module:nn.Module)->torch.device
torch.distributed.pipeline.sync.pipe._split_module(modules:nn.Sequential)->Tuple[List[nn.Sequential], List[torch.device]]
torch.distributed.pipeline.sync.pipe._verify_module(module:nn.Sequential)->None
torch.distributed.pipeline.sync.pipe._verify_splitting(module:nn.Sequential,partitions:List[nn.Sequential],devices:List[torch.device])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/dependency.py----------------------------------------
A:torch.distributed.pipeline.sync.dependency.(input, phony)->Fork.apply(input)
A:torch.distributed.pipeline.sync.dependency.phony->get_phony(input.device, requires_grad=False)
A:torch.distributed.pipeline.sync.dependency.input->Join.apply(input, phony)
torch.distributed.pipeline.sync.dependency.Fork(torch.autograd.Function)
torch.distributed.pipeline.sync.dependency.Fork.backward(ctx:'Fork',grad_input:Tensor,grad_grad:Tensor)->Tensor
torch.distributed.pipeline.sync.dependency.Fork.forward(ctx:'Fork',input:Tensor)->Tuple[Tensor, Tensor]
torch.distributed.pipeline.sync.dependency.Join(torch.autograd.Function)
torch.distributed.pipeline.sync.dependency.Join.backward(ctx:'Join',grad_input:Tensor)->Tuple[Tensor, None]
torch.distributed.pipeline.sync.dependency.Join.forward(ctx:'Join',input:Tensor,phony:Tensor)->Tensor
torch.distributed.pipeline.sync.dependency.fork(input:Tensor)->Tuple[Tensor, Tensor]
torch.distributed.pipeline.sync.dependency.join(input:Tensor,phony:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/phony.py----------------------------------------
A:torch.distributed.pipeline.sync.phony.phony->torch.empty(0, device=device, requires_grad=requires_grad)
torch.distributed.pipeline.sync.phony.get_phony(device:torch.device,*,requires_grad:bool)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/_balance/blockpartition.py----------------------------------------
A:torch.distributed.pipeline.sync._balance.blockpartition.n->len(sequence)
A:torch.distributed.pipeline.sync._balance.blockpartition.minimum->min(sequence)
A:torch.distributed.pipeline.sync._balance.blockpartition.(max_size, p)->max(leaderboard())
A:torch.distributed.pipeline.sync._balance.blockpartition.(min_size, q)->min(leaderboard())
torch.distributed.pipeline.sync._balance.blockpartition.solve(sequence:List[int],partitions:int=1)->List[List[int]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/_balance/profile.py----------------------------------------
A:torch.distributed.pipeline.sync._balance.profile.layer_copy->copy.deepcopy(layer)
A:torch.distributed.pipeline.sync._balance.profile.batch[i]->x[:1].detach().to(device).requires_grad_(x.requires_grad)
A:torch.distributed.pipeline.sync._balance.profile._batch->Batch(sample)
A:torch.distributed.pipeline.sync._balance.profile._batch[i]->x.detach().to(device).requires_grad_(x.requires_grad)
A:torch.distributed.pipeline.sync._balance.profile.begun_at->time.time()
A:torch.distributed.pipeline.sync._balance.profile.tick->time.time()
A:torch.distributed.pipeline.sync._balance.profile.batch->batch.call(layer).call(layer)
A:torch.distributed.pipeline.sync._balance.profile.backward_tensors->tuple((y for y in batch if y.requires_grad))
A:torch.distributed.pipeline.sync._balance.profile.tock->time.time()
A:torch.distributed.pipeline.sync._balance.profile.memory_before->torch.cuda.memory_allocated(device)
A:torch.distributed.pipeline.sync._balance.profile.memory_after->torch.cuda.memory_allocated(device)
A:torch.distributed.pipeline.sync._balance.profile.param_size->sum((p.storage().nbytes() for p in layer.parameters()))
torch.distributed.pipeline.sync._balance.profile.detach(batch:Batch)->None
torch.distributed.pipeline.sync._balance.profile.layerwise_sandbox(module:nn.Sequential,device:torch.device)->Generator[nn.Module, None, None]
torch.distributed.pipeline.sync._balance.profile.profile_sizes(module:nn.Sequential,input:Union[List[Any],Tensor],chunks:int,param_scale:float,device:torch.device)->List[int]
torch.distributed.pipeline.sync._balance.profile.profile_times(module:nn.Sequential,sample:Union[List[Any],Tensor],timeout:float,device:torch.device)->List[int]
torch.distributed.pipeline.sync._balance.profile_sizes(module:nn.Sequential,input:Union[List[Any],Tensor],chunks:int,param_scale:float,device:torch.device)->List[int]
torch.distributed.pipeline.sync._balance.profile_times(module:nn.Sequential,sample:Union[List[Any],Tensor],timeout:float,device:torch.device)->List[int]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/_balance/__init__.py----------------------------------------
A:torch.distributed.pipeline.sync._balance.__init__.partitioned->blockpartition.solve(cost, partitions)
A:torch.distributed.pipeline.sync._balance.__init__.times->profile_times(module, sample, timeout, torch.device(device))
A:torch.distributed.pipeline.sync._balance.__init__.sizes->profile_sizes(module, input, chunks, param_scale, torch.device(device))
torch.distributed.pipeline.sync._balance.__init__.balance_by_size(partitions:int,module:nn.Sequential,input:Union[List[Any],Tensor],*,chunks:int=1,param_scale:float=2.0,device:Device=torch.device('cuda'))->List[int]
torch.distributed.pipeline.sync._balance.__init__.balance_by_time(partitions:int,module:nn.Sequential,sample:Union[List[Any],Tensor],*,timeout:float=1.0,device:Device=torch.device('cuda'))->List[int]
torch.distributed.pipeline.sync._balance.__init__.balance_cost(cost:List[int],partitions:int)->List[int]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/skip/portal.py----------------------------------------
A:torch.distributed.pipeline.sync.skip.portal.tensor->portal.use_tensor()
A:torch.distributed.pipeline.sync.skip.portal.phony->get_phony(get_device(next_stream), requires_grad=False)
A:torch.distributed.pipeline.sync.skip.portal.grad->ctx.portal.use_grad()
A:torch.distributed.pipeline.sync.skip.portal.(portal.tensor,)->copy.Copy.forward(ctx, prev_stream, next_stream, portal.tensor)
A:torch.distributed.pipeline.sync.skip.portal.(_, _, portal.grad)->copy.Copy.backward(ctx, portal.grad)
torch.distributed.pipeline.sync.skip.portal.Context(CopyContext)
torch.distributed.pipeline.sync.skip.portal.Portal(self,tensor:Optional[Tensor],tensor_life:int)
torch.distributed.pipeline.sync.skip.portal.Portal.__init__(self,tensor:Optional[Tensor],tensor_life:int)
torch.distributed.pipeline.sync.skip.portal.Portal.blue(self)->Tensor
torch.distributed.pipeline.sync.skip.portal.Portal.check_tensor_life(self)->None
torch.distributed.pipeline.sync.skip.portal.Portal.copy(self,prev_stream:AbstractStream,next_stream:AbstractStream,phony:Tensor)->Tensor
torch.distributed.pipeline.sync.skip.portal.Portal.orange(self,phony:Tensor)->Optional[Tensor]
torch.distributed.pipeline.sync.skip.portal.Portal.put_grad(self,grad:Tensor)->None
torch.distributed.pipeline.sync.skip.portal.Portal.put_tensor(self,tensor:Optional[Tensor],tensor_life:int)->None
torch.distributed.pipeline.sync.skip.portal.Portal.use_grad(self)->Tensor
torch.distributed.pipeline.sync.skip.portal.Portal.use_tensor(self)->Optional[Tensor]
torch.distributed.pipeline.sync.skip.portal.PortalBlue(torch.autograd.Function)
torch.distributed.pipeline.sync.skip.portal.PortalBlue.backward(ctx:Context,grad_phony:Tensor)->Tuple[None, Tensor]
torch.distributed.pipeline.sync.skip.portal.PortalBlue.forward(ctx:Context,portal:Portal,tensor:Tensor)->Tensor
torch.distributed.pipeline.sync.skip.portal.PortalCopy(torch.autograd.Function)
torch.distributed.pipeline.sync.skip.portal.PortalCopy.backward(ctx:Context,grad_phony:Tensor)->Tuple[None, None, None, None]
torch.distributed.pipeline.sync.skip.portal.PortalCopy.forward(ctx:Context,portal:Portal,prev_stream:AbstractStream,next_stream:AbstractStream,phony:Tensor)->Tensor
torch.distributed.pipeline.sync.skip.portal.PortalOrange(torch.autograd.Function)
torch.distributed.pipeline.sync.skip.portal.PortalOrange.backward(ctx:Context,grad:Tensor)->Tuple[None, None]
torch.distributed.pipeline.sync.skip.portal.PortalOrange.forward(ctx:Context,portal:Portal,phony:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/skip/skippable.py----------------------------------------
A:torch.distributed.pipeline.sync.skip.skippable.T->TypeVar('T', bound='Skippable')
A:torch.distributed.pipeline.sync.skip.skippable.self.module->self.module_cls(*args, **kwargs)
A:torch.distributed.pipeline.sync.skip.skippable.ns->cast(Namespace, ns)
A:torch.distributed.pipeline.sync.skip.skippable.names->set(only)
A:torch.distributed.pipeline.sync.skip.skippable.generator->self.module(input)
A:torch.distributed.pipeline.sync.skip.skippable.op->self.module(input).send(tensor)
A:torch.distributed.pipeline.sync.skip.skippable.tensor->handle_pop(op.name)
A:torch.distributed.pipeline.sync.skip.skippable.skip_tracker->current_skip_tracker()
A:torch.distributed.pipeline.sync.skip.skippable.batch->Batch(output)
A:torch.distributed.pipeline.sync.skip.skippable.poppable_tensors[name]->current_skip_tracker().load(batch, ns, name)
A:torch.distributed.pipeline.sync.skip.skippable.output->self.dispatch(input, handle_stash, handle_pop)
A:torch.distributed.pipeline.sync.skip.skippable.comma_names->', '.join(("'%s'" % n for n in not_popped))
A:torch.distributed.pipeline.sync.skip.skippable.not_popped->poppable_tensors.keys()
A:torch.distributed.pipeline.sync.skip.skippable.stashable_names->frozenset(stash)
A:torch.distributed.pipeline.sync.skip.skippable.poppable_names->frozenset(pop)
torch.distributed.pipeline.sync.skip.pop(self,name:str)
torch.distributed.pipeline.sync.skip.skippable(stash:Iterable[str]=(),pop:Iterable[str]=())->Callable[[Type[SkippableModule]], Type[Skippable]]
torch.distributed.pipeline.sync.skip.skippable.Skippable(self,*args:Any,**kwargs:Any)
torch.distributed.pipeline.sync.skip.skippable.Skippable.__init__(self,*args:Any,**kwargs:Any)
torch.distributed.pipeline.sync.skip.skippable.Skippable.__repr__(self)->str
torch.distributed.pipeline.sync.skip.skippable.Skippable.dispatch(self,input,handle_stash:Callable[[str,Optional[Tensor]],None],handle_pop:Callable[[str],Optional[Tensor]])
torch.distributed.pipeline.sync.skip.skippable.Skippable.forward(self,input:Union[List[Any],Tensor])->TensorOrTensors
torch.distributed.pipeline.sync.skip.skippable.Skippable.isolate(self:T,ns:Namespace,*,only:Optional[Iterable[str]]=None)->T
torch.distributed.pipeline.sync.skip.skippable.Skippable.namespaced(self,name:str)->Tuple[Namespace, str]
torch.distributed.pipeline.sync.skip.skippable.Skippable.poppable(self)->Iterable[Tuple[Namespace, str]]
torch.distributed.pipeline.sync.skip.skippable.Skippable.stashable(self)->Iterable[Tuple[Namespace, str]]
torch.distributed.pipeline.sync.skip.skippable.pop(self,name:str)
torch.distributed.pipeline.sync.skip.skippable.pop.__init__(self,name:str)
torch.distributed.pipeline.sync.skip.skippable.skippable(stash:Iterable[str]=(),pop:Iterable[str]=())->Callable[[Type[SkippableModule]], Type[Skippable]]
torch.distributed.pipeline.sync.skip.skippable.stash(self,name:str,tensor:Optional[Tensor])
torch.distributed.pipeline.sync.skip.skippable.stash.__init__(self,name:str,tensor:Optional[Tensor])
torch.distributed.pipeline.sync.skip.skippable.verify_skippables(module:nn.Sequential)->None
torch.distributed.pipeline.sync.skip.stash(self,name:str,tensor:Optional[Tensor])
torch.distributed.pipeline.sync.skip.verify_skippables(module:nn.Sequential)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/skip/tracker.py----------------------------------------
A:torch.distributed.pipeline.sync.skip.tracker.portal->Portal(tensor, tensor_life)
A:torch.distributed.pipeline.sync.skip.tracker.phony->Portal(tensor, tensor_life).copy(prev_stream, next_stream, phony)
A:torch.distributed.pipeline.sync.skip.tracker.tensor_idx->batch.find_tensor_idx()
A:torch.distributed.pipeline.sync.skip.tracker.batch[tensor_idx]->join(batch[tensor_idx], phony)
A:torch.distributed.pipeline.sync.skip.tracker.tensor->Portal(tensor, tensor_life).orange(phony)
A:torch.distributed.pipeline.sync.skip.tracker.(batch[tensor_idx], phony)->fork(batch[tensor_idx])
A:torch.distributed.pipeline.sync.skip.tracker.thread_local->ThreadLocal()
A:torch.distributed.pipeline.sync.skip.tracker.skip_tracker->SkipTracker()
torch.distributed.pipeline.sync.skip.tracker.SkipTracker(self)
torch.distributed.pipeline.sync.skip.tracker.SkipTracker.__init__(self)
torch.distributed.pipeline.sync.skip.tracker.SkipTracker.copy(self,batch:Batch,prev_stream:AbstractStream,next_stream:AbstractStream,ns:Namespace,name:str)->None
torch.distributed.pipeline.sync.skip.tracker.SkipTracker.load(self,batch:Batch,ns:Namespace,name:str)->Optional[Tensor]
torch.distributed.pipeline.sync.skip.tracker.SkipTracker.save(self,batch:Batch,ns:Namespace,name:str,tensor:Optional[Tensor])->None
torch.distributed.pipeline.sync.skip.tracker.SkipTrackerThroughPotals(self,skip_layout:SkipLayout)
torch.distributed.pipeline.sync.skip.tracker.SkipTrackerThroughPotals.__init__(self,skip_layout:SkipLayout)
torch.distributed.pipeline.sync.skip.tracker.SkipTrackerThroughPotals.copy(self,batch:Batch,prev_stream:AbstractStream,next_stream:AbstractStream,ns:Namespace,name:str)->None
torch.distributed.pipeline.sync.skip.tracker.SkipTrackerThroughPotals.load(self,batch:Batch,ns:Namespace,name:str)->Optional[Tensor]
torch.distributed.pipeline.sync.skip.tracker.SkipTrackerThroughPotals.save(self,batch:Batch,ns:Namespace,name:str,tensor:Optional[Tensor])->None
torch.distributed.pipeline.sync.skip.tracker.ThreadLocal(self)
torch.distributed.pipeline.sync.skip.tracker.ThreadLocal.__init__(self)
torch.distributed.pipeline.sync.skip.tracker.current_skip_tracker()->SkipTracker
torch.distributed.pipeline.sync.skip.tracker.use_skip_tracker(skip_tracker:SkipTracker)->Generator[None, None, None]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/skip/layout.py----------------------------------------
A:torch.distributed.pipeline.sync.skip.layout.(prev_j, next_j)->self.by_ns_name.get((ns, name), (-1, -1))
A:torch.distributed.pipeline.sync.skip.layout.prev_j->stashed_at.pop((ns, name))
torch.distributed.pipeline.sync.skip.layout.SkipLayout(self,num_partitions:int,skip_routes:Dict[Tuple[Namespace,str],Tuple[int,int]])
torch.distributed.pipeline.sync.skip.layout.SkipLayout.__init__(self,num_partitions:int,skip_routes:Dict[Tuple[Namespace,str],Tuple[int,int]])
torch.distributed.pipeline.sync.skip.layout.SkipLayout.copy_policy(self,next_j:int)->Iterable[Tuple[int, Namespace, str]]
torch.distributed.pipeline.sync.skip.layout.SkipLayout.requires_copy(self,ns:Namespace,name:str)->bool
torch.distributed.pipeline.sync.skip.layout.inspect_skip_layout(partitions:List[nn.Sequential])->SkipLayout


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/skip/namespace.py----------------------------------------
A:torch.distributed.pipeline.sync.skip.namespace.self.id->uuid.uuid4()
torch.distributed.pipeline.sync.skip.Namespace(self)
torch.distributed.pipeline.sync.skip.Namespace.__eq__(self,other:Any)->bool
torch.distributed.pipeline.sync.skip.Namespace.__hash__(self)->int
torch.distributed.pipeline.sync.skip.Namespace.__lt__(self,other:Any)->bool
torch.distributed.pipeline.sync.skip.Namespace.__repr__(self)->str
torch.distributed.pipeline.sync.skip.namespace.Namespace(self)
torch.distributed.pipeline.sync.skip.namespace.Namespace.__eq__(self,other:Any)->bool
torch.distributed.pipeline.sync.skip.namespace.Namespace.__hash__(self)->int
torch.distributed.pipeline.sync.skip.namespace.Namespace.__init__(self)
torch.distributed.pipeline.sync.skip.namespace.Namespace.__lt__(self,other:Any)->bool
torch.distributed.pipeline.sync.skip.namespace.Namespace.__repr__(self)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/pipeline/sync/skip/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/launcher/api.py----------------------------------------
A:torch.distributed.launcher.api.logger->get_logger()
A:torch.distributed.launcher.api.endpoint->endpoint.strip().strip()
A:torch.distributed.launcher.api.(master_addr, master_port)->_get_addr_and_port(rdzv_parameters)
A:torch.distributed.launcher.api.run_id->str(uuid.uuid4().int)
A:torch.distributed.launcher.api.entrypoint_name->_get_entrypoint_name(entrypoint, args)
A:torch.distributed.launcher.api.rdzv_parameters->RendezvousParameters(backend=config.rdzv_backend, endpoint=config.rdzv_endpoint, run_id=config.run_id, min_nodes=config.min_nodes, max_nodes=config.max_nodes, **config.rdzv_configs)
A:torch.distributed.launcher.api.spec->WorkerSpec(role=config.role, local_world_size=config.nproc_per_node, entrypoint=entrypoint, args=tuple(args), rdzv_handler=rdzv_registry.get_rendezvous_handler(rdzv_parameters), max_restarts=config.max_restarts, monitor_interval=config.monitor_interval, redirects=config.redirects, tee=config.tee, master_addr=master_addr, master_port=master_port)
A:torch.distributed.launcher.api.agent->LocalElasticAgent(spec=spec, start_method=config.start_method, log_dir=config.log_dir)
A:torch.distributed.launcher.api.result->LocalElasticAgent(spec=spec, start_method=config.start_method, log_dir=config.log_dir).run()
torch.distributed.launcher.LaunchConfig
torch.distributed.launcher.LaunchConfig.__post_init__(self)
torch.distributed.launcher.api.LaunchConfig
torch.distributed.launcher.api.LaunchConfig.__post_init__(self)
torch.distributed.launcher.api._get_addr_and_port(rdzv_parameters:RendezvousParameters)->Tuple[Optional[str], Optional[int]]
torch.distributed.launcher.api._get_entrypoint_name(entrypoint:Union[Callable,str,None],args:List[Any])->str
torch.distributed.launcher.api.elastic_launch(self,config:LaunchConfig,entrypoint:Union[Callable,str,None])
torch.distributed.launcher.api.elastic_launch.__init__(self,config:LaunchConfig,entrypoint:Union[Callable,str,None])
torch.distributed.launcher.api.launch_agent(config:LaunchConfig,entrypoint:Union[Callable,str,None],args:List[Any])->Dict[int, Any]
torch.distributed.launcher.elastic_launch(self,config:LaunchConfig,entrypoint:Union[Callable,str,None])
torch.distributed.launcher.launch_agent(config:LaunchConfig,entrypoint:Union[Callable,str,None],args:List[Any])->Dict[int, Any]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/launcher/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/api.py----------------------------------------
A:torch.distributed._shard.api.world_size->torch.distributed.get_world_size(pg)
A:torch.distributed._shard.api.rank->torch.distributed.get_rank(pg)
A:torch.distributed._shard.api.sharding_dim_size->getattr(module, param_name).size(sharding_spec.dim)
A:torch.distributed._shard.api.split_size->get_split_size(sharding_dim_size, world_size)
A:torch.distributed._shard.api.tensor_sizes->list(tensor.size())
A:torch.distributed._shard.api.chunked_dim_size->get_chunked_dim_size(sharding_dim_size, split_size, idx)
A:torch.distributed._shard.api.shard_size->copy.deepcopy(tensor_sizes)
A:torch.distributed._shard.api.shard_metadata->ShardMetadata(shard_offsets=copy.deepcopy(current_offsets), shard_sizes=shard_size, placement=placement)
A:torch.distributed._shard.api.local_shard->getattr(module, param_name).narrow(sharding_spec.dim, local_metadata.shard_offsets[sharding_spec.dim], local_metadata.shard_sizes[sharding_spec.dim]).clone().detach().contiguous()
A:torch.distributed._shard.api.st->_shard_tensor(tensor, sharding_spec, src_rank, process_group)
A:torch.distributed._shard.api.tensor->getattr(module, param_name)
torch.distributed._shard._shard_tensor(tensor:torch.Tensor,sharding_spec:ShardingSpec,src_rank=0,process_group=None)
torch.distributed._shard.api._shard_tensor(tensor:torch.Tensor,sharding_spec:ShardingSpec,src_rank=0,process_group=None)
torch.distributed._shard.api.shard_parameter(module:torch.nn.Module,param_name:str,sharding_spec:ShardingSpec,src_rank=0,process_group=None)
torch.distributed._shard.shard_parameter(module:torch.nn.Module,param_name:str,sharding_spec:ShardingSpec,src_rank=0,process_group=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_optim/api.py----------------------------------------
A:torch.distributed._shard.sharded_optim.api.self._optim->optimizer_class(tensors, *optimizer_args, **optimizer_kwargs)
torch.distributed._shard.sharded_optim.ShardedOptimizer(self,named_params:Mapping[str,Union[Tensor,ShardedTensor]],optimizer_class,*optimizer_args,**optimizer_kwargs)
torch.distributed._shard.sharded_optim.ShardedOptimizer.add_param_group(self,param_group:Any)
torch.distributed._shard.sharded_optim.ShardedOptimizer.load_state_dict(self,state_dict:Mapping[str,Any])
torch.distributed._shard.sharded_optim.ShardedOptimizer.state_dict(self)->Dict[str, Any]
torch.distributed._shard.sharded_optim.ShardedOptimizer.step(self,closure=None)
torch.distributed._shard.sharded_optim.ShardedOptimizer.zero_grad(self,set_to_none:bool=False)
torch.distributed._shard.sharded_optim.api.ShardedOptimizer(self,named_params:Mapping[str,Union[Tensor,ShardedTensor]],optimizer_class,*optimizer_args,**optimizer_kwargs)
torch.distributed._shard.sharded_optim.api.ShardedOptimizer.__init__(self,named_params:Mapping[str,Union[Tensor,ShardedTensor]],optimizer_class,*optimizer_args,**optimizer_kwargs)
torch.distributed._shard.sharded_optim.api.ShardedOptimizer.add_param_group(self,param_group:Any)
torch.distributed._shard.sharded_optim.api.ShardedOptimizer.load_state_dict(self,state_dict:Mapping[str,Any])
torch.distributed._shard.sharded_optim.api.ShardedOptimizer.state_dict(self)->Dict[str, Any]
torch.distributed._shard.sharded_optim.api.ShardedOptimizer.step(self,closure=None)
torch.distributed._shard.sharded_optim.api.ShardedOptimizer.zero_grad(self,set_to_none:bool=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_optim/__init__.py----------------------------------------
A:torch.distributed._shard.sharded_optim.__init__.memo->set()
torch.distributed._shard.sharded_optim.__init__.named_params_with_sharded_tensor(module:nn.Module,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Union[nn.Parameter, ShardedTensor]]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharding_spec/_internals.py----------------------------------------
A:torch.distributed._shard.sharding_spec._internals.self.placement->torch.distributed._remote_device(self.placement)
A:torch.distributed._shard.sharding_spec._internals.ndims->len(shard1.shard_offsets)
A:torch.distributed._shard.sharding_spec._internals.tensor_rank->len(tensor_dims)
A:torch.distributed._shard.sharding_spec._internals.shards_rank->len(shards_metadata[0].shard_offsets)
A:torch.distributed._shard.sharding_spec._internals.split_size->get_split_size(sharding_dim_size, world_size)
A:torch.distributed._shard.sharding_spec._internals.chunk_size->get_chunked_dim_size(sharding_dim_size, split_size, idx)
torch.distributed._shard.sharding_spec._internals.ShardMetadata(object)
torch.distributed._shard.sharding_spec._internals.ShardMetadata.__post_init__(self)
torch.distributed._shard.sharding_spec._internals._check_shard_metadata_pair_overlap(shard1:ShardMetadata,shard2:ShardMetadata)
torch.distributed._shard.sharding_spec._internals.check_tensor(shards_metadata,tensor_dims)->None
torch.distributed._shard.sharding_spec._internals.get_chunk_sharding_params(sharding_dim_size,world_size,spec,rank)
torch.distributed._shard.sharding_spec._internals.get_chunked_dim_size(dim_size,split_size,idx)
torch.distributed._shard.sharding_spec._internals.get_split_size(dim_size,chunks)
torch.distributed._shard.sharding_spec._internals.validate_non_overlapping_shards_metadata(shards:List[ShardMetadata])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharding_spec/api.py----------------------------------------
A:torch.distributed._shard.sharding_spec.api.self.device->torch.distributed._remote_device(self.device)
A:torch.distributed._shard.sharding_spec.api.self.placements[i]->torch.distributed._remote_device(remote_device)
A:torch.distributed._shard.sharding_spec.api.rank->len(shard.shard_offsets)
torch.distributed._shard.sharding_spec.ChunkShardingSpec(ShardingSpec)
torch.distributed._shard.sharding_spec.ChunkShardingSpec.__post_init__(self)
torch.distributed._shard.sharding_spec.ChunkShardingSpec._verify_dim(dim)
torch.distributed._shard.sharding_spec.DevicePlacementSpec(PlacementSpec)
torch.distributed._shard.sharding_spec.DevicePlacementSpec.__post_init__(self)
torch.distributed._shard.sharding_spec.EnumerableShardingSpec(ShardingSpec)
torch.distributed._shard.sharding_spec.EnumerableShardingSpec.__post_init__(self)
torch.distributed._shard.sharding_spec.PlacementSpec(ABC)
torch.distributed._shard.sharding_spec.ShardingSpec(PlacementSpec)
torch.distributed._shard.sharding_spec.api.ChunkShardingSpec(ShardingSpec)
torch.distributed._shard.sharding_spec.api.ChunkShardingSpec.__post_init__(self)
torch.distributed._shard.sharding_spec.api.ChunkShardingSpec._verify_dim(dim)
torch.distributed._shard.sharding_spec.api.DevicePlacementSpec(PlacementSpec)
torch.distributed._shard.sharding_spec.api.DevicePlacementSpec.__post_init__(self)
torch.distributed._shard.sharding_spec.api.EnumerableShardingSpec(ShardingSpec)
torch.distributed._shard.sharding_spec.api.EnumerableShardingSpec.__post_init__(self)
torch.distributed._shard.sharding_spec.api.PlacementSpec(ABC)
torch.distributed._shard.sharding_spec.api.ShardingSpec(PlacementSpec)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharding_spec/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/utils.py----------------------------------------
A:torch.distributed._shard.sharded_tensor.utils.worker_name->remote_device.worker_name()
A:torch.distributed._shard.sharded_tensor.utils.rank->remote_device.rank()
A:torch.distributed._shard.sharded_tensor.utils.device->remote_device.device()
A:torch.distributed._shard.sharded_tensor.utils.workers->torch.distributed.rpc._get_current_rpc_agent().get_worker_infos()
A:torch.distributed._shard.sharded_tensor.utils.dims->list(size)
A:torch.distributed._shard.sharded_tensor.utils.first_shard_is_pinned->local_shards[0].tensor.is_pinned()
A:torch.distributed._shard.sharded_tensor.utils.(rank, local_device)->_parse_and_validate_remote_device(pg, local_shard_meta.placement)
A:torch.distributed._shard.sharded_tensor.utils.local_tensor_properties->TensorProperties(dtype=first_shard_dtype, layout=first_shard_layout, requires_grad=first_shard_requires_grad, memory_format=torch.contiguous_format, pin_memory=first_shard_is_pinned)
A:torch.distributed._shard.sharded_tensor.utils.local_sharded_tensor_metadata->ShardedTensorMetadata(shards_metadata=local_shard_metadatas, size=torch.Size(global_size), tensor_properties=local_tensor_properties)
torch.distributed._shard.sharded_tensor.load_with_process_group(process_group)
torch.distributed._shard.sharded_tensor.utils._flatten_tensor_size(size)->List[int]
torch.distributed._shard.sharded_tensor.utils._parse_and_validate_remote_device(pg,remote_device)
torch.distributed._shard.sharded_tensor.utils._raise_if_mismatch(expected,actual,prop_name,ranks,is_local=True)
torch.distributed._shard.sharded_tensor.utils._validate_output_tensor_for_gather(my_rank:int,dst_rank:int,size:torch.Size,dst_tensor:Optional[torch.Tensor])->None
torch.distributed._shard.sharded_tensor.utils.build_global_metadata(gathered_metadatas:Sequence[Optional[ShardedTensorMetadata]])
torch.distributed._shard.sharded_tensor.utils.build_metadata_from_local_shards(local_shards:List[Shard],global_size:List[int],current_rank:int,pg:distributed_c10d.ProcessGroup)->ShardedTensorMetadata
torch.distributed._shard.sharded_tensor.utils.get_current_process_group()
torch.distributed._shard.sharded_tensor.utils.load_with_process_group(process_group)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/reshard.py----------------------------------------
A:torch.distributed._shard.sharded_tensor.reshard.shard_dim->int(sharding_spec.dim)
A:torch.distributed._shard.sharded_tensor.reshard.split_size->get_split_size(st_size[current_sharding_dim], world_size)
A:torch.distributed._shard.sharded_tensor.reshard.sharded_dim_size->get_chunked_dim_size(st_size[current_sharding_dim], split_size, idx)
A:torch.distributed._shard.sharded_tensor.reshard.local_tensor_size->list(st_size)
A:torch.distributed._shard.sharded_tensor.reshard.shards_metadata[placement.rank()]->ShardMetadata(shard_offsets=copy.deepcopy(offsets), shard_sizes=local_tensor_size, placement=placement)
A:torch.distributed._shard.sharded_tensor.reshard.current_rank->torch.distributed.get_rank(pg)
A:torch.distributed._shard.sharded_tensor.reshard.world_size->torch.distributed.get_world_size(pg)
A:torch.distributed._shard.sharded_tensor.reshard.(shards_metadata, ranks)->build_reshard_metadata(st_size, resharding_spec, world_size)
A:torch.distributed._shard.sharded_tensor.reshard.reshard_dim->int(resharding_spec.dim)
A:torch.distributed._shard.sharded_tensor.reshard.idx->get_idx_from_placements(sharding_spec.placements, current_rank)
A:torch.distributed._shard.sharded_tensor.reshard.new_rank->resharding_spec.placements[idx].rank()
A:torch.distributed._shard.sharded_tensor.reshard.input_split_sizes[new_rank]->all_to_all_single(gathered_input, local_shard, input_split_sizes=input_split_sizes, output_split_sizes=output_split_sizes, group=pg).size(reshard_dim)
A:torch.distributed._shard.sharded_tensor.reshard.new_idx->ranks.index(current_rank)
A:torch.distributed._shard.sharded_tensor.reshard.local_shard->all_to_all_single(gathered_input, local_shard, input_split_sizes=input_split_sizes, output_split_sizes=output_split_sizes, group=pg)
A:torch.distributed._shard.sharded_tensor.reshard.gathered_input_size->list(local_shard.size())
A:torch.distributed._shard.sharded_tensor.reshard.gathered_input->torch.empty(gathered_input_size, device=local_shard.device)
A:torch.distributed._shard.sharded_tensor.reshard.local_tensor->torch.cat(output_tensor_list, dim=current_sharding_dim)
A:torch.distributed._shard.sharded_tensor.reshard.current_sharding_dim->int(sharding_spec.dim)
A:torch.distributed._shard.sharded_tensor.reshard.rearrange_input->any((ranks[i] > ranks[i + 1] for i in range(len(ranks) - 1)))
A:torch.distributed._shard.sharded_tensor.reshard.output_tensor_size->list(st_size)
A:torch.distributed._shard.sharded_tensor.reshard.output_tensor_list[placement.rank()]->torch.empty(output_tensor_size, device=local_tensor.device)
A:torch.distributed._shard.sharded_tensor.reshard.input_tensor_list->torch.split(local_tensor, input_split_sizes, dim=reshard_dim)
A:torch.distributed._shard.sharded_tensor.reshard.output_tensor_list->all_to_all(output_tensor_list, input_tensor_list, group=pg)
torch.distributed._shard.sharded_tensor.reshard.build_reshard_metadata(st_size:torch.Size,sharding_spec:ShardingSpec,world_size:int)->Tuple[List[ShardMetadata], List[int]]
torch.distributed._shard.sharded_tensor.reshard.get_idx_from_placements(placements,current_rank)->int
torch.distributed._shard.sharded_tensor.reshard.reshard_local_shard(local_tensor:torch.Tensor,st_size:torch.Size,sharding_spec:ShardingSpec,resharding_spec:ShardingSpec,pg:ProcessGroup)->Tuple[List[Shard], List[ShardMetadata]]
torch.distributed._shard.sharded_tensor.reshard.reshuffle_local_shard(local_shard:torch.Tensor,st_size:torch.Size,sharding_spec:ShardingSpec,resharding_spec:ShardingSpec,pg:ProcessGroup)->Tuple[List[Shard], List[ShardMetadata]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/metadata.py----------------------------------------
A:torch.distributed._shard.sharded_tensor.metadata.self.tensor_properties->TensorProperties(dtype=dtype, layout=layout, requires_grad=requires_grad, memory_format=memory_format, pin_memory=pin_memory)
torch.distributed._shard.sharded_tensor.metadata.MEM_FORMAT_ENCODING(Enum)
torch.distributed._shard.sharded_tensor.metadata.ShardedTensorMetadata(object)
torch.distributed._shard.sharded_tensor.metadata.ShardedTensorMetadata.__getstate__(self)
torch.distributed._shard.sharded_tensor.metadata.ShardedTensorMetadata.__setstate__(self,state)
torch.distributed._shard.sharded_tensor.metadata.TensorProperties(object)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/api.py----------------------------------------
A:torch.distributed._shard.sharded_tensor.api._sharded_tensor_lock->threading.Lock()
A:torch.distributed._shard.sharded_tensor.api.sharded_tensor->cls.__new__(cls)
A:torch.distributed._shard.sharded_tensor.api.tensor_properties->TensorProperties(dtype, layout, requires_grad, memory_format, pin_memory)
A:torch.distributed._shard.sharded_tensor.api.tensor_properties.dtype->torch.get_default_dtype()
A:torch.distributed._shard.sharded_tensor.api.dims->len(full_size)
A:torch.distributed._shard.sharded_tensor.api._sharded_tensor_map[self._sharded_tensor_id]->weakref.ref(self)
A:torch.distributed._shard.sharded_tensor.api.pg_rank->torch.distributed.get_rank()
A:torch.distributed._shard.sharded_tensor.api.worker_infos->torch.distributed.rpc._get_current_rpc_agent().get_worker_infos()
A:torch.distributed._shard.sharded_tensor.api.all_tensor_ids->torch.distributed.rpc.api._all_gather(self._sharded_tensor_id)
A:torch.distributed._shard.sharded_tensor.api.fut->torch.distributed.rpc.rpc_async(rank, _register_remote_shards, args=(all_tensor_ids[rank_to_name[rank]], rrefs, rpc_rank))
A:torch.distributed._shard.sharded_tensor.api.rank->self.process_group.rank()
A:torch.distributed._shard.sharded_tensor.api.local_shards->self.local_shard.chunk(self.process_group.size(), dim=sharding_dim)
A:torch.distributed._shard.sharded_tensor.api.world_size->torch.distributed.get_world_size(process_group)
A:torch.distributed._shard.sharded_tensor.api.out_narrow_view->out_narrow_view.narrow(dim, metadata.shard_offsets[dim], metadata.shard_sizes[dim]).narrow(dim, metadata.shard_offsets[dim], metadata.shard_sizes[dim])
A:torch.distributed._shard.sharded_tensor.api.current_rank->torch.distributed.get_rank(self._process_group)
A:torch.distributed._shard.sharded_tensor.api.global_tensor_size->_flatten_tensor_size(global_size)
A:torch.distributed._shard.sharded_tensor.api.local_sharded_tensor_metadata->build_metadata_from_local_shards(local_shards, global_tensor_size, current_rank, process_group)
A:torch.distributed._shard.sharded_tensor.api.global_sharded_tensor_metadata->build_global_metadata(gathered_metadatas)
A:torch.distributed._shard.sharded_tensor.api.sharded_tensor._sharding_spec->EnumerableShardingSpec(shards_metadata)
A:torch.distributed._shard.sharded_tensor.api.(rank, local_device)->_parse_and_validate_remote_device(self._process_group, shard_metadata.placement)
A:torch.distributed._shard.sharded_tensor.api.chunks->len(remote_devices)
A:torch.distributed._shard.sharded_tensor.api.split_size->get_split_size(dim_size, chunks)
A:torch.distributed._shard.sharded_tensor.api.sharded_dim_size->get_chunked_dim_size(dim_size, split_size, idx)
A:torch.distributed._shard.sharded_tensor.api.rank_dims->len(full_size).copy()
A:torch.distributed._shard.sharded_tensor.api.shard_metadata->ShardMetadata(rank_offsets, rank_dims, remote_device)
A:torch.distributed._shard.sharded_tensor.api.local_shard->_create_tensor_from_params(*shard_metadata.shard_sizes, local_device=local_device, tensor_properties=tensor_properties)
A:torch.distributed._shard.sharded_tensor.api.self._metadata->ShardedTensorMetadata(shards_metadata, dims, tensor_properties)
A:torch.distributed._shard.sharded_tensor.api.(local_shards, shards_metadata)->reshard_local_shard(self.local_tensor(), self.size(), self._sharding_spec, resharding_spec, self._process_group)
A:torch.distributed._shard.sharded_tensor.api.pg_state->ShardedTensor.ProcessGroupState(distributed_c10d.get_rank(self._process_group), distributed_c10d.get_rank(), distributed_c10d.get_world_size(self._process_group), distributed_c10d.get_world_size())
A:torch.distributed._shard.sharded_tensor.api.self._process_group->get_current_process_group()
A:torch.distributed._shard.sharded_tensor.api.local_rank->torch.distributed.distributed_c10d.get_rank(self._process_group)
A:torch.distributed._shard.sharded_tensor.api.global_rank->torch.distributed.distributed_c10d.get_rank()
A:torch.distributed._shard.sharded_tensor.api.local_world_size->torch.distributed.distributed_c10d.get_world_size(self._process_group)
A:torch.distributed._shard.sharded_tensor.api.global_world_size->torch.distributed.distributed_c10d.get_world_size()
A:torch.distributed._shard.sharded_tensor.api.sharding_dim->int(resharding_spec.dim)
A:torch.distributed._shard.sharded_tensor.api.local_result->reduce_scatter(torch.empty_like(local_shards[0]), list(local_shards), op=self.reduce_op)
A:torch.distributed._shard.sharded_tensor.api.sharded_tensor_size->self.local_shard.size()
A:torch.distributed._shard.sharded_tensor.api.local_metadata->ShardMetadata(shard_offsets=current_offsets, shard_sizes=list(local_result.size()), placement=placement)
A:torch.distributed._shard.sharded_tensor.api.st->ShardedTensor._init_from_local_shards(shards, tuple(sharded_tensor_size), process_group=self.process_group)
A:torch.distributed._shard.sharded_tensor.api.st._sharding_spec->copy.deepcopy(resharding_spec)
torch.distributed._shard.sharded_tensor.ShardedTensor(self,sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.ShardedTensor.ProcessGroupState
torch.distributed._shard.sharded_tensor.ShardedTensor.__del__(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.__getstate__(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.__hash__(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.__repr__(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.__setstate__(self,state)
torch.distributed._shard.sharded_tensor.ShardedTensor.__torch_function__(self,func,types,args=(),kwargs=None)
torch.distributed._shard.sharded_tensor.ShardedTensor._init_chunked(self,dims,tensor_properties:TensorProperties)
torch.distributed._shard.sharded_tensor.ShardedTensor._init_enumerable(self,dims,tensor_properties:TensorProperties)
torch.distributed._shard.sharded_tensor.ShardedTensor._init_from_local_shards(cls,local_shards:List[Shard],*global_size,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.ShardedTensor._init_from_local_shards_and_global_metadata(cls,local_shards:List[Shard],sharded_tensor_metadata:ShardedTensorMetadata,process_group=None,init_rrefs=False)->'ShardedTensor'
torch.distributed._shard.sharded_tensor.ShardedTensor._init_rpc(self)
torch.distributed._shard.sharded_tensor.ShardedTensor._post_init(self)
torch.distributed._shard.sharded_tensor.ShardedTensor._prepare_init(self,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.ShardedTensor._register_remote_shards(self,remote_shards:List[rpc.RRef[Shard]],rpc_rank:int)
torch.distributed._shard.sharded_tensor.ShardedTensor.dtype(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.gather(self,dst:int=0,out:Optional[torch.Tensor]=None)->None
torch.distributed._shard.sharded_tensor.ShardedTensor.is_contiguous(self)->bool
torch.distributed._shard.sharded_tensor.ShardedTensor.is_pinned(self)->bool
torch.distributed._shard.sharded_tensor.ShardedTensor.layout(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.local_shards(self)->List[Shard]
torch.distributed._shard.sharded_tensor.ShardedTensor.local_tensor(self)->torch.Tensor
torch.distributed._shard.sharded_tensor.ShardedTensor.metadata(self)->ShardedTensorMetadata
torch.distributed._shard.sharded_tensor.ShardedTensor.remote_shards(self)->Dict[int, List[rpc.RRef[Shard]]]
torch.distributed._shard.sharded_tensor.ShardedTensor.requires_grad(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.reshard(self,resharding_spec:ShardingSpec)->ShardedTensor
torch.distributed._shard.sharded_tensor.ShardedTensor.shape(self)
torch.distributed._shard.sharded_tensor.ShardedTensor.sharding_spec(self)->ShardingSpec
torch.distributed._shard.sharded_tensor.ShardedTensor.size(self,dim:int=None)->Union[torch.Size, int]
torch.distributed._shard.sharded_tensor._PartialTensor(self,local_shard,process_group=None,reduce_op=distributed_c10d.ReduceOp.SUM)
torch.distributed._shard.sharded_tensor._PartialTensor.__post_init__(self)
torch.distributed._shard.sharded_tensor._PartialTensor.reshard(self,resharding_spec:ShardingSpec)->ShardedTensor
torch.distributed._shard.sharded_tensor._register_sharded_op(op,func)
torch.distributed._shard.sharded_tensor.api.ShardedTensor(self,sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.ProcessGroupState
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__del__(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__getstate__(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__hash__(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__init__(self,sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__repr__(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__setstate__(self,state)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.__torch_function__(self,func,types,args=(),kwargs=None)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._init_chunked(self,dims,tensor_properties:TensorProperties)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._init_enumerable(self,dims,tensor_properties:TensorProperties)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._init_from_local_shards(cls,local_shards:List[Shard],*global_size,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._init_from_local_shards_and_global_metadata(cls,local_shards:List[Shard],sharded_tensor_metadata:ShardedTensorMetadata,process_group=None,init_rrefs=False)->'ShardedTensor'
torch.distributed._shard.sharded_tensor.api.ShardedTensor._init_rpc(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._post_init(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._prepare_init(self,process_group=None,init_rrefs=False)
torch.distributed._shard.sharded_tensor.api.ShardedTensor._register_remote_shards(self,remote_shards:List[rpc.RRef[Shard]],rpc_rank:int)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.dtype(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.gather(self,dst:int=0,out:Optional[torch.Tensor]=None)->None
torch.distributed._shard.sharded_tensor.api.ShardedTensor.is_contiguous(self)->bool
torch.distributed._shard.sharded_tensor.api.ShardedTensor.is_pinned(self)->bool
torch.distributed._shard.sharded_tensor.api.ShardedTensor.layout(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.local_shards(self)->List[Shard]
torch.distributed._shard.sharded_tensor.api.ShardedTensor.local_tensor(self)->torch.Tensor
torch.distributed._shard.sharded_tensor.api.ShardedTensor.metadata(self)->ShardedTensorMetadata
torch.distributed._shard.sharded_tensor.api.ShardedTensor.remote_shards(self)->Dict[int, List[rpc.RRef[Shard]]]
torch.distributed._shard.sharded_tensor.api.ShardedTensor.requires_grad(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.reshard(self,resharding_spec:ShardingSpec)->ShardedTensor
torch.distributed._shard.sharded_tensor.api.ShardedTensor.shape(self)
torch.distributed._shard.sharded_tensor.api.ShardedTensor.sharding_spec(self)->ShardingSpec
torch.distributed._shard.sharded_tensor.api.ShardedTensor.size(self,dim:int=None)->Union[torch.Size, int]
torch.distributed._shard.sharded_tensor.api._PartialTensor(self,local_shard,process_group=None,reduce_op=distributed_c10d.ReduceOp.SUM)
torch.distributed._shard.sharded_tensor.api._PartialTensor.__init__(self,local_shard,process_group=None,reduce_op=distributed_c10d.ReduceOp.SUM)
torch.distributed._shard.sharded_tensor.api._PartialTensor.__post_init__(self)
torch.distributed._shard.sharded_tensor.api._PartialTensor.reshard(self,resharding_spec:ShardingSpec)->ShardedTensor
torch.distributed._shard.sharded_tensor.api._create_tensor_from_params(*size,local_device,tensor_properties:TensorProperties)
torch.distributed._shard.sharded_tensor.api._register_remote_shards(sharded_tensor_id:int,rrefs:List[rpc.RRef[Shard]],rpc_rank:int)
torch.distributed._shard.sharded_tensor.api._register_sharded_op(op,func)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/__init__.py----------------------------------------
A:torch.distributed._shard.sharded_tensor.__init__.sharded_tensor->ShardedTensor(sharding_spec, *size, dtype=dtype, layout=layout, requires_grad=requires_grad, pin_memory=pin_memory, memory_format=memory_format, process_group=process_group, init_rrefs=init_rrefs)
A:torch.distributed._shard.sharded_tensor.__init__.local_tensor->local_tensor.squeeze(output._sharding_spec.dim).squeeze(output._sharding_spec.dim)
torch.distributed._shard.sharded_tensor.__init__._collect_local_shard(module:torch.nn.Module)->torch.nn.Module
torch.distributed._shard.sharded_tensor.__init__._reshard_output(module:torch.nn.Module,resharding_spec:ShardingSpec)->torch.nn.Module
torch.distributed._shard.sharded_tensor.__init__.empty(sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)->ShardedTensor
torch.distributed._shard.sharded_tensor.__init__.full(sharding_spec:ShardingSpec,size,fill_value=torch.types.Number,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)->ShardedTensor
torch.distributed._shard.sharded_tensor.__init__.init_from_local_shards(local_shards:List[Shard],*global_size,process_group=None,init_rrefs=False)->ShardedTensor
torch.distributed._shard.sharded_tensor.__init__.ones(sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)->ShardedTensor
torch.distributed._shard.sharded_tensor.__init__.pre_load_state_dict_hook(module,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.distributed._shard.sharded_tensor.__init__.rand(sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)->ShardedTensor
torch.distributed._shard.sharded_tensor.__init__.sharded_op_impl(func)
torch.distributed._shard.sharded_tensor.__init__.state_dict_hook(module,destination,prefix,local_metadata)
torch.distributed._shard.sharded_tensor.__init__.zeros(sharding_spec:ShardingSpec,*size,dtype=None,layout=torch.strided,requires_grad=False,pin_memory=False,memory_format=torch.contiguous_format,process_group=None,init_rrefs=False)->ShardedTensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/shard.py----------------------------------------
A:torch.distributed._shard.sharded_tensor.shard.placement_device->cast(_remote_device, self.metadata.placement).device()
A:torch.distributed._shard.sharded_tensor.shard.shard_sizes->list(tensor.size())
A:torch.distributed._shard.sharded_tensor.shard.placement->_remote_device(f'rank:{rank}/{str(tensor.device)}')
A:torch.distributed._shard.sharded_tensor.shard.shard_meta->ShardMetadata(shard_offsets=shard_offsets, shard_sizes=shard_sizes, placement=placement)
torch.distributed._shard.sharded_tensor.shard.Shard(object)
torch.distributed._shard.sharded_tensor.shard.Shard.__post_init__(self)
torch.distributed._shard.sharded_tensor.shard.Shard.from_tensor_and_offsets(cls,tensor:torch.Tensor,shard_offsets:List[int],rank:int)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/init.py----------------------------------------
torch.distributed._shard.sharded_tensor._ops.init.constant_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor._ops.init.kaiming_uniform_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor._ops.init.normal_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor._ops.init.uniform_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor._ops.init.validate_param(param,param_name)
torch.distributed._shard.sharded_tensor.constant_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor.init.validate_param(param,param_name)
torch.distributed._shard.sharded_tensor.kaiming_uniform_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor.normal_(types,args=(),kwargs=None,pg=None)
torch.distributed._shard.sharded_tensor.uniform_(types,args=(),kwargs=None,pg=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/_common.py----------------------------------------
A:torch.distributed._shard.sharded_tensor._ops._common.gathered_inputs->all_gather(input, group=pg)
A:torch.distributed._shard.sharded_tensor._ops._common.result->op_func(inp, local_shard)
A:torch.distributed._shard.sharded_tensor._ops._common.output->all_to_all_single(output, combined_results, output_split_sizes=output_split_sizes, group=pg)
A:torch.distributed._shard.sharded_tensor._ops._common.sharding_dim_size->weight.size(sharding_dim)
A:torch.distributed._shard.sharded_tensor._ops._common.dims->list(results[0].size())
A:torch.distributed._shard.sharded_tensor._ops._common.combined_results->torch.cat(results)
A:torch.distributed._shard.sharded_tensor._ops._common.split_size->get_split_size(weight.size(0), world_size)
A:torch.distributed._shard.sharded_tensor._ops._common.output_split_sizes[placement.rank()]->get_chunked_dim_size(sharding_dim_size, split_size, idx)
A:torch.distributed._shard.sharded_tensor._ops._common.rearrange_columns->any([idx != placement.rank() for (idx, placement) in enumerate(weight._sharding_spec.placements)])
A:torch.distributed._shard.sharded_tensor._ops._common.start->sum([split_size if i < placement.rank() else 0 for (i, split_size) in enumerate(output_split_sizes)])
A:torch.distributed._shard.sharded_tensor._ops._common.sharded_dim_size_max->get_chunked_dim_size(weight.size(0), split_size, 0)
A:torch.distributed._shard.sharded_tensor._ops._common.sharded_dim_size->get_chunked_dim_size(weight.size(0), split_size, idx)
A:torch.distributed._shard.sharded_tensor._ops._common.start_idx->torch.searchsorted(input_sorted, start_row_idx).item()
A:torch.distributed._shard.sharded_tensor._ops._common.end_idx->torch.searchsorted(input_sorted, end_row_idx).item()
A:torch.distributed._shard.sharded_tensor._ops._common.input_split_sizes[placement.rank()]->int(end_idx - start_idx)
A:torch.distributed._shard.sharded_tensor._ops._common.input_split_start_indices[placement.rank()]->int(start_idx)
A:torch.distributed._shard.sharded_tensor._ops._common.indices[placement.rank()]->list(range(offset_idx, offset_idx + split_length))
A:torch.distributed._shard.sharded_tensor._ops._common.indices_flatten->list((idx for indice in indices for idx in indice))
A:torch.distributed._shard.sharded_tensor._ops._common.input_sorted->input_sorted.index_select(0, torch.tensor(indices_flatten, device=input.device)).index_select(0, torch.tensor(indices_flatten, device=input.device))
A:torch.distributed._shard.sharded_tensor._ops._common.rearrange_indices_1d_second_order->torch.argsort(torch.Tensor(indices_flatten))
A:torch.distributed._shard.sharded_tensor._ops._common.input_size_list_tensor->torch.tensor(input_size_list, dtype=tensor_type, device=input.device)
A:torch.distributed._shard.sharded_tensor._ops._common.output_size_list_tensor->torch.empty(output_size, dtype=tensor_type, device=input.device)
A:torch.distributed._shard.sharded_tensor._ops._common.unique_inp->torch.unique(torch.cat(gathered_inputs))
A:torch.distributed._shard.sharded_tensor._ops._common.local_shard_sum->torch.sum(torch.pow(torch.abs(local_shard), norm_type), dim=1, dtype=local_shard.dtype)
A:torch.distributed._shard.sharded_tensor._ops._common.local_shard_norm->torch.pow(local_shard_sum, 1.0 / norm_type)
A:torch.distributed._shard.sharded_tensor._ops._common.max_norm_tensor->torch.full((local_shard.size(0),), float('inf'), dtype=local_shard.dtype, device=input.device)
A:torch.distributed._shard.sharded_tensor._ops._common.local_shard_t->local_shard.t().contiguous()
A:torch.distributed._shard.sharded_tensor._ops._common.normalized_tensor->torch.where(local_shard_norm > max_norm_tensor, max_norm_tensor, local_shard_norm)
A:torch.distributed._shard.sharded_tensor._ops._common.local_shard_norm_renormed->torch.div(torch.mul(local_shard_t, normalized_tensor), local_shard_norm).t().contiguous()
torch.distributed._shard.sharded_tensor._common._communicate_list_to_each_rank(input_tensor_list,output_lists,input,pg,tensor_type=torch.int64)
torch.distributed._shard.sharded_tensor._common._communicate_size_to_each_rank(input_size_list,output_size,input,pg,tensor_type=torch.int)
torch.distributed._shard.sharded_tensor._common._handle_col_wise_sharding_base(op_func,col_dim,input,world_size,weight,local_shard,pg,gathered_inputs=None,mode=None,gathered_per_sample_weights=None,gathered_offsets=None,padding_idx=None)
torch.distributed._shard.sharded_tensor._common._handle_max_norm_col_wise(max_norm,norm_type,local_shard,input,world_size,pg)
torch.distributed._shard.sharded_tensor._common._handle_row_wise_lookup_distribute(input_sorted,input,world_size,weight,rank,padding_idx)
torch.distributed._shard.sharded_tensor._common._result_distribute_with_col_rearrange(results,input,world_size,weight,pg)
torch.distributed._shard.sharded_tensor._ops._common._communicate_list_to_each_rank(input_tensor_list,output_lists,input,pg,tensor_type=torch.int64)
torch.distributed._shard.sharded_tensor._ops._common._communicate_size_to_each_rank(input_size_list,output_size,input,pg,tensor_type=torch.int)
torch.distributed._shard.sharded_tensor._ops._common._handle_col_wise_sharding_base(op_func,col_dim,input,world_size,weight,local_shard,pg,gathered_inputs=None,mode=None,gathered_per_sample_weights=None,gathered_offsets=None,padding_idx=None)
torch.distributed._shard.sharded_tensor._ops._common._handle_max_norm_col_wise(max_norm,norm_type,local_shard,input,world_size,pg)
torch.distributed._shard.sharded_tensor._ops._common._handle_row_wise_lookup_distribute(input_sorted,input,world_size,weight,rank,padding_idx)
torch.distributed._shard.sharded_tensor._ops._common._result_distribute_with_col_rearrange(results,input,world_size,weight,pg)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/linear.py----------------------------------------
A:torch.distributed._shard.sharded_tensor._ops.linear.local_shard->weight.local_tensor()
A:torch.distributed._shard.sharded_tensor._ops.linear.local_shard_t->weight.local_tensor().t().contiguous()
A:torch.distributed._shard.sharded_tensor._ops.linear.world_size->torch.distributed.get_world_size(pg)
A:torch.distributed._shard.sharded_tensor._ops.linear.rank->torch.distributed.get_rank(pg)
A:torch.distributed._shard.sharded_tensor._ops.linear.weight_size->cast(torch.Size, weight.size())
A:torch.distributed._shard.sharded_tensor._ops.linear.gathered_inputs->all_gather(input, group=pg)
A:torch.distributed._shard.sharded_tensor._ops.linear.(start_pos, chunk_size)->get_chunk_sharding_params(bias.size(0), world_size, weight._sharding_spec, rank)
A:torch.distributed._shard.sharded_tensor._ops.linear.local_bias->_BiasTensorNarrow.apply(world_size, start_pos, chunk_size, weight, pg, bias)
A:torch.distributed._shard.sharded_tensor._ops.linear.result->torch.cat(results)
A:torch.distributed._shard.sharded_tensor._ops.linear.input_t->input_t.index_select(0, torch.tensor(indices_flatten, device=input_t.device)).index_select(0, torch.tensor(indices_flatten, device=input_t.device))
A:torch.distributed._shard.sharded_tensor._ops.linear.input_t_size->input_t.index_select(0, torch.tensor(indices_flatten, device=input_t.device)).index_select(0, torch.tensor(indices_flatten, device=input_t.device)).size()
A:torch.distributed._shard.sharded_tensor._ops.linear.split_size->get_split_size(input_t_size[0], world_size)
A:torch.distributed._shard.sharded_tensor._ops.linear.sharded_dim_size->get_chunked_dim_size(input_t_size[0], split_size, idx)
A:torch.distributed._shard.sharded_tensor._ops.linear.sharded_dim_size_max->max(input_split_sizes)
A:torch.distributed._shard.sharded_tensor._ops.linear.indices[placement.rank()]->list(range(offset_start_idx, offset_start_idx + split_size))
A:torch.distributed._shard.sharded_tensor._ops.linear.indices_flatten->list((idx for indice in indices for idx in indice))
A:torch.distributed._shard.sharded_tensor._ops.linear.gathered_input->gathered_input.transpose(0, -1).transpose(0, -1)
A:torch.distributed._shard.sharded_tensor._ops.linear.inp->torch.narrow(gathered_input, -1, r * shard_size, shard_size)
A:torch.distributed._shard.sharded_tensor._ops.linear.sharded_weight_metadata->copy.deepcopy(sharded_tensor.local_shards()[0].metadata)
A:torch.distributed._shard.sharded_tensor._ops.linear.global_size->list(local_result.size())
A:torch.distributed._shard.sharded_tensor._ops.linear.global_size[result_shard_dim]->sharded_tensor.size(tensor_shard_dim)
A:torch.distributed._shard.sharded_tensor._ops.linear.local_shard_metadata->ShardMetadata(shard_offsets=current_offsets, shard_sizes=list(local_result.size()), placement=sharded_weight_metadata.placement)
A:torch.distributed._shard.sharded_tensor._ops.linear.new_st->torch.distributed._shard.sharded_tensor.ShardedTensor._init_from_local_shards(local_shards, tuple(global_size), process_group=pg)
A:torch.distributed._shard.sharded_tensor._ops.linear.new_st._sharding_spec->copy.deepcopy(sharded_tensor._sharding_spec)
torch.distributed._shard.sharded_tensor._ops.linear._BiasTensorNarrow(Function)
torch.distributed._shard.sharded_tensor._ops.linear._BiasTensorNarrow.backward(ctx,grad_output)
torch.distributed._shard.sharded_tensor._ops.linear._BiasTensorNarrow.forward(ctx,world_size,start_pos,chunk_size,weight,pg,bias)
torch.distributed._shard.sharded_tensor._ops.linear._BiasTensorPartial(Function)
torch.distributed._shard.sharded_tensor._ops.linear._BiasTensorPartial.backward(ctx,grad_output)
torch.distributed._shard.sharded_tensor._ops.linear._BiasTensorPartial.forward(ctx,world_size,bias)
torch.distributed._shard.sharded_tensor._ops.linear._handle_col_wise_sharding(input,world_size,weight,rank,local_shard_t,bias,pg)
torch.distributed._shard.sharded_tensor._ops.linear._handle_row_wise_sharding_sharded_tensor(input,world_size,weight,local_shard_t,bias,pg)
torch.distributed._shard.sharded_tensor._ops.linear._handle_row_wise_sharding_tensor(input,world_size,weight,rank,local_shard_t,bias,pg)
torch.distributed._shard.sharded_tensor._ops.linear._init_sharded_tensor_from_local_result(sharded_tensor,local_result,tensor_shard_dim,result_shard_dim,world_size,pg)
torch.distributed._shard.sharded_tensor._ops.linear._validate_linear_op_param(args,kwargs)
torch.distributed._shard.sharded_tensor._ops.linear.sharded_linear(types,args,kwargs,pg)
torch.distributed._shard.sharded_tensor.linear._BiasTensorNarrow(Function)
torch.distributed._shard.sharded_tensor.linear._BiasTensorNarrow.backward(ctx,grad_output)
torch.distributed._shard.sharded_tensor.linear._BiasTensorNarrow.forward(ctx,world_size,start_pos,chunk_size,weight,pg,bias)
torch.distributed._shard.sharded_tensor.linear._BiasTensorPartial(Function)
torch.distributed._shard.sharded_tensor.linear._BiasTensorPartial.backward(ctx,grad_output)
torch.distributed._shard.sharded_tensor.linear._BiasTensorPartial.forward(ctx,world_size,bias)
torch.distributed._shard.sharded_tensor.linear._handle_col_wise_sharding(input,world_size,weight,rank,local_shard_t,bias,pg)
torch.distributed._shard.sharded_tensor.linear._handle_row_wise_sharding_sharded_tensor(input,world_size,weight,local_shard_t,bias,pg)
torch.distributed._shard.sharded_tensor.linear._handle_row_wise_sharding_tensor(input,world_size,weight,rank,local_shard_t,bias,pg)
torch.distributed._shard.sharded_tensor.linear._init_sharded_tensor_from_local_result(sharded_tensor,local_result,tensor_shard_dim,result_shard_dim,world_size,pg)
torch.distributed._shard.sharded_tensor.linear._validate_linear_op_param(args,kwargs)
torch.distributed._shard.sharded_tensor.sharded_linear(types,args,kwargs,pg)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/embedding_bag.py----------------------------------------
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.offsets->kwargs.get('offsets')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.per_sample_weights->per_sample_weights.index_select(0, torch.tensor(input_combined_rearrange_indices, device=input.device)).index_select(0, torch.tensor(input_combined_rearrange_indices, device=input.device))
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.mode->kwargs.get('mode')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.max_norm->kwargs.get('max_norm')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.norm_type->kwargs.get('norm_type')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.include_last_offset->kwargs.get('include_last_offset')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.padding_idx->kwargs.get('padding_idx')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.local_shard->weight.local_tensor().contiguous()
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.world_size->torch.distributed.get_world_size(pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.rank->torch.distributed.get_rank(pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(output, local_shard)->_handle_col_wise_sharding(input, world_size, weight, local_shard, offsets, per_sample_weights, mode, max_norm, norm_type, padding_idx, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.scale_grad_by_freq->kwargs.get('scale_grad_by_freq')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.sparse->kwargs.get('sparse')
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.weight_size->cast(torch.Size, weight.size())
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(local_shard, gathered_inputs)->_handle_max_norm_col_wise(max_norm, norm_type, local_shard, input, world_size, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.output->_handle_col_wise_sharding_base(torch.nn.functional.embedding_bag, 1, input, world_size, weight, local_shard, pg, mode=mode, gathered_per_sample_weights=gathered_per_sample_weights, gathered_offsets=gathered_offsets, padding_idx=padding_idx, gathered_inputs=gathered_inputs)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.input_size->input.size()
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(input_split_sorted_list, input_split_sorted_indices, split_sizes_1d, split_sizes_1d_with_padding)->_input_split_sort(input, offsets, padding_idx)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(input_combined, input_combined_split_sizes, offsets_rearrange_list, offsets_rearrange_sizes, per_sample_weights, sharded_dim_size_max, padding_idx)->_sorted_input_distribute_prepare(input_split_sorted_list, input_split_sorted_indices, world_size, input, weight, per_sample_weights, rank, padding_idx)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(gathered_input, output_offsets_tensor_list, output_split_sizes, gathered_per_sample_weights)->_distribute_input(input_combined, input_combined_split_sizes, offsets_rearrange_list, offsets_rearrange_sizes, sharded_dim_size_max, world_size, input, per_sample_weights, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.result->torch.nn.functional.embedding_bag(inp, local_shard, offsets=output_offsets_tensor_list[i], mode=mode if mode != 'mean' else 'sum', per_sample_weights=per_sample_weights, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.gathered_output->torch.empty(row_size, weight.size(1), device=input.device)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.split_sizes_1d_tensor->torch.tensor(split_sizes_1d_with_padding, dtype=torch.float, device=input.device)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(sorted_input, input_split_sorted_indices)->torch.sort(input)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.input_split_sorted_indices->torch.cat(input_split_sorted_indices_list)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.offset_len->len(offsets)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.split_sizes_1d->split_size.tolist()
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(split_result_sorted, indices)->torch.sort(split_result)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.(input_sorted, input_split_sizes, sharded_dim_size_max, input_split_rearrange_indices, _, padding_idx_modular)->_handle_row_wise_lookup_distribute(split_result_sorted, input, world_size, weight, rank, padding_idx)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.input_split_rearrange_indices_combined->torch.cat(rearrange_indices_list)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.input_combined->torch.remainder(input_combined, sharded_dim_size_max)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.output_size_list->_communicate_size_to_each_rank(input_size_list, world_size * 2, input, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.input_combined_list->list(torch.split(input_combined, input_combined_split_sizes))
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.output_tensor_list->list(torch.split(torch.cat(output_tensor_list), output_size_list))
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.per_sample_weights_list->list(torch.split(per_sample_weights, input_combined_split_sizes))
A:torch.distributed._shard.sharded_tensor._ops.embedding_bag.gathered_per_sample_weights->_communicate_list_to_each_rank(per_sample_weights_list, output_split_sizes, input, pg, tensor_type=per_sample_weights.dtype)
torch.distributed._shard.sharded_tensor._ops.embedding_bag._distribute_input(input_combined,input_combined_split_sizes,offsets_rearrange_list,offsets_rearrange_sizes,sharded_dim_size_max,world_size,input,per_sample_weights,pg)
torch.distributed._shard.sharded_tensor._ops.embedding_bag._handle_col_wise_sharding(input,world_size,weight,local_shard,offsets,per_sample_weights,mode,max_norm,norm_type,padding_idx,pg)
torch.distributed._shard.sharded_tensor._ops.embedding_bag._handle_row_wise_sharding(input,world_size,weight,local_shard,offsets,per_sample_weights,mode,max_norm,norm_type,padding_idx,rank,pg)
torch.distributed._shard.sharded_tensor._ops.embedding_bag._input_split_sort(input,offsets,padding_idx)
torch.distributed._shard.sharded_tensor._ops.embedding_bag._sorted_input_distribute_prepare(input_split_sorted_list,input_split_sorted_indices,world_size,input,weight,per_sample_weights,rank,padding_idx)
torch.distributed._shard.sharded_tensor._ops.embedding_bag._validate_embedding_bag_param(args,kwargs)
torch.distributed._shard.sharded_tensor._ops.embedding_bag.sharded_embedding_bag(types,args,kwargs,pg)
torch.distributed._shard.sharded_tensor.embedding_bag._distribute_input(input_combined,input_combined_split_sizes,offsets_rearrange_list,offsets_rearrange_sizes,sharded_dim_size_max,world_size,input,per_sample_weights,pg)
torch.distributed._shard.sharded_tensor.embedding_bag._handle_col_wise_sharding(input,world_size,weight,local_shard,offsets,per_sample_weights,mode,max_norm,norm_type,padding_idx,pg)
torch.distributed._shard.sharded_tensor.embedding_bag._handle_row_wise_sharding(input,world_size,weight,local_shard,offsets,per_sample_weights,mode,max_norm,norm_type,padding_idx,rank,pg)
torch.distributed._shard.sharded_tensor.embedding_bag._input_split_sort(input,offsets,padding_idx)
torch.distributed._shard.sharded_tensor.embedding_bag._sorted_input_distribute_prepare(input_split_sorted_list,input_split_sorted_indices,world_size,input,weight,per_sample_weights,rank,padding_idx)
torch.distributed._shard.sharded_tensor.embedding_bag._validate_embedding_bag_param(args,kwargs)
torch.distributed._shard.sharded_tensor.sharded_embedding_bag(types,args,kwargs,pg)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/binary_cmp.py----------------------------------------
A:torch.distributed._shard.sharded_tensor._ops.binary_cmp.result_tensor->torch.zeros(1, device=torch.device(torch.cuda.current_device()))
A:torch.distributed._shard.sharded_tensor._ops.binary_cmp.st1_local_shards->st1.local_shards()
A:torch.distributed._shard.sharded_tensor._ops.binary_cmp.st2_local_shards->st2.local_shards()
torch.distributed._shard.sharded_tensor._ops.binary_cmp._communicate_result(result,pg)
torch.distributed._shard.sharded_tensor._ops.binary_cmp.allclose(types,args,kwargs,process_group)
torch.distributed._shard.sharded_tensor._ops.binary_cmp.binary_cmp(cmp_fun,types,args,kwargs=None,process_group=None)
torch.distributed._shard.sharded_tensor._ops.binary_cmp.equal(types,args,kwargs,process_group)
torch.distributed._shard.sharded_tensor.allclose(types,args,kwargs,process_group)
torch.distributed._shard.sharded_tensor.binary_cmp._communicate_result(result,pg)
torch.distributed._shard.sharded_tensor.binary_cmp.binary_cmp(cmp_fun,types,args,kwargs=None,process_group=None)
torch.distributed._shard.sharded_tensor.equal(types,args,kwargs,process_group)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/embedding.py----------------------------------------
A:torch.distributed._shard.sharded_tensor._ops.embedding.max_norm->kwargs.get('max_norm')
A:torch.distributed._shard.sharded_tensor._ops.embedding.norm_type->kwargs.get('norm_type')
A:torch.distributed._shard.sharded_tensor._ops.embedding.padding_idx->kwargs.get('padding_idx')
A:torch.distributed._shard.sharded_tensor._ops.embedding.local_shard->weight.local_tensor().contiguous()
A:torch.distributed._shard.sharded_tensor._ops.embedding.world_size->torch.distributed.get_world_size(pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding.rank->torch.distributed.get_rank(pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding.(output, local_shard)->_handle_col_wise_sharding(input, world_size, weight, local_shard, max_norm, norm_type, padding_idx, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding.scale_grad_by_freq->kwargs.get('scale_grad_by_freq')
A:torch.distributed._shard.sharded_tensor._ops.embedding.sparse->kwargs.get('sparse')
A:torch.distributed._shard.sharded_tensor._ops.embedding.weight_size->cast(torch.Size, weight.size())
A:torch.distributed._shard.sharded_tensor._ops.embedding.(local_shard, gathered_inputs)->_handle_max_norm_col_wise(max_norm, norm_type, local_shard, input, world_size, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding.output->_handle_col_wise_sharding_base(torch.nn.functional.embedding, len(input.size()), input, world_size, weight, local_shard, pg, padding_idx=padding_idx, gathered_inputs=gathered_inputs)
A:torch.distributed._shard.sharded_tensor._ops.embedding.input_size->input.size()
A:torch.distributed._shard.sharded_tensor._ops.embedding.input_1d->torch.reshape(input, (-1,)).contiguous()
A:torch.distributed._shard.sharded_tensor._ops.embedding.(input_sorted, indices_1d)->torch.sort(input_1d)
A:torch.distributed._shard.sharded_tensor._ops.embedding.rearrange_indices_1d->torch.argsort(indices_1d)
A:torch.distributed._shard.sharded_tensor._ops.embedding.(input_sorted, input_split_sizes, sharded_dim_size_max, _, rearrange_indices_1d_second_order, padding_idx)->_handle_row_wise_lookup_distribute(input_sorted, input, world_size, weight, rank, padding_idx)
A:torch.distributed._shard.sharded_tensor._ops.embedding.output_split_sizes->_communicate_size_to_each_rank(input_split_sizes, world_size, input, pg)
A:torch.distributed._shard.sharded_tensor._ops.embedding.gathered_input->torch.empty(sum(output_split_sizes), dtype=torch.int64, device=input.device)
A:torch.distributed._shard.sharded_tensor._ops.embedding.input_sorted->torch.remainder(input_sorted, sharded_dim_size_max)
A:torch.distributed._shard.sharded_tensor._ops.embedding.gathered_input_embeddings->torch.nn.functional.embedding(gathered_input, local_shard, padding_idx=padding_idx, max_norm=max_norm, norm_type=norm_type)
A:torch.distributed._shard.sharded_tensor._ops.embedding.gathered_output->torch.empty(input_sorted.size(0), weight.size(1), device=input.device)
torch.distributed._shard.sharded_tensor._ops.embedding._handle_col_wise_sharding(input,world_size,weight,local_shard,max_norm,norm_type,padding_idx,pg)
torch.distributed._shard.sharded_tensor._ops.embedding._handle_row_wise_sharding(input,world_size,weight,local_shard,max_norm,norm_type,padding_idx,rank,pg)
torch.distributed._shard.sharded_tensor._ops.embedding._validate_embedding_param(args,kwargs)
torch.distributed._shard.sharded_tensor._ops.embedding.sharded_embedding(types,args,kwargs,pg)
torch.distributed._shard.sharded_tensor.embedding._handle_col_wise_sharding(input,world_size,weight,local_shard,max_norm,norm_type,padding_idx,pg)
torch.distributed._shard.sharded_tensor.embedding._handle_row_wise_sharding(input,world_size,weight,local_shard,max_norm,norm_type,padding_idx,rank,pg)
torch.distributed._shard.sharded_tensor.embedding._validate_embedding_param(args,kwargs)
torch.distributed._shard.sharded_tensor.sharded_embedding(types,args,kwargs,pg)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_shard/sharded_tensor/_ops/elementwise_ops.py----------------------------------------
A:torch.distributed._shard.sharded_tensor._ops.elementwise_ops.new_st->torch.distributed._shard.sharded_tensor.ShardedTensor._init_from_local_shards(local_shards_new, input.size(), process_group=pg)
A:torch.distributed._shard.sharded_tensor._ops.elementwise_ops.new_st._sharding_spec->copy.deepcopy(input._sharding_spec)
torch.distributed._shard.sharded_tensor._ops.elementwise_ops.register_elementwise_op(op)
torch.distributed._shard.sharded_tensor.elementwiseregister_elementwise_op(op)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/_sharded_tensor/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/autograd/__init__.py----------------------------------------
A:torch.distributed.autograd.__init__.self.autograd_context->_new_context()
torch.distributed.autograd.__init__.context(object)
torch.distributed.autograd.__init__.context.__enter__(self)
torch.distributed.autograd.__init__.context.__exit__(self,type,value,traceback)
torch.distributed.autograd.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/events/api.py----------------------------------------
A:torch.distributed.elastic.events.api.data_dict->json.loads(data)
torch.distributed.elastic.events.Event
torch.distributed.elastic.events.Event.__str__(self)
torch.distributed.elastic.events.Event.deserialize(data:Union[str,'Event'])->'Event'
torch.distributed.elastic.events.Event.serialize(self)->str
torch.distributed.elastic.events.EventSource(str,Enum)
torch.distributed.elastic.events.NodeState(str,Enum)
torch.distributed.elastic.events.RdzvEvent
torch.distributed.elastic.events.RdzvEvent.__str__(self)
torch.distributed.elastic.events.RdzvEvent.deserialize(data:Union[str,'RdzvEvent'])->'RdzvEvent'
torch.distributed.elastic.events.RdzvEvent.serialize(self)->str
torch.distributed.elastic.events.api.Event
torch.distributed.elastic.events.api.Event.__str__(self)
torch.distributed.elastic.events.api.Event.deserialize(data:Union[str,'Event'])->'Event'
torch.distributed.elastic.events.api.Event.serialize(self)->str
torch.distributed.elastic.events.api.EventSource(str,Enum)
torch.distributed.elastic.events.api.NodeState(str,Enum)
torch.distributed.elastic.events.api.RdzvEvent
torch.distributed.elastic.events.api.RdzvEvent.__str__(self)
torch.distributed.elastic.events.api.RdzvEvent.deserialize(data:Union[str,'RdzvEvent'])->'RdzvEvent'
torch.distributed.elastic.events.api.RdzvEvent.serialize(self)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/events/handlers.py----------------------------------------
torch.distributed.elastic.events.get_logging_handler(destination:str='null')->logging.Handler
torch.distributed.elastic.events.handlers.get_logging_handler(destination:str='null')->logging.Handler


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/events/__init__.py----------------------------------------
A:torch.distributed.elastic.events.__init__._events_logger->logging.getLogger(f'torchelastic-events-{destination}')
A:torch.distributed.elastic.events.__init__.logging_handler->get_logging_handler(destination)
A:torch.distributed.elastic.events.__init__.hostname->socket.getfqdn()
A:torch.distributed.elastic.events.__init__.pid->os.getpid()
A:torch.distributed.elastic.events.__init__.callstack->inspect.stack()
A:torch.distributed.elastic.events.__init__.filename->os.path.basename(stack_depth_1.filename)
A:torch.distributed.elastic.events.__init__.error_trace->traceback.format_exc()
A:torch.distributed.elastic.events.__init__.event->RdzvEvent(name=f'{filename}:{name}', run_id=run_id, message=message, hostname=hostname, pid=pid, node_state=node_state, master_endpoint=master_endpoint, rank=rank, local_id=local_id, error_trace=error_trace)
torch.distributed.elastic.events.__init__._get_or_create_logger(destination:str='null')->logging.Logger
torch.distributed.elastic.events.__init__.construct_and_record_rdzv_event(run_id:str,message:str,node_state:NodeState,name:str='',hostname:str='',pid:Optional[int]=None,master_endpoint:str='',local_id:Optional[int]=None,rank:Optional[int]=None)->None
torch.distributed.elastic.events.__init__.record(event:Event,destination:str='null')->None
torch.distributed.elastic.events.__init__.record_rdzv_event(event:RdzvEvent)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/store.py----------------------------------------
A:torch.distributed.elastic.utils.store.data->f'{rank}'.encode(encoding='UTF-8')
A:torch.distributed.elastic.utils.store.agent_data->get_all(store, key_prefix, world_size)
torch.distributed.elastic.utils.store.barrier(store,rank:int,world_size:int,key_prefix:str,barrier_timeout:float=300)->None
torch.distributed.elastic.utils.store.get_all(store,prefix:str,size:int)
torch.distributed.elastic.utils.store.synchronize(store,data:bytes,rank:int,world_size:int,key_prefix:str,barrier_timeout:float=300)->List[bytes]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/logging.py----------------------------------------
A:torch.distributed.elastic.utils.logging.log->logging.getLogger(name)
A:torch.distributed.elastic.utils.logging.stack->inspect.stack()
A:torch.distributed.elastic.utils.logging.module->inspect.getmodule(frame_info[0])
torch.distributed.elastic.utils.logging._derive_module_name(depth:int=1)->Optional[str]
torch.distributed.elastic.utils.logging._setup_logger(name:Optional[str]=None)
torch.distributed.elastic.utils.logging.get_logger(name:Optional[str]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/distributed.py----------------------------------------
A:torch.distributed.elastic.utils.distributed.log->get_logger()
A:torch.distributed.elastic.utils.distributed.port->get_free_port()
A:torch.distributed.elastic.utils.distributed.store->torch.distributed.TCPStore(host_name=server_addr, port=port, world_size=world_size, is_master=is_server, timeout=datetime.timedelta(seconds=timeout), wait_for_workers=wait_for_workers)
A:torch.distributed.elastic.utils.distributed.idx->torch.distributed.TCPStore(host_name=server_addr, port=port, world_size=world_size, is_master=is_server, timeout=datetime.timedelta(seconds=timeout), wait_for_workers=wait_for_workers).add(_MEMBER_CHECKIN, 1)
A:torch.distributed.elastic.utils.distributed.sock->get_socket_with_port()
A:torch.distributed.elastic.utils.distributed.addrs->socket.getaddrinfo(host='localhost', port=None, family=socket.AF_UNSPEC, type=socket.SOCK_STREAM)
A:torch.distributed.elastic.utils.distributed.s->socket.socket(family, type, proto)
torch.distributed.elastic.utils.distributed._check_full_rank(store,world_size)
torch.distributed.elastic.utils.distributed.create_c10d_store(is_server:bool,server_addr:str,server_port:int=-1,world_size:int=1,timeout:float=60*10,wait_for_workers:bool=True,retries=3)
torch.distributed.elastic.utils.distributed.get_free_port()
torch.distributed.elastic.utils.distributed.get_socket_with_port()->socket.socket


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/api.py----------------------------------------
A:torch.distributed.elastic.utils.api.value->os.environ.get(env_name, None)
A:torch.distributed.elastic.utils.api.addrs->socket.getaddrinfo(host='localhost', port=None, family=socket.AF_UNSPEC, type=socket.SOCK_STREAM)
A:torch.distributed.elastic.utils.api.s->socket.socket(family, type, proto)
A:torch.distributed.elastic.utils.api.sub->Template(arg).safe_substitute(local_rank=local_rank)
torch.distributed.elastic.utils.api.get_env_variable_or_raise(env_name:str)->str
torch.distributed.elastic.utils.api.get_socket_with_port()->socket.socket
torch.distributed.elastic.utils.api.macros
torch.distributed.elastic.utils.api.macros.substitute(args:List[Any],local_rank:str)->List[str]
torch.distributed.elastic.utils.get_env_variable_or_raise(env_name:str)->str
torch.distributed.elastic.utils.get_socket_with_port()->socket.socket
torch.distributed.elastic.utils.macros
torch.distributed.elastic.utils.macros.substitute(args:List[Any],local_rank:str)->List[str]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/log_level.py----------------------------------------
torch.distributed.elastic.utils.log_level.get_log_level()->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/data/cycling_iterator.py----------------------------------------
A:torch.distributed.elastic.utils.data.cycling_iterator.self._iter->self._generator_fn(self._epoch)
torch.distributed.elastic.utils.data.CyclingIterator(self,n:int,generator_fn,start_epoch=0)
torch.distributed.elastic.utils.data.CyclingIterator.__iter__(self)
torch.distributed.elastic.utils.data.CyclingIterator.__next__(self)
torch.distributed.elastic.utils.data.cycling_iterator.CyclingIterator(self,n:int,generator_fn,start_epoch=0)
torch.distributed.elastic.utils.data.cycling_iterator.CyclingIterator.__init__(self,n:int,generator_fn,start_epoch=0)
torch.distributed.elastic.utils.data.cycling_iterator.CyclingIterator.__iter__(self)
torch.distributed.elastic.utils.data.cycling_iterator.CyclingIterator.__next__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/data/elastic_distributed_sampler.py----------------------------------------
A:torch.distributed.elastic.utils.data.elastic_distributed_sampler.self.num_samples->int(math.ceil(float(len(self.dataset) - self.start_index) / self.num_replicas))
A:torch.distributed.elastic.utils.data.elastic_distributed_sampler.g->torch.Generator()
A:torch.distributed.elastic.utils.data.elastic_distributed_sampler.indices->torch.randperm(len(self.dataset) - self.start_index, generator=g).add(self.start_index).tolist()
torch.distributed.elastic.utils.data.ElasticDistributedSampler(self,dataset,num_replicas=None,rank=None,start_index=0)
torch.distributed.elastic.utils.data.ElasticDistributedSampler.__iter__(self)
torch.distributed.elastic.utils.data.ElasticDistributedSampler.__len__(self)
torch.distributed.elastic.utils.data.elastic_distributed_sampler.ElasticDistributedSampler(self,dataset,num_replicas=None,rank=None,start_index=0)
torch.distributed.elastic.utils.data.elastic_distributed_sampler.ElasticDistributedSampler.__init__(self,dataset,num_replicas=None,rank=None,start_index=0)
torch.distributed.elastic.utils.data.elastic_distributed_sampler.ElasticDistributedSampler.__iter__(self)
torch.distributed.elastic.utils.data.elastic_distributed_sampler.ElasticDistributedSampler.__len__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/utils/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/api.py----------------------------------------
A:torch.distributed.elastic.multiprocessing.api.log->logging.getLogger(__name__)
A:torch.distributed.elastic.multiprocessing.api.sigval->signal.Signals(signum)
A:torch.distributed.elastic.multiprocessing.api.actual_keys->set(d.keys())
A:torch.distributed.elastic.multiprocessing.api.expected_keys->set(range(nprocs))
A:torch.distributed.elastic.multiprocessing.api.v->int(v)
A:torch.distributed.elastic.multiprocessing.api.(i, v)->m.split(':')
A:torch.distributed.elastic.multiprocessing.api.d[int(i)]->to_std(v)
A:torch.distributed.elastic.multiprocessing.api.map[i]->val_or_map.get(i, Std.NONE)
A:torch.distributed.elastic.multiprocessing.api.nprocs->len(args)
A:torch.distributed.elastic.multiprocessing.api.self._stdout_tail->TailLog(name, tee_stdouts, sys.stdout)
A:torch.distributed.elastic.multiprocessing.api.self._stderr_tail->TailLog(name, tee_stderrs, sys.stderr)
A:torch.distributed.elastic.multiprocessing.api.pr->self._poll()
A:torch.distributed.elastic.multiprocessing.api.death_sig->_get_default_signal()
A:torch.distributed.elastic.multiprocessing.api.stdout_cm->get_std_cm(stdout_rd, redirect_stdout)
A:torch.distributed.elastic.multiprocessing.api.stderr_cm->get_std_cm(stderr_rd, redirect_stderr)
A:torch.distributed.elastic.multiprocessing.api.ret->record(fn)(*args_)
A:torch.distributed.elastic.multiprocessing.api.self._worker_finished_event->torch.multiprocessing.get_context(self.start_method).Event()
A:torch.distributed.elastic.multiprocessing.api.self._pc->torch.multiprocessing.start_processes(fn=_wrap, args=(self.entrypoint, self.args, self.envs, self.stdouts, self.stderrs, self._ret_vals, self._worker_finished_event), nprocs=self.nprocs, join=False, daemon=False, start_method=self.start_method)
A:torch.distributed.elastic.multiprocessing.api.self._return_values[local_rank]->return_queue.get()
A:torch.distributed.elastic.multiprocessing.api.env_vars->os.environ.copy()
A:torch.distributed.elastic.multiprocessing.api.done_local_ranks->set()
A:torch.distributed.elastic.multiprocessing.api.exitcode->handler.proc.poll()
A:torch.distributed.elastic.multiprocessing.api.self._failures[local_rank]->ProcessFailure(local_rank=local_rank, pid=handler.proc.pid, exitcode=exitcode, error_file=self.error_files[local_rank])
A:torch.distributed.elastic.multiprocessing.api.result->RunProcsResult(failures=self._failures, stdouts=self.stdouts, stderrs=self.stderrs)
A:torch.distributed.elastic.multiprocessing.api.first_failure->min(result.failures.values(), key=lambda f: f.timestamp)
torch.distributed.elastic.multiprocessing.MultiprocessContext(self,name:str,entrypoint:Callable,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str],start_method:str)
torch.distributed.elastic.multiprocessing.MultiprocessContext._close(self,death_sig:signal.Signals,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.MultiprocessContext._is_done(self)->bool
torch.distributed.elastic.multiprocessing.MultiprocessContext._poll(self)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.MultiprocessContext._start(self)
torch.distributed.elastic.multiprocessing.MultiprocessContext.pids(self)->Dict[int, int]
torch.distributed.elastic.multiprocessing.PContext(self,name:str,entrypoint:Union[Callable,str],args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str])
torch.distributed.elastic.multiprocessing.PContext._close(self,death_sig:signal.Signals,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.PContext._poll(self)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.PContext._start(self)->None
torch.distributed.elastic.multiprocessing.PContext.close(self,death_sig:Optional[signal.Signals]=None,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.PContext.pids(self)->Dict[int, int]
torch.distributed.elastic.multiprocessing.PContext.start(self)->None
torch.distributed.elastic.multiprocessing.PContext.wait(self,timeout:float=-1,period:float=1)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.RunProcsResult
torch.distributed.elastic.multiprocessing.RunProcsResult.is_failed(self)->bool
torch.distributed.elastic.multiprocessing.SignalException(self,msg:str,sigval:signal.Signals)
torch.distributed.elastic.multiprocessing.Std(IntFlag)
torch.distributed.elastic.multiprocessing.Std.from_str(cls,vm:str)->Union['Std', Dict[int, 'Std']]
torch.distributed.elastic.multiprocessing.SubprocessContext(self,name:str,entrypoint:str,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str])
torch.distributed.elastic.multiprocessing.SubprocessContext._close(self,death_sig:signal.Signals,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.SubprocessContext._poll(self)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.SubprocessContext._start(self)
torch.distributed.elastic.multiprocessing.SubprocessContext.pids(self)->Dict[int, int]
torch.distributed.elastic.multiprocessing._validate_full_rank(d:Dict[int,Any],nprocs:int,what:str)
torch.distributed.elastic.multiprocessing.api.MultiprocessContext(self,name:str,entrypoint:Callable,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str],start_method:str)
torch.distributed.elastic.multiprocessing.api.MultiprocessContext.__init__(self,name:str,entrypoint:Callable,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str],start_method:str)
torch.distributed.elastic.multiprocessing.api.MultiprocessContext._close(self,death_sig:signal.Signals,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.api.MultiprocessContext._is_done(self)->bool
torch.distributed.elastic.multiprocessing.api.MultiprocessContext._poll(self)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.api.MultiprocessContext._start(self)
torch.distributed.elastic.multiprocessing.api.MultiprocessContext.pids(self)->Dict[int, int]
torch.distributed.elastic.multiprocessing.api.PContext(self,name:str,entrypoint:Union[Callable,str],args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str])
torch.distributed.elastic.multiprocessing.api.PContext.__init__(self,name:str,entrypoint:Union[Callable,str],args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str])
torch.distributed.elastic.multiprocessing.api.PContext._close(self,death_sig:signal.Signals,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.api.PContext._poll(self)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.api.PContext._start(self)->None
torch.distributed.elastic.multiprocessing.api.PContext.close(self,death_sig:Optional[signal.Signals]=None,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.api.PContext.pids(self)->Dict[int, int]
torch.distributed.elastic.multiprocessing.api.PContext.start(self)->None
torch.distributed.elastic.multiprocessing.api.PContext.wait(self,timeout:float=-1,period:float=1)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.api.RunProcsResult
torch.distributed.elastic.multiprocessing.api.RunProcsResult.is_failed(self)->bool
torch.distributed.elastic.multiprocessing.api.SignalException(self,msg:str,sigval:signal.Signals)
torch.distributed.elastic.multiprocessing.api.SignalException.__init__(self,msg:str,sigval:signal.Signals)
torch.distributed.elastic.multiprocessing.api.Std(IntFlag)
torch.distributed.elastic.multiprocessing.api.Std.from_str(cls,vm:str)->Union['Std', Dict[int, 'Std']]
torch.distributed.elastic.multiprocessing.api.SubprocessContext(self,name:str,entrypoint:str,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str])
torch.distributed.elastic.multiprocessing.api.SubprocessContext.__init__(self,name:str,entrypoint:str,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdouts:Dict[int,str],stderrs:Dict[int,str],tee_stdouts:Dict[int,str],tee_stderrs:Dict[int,str],error_files:Dict[int,str])
torch.distributed.elastic.multiprocessing.api.SubprocessContext._close(self,death_sig:signal.Signals,timeout:int=30)->None
torch.distributed.elastic.multiprocessing.api.SubprocessContext._poll(self)->Optional[RunProcsResult]
torch.distributed.elastic.multiprocessing.api.SubprocessContext._start(self)
torch.distributed.elastic.multiprocessing.api.SubprocessContext.pids(self)->Dict[int, int]
torch.distributed.elastic.multiprocessing.api.SubprocessHandler(self,entrypoint:str,args:Tuple,env:Dict[str,str],stdout:str,stderr:str)
torch.distributed.elastic.multiprocessing.api.SubprocessHandler.__init__(self,entrypoint:str,args:Tuple,env:Dict[str,str],stdout:str,stderr:str)
torch.distributed.elastic.multiprocessing.api.SubprocessHandler._popen(self,args:Tuple,env:Dict[str,str])->subprocess.Popen
torch.distributed.elastic.multiprocessing.api.SubprocessHandler.close(self,death_sig:Optional[signal.Signals]=None)->None
torch.distributed.elastic.multiprocessing.api._get_default_signal()->signal.Signals
torch.distributed.elastic.multiprocessing.api._get_kill_signal()->signal.Signals
torch.distributed.elastic.multiprocessing.api._nullcontext(self,enter_result=None)
torch.distributed.elastic.multiprocessing.api._nullcontext.__enter__(self)
torch.distributed.elastic.multiprocessing.api._nullcontext.__exit__(self,*excinfo)
torch.distributed.elastic.multiprocessing.api._nullcontext.__init__(self,enter_result=None)
torch.distributed.elastic.multiprocessing.api._terminate_process_handler(signum:int,frame:FrameType)->None
torch.distributed.elastic.multiprocessing.api._validate_full_rank(d:Dict[int,Any],nprocs:int,what:str)
torch.distributed.elastic.multiprocessing.api._wrap(local_rank:int,fn:Callable,args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],stdout_redirects:Dict[int,str],stderr_redirects:Dict[int,str],ret_vals:Dict[int,mp.SimpleQueue],queue_finished_reading_event:synchronize.Event)->None
torch.distributed.elastic.multiprocessing.api.get_std_cm(std_rd:str,redirect_fn)
torch.distributed.elastic.multiprocessing.api.to_map(val_or_map:Union[Std,Dict[int,Std]],local_world_size:int)->Dict[int, Std]
torch.distributed.elastic.multiprocessing.to_map(val_or_map:Union[Std,Dict[int,Std]],local_world_size:int)->Dict[int, Std]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/tail_log.py----------------------------------------
A:torch.distributed.elastic.multiprocessing.tail_log.log->logging.getLogger(__name__)
A:torch.distributed.elastic.multiprocessing.tail_log.line->fp.readline()
A:torch.distributed.elastic.multiprocessing.tail_log.n->len(log_files)
A:torch.distributed.elastic.multiprocessing.tail_log.self._threadpool->ThreadPoolExecutor(max_workers=n, thread_name_prefix=f'{self.__class__.__qualname__}_{name}')
torch.distributed.elastic.multiprocessing.tail_log.TailLog(self,name:str,log_files:Dict[int,str],dst:TextIO,interval_sec:float=0.1)
torch.distributed.elastic.multiprocessing.tail_log.TailLog.__init__(self,name:str,log_files:Dict[int,str],dst:TextIO,interval_sec:float=0.1)
torch.distributed.elastic.multiprocessing.tail_log.TailLog.start(self)->'TailLog'
torch.distributed.elastic.multiprocessing.tail_log.TailLog.stop(self)->None
torch.distributed.elastic.multiprocessing.tail_log.TailLog.stopped(self)->bool
torch.distributed.elastic.multiprocessing.tail_log.tail_logfile(header:str,file:str,dst:TextIO,finished:Event,interval_sec:float)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/redirects.py----------------------------------------
A:torch.distributed.elastic.multiprocessing.redirects.logger->logging.getLogger(__name__)
A:torch.distributed.elastic.multiprocessing.redirects.libc->get_libc()
A:torch.distributed.elastic.multiprocessing.redirects.c_std->_c_std(std)
A:torch.distributed.elastic.multiprocessing.redirects.python_std->_python_std(std)
A:torch.distributed.elastic.multiprocessing.redirects.std_fd->_python_std(std).fileno()
A:torch.distributed.elastic.multiprocessing.redirects.redirect_stdout->partial(redirect, 'stdout')
A:torch.distributed.elastic.multiprocessing.redirects.redirect_stderr->partial(redirect, 'stderr')
torch.distributed.elastic.multiprocessing.redirects._c_std(stream:str)
torch.distributed.elastic.multiprocessing.redirects._python_std(stream:str)
torch.distributed.elastic.multiprocessing.redirects.get_libc()
torch.distributed.elastic.multiprocessing.redirects.redirect(std:str,to_file:str)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/__init__.py----------------------------------------
A:torch.distributed.elastic.multiprocessing.__init__.log->get_logger()
A:torch.distributed.elastic.multiprocessing.__init__.nprocs->len(args)
A:torch.distributed.elastic.multiprocessing.__init__.redirs->to_map(redirects, nprocs)
A:torch.distributed.elastic.multiprocessing.__init__.ts->to_map(tee, nprocs)
A:torch.distributed.elastic.multiprocessing.__init__.clogdir->os.path.join(log_dir, str(local_rank))
A:torch.distributed.elastic.multiprocessing.__init__.stdouts[local_rank]->os.path.join(clogdir, 'stdout.log')
A:torch.distributed.elastic.multiprocessing.__init__.stderrs[local_rank]->os.path.join(clogdir, 'stderr.log')
A:torch.distributed.elastic.multiprocessing.__init__.error_file->os.path.join(clogdir, 'error.json')
A:torch.distributed.elastic.multiprocessing.__init__.context->MultiprocessContext(name=name, entrypoint=entrypoint, args=args, envs=envs, stdouts=stdouts, stderrs=stderrs, tee_stdouts=tee_stdouts, tee_stderrs=tee_stderrs, error_files=error_files, start_method=start_method)
torch.distributed.elastic.multiprocessing.__init__.start_processes(name:str,entrypoint:Union[Callable,str],args:Dict[int,Tuple],envs:Dict[int,Dict[str,str]],log_dir:str,start_method:str='spawn',redirects:Union[Std,Dict[int,Std]]=Std.NONE,tee:Union[Std,Dict[int,Std]]=Std.NONE)->PContext


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/errors/handlers.py----------------------------------------
torch.distributed.elastic.multiprocessing.errors.get_error_handler()
torch.distributed.elastic.multiprocessing.errors.handlers.get_error_handler()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/errors/__init__.py----------------------------------------
A:torch.distributed.elastic.multiprocessing.errors.__init__.log->get_logger()
A:torch.distributed.elastic.multiprocessing.errors.__init__.T->TypeVar('T')
A:torch.distributed.elastic.multiprocessing.errors.__init__.self.error_file_data->json.load(fp)
A:torch.distributed.elastic.multiprocessing.errors.__init__.(self.message, self.timestamp)->self._get_error_data(self.error_file_data)
A:torch.distributed.elastic.multiprocessing.errors.__init__.timestamp->int(message['extraInfo']['timestamp'])
A:torch.distributed.elastic.multiprocessing.errors.__init__.self.timestamp->int(time.time())
A:torch.distributed.elastic.multiprocessing.errors.__init__.rank->min(self.failures.keys(), key=lambda r: self.failures[r].timestamp)
A:torch.distributed.elastic.multiprocessing.errors.__init__.(root_rank, root_failure)->self.get_first_failure()
A:torch.distributed.elastic.multiprocessing.errors.__init__.width->max(width, len(line))
A:torch.distributed.elastic.multiprocessing.errors.__init__.(fmt, w)->self._format_failure(idx, rank, failure)
A:torch.distributed.elastic.multiprocessing.errors.__init__.msg->failure.message.get('extraInfo', {}).get('py_callstack', failure.message.get('message', '<N/A>')).replace('\n', '\n  ')
A:torch.distributed.elastic.multiprocessing.errors.__init__.fmt->Template(_FAILURE_FORMAT_TEMPLATE).substitute(idx=idx, time=failure.timestamp_isoformat(), hostname=socket.getfqdn(), rank=rank, local_rank=failure.local_rank, exitcode=failure.exitcode, pid=failure.pid, error_file=failure.error_file, message=msg)
A:torch.distributed.elastic.multiprocessing.errors.__init__.error_handler->get_error_handler()
A:torch.distributed.elastic.multiprocessing.errors.__init__.(rank, failure)->e.get_first_failure()
torch.distributed.elastic.multiprocessing.errors.__init__.ChildFailedError(self,name:str,failures:Dict[GlobalRank,ProcessFailure])
torch.distributed.elastic.multiprocessing.errors.__init__.ChildFailedError.__init__(self,name:str,failures:Dict[GlobalRank,ProcessFailure])
torch.distributed.elastic.multiprocessing.errors.__init__.ChildFailedError._format_failure(self,idx:int,rank:int,failure:ProcessFailure)->Tuple[str, int]
torch.distributed.elastic.multiprocessing.errors.__init__.ChildFailedError.format_msg(self,boarder_delim='=',section_delim='-')
torch.distributed.elastic.multiprocessing.errors.__init__.ChildFailedError.get_first_failure(self)->Tuple[GlobalRank, ProcessFailure]
torch.distributed.elastic.multiprocessing.errors.__init__.ProcessFailure
torch.distributed.elastic.multiprocessing.errors.__init__.ProcessFailure.__post_init__(self)
torch.distributed.elastic.multiprocessing.errors.__init__.ProcessFailure._get_error_data(self,error_file_data:Dict[str,Any])->Tuple[str, int]
torch.distributed.elastic.multiprocessing.errors.__init__.ProcessFailure._set_no_reply_file(self)
torch.distributed.elastic.multiprocessing.errors.__init__.ProcessFailure.signal_name(self)->str
torch.distributed.elastic.multiprocessing.errors.__init__.ProcessFailure.timestamp_isoformat(self)
torch.distributed.elastic.multiprocessing.errors.__init__.record(fn:Callable[...,T],error_handler:Optional[ErrorHandler]=None)->Callable[..., T]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/multiprocessing/errors/error_handler.py----------------------------------------
A:torch.distributed.elastic.multiprocessing.errors.error_handler.log->logging.getLogger(__name__)
A:torch.distributed.elastic.multiprocessing.errors.error_handler.rootcause_error->json.load(fp)
A:torch.distributed.elastic.multiprocessing.errors.error_handler.my_error_file->self._get_error_file_path()
A:torch.distributed.elastic.multiprocessing.errors.error_handler.original->json.dumps(json.load(fp), indent=2)
torch.distributed.elastic.multiprocessing.errors.ErrorHandler
torch.distributed.elastic.multiprocessing.errors.ErrorHandler._get_error_file_path(self)->Optional[str]
torch.distributed.elastic.multiprocessing.errors.ErrorHandler._rm(self,my_error_file)
torch.distributed.elastic.multiprocessing.errors.ErrorHandler._write_error_file(self,file_path:str,error_msg:str)->None
torch.distributed.elastic.multiprocessing.errors.ErrorHandler.dump_error_file(self,rootcause_error_file:str,error_code:int=0)
torch.distributed.elastic.multiprocessing.errors.ErrorHandler.initialize(self)->None
torch.distributed.elastic.multiprocessing.errors.ErrorHandler.record_exception(self,e:BaseException)->None
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler._get_error_file_path(self)->Optional[str]
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler._rm(self,my_error_file)
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler._write_error_file(self,file_path:str,error_msg:str)->None
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler.dump_error_file(self,rootcause_error_file:str,error_code:int=0)
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler.initialize(self)->None
torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler.record_exception(self,e:BaseException)->None
torch.distributed.elastic.multiprocessing.errors.error_handler._write_error(e:BaseException,error_file:Optional[str])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/metrics/api.py----------------------------------------
A:torch.distributed.elastic.metrics.api.MetricData->namedtuple('MetricData', ['timestamp', 'group_name', 'name', 'value'])
A:torch.distributed.elastic.metrics.api._default_metrics_handler->NullMetricHandler()
A:torch.distributed.elastic.metrics.api.split->qualname.split('.')
A:torch.distributed.elastic.metrics.api.key->_get_metric_name(f)
A:torch.distributed.elastic.metrics.api.start->time.time()
A:torch.distributed.elastic.metrics.api.result->func(*args, **kwargs)
A:torch.distributed.elastic.metrics.api.start_time->time.time()
A:torch.distributed.elastic.metrics.api.metric_stream->getStream(metric_group)
A:torch.distributed.elastic.metrics.api.end_time->time.time()
torch.distributed.elastic.metrics.ConsoleMetricHandler(MetricHandler)
torch.distributed.elastic.metrics.ConsoleMetricHandler.emit(self,metric_data:MetricData)
torch.distributed.elastic.metrics.MetricHandler(abc.ABC)
torch.distributed.elastic.metrics.MetricHandler.emit(self,metric_data:MetricData)
torch.distributed.elastic.metrics.MetricsConfig(self,params:Optional[Dict[str,str]]=None)
torch.distributed.elastic.metrics.NullMetricHandler(MetricHandler)
torch.distributed.elastic.metrics.NullMetricHandler.emit(self,metric_data:MetricData)
torch.distributed.elastic.metrics.api.ConsoleMetricHandler(MetricHandler)
torch.distributed.elastic.metrics.api.ConsoleMetricHandler.emit(self,metric_data:MetricData)
torch.distributed.elastic.metrics.api.MetricHandler(abc.ABC)
torch.distributed.elastic.metrics.api.MetricHandler.emit(self,metric_data:MetricData)
torch.distributed.elastic.metrics.api.MetricStream(self,group_name:str,handler:MetricHandler)
torch.distributed.elastic.metrics.api.MetricStream.__init__(self,group_name:str,handler:MetricHandler)
torch.distributed.elastic.metrics.api.MetricStream.add_value(self,metric_name:str,metric_value:int)
torch.distributed.elastic.metrics.api.MetricsConfig(self,params:Optional[Dict[str,str]]=None)
torch.distributed.elastic.metrics.api.MetricsConfig.__init__(self,params:Optional[Dict[str,str]]=None)
torch.distributed.elastic.metrics.api.NullMetricHandler(MetricHandler)
torch.distributed.elastic.metrics.api.NullMetricHandler.emit(self,metric_data:MetricData)
torch.distributed.elastic.metrics.api._get_metric_name(fn)
torch.distributed.elastic.metrics.api.configure(handler:MetricHandler,group:str=None)
torch.distributed.elastic.metrics.api.getStream(group:str)
torch.distributed.elastic.metrics.api.get_elapsed_time_ms(start_time_in_seconds:float)
torch.distributed.elastic.metrics.api.prof(fn=None,group:str='torchelastic')
torch.distributed.elastic.metrics.api.profile(group=None)
torch.distributed.elastic.metrics.api.publish_metric(metric_group:str,metric_name:str,metric_value:int)
torch.distributed.elastic.metrics.api.put_metric(metric_name:str,metric_value:int,metric_group:str='torchelastic')
torch.distributed.elastic.metrics.configure(handler:MetricHandler,group:str=None)
torch.distributed.elastic.metrics.getStream(group:str)
torch.distributed.elastic.metrics.get_elapsed_time_ms(start_time_in_seconds:float)
torch.distributed.elastic.metrics.prof(fn=None,group:str='torchelastic')
torch.distributed.elastic.metrics.profile(group=None)
torch.distributed.elastic.metrics.publish_metric(metric_group:str,metric_name:str,metric_value:int)
torch.distributed.elastic.metrics.put_metric(metric_name:str,metric_value:int,metric_group:str='torchelastic')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/metrics/__init__.py----------------------------------------
torch.distributed.elastic.metrics.__init__.initialize_metrics(cfg:Optional[MetricsConfig]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/static_tcp_rendezvous.py----------------------------------------
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.log->logging.getLogger(__name__)
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.self.timeout->datetime.timedelta(seconds=timeout)
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.self._store->TCPStore(self.master_addr, self.master_port, self.world_size, is_master, self.timeout, multi_tenant=True)
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.store->PrefixStore(self.run_id, self._store)
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.endpoint->params.endpoint.strip()
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.(master_addr, master_port)->parse_rendezvous_endpoint(endpoint, -1)
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.rank->cast(int, params.config.get('rank'))
A:torch.distributed.elastic.rendezvous.static_tcp_rendezvous.timeout->int(params.config['timeout'])
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous(self,master_addr:str,master_port:int,rank:int,world_size:int,run_id:str,timeout:int)
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.__init__(self,master_addr:str,master_port:int,rank:int,world_size:int,run_id:str,timeout:int)
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.get_backend(self)->str
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.get_run_id(self)->str
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.is_closed(self)
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.next_rendezvous(self)->Tuple[Store, int, int]
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.num_nodes_waiting(self)
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.set_closed(self)
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.StaticTCPRendezvous.shutdown(self)->bool
torch.distributed.elastic.rendezvous.static_tcp_rendezvous.create_rdzv_handler(params:RendezvousParameters)->RendezvousHandler


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/c10d_rendezvous_backend.py----------------------------------------
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.log->logging.getLogger(__name__)
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.result->self.get_state()
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.token->token.decode().decode()
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.state_token_pair->self._decode_state(base64_state)
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.state->b64decode(base64_state)
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.(host, port)->parse_rendezvous_endpoint(params.endpoint, default_port=29400)
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.cfg_is_host->params.get_as_bool('is_host')
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.is_host->_matches_machine_hostname(host)
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.read_timeout->cast(int, params.get_as_int('read_timeout', 60))
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.store->_create_tcp_store(params)
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.(_, path)->tempfile.mkstemp()
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.store_type->params.get('store_type', 'tcp').strip().lower()
A:torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.backend->C10dRendezvousBackend(store, params.run_id)
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend(self,store:Store,run_id:str)
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.__init__(self,store:Store,run_id:str)
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend._call_store(self,store_op:str,*args,**kwargs)->Any
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend._decode_state(self,base64_state:bytes)->Optional[Tuple[bytes, Token]]
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state(self)->Optional[Tuple[bytes, Token]]
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name(self)->str
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state(self,state:bytes,token:Optional[Token]=None)->Optional[Tuple[bytes, Token, bool]]
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend._create_file_store(params:RendezvousParameters)->FileStore
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend._create_tcp_store(params:RendezvousParameters)->TCPStore
torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend(params:RendezvousParameters)->Tuple[C10dRendezvousBackend, Store]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/etcd_store.py----------------------------------------
A:torch.distributed.elastic.rendezvous.etcd_store.kvs->self._try_wait_get(b64_keys, override_timeout=datetime.timedelta(microseconds=1))
A:torch.distributed.elastic.rendezvous.etcd_store.b64_key->self._encode(key)
A:torch.distributed.elastic.rendezvous.etcd_store.node->self.client.test_and_set(key=node.key, value=new_value, prev_value=node.value)
A:torch.distributed.elastic.rendezvous.etcd_store.new_value->self._encode(str(int(self._decode(node.value)) + num))
A:torch.distributed.elastic.rendezvous.etcd_store.all_nodes->self.client.get(key=self.prefix)
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore(self,etcd_client,etcd_store_prefix,timeout:Optional[datetime.timedelta]=None)
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.__init__(self,etcd_client,etcd_store_prefix,timeout:Optional[datetime.timedelta]=None)
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore._decode(self,value)->bytes
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore._encode(self,value)->str
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore._try_wait_get(self,b64_keys,override_timeout=None)
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add(self,key,num:int)->int
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check(self,keys)->bool
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get(self,key)->bytes
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set(self,key,value)
torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait(self,keys,override_timeout:Optional[datetime.timedelta]=None)
torch.distributed.elastic.rendezvous.etcd_store.cas_delay()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/utils.py----------------------------------------
A:torch.distributed.elastic.rendezvous.utils.config_str->config_str.strip().strip()
A:torch.distributed.elastic.rendezvous.utils.key_values->config_str.strip().strip().split(',')
A:torch.distributed.elastic.rendezvous.utils.(key, *values)->kv.split('=', 1)
A:torch.distributed.elastic.rendezvous.utils.key->key.strip().strip()
A:torch.distributed.elastic.rendezvous.utils.value->values[0].strip()
A:torch.distributed.elastic.rendezvous.utils.endpoint->endpoint.strip().strip()
A:torch.distributed.elastic.rendezvous.utils.(host, *rest)->endpoint.strip().strip().rsplit(':', 1)
A:torch.distributed.elastic.rendezvous.utils.port->_try_parse_port(rest[0])
A:torch.distributed.elastic.rendezvous.utils.addr->ipaddress.ip_address(host)
A:torch.distributed.elastic.rendezvous.utils.this_host->socket.gethostname()
A:torch.distributed.elastic.rendezvous.utils.addr_list->socket.getaddrinfo(this_host, None, proto=socket.IPPROTO_TCP, flags=socket.AI_CANONNAME)
A:torch.distributed.elastic.rendezvous.utils.seconds->random.uniform(*seconds)
A:torch.distributed.elastic.rendezvous.utils.self._ctx->self._Context()
A:torch.distributed.elastic.rendezvous.utils.self._ctx.interval->interval.total_seconds()
A:torch.distributed.elastic.rendezvous.utils.self._ctx.stop_event->Event()
A:torch.distributed.elastic.rendezvous.utils.self._thread->Thread(target=self._run, name=self._name or 'PeriodicTimer', args=(self._ctx,), daemon=True)
A:torch.distributed.elastic.rendezvous.utils.self._finalizer->weakref.finalize(self, self._stop_thread, self._thread, self._ctx.stop_event)
torch.distributed.elastic.rendezvous.utils._PeriodicTimer(self,interval:timedelta,function:Callable[...,None],*args:Any,**kwargs:Any)
torch.distributed.elastic.rendezvous.utils._PeriodicTimer._Context
torch.distributed.elastic.rendezvous.utils._PeriodicTimer.__init__(self,interval:timedelta,function:Callable[...,None],*args:Any,**kwargs:Any)
torch.distributed.elastic.rendezvous.utils._PeriodicTimer._run(ctx)->None
torch.distributed.elastic.rendezvous.utils._PeriodicTimer._stop_thread(thread,stop_event)
torch.distributed.elastic.rendezvous.utils._PeriodicTimer.cancel(self)->None
torch.distributed.elastic.rendezvous.utils._PeriodicTimer.name(self)->Optional[str]
torch.distributed.elastic.rendezvous.utils._PeriodicTimer.set_name(self,name:str)->None
torch.distributed.elastic.rendezvous.utils._PeriodicTimer.start(self)->None
torch.distributed.elastic.rendezvous.utils._delay(seconds:Union[float,Tuple[float,float]])->None
torch.distributed.elastic.rendezvous.utils._matches_machine_hostname(host:str)->bool
torch.distributed.elastic.rendezvous.utils._parse_rendezvous_config(config_str:str)->Dict[str, str]
torch.distributed.elastic.rendezvous.utils._try_parse_port(port_str:str)->Optional[int]
torch.distributed.elastic.rendezvous.utils.parse_rendezvous_endpoint(endpoint:Optional[str],default_port:int)->Tuple[str, int]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/etcd_rendezvous_backend.py----------------------------------------
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.result->self._client.write(self._key, base64_state, self._ttl, **kwargs)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.base64_state->self._client.write(self._key, base64_state, self._ttl, **kwargs).value.encode()
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.token->int(token)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.state->b64decode(base64_state)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.(host, port)->parse_rendezvous_endpoint(params.endpoint, default_port=2379)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.read_timeout->cast(int, params.get_as_int('read_timeout', 60))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.protocol->params.get('protocol', 'http').strip().lower()
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.ssl_cert->params.get('ssl_cert')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.ssl_cert_key->params.get('ssl_cert_key')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.ca_cert->params.get('ca_cert')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.client->_create_etcd_client(params)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.backend->EtcdRendezvousBackend(client, params.run_id, key_prefix='/torch/elastic/rendezvous')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.store->EtcdStore(client, '/torch/elastic/store')
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend(self,client:EtcdClient,run_id:str,key_prefix:Optional[str]=None,ttl:Optional[int]=None)
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.__init__(self,client:EtcdClient,run_id:str,key_prefix:Optional[str]=None,ttl:Optional[int]=None)
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend._decode_state(self,result:EtcdResult)->Tuple[bytes, Token]
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state(self)->Optional[Tuple[bytes, Token]]
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name(self)->str
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state(self,state:bytes,token:Optional[Token]=None)->Optional[Tuple[bytes, Token, bool]]
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend._create_etcd_client(params:RendezvousParameters)->EtcdClient
torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend(params:RendezvousParameters)->Tuple[EtcdRendezvousBackend, Store]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/registry.py----------------------------------------
A:torch.distributed.elastic.rendezvous.registry.(backend, store)->create_backend(params)
torch.distributed.elastic.rendezvous._register_default_handlers()->None
torch.distributed.elastic.rendezvous.registry._create_c10d_handler(params:RendezvousParameters)->RendezvousHandler
torch.distributed.elastic.rendezvous.registry._create_etcd_handler(params:RendezvousParameters)->RendezvousHandler
torch.distributed.elastic.rendezvous.registry._create_etcd_v2_handler(params:RendezvousParameters)->RendezvousHandler
torch.distributed.elastic.rendezvous.registry._create_static_handler(params:RendezvousParameters)->RendezvousHandler
torch.distributed.elastic.rendezvous.registry._register_default_handlers()->None
torch.distributed.elastic.rendezvous.registry.get_rendezvous_handler(params:RendezvousParameters)->RendezvousHandler


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/etcd_rendezvous.py----------------------------------------
A:torch.distributed.elastic.rendezvous.etcd_rendezvous._log_fmt->logging.Formatter('%(levelname)s %(asctime)s %(message)s')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous._log_handler->logging.StreamHandler(sys.stderr)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.log->logging.getLogger(__name__)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.(rdzv_version, rank, world_size)->self._rdzv_impl.rendezvous_barrier()
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.store->self._rdzv_impl.setup_kv_store(rdzv_version)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.(_, state)->self._rdzv_impl.get_rdzv_state()
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.self._lease_run_id_stop->self.setup_lease_renewal(self.get_path(''), ttl=CONST_RUNID_SUBROOT_TTL)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.active_version->self.client.get(key=self.get_path('/rdzv/active_version'))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.state->json.loads(active_version.value)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.(active_version, state)->self.get_rdzv_state()
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.(active_version, this_rank)->self.join_rendezvous(expected_version)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.active_state->self.announce_self_waiting(expected_version)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.version_counter->self.client.get(self.get_path('/rdzv/version_counter'))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.version_counter.value->str(int(version_counter.value) + 1)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.this_rank->len(state['participants'])
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.this_lease_key->self.get_path('/rdzv/v_{}/rank_{}'.format(expected_version, this_rank))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.self._lease_this_rank_stop->self.setup_lease_renewal(this_lease_key, ttl=CONST_WORKER_KEEPALIVE_TTL)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.alive_members->self.client.get(self.get_path('/rdzv/v_{version}'.format(version=expected_version)))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.timeout->min(CONST_ETCD_JOINABLE_EPHEMERAL_TTL / 2, deadline - time.time() + 1.0)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.lease_stop_event->threading.Event()
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.lease_thread->threading.Thread(target=lease_worker, args=(self.client, full_path, ttl, lease_stop_event))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.node->self.get_path('/rdzv/v_{}/extra_data'.format(rdzv_version))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.extra_data->self.client.test_and_set(key=node, value=json.dumps(new_extra_data_value), prev_value=extra_data.value)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.new_extra_data_value->json.loads(extra_data.value)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.node_dir->self.get_path('/rdzv/v_{}'.format(rdzv_version))
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.root->self.client.get(node_dir)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.extra_data_dict->json.loads(extra_data[0].value)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.store_path->self.get_path(f'/rdzv/v_{rdzv_version}/kv')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.(hostname, port)->parse_rendezvous_endpoint(params.endpoint, 2379)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.protocol->params.config.get('protocol')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.ssl_cert->params.config.get('cert')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.cert_key->params.config.get('key')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.ca_cert->params.config.get('cacert')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.client->_create_etcd_client(params)
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.etcd_prefix->params.get('etcd_prefix', '/torchelastic/p2p')
A:torch.distributed.elastic.rendezvous.etcd_rendezvous.rdzv->EtcdRendezvous(client=client, prefix=etcd_prefix, run_id=params.run_id, num_min_workers=params.min_nodes, num_max_workers=params.max_nodes, timeout=params.get_as_int('timeout', _DEFAULT_TIMEOUT), last_call_timeout=params.get_as_int('last_call_timeout', _DEFAULT_LAST_CALL_TIMEOUT))
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous(self,client,prefix,run_id,num_min_workers,num_max_workers,timeout,last_call_timeout)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.__del__(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.__init__(self,client,prefix,run_id,num_min_workers,num_max_workers,timeout,last_call_timeout)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.announce_self_waiting(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.confirm_membership(self,expected_version,this_rank)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.confirm_phase(self,expected_version,this_rank)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.create_path_if_not_exists(self,full_path,ttl=None)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.get_path(self,path)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.get_rdzv_state(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.handle_existing_rendezvous(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.handle_join_last_call(self,expected_version,deadline)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.init_phase(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.join_phase(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.join_rendezvous(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.load_extra_data(self,rdzv_version,key,timeout=None)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.rendezvous_barrier(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.set_closed(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.setup_kv_store(self,rdzv_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.setup_lease_renewal(self,full_path,ttl)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.store_extra_data(self,rdzv_version,key,value)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.try_create_rendezvous(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.try_wait_for_state_change(self,etcd_index,timeout=None)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.wait_for_final(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.wait_for_peers(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.wait_for_rendezvous_to_free(self,expected_version)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler(self,rdzv_impl)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.__del__(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.__init__(self,rdzv_impl)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.get_backend(self)->str
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.get_run_id(self)->str
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.is_closed(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.next_rendezvous(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.num_nodes_waiting(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.set_closed(self)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler.shutdown(self)->bool
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousRetryImmediately(Exception)
torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousRetryableFailure(Exception)
torch.distributed.elastic.rendezvous.etcd_rendezvous._create_etcd_client(params:RendezvousParameters)->etcd.Client
torch.distributed.elastic.rendezvous.etcd_rendezvous.create_rdzv_handler(params:RendezvousParameters)->RendezvousHandler


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/api.py----------------------------------------
A:torch.distributed.elastic.rendezvous.api.value->self.get(key, default)
A:torch.distributed.elastic.rendezvous.api.handler->creator(params)
A:torch.distributed.elastic.rendezvous.api.rendezvous_handler_registry->RendezvousHandlerRegistry()
torch.distributed.elastic.rendezvous.RendezvousClosedError(RendezvousError)
torch.distributed.elastic.rendezvous.RendezvousConnectionError(RendezvousError)
torch.distributed.elastic.rendezvous.RendezvousError(Exception)
torch.distributed.elastic.rendezvous.RendezvousHandler(ABC)
torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend(self)->str
torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id(self)->str
torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed(self)->bool
torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous(self)->Tuple[Store, int, int]
torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting(self)->int
torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed(self)
torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown(self)->bool
torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry(self)
torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler(self,params:RendezvousParameters)->RendezvousHandler
torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register(self,backend:str,creator:RendezvousHandlerCreator)->None
torch.distributed.elastic.rendezvous.RendezvousParameters(self,backend:str,endpoint:str,run_id:str,min_nodes:int,max_nodes:int,**kwargs)
torch.distributed.elastic.rendezvous.RendezvousParameters.get(self,key:str,default:Any=None)->Any
torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool(self,key:str,default:Optional[bool]=None)->Optional[bool]
torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int(self,key:str,default:Optional[int]=None)->Optional[int]
torch.distributed.elastic.rendezvous.RendezvousStateError(RendezvousError)
torch.distributed.elastic.rendezvous.RendezvousTimeoutError(RendezvousError)
torch.distributed.elastic.rendezvous.api.RendezvousClosedError(RendezvousError)
torch.distributed.elastic.rendezvous.api.RendezvousConnectionError(RendezvousError)
torch.distributed.elastic.rendezvous.api.RendezvousError(Exception)
torch.distributed.elastic.rendezvous.api.RendezvousHandler(ABC)
torch.distributed.elastic.rendezvous.api.RendezvousHandler.get_backend(self)->str
torch.distributed.elastic.rendezvous.api.RendezvousHandler.get_run_id(self)->str
torch.distributed.elastic.rendezvous.api.RendezvousHandler.is_closed(self)->bool
torch.distributed.elastic.rendezvous.api.RendezvousHandler.next_rendezvous(self)->Tuple[Store, int, int]
torch.distributed.elastic.rendezvous.api.RendezvousHandler.num_nodes_waiting(self)->int
torch.distributed.elastic.rendezvous.api.RendezvousHandler.set_closed(self)
torch.distributed.elastic.rendezvous.api.RendezvousHandler.shutdown(self)->bool
torch.distributed.elastic.rendezvous.api.RendezvousHandlerRegistry(self)
torch.distributed.elastic.rendezvous.api.RendezvousHandlerRegistry.__init__(self)
torch.distributed.elastic.rendezvous.api.RendezvousHandlerRegistry.create_handler(self,params:RendezvousParameters)->RendezvousHandler
torch.distributed.elastic.rendezvous.api.RendezvousHandlerRegistry.register(self,backend:str,creator:RendezvousHandlerCreator)->None
torch.distributed.elastic.rendezvous.api.RendezvousParameters(self,backend:str,endpoint:str,run_id:str,min_nodes:int,max_nodes:int,**kwargs)
torch.distributed.elastic.rendezvous.api.RendezvousParameters.__init__(self,backend:str,endpoint:str,run_id:str,min_nodes:int,max_nodes:int,**kwargs)
torch.distributed.elastic.rendezvous.api.RendezvousParameters.get(self,key:str,default:Any=None)->Any
torch.distributed.elastic.rendezvous.api.RendezvousParameters.get_as_bool(self,key:str,default:Optional[bool]=None)->Optional[bool]
torch.distributed.elastic.rendezvous.api.RendezvousParameters.get_as_int(self,key:str,default:Optional[int]=None)->Optional[int]
torch.distributed.elastic.rendezvous.api.RendezvousStateError(RendezvousError)
torch.distributed.elastic.rendezvous.api.RendezvousTimeoutError(RendezvousError)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/etcd_server.py----------------------------------------
A:torch.distributed.elastic.rendezvous.etcd_server.log->logging.getLogger(__name__)
A:torch.distributed.elastic.rendezvous.etcd_server.addrs->socket.getaddrinfo(host='localhost', port=None, family=socket.AF_UNSPEC, type=socket.SOCK_STREAM)
A:torch.distributed.elastic.rendezvous.etcd_server.s->socket.socket(family, type, proto)
A:torch.distributed.elastic.rendezvous.etcd_server.root->os.path.dirname(__file__)
A:torch.distributed.elastic.rendezvous.etcd_server.default_etcd_bin->os.path.join(root, 'bin/etcd')
A:torch.distributed.elastic.rendezvous.etcd_server.self._etcd_binary_path->os.environ.get('TORCHELASTIC_ETCD_BINARY_PATH', default_etcd_bin)
A:torch.distributed.elastic.rendezvous.etcd_server.data_dir->os.path.join(self._base_data_dir, str(curr_retries))
A:torch.distributed.elastic.rendezvous.etcd_server.sock->find_free_port()
A:torch.distributed.elastic.rendezvous.etcd_server.sock_peer->find_free_port()
A:torch.distributed.elastic.rendezvous.etcd_server.etcd_cmd->shlex.split(' '.join([self._etcd_binary_path, '--enable-v2', '--data-dir', data_dir, '--listen-client-urls', f'http://{self._host}:{self._port}', '--advertise-client-urls', f'http://{self._host}:{self._port}', '--listen-peer-urls', f'http://{self._host}:{peer_port}']))
A:torch.distributed.elastic.rendezvous.etcd_server.self._etcd_proc->subprocess.Popen(etcd_cmd, close_fds=True, stderr=stderr)
A:torch.distributed.elastic.rendezvous.etcd_server.client->etcd.Client(host=f'{self._host}', port=self._port, version_prefix='/v2', read_timeout=5)
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer(self,data_dir:Optional[str]=None)
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.__init__(self,data_dir:Optional[str]=None)
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer._get_etcd_server_process(self)->subprocess.Popen
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer._start(self,data_dir:str,timeout:int=60,stderr:Union[int,TextIO,None]=None)->None
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer._wait_for_ready(self,timeout:int=60)->None
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.get_client(self)
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.get_endpoint(self)->str
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.get_host(self)->str
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.get_port(self)->int
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.start(self,timeout:int=60,num_retries:int=3,stderr:Union[int,TextIO,None]=None)->None
torch.distributed.elastic.rendezvous.etcd_server.EtcdServer.stop(self)->None
torch.distributed.elastic.rendezvous.etcd_server.find_free_port()
torch.distributed.elastic.rendezvous.etcd_server.stop_etcd(subprocess,data_dir:Optional[str]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/rendezvous/dynamic_rendezvous.py----------------------------------------
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.log->logging.getLogger(__name__)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous._ZERO->timedelta(0)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._lock->threading.Lock()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self.wait_list->set()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._state->_RendezvousState()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.state_bits->pickle.dumps(self._state)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.set_response->weak_self()._backend.set_state(state_bits, self._token)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.get_response->weak_self()._backend.get_state()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.node_list->', '.join((f"'{dead_node}'" for dead_node in self._dead_nodes))
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._last_sync_time->time.monotonic()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.has_set->weak_self()._state_holder.sync()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.ctx->_RendezvousContext(self._node, self._state, self._settings)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.action->state_handler(ctx, deadline)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._state.last_heartbeats[self._node]->datetime.datetime.utcnow()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.now->time.monotonic()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous._node_desc_generator->_NodeDescGenerator()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.node->cls._node_desc_generator.generate()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.settings->RendezvousSettings(run_id, min_nodes, max_nodes, timeout or RendezvousTimeout(), keep_alive_interval=timedelta(seconds=5), keep_alive_max_attempt=3)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.state_holder->_BackendRendezvousStateHolder(backend, settings)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._op_executor->_DistributedRendezvousOpExecutor(self._this_node, self._state_holder, self._settings)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._heartbeat_lock->threading.Lock()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.exit_op->_RendezvousExitOp()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.join_op->_RendezvousJoinOp()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.deadline->weak_self()._get_deadline(self._settings.timeout.heartbeat)
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.(rank, world_size)->weak_self()._get_world()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.store->weak_self()._get_store()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.op->_RendezvousKeepAliveOp()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self->weak_self()
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.self._keep_alive_timer->_PeriodicTimer(self._settings.keep_alive_interval, self._keep_alive_weak, weakref.ref(self))
A:torch.distributed.elastic.rendezvous.dynamic_rendezvous.timeout->RendezvousTimeout(_get_timeout(params, 'join'), _get_timeout(params, 'last_call'), _get_timeout(params, 'close'))
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler(self,node:_NodeDesc,settings:RendezvousSettings,backend_name:str,store:Store,state_holder:_RendezvousStateHolder)
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.__init__(self,node:_NodeDesc,settings:RendezvousSettings,backend_name:str,store:Store,state_holder:_RendezvousStateHolder)
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._close(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._get_deadline(self,timeout:timedelta)->float
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._get_store(self)->Store
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._get_world(self)->Tuple[int, int]
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._keep_alive(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._keep_alive_weak(weak_self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._record(self,message:str,node_state:NodeState=NodeState.RUNNING,rank:Optional[int]=None)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._start_heartbeats(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler._stop_heartbeats(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend(cls,run_id:str,store:Store,backend:RendezvousBackend,min_nodes:int,max_nodes:int,timeout:Optional[RendezvousTimeout]=None)
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.get_backend(self)->str
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.get_run_id(self)->str
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.is_closed(self)->bool
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.next_rendezvous(self)->Tuple[Store, int, int]
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.num_nodes_waiting(self)->int
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.set_closed(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.settings(self)->RendezvousSettings
torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.shutdown(self)->bool
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend(ABC)
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state(self)->Optional[Tuple[bytes, Token]]
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name(self)->str
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state(self,state:bytes,token:Optional[Token]=None)->Optional[Tuple[bytes, Token, bool]]
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousSettings
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout(self,join:Optional[timedelta]=None,last_call:Optional[timedelta]=None,close:Optional[timedelta]=None,heartbeat:Optional[timedelta]=None)
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.__init__(self,join:Optional[timedelta]=None,last_call:Optional[timedelta]=None,close:Optional[timedelta]=None,heartbeat:Optional[timedelta]=None)
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout._set_timeouts(self,**timeouts:Optional[timedelta])
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close(self)->timedelta
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat(self)->timedelta
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join(self)->timedelta
torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call(self)->timedelta
torch.distributed.elastic.rendezvous.dynamic_rendezvous._Action(Enum)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder(self,backend:RendezvousBackend,settings:RendezvousSettings,cache_duration:int=1)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder.__init__(self,backend:RendezvousBackend,settings:RendezvousSettings,cache_duration:int=1)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder._record(self,message:str,node_state:NodeState=NodeState.RUNNING)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder._sanitize(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder.mark_dirty(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder.state(self)->_RendezvousState
torch.distributed.elastic.rendezvous.dynamic_rendezvous._BackendRendezvousStateHolder.sync(self)->Optional[bool]
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor(self,node:_NodeDesc,state_holder:_RendezvousStateHolder,settings:RendezvousSettings)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor.__init__(self,node:_NodeDesc,state_holder:_RendezvousStateHolder,settings:RendezvousSettings)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._add_to_participants(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._add_to_wait_list(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._keep_alive(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._mark_rendezvous_closed(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._mark_rendezvous_complete(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._record(self,message:str,node_state:NodeState=NodeState.RUNNING)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._remove_from_participants(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor._remove_from_wait_list(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._DistributedRendezvousOpExecutor.run(self,state_handler:Callable[[_RendezvousContext,float],_Action],deadline:float)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._NodeDesc
torch.distributed.elastic.rendezvous.dynamic_rendezvous._NodeDesc.__repr__(self)->str
torch.distributed.elastic.rendezvous.dynamic_rendezvous._NodeDescGenerator(self)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._NodeDescGenerator.__init__(self)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._NodeDescGenerator.generate(self)->_NodeDesc
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousCloseOp(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousCloseOp.__call__(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousContext(self,node:_NodeDesc,state:_RendezvousState,settings:RendezvousSettings)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousContext.__init__(self,node:_NodeDesc,state:_RendezvousState,settings:RendezvousSettings)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousExitOp(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousExitOp.__call__(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousJoinOp(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousJoinOp.__call__(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousKeepAliveOp(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousKeepAliveOp.__call__(self,ctx:_RendezvousContext,deadline:float)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousOpExecutor(ABC)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousOpExecutor.run(self,state_handler:Callable[[_RendezvousContext,float],_Action],deadline:float)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousState(self)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousState.__init__(self)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousStateHolder(ABC)
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousStateHolder.mark_dirty(self)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousStateHolder.state(self)->_RendezvousState
torch.distributed.elastic.rendezvous.dynamic_rendezvous._RendezvousStateHolder.sync(self)->Optional[bool]
torch.distributed.elastic.rendezvous.dynamic_rendezvous._get_timeout(params:RendezvousParameters,key:str)->Optional[timedelta]
torch.distributed.elastic.rendezvous.dynamic_rendezvous._remove_participant_epilogue(state:_RendezvousState,settings:RendezvousSettings)->None
torch.distributed.elastic.rendezvous.dynamic_rendezvous._should_keep_alive(ctx:_RendezvousContext)->bool
torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler(store:Store,backend:RendezvousBackend,params:RendezvousParameters)->DynamicRendezvousHandler
torch.distributed.elastic.rendezvous.dynamic_rendezvous.get_method_name(depth=2)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/agent/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/agent/server/local_elastic_agent.py----------------------------------------
A:torch.distributed.elastic.agent.server.local_elastic_agent.log->get_logger()
A:torch.distributed.elastic.agent.server.local_elastic_agent.rdzv_run_id->spec.rdzv_handler.get_run_id()
A:torch.distributed.elastic.agent.server.local_elastic_agent.self._log_dir->self._make_log_dir(log_dir, rdzv_run_id)
A:torch.distributed.elastic.agent.server.local_elastic_agent.dir->tempfile.mkdtemp(prefix=f'{rdzv_run_id}_', dir=base_log_dir)
A:torch.distributed.elastic.agent.server.local_elastic_agent.(master_addr, master_port)->super()._get_master_addr_port(store)
A:torch.distributed.elastic.agent.server.local_elastic_agent.worker_args->torch.distributed.elastic.utils.macros.substitute(worker_args, str(local_rank))
A:torch.distributed.elastic.agent.server.local_elastic_agent.args[local_rank]->tuple(worker_args)
A:torch.distributed.elastic.agent.server.local_elastic_agent.attempt_log_dir->os.path.join(self._log_dir, f'attempt_{restart_count}')
A:torch.distributed.elastic.agent.server.local_elastic_agent.self._pcontext->start_processes(name=spec.role, entrypoint=spec.entrypoint, args=args, envs=envs, log_dir=attempt_log_dir, start_method=self._start_method, redirects=spec.redirects, tee=spec.tee)
A:torch.distributed.elastic.agent.server.local_elastic_agent.pc_pids->set(self._pcontext.pids().values())
A:torch.distributed.elastic.agent.server.local_elastic_agent.result->self._pcontext.wait(0)
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent(self,spec:WorkerSpec,start_method='spawn',exit_barrier_timeout:float=300,log_dir:Optional[str]=None)
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent.__init__(self,spec:WorkerSpec,start_method='spawn',exit_barrier_timeout:float=300,log_dir:Optional[str]=None)
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent._make_log_dir(self,log_dir:Optional[str],rdzv_run_id:str)
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent._monitor_workers(self,worker_group:WorkerGroup)->RunResult
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent._shutdown(self,death_sig:signal.Signals=signal.SIGTERM)->None
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent._start_workers(self,worker_group:WorkerGroup)->Dict[int, Any]
torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent._stop_workers(self,worker_group:WorkerGroup)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/agent/server/api.py----------------------------------------
A:torch.distributed.elastic.agent.server.api.log->get_logger()
A:torch.distributed.elastic.agent.server.api.dict_data->json.loads(data.decode(encoding='UTF-8'))
A:torch.distributed.elastic.agent.server.api.addrs->socket.getaddrinfo(host='localhost', port=None, family=socket.AF_UNSPEC, type=socket.SOCK_STREAM)
A:torch.distributed.elastic.agent.server.api.s->socket.socket(family, type, proto)
A:torch.distributed.elastic.agent.server.api.self._worker_group->WorkerGroup(spec)
A:torch.distributed.elastic.agent.server.api.sock->_get_socket_with_port()
A:torch.distributed.elastic.agent.server.api.master_addr->store.get('MASTER_ADDR').decode(encoding='UTF-8')
A:torch.distributed.elastic.agent.server.api.master_port->int(store.get('MASTER_PORT').decode(encoding='UTF-8'))
A:torch.distributed.elastic.agent.server.api.(store, group_rank, group_world_size)->spec.rdzv_handler.next_rendezvous()
A:torch.distributed.elastic.agent.server.api.workers->self._assign_worker_ranks(store, group_rank, group_world_size, spec)
A:torch.distributed.elastic.agent.server.api.(master_addr, master_port)->self._get_master_addr_port(store)
A:torch.distributed.elastic.agent.server.api.end_idx->len(role_infos)
A:torch.distributed.elastic.agent.server.api.role_infos->sorted(role_infos, key=functools.cmp_to_key(_RoleInstanceInfo.compare))
A:torch.distributed.elastic.agent.server.api.(worker_world_size, worker_global_ranks)->self._get_ranks(role_infos, group_rank)
A:torch.distributed.elastic.agent.server.api.(role_start_idx, role_end_idx)->_RoleInstanceInfo.find_role_boundaries(role_infos, my_role_info.role)
A:torch.distributed.elastic.agent.server.api.role_pos->next((idx for (idx, role_info) in enumerate(role_infos) if _RoleInstanceInfo.compare(role_info, my_role_info) == 0))
A:torch.distributed.elastic.agent.server.api.(role_world_size, role_ranks)->self._get_ranks(role_infos, role_pos, role_start_idx, role_end_idx + 1)
A:torch.distributed.elastic.agent.server.api.worker->Worker(local_rank=ind, global_rank=worker_global_ranks[ind], role_rank=role_ranks[ind], world_size=worker_world_size, role_world_size=role_world_size)
A:torch.distributed.elastic.agent.server.api.agent_role_info->_RoleInstanceInfo(spec.role, group_rank, spec.local_world_size)
A:torch.distributed.elastic.agent.server.api.agent_config_enc->_RoleInstanceInfo(spec.role, group_rank, spec.local_world_size).serialize()
A:torch.distributed.elastic.agent.server.api.role_infos_bytes->torch.distributed.elastic.utils.store.synchronize(store, agent_config_enc, group_rank, group_world_size, key_prefix)
A:torch.distributed.elastic.agent.server.api.worker_ids->self._start_workers(worker_group)
A:torch.distributed.elastic.agent.server.api.start_time->time.monotonic()
A:torch.distributed.elastic.agent.server.api.result->self._invoke_run(role)
A:torch.distributed.elastic.agent.server.api.self._total_execution_time->int(time.monotonic() - start_time)
A:torch.distributed.elastic.agent.server.api.failure->self._invoke_run(role).failures.get(worker.global_rank)
A:torch.distributed.elastic.agent.server.api.worker_id->str(worker.id)
A:torch.distributed.elastic.agent.server.api.md_str->json.dumps(md)
A:torch.distributed.elastic.agent.server.api.is_failed->group_results.is_failed()
A:torch.distributed.elastic.agent.server.api.run_result->self._monitor_workers(self._worker_group)
A:torch.distributed.elastic.agent.server.api.num_nodes_waiting->rdzv_handler.num_nodes_waiting()
A:torch.distributed.elastic.agent.server.api.start->time.time()
torch.distributed.elastic.agent.server.ElasticAgent(abc.ABC)
torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group(self,role:str=DEFAULT_ROLE)->WorkerGroup
torch.distributed.elastic.agent.server.ElasticAgent.run(self,role:str=DEFAULT_ROLE)->RunResult
torch.distributed.elastic.agent.server.RunResult
torch.distributed.elastic.agent.server.RunResult.is_failed(self)->bool
torch.distributed.elastic.agent.server.SimpleElasticAgent(self,spec:WorkerSpec,exit_barrier_timeout:float=300)
torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks(self,store,group_rank:int,group_world_size:int,spec:WorkerSpec)->List[Worker]
torch.distributed.elastic.agent.server.SimpleElasticAgent._construct_event(self,state:str,source:EventSource,worker:Optional[Worker]=None,raw_error:Optional[str]=None)->Event
torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier(self)
torch.distributed.elastic.agent.server.SimpleElasticAgent._get_master_addr_port(store:Store)->Tuple[str, int]
torch.distributed.elastic.agent.server.SimpleElasticAgent._get_ranks(self,role_infos:List[_RoleInstanceInfo],role_idx:int,start_idx:int=0,end_idx:int=-1)->Tuple[int, List[int]]
torch.distributed.elastic.agent.server.SimpleElasticAgent._get_worker_state(self,worker:Worker,result:RunResult)->str
torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.SimpleElasticAgent._invoke_run(self,role:str=DEFAULT_ROLE)->RunResult
torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers(self,worker_group:WorkerGroup)->RunResult
torch.distributed.elastic.agent.server.SimpleElasticAgent._record_flakiness_metric(self,is_failed:bool=False)
torch.distributed.elastic.agent.server.SimpleElasticAgent._record_metric_with_condition(self,metric_name,condition)
torch.distributed.elastic.agent.server.SimpleElasticAgent._record_metrics(self,group_results:RunResult)
torch.distributed.elastic.agent.server.SimpleElasticAgent._record_worker_events(self,result:RunResult)->None
torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.SimpleElasticAgent._set_master_addr_port(store:Store,master_addr:Optional[str],master_port:Optional[int])
torch.distributed.elastic.agent.server.SimpleElasticAgent._share_and_gather(self,store,group_rank:int,group_world_size:int,spec:WorkerSpec)->List
torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown(self,death_sig:signal.Signals=signal.SIGTERM)->None
torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers(self,worker_group:WorkerGroup)->Dict[int, Any]
torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.SimpleElasticAgent.get_event_failed(self)->Event
torch.distributed.elastic.agent.server.SimpleElasticAgent.get_event_succeeded(self)->Event
torch.distributed.elastic.agent.server.SimpleElasticAgent.get_worker_group(self,role:str=DEFAULT_ROLE)->WorkerGroup
torch.distributed.elastic.agent.server.SimpleElasticAgent.run(self,role:str=DEFAULT_ROLE)->RunResult
torch.distributed.elastic.agent.server.Worker(self,local_rank:int,global_rank:int=-1,role_rank:int=-1,world_size:int=-1,role_world_size:int=-1)
torch.distributed.elastic.agent.server.Worker.__repr__(self)
torch.distributed.elastic.agent.server.Worker.__str__(self)
torch.distributed.elastic.agent.server.WorkerGroup(self,spec:WorkerSpec)
torch.distributed.elastic.agent.server.WorkerSpec
torch.distributed.elastic.agent.server.WorkerSpec.__post_init__(self)
torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name(self)
torch.distributed.elastic.agent.server.WorkerState(str,Enum)
torch.distributed.elastic.agent.server.WorkerState.is_running(state:'WorkerState')->bool
torch.distributed.elastic.agent.server.api.ElasticAgent(abc.ABC)
torch.distributed.elastic.agent.server.api.ElasticAgent.get_worker_group(self,role:str=DEFAULT_ROLE)->WorkerGroup
torch.distributed.elastic.agent.server.api.ElasticAgent.run(self,role:str=DEFAULT_ROLE)->RunResult
torch.distributed.elastic.agent.server.api.RunResult
torch.distributed.elastic.agent.server.api.RunResult.is_failed(self)->bool
torch.distributed.elastic.agent.server.api.SimpleElasticAgent(self,spec:WorkerSpec,exit_barrier_timeout:float=300)
torch.distributed.elastic.agent.server.api.SimpleElasticAgent.__init__(self,spec:WorkerSpec,exit_barrier_timeout:float=300)
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._assign_worker_ranks(self,store,group_rank:int,group_world_size:int,spec:WorkerSpec)->List[Worker]
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._construct_event(self,state:str,source:EventSource,worker:Optional[Worker]=None,raw_error:Optional[str]=None)->Event
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._exit_barrier(self)
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._get_master_addr_port(store:Store)->Tuple[str, int]
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._get_ranks(self,role_infos:List[_RoleInstanceInfo],role_idx:int,start_idx:int=0,end_idx:int=-1)->Tuple[int, List[int]]
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._get_worker_state(self,worker:Worker,result:RunResult)->str
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._initialize_workers(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._invoke_run(self,role:str=DEFAULT_ROLE)->RunResult
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._monitor_workers(self,worker_group:WorkerGroup)->RunResult
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._record_flakiness_metric(self,is_failed:bool=False)
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._record_metric_with_condition(self,metric_name,condition)
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._record_metrics(self,group_results:RunResult)
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._record_worker_events(self,result:RunResult)->None
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._rendezvous(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._restart_workers(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._set_master_addr_port(store:Store,master_addr:Optional[str],master_port:Optional[int])
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._share_and_gather(self,store,group_rank:int,group_world_size:int,spec:WorkerSpec)->List
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._shutdown(self,death_sig:signal.Signals=signal.SIGTERM)->None
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._start_workers(self,worker_group:WorkerGroup)->Dict[int, Any]
torch.distributed.elastic.agent.server.api.SimpleElasticAgent._stop_workers(self,worker_group:WorkerGroup)->None
torch.distributed.elastic.agent.server.api.SimpleElasticAgent.get_event_failed(self)->Event
torch.distributed.elastic.agent.server.api.SimpleElasticAgent.get_event_succeeded(self)->Event
torch.distributed.elastic.agent.server.api.SimpleElasticAgent.get_worker_group(self,role:str=DEFAULT_ROLE)->WorkerGroup
torch.distributed.elastic.agent.server.api.SimpleElasticAgent.run(self,role:str=DEFAULT_ROLE)->RunResult
torch.distributed.elastic.agent.server.api.Worker(self,local_rank:int,global_rank:int=-1,role_rank:int=-1,world_size:int=-1,role_world_size:int=-1)
torch.distributed.elastic.agent.server.api.Worker.__init__(self,local_rank:int,global_rank:int=-1,role_rank:int=-1,world_size:int=-1,role_world_size:int=-1)
torch.distributed.elastic.agent.server.api.Worker.__repr__(self)
torch.distributed.elastic.agent.server.api.Worker.__str__(self)
torch.distributed.elastic.agent.server.api.WorkerGroup(self,spec:WorkerSpec)
torch.distributed.elastic.agent.server.api.WorkerGroup.__init__(self,spec:WorkerSpec)
torch.distributed.elastic.agent.server.api.WorkerSpec
torch.distributed.elastic.agent.server.api.WorkerSpec.__post_init__(self)
torch.distributed.elastic.agent.server.api.WorkerSpec.get_entrypoint_name(self)
torch.distributed.elastic.agent.server.api.WorkerState(str,Enum)
torch.distributed.elastic.agent.server.api.WorkerState.is_running(state:'WorkerState')->bool
torch.distributed.elastic.agent.server.api._RoleInstanceInfo(self,role:str,rank:int,local_world_size:int)
torch.distributed.elastic.agent.server.api._RoleInstanceInfo.__init__(self,role:str,rank:int,local_world_size:int)
torch.distributed.elastic.agent.server.api._RoleInstanceInfo.compare(obj1,obj2)->int
torch.distributed.elastic.agent.server.api._RoleInstanceInfo.deserialize(data:bytes)
torch.distributed.elastic.agent.server.api._RoleInstanceInfo.find_role_boundaries(roles_infos:List,role:str)->Tuple[int, int]
torch.distributed.elastic.agent.server.api._RoleInstanceInfo.serialize(self)->bytes
torch.distributed.elastic.agent.server.api._get_fq_hostname()->str
torch.distributed.elastic.agent.server.api._get_socket_with_port()->socket.socket


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/agent/server/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/timer/local_timer.py----------------------------------------
A:torch.distributed.elastic.timer.local_timer.pid->os.getpid()
A:torch.distributed.elastic.timer.local_timer.acquire_request->TimerRequest(pid, scope_id, expiration_time)
A:torch.distributed.elastic.timer.local_timer.release_request->TimerRequest(pid, scope_id, -1)
A:torch.distributed.elastic.timer.local_timer.start->time.time()
A:torch.distributed.elastic.timer.local_timer.r->self._mp_queue.get(block=True, timeout=wait)
A:torch.distributed.elastic.timer.local_timer.expired_scopes->expired_timers.setdefault(request.worker_id, [])
torch.distributed.elastic.timer.LocalTimerClient(self,mp_queue)
torch.distributed.elastic.timer.LocalTimerClient.acquire(self,scope_id,expiration_time)
torch.distributed.elastic.timer.LocalTimerClient.release(self,scope_id)
torch.distributed.elastic.timer.LocalTimerServer(self,mp_queue:mp.Queue,max_interval:float=60,daemon:bool=True)
torch.distributed.elastic.timer.LocalTimerServer._reap_worker(self,worker_id:int)->bool
torch.distributed.elastic.timer.LocalTimerServer.clear_timers(self,worker_ids:Set[int])->None
torch.distributed.elastic.timer.LocalTimerServer.get_expired_timers(self,deadline:float)->Dict[Any, List[TimerRequest]]
torch.distributed.elastic.timer.LocalTimerServer.register_timers(self,timer_requests:List[TimerRequest])->None
torch.distributed.elastic.timer.local_timer.LocalTimerClient(self,mp_queue)
torch.distributed.elastic.timer.local_timer.LocalTimerClient.__init__(self,mp_queue)
torch.distributed.elastic.timer.local_timer.LocalTimerClient.acquire(self,scope_id,expiration_time)
torch.distributed.elastic.timer.local_timer.LocalTimerClient.release(self,scope_id)
torch.distributed.elastic.timer.local_timer.LocalTimerServer(self,mp_queue:mp.Queue,max_interval:float=60,daemon:bool=True)
torch.distributed.elastic.timer.local_timer.LocalTimerServer.__init__(self,mp_queue:mp.Queue,max_interval:float=60,daemon:bool=True)
torch.distributed.elastic.timer.local_timer.LocalTimerServer._reap_worker(self,worker_id:int)->bool
torch.distributed.elastic.timer.local_timer.LocalTimerServer.clear_timers(self,worker_ids:Set[int])->None
torch.distributed.elastic.timer.local_timer.LocalTimerServer.get_expired_timers(self,deadline:float)->Dict[Any, List[TimerRequest]]
torch.distributed.elastic.timer.local_timer.LocalTimerServer.register_timers(self,timer_requests:List[TimerRequest])->None
torch.distributed.elastic.timer.local_timer.MultiprocessingRequestQueue(self,mp_queue:mp.Queue)
torch.distributed.elastic.timer.local_timer.MultiprocessingRequestQueue.__init__(self,mp_queue:mp.Queue)
torch.distributed.elastic.timer.local_timer.MultiprocessingRequestQueue.get(self,size,timeout:float)->List[TimerRequest]
torch.distributed.elastic.timer.local_timer.MultiprocessingRequestQueue.size(self)->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/timer/api.py----------------------------------------
A:torch.distributed.elastic.timer.api.batch_size->max(1, self._request_queue.size())
A:torch.distributed.elastic.timer.api.timer_requests->self._request_queue.get(batch_size, self._max_interval)
A:torch.distributed.elastic.timer.api.now->time.time()
A:torch.distributed.elastic.timer.api.reaped_worker_ids->set()
A:torch.distributed.elastic.timer.api.self._watchdog_thread->threading.Thread(target=self._watchdog_loop, daemon=self._daemon)
A:torch.distributed.elastic.timer.api.caller->getframeinfo(stack()[1][0])
torch.distributed.elastic.timer.TimerClient(abc.ABC)
torch.distributed.elastic.timer.TimerClient.acquire(self,scope_id:str,expiration_time:float)->None
torch.distributed.elastic.timer.TimerClient.release(self,scope_id:str)
torch.distributed.elastic.timer.TimerRequest(self,worker_id:Any,scope_id:str,expiration_time:float)
torch.distributed.elastic.timer.TimerRequest.__eq__(self,other)
torch.distributed.elastic.timer.TimerServer(self,request_queue:RequestQueue,max_interval:float,daemon:bool=True)
torch.distributed.elastic.timer.TimerServer._get_scopes(self,timer_requests)
torch.distributed.elastic.timer.TimerServer._reap_worker(self,worker_id:Any)->bool
torch.distributed.elastic.timer.TimerServer._reap_worker_no_throw(self,worker_id:Any)->bool
torch.distributed.elastic.timer.TimerServer._run_watchdog(self)
torch.distributed.elastic.timer.TimerServer._watchdog_loop(self)
torch.distributed.elastic.timer.TimerServer.clear_timers(self,worker_ids:Set[Any])->None
torch.distributed.elastic.timer.TimerServer.get_expired_timers(self,deadline:float)->Dict[str, List[TimerRequest]]
torch.distributed.elastic.timer.TimerServer.register_timers(self,timer_requests:List[TimerRequest])->None
torch.distributed.elastic.timer.TimerServer.start(self)->None
torch.distributed.elastic.timer.TimerServer.stop(self)->None
torch.distributed.elastic.timer.api.RequestQueue(abc.ABC)
torch.distributed.elastic.timer.api.RequestQueue.get(self,size:int,timeout:float)->List[TimerRequest]
torch.distributed.elastic.timer.api.RequestQueue.size(self)->int
torch.distributed.elastic.timer.api.TimerClient(abc.ABC)
torch.distributed.elastic.timer.api.TimerClient.acquire(self,scope_id:str,expiration_time:float)->None
torch.distributed.elastic.timer.api.TimerClient.release(self,scope_id:str)
torch.distributed.elastic.timer.api.TimerRequest(self,worker_id:Any,scope_id:str,expiration_time:float)
torch.distributed.elastic.timer.api.TimerRequest.__eq__(self,other)
torch.distributed.elastic.timer.api.TimerRequest.__init__(self,worker_id:Any,scope_id:str,expiration_time:float)
torch.distributed.elastic.timer.api.TimerServer(self,request_queue:RequestQueue,max_interval:float,daemon:bool=True)
torch.distributed.elastic.timer.api.TimerServer.__init__(self,request_queue:RequestQueue,max_interval:float,daemon:bool=True)
torch.distributed.elastic.timer.api.TimerServer._get_scopes(self,timer_requests)
torch.distributed.elastic.timer.api.TimerServer._reap_worker(self,worker_id:Any)->bool
torch.distributed.elastic.timer.api.TimerServer._reap_worker_no_throw(self,worker_id:Any)->bool
torch.distributed.elastic.timer.api.TimerServer._run_watchdog(self)
torch.distributed.elastic.timer.api.TimerServer._watchdog_loop(self)
torch.distributed.elastic.timer.api.TimerServer.clear_timers(self,worker_ids:Set[Any])->None
torch.distributed.elastic.timer.api.TimerServer.get_expired_timers(self,deadline:float)->Dict[str, List[TimerRequest]]
torch.distributed.elastic.timer.api.TimerServer.register_timers(self,timer_requests:List[TimerRequest])->None
torch.distributed.elastic.timer.api.TimerServer.start(self)->None
torch.distributed.elastic.timer.api.TimerServer.stop(self)->None
torch.distributed.elastic.timer.api.configure(timer_client:TimerClient)
torch.distributed.elastic.timer.api.expires(after:float,scope:Optional[str]=None,client:Optional[TimerClient]=None)
torch.distributed.elastic.timer.configure(timer_client:TimerClient)
torch.distributed.elastic.timer.expires(after:float,scope:Optional[str]=None,client:Optional[TimerClient]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/elastic/timer/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/server_process_global_profiler.py----------------------------------------
A:torch.distributed.rpc.server_process_global_profiler.profiler_config->torch.autograd.ProfilerConfig(profiler_kind, self.record_shapes, self.profile_memory, False, False, False)
A:torch.distributed.rpc.server_process_global_profiler.process_global_events->_disable_server_process_global_profiler()
A:torch.distributed.rpc.server_process_global_profiler.thread_local_function_events->torch.autograd.profiler_legacy._parse_legacy_records(thread_local_events)
A:torch.distributed.rpc.server_process_global_profiler.flattened_function_events->list(itertools.chain(*process_global_function_events))
A:torch.distributed.rpc.server_process_global_profiler.self.function_events->torch.autograd.profiler_util.EventList(flattened_function_events, use_cuda=self.use_cuda, profile_memory=self.profile_memory)
torch.distributed.rpc._server_process_global_profile(self,*args,**kwargs)
torch.distributed.rpc._server_process_global_profile.__enter__(self)
torch.distributed.rpc._server_process_global_profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.distributed.rpc.server_process_global_profiler._server_process_global_profile(self,*args,**kwargs)
torch.distributed.rpc.server_process_global_profiler._server_process_global_profile.__enter__(self)
torch.distributed.rpc.server_process_global_profiler._server_process_global_profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.distributed.rpc.server_process_global_profiler._server_process_global_profile.__init__(self,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/rref_proxy.py----------------------------------------
A:torch.distributed.rpc.rref_proxy.rref_type->rref._get_type(timeout=timeout, blocking=False).value()
A:torch.distributed.rpc.rref_proxy.func->getattr(rref_type, func_name)
A:torch.distributed.rpc.rref_proxy.rref_fut->rref._get_type(timeout=timeout, blocking=False)
torch.distributed.rpc.rref_proxy.RRefProxy(self,rref,rpc_api,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.rref_proxy.RRefProxy.__getattr__(self,func_name)
torch.distributed.rpc.rref_proxy.RRefProxy.__init__(self,rref,rpc_api,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.rref_proxy._invoke_rpc(rref,rpc_api,func_name,timeout,*args,**kwargs)
torch.distributed.rpc.rref_proxy._local_invoke(rref,func_name,args,kwargs)
torch.distributed.rpc.rref_proxy._local_invoke_async_execution(rref,func_name,args,kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/functions.py----------------------------------------
torch.distributed.rpc.functions.async_execution(fn)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/options.py----------------------------------------
A:torch.distributed.rpc.options.device->torch.device(device)
A:torch.distributed.rpc.options.full_device_map->_to_device_map(device_map)
A:torch.distributed.rpc.options.self.devices->_to_device_list(devices)
torch.distributed.rpc.TensorPipeRpcBackendOptions(self,*,num_worker_threads:int=rpc_contants.DEFAULT_NUM_WORKER_THREADS,rpc_timeout:float=rpc_contants.DEFAULT_RPC_TIMEOUT_SEC,init_method:str=rpc_contants.DEFAULT_INIT_METHOD,device_maps:Optional[Dict[str,Dict[DeviceType,DeviceType]]]=None,devices:Optional[List[DeviceType]]=None,_transports:Optional[List]=None,_channels:Optional[List]=None)
torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map(self,to:str,device_map:Dict[DeviceType,DeviceType])
torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices(self,devices:List[DeviceType])
torch.distributed.rpc.options.TensorPipeRpcBackendOptions(self,*,num_worker_threads:int=rpc_contants.DEFAULT_NUM_WORKER_THREADS,rpc_timeout:float=rpc_contants.DEFAULT_RPC_TIMEOUT_SEC,init_method:str=rpc_contants.DEFAULT_INIT_METHOD,device_maps:Optional[Dict[str,Dict[DeviceType,DeviceType]]]=None,devices:Optional[List[DeviceType]]=None,_transports:Optional[List]=None,_channels:Optional[List]=None)
torch.distributed.rpc.options.TensorPipeRpcBackendOptions.__init__(self,*,num_worker_threads:int=rpc_contants.DEFAULT_NUM_WORKER_THREADS,rpc_timeout:float=rpc_contants.DEFAULT_RPC_TIMEOUT_SEC,init_method:str=rpc_contants.DEFAULT_INIT_METHOD,device_maps:Optional[Dict[str,Dict[DeviceType,DeviceType]]]=None,devices:Optional[List[DeviceType]]=None,_transports:Optional[List]=None,_channels:Optional[List]=None)
torch.distributed.rpc.options.TensorPipeRpcBackendOptions.set_device_map(self,to:str,device_map:Dict[DeviceType,DeviceType])
torch.distributed.rpc.options.TensorPipeRpcBackendOptions.set_devices(self,devices:List[DeviceType])
torch.distributed.rpc.options._to_device(device:DeviceType)->torch.device
torch.distributed.rpc.options._to_device_list(devices:List[DeviceType])->List[torch.device]
torch.distributed.rpc.options._to_device_map(device_map:Dict[DeviceType,DeviceType])->Dict[torch.device, torch.device]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/internal.py----------------------------------------
A:torch.distributed.rpc.internal._thread_local_tensor_tables->threading.local()
A:torch.distributed.rpc.internal.self._dispatch_table->copyreg.dispatch_table.copy()
A:torch.distributed.rpc.internal.rref_fork_data->py_rref._serialize()
A:torch.distributed.rpc.internal.f->io.BytesIO()
A:torch.distributed.rpc.internal.m->torch.jit.load(f)
A:torch.distributed.rpc.internal.p->_pickler(f)
A:torch.distributed.rpc.internal.unpickler->_unpickler(io.BytesIO(binary_data))
A:torch.distributed.rpc.internal.ret->AttributeError(except_str)
A:torch.distributed.rpc.internal._internal_rpc_pickler->_InternalRPCPickler()
A:torch.distributed.rpc.internal.result->RemoteException(except_str, type(e))
A:torch.distributed.rpc.internal.profile_key->'rpc_{}#{}({} -> {})'.format(exec_type.value, str(func_name), current_worker_name, dest_worker_name)
A:torch.distributed.rpc.internal.rf->torch.autograd._RecordFunction()
A:torch.distributed.rpc.internal.PythonUDF->collections.namedtuple('PythonUDF', ['func', 'args', 'kwargs'])
A:torch.distributed.rpc.internal.RemoteException->collections.namedtuple('RemoteException', ['msg', 'exception_type'])
torch.distributed.rpc.internal.RPCExecMode(Enum)
torch.distributed.rpc.internal._InternalRPCPickler(self)
torch.distributed.rpc.internal._InternalRPCPickler.__init__(self)
torch.distributed.rpc.internal._InternalRPCPickler._py_rref_receiver(cls,rref_fork_data)
torch.distributed.rpc.internal._InternalRPCPickler._py_rref_reducer(self,py_rref)
torch.distributed.rpc.internal._InternalRPCPickler._register_reducer(self,obj_class,reducer)
torch.distributed.rpc.internal._InternalRPCPickler._rref_reducer(self,rref)
torch.distributed.rpc.internal._InternalRPCPickler._script_module_receiver(cls,script_module_serialized)
torch.distributed.rpc.internal._InternalRPCPickler._script_module_reducer(self,script_module)
torch.distributed.rpc.internal._InternalRPCPickler._tensor_receiver(cls,tensor_index)
torch.distributed.rpc.internal._InternalRPCPickler._tensor_reducer(self,tensor)
torch.distributed.rpc.internal._InternalRPCPickler.deserialize(self,binary_data,tensor_table)
torch.distributed.rpc.internal._InternalRPCPickler.serialize(self,obj)
torch.distributed.rpc.internal._build_rpc_profiling_key(exec_type,func_name,current_worker_name,dst_worker_name)
torch.distributed.rpc.internal._handle_exception(result)
torch.distributed.rpc.internal._run_function(python_udf)
torch.distributed.rpc.internal._start_record_function(exec_type,func_name,current_worker_name,dest_worker_name)
torch.distributed.rpc.internal.deserialize(binary_data,tensor_table)
torch.distributed.rpc.internal.serialize(obj)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/api.py----------------------------------------
A:torch.distributed.rpc.api.logger->logging.getLogger(__name__)
A:torch.distributed.rpc.api.self.proceed_signal->threading.Event()
A:torch.distributed.rpc.api._all_gather_dict_lock->threading.RLock()
A:torch.distributed.rpc.api.worker_infos->agent.get_worker_infos()
A:torch.distributed.rpc.api._thread_local_var->threading.local()
A:torch.distributed.rpc.api.concat_names->''.join(sorted(worker_names))
A:torch.distributed.rpc.api.sequence_num->_all_gather_sequence_id.get(concat_names, 0)
A:torch.distributed.rpc.api.timeout->get_rpc_timeout()
A:torch.distributed.rpc.api.worker_name_to_response_future_dict->dict()
A:torch.distributed.rpc.api.fut->_invoke_rpc(to, func, RPCExecMode.ASYNC, args, kwargs, timeout)
A:torch.distributed.rpc.api.states->_all_gather_sequence_id_to_states.pop(sequence_id)
A:torch.distributed.rpc.api.rref_type->type(rref.local_value())
A:torch.distributed.rpc.api.future->Future[type]()
A:torch.distributed.rpc.api.T->TypeVar('T')
A:torch.distributed.rpc.api.docstring->docstring.replace('torch.distributed.rpc.PyRRef', 'torch.distributed.rpc.RRef').replace('torch.distributed.rpc.PyRRef', 'torch.distributed.rpc.RRef')
A:torch.distributed.rpc.api.new_method->method_factory(method_name, docstring)
A:torch.distributed.rpc.api.qualified_name->torch.jit._builtins._find_builtin(func)
A:torch.distributed.rpc.api.dst_worker_info->_to_worker_info(to)
A:torch.distributed.rpc.api.should_profile->torch.autograd._profiler_enabled()
A:torch.distributed.rpc.api.ctx_manager->torch.autograd.profiler.record_function(rpc_profiling_key)
A:torch.distributed.rpc.api.is_async_exec->hasattr(func, '_wrapped_async_rpc_function')
A:torch.distributed.rpc.api.rref->_invoke_remote_python_udf(dst_worker_info, pickled_python_udf, tensors, timeout, is_async_exec)
A:torch.distributed.rpc.api.(pickled_python_udf, tensors)->_default_pickler.serialize(PythonUDF(func, args, kwargs))
A:torch.distributed.rpc.api.rpc_profiling_key->_build_rpc_profiling_key(rpc_type, func_name, get_worker_info().name, dst_worker_info.name)
torch.distributed.rpc.AllGatherStates(self)
torch.distributed.rpc._all_gather(obj,worker_names=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc._barrier(worker_names)
torch.distributed.rpc._broadcast_to_followers(sequence_id,objects_map)
torch.distributed.rpc._enable_rpc_profiler(should_profile,qualified_name,func,rpc_type,dst_worker_info)
torch.distributed.rpc._finalize_shutdown()
torch.distributed.rpc._gather_to_leader(sequence_id,worker_name,obj,worker_names=None)
torch.distributed.rpc._init_rpc_states(agent)
torch.distributed.rpc._invoke_rpc(to,func,rpc_type,args=None,kwargs=None,rpc_timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc._require_initialized(func)
torch.distributed.rpc._rref_typeof_on_owner(rref,blocking=True)
torch.distributed.rpc._rref_typeof_on_user(rref,timeout=UNSET_RPC_TIMEOUT,blocking=True)
torch.distributed.rpc._to_worker_info(to)
torch.distributed.rpc._use_rpc_pickler(rpc_pickler)
torch.distributed.rpc._wait_all()
torch.distributed.rpc._wait_all_workers(timeout=DEFAULT_SHUTDOWN_TIMEOUT)
torch.distributed.rpc.api.AllGatherStates(self)
torch.distributed.rpc.api.AllGatherStates.__init__(self)
torch.distributed.rpc.api._all_gather(obj,worker_names=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.api._barrier(worker_names)
torch.distributed.rpc.api._broadcast_to_followers(sequence_id,objects_map)
torch.distributed.rpc.api._enable_rpc_profiler(should_profile,qualified_name,func,rpc_type,dst_worker_info)
torch.distributed.rpc.api._finalize_shutdown()
torch.distributed.rpc.api._gather_to_leader(sequence_id,worker_name,obj,worker_names=None)
torch.distributed.rpc.api._init_rpc_states(agent)
torch.distributed.rpc.api._invoke_rpc(to,func,rpc_type,args=None,kwargs=None,rpc_timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.api._require_initialized(func)
torch.distributed.rpc.api._rref_typeof_on_owner(rref,blocking=True)
torch.distributed.rpc.api._rref_typeof_on_user(rref,timeout=UNSET_RPC_TIMEOUT,blocking=True)
torch.distributed.rpc.api._to_worker_info(to)
torch.distributed.rpc.api._use_rpc_pickler(rpc_pickler)
torch.distributed.rpc.api._wait_all()
torch.distributed.rpc.api._wait_all_workers(timeout=DEFAULT_SHUTDOWN_TIMEOUT)
torch.distributed.rpc.api.get_worker_info(worker_name=None)
torch.distributed.rpc.api.method_factory(method_name,docstring)
torch.distributed.rpc.api.remote(to,func,args=None,kwargs=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.api.rpc_async(to,func,args=None,kwargs=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.api.rpc_sync(to,func,args=None,kwargs=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.api.shutdown(graceful=True,timeout=DEFAULT_SHUTDOWN_TIMEOUT)
torch.distributed.rpc.get_worker_info(worker_name=None)
torch.distributed.rpc.method_factory(method_name,docstring)
torch.distributed.rpc.remote(to,func,args=None,kwargs=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.rpc_async(to,func,args=None,kwargs=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.rpc_sync(to,func,args=None,kwargs=None,timeout=UNSET_RPC_TIMEOUT)
torch.distributed.rpc.shutdown(graceful=True,timeout=DEFAULT_SHUTDOWN_TIMEOUT)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/backend_registry.py----------------------------------------
A:torch.distributed.rpc.backend_registry.BackendValue->collections.namedtuple('BackendValue', ['construct_rpc_backend_options_handler', 'init_backend_handler'])
A:torch.distributed.rpc.backend_registry.BackendType->enum.Enum(value='BackendType', names=extended_enum_dict)
A:torch.distributed.rpc.backend_registry.extended_enum_dict->dict({backend_name: BackendValue(construct_rpc_backend_options_handler=construct_rpc_backend_options_handler, init_backend_handler=init_backend_handler)}, **existing_enum_dict)
A:torch.distributed.rpc.backend_registry.group->_init_process_group(store, rank, world_size)
A:torch.distributed.rpc.backend_registry.my_devices->sorted(my_devices, key=lambda d: d.index)
A:torch.distributed.rpc.backend_registry.device_count->torch.cuda.device_count()
A:torch.distributed.rpc.backend_registry.(reverse_device_maps, devices)->_tensorpipe_exchange_and_check_all_device_maps(name, device_count, rpc_backend_options.device_maps, rpc_backend_options.devices, group)
A:torch.distributed.rpc.backend_registry.agent->TensorPipeAgent(store, name, rank, world_size, rpc_backend_options, reverse_device_maps, devices)
torch.distributed.rpc.backend_registry._backend_type_repr(self)
torch.distributed.rpc.backend_registry._init_process_group(store,rank,world_size)
torch.distributed.rpc.backend_registry._tensorpipe_construct_rpc_backend_options_handler(rpc_timeout,init_method,num_worker_threads=rpc_constants.DEFAULT_NUM_WORKER_THREADS,_transports=None,_channels=None,**kwargs)
torch.distributed.rpc.backend_registry._tensorpipe_exchange_and_check_all_device_maps(my_name,my_device_count,my_device_maps,my_devices,group)
torch.distributed.rpc.backend_registry._tensorpipe_init_backend_handler(store,name,rank,world_size,rpc_backend_options)
torch.distributed.rpc.backend_registry._tensorpipe_validate_devices(devices,device_count)
torch.distributed.rpc.backend_registry.backend_registered(backend_name)
torch.distributed.rpc.backend_registry.construct_rpc_backend_options(backend,rpc_timeout=rpc_constants.DEFAULT_RPC_TIMEOUT_SEC,init_method=rpc_constants.DEFAULT_INIT_METHOD,**kwargs)
torch.distributed.rpc.backend_registry.init_backend(backend,*args,**kwargs)
torch.distributed.rpc.backend_registry.register_backend(backend_name,construct_rpc_backend_options_handler,init_backend_handler)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/__init__.py----------------------------------------
A:torch.distributed.rpc.__init__.logger->logging.getLogger(__name__)
A:torch.distributed.rpc.__init__._init_counter_lock->threading.Lock()
A:torch.distributed.rpc.__init__.rpc_backend_options->backend_registry.construct_rpc_backend_options(backend)
A:torch.distributed.rpc.__init__.rendezvous_iterator->torch.distributed.rendezvous(rpc_backend_options.init_method, rank=rank, world_size=world_size)
A:torch.distributed.rpc.__init__.(store, _, _)->next(rendezvous_iterator)
A:torch.distributed.rpc.__init__.store->torch.distributed.PrefixStore(str('rpc_prefix_{}'.format(_init_counter)), store)
A:torch.distributed.rpc.__init__.rpc_agent->backend_registry.init_backend(backend, store=store, name=name, rank=rank, world_size=world_size, rpc_backend_options=rpc_backend_options)
A:torch.distributed.rpc.__init__.info->_rref_context_get_debug_info()
torch.distributed.rpc.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/constants.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/_testing/faulty_agent_backend_registry.py----------------------------------------
A:torch.distributed.rpc._testing.faulty_agent_backend_registry.agent->FaultyTensorPipeAgent(store, name, rank, world_size, rpc_backend_options, {}, [])
torch.distributed.rpc._testing.faulty_agent_backend_registry._faulty_tensorpipe_construct_rpc_backend_options_handler(rpc_timeout,init_method,num_worker_threads,messages_to_fail,messages_to_delay,num_fail_sends,**kwargs)
torch.distributed.rpc._testing.faulty_agent_backend_registry._faulty_tensorpipe_init_backend_handler(store,name,rank,world_size,rpc_backend_options)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/rpc/_testing/__init__.py----------------------------------------
torch.distributed.rpc._testing.__init__.is_available()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/functional.py----------------------------------------
A:torch.distributed.nn.functional.ctx.rank->torch.distributed.get_rank()
A:torch.distributed.nn.functional.tensor->tensor.clone().clone()
A:torch.distributed.nn.functional.gx->torch.sum(torch.stack(gxs), dim=0)
A:torch.distributed.nn.functional.output->torch.zeros_like(tensors[0])
A:torch.distributed.nn.functional.gxs->_AlltoAll.apply(ctx.group, tensor_list, *grad_outputs)
A:torch.distributed.nn.functional.my_rank->torch.distributed.get_rank(group=group)
A:torch.distributed.nn.functional.to_send->list(tensors)
A:torch.distributed.nn.functional.grad_outputs->tuple((tensor.contiguous() for tensor in grad_outputs))
A:torch.distributed.nn.functional.ctx.input_size->input.size()
torch.distributed.nn._AllGather(Function)
torch.distributed.nn._AllGather.backward(ctx,*grad_outputs)
torch.distributed.nn._AllGather.forward(ctx,group,tensor)
torch.distributed.nn._AllReduce(Function)
torch.distributed.nn._AllReduce.backward(ctx,grad_output)
torch.distributed.nn._AllReduce.forward(ctx,op,group,tensor)
torch.distributed.nn._AlltoAll(Function)
torch.distributed.nn._AlltoAll.backward(ctx,*grad_outputs)
torch.distributed.nn._AlltoAll.forward(ctx,group,out_tensor_list,*tensors)
torch.distributed.nn._AlltoAllSingle(Function)
torch.distributed.nn._AlltoAllSingle.backward(ctx,grad_output)
torch.distributed.nn._AlltoAllSingle.forward(ctx,group,output,output_split_sizes,input_split_sizes,input)
torch.distributed.nn._Broadcast(Function)
torch.distributed.nn._Broadcast.backward(ctx,grad_output)
torch.distributed.nn._Broadcast.forward(ctx,src,group,tensor)
torch.distributed.nn._Gather(Function)
torch.distributed.nn._Gather.backward(ctx,*grad_outputs)
torch.distributed.nn._Gather.forward(ctx,dst,group,tensor)
torch.distributed.nn._Reduce(Function)
torch.distributed.nn._Reduce.backward(ctx,grad_output)
torch.distributed.nn._Reduce.forward(ctx,src,op,group,tensor)
torch.distributed.nn._Reduce_Scatter(Function)
torch.distributed.nn._Reduce_Scatter.backward(ctx,grad_output)
torch.distributed.nn._Reduce_Scatter.forward(ctx,op,group,tensor,*input_tensor_list)
torch.distributed.nn._Scatter(Function)
torch.distributed.nn._Scatter.backward(ctx,grad_output)
torch.distributed.nn._Scatter.forward(ctx,src,group,*tensors)
torch.distributed.nn.all_gather(tensor,group=dist.group.WORLD)
torch.distributed.nn.all_reduce(tensor,op=dist.ReduceOp.SUM,group=dist.group.WORLD)
torch.distributed.nn.all_to_all(output_tensor_list,input_tensor_list,group=dist.group.WORLD)
torch.distributed.nn.all_to_all_single(output,input,output_split_sizes=None,input_split_sizes=None,group=dist.group.WORLD)
torch.distributed.nn.broadcast(tensor,src,group=dist.group.WORLD)
torch.distributed.nn.functional._AllGather(Function)
torch.distributed.nn.functional._AllGather.backward(ctx,*grad_outputs)
torch.distributed.nn.functional._AllGather.forward(ctx,group,tensor)
torch.distributed.nn.functional._AllReduce(Function)
torch.distributed.nn.functional._AllReduce.backward(ctx,grad_output)
torch.distributed.nn.functional._AllReduce.forward(ctx,op,group,tensor)
torch.distributed.nn.functional._AlltoAll(Function)
torch.distributed.nn.functional._AlltoAll.backward(ctx,*grad_outputs)
torch.distributed.nn.functional._AlltoAll.forward(ctx,group,out_tensor_list,*tensors)
torch.distributed.nn.functional._AlltoAllSingle(Function)
torch.distributed.nn.functional._AlltoAllSingle.backward(ctx,grad_output)
torch.distributed.nn.functional._AlltoAllSingle.forward(ctx,group,output,output_split_sizes,input_split_sizes,input)
torch.distributed.nn.functional._Broadcast(Function)
torch.distributed.nn.functional._Broadcast.backward(ctx,grad_output)
torch.distributed.nn.functional._Broadcast.forward(ctx,src,group,tensor)
torch.distributed.nn.functional._Gather(Function)
torch.distributed.nn.functional._Gather.backward(ctx,*grad_outputs)
torch.distributed.nn.functional._Gather.forward(ctx,dst,group,tensor)
torch.distributed.nn.functional._Reduce(Function)
torch.distributed.nn.functional._Reduce.backward(ctx,grad_output)
torch.distributed.nn.functional._Reduce.forward(ctx,src,op,group,tensor)
torch.distributed.nn.functional._Reduce_Scatter(Function)
torch.distributed.nn.functional._Reduce_Scatter.backward(ctx,grad_output)
torch.distributed.nn.functional._Reduce_Scatter.forward(ctx,op,group,tensor,*input_tensor_list)
torch.distributed.nn.functional._Scatter(Function)
torch.distributed.nn.functional._Scatter.backward(ctx,grad_output)
torch.distributed.nn.functional._Scatter.forward(ctx,src,group,*tensors)
torch.distributed.nn.functional.all_gather(tensor,group=dist.group.WORLD)
torch.distributed.nn.functional.all_reduce(tensor,op=dist.ReduceOp.SUM,group=dist.group.WORLD)
torch.distributed.nn.functional.all_to_all(output_tensor_list,input_tensor_list,group=dist.group.WORLD)
torch.distributed.nn.functional.all_to_all_single(output,input,output_split_sizes=None,input_split_sizes=None,group=dist.group.WORLD)
torch.distributed.nn.functional.broadcast(tensor,src,group=dist.group.WORLD)
torch.distributed.nn.functional.gather(tensor,dst=0,group=dist.group.WORLD)
torch.distributed.nn.functional.reduce(tensor,dst,op=dist.ReduceOp.SUM,group=dist.group.WORLD)
torch.distributed.nn.functional.reduce_scatter(output,input_list,op=dist.ReduceOp.SUM,group=dist.group.WORLD)
torch.distributed.nn.functional.scatter(tensors,src=0,group=dist.group.WORLD)
torch.distributed.nn.gather(tensor,dst=0,group=dist.group.WORLD)
torch.distributed.nn.reduce(tensor,dst,op=dist.ReduceOp.SUM,group=dist.group.WORLD)
torch.distributed.nn.reduce_scatter(output,input_list,op=dist.ReduceOp.SUM,group=dist.group.WORLD)
torch.distributed.nn.scatter(tensors,src=0,group=dist.group.WORLD)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/api/remote_module.py----------------------------------------
A:torch.distributed.nn.api.remote_module.T->TypeVar('T', bound='Module')
A:torch.distributed.nn.api.remote_module._NON_SCRIPTABLE_REMOTE_MODULE_MODULE->torch.distributed.nn.jit.instantiator.instantiate_non_scriptable_remote_module_template()
A:torch.distributed.nn.api.remote_module._SerializedRemoteModule->collections.namedtuple('_SerializedRemoteModule', _REMOTE_MODULE_PICKLED_ATTRIBUTES)
A:torch.distributed.nn.api.remote_module.module->torch.jit.script(module)
A:torch.distributed.nn.api.remote_module.enable_moving_cpu_tensors_to_cuda->object.__new__(RemoteModule)._prepare_init(remote_device)
A:torch.distributed.nn.api.remote_module.fut->torch.distributed.rpc.rpc_async(self.on, _instantiate_template, (_module_interface_cls, enable_moving_cpu_tensors_to_cuda))
A:torch.distributed.nn.api.remote_module.self.module_rref->torch.distributed.rpc.remote(self.on, _create_module, (module_cls, args, kwargs, self.device))
A:torch.distributed.nn.api.remote_module.remote_device->_remote_device(remote_device_str)
A:torch.distributed.nn.api.remote_module.self.device->str(remote_device.device())
A:torch.distributed.nn.api.remote_module.agent->torch.distributed.rpc._get_current_rpc_agent()
A:torch.distributed.nn.api.remote_module.self.is_device_map_set->bool(agent._get_device_map(agent.get_worker_info(self.on)))
A:torch.distributed.nn.api.remote_module.generated_module->torch.distributed.nn.jit.instantiator.instantiate_scriptable_remote_module_template(module_interface_cls, enable_moving_cpu_tensors_to_cuda)
A:torch.distributed.nn.api.remote_module.method->torch.jit.export(method)
A:torch.distributed.nn.api.remote_module.remote_module->object.__new__(RemoteModule)
A:torch.distributed.nn.api.remote_module.serialized_remote_module->collections.namedtuple('_SerializedRemoteModule', _REMOTE_MODULE_PICKLED_ATTRIBUTES)._make(remote_module_pickled_attrs)
A:torch.distributed.nn.api.remote_module.m->torch.jit.load(f)
A:torch.distributed.nn.api.remote_module.m.module_rref->torch.distributed.rpc.PyRRef._deserialize(m.module_rref)
A:torch.distributed.nn.api.remote_module.pickled_attrs[k]->v._serialize()
A:torch.distributed.nn.api.remote_module.f->io.BytesIO()
torch.distributed.nn.RemoteModule(self,remote_device:str,module_cls:Type[nn.Module],args:Tuple=None,kwargs:Dict[str,Any]=None)
torch.distributed.nn.api.remote_module.RemoteModule(self,remote_device:str,module_cls:Type[nn.Module],args:Tuple=None,kwargs:Dict[str,Any]=None)
torch.distributed.nn.api.remote_module.RemoteModule.__init__(self,remote_device:str,module_cls:Type[nn.Module],args:Tuple=None,kwargs:Dict[str,Any]=None)
torch.distributed.nn.api.remote_module._RemoteModule(self,remote_device:str,module_cls:Type[nn.Module],args:Tuple=None,kwargs:Dict[str,Any]=None,_module_interface_cls:Any=None)
torch.distributed.nn.api.remote_module._RemoteModule.__getstate__(self)
torch.distributed.nn.api.remote_module._RemoteModule.__init__(self,remote_device:str,module_cls:Type[nn.Module],args:Tuple=None,kwargs:Dict[str,Any]=None,_module_interface_cls:Any=None)
torch.distributed.nn.api.remote_module._RemoteModule.__setstate__(self,state)
torch.distributed.nn.api.remote_module._RemoteModule._check_attribute_picklability(self)
torch.distributed.nn.api.remote_module._RemoteModule._init_template(self,module_interface_cls,enable_moving_cpu_tensors_to_cuda)
torch.distributed.nn.api.remote_module._RemoteModule._install_generated_methods(self)
torch.distributed.nn.api.remote_module._RemoteModule._prepare_init(self,remote_device_str:str)->bool
torch.distributed.nn.api.remote_module._RemoteModule.add_module(self,name:str,module:Optional[Module])->None
torch.distributed.nn.api.remote_module._RemoteModule.apply(self:T,fn:Callable[[Module],None])->T
torch.distributed.nn.api.remote_module._RemoteModule.bfloat16(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.buffers(self,recurse:bool=True)->Iterator[Tensor]
torch.distributed.nn.api.remote_module._RemoteModule.children(self)->Iterator[Module]
torch.distributed.nn.api.remote_module._RemoteModule.cpu(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.cuda(self:T,device:Optional[Union[int,device]]=None)->T
torch.distributed.nn.api.remote_module._RemoteModule.double(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.eval(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.extra_repr(self)->str
torch.distributed.nn.api.remote_module._RemoteModule.float(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.get_module_rref(self)->rpc.RRef[nn.Module]
torch.distributed.nn.api.remote_module._RemoteModule.half(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.init_from_module_rref(remote_device:str,module_rref:rpc.RRef[nn.Module],_module_interface_cls:Any=None)
torch.distributed.nn.api.remote_module._RemoteModule.load_state_dict(self,state_dict:Union[Dict[str,Tensor],Dict[str,Tensor]],strict:bool=True)
torch.distributed.nn.api.remote_module._RemoteModule.modules(self)->Iterator[Module]
torch.distributed.nn.api.remote_module._RemoteModule.named_buffers(self,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Tensor]]
torch.distributed.nn.api.remote_module._RemoteModule.named_children(self)->Iterator[Tuple[str, Module]]
torch.distributed.nn.api.remote_module._RemoteModule.named_modules(self,memo:Optional[Set[Module]]=None,prefix:str='',remove_duplicate:bool=True)
torch.distributed.nn.api.remote_module._RemoteModule.named_parameters(self,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Parameter]]
torch.distributed.nn.api.remote_module._RemoteModule.parameters(self,recurse:bool=True)->Iterator[Parameter]
torch.distributed.nn.api.remote_module._RemoteModule.register_backward_hook(self,hook:Callable[[Module,_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.distributed.nn.api.remote_module._RemoteModule.register_buffer(self,name:str,tensor:Optional[Tensor],persistent:bool=True)->None
torch.distributed.nn.api.remote_module._RemoteModule.register_forward_hook(self,hook:Callable[...,None])->RemovableHandle
torch.distributed.nn.api.remote_module._RemoteModule.register_forward_pre_hook(self,hook:Callable[...,None])->RemovableHandle
torch.distributed.nn.api.remote_module._RemoteModule.register_parameter(self,name:str,param:Optional[Parameter])->None
torch.distributed.nn.api.remote_module._RemoteModule.remote_parameters(self,recurse:bool=True)->List[rpc.RRef[Parameter]]
torch.distributed.nn.api.remote_module._RemoteModule.requires_grad_(self:T,requires_grad:bool=True)->T
torch.distributed.nn.api.remote_module._RemoteModule.share_memory(self:T)->T
torch.distributed.nn.api.remote_module._RemoteModule.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.distributed.nn.api.remote_module._RemoteModule.to(self,*args,**kwargs)->T
torch.distributed.nn.api.remote_module._RemoteModule.train(self:T,mode:bool=True)->T
torch.distributed.nn.api.remote_module._RemoteModule.type(self:T,dst_type:Union[dtype,str])->T
torch.distributed.nn.api.remote_module._RemoteModule.xpu(self:T,device:Optional[Union[int,device]]=None)->T
torch.distributed.nn.api.remote_module._RemoteModule.zero_grad(self,set_to_none:bool=False)->None
torch.distributed.nn.api.remote_module._create_module(module_cls,args,kwargs,device)
torch.distributed.nn.api.remote_module._create_module_with_interface(module_cls,args,kwargs,device,module_interface_cls)
torch.distributed.nn.api.remote_module._instantiate_template(module_interface_cls,enable_moving_cpu_tensors_to_cuda)
torch.distributed.nn.api.remote_module._param_rrefs(module_rref,recurse)->List[rpc.RRef[Parameter]]
torch.distributed.nn.api.remote_module._raise_not_supported(name:str)->None
torch.distributed.nn.api.remote_module._recursive_script_module_receiver(recursive_script_module_serialized)
torch.distributed.nn.api.remote_module._recursive_script_module_reducer(recursive_script_module)
torch.distributed.nn.api.remote_module._remote_module_receiver(*remote_module_pickled_attrs)
torch.distributed.nn.api.remote_module._remote_module_reducer(remote_module)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/api/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/jit/instantiator.py----------------------------------------
A:torch.distributed.nn.jit.instantiator.logger->logging.getLogger(__name__)
A:torch.distributed.nn.jit.instantiator._TEMP_DIR->tempfile.TemporaryDirectory()
A:torch.distributed.nn.jit.instantiator.qualified_name->torch._jit_internal._qualified_name(module_interface)
A:torch.distributed.nn.jit.instantiator.module_interface_c->cu.get_interface(qualified_name)
A:torch.distributed.nn.jit.instantiator.method_schema->cu.get_interface(qualified_name).getMethod('forward')
A:torch.distributed.nn.jit.instantiator.default_value_str->' = {}'.format(argument.default_value)
A:torch.distributed.nn.jit.instantiator.arg_type_str->'{name}: {type}{default_value}'.format(name=argument.name, type=argument.type, default_value=default_value_str)
A:torch.distributed.nn.jit.instantiator.args_str->', '.join(arg_str_list)
A:torch.distributed.nn.jit.instantiator.arg_types_str->', '.join(arg_type_str_list)
A:torch.distributed.nn.jit.instantiator.return_type_str->str(argument.type)
A:torch.distributed.nn.jit.instantiator.old_text->f.read()
A:torch.distributed.nn.jit.instantiator.generated_code_text->get_remote_module_template(enable_moving_cpu_tensors_to_cuda).format(**str_dict)
A:torch.distributed.nn.jit.instantiator.out_path->os.path.join(INSTANTIATED_TEMPLATE_DIR_PATH, f'{generated_module_name}.py')
A:torch.distributed.nn.jit.instantiator.generated_module->importlib.import_module(f'{generated_module_name}')
A:torch.distributed.nn.jit.instantiator.module_interface_cls_name->torch._jit_internal._qualified_name(module_interface_cls).replace('.', '_')
A:torch.distributed.nn.jit.instantiator.(args_str, arg_types_str, return_type_str)->get_arg_return_types_from_interface(module_interface_cls)
A:torch.distributed.nn.jit.instantiator.str_dict->dict(assign_module_interface_cls='module_interface_cls = None', args='*args', kwargs='**kwargs', arg_types='*args, **kwargs', arrow_and_return_type='', arrow_and_future_return_type='', jit_script_decorator='')
torch.distributed.nn.jit.instantiator._do_instantiate_remote_module_template(generated_module_name,str_dict,enable_moving_cpu_tensors_to_cuda)
torch.distributed.nn.jit.instantiator._write(out_path,text)
torch.distributed.nn.jit.instantiator.get_arg_return_types_from_interface(module_interface)
torch.distributed.nn.jit.instantiator.instantiate_non_scriptable_remote_module_template()
torch.distributed.nn.jit.instantiator.instantiate_scriptable_remote_module_template(module_interface_cls,enable_moving_cpu_tensors_to_cuda=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/jit/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/jit/templates/remote_module_template.py----------------------------------------
torch.distributed.nn.jit.templates.remote_module_template.get_remote_module_template(enable_moving_cpu_tensors_to_cuda:bool)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/nn/jit/templates/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/join.py----------------------------------------
A:torch.distributed.algorithms.join.self._join_config->_JoinConfig.construct_disabled_join_config()
A:torch.distributed.algorithms.join.joinable._join_config->_JoinConfig(enable=self._enable, throw_on_early_termination=self._throw_on_early_termination, is_first_joinable=is_first_joinable)
A:torch.distributed.algorithms.join.self._rank->torch.distributed.get_rank(self._process_group)
A:torch.distributed.algorithms.join.num_nonjoined_procs->torch.zeros(1, device=self._device)
A:torch.distributed.algorithms.join.ones->torch.ones(1, device=device)
A:torch.distributed.algorithms.join.work->torch.distributed.all_reduce(ones, group=process_group, async_op=True)
A:torch.distributed.algorithms.join.zeros->torch.zeros(1, device=device)
A:torch.distributed.algorithms.join.should_throw->torch.zeros(1, device=device).item()
torch.distributed.algorithms.Join(self,joinables:List[Joinable],enable:bool=True,throw_on_early_termination:bool=False,**kwargs)
torch.distributed.algorithms.Join.__enter__(self)
torch.distributed.algorithms.Join.__exit__(self,type:Optional[Type[BaseException]],value:Optional[BaseException],traceback:Optional[TracebackType])
torch.distributed.algorithms.Join._extract_dist_info(self)->None
torch.distributed.algorithms.Join._get_num_nonjoined_procs(self)
torch.distributed.algorithms.Join._notify_procs_to_terminate(self)
torch.distributed.algorithms.Join._set_joinable_configs(self)->None
torch.distributed.algorithms.Join.notify_join_context(joinable:Joinable)
torch.distributed.algorithms.JoinHook
torch.distributed.algorithms.JoinHook.main_hook(self)->None
torch.distributed.algorithms.JoinHook.post_hook(self,is_last_joiner:bool)->None
torch.distributed.algorithms.Joinable(self)
torch.distributed.algorithms.Joinable.join_device(self)->torch.device
torch.distributed.algorithms.Joinable.join_hook(self,**kwargs)->JoinHook
torch.distributed.algorithms.Joinable.join_process_group(self)->Any
torch.distributed.algorithms.join.Join(self,joinables:List[Joinable],enable:bool=True,throw_on_early_termination:bool=False,**kwargs)
torch.distributed.algorithms.join.Join.__enter__(self)
torch.distributed.algorithms.join.Join.__exit__(self,type:Optional[Type[BaseException]],value:Optional[BaseException],traceback:Optional[TracebackType])
torch.distributed.algorithms.join.Join.__init__(self,joinables:List[Joinable],enable:bool=True,throw_on_early_termination:bool=False,**kwargs)
torch.distributed.algorithms.join.Join._extract_dist_info(self)->None
torch.distributed.algorithms.join.Join._get_num_nonjoined_procs(self)
torch.distributed.algorithms.join.Join._notify_procs_to_terminate(self)
torch.distributed.algorithms.join.Join._set_joinable_configs(self)->None
torch.distributed.algorithms.join.Join.notify_join_context(joinable:Joinable)
torch.distributed.algorithms.join.JoinHook
torch.distributed.algorithms.join.JoinHook.main_hook(self)->None
torch.distributed.algorithms.join.JoinHook.post_hook(self,is_last_joiner:bool)->None
torch.distributed.algorithms.join.Joinable(self)
torch.distributed.algorithms.join.Joinable.__init__(self)
torch.distributed.algorithms.join.Joinable.join_device(self)->torch.device
torch.distributed.algorithms.join.Joinable.join_hook(self,**kwargs)->JoinHook
torch.distributed.algorithms.join.Joinable.join_process_group(self)->Any
torch.distributed.algorithms.join._JoinConfig(NamedTuple)
torch.distributed.algorithms.join._JoinConfig.construct_disabled_join_config()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/model_averaging/utils.py----------------------------------------
A:torch.distributed.algorithms.model_averaging.utils.(params_it1, params_it2)->itertools.tee(params)
A:torch.distributed.algorithms.model_averaging.utils.flat_params->torch.cat([p.data.view(-1) for p in params_it1])
A:torch.distributed.algorithms.model_averaging.utils.p.data->flat_params[offset:offset + p.numel()].view_as(p).type_as(p)
torch.distributed.algorithms.model_averaging.utils.average_parameters(params:Iterator[torch.nn.Parameter],process_group:dist.ProcessGroup)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/model_averaging/averagers.py----------------------------------------
torch.distributed.algorithms.model_averaging.averagers.ModelAverager(self,process_group=None)
torch.distributed.algorithms.model_averaging.averagers.ModelAverager.__init__(self,process_group=None)
torch.distributed.algorithms.model_averaging.averagers.ModelAverager.average_parameters(self,params)
torch.distributed.algorithms.model_averaging.averagers.PeriodicModelAverager(self,period,warmup_steps=0,process_group=None)
torch.distributed.algorithms.model_averaging.averagers.PeriodicModelAverager.__init__(self,period,warmup_steps=0,process_group=None)
torch.distributed.algorithms.model_averaging.averagers.PeriodicModelAverager.average_parameters(self,params)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/model_averaging/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py----------------------------------------
A:torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.f_optim->as_functional_optim(self.optim_cls, *optim_args, **optim_kwargs)
A:torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.self._opt_hook_state->_OptimizerHookState(f_optim, params)
torch.distributed.algorithms._optimizer_overlap._as_overlapped_optim(optim_cls:Type,params,*args,**kwargs)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.OverlappedOptimizer(self,optim_cls:Type)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.OverlappedOptimizer.__init__(self,optim_cls:Type)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.OverlappedOptimizer.register_ddp(self,ddp:DistributedDataParallel)->None
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.OverlappedOptimizer.register_fsdp(self,fsdp:FullyShardedDataParallel)->None
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap._OverlappedStandardOptimizer(self,optim_cls:Type,params,*optim_args,**optim_kwargs)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap._OverlappedStandardOptimizer.__init__(self,optim_cls:Type,params,*optim_args,**optim_kwargs)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap._OverlappedStandardOptimizer.register_ddp(self,ddp_inst:DistributedDataParallel)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap._as_overlapped_optim(optim_cls:Type,params,*args,**kwargs)
torch.distributed.algorithms._optimizer_overlap.optimizer_overlap.register_overlapped(optim_cls)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/_optimizer_overlap/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/_checkpoint/_checkpoint_wrapper.py----------------------------------------
A:torch.distributed.algorithms._checkpoint._checkpoint_wrapper.REENTRANT->auto()
A:torch.distributed.algorithms._checkpoint._checkpoint_wrapper.NO_REENTRANT->auto()
torch.distributed.algorithms._checkpoint._checkpoint_wrapper.CheckpointImpl(Enum)
torch.distributed.algorithms._checkpoint._checkpoint_wrapper._CheckpointWrapper(self,mod:torch.nn.Module,checkpoint_impl:CheckpointImpl=CheckpointImpl.REENTRANT,offload_to_cpu:bool=False)
torch.distributed.algorithms._checkpoint._checkpoint_wrapper._CheckpointWrapper.__init__(self,mod:torch.nn.Module,checkpoint_impl:CheckpointImpl=CheckpointImpl.REENTRANT,offload_to_cpu:bool=False)
torch.distributed.algorithms._checkpoint._checkpoint_wrapper._CheckpointWrapper.forward(self,*args,**kwargs)
torch.distributed.algorithms._checkpoint._checkpoint_wrapper.checkpoint_wrapper(module:torch.nn.Module,checkpoint_impl:CheckpointImpl=CheckpointImpl.REENTRANT,offload_to_cpu:bool=False)->torch.nn.Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/_checkpoint/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.y->y.to(torch.float32).cuda(y.device).to(torch.float32).cuda(y.device)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.x->torch.zeros_like(y, device=y.device)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.world_size->group_to_use.size()
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.tensor->bucket.buffer()
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.myObserver->torch.quantization.MinMaxObserver().cuda(tensor.device)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.(s, z)->torch.quantization.MinMaxObserver().cuda(tensor.device).calculate_qparams()
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.s_and_z->torch.stack((s_ch, z_ch)).cuda(tensor.device)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.all_ranks_s_and_z->_get_allgather_out_list(s_and_z, world_size)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.fut->torch.distributed.all_gather(_get_allgather_out_list(quantized_tensor, world_size), quantized_tensor, group=group_to_use, async_op=True).get_future()
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.quantized_tensor->_quantize_per_channel_cuda(tensor_in_channels, all_ranks_s_and_z[rank, 0, :], all_ranks_s_and_z[rank, 1, :])
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.aggregated_dequantized_tensor->torch.zeros_like(all_ranks_quantized_tensor[0], device=tensor.device, dtype=torch.float32)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.tensor_in_channels->torch.nn.functional.pad(input=tensor, pad=(0, bucket_size - len(tensor) % bucket_size), mode='constant', value=0).view(-1, bucket_size).cuda(tensor.device)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.myPerChannelObserver->torch.quantization.PerChannelMinMaxObserver().cuda(tensor.device)
A:torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.(s_ch, z_ch)->torch.quantization.PerChannelMinMaxObserver().cuda(tensor.device).calculate_qparams()
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks._dequantize_per_channel_cuda(y,scale,zero_point)
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks._dequantize_per_tensor_cuda(y,scale,zero_point)
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks._get_allgather_out_list(all_gather_in_list,world_size)
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks._quantize_per_channel_cuda(x,scale,zero_point)
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks._quantize_per_tensor_cuda(x,scale,zero_point)
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.quantization_perchannel_hook(process_group:dist.ProcessGroup,bucket:dist.GradBucket,bucket_size=512)->torch.futures.Future[torch.Tensor]
torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks.quantization_pertensor_hook(process_group:dist.ProcessGroup,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/debugging_hooks.py----------------------------------------
torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook(_:Any,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.stats->state.compression_stats()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.self.rng->numpy.random.RandomState(random_seed)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.self.compression_stats_logging_frequency->max(1, compression_stats_logging_frequency)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.world_size->group_to_use.size()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.input_tensor->bucket.buffer()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.bucket_index->bucket.index()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.state.error_dict[bucket_index]->torch.zeros(padded_total_length, device=device, dtype=input_tensor.dtype)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.input_tensor_cp->torch.clone(input_tensor).detach()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.tensors->bucket.gradients()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.matrix->bucket.buffer().view(square_side_length, square_side_length)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.matrix_approximation_rank->min(n, m, state.matrix_approximation_rank)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.compress_test->_should_compress(n, m, matrix_approximation_rank, state.min_compression_rate)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.state.p_memory_dict[bucket_index]->create_low_rank_tensor(fill_random_values=False, rng=state.rng)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.state.q_memory_dict[bucket_index]->fut.value().div_(world_size)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.allreduce_contiguous_uncompressed_tensors_fut->torch.distributed.all_reduce(uncompressed_tensors_memory, group=group_to_use, async_op=True).get_future()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.uncompressed_tensors_memory->fut.value()[0].div_(world_size)
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.square_side_length->math.ceil(math.sqrt(total_length))
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.allreduce_p_fut->torch.distributed.all_reduce(state.p_memory_dict[bucket_index], group=group_to_use, async_op=True).get_future()
A:torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.ret->bucket.buffer().resize_(total_length)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(self,process_group,matrix_approximation_rank=1,start_powerSGD_iter=1000,min_compression_rate=2,use_error_feedback=True,warm_start=True,orthogonalization_epsilon=0,random_seed=0,compression_stats_logging_frequency=10000)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__init__(self,process_group,matrix_approximation_rank=1,start_powerSGD_iter=1000,min_compression_rate=2,use_error_feedback=True,warm_start=True,orthogonalization_epsilon=0,random_seed=0,compression_stats_logging_frequency=10000)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.compression_stats(self)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.maybe_increase_iter(self,bucket)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook._orthogonalize(matrix,epsilon=0)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook._report_compression_stats(bucket,state)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook._should_compress(num_rows,num_cols,matrix_approximation_rank,min_compression_rate)
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state:PowerSGDState,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]
torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state:PowerSGDState,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/post_localSGD_hook.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.world_size->global_group_to_use.size()
A:torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.input_tensor->bucket.buffer()
A:torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.(state.subgroup, _)->torch.distributed.new_subgroups()
torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.PostLocalSGDState(self,process_group,subgroup,start_localSGD_iter)
torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.PostLocalSGDState.__init__(self,process_group,subgroup,start_localSGD_iter)
torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.PostLocalSGDState.maybe_increase_iter(self,bucket)
torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.post_localSGD_hook(state:PostLocalSGDState,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks.self.params_to_optimize->set(params)
A:torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks.fut->hook(hook_state, bucket)
A:torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks.gradient_tensors->bucket.gradients()
A:torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks.model_params->bucket.parameters()
torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks._OptimizerHookState(self,functional_optim,params=None)
torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks._OptimizerHookState.__init__(self,functional_optim,params=None)
torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks._OptimizerHookState._check_valid_functional_optim(self)
torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks._OptimizerHookState._set_params_to_optimize(self,params)
torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks._hook_then_optimizer(hook:Callable[[Any,dist.GradBucket],torch.futures.Future[torch.Tensor]],optimizer_state:_OptimizerHookState)->Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.bucket_index->bucket.index()
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.num_local_optim_params->len(zero.optim.param_groups[0]['params'])
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.length->len(bucket_assignment.parameters)
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.assigned_ranks->sorted(overlap_info.assigned_ranks_per_bucket[bucket_index])
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.bucket_params->bucket.parameters()
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.ddp_ref->weakref.ref(ddp)
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.fut->hook(state, bucket)
A:torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.num_buckets->len(overlap_info.params_per_bucket)
torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook._broadcast_bucket(bucket_index:int,zero:ZeroRedundancyOptimizer)
torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook._hook_with_zero_step_setup(ddp_ref:weakref.ReferenceType,zero:ZeroRedundancyOptimizer,bucket:dist.GradBucket)
torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook._perform_local_step(bucket:dist.GradBucket,zero:ZeroRedundancyOptimizer,rank:int)
torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook._save_ddp_bucket_info(bucket:dist.GradBucket,zero:ZeroRedundancyOptimizer)
torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.hook_with_zero_step(hook:Callable[[Any,dist.GradBucket],torch.futures.Future],ddp:DistributedDataParallel,zero:ZeroRedundancyOptimizer,shard_buckets:bool=False)->Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]
torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.hook_with_zero_step_interleaved(hook:Callable[[Any,dist.GradBucket],torch.futures.Future],ddp:DistributedDataParallel,zero:ZeroRedundancyOptimizer,shard_buckets:bool=False)->Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/__init__.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.powerSGD_state->powerSGD.PowerSGDState(process_group=state, matrix_approximation_rank=matrix_approximation_rank, start_powerSGD_iter=start_powerSGD_iter)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.ALLREDUCE->partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.FP16_COMPRESS->partial(_ddp_comm_hook_wrapper, comm_hook=default.fp16_compress_hook)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.BF16_COMPRESS->partial(_ddp_comm_hook_wrapper, comm_hook=default.bf16_compress_hook)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.QUANTIZE_PER_TENSOR->partial(_ddp_comm_hook_wrapper, comm_hook=quantization.quantization_pertensor_hook)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.QUANTIZE_PER_CHANNEL->partial(_ddp_comm_hook_wrapper, comm_hook=quantization.quantization_perchannel_hook)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.POWER_SGD->partial(_powerSGD_comm_hook_wrapper, comm_hook=powerSGD.powerSGD_hook, matrix_approximation_rank=1)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.POWER_SGD_RANK2->partial(_powerSGD_comm_hook_wrapper, comm_hook=powerSGD.powerSGD_hook, matrix_approximation_rank=2)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.BATCHED_POWER_SGD->partial(_powerSGD_comm_hook_wrapper, comm_hook=powerSGD.batched_powerSGD_hook, matrix_approximation_rank=1)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.BATCHED_POWER_SGD_RANK2->partial(_powerSGD_comm_hook_wrapper, comm_hook=powerSGD.batched_powerSGD_hook, matrix_approximation_rank=2)
A:torch.distributed.algorithms.ddp_comm_hooks.__init__.NOOP->partial(_ddp_comm_hook_wrapper, comm_hook=debugging.noop_hook)
torch.distributed.algorithms.ddp_comm_hooks.__init__.DDPCommHookType(Enum)
torch.distributed.algorithms.ddp_comm_hooks.__init__._ddp_comm_hook_wrapper(comm_hook,model,state)
torch.distributed.algorithms.ddp_comm_hooks.__init__._powerSGD_comm_hook_wrapper(comm_hook,model,state,matrix_approximation_rank,start_powerSGD_iter=1000)
torch.distributed.algorithms.ddp_comm_hooks.__init__.register_ddp_comm_hook(comm_hook_type:DDPCommHookType,model:DistributedDataParallel,state=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/algorithms/ddp_comm_hooks/default_hooks.py----------------------------------------
A:torch.distributed.algorithms.ddp_comm_hooks.default_hooks.world_size->group_to_use.size()
A:torch.distributed.algorithms.ddp_comm_hooks.default_hooks.compressed_tensor->bucket.buffer().to(torch.bfloat16).div_(world_size)
A:torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fut->hook(hook_state, bucket)
A:torch.distributed.algorithms.ddp_comm_hooks.default_hooks.decompressed_tensor->bucket.buffer()
torch.distributed.algorithms.ddp_comm_hooks.default_hooks._allreduce_fut(process_group:dist.ProcessGroup,tensor:torch.Tensor)->torch.futures.Future[torch.Tensor]
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group:dist.ProcessGroup,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook(process_group:dist.ProcessGroup,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper(hook:Callable[[Any,dist.GradBucket],torch.futures.Future[torch.Tensor]])->Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group:dist.ProcessGroup,bucket:dist.GradBucket)->torch.futures.Future[torch.Tensor]
torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper(hook:Callable[[Any,dist.GradBucket],torch.futures.Future[torch.Tensor]])->Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_adagrad.py----------------------------------------
A:torch.distributed.optim.functional_adagrad.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
torch.distributed.optim._FunctionalAdagrad(self,params:List[Tensor],lr:float=0.01,lr_decay:float=0.0,weight_decay:float=0.0,initial_accumulator_value:float=0.0,warmup_lr_multiplier:float=1.0,warmup_num_iters:float=0.0,eps:float=1e-10,coalesce_grad:bool=True,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalAdagrad.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_adagrad._FunctionalAdagrad(self,params:List[Tensor],lr:float=0.01,lr_decay:float=0.0,weight_decay:float=0.0,initial_accumulator_value:float=0.0,warmup_lr_multiplier:float=1.0,warmup_num_iters:float=0.0,eps:float=1e-10,coalesce_grad:bool=True,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adagrad._FunctionalAdagrad.__init__(self,params:List[Tensor],lr:float=0.01,lr_decay:float=0.0,weight_decay:float=0.0,initial_accumulator_value:float=0.0,warmup_lr_multiplier:float=1.0,warmup_num_iters:float=0.0,eps:float=1e-10,coalesce_grad:bool=True,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adagrad._FunctionalAdagrad.step(self,gradients:List[Optional[Tensor]])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/utils.py----------------------------------------
torch.distributed.optim.as_functional_optim(optim_cls:Type,*args,**kwargs)
torch.distributed.optim.utils._create_functional_optim(functional_optim_cls:Type,*args,**kwargs)
torch.distributed.optim.utils.as_functional_optim(optim_cls:Type,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_sgd.py----------------------------------------
A:torch.distributed.optim.functional_sgd.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
torch.distributed.optim._FunctionalSGD(self,params:List[Tensor],lr:float=0.01,momentum:float=0.0,dampening:float=0.0,weight_decay:float=0.0,nesterov:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalSGD.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim._FunctionalSGD.step_param(self,param:Tensor,grad:Optional[Tensor])
torch.distributed.optim.functional_sgd._FunctionalSGD(self,params:List[Tensor],lr:float=0.01,momentum:float=0.0,dampening:float=0.0,weight_decay:float=0.0,nesterov:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_sgd._FunctionalSGD.__init__(self,params:List[Tensor],lr:float=0.01,momentum:float=0.0,dampening:float=0.0,weight_decay:float=0.0,nesterov:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_sgd._FunctionalSGD.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_sgd._FunctionalSGD.step_param(self,param:Tensor,grad:Optional[Tensor])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/zero_redundancy_optimizer.py----------------------------------------
A:torch.distributed.optim.zero_redundancy_optimizer.buffer->io.BytesIO(data_recv_tensor.cpu().numpy())
A:torch.distributed.optim.zero_redundancy_optimizer.data->bytearray(buffer.getbuffer())
A:torch.distributed.optim.zero_redundancy_optimizer.length_tensor->torch.LongTensor([0]).to(device)
A:torch.distributed.optim.zero_redundancy_optimizer.data_send_tensor->torch.ByteTensor(data).to(device)
A:torch.distributed.optim.zero_redundancy_optimizer.data_recv_tensor->torch.empty([int(length_tensor.item())], dtype=torch.uint8, device=device)
A:torch.distributed.optim.zero_redundancy_optimizer.obj->torch.load(buffer, map_location=device)
A:torch.distributed.optim.zero_redundancy_optimizer._->list(map(lambda x: x.wait(), handles))
A:torch.distributed.optim.zero_redundancy_optimizer.self._is_trainable_mask->self._get_is_trainable_mask()
A:torch.distributed.optim.zero_redundancy_optimizer.self._optim_constructor->self._get_optimizer_constructor(optimizer_class)
A:torch.distributed.optim.zero_redundancy_optimizer.empty_messenger->torch.tensor([0], dtype=torch.uint8, device=self._default_device)
A:torch.distributed.optim.zero_redundancy_optimizer.global_rank->_get_global_rank(self.process_group, rank)
A:torch.distributed.optim.zero_redundancy_optimizer.local_state_dict->_broadcast_object(empty_messenger, src_rank=global_rank, group=self.process_group, device=self._default_device)
A:torch.distributed.optim.zero_redundancy_optimizer.all_params_set->set(self._all_params)
A:torch.distributed.optim.zero_redundancy_optimizer.rank_param_group->copy.copy(param_group)
A:torch.distributed.optim.zero_redundancy_optimizer.params_sorted->sorted(param_group['params'], key=lambda t: t.numel(), reverse=True)
A:torch.distributed.optim.zero_redundancy_optimizer.rank->self._get_min_index(sizes)
A:torch.distributed.optim.zero_redundancy_optimizer.self._index_to_param_cache->list(chain(*(g['params'] for g in self.param_groups)))
A:torch.distributed.optim.zero_redundancy_optimizer.min_value->float('inf')
A:torch.distributed.optim.zero_redundancy_optimizer.self._bucket_assignments_per_rank_cache[assigned_rank][bucket_index]->_DDPBucketAssignment(bucket_index, bucket_params, bucket_offset)
A:torch.distributed.optim.zero_redundancy_optimizer.offsets[bucket_index]->len(params_per_rank[assigned_rank])
A:torch.distributed.optim.zero_redundancy_optimizer.num_buckets->len(params_per_bucket)
A:torch.distributed.optim.zero_redundancy_optimizer.assigned_rank->self._get_min_index(size_per_rank, assigned_ranks_per_bucket[bucket_index])
A:torch.distributed.optim.zero_redundancy_optimizer.params_per_bucket_enum->sorted(enumerate(params_per_bucket), key=lambda x: sum((p.numel() for p in x[1])))
A:torch.distributed.optim.zero_redundancy_optimizer.param_numel->param.numel()
A:torch.distributed.optim.zero_redundancy_optimizer.is_trainable_mask->self._get_is_trainable_mask()
A:torch.distributed.optim.zero_redundancy_optimizer.loss->self._local_step(closure=closure, **kwargs)
A:torch.distributed.optim.zero_redundancy_optimizer.self.optim.state[param]->_recursive_copy_to_device(value, non_blocking=True, device=param.device)
A:torch.distributed.optim.zero_redundancy_optimizer.state_dict->super().state_dict()
A:torch.distributed.optim.zero_redundancy_optimizer.state_dict['state']->dict(sorted(state_dict['state'].items()))
A:torch.distributed.optim.zero_redundancy_optimizer.num_devices->len(self._device_to_params_per_rank)
A:torch.distributed.optim.zero_redundancy_optimizer.param.data->tensor[offset:offset_next].view_as(param.data)
A:torch.distributed.optim.zero_redundancy_optimizer.bucket->torch.empty(bucket_size, dtype=dtype, device=device)
A:torch.distributed.optim.zero_redundancy_optimizer.tensor->torch.empty(bucket_size, dtype=dtype, device=bucket_assignment.device)
A:torch.distributed.optim.zero_redundancy_optimizer.self._all_params->list(params)
A:torch.distributed.optim.zero_redundancy_optimizer.typename->torch.typename(self._all_params[0])
A:torch.distributed.optim.zero_redundancy_optimizer.other_typename->torch.typename(param)
A:torch.distributed.optim.zero_redundancy_optimizer.local_numel->sum((p.numel() for p in params))
A:torch.distributed.optim.zero_redundancy_optimizer.num_assigned_buckets->len(self._bucket_assignments_per_rank[self.global_rank])
A:torch.distributed.optim.zero_redundancy_optimizer.functional_optims->torch.distributed.optim.utils.functional_optim_map.values()
torch.distributed.optim.ZeroRedundancyOptimizer(self,params,optimizer_class:Type[Optimizer],process_group:Optional[Any]=None,parameters_as_bucket_view:bool=False,overlap_with_ddp:bool=False,**defaults:Any)
torch.distributed.optim.ZeroRedundancyOptimizer._assign_bucket_subset_to_rank(self,bucket_index:int,bucket_params:List[torch.Tensor],bucket_offset:int,assigned_rank:int,assigned_ranks_per_bucket:List[Set[int]])->None
torch.distributed.optim.ZeroRedundancyOptimizer._broadcast_params_from_rank(self,rank:int)
torch.distributed.optim.ZeroRedundancyOptimizer._bucket_assignments_per_rank(self)->List[Dict[int, _DDPBucketAssignment]]
torch.distributed.optim.ZeroRedundancyOptimizer._build_ddp_param_buckets(self)->None
torch.distributed.optim.ZeroRedundancyOptimizer._build_param_buckets(self)->None
torch.distributed.optim.ZeroRedundancyOptimizer._check_overlap_initialized(self)
torch.distributed.optim.ZeroRedundancyOptimizer._clear_cache(self)->None
torch.distributed.optim.ZeroRedundancyOptimizer._device_to_params_per_rank(self)->Dict[torch.device, List[List[torch.Tensor]]]
torch.distributed.optim.ZeroRedundancyOptimizer._get_assigned_rank(self,bucket_index:int)->int
torch.distributed.optim.ZeroRedundancyOptimizer._get_is_trainable_mask(self)->List[bool]
torch.distributed.optim.ZeroRedundancyOptimizer._get_min_index(self,values:List[int],disallowed_indices:Optional[Set[int]]=None)->int
torch.distributed.optim.ZeroRedundancyOptimizer._get_optimizer_constructor(self,optimizer_class:Any)->Any
torch.distributed.optim.ZeroRedundancyOptimizer._index_to_param(self)->List[torch.Tensor]
torch.distributed.optim.ZeroRedundancyOptimizer._init_local_optimizer(self)->None
torch.distributed.optim.ZeroRedundancyOptimizer._init_zero_for_overlap(self)->None
torch.distributed.optim.ZeroRedundancyOptimizer._local_step(self,gradients:Optional[List[Optional[torch.Tensor]]]=None,closure:Optional[Callable[[],float]]=None,**kwargs:Any)->Optional[float]
torch.distributed.optim.ZeroRedundancyOptimizer._param_to_index(self)->Dict[torch.Tensor, int]
torch.distributed.optim.ZeroRedundancyOptimizer._param_to_rank(self)->Dict[torch.Tensor, int]
torch.distributed.optim.ZeroRedundancyOptimizer._partition_param_group(self,param_group:Dict[str,Any],params_per_rank:List[List[torch.Tensor]])->None
torch.distributed.optim.ZeroRedundancyOptimizer._partition_parameters(self,params_per_rank:Optional[List[List[torch.Tensor]]]=None)->List[List[Dict]]
torch.distributed.optim.ZeroRedundancyOptimizer._sync_param_groups(src_param_groups:List[Dict[Any,Any]],dst_param_groups:List[Dict[Any,Any]])->None
torch.distributed.optim.ZeroRedundancyOptimizer._sync_params(self)
torch.distributed.optim.ZeroRedundancyOptimizer._verify_and_init_params(self,params:Any)->None
torch.distributed.optim.ZeroRedundancyOptimizer._verify_params_per_rank(self,params_per_rank:List[List[torch.Tensor]])->None
torch.distributed.optim.ZeroRedundancyOptimizer._verify_same_dense_param_type(self)->None
torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group(self,param_group:dict)->None
torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict(self,to:int=0)->None
torch.distributed.optim.ZeroRedundancyOptimizer.join_device(self)->torch.device
torch.distributed.optim.ZeroRedundancyOptimizer.join_hook(self,**kwargs)
torch.distributed.optim.ZeroRedundancyOptimizer.join_process_group(self)->Any
torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict(self,state_dict:Dict[str,Any])->None
torch.distributed.optim.ZeroRedundancyOptimizer.state_dict(self)->Dict[str, Any]
torch.distributed.optim.ZeroRedundancyOptimizer.step(self,closure:Optional[Callable[[],float]]=None,**kwargs:Any)->Optional[float]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer(self,params,optimizer_class:Type[Optimizer],process_group:Optional[Any]=None,parameters_as_bucket_view:bool=False,overlap_with_ddp:bool=False,**defaults:Any)
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.__init__(self,params,optimizer_class:Type[Optimizer],process_group:Optional[Any]=None,parameters_as_bucket_view:bool=False,overlap_with_ddp:bool=False,**defaults:Any)
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._assign_bucket_subset_to_rank(self,bucket_index:int,bucket_params:List[torch.Tensor],bucket_offset:int,assigned_rank:int,assigned_ranks_per_bucket:List[Set[int]])->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._broadcast_params_from_rank(self,rank:int)
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._bucket_assignments_per_rank(self)->List[Dict[int, _DDPBucketAssignment]]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._build_ddp_param_buckets(self)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._build_param_buckets(self)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._check_overlap_initialized(self)
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._clear_cache(self)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._device_to_params_per_rank(self)->Dict[torch.device, List[List[torch.Tensor]]]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._get_assigned_rank(self,bucket_index:int)->int
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._get_is_trainable_mask(self)->List[bool]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._get_min_index(self,values:List[int],disallowed_indices:Optional[Set[int]]=None)->int
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._get_optimizer_constructor(self,optimizer_class:Any)->Any
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._index_to_param(self)->List[torch.Tensor]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._init_local_optimizer(self)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._init_zero_for_overlap(self)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._local_step(self,gradients:Optional[List[Optional[torch.Tensor]]]=None,closure:Optional[Callable[[],float]]=None,**kwargs:Any)->Optional[float]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._param_to_index(self)->Dict[torch.Tensor, int]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._param_to_rank(self)->Dict[torch.Tensor, int]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._partition_param_group(self,param_group:Dict[str,Any],params_per_rank:List[List[torch.Tensor]])->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._partition_parameters(self,params_per_rank:Optional[List[List[torch.Tensor]]]=None)->List[List[Dict]]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._sync_param_groups(src_param_groups:List[Dict[Any,Any]],dst_param_groups:List[Dict[Any,Any]])->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._sync_params(self)
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._verify_and_init_params(self,params:Any)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._verify_params_per_rank(self,params_per_rank:List[List[torch.Tensor]])->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer._verify_same_dense_param_type(self)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.add_param_group(self,param_group:dict)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.consolidate_state_dict(self,to:int=0)->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.join_device(self)->torch.device
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.join_hook(self,**kwargs)
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.join_process_group(self)->Any
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.load_state_dict(self,state_dict:Dict[str,Any])->None
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.state_dict(self)->Dict[str, Any]
torch.distributed.optim.zero_redundancy_optimizer.ZeroRedundancyOptimizer.step(self,closure:Optional[Callable[[],float]]=None,**kwargs:Any)->Optional[float]
torch.distributed.optim.zero_redundancy_optimizer._DDPBucketAssignment(self,bucket_index:int,parameters:List[torch.Tensor],offset:int)
torch.distributed.optim.zero_redundancy_optimizer._DDPBucketAssignment.__init__(self,bucket_index:int,parameters:List[torch.Tensor],offset:int)
torch.distributed.optim.zero_redundancy_optimizer._OverlapInfo(self,world_size)
torch.distributed.optim.zero_redundancy_optimizer._OverlapInfo.__init__(self,world_size)
torch.distributed.optim.zero_redundancy_optimizer._OverlapInfo.clear_per_iter_info(self)->None
torch.distributed.optim.zero_redundancy_optimizer._OverlapInfo.wait_for_broadcasts(self)->None
torch.distributed.optim.zero_redundancy_optimizer._OverlapStatus(enum.IntEnum)
torch.distributed.optim.zero_redundancy_optimizer._ZeROJoinHook(self,zero)
torch.distributed.optim.zero_redundancy_optimizer._ZeROJoinHook.__init__(self,zero)
torch.distributed.optim.zero_redundancy_optimizer._ZeROJoinHook.main_hook(self)
torch.distributed.optim.zero_redundancy_optimizer._broadcast_object(obj:Any,src_rank:int,group:object=dist.group.WORLD,device:torch.device=torch.device('cpu'))->Any
torch.distributed.optim.zero_redundancy_optimizer._get_global_rank(group:Any,rank:int)->int
torch.distributed.optim.zero_redundancy_optimizer._is_trainable(param:torch.Tensor)->bool
torch.distributed.optim.zero_redundancy_optimizer._recursive_copy_to_device(value:Any,non_blocking:bool,device:torch.device)->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_adamw.py----------------------------------------
A:torch.distributed.optim.functional_adamw.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
A:torch.distributed.optim.functional_adamw.state['step']->torch.tensor(0.0)
A:torch.distributed.optim.functional_adamw.state['exp_avg']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_adamw.state['exp_avg_sq']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_adamw.state['max_exp_avg_sq']->torch.zeros_like(param, memory_format=torch.preserve_format)
torch.distributed.optim._FunctionalAdamW(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.01,amsgrad:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalAdamW.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim._FunctionalAdamW.step_param(self,param:Tensor,grad:Optional[Tensor])
torch.distributed.optim.functional_adamw._FunctionalAdamW(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.01,amsgrad:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adamw._FunctionalAdamW.__init__(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.01,amsgrad:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adamw._FunctionalAdamW.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_adamw._FunctionalAdamW.step_param(self,param:Tensor,grad:Optional[Tensor])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_rmsprop.py----------------------------------------
A:torch.distributed.optim.functional_rmsprop.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
A:torch.distributed.optim.functional_rmsprop.state['step']->torch.tensor(0.0)
A:torch.distributed.optim.functional_rmsprop.state['square_avg']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_rmsprop.state['momentum_buffer']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_rmsprop.state['grad_avg']->torch.zeros_like(param, memory_format=torch.preserve_format)
torch.distributed.optim._FunctionalRMSprop(self,params:List[Tensor],lr:float=0.01,alpha:float=0.99,eps:float=1e-08,weight_decay:float=0.0,momentum:float=0.0,centered:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalRMSprop.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_rmsprop._FunctionalRMSprop(self,params:List[Tensor],lr:float=0.01,alpha:float=0.99,eps:float=1e-08,weight_decay:float=0.0,momentum:float=0.0,centered:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_rmsprop._FunctionalRMSprop.__init__(self,params:List[Tensor],lr:float=0.01,alpha:float=0.99,eps:float=1e-08,weight_decay:float=0.0,momentum:float=0.0,centered:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_rmsprop._FunctionalRMSprop.step(self,gradients:List[Optional[Tensor]])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/optimizer.py----------------------------------------
A:torch.distributed.optim.optimizer.logger->logging.getLogger(__name__)
A:torch.distributed.optim.optimizer.compile_lock->Lock()
A:torch.distributed.optim.optimizer.self.optim->optim_cls(self._local_params, *args, **kwargs)
A:torch.distributed.optim.optimizer.all_local_grads->torch.distributed.autograd.get_gradients(autograd_ctx_id)
A:torch.distributed.optim.optimizer.global_lock->Lock()
A:torch.distributed.optim.optimizer.local_optim->local_optim_rref.local_value()
A:torch.distributed.optim.optimizer.optim->_ScriptLocalOptimizer(optim_cls, local_params_rref, *args, **kwargs)
A:torch.distributed.optim.optimizer.script_optim->torch.jit.script(optim)
A:torch.distributed.optim.optimizer.per_worker_params_rref->defaultdict(list)
A:torch.distributed.optim.optimizer.optim_ctor->utils.functional_optim_map.get(optimizer_class)
A:torch.distributed.optim.optimizer.remote_optim_rref_fut->torch.distributed.rpc.rpc_async(worker, optimizer_new_func, args=(optim_ctor, param_rrefs) + args, kwargs=kwargs)
A:torch.distributed.optim.optimizer.self.remote_optimizers->_wait_for_all(remote_optim_futs)
torch.distributed.optim.DistributedOptimizer(self,optimizer_class,params_rref,*args,**kwargs)
torch.distributed.optim.DistributedOptimizer.step(self,context_id)
torch.distributed.optim.optimizer.DistributedOptimizer(self,optimizer_class,params_rref,*args,**kwargs)
torch.distributed.optim.optimizer.DistributedOptimizer.__init__(self,optimizer_class,params_rref,*args,**kwargs)
torch.distributed.optim.optimizer.DistributedOptimizer.step(self,context_id)
torch.distributed.optim.optimizer._LocalOptimizer(self,optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._LocalOptimizer.__init__(self,optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._LocalOptimizer.step(self,autograd_ctx_id)
torch.distributed.optim.optimizer._ScriptLocalOptimizer(self,optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._ScriptLocalOptimizer.__init__(self,optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._ScriptLocalOptimizer.step(self,autograd_ctx_id:int)
torch.distributed.optim.optimizer._ScriptLocalOptimizerInterface(object)
torch.distributed.optim.optimizer._ScriptLocalOptimizerInterface.step(self,autograd_ctx_id:int)->None
torch.distributed.optim.optimizer._local_optimizer_step(local_optim_rref,autograd_ctx_id)
torch.distributed.optim.optimizer._new_local_optimizer(optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._new_script_local_optimizer(optim_cls,local_params_rref,*args,**kwargs)
torch.distributed.optim.optimizer._script_local_optimizer_step(local_optim_rref:RRef[_ScriptLocalOptimizerInterface],autograd_ctx_id:int)->None
torch.distributed.optim.optimizer._wait_for_all(rpc_futs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/post_localSGD_optimizer.py----------------------------------------
torch.distributed.optim.PostLocalSGDOptimizer(self,optim:torch.optim.Optimizer,averager:averagers.ModelAverager)
torch.distributed.optim.PostLocalSGDOptimizer.__repr__(self)
torch.distributed.optim.PostLocalSGDOptimizer.add_param_group(self,param_group)
torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict(self,state_dict)
torch.distributed.optim.PostLocalSGDOptimizer.state(self)
torch.distributed.optim.PostLocalSGDOptimizer.state_dict(self)
torch.distributed.optim.PostLocalSGDOptimizer.step(self)
torch.distributed.optim.PostLocalSGDOptimizer.zero_grad(self)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer(self,optim:torch.optim.Optimizer,averager:averagers.ModelAverager)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.__init__(self,optim:torch.optim.Optimizer,averager:averagers.ModelAverager)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.__repr__(self)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.add_param_group(self,param_group)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.load_state_dict(self,state_dict)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.state(self)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.state_dict(self)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.step(self)
torch.distributed.optim.post_localSGD_optimizer.PostLocalSGDOptimizer.zero_grad(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_adadelta.py----------------------------------------
A:torch.distributed.optim.functional_adadelta.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
A:torch.distributed.optim.functional_adadelta.state['step']->torch.tensor(0.0)
A:torch.distributed.optim.functional_adadelta.state['square_avg']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_adadelta.state['acc_delta']->torch.zeros_like(param, memory_format=torch.preserve_format)
torch.distributed.optim._FunctionalAdadelta(self,params:List[Tensor],lr:float=1.0,rho:float=0.9,eps:float=1e-06,weight_decay:float=0.0,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalAdadelta.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_adadelta._FunctionalAdadelta(self,params:List[Tensor],lr:float=1.0,rho:float=0.9,eps:float=1e-06,weight_decay:float=0.0,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adadelta._FunctionalAdadelta.__init__(self,params:List[Tensor],lr:float=1.0,rho:float=0.9,eps:float=1e-06,weight_decay:float=0.0,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adadelta._FunctionalAdadelta.step(self,gradients:List[Optional[Tensor]])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_adam.py----------------------------------------
A:torch.distributed.optim.functional_adam.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
A:torch.distributed.optim.functional_adam.state['step']->torch.tensor(0.0)
A:torch.distributed.optim.functional_adam.state['exp_avg']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_adam.state['exp_avg_sq']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_adam.state['max_exp_avg_sq']->torch.zeros_like(param, memory_format=torch.preserve_format)
torch.distributed.optim._FunctionalAdam(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.0,amsgrad:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalAdam.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim._FunctionalAdam.step_param(self,param:Tensor,grad:Optional[Tensor])
torch.distributed.optim.functional_adam._FunctionalAdam(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.0,amsgrad:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adam._FunctionalAdam.__init__(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.0,amsgrad:bool=False,maximize:bool=False,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adam._FunctionalAdam.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_adam._FunctionalAdam.step_param(self,param:Tensor,grad:Optional[Tensor])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_rprop.py----------------------------------------
A:torch.distributed.optim.functional_rprop.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
A:torch.distributed.optim.functional_rprop.state['step']->torch.tensor(0.0)
A:torch.distributed.optim.functional_rprop.state['prev']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_rprop.state['step_size']->torch.full_like(gradient, lr)
torch.distributed.optim._FunctionalRprop(self,params:List[Tensor],lr:float=0.01,etas:Tuple[float,float]=(0.5,1.2),step_sizes:Tuple[float,float]=(1e-06,50),_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalRprop.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_rprop._FunctionalRprop(self,params:List[Tensor],lr:float=0.01,etas:Tuple[float,float]=(0.5,1.2),step_sizes:Tuple[float,float]=(1e-06,50),_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_rprop._FunctionalRprop.__init__(self,params:List[Tensor],lr:float=0.01,etas:Tuple[float,float]=(0.5,1.2),step_sizes:Tuple[float,float]=(1e-06,50),_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_rprop._FunctionalRprop.step(self,gradients:List[Optional[Tensor]])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributed/optim/functional_adamax.py----------------------------------------
A:torch.distributed.optim.functional_adamax.self.state->torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})
A:torch.distributed.optim.functional_adamax.state['step']->torch.tensor(0.0)
A:torch.distributed.optim.functional_adamax.state['exp_avg']->torch.zeros_like(param, memory_format=torch.preserve_format)
A:torch.distributed.optim.functional_adamax.state['exp_inf']->torch.zeros_like(param, memory_format=torch.preserve_format)
torch.distributed.optim._FunctionalAdamax(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.0,_allow_empty_param_list:bool=False)
torch.distributed.optim._FunctionalAdamax.step(self,gradients:List[Optional[Tensor]])
torch.distributed.optim.functional_adamax._FunctionalAdamax(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.0,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adamax._FunctionalAdamax.__init__(self,params:List[Tensor],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-08,weight_decay:float=0.0,_allow_empty_param_list:bool=False)
torch.distributed.optim.functional_adamax._FunctionalAdamax.step(self,gradients:List[Optional[Tensor]])


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/quantization_mappings.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/_numeric_suite.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/utils.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fuse_modules.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/_numeric_suite_fx.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/observer.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/quantize_jit.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/qconfig.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fake_quantize.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fuser_method_mappings.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/quant_type.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/__init__.py----------------------------------------
torch.quantization.__init__.default_eval_fn(model,calib_data)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/stubs.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/quantize.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/quantize_fx.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/_equalize.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/quantization_types.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/utils.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/prepare.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/fuse.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/quantization_patterns.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/graph_module.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/match_utils.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/convert.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/pattern_utils.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/quantization/fx/fusion_patterns.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_cudnn.pyi----------------------------------------
torch._C._cudnn.RNNMode(int,Enum)
torch._C._cudnn.getCompileVersion()->Tuple[int, int, int]
torch._C._cudnn.getRuntimeVersion()->Tuple[int, int, int]
torch._C._cudnn.getVersionInt()->int


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_distributed_c10d.pyi----------------------------------------
torch._C._distributed_c10d.AllGatherOptions
torch._C._distributed_c10d.AllToAllOptions
torch._C._distributed_c10d.AllreduceCoalescedOptions(AllreduceOptions)
torch._C._distributed_c10d.AllreduceOptions
torch._C._distributed_c10d.BarrierOptions
torch._C._distributed_c10d.BroadcastOptions
torch._C._distributed_c10d.BuiltinCommHookType(Enum)
torch._C._distributed_c10d.DebugLevel(Enum)
torch._C._distributed_c10d.FileStore(self,path:str,numWorkers:int=...)
torch._C._distributed_c10d.FileStore.__init__(self,path:str,numWorkers:int=...)
torch._C._distributed_c10d.GatherOptions
torch._C._distributed_c10d.GradBucket
torch._C._distributed_c10d.GradBucket.buffer(self)->Tensor
torch._C._distributed_c10d.GradBucket.gradients(self)->List[Tensor]
torch._C._distributed_c10d.GradBucket.index(self)->int
torch._C._distributed_c10d.GradBucket.is_last(self)->bool
torch._C._distributed_c10d.GradBucket.parameters(self)->List[Tensor]
torch._C._distributed_c10d.GradBucket.set_buffer(self,tensor:Tensor)->None
torch._C._distributed_c10d.HashStore(self)
torch._C._distributed_c10d.HashStore.__init__(self)
torch._C._distributed_c10d.Logger(self,reducer:Reducer)
torch._C._distributed_c10d.Logger.__init__(self,reducer:Reducer)
torch._C._distributed_c10d.Logger.set_construction_data_and_log(self,module_name:str,device_ids:List[int],output_device:int,broadcast_buffers:bool,has_sync_bn:bool)
torch._C._distributed_c10d.PrefixStore(self,prefix:str,store:Store)
torch._C._distributed_c10d.PrefixStore.__init__(self,prefix:str,store:Store)
torch._C._distributed_c10d.ProcessGroup(self)
torch._C._distributed_c10d.ProcessGroup.Options
torch._C._distributed_c10d.ProcessGroup.__init__(self)
torch._C._distributed_c10d.ProcessGroup._allgather_base(self,output:Tensor,input:Tensor,opts=AllGatherOptions())->Work
torch._C._distributed_c10d.ProcessGroup._reduce_scatter_base(self,outputTensor:Tensor,inputTensor:Tensor)->Work
torch._C._distributed_c10d.ProcessGroup.allgather(self,output_tensors:List[List[Tensor]],input_tensors:List[Tensor],opts=AllGatherOptions())->Work
torch._C._distributed_c10d.ProcessGroup.allgather(self,output_tensors:List[Tensor],input_tensor:Tensor)->Work
torch._C._distributed_c10d.ProcessGroup.allgather_coalesced(self,output_lists:List[List[Tensor]],input_list:List[Tensor],opts=AllGatherOptions())->Work
torch._C._distributed_c10d.ProcessGroup.allreduce(self,tensor:Tensor,op=ReduceOp.SUM)->Work
torch._C._distributed_c10d.ProcessGroup.allreduce(self,tensors:List[Tensor],op=ReduceOp.SUM)->Work
torch._C._distributed_c10d.ProcessGroup.allreduce(self,tensors:List[Tensor],opts:AllreduceOptions=AllreduceOptions())->Work
torch._C._distributed_c10d.ProcessGroup.allreduce_coalesced(self,tensors:List[Tensor],opts=AllreduceCoalescedOptions())->Work
torch._C._distributed_c10d.ProcessGroup.alltoall(self,output:List[Tensor],input:List[Tensor])->Work
torch._C._distributed_c10d.ProcessGroup.alltoall(self,output_tensor:List[Tensor],input_tensor:List[Tensor],opts=AllToAllOptions())->Work
torch._C._distributed_c10d.ProcessGroup.alltoall_base(self,output:Tensor,input:Tensor,output_split_sizes:List[int],input_split_sizes:List[int])->Work
torch._C._distributed_c10d.ProcessGroup.alltoall_base(self,output_tensor:Tensor,input_tensor:Tensor,output_split_sizes:List[int],input_split_sizes:List[int],opts=AllToAllOptions())->Work
torch._C._distributed_c10d.ProcessGroup.barrier(self,opts=BarrierOptions())->Work
torch._C._distributed_c10d.ProcessGroup.broadcast(self,tensor:Tensor,root:int)->Work
torch._C._distributed_c10d.ProcessGroup.broadcast(self,tensors:List[Tensor],opts=BroadcastOptions())->Work
torch._C._distributed_c10d.ProcessGroup.gather(self,output_tensors:List[List[Tensor]],input_tensors:List[Tensor],opts=GatherOptions())->Work
torch._C._distributed_c10d.ProcessGroup.gather(self,output_tensors:List[Tensor],input_tensor:Tensor,root:int)->Work
torch._C._distributed_c10d.ProcessGroup.rank(self)->int
torch._C._distributed_c10d.ProcessGroup.recv(self,tensors:List[Tensor],srcRank:int,tag:int)->Work
torch._C._distributed_c10d.ProcessGroup.recv_anysource(self,tensors:List[Tensor],tag:int)->Work
torch._C._distributed_c10d.ProcessGroup.reduce(self,tensor:Tensor,root:int,op=ReduceOp.SUM)->Work
torch._C._distributed_c10d.ProcessGroup.reduce(self,tensors:List[Tensor],opts=ReduceOptions())->Work
torch._C._distributed_c10d.ProcessGroup.reduce_scatter(self,output_tensors:List[Tensor],input_tensors:List[List[Tensor]],opts=ReduceScatterOptions())->Work
torch._C._distributed_c10d.ProcessGroup.reduce_scatter(self,output_tensors:Tensor,input_tensor:List[Tensor])->Work
torch._C._distributed_c10d.ProcessGroup.scatter(self,output_tensor:Tensor,input_tensors:List[Tensor],root:int)->Work
torch._C._distributed_c10d.ProcessGroup.scatter(self,output_tensors:List[Tensor],input_tensors:List[List[Tensor]],opts=ScatterOptions())->Work
torch._C._distributed_c10d.ProcessGroup.send(self,tensors:List[Tensor],dstRank:int,tag:int)->Work
torch._C._distributed_c10d.ProcessGroup.size(self)->int
torch._C._distributed_c10d.ProcessGroupGloo(self,store:Store,rank:int,size:int,timeout:timedelta)
torch._C._distributed_c10d.ProcessGroupGloo.Device
torch._C._distributed_c10d.ProcessGroupGloo.Options
torch._C._distributed_c10d.ProcessGroupGloo.__init__(self,store:Store,rank:int,size:int,timeout:timedelta)
torch._C._distributed_c10d.ProcessGroupGloo.create_default_device()->Device
torch._C._distributed_c10d.ProcessGroupGloo.create_device(hostname=str(),interface=str())->Device
torch._C._distributed_c10d.ProcessGroupMPI(self,rank:int,size:int,pgComm:int)
torch._C._distributed_c10d.ProcessGroupMPI.__init__(self,rank:int,size:int,pgComm:int)
torch._C._distributed_c10d.ProcessGroupMPI.create(ranks:List[int])->ProcessGroupMPI
torch._C._distributed_c10d.ProcessGroupNCCL(self,store:Store,rank:int,size:int,timeout:timedelta)
torch._C._distributed_c10d.ProcessGroupNCCL.Options
torch._C._distributed_c10d.ProcessGroupNCCL.__init__(self,store:Store,rank:int,size:int,timeout:timedelta)
torch._C._distributed_c10d.ProcessGroupNCCL._group_end()->None
torch._C._distributed_c10d.ProcessGroupNCCL._group_start()->None
torch._C._distributed_c10d.ProcessGroupRoundRobin(ProcessGroup)
torch._C._distributed_c10d.ReduceOp(Enum)
torch._C._distributed_c10d.ReduceOptions
torch._C._distributed_c10d.ReduceScatterOptions
torch._C._distributed_c10d.Reducer(self,params:List[Tensor],bucket_indices:List[List[int]],process_group:ProcessGroup,expect_sparse_gradients:List[bool],bucket_bytes_cap:int,find_unused_parameters:bool,gradient_as_bucket_view:bool)
torch._C._distributed_c10d.Reducer.__init__(self,params:List[Tensor],bucket_indices:List[List[int]],process_group:ProcessGroup,expect_sparse_gradients:List[bool],bucket_bytes_cap:int,find_unused_parameters:bool,gradient_as_bucket_view:bool)
torch._C._distributed_c10d.ScatterOptions
torch._C._distributed_c10d.Store
torch._C._distributed_c10d.Store.add(self,key:str,value:int)->int
torch._C._distributed_c10d.Store.compare_set(self,key:str,expected_value:str,desired_value:str)->bytes
torch._C._distributed_c10d.Store.delete_key(self,key:str)->bool
torch._C._distributed_c10d.Store.get(self,key:str)->bytes
torch._C._distributed_c10d.Store.num_keys(self)->int
torch._C._distributed_c10d.Store.set(self,key:str,value:str)
torch._C._distributed_c10d.Store.set_timeout(self,timeout:timedelta)
torch._C._distributed_c10d.Store.wait(self,keys:List[str])
torch._C._distributed_c10d.Store.wait(self,keys:List[str],timeout:timedelta)
torch._C._distributed_c10d.TCPStore(self,host_name:str,port:int,world_size:int=...,is_master:bool=...,timeout:timedelta=...,wait_for_workers:bool=...,multi_tenant:bool=...)
torch._C._distributed_c10d.TCPStore.__init__(self,host_name:str,port:int,world_size:int=...,is_master:bool=...,timeout:timedelta=...,wait_for_workers:bool=...,multi_tenant:bool=...)
torch._C._distributed_c10d.Work
torch._C._distributed_c10d.Work._source_rank(self)->int
torch._C._distributed_c10d.Work.exception(self)->Any
torch._C._distributed_c10d.Work.is_completed(self)->bool
torch._C._distributed_c10d.Work.is_success(self)->bool
torch._C._distributed_c10d.Work.result(self)->List[Tensor]
torch._C._distributed_c10d.Work.source_rank(self)->int
torch._C._distributed_c10d.Work.synchronize(self)
torch._C._distributed_c10d.Work.wait(self,timeout:timedelta=_DEFAULT_NO_TIMEOUT)->bool
torch._C._distributed_c10d._ProcessGroupWrapper(self,pg:ProcessGroup,gloo_pg:ProcessGroupGloo)
torch._C._distributed_c10d._ProcessGroupWrapper.__init__(self,pg:ProcessGroup,gloo_pg:ProcessGroupGloo)
torch._C._distributed_c10d._broadcast_coalesced(process_group:ProcessGroup,tensors:List[Tensor],buffer_size:int,src:int)
torch._C._distributed_c10d._compute_bucket_assignment_by_size(tensors:List[Tensor],bucket_size:int,expect_sparse_gradient:List[bool],tensor_indices:List[int])->Tuple[List[List[int]], List[int]]
torch._C._distributed_c10d._register_builtin_comm_hook(reducer:Reducer,comm_hook_type:BuiltinCommHookType)
torch._C._distributed_c10d._register_comm_hook(reducer:Reducer,state:Any,comm_hook:Any)
torch._C._distributed_c10d._round_robin_process_groups(process_groups:List[ProcessGroup])->ProcessGroupRoundRobin
torch._C._distributed_c10d._test_python_store(store:Store)
torch._C._distributed_c10d._verify_params_across_processes(process_group:ProcessGroup,params:List[Tensor])
torch._C._distributed_c10d.get_debug_level()
torch._C._distributed_c10d.set_debug_level()
torch._C._distributed_c10d.set_debug_level_from_env()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_monitor.pyi----------------------------------------
torch._C._monitor.Aggregation(Enum)
torch._C._monitor.Event(self,name:str,timestamp:datetime.datetime,data:Dict[str,Union[int,float,bool,str]])
torch._C._monitor.Event.__init__(self,name:str,timestamp:datetime.datetime,data:Dict[str,Union[int,float,bool,str]])
torch._C._monitor.EventHandlerHandle
torch._C._monitor.Stat(self,name:str,aggregations:List[Aggregation],window_size:int,max_samples:int=-1)
torch._C._monitor.Stat.__init__(self,name:str,aggregations:List[Aggregation],window_size:int,max_samples:int=-1)
torch._C._monitor.Stat.add(self,v:float)->None
torch._C._monitor.Stat.get(self)->Dict[Aggregation, float]
torch._C._monitor.log_event(e:Event)->None
torch._C._monitor.register_event_handler(handler:Callable[[Event],None])->EventHandlerHandle
torch._C._monitor.unregister_event_handler(handle:EventHandlerHandle)->None
torch._monitor.Stat(self,name:str,aggregations:List[Aggregation],window_size:int,max_samples:int=-1)
torch._monitor.Stat.add(self,v:float)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_nn.pyi----------------------------------------
torch._C._nn._parse_to(device:_device,dtype:_dtype,non_blocking:_bool,copy:_bool,*,memory_format:memory_format)->Tuple[_device, _dtype, _bool, memory_format]
torch._C._nn._parse_to(dtype:_dtype,non_blocking:_bool,copy:_bool,*,memory_format:memory_format)->Tuple[_device, _dtype, _bool, memory_format]
torch._C._nn._parse_to(tensor:Tensor,non_blocking:_bool,copy:_bool,*,memory_format:memory_format)->Tuple[_device, _dtype, _bool, memory_format]
torch._C._nn.flatten_dense_tensors(tensors:List[Tensor])->Tensor
torch._C._nn.mkldnn_linear(input:Tensor,weight:Tensor,bias:Optional[Tensor])->Tensor
torch._C._nn.mkldnn_reorder_conv2d_weight(self:Tensor,padding:List,stride:List,dilatation:List,groups:int)->Tensor
torch._C._nn.mkldnn_reorder_conv3d_weight(self:Tensor,padding:List,stride:List,dilatation:List,groups:int)->Tensor
torch._C._nn.pad_sequence(sequences:List[Tensor],batch_first:bool=False,padding_value:float=...)->Tensor
torch._C._nn.unflatten_dense_tensors(flat:Tensor,tensors:List[Tensor])->List[Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_distributed_autograd.pyi----------------------------------------
torch._C._distributed_autograd.DistAutogradContext
torch._C._distributed_autograd.DistAutogradContext._context_id(self)->int
torch._C._distributed_autograd.DistAutogradContext._known_worker_ids(self)->Set[int]
torch._C._distributed_autograd.DistAutogradContext._recv_functions(self)->Dict[int, Any]
torch._C._distributed_autograd.DistAutogradContext._send_functions(self)->Dict[int, Any]
torch._C._distributed_autograd._current_context()->DistAutogradContext
torch._C._distributed_autograd._get_debug_info()->Dict[str, str]
torch._C._distributed_autograd._get_max_id()->int
torch._C._distributed_autograd._init(worker_id:int)->None
torch._C._distributed_autograd._is_valid_context(worker_id:int)->bool
torch._C._distributed_autograd._new_context()->DistAutogradContext
torch._C._distributed_autograd._release_context(context_id:int)->None
torch._C._distributed_autograd._retrieve_context(context_id:int)->DistAutogradContext
torch._C._distributed_autograd.backward(context_id:int,roots:List[torch.Tensor],retain_graph=False)->None
torch._C._distributed_autograd.get_gradients(context_id:int)->Dict[torch.Tensor, torch.Tensor]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_distributed_rpc.pyi----------------------------------------
torch._C._distributed_rpc.PyRRef(self,value:Any,type_hint:Any=None)
torch._C._distributed_rpc.PyRRef.__init__(self,value:Any,type_hint:Any=None)
torch._C._distributed_rpc.PyRRef.__repr__(self)->str
torch._C._distributed_rpc.PyRRef._deserialize(tp:Tuple)->'PyRRef'
torch._C._distributed_rpc.PyRRef._get_future(self)->Future
torch._C._distributed_rpc.PyRRef._get_profiling_future(self)->Future
torch._C._distributed_rpc.PyRRef._get_type(self)->Any
torch._C._distributed_rpc.PyRRef._serialize(self)->Tuple
torch._C._distributed_rpc.PyRRef._set_profiling_future(self,profilingFuture:Future)
torch._C._distributed_rpc.PyRRef.confirmed_by_owner(self)->bool
torch._C._distributed_rpc.PyRRef.is_owner(self)->bool
torch._C._distributed_rpc.PyRRef.local_value(self)->Any
torch._C._distributed_rpc.PyRRef.owner(self)->WorkerInfo
torch._C._distributed_rpc.PyRRef.owner_name(self)->str
torch._C._distributed_rpc.PyRRef.remote(self,timeout:float=_UNSET_RPC_TIMEOUT)->Any
torch._C._distributed_rpc.PyRRef.rpc_async(self,timeout:float=_UNSET_RPC_TIMEOUT)->Any
torch._C._distributed_rpc.PyRRef.rpc_sync(self,timeout:float=_UNSET_RPC_TIMEOUT)->Any
torch._C._distributed_rpc.PyRRef.to_here(self,timeout:float=_UNSET_RPC_TIMEOUT)->Any
torch._C._distributed_rpc.RemoteProfilerManager
torch._C._distributed_rpc.RemoteProfilerManager.set_current_profiling_key(key:str)
torch._C._distributed_rpc.RpcAgent
torch._C._distributed_rpc.RpcAgent._get_device_map(self,dst:WorkerInfo)->Dict[torch.device, torch.device]
torch._C._distributed_rpc.RpcAgent.get_debug_info(self)->Dict[str, str]
torch._C._distributed_rpc.RpcAgent.get_metrics(self)->Dict[str, str]
torch._C._distributed_rpc.RpcAgent.get_worker_info(self)->WorkerInfo
torch._C._distributed_rpc.RpcAgent.get_worker_info(self,workerName:str)->WorkerInfo
torch._C._distributed_rpc.RpcAgent.get_worker_infos(self)->List[WorkerInfo]
torch._C._distributed_rpc.RpcAgent.join(self,shutdown:bool=False)
torch._C._distributed_rpc.RpcAgent.shutdown(self)
torch._C._distributed_rpc.RpcAgent.sync(self)
torch._C._distributed_rpc.RpcBackendOptions(self,rpc_timeout:float=_DEFAULT_RPC_TIMEOUT_SEC,init_method:str=_DEFAULT_INIT_METHOD)
torch._C._distributed_rpc.RpcBackendOptions.__init__(self,rpc_timeout:float=_DEFAULT_RPC_TIMEOUT_SEC,init_method:str=_DEFAULT_INIT_METHOD)
torch._C._distributed_rpc.TensorPipeAgent(self,store:Store,name:str,worker_id:int,world_size:int,opts:_TensorPipeRpcBackendOptionsBase,reverse_device_maps:Dict[str,Dict[torch.device,torch.device]],devices:List[torch.device])
torch._C._distributed_rpc.TensorPipeAgent.__init__(self,store:Store,name:str,worker_id:int,world_size:int,opts:_TensorPipeRpcBackendOptionsBase,reverse_device_maps:Dict[str,Dict[torch.device,torch.device]],devices:List[torch.device])
torch._C._distributed_rpc.TensorPipeAgent._get_device_map(self,dst:WorkerInfo)->Dict[torch.device, torch.device]
torch._C._distributed_rpc.TensorPipeAgent.get_worker_info(self)->WorkerInfo
torch._C._distributed_rpc.TensorPipeAgent.get_worker_info(self,id:int)->WorkerInfo
torch._C._distributed_rpc.TensorPipeAgent.get_worker_info(self,workerName:str)->WorkerInfo
torch._C._distributed_rpc.TensorPipeAgent.get_worker_infos(self)->List[WorkerInfo]
torch._C._distributed_rpc.TensorPipeAgent.join(self)
torch._C._distributed_rpc.TensorPipeAgent.shutdown(self)
torch._C._distributed_rpc.WorkerInfo(self,name:str,worker_id:int)
torch._C._distributed_rpc.WorkerInfo.__eq__(self,other:object)->bool
torch._C._distributed_rpc.WorkerInfo.__init__(self,name:str,worker_id:int)
torch._C._distributed_rpc.WorkerInfo.__repr__(self)->str
torch._C._distributed_rpc.WorkerInfo.id(self)->int
torch._C._distributed_rpc.WorkerInfo.name(self)->str
torch._C._distributed_rpc._TensorPipeRpcBackendOptionsBase(self,num_worker_threads:int,_transports:Optional[List],_channels:Optional[List],rpc_timeout:float=_DEFAULT_RPC_TIMEOUT_SEC,init_method:str=_DEFAULT_INIT_METHOD,device_maps:Dict[str,Dict[torch.device,torch.device]]=dict(),devices:List[torch.device]=list())
torch._C._distributed_rpc._TensorPipeRpcBackendOptionsBase.__init__(self,num_worker_threads:int,_transports:Optional[List],_channels:Optional[List],rpc_timeout:float=_DEFAULT_RPC_TIMEOUT_SEC,init_method:str=_DEFAULT_INIT_METHOD,device_maps:Dict[str,Dict[torch.device,torch.device]]=dict(),devices:List[torch.device]=list())
torch._C._distributed_rpc._TensorPipeRpcBackendOptionsBase._set_device_map(self,to:str,device_map:Dict[torch.device,torch.device])
torch._C._distributed_rpc._cleanup_python_rpc_handler()
torch._C._distributed_rpc._delete_all_user_and_unforked_owner_rrefs(timeout:timedelta=...)
torch._C._distributed_rpc._destroy_rref_context(ignoreRRefLeak:bool)
torch._C._distributed_rpc._disable_jit_rref_pickle()
torch._C._distributed_rpc._disable_server_process_global_profiler()->List[List[List[ProfilerEvent]]]
torch._C._distributed_rpc._enable_jit_rref_pickle()
torch._C._distributed_rpc._enable_server_process_global_profiler(new_config:ProfilerConfig)
torch._C._distributed_rpc._get_current_rpc_agent()->RpcAgent
torch._C._distributed_rpc._invoke_remote_builtin(dst:WorkerInfo,opName:str,rpcTimeoutSeconds:float,*args:Any,**kwargs:Any)
torch._C._distributed_rpc._invoke_remote_python_udf(dst:WorkerInfo,pickledPythonUDF:str,tensors:List[torch.Tensor],rpcTimeoutSeconds:float,isAsyncExecution:bool)
torch._C._distributed_rpc._invoke_remote_torchscript(dstWorkerName:WorkerInfo,qualifiedNameStr:str,rpcTimeoutSeconds:float,isAsyncExecution:bool,*args:Any,**kwargs:Any)
torch._C._distributed_rpc._invoke_rpc_builtin(dst:WorkerInfo,opName:str,rpcTimeoutSeconds:float,*args:Any,**kwargs:Any)
torch._C._distributed_rpc._invoke_rpc_python_udf(dst:WorkerInfo,pickledPythonUDF:str,tensors:List[torch.Tensor],rpcTimeoutSeconds:float,isAsyncExecution:bool)
torch._C._distributed_rpc._invoke_rpc_torchscript(dstWorkerName:str,qualifiedNameStr:str,argsTuple:Tuple,kwargsDict:Dict,rpcTimeoutSeconds:float,isAsyncExecution:bool)
torch._C._distributed_rpc._is_current_rpc_agent_set()->bool
torch._C._distributed_rpc._reset_current_rpc_agent()
torch._C._distributed_rpc._rref_context_get_debug_info()->Dict[str, str]
torch._C._distributed_rpc._set_and_start_rpc_agent(agent:RpcAgent)
torch._C._distributed_rpc._set_profiler_node_id(default_node_id:int)
torch._C._distributed_rpc._set_rpc_timeout(rpcTimeoutSeconds:float)
torch._C._distributed_rpc.enable_gil_profiling(flag:bool)
torch._C._distributed_rpc.get_rpc_timeout()->float


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/__init__.pyi----------------------------------------
torch._C.__init__.AggregationType(Enum)
torch._C.__init__.AliasDb
torch._C.__init__.AliasDb.__str__(self)->str
torch._C.__init__.AnyType(JitType)
torch._C.__init__.AnyType.get()->AnyType
torch._C.__init__.Argument
torch._C.__init__.Argument.has_default_value(self)->_bool
torch._C.__init__.BFloat16StorageBase(object)
torch._C.__init__.BenchmarkConfig(object)
torch._C.__init__.BenchmarkExecutionStats(object)
torch._C.__init__.Block
torch._C.__init__.BoolStorageBase(object)
torch._C.__init__.BoolTensor(Tensor)
torch._C.__init__.BoolType(JitType)
torch._C.__init__.BoolType.get()->BoolType
torch._C.__init__.BufferDict(self,mod:ScriptModule)
torch._C.__init__.BufferDict.__init__(self,mod:ScriptModule)
torch._C.__init__.ByteStorageBase(object)
torch._C.__init__.ByteTensor(Tensor)
torch._C.__init__.CallStack(self,name:str,range:SourceRange)
torch._C.__init__.CallStack.__init__(self,name:str,range:SourceRange)
torch._C.__init__.CharStorageBase(object)
torch._C.__init__.CharTensor(Tensor)
torch._C.__init__.ClassDef(TreeView)
torch._C.__init__.ClassType(self,qualified_name:str)
torch._C.__init__.ClassType.__init__(self,qualified_name:str)
torch._C.__init__.CompilationUnit(self,lang:str=...,_frames_up:_int=...)
torch._C.__init__.CompilationUnit.__getattr__(self,name:str)->ScriptFunction
torch._C.__init__.CompilationUnit.__init__(self,lang:str=...,_frames_up:_int=...)
torch._C.__init__.CompilationUnit.create_function(self,name:str,graph:Graph,shouldMangle:_bool=...)->ScriptFunction
torch._C.__init__.CompilationUnit.define(self,script:str,rcb:ResolutionCallback=...,_frames_up:_int=...)
torch._C.__init__.CompilationUnit.find_function(self,name:str)->ScriptFunction
torch._C.__init__.CompilationUnit.get_class(self,name:str)->ClassType
torch._C.__init__.CompilationUnit.get_functions(self)->List[ScriptFunction]
torch._C.__init__.CompilationUnit.get_interface(self,name:str)->InterfaceType
torch._C.__init__.ComplexDoubleStorageBase(object)
torch._C.__init__.ComplexFloatStorageBase(object)
torch._C.__init__.ComplexType(JitType)
torch._C.__init__.ComplexType.get()->ComplexType
torch._C.__init__.ConcreteModuleType
torch._C.__init__.ConcreteModuleType.equals(self,other:'ConcreteModuleType')->_bool
torch._C.__init__.ConcreteModuleType.from_jit_type(ty:JitType)->ConcreteModuleType
torch._C.__init__.ConcreteModuleType.get_constants(self)->Dict[str, Any]
torch._C.__init__.ConcreteModuleTypeBuilder(self,obj:Any)
torch._C.__init__.ConcreteModuleTypeBuilder.__init__(self,obj:Any)
torch._C.__init__.ConcreteModuleTypeBuilder.add_attribute(self,name:str,ty:JitType,is_param:_bool,is_buffer:_bool)
torch._C.__init__.ConcreteModuleTypeBuilder.add_builtin_function(self,name:str,symbol_name:str)
torch._C.__init__.ConcreteModuleTypeBuilder.add_constant(self,name:str,value:Any)
torch._C.__init__.ConcreteModuleTypeBuilder.add_failed_attribute(self,name:str,failure_reason:str)
torch._C.__init__.ConcreteModuleTypeBuilder.add_forward_hook(self,hook:Callable[...,Any])
torch._C.__init__.ConcreteModuleTypeBuilder.add_forward_pre_hook(self,pre_hook:Callable[...,Any])
torch._C.__init__.ConcreteModuleTypeBuilder.add_function_attribute(self,name:str,ty:JitType,func:Callable[...,Any])
torch._C.__init__.ConcreteModuleTypeBuilder.add_ignored_attribute(self,name:str)
torch._C.__init__.ConcreteModuleTypeBuilder.add_ignored_attributes(self,names:List[str])
torch._C.__init__.ConcreteModuleTypeBuilder.add_module(self,name:str,meta:ConcreteModuleType)
torch._C.__init__.ConcreteModuleTypeBuilder.add_overload(self,method_name:str,overloaded_method_names:List[str])
torch._C.__init__.ConcreteModuleTypeBuilder.set_module_dict(self)
torch._C.__init__.ConcreteModuleTypeBuilder.set_module_list(self)
torch._C.__init__.CudaBFloat16StorageBase(object)
torch._C.__init__.CudaBoolStorageBase(object)
torch._C.__init__.CudaByteStorageBase(object)
torch._C.__init__.CudaCharStorageBase(object)
torch._C.__init__.CudaComplexDoubleStorageBase(object)
torch._C.__init__.CudaComplexFloatStorageBase(object)
torch._C.__init__.CudaDoubleStorageBase(object)
torch._C.__init__.CudaFloatStorageBase(object)
torch._C.__init__.CudaHalfStorageBase(object)
torch._C.__init__.CudaIntStorageBase(object)
torch._C.__init__.CudaLongStorageBase(object)
torch._C.__init__.CudaQInt32StorageBase(object)
torch._C.__init__.CudaQInt8StorageBase(object)
torch._C.__init__.CudaQUInt2x4StorageBase(object)
torch._C.__init__.CudaQUInt4x2StorageBase(object)
torch._C.__init__.CudaQUInt8StorageBase(object)
torch._C.__init__.CudaShortStorageBase(object)
torch._C.__init__.Decl(TreeView)
torch._C.__init__.Def(TreeView)
torch._C.__init__.Def.name(self)->Ident
torch._C.__init__.DeserializationStorageContext(self)
torch._C.__init__.DeserializationStorageContext.__init__(self)
torch._C.__init__.DeserializationStorageContext.add_storage(self,name:str,tensor:Tensor)->_int
torch._C.__init__.DeserializationStorageContext.get_storage(self,name:str,dtype:_dtype)->Tensor
torch._C.__init__.DeserializationStorageContext.has_storage(self,name:str)->_bool
torch._C.__init__.DeviceObjType(JitType)
torch._C.__init__.DeviceObjType.get()->DeviceObjType
torch._C.__init__.DictType(self,key:JitType,value:JitType)
torch._C.__init__.DictType.__init__(self,key:JitType,value:JitType)
torch._C.__init__.DictType.getKeyType(self)->JitType
torch._C.__init__.DictType.getValueType(self)->JitType
torch._C.__init__.DisableTorchFunction()
torch._C.__init__.DoubleStorageBase(object)
torch._C.__init__.DoubleTensor(Tensor)
torch._C.__init__.EnumType(self,qualified_name:str,value_type:JitType,enum_names_values:List[Any])
torch._C.__init__.EnumType.__init__(self,qualified_name:str,value_type:JitType,enum_names_values:List[Any])
torch._C.__init__.ErrorReport(self,range:SourceRange)
torch._C.__init__.ErrorReport.__init__(self,range:SourceRange)
torch._C.__init__.ErrorReport.call_stack()->str
torch._C.__init__.ErrorReport.what(self)->str
torch._C.__init__.FileCheck(object)
torch._C.__init__.FileCheck.check_source_highlighted(self,highlight:str)->'FileCheck'
torch._C.__init__.FileCheck.run(self,test_string:str)->None
torch._C.__init__.FloatStorageBase(object)
torch._C.__init__.FloatTensor(Tensor)
torch._C.__init__.FloatType(JitType)
torch._C.__init__.FloatType.get()->FloatType
torch._C.__init__.FunctionSchema
torch._C.__init__.Future(self,devices:List[device])
torch._C.__init__.Future.__init__(self,devices:List[device])
torch._C.__init__.Future._set_unwrap_func(self,callback:Callable)->None
torch._C.__init__.Future.add_done_callback(self,callback:Callable)->None
torch._C.__init__.Future.done(self)->_bool
torch._C.__init__.Future.set_result(self,result:Any)->None
torch._C.__init__.Future.then(self,callback:Callable)->Future
torch._C.__init__.Future.value(self)->Any
torch._C.__init__.Future.wait(self)->Any
torch._C.__init__.FutureType(self,a:JitType)
torch._C.__init__.FutureType.__init__(self,a:JitType)
torch._C.__init__.FutureType.getElementType(self)->JitType
torch._C.__init__.Generator(self,device:Union[_device,str,None]=None)
torch._C.__init__.Generator.__init__(self,device:Union[_device,str,None]=None)
torch._C.__init__.Generator.get_state(self)->Tensor
torch._C.__init__.Generator.initial_seed(self)->_int
torch._C.__init__.Generator.manual_seed(self,seed:_int)->Generator
torch._C.__init__.Generator.seed(self)->_int
torch._C.__init__.Generator.set_state(self,_new_state:Tensor)->Generator
torch._C.__init__.Graph
torch._C.__init__.Graph.alias_db(self)->AliasDb
torch._C.__init__.Graph.eraseInput(self,i:_int)->None
torch._C.__init__.Graph.inputs(self)->List[Value]
torch._C.__init__.GraphExecutorState
torch._C.__init__.HalfStorageBase(object)
torch._C.__init__.HalfTensor(Tensor)
torch._C.__init__.IODescriptor
torch._C.__init__.IValue
torch._C.__init__.Ident(TreeView)
torch._C.__init__.Ident.name(self)->str
torch._C.__init__.InferredType(self,arg:Union[JitType,str])
torch._C.__init__.InferredType.__init__(self,arg:Union[JitType,str])
torch._C.__init__.InferredType.reason(self)->str
torch._C.__init__.InferredType.success(self)->_bool
torch._C.__init__.InferredType.type(self)->JitType
torch._C.__init__.IntStorageBase(object)
torch._C.__init__.IntTensor(Tensor)
torch._C.__init__.IntType(JitType)
torch._C.__init__.IntType.get()->IntType
torch._C.__init__.InterfaceType(self,qualified_name:str)
torch._C.__init__.InterfaceType.__init__(self,qualified_name:str)
torch._C.__init__.InterfaceType.getMethod(self,name:str)->Optional[FunctionSchema]
torch._C.__init__.InterfaceType.getMethodNames(self)->List[str]
torch._C.__init__.JITException
torch._C.__init__.JitType
torch._C.__init__.JitType.isSubtypeOf(self,other:JitType)->_bool
torch._C.__init__.JitType.with_dtype(self,dtype:_dtype)->JitType
torch._C.__init__.JitType.with_sizes(self,sizes:List[Optional[_int]])->JitType
torch._C.__init__.ListType(self,a:JitType)
torch._C.__init__.ListType.__init__(self,a:JitType)
torch._C.__init__.ListType.getElementType(self)->JitType
torch._C.__init__.ListType.ofBools()->ListType
torch._C.__init__.ListType.ofComplexDoubles()->ListType
torch._C.__init__.ListType.ofFloats()->ListType
torch._C.__init__.ListType.ofInts()->ListType
torch._C.__init__.ListType.ofTensors()->ListType
torch._C.__init__.LiteScriptModule(self,*input)
torch._C.__init__.LiteScriptModule.__call__(self,*input)
torch._C.__init__.LiteScriptModule.find_method(self,method_name:str)
torch._C.__init__.LiteScriptModule.forward(self,*input)->List[str]
torch._C.__init__.LiteScriptModule.run_method(self,method_name:str,*input)
torch._C.__init__.LockingLogger(LoggerBase)
torch._C.__init__.LoggerBase(object)
torch._C.__init__.LongStorageBase(object)
torch._C.__init__.LongTensor(Tensor)
torch._C.__init__.MobileOptimizerType
torch._C.__init__.Module
torch._C.__init__.ModuleDict(self,mod:ScriptModule)
torch._C.__init__.ModuleDict.__init__(self,mod:ScriptModule)
torch._C.__init__.ModuleDict.items(self)->List[Tuple[str, Any]]
torch._C.__init__.Node
torch._C.__init__.NoneType(JitType)
torch._C.__init__.NoneType.get()->NoneType
torch._C.__init__.NoopLogger(LoggerBase)
torch._C.__init__.NumberType(JitType)
torch._C.__init__.NumberType.get()->NumberType
torch._C.__init__.OptionalType(self,a:JitType)
torch._C.__init__.OptionalType.__init__(self,a:JitType)
torch._C.__init__.OptionalType.getElementType(self)->JitType
torch._C.__init__.OptionalType.ofTensor()->OptionalType
torch._C.__init__.ParameterDict(self,mod:ScriptModule)
torch._C.__init__.ParameterDict.__init__(self,mod:ScriptModule)
torch._C.__init__.PyTorchFileReader(self,buffer:BinaryIO)
torch._C.__init__.PyTorchFileReader.__init__(self,buffer:BinaryIO)
torch._C.__init__.PyTorchFileReader.get_record(self,name:str)->bytes
torch._C.__init__.PyTorchFileWriter(self,buffer:BinaryIO)
torch._C.__init__.PyTorchFileWriter.__init__(self,buffer:BinaryIO)
torch._C.__init__.PyTorchFileWriter.archive_name(self)->str
torch._C.__init__.PyTorchFileWriter.get_all_written_records(self)->List[str]
torch._C.__init__.PyTorchFileWriter.set_min_version(self,version:_int)->None
torch._C.__init__.PyTorchFileWriter.write_end_of_file(self)->None
torch._C.__init__.PyTorchFileWriter.write_record(self,name:str,data:Union[bytes,_int],size:_int)->None
torch._C.__init__.QInt32StorageBase(object)
torch._C.__init__.QInt8StorageBase(object)
torch._C.__init__.QUInt2x4StorageBase(object)
torch._C.__init__.QUInt4x2StorageBase(object)
torch._C.__init__.QUInt8StorageBase(object)
torch._C.__init__.RRefType(self,a:JitType)
torch._C.__init__.RRefType.__init__(self,a:JitType)
torch._C.__init__.ScriptFunction(self,*args,**kwargs)
torch._C.__init__.ScriptFunction.__call__(self,*args,**kwargs)
torch._C.__init__.ScriptFunction.code(self)->str
torch._C.__init__.ScriptFunction.graph(self)->Graph
torch._C.__init__.ScriptFunction.inlined_graph(self)->Graph
torch._C.__init__.ScriptFunction.name(self)->str
torch._C.__init__.ScriptFunction.qualified_name(self)->str
torch._C.__init__.ScriptFunction.save(self,filename:str,_extra_files:Dict[str,bytes])->None
torch._C.__init__.ScriptFunction.save_to_buffer(self,_extra_files:Dict[str,bytes])->bytes
torch._C.__init__.ScriptFunction.schema(self)->FunctionSchema
torch._C.__init__.ScriptMethod
torch._C.__init__.ScriptMethod.name(self)->str
torch._C.__init__.ScriptMethod.owner(self)->ScriptModule
torch._C.__init__.ScriptModule(ScriptObject)
torch._C.__init__.ScriptModule._get_method(self,name:str)->ScriptMethod
torch._C.__init__.ScriptModule._method_names(self)->List[str]
torch._C.__init__.ScriptModuleSerializer(self,export_writer:PyTorchFileWriter)
torch._C.__init__.ScriptModuleSerializer.__init__(self,export_writer:PyTorchFileWriter)
torch._C.__init__.ScriptModuleSerializer.serialize(self,model:ScriptModule,script_module_id:_int)->None
torch._C.__init__.ScriptModuleSerializer.storage_context(self)->SerializationStorageContext
torch._C.__init__.ScriptModuleSerializer.write_files(self)->None
torch._C.__init__.ScriptObject
torch._C.__init__.ScriptObject.setattr(self,name:str,value:Any)
torch._C.__init__.SerializationStorageContext(self)
torch._C.__init__.SerializationStorageContext.__init__(self)
torch._C.__init__.SerializationStorageContext.get_or_add_storage(self,storage:Storage)->_int
torch._C.__init__.SerializationStorageContext.has_storage(self,storage:Storage)->_bool
torch._C.__init__.ShortStorageBase(object)
torch._C.__init__.ShortTensor(Tensor)
torch._C.__init__.Size(Tuple[_int,...])
torch._C.__init__.Size.__getitem__(self:Size,key:_int)->_int
torch._C.__init__.Size.__getitem__(self:Size,key:slice)->Size
torch._C.__init__.Size.numel(self:Size)->_int
torch._C.__init__.SourceRange
torch._C.__init__.Stream
torch._C.__init__.StreamObjType(JitType)
torch._C.__init__.StreamObjType.get()->StreamObjType
torch._C.__init__.StringType(JitType)
torch._C.__init__.StringType.get()->StringType
torch._C.__init__.TensorType(JitType)
torch._C.__init__.TensorType.create_from_tensor(t:Tensor)->TensorType
torch._C.__init__.TensorType.get(cls)->TensorType
torch._C.__init__.TensorType.getInferred(cls)->TensorType
torch._C.__init__.TensorType.sizes(self)->Optional[List[_int]]
torch._C.__init__.TensorType.with_sizes(self,other:Optional[List[Optional[_int]]])->TensorType
torch._C.__init__.ThroughputBenchmark(self,module:Any)
torch._C.__init__.ThroughputBenchmark.__init__(self,module:Any)
torch._C.__init__.ThroughputBenchmark.add_input(self,*args:Any,**kwargs:Any)->None
torch._C.__init__.ThroughputBenchmark.benchmark(self,config:BenchmarkConfig)->BenchmarkExecutionStats
torch._C.__init__.ThroughputBenchmark.run_once(self,*args:Any,**kwargs:Any)->Any
torch._C.__init__.TracingState
torch._C.__init__.TracingState.current_scope(self)->str
torch._C.__init__.TracingState.graph(self)->Graph
torch._C.__init__.TracingState.pop_scope(self)->None
torch._C.__init__.TracingState.push_scope(self,scope_name:str)->None
torch._C.__init__.TracingState.set_graph(self,graph:Graph)->None
torch._C.__init__.TreeView
torch._C.__init__.TupleType(self,a:List[Optional[JitType]])
torch._C.__init__.TupleType.__init__(self,a:List[Optional[JitType]])
torch._C.__init__.TupleType.elements(self)->List[JitType]
torch._C.__init__.UnionType(self,a:List[JitType])
torch._C.__init__.UnionType.__init__(self,a:List[JitType])
torch._C.__init__.Value
torch._C.__init__.Value.debugName(self)->str
torch._C.__init__.Value.setType(self,t:JitType)->Value
torch._C.__init__.Value.type(self)->JitType
torch._C.__init__._CUDAGraph
torch._C.__init__._CUDAGraph.capture_begin(self,pool:Optional[Tuple[_int,_int]]=...)->None
torch._C.__init__._CUDAGraph.capture_end(self)->None
torch._C.__init__._CUDAGraph.pool(self)->Tuple[_int, _int]
torch._C.__init__._CUDAGraph.replay(self)->None
torch._C.__init__._CUDAGraph.reset(self)->None
torch._C.__init__._CudaDeviceProperties
torch._C.__init__._CudaEventBase(cls,enable_timing:_bool=False,blocking:_bool=False,interprocess:_bool=False)
torch._C.__init__._CudaEventBase.__new__(cls,enable_timing:_bool=False,blocking:_bool=False,interprocess:_bool=False)
torch._C.__init__._CudaEventBase.elapsed_time(self,other:_CudaEventBase)->_float
torch._C.__init__._CudaEventBase.from_ipc_handle(cls,device:_device,ipc_handle:bytes)->_CudaEventBase
torch._C.__init__._CudaEventBase.ipc_handle(self)->bytes
torch._C.__init__._CudaEventBase.query(self)->_bool
torch._C.__init__._CudaEventBase.record(self,stream:_CudaStreamBase)->None
torch._C.__init__._CudaEventBase.synchronize(self)->None
torch._C.__init__._CudaEventBase.wait(self,stream:_CudaStreamBase)->None
torch._C.__init__._CudaStreamBase(self,priority:_int=0,_cdata:_int=0,stream_ptr:_int=0)
torch._C.__init__._CudaStreamBase.__new__(self,priority:_int=0,_cdata:_int=0,stream_ptr:_int=0)
torch._C.__init__._CudaStreamBase.priority_range(self)->Tuple[_int, _int]
torch._C.__init__._CudaStreamBase.query(self)->_bool
torch._C.__init__._CudaStreamBase.synchronize(self)->None
torch._C.__init__._FunctionBase(object)
torch._C.__init__._ImperativeEngine
torch._C.__init__._InferenceMode(self,mode:_bool)
torch._C.__init__._InferenceMode.__init__(self,mode:_bool)
torch._C.__init__._LegacyVariableBase(self,data:Optional[Tensor]=...,requires_grad:Optional[_bool]=...,volatile:Optional[_bool]=...,_grad_fn:Optional[_FunctionBase]=...)
torch._C.__init__._LegacyVariableBase.__init__(self,data:Optional[Tensor]=...,requires_grad:Optional[_bool]=...,volatile:Optional[_bool]=...,_grad_fn:Optional[_FunctionBase]=...)
torch._C.__init__._LinalgBackend
torch._C.__init__._TensorBase(self,size:_size,*,device:Union[_device,str,None]=None)
torch._C.__init__._TensorBase.__abs__(self)->Tensor
torch._C.__init__._TensorBase.__add__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__and__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__and__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__and__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__bool__(self)->builtins.bool
torch._C.__init__._TensorBase.__complex__(self)->builtins.complex
torch._C.__init__._TensorBase.__div__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__eq__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__float__(self)->builtins.float
torch._C.__init__._TensorBase.__floordiv__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ge__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__getitem__(self,indices:Union[None,_int,slice,Tensor,List,Tuple])->Tensor
torch._C.__init__._TensorBase.__gt__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__iadd__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__iand__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__iand__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__iand__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__idiv__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ifloordiv__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ilshift__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ilshift__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__ilshift__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__imod__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__imul__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__index__(self)->builtins.int
torch._C.__init__._TensorBase.__init__(self,size:_size,*,device:Union[_device,str,None]=None)
torch._C.__init__._TensorBase.__int__(self)->builtins.int
torch._C.__init__._TensorBase.__invert__(self)->Tensor
torch._C.__init__._TensorBase.__ior__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ior__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__ior__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__irshift__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__irshift__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__irshift__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__isub__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ixor__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ixor__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__ixor__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__le__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__long__(self)->builtins.int
torch._C.__init__._TensorBase.__lshift__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__lshift__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__lshift__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__lt__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__matmul__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__mod__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__mul__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ne__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__neg__(self)->Tensor
torch._C.__init__._TensorBase.__nonzero__(self)->builtins.bool
torch._C.__init__._TensorBase.__or__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__or__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__or__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__pow__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__radd__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rand__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rfloordiv__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rmul__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__ror__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rpow__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rshift__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rshift__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__rshift__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.__rsub__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rtruediv__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__rxor__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__setitem__(self,indices:Union[None,_int,slice,Tensor,List,Tuple],val:Union[Tensor,Number])->None
torch._C.__init__._TensorBase.__sub__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__truediv__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__xor__(self,other:Any)->Tensor
torch._C.__init__._TensorBase.__xor__(self,other:Number)->Tensor
torch._C.__init__._TensorBase.__xor__(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase._autocast_to_full_precision(self,cuda_enabled:_bool,cpu_enabled:_bool)->Tensor
torch._C.__init__._TensorBase._autocast_to_reduced_precision(self,cuda_enabled:_bool,cpu_enabled:_bool,cuda_dtype:_dtype,cpu_dtype:_dtype)->Tensor
torch._C.__init__._TensorBase._coalesced_(self,coalesced:_bool)->Tensor
torch._C.__init__._TensorBase._conj(self)->Tensor
torch._C.__init__._TensorBase._conj_physical(self)->Tensor
torch._C.__init__._TensorBase._dimI(self)->_int
torch._C.__init__._TensorBase._dimV(self)->_int
torch._C.__init__._TensorBase._indices(self)->Tensor
torch._C.__init__._TensorBase._is_view(self)->_bool
torch._C.__init__._TensorBase._is_zerotensor(self)->_bool
torch._C.__init__._TensorBase._make_subclass(cls,data:Tensor,require_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase._neg_view(self)->Tensor
torch._C.__init__._TensorBase._nnz(self)->_int
torch._C.__init__._TensorBase._storage(self)->Storage
torch._C.__init__._TensorBase._values(self)->Tensor
torch._C.__init__._TensorBase.abs(self)->Tensor
torch._C.__init__._TensorBase.abs_(self)->Tensor
torch._C.__init__._TensorBase.absolute(self)->Tensor
torch._C.__init__._TensorBase.absolute_(self)->Tensor
torch._C.__init__._TensorBase.acos(self)->Tensor
torch._C.__init__._TensorBase.acos_(self)->Tensor
torch._C.__init__._TensorBase.acosh(self)->Tensor
torch._C.__init__._TensorBase.acosh_(self)->Tensor
torch._C.__init__._TensorBase.add(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.add_(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1)->Tensor
torch._C.__init__._TensorBase.addbmm(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addbmm_(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addcdiv(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch._C.__init__._TensorBase.addcdiv_(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch._C.__init__._TensorBase.addcmul(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch._C.__init__._TensorBase.addcmul_(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch._C.__init__._TensorBase.addmm(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addmm_(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addmv(self,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addmv_(self,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addr(self,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.addr_(self,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.adjoint(self)->Tensor
torch._C.__init__._TensorBase.align_as(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.align_to(self,names:Sequence[Union[str,ellipsis,None]])->Tensor
torch._C.__init__._TensorBase.align_to(self,order:Sequence[Union[str,ellipsis,None]],ellipsis_idx:_int)->Tensor
torch._C.__init__._TensorBase.all(self)->Tensor
torch._C.__init__._TensorBase.all(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.all(self,dim:_int,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.allclose(self,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->_bool
torch._C.__init__._TensorBase.amax(self,dim:Union[_int,_size]=(),keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.amin(self,dim:Union[_int,_size]=(),keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.aminmax(self,*,dim:Optional[_int]=None,keepdim:_bool=False)->namedtuple_min_max
torch._C.__init__._TensorBase.angle(self)->Tensor
torch._C.__init__._TensorBase.any(self)->Tensor
torch._C.__init__._TensorBase.any(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.any(self,dim:_int,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.apply_(self,callable:Callable)->Tensor
torch._C.__init__._TensorBase.arccos(self)->Tensor
torch._C.__init__._TensorBase.arccos_(self)->Tensor
torch._C.__init__._TensorBase.arccosh(self)->Tensor
torch._C.__init__._TensorBase.arccosh_(self)->Tensor
torch._C.__init__._TensorBase.arcsin(self)->Tensor
torch._C.__init__._TensorBase.arcsin_(self)->Tensor
torch._C.__init__._TensorBase.arcsinh(self)->Tensor
torch._C.__init__._TensorBase.arcsinh_(self)->Tensor
torch._C.__init__._TensorBase.arctan(self)->Tensor
torch._C.__init__._TensorBase.arctan2(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.arctan2_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.arctan_(self)->Tensor
torch._C.__init__._TensorBase.arctanh(self)->Tensor
torch._C.__init__._TensorBase.arctanh_(self)->Tensor
torch._C.__init__._TensorBase.argmax(self,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.argmin(self,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.argsort(self,dim:Union[str,ellipsis,None],descending:_bool=False)->Tensor
torch._C.__init__._TensorBase.argsort(self,dim:_int=-1,descending:_bool=False)->Tensor
torch._C.__init__._TensorBase.argwhere(self)->Tensor
torch._C.__init__._TensorBase.as_strided(self,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.as_strided_(self,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.as_subclass(self,cls:Tensor)->Tensor
torch._C.__init__._TensorBase.asin(self)->Tensor
torch._C.__init__._TensorBase.asin_(self)->Tensor
torch._C.__init__._TensorBase.asinh(self)->Tensor
torch._C.__init__._TensorBase.asinh_(self)->Tensor
torch._C.__init__._TensorBase.atan(self)->Tensor
torch._C.__init__._TensorBase.atan2(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.atan2_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.atan_(self)->Tensor
torch._C.__init__._TensorBase.atanh(self)->Tensor
torch._C.__init__._TensorBase.atanh_(self)->Tensor
torch._C.__init__._TensorBase.baddbmm(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.baddbmm_(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.bernoulli(self,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.bernoulli(self,p:_float,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.bernoulli_(self,p:Tensor,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.bernoulli_(self,p:_float=0.5,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.bfloat16(self)->Tensor
torch._C.__init__._TensorBase.bincount(self,weights:Optional[Tensor]=None,minlength:_int=0)->Tensor
torch._C.__init__._TensorBase.bitwise_and(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_and(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_and_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_and_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_left_shift(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_left_shift(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_left_shift_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_left_shift_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_not(self)->Tensor
torch._C.__init__._TensorBase.bitwise_not_(self)->Tensor
torch._C.__init__._TensorBase.bitwise_or(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_or(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_or_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_or_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_right_shift(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_right_shift(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_right_shift_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_right_shift_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_xor(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_xor(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bitwise_xor_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.bitwise_xor_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.bmm(self,mat2:Tensor)->Tensor
torch._C.__init__._TensorBase.bool(self)->Tensor
torch._C.__init__._TensorBase.broadcast_to(self,*size:_int)->Tensor
torch._C.__init__._TensorBase.broadcast_to(self,size:_size)->Tensor
torch._C.__init__._TensorBase.byte(self)->Tensor
torch._C.__init__._TensorBase.cauchy_(self,median:_float=0,sigma:_float=1,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.ceil(self)->Tensor
torch._C.__init__._TensorBase.ceil_(self)->Tensor
torch._C.__init__._TensorBase.char(self)->Tensor
torch._C.__init__._TensorBase.cholesky(self,upper:_bool=False)->Tensor
torch._C.__init__._TensorBase.cholesky_inverse(self,upper:_bool=False)->Tensor
torch._C.__init__._TensorBase.cholesky_solve(self,input2:Tensor,upper:_bool=False)->Tensor
torch._C.__init__._TensorBase.chunk(self,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.clamp(self,min:Optional[Number]=None,max:Optional[Number]=None)->Tensor
torch._C.__init__._TensorBase.clamp(self,min:Optional[Tensor]=None,max:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.clamp_(self,min:Optional[Number]=None,max:Optional[Number]=None)->Tensor
torch._C.__init__._TensorBase.clamp_(self,min:Optional[Tensor]=None,max:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.clamp_max(self,max:Number)->Tensor
torch._C.__init__._TensorBase.clamp_max(self,max:Tensor)->Tensor
torch._C.__init__._TensorBase.clamp_max_(self,max:Number)->Tensor
torch._C.__init__._TensorBase.clamp_max_(self,max:Tensor)->Tensor
torch._C.__init__._TensorBase.clamp_min(self,min:Number)->Tensor
torch._C.__init__._TensorBase.clamp_min(self,min:Tensor)->Tensor
torch._C.__init__._TensorBase.clamp_min_(self,min:Number)->Tensor
torch._C.__init__._TensorBase.clamp_min_(self,min:Tensor)->Tensor
torch._C.__init__._TensorBase.clip(self,min:Optional[Number]=None,max:Optional[Number]=None)->Tensor
torch._C.__init__._TensorBase.clip(self,min:Optional[Tensor]=None,max:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.clip_(self,min:Optional[Number]=None,max:Optional[Number]=None)->Tensor
torch._C.__init__._TensorBase.clip_(self,min:Optional[Tensor]=None,max:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.clone(self,*,memory_format:Optional[memory_format]=None)->Tensor
torch._C.__init__._TensorBase.coalesce(self)->Tensor
torch._C.__init__._TensorBase.col_indices(self)->Tensor
torch._C.__init__._TensorBase.conj(self)->Tensor
torch._C.__init__._TensorBase.conj_physical(self)->Tensor
torch._C.__init__._TensorBase.conj_physical_(self)->Tensor
torch._C.__init__._TensorBase.contiguous(self,memory_format=torch.contiguous_format)->Tensor
torch._C.__init__._TensorBase.copy_(self,src:Tensor,non_blocking:_bool=False)->Tensor
torch._C.__init__._TensorBase.copysign(self,other:Number)->Tensor
torch._C.__init__._TensorBase.copysign(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.copysign_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.copysign_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.corrcoef(self)->Tensor
torch._C.__init__._TensorBase.cos(self)->Tensor
torch._C.__init__._TensorBase.cos_(self)->Tensor
torch._C.__init__._TensorBase.cosh(self)->Tensor
torch._C.__init__._TensorBase.cosh_(self)->Tensor
torch._C.__init__._TensorBase.count_nonzero(self,*dim:_int)->Tensor
torch._C.__init__._TensorBase.count_nonzero(self,dim:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.count_nonzero(self,dim:_size)->Tensor
torch._C.__init__._TensorBase.cov(self,*,correction:_int=1,fweights:Optional[Tensor]=None,aweights:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.cpu(self)->Tensor
torch._C.__init__._TensorBase.cross(self,other:Tensor,dim:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.crow_indices(self)->Tensor
torch._C.__init__._TensorBase.cuda(self,device:Optional[Union[_device,_int,str]]=None,non_blocking:_bool=False)->Tensor
torch._C.__init__._TensorBase.cummax(self,dim:Union[str,ellipsis,None])->namedtuple_values_indices
torch._C.__init__._TensorBase.cummax(self,dim:_int)->namedtuple_values_indices
torch._C.__init__._TensorBase.cummin(self,dim:Union[str,ellipsis,None])->namedtuple_values_indices
torch._C.__init__._TensorBase.cummin(self,dim:_int)->namedtuple_values_indices
torch._C.__init__._TensorBase.cumprod(self,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumprod(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumprod_(self,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumprod_(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumsum(self,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumsum(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumsum_(self,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.cumsum_(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.data_ptr(self)->_int
torch._C.__init__._TensorBase.deg2rad(self)->Tensor
torch._C.__init__._TensorBase.deg2rad_(self)->Tensor
torch._C.__init__._TensorBase.dense_dim(self)->_int
torch._C.__init__._TensorBase.dequantize(self)->Tensor
torch._C.__init__._TensorBase.det(self)->Tensor
torch._C.__init__._TensorBase.detach(self)->Tensor
torch._C.__init__._TensorBase.detach_(self)->Tensor
torch._C.__init__._TensorBase.diag(self,diagonal:_int=0)->Tensor
torch._C.__init__._TensorBase.diag_embed(self,offset:_int=0,dim1:_int=-2,dim2:_int=-1)->Tensor
torch._C.__init__._TensorBase.diagflat(self,offset:_int=0)->Tensor
torch._C.__init__._TensorBase.diagonal(self,*,outdim:Union[str,ellipsis,None],dim1:Union[str,ellipsis,None],dim2:Union[str,ellipsis,None],offset:_int=0)->Tensor
torch._C.__init__._TensorBase.diagonal(self,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch._C.__init__._TensorBase.diagonal_scatter(self,src:Tensor,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch._C.__init__._TensorBase.diff(self,n:_int=1,dim:_int=-1,prepend:Optional[Tensor]=None,append:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.digamma(self)->Tensor
torch._C.__init__._TensorBase.digamma_(self)->Tensor
torch._C.__init__._TensorBase.dim(self)->_int
torch._C.__init__._TensorBase.dist(self,other:Tensor,p:Number=2)->Tensor
torch._C.__init__._TensorBase.div(self,other:Union[Tensor,Number],*,rounding_mode:Optional[str]=None)->Tensor
torch._C.__init__._TensorBase.div_(self,other:Union[Tensor,Number],*,rounding_mode:Optional[str]=None)->Tensor
torch._C.__init__._TensorBase.divide(self,other:Number)->Tensor
torch._C.__init__._TensorBase.divide(self,other:Number,*,rounding_mode:Optional[str])->Tensor
torch._C.__init__._TensorBase.divide(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.divide(self,other:Tensor,*,rounding_mode:Optional[str])->Tensor
torch._C.__init__._TensorBase.divide_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.divide_(self,other:Number,*,rounding_mode:Optional[str])->Tensor
torch._C.__init__._TensorBase.divide_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.divide_(self,other:Tensor,*,rounding_mode:Optional[str])->Tensor
torch._C.__init__._TensorBase.dot(self,tensor:Tensor)->Tensor
torch._C.__init__._TensorBase.double(self)->Tensor
torch._C.__init__._TensorBase.dsplit(self,*indices:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.dsplit(self,indices:_size)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.dsplit(self,sections:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.eig(self,eigenvectors:_bool=False)->namedtuple_eigenvalues_eigenvectors
torch._C.__init__._TensorBase.element_size(self)->_int
torch._C.__init__._TensorBase.eq(self,other:Number)->Tensor
torch._C.__init__._TensorBase.eq(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.eq_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.eq_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.equal(self,other:Tensor)->_bool
torch._C.__init__._TensorBase.erf(self)->Tensor
torch._C.__init__._TensorBase.erf_(self)->Tensor
torch._C.__init__._TensorBase.erfc(self)->Tensor
torch._C.__init__._TensorBase.erfc_(self)->Tensor
torch._C.__init__._TensorBase.erfinv(self)->Tensor
torch._C.__init__._TensorBase.erfinv_(self)->Tensor
torch._C.__init__._TensorBase.exp(self)->Tensor
torch._C.__init__._TensorBase.exp2(self)->Tensor
torch._C.__init__._TensorBase.exp2_(self)->Tensor
torch._C.__init__._TensorBase.exp_(self)->Tensor
torch._C.__init__._TensorBase.expand(self,*size:_int,implicit:_bool=False)->Tensor
torch._C.__init__._TensorBase.expand(self,size:_size,*,implicit:_bool=False)->Tensor
torch._C.__init__._TensorBase.expand_as(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.expm1(self)->Tensor
torch._C.__init__._TensorBase.expm1_(self)->Tensor
torch._C.__init__._TensorBase.exponential_(self,lambd:_float=1,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.fill_(self,value:Number)->Tensor
torch._C.__init__._TensorBase.fill_(self,value:Tensor)->Tensor
torch._C.__init__._TensorBase.fill_diagonal_(self,fill_value:Number,wrap:_bool=False)->Tensor
torch._C.__init__._TensorBase.fix(self)->Tensor
torch._C.__init__._TensorBase.fix_(self)->Tensor
torch._C.__init__._TensorBase.flatten(self,dims:Sequence[Union[str,ellipsis,None]],out_dim:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.flatten(self,start_dim:Union[str,ellipsis,None],end_dim:Union[str,ellipsis,None],out_dim:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.flatten(self,start_dim:_int,end_dim:_int,out_dim:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.flatten(self,start_dim:_int=0,end_dim:_int=-1)->Tensor
torch._C.__init__._TensorBase.flip(self,*dims:_int)->Tensor
torch._C.__init__._TensorBase.flip(self,dims:_size)->Tensor
torch._C.__init__._TensorBase.fliplr(self)->Tensor
torch._C.__init__._TensorBase.flipud(self)->Tensor
torch._C.__init__._TensorBase.float(self)->Tensor
torch._C.__init__._TensorBase.float_power(self,exponent:Number)->Tensor
torch._C.__init__._TensorBase.float_power(self,exponent:Tensor)->Tensor
torch._C.__init__._TensorBase.float_power_(self,exponent:Number)->Tensor
torch._C.__init__._TensorBase.float_power_(self,exponent:Tensor)->Tensor
torch._C.__init__._TensorBase.floor(self)->Tensor
torch._C.__init__._TensorBase.floor_(self)->Tensor
torch._C.__init__._TensorBase.floor_divide(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.floor_divide_(self,other:Union[Tensor,Number])->Tensor
torch._C.__init__._TensorBase.fmax(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.fmin(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.fmod(self,other:Number)->Tensor
torch._C.__init__._TensorBase.fmod(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.fmod_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.fmod_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.frac(self)->Tensor
torch._C.__init__._TensorBase.frac_(self)->Tensor
torch._C.__init__._TensorBase.frexp(self)->namedtuple_mantissa_exponent
torch._C.__init__._TensorBase.gather(self,dim:Union[str,ellipsis,None],index:Tensor,*,sparse_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.gather(self,dim:_int,index:Tensor,*,sparse_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.gcd(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.gcd_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.ge(self,other:Number)->Tensor
torch._C.__init__._TensorBase.ge(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.ge_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.ge_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.geometric_(self,p:_float,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.geqrf(self)->namedtuple_a_tau
torch._C.__init__._TensorBase.ger(self,vec2:Tensor)->Tensor
torch._C.__init__._TensorBase.get_device(self)->_int
torch._C.__init__._TensorBase.greater(self,other:Number)->Tensor
torch._C.__init__._TensorBase.greater(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.greater_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.greater_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.greater_equal(self,other:Number)->Tensor
torch._C.__init__._TensorBase.greater_equal(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.greater_equal_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.greater_equal_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.gt(self,other:Number)->Tensor
torch._C.__init__._TensorBase.gt(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.gt_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.gt_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.half(self)->Tensor
torch._C.__init__._TensorBase.hardshrink(self,lambd:Number=0.5)->Tensor
torch._C.__init__._TensorBase.has_names(self)->_bool
torch._C.__init__._TensorBase.heaviside(self,values:Tensor)->Tensor
torch._C.__init__._TensorBase.heaviside_(self,values:Tensor)->Tensor
torch._C.__init__._TensorBase.histc(self,bins:_int=100,min:Number=0,max:Number=0)->Tensor
torch._C.__init__._TensorBase.histogram(self,bins:Tensor,*,weight:Optional[Tensor]=None,density:_bool=False)->namedtuple_hist_bin_edges
torch._C.__init__._TensorBase.histogram(self,bins:_int=100,*,range:Optional[Sequence[_float]]=None,weight:Optional[Tensor]=None,density:_bool=False)->namedtuple_hist_bin_edges
torch._C.__init__._TensorBase.hsplit(self,*indices:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.hsplit(self,indices:_size)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.hsplit(self,sections:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.hypot(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.hypot_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.i0(self)->Tensor
torch._C.__init__._TensorBase.i0_(self)->Tensor
torch._C.__init__._TensorBase.igamma(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.igamma_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.igammac(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.igammac_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.index_add(self,dim:Union[str,ellipsis,None],index:Tensor,source:Tensor,*,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.index_add(self,dim:_int,index:Tensor,source:Tensor,*,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.index_add_(self,dim:_int,index:Tensor,source:Tensor,*,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.index_copy(self,dim:Union[str,ellipsis,None],index:Tensor,source:Tensor)->Tensor
torch._C.__init__._TensorBase.index_copy(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch._C.__init__._TensorBase.index_copy_(self,dim:Union[str,ellipsis,None],index:Tensor,source:Tensor)->Tensor
torch._C.__init__._TensorBase.index_copy_(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch._C.__init__._TensorBase.index_fill(self,dim:Union[str,ellipsis,None],index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.index_fill(self,dim:Union[str,ellipsis,None],index:Tensor,value:Tensor)->Tensor
torch._C.__init__._TensorBase.index_fill(self,dim:_int,index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.index_fill(self,dim:_int,index:Tensor,value:Tensor)->Tensor
torch._C.__init__._TensorBase.index_fill_(self,dim:Union[str,ellipsis,None],index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.index_fill_(self,dim:Union[str,ellipsis,None],index:Tensor,value:Tensor)->Tensor
torch._C.__init__._TensorBase.index_fill_(self,dim:_int,index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.index_fill_(self,dim:_int,index:Tensor,value:Tensor)->Tensor
torch._C.__init__._TensorBase.index_put(self,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch._C.__init__._TensorBase.index_put_(self,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch._C.__init__._TensorBase.index_select(self,dim:Union[str,ellipsis,None],index:Tensor)->Tensor
torch._C.__init__._TensorBase.index_select(self,dim:_int,index:Tensor)->Tensor
torch._C.__init__._TensorBase.indices(self)->Tensor
torch._C.__init__._TensorBase.inner(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.int(self)->Tensor
torch._C.__init__._TensorBase.int_repr(self)->Tensor
torch._C.__init__._TensorBase.inverse(self)->Tensor
torch._C.__init__._TensorBase.is_coalesced(self)->_bool
torch._C.__init__._TensorBase.is_complex(self)->_bool
torch._C.__init__._TensorBase.is_conj(self)->_bool
torch._C.__init__._TensorBase.is_contiguous(self,memory_format=torch.contiguous_format)->_bool
torch._C.__init__._TensorBase.is_distributed(self)->_bool
torch._C.__init__._TensorBase.is_floating_point(self)->_bool
torch._C.__init__._TensorBase.is_inference(self)->_bool
torch._C.__init__._TensorBase.is_neg(self)->_bool
torch._C.__init__._TensorBase.is_nonzero(self)->_bool
torch._C.__init__._TensorBase.is_pinned(self,device:Optional[Union[_device,str,None]]=None)->_bool
torch._C.__init__._TensorBase.is_same_size(self,other:Tensor)->_bool
torch._C.__init__._TensorBase.is_set_to(self,tensor:Tensor)->_bool
torch._C.__init__._TensorBase.is_signed(self)->_bool
torch._C.__init__._TensorBase.isclose(self,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->Tensor
torch._C.__init__._TensorBase.isfinite(self)->Tensor
torch._C.__init__._TensorBase.isinf(self)->Tensor
torch._C.__init__._TensorBase.isnan(self)->Tensor
torch._C.__init__._TensorBase.isneginf(self)->Tensor
torch._C.__init__._TensorBase.isposinf(self)->Tensor
torch._C.__init__._TensorBase.isreal(self)->Tensor
torch._C.__init__._TensorBase.istft(self,n_fft:_int,hop_length:Optional[_int]=None,win_length:Optional[_int]=None,window:Optional[Tensor]=None,center:_bool=True,normalized:_bool=False,onesided:Optional[_bool]=None,length:Optional[_int]=None,return_complex:_bool=False)->Tensor
torch._C.__init__._TensorBase.item(self)->Number
torch._C.__init__._TensorBase.kron(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.kthvalue(self,k:_int,dim:Union[str,ellipsis,None],keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.kthvalue(self,k:_int,dim:_int=-1,keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.lcm(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.lcm_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.ldexp(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.ldexp_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.le(self,other:Number)->Tensor
torch._C.__init__._TensorBase.le(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.le_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.le_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.lerp(self,end:Tensor,weight:Number)->Tensor
torch._C.__init__._TensorBase.lerp(self,end:Tensor,weight:Tensor)->Tensor
torch._C.__init__._TensorBase.lerp_(self,end:Tensor,weight:Number)->Tensor
torch._C.__init__._TensorBase.lerp_(self,end:Tensor,weight:Tensor)->Tensor
torch._C.__init__._TensorBase.less(self,other:Number)->Tensor
torch._C.__init__._TensorBase.less(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.less_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.less_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.less_equal(self,other:Number)->Tensor
torch._C.__init__._TensorBase.less_equal(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.less_equal_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.less_equal_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.lgamma(self)->Tensor
torch._C.__init__._TensorBase.lgamma_(self)->Tensor
torch._C.__init__._TensorBase.log(self)->Tensor
torch._C.__init__._TensorBase.log10(self)->Tensor
torch._C.__init__._TensorBase.log10_(self)->Tensor
torch._C.__init__._TensorBase.log1p(self)->Tensor
torch._C.__init__._TensorBase.log1p_(self)->Tensor
torch._C.__init__._TensorBase.log2(self)->Tensor
torch._C.__init__._TensorBase.log2_(self)->Tensor
torch._C.__init__._TensorBase.log_(self)->Tensor
torch._C.__init__._TensorBase.log_normal_(self,mean:_float=1,std:_float=2,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.log_softmax(self,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.log_softmax(self,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.logaddexp(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logaddexp2(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logcumsumexp(self,dim:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.logcumsumexp(self,dim:_int)->Tensor
torch._C.__init__._TensorBase.logdet(self)->Tensor
torch._C.__init__._TensorBase.logical_and(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logical_and_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logical_not(self)->Tensor
torch._C.__init__._TensorBase.logical_not_(self)->Tensor
torch._C.__init__._TensorBase.logical_or(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logical_or_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logical_xor(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logical_xor_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.logit(self,eps:Optional[_float]=None)->Tensor
torch._C.__init__._TensorBase.logit_(self,eps:Optional[_float]=None)->Tensor
torch._C.__init__._TensorBase.logsumexp(self,dim:Sequence[Union[str,ellipsis,None]],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.logsumexp(self,dim:Union[_int,_size],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.long(self)->Tensor
torch._C.__init__._TensorBase.lstsq(self,A:Tensor)->namedtuple_solution_QR
torch._C.__init__._TensorBase.lt(self,other:Number)->Tensor
torch._C.__init__._TensorBase.lt(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.lt_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.lt_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.lu_solve(self,LU_data:Tensor,LU_pivots:Tensor)->Tensor
torch._C.__init__._TensorBase.map2_(self,x:Tensor,y:Tensor,callable:Callable)->Tensor
torch._C.__init__._TensorBase.map_(self,tensor:Tensor,callable:Callable)->Tensor
torch._C.__init__._TensorBase.masked_fill(self,mask:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.masked_fill(self,mask:Tensor,value:Tensor)->Tensor
torch._C.__init__._TensorBase.masked_fill_(self,mask:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.masked_fill_(self,mask:Tensor,value:Tensor)->Tensor
torch._C.__init__._TensorBase.masked_scatter(self,mask:Tensor,source:Tensor)->Tensor
torch._C.__init__._TensorBase.masked_scatter_(self,mask:Tensor,source:Tensor)->Tensor
torch._C.__init__._TensorBase.masked_select(self,mask:Tensor)->Tensor
torch._C.__init__._TensorBase.matmul(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.matrix_exp(self)->Tensor
torch._C.__init__._TensorBase.matrix_power(self,n:_int)->Tensor
torch._C.__init__._TensorBase.max(self)->Tensor
torch._C.__init__._TensorBase.max(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.max(self,dim:_int,keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.max(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.maximum(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.mean(self,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.mean(self,dim:Sequence[Union[str,ellipsis,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.mean(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.median(self)->Tensor
torch._C.__init__._TensorBase.median(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.median(self,dim:_int,keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.min(self)->Tensor
torch._C.__init__._TensorBase.min(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.min(self,dim:_int,keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.min(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.minimum(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.mm(self,mat2:Tensor)->Tensor
torch._C.__init__._TensorBase.mode(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.mode(self,dim:_int=-1,keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.moveaxis(self,source:_int,destination:_int)->Tensor
torch._C.__init__._TensorBase.moveaxis(self,source:_size,destination:_size)->Tensor
torch._C.__init__._TensorBase.movedim(self,source:_int,destination:_int)->Tensor
torch._C.__init__._TensorBase.movedim(self,source:_size,destination:_size)->Tensor
torch._C.__init__._TensorBase.msort(self)->Tensor
torch._C.__init__._TensorBase.mul(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.mul_(self,other:Union[Tensor,Number])->Tensor
torch._C.__init__._TensorBase.multinomial(self,num_samples:_int,replacement:_bool=False,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.multiply(self,other:Number)->Tensor
torch._C.__init__._TensorBase.multiply(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.multiply_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.multiply_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.mv(self,vec:Tensor)->Tensor
torch._C.__init__._TensorBase.mvlgamma(self,p:_int)->Tensor
torch._C.__init__._TensorBase.mvlgamma_(self,p:_int)->Tensor
torch._C.__init__._TensorBase.nan_to_num(self,nan:Optional[_float]=None,posinf:Optional[_float]=None,neginf:Optional[_float]=None)->Tensor
torch._C.__init__._TensorBase.nan_to_num_(self,nan:Optional[_float]=None,posinf:Optional[_float]=None,neginf:Optional[_float]=None)->Tensor
torch._C.__init__._TensorBase.nanmean(self,dim:Union[_int,_size]=(),keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.nanmedian(self)->Tensor
torch._C.__init__._TensorBase.nanmedian(self,dim:Union[str,ellipsis,None],keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.nanmedian(self,dim:_int,keepdim:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.nanquantile(self,q:Tensor,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear')->Tensor
torch._C.__init__._TensorBase.nanquantile(self,q:_float,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear')->Tensor
torch._C.__init__._TensorBase.nansum(self,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.nansum(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.narrow(self,dim:_int,start:Tensor,length:_int)->Tensor
torch._C.__init__._TensorBase.narrow(self,dim:_int,start:_int,length:_int)->Tensor
torch._C.__init__._TensorBase.narrow_copy(self,dim:_int,start:_int,length:_int)->Tensor
torch._C.__init__._TensorBase.ndimension(self)->_int
torch._C.__init__._TensorBase.ne(self,other:Number)->Tensor
torch._C.__init__._TensorBase.ne(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.ne_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.ne_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.neg(self)->Tensor
torch._C.__init__._TensorBase.neg_(self)->Tensor
torch._C.__init__._TensorBase.negative(self)->Tensor
torch._C.__init__._TensorBase.negative_(self)->Tensor
torch._C.__init__._TensorBase.nelement(self)->_int
torch._C.__init__._TensorBase.new(self,*args:Any,device:Union[_device,str,None]=None)->Tensor
torch._C.__init__._TensorBase.new(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.new(self,size:_size,*,device:Union[_device,str,None]=None)->Tensor
torch._C.__init__._TensorBase.new(self,storage:Storage)->Tensor
torch._C.__init__._TensorBase.new_empty(self,*size:_int,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_empty(self,size:_size,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_empty_strided(self,size:_size,stride:_size,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_full(self,size:_size,fill_value:Number,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_ones(self,*size:_int,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_ones(self,size:_size,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_ones(self,size:_size,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_tensor(self,data:Any,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_zeros(self,*size:_int,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.new_zeros(self,size:_size,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C.__init__._TensorBase.nextafter(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.nextafter_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.nonzero(self,*,as_tuple:Literal[False]=False)->Tensor
torch._C.__init__._TensorBase.nonzero(self,*,as_tuple:Literal[True])->Tuple[Tensor, ...]
torch._C.__init__._TensorBase.normal_(self,mean:_float=0,std:_float=1,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.not_equal(self,other:Number)->Tensor
torch._C.__init__._TensorBase.not_equal(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.not_equal_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.not_equal_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.numel(self)->_int
torch._C.__init__._TensorBase.numpy(self)->Any
torch._C.__init__._TensorBase.orgqr(self,input2:Tensor)->Tensor
torch._C.__init__._TensorBase.ormqr(self,input2:Tensor,input3:Tensor,left:_bool=True,transpose:_bool=False)->Tensor
torch._C.__init__._TensorBase.outer(self,vec2:Tensor)->Tensor
torch._C.__init__._TensorBase.permute(self,*dims:_int)->Tensor
torch._C.__init__._TensorBase.permute(self,dims:_size)->Tensor
torch._C.__init__._TensorBase.pin_memory(self,device:Optional[Union[_device,str,None]]=None)->Tensor
torch._C.__init__._TensorBase.pinverse(self,rcond:_float=1e-15)->Tensor
torch._C.__init__._TensorBase.polygamma(self,n:_int)->Tensor
torch._C.__init__._TensorBase.polygamma_(self,n:_int)->Tensor
torch._C.__init__._TensorBase.positive(self)->Tensor
torch._C.__init__._TensorBase.pow(self,exponent:Number)->Tensor
torch._C.__init__._TensorBase.pow(self,exponent:Tensor)->Tensor
torch._C.__init__._TensorBase.pow_(self,exponent:Number)->Tensor
torch._C.__init__._TensorBase.pow_(self,exponent:Tensor)->Tensor
torch._C.__init__._TensorBase.prelu(self,weight:Tensor)->Tensor
torch._C.__init__._TensorBase.prod(self,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.prod(self,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.prod(self,dim:_int,keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.put(self,index:Tensor,source:Tensor,accumulate:_bool=False)->Tensor
torch._C.__init__._TensorBase.put_(self,index:Tensor,source:Tensor,accumulate:_bool=False)->Tensor
torch._C.__init__._TensorBase.q_per_channel_axis(self)->_int
torch._C.__init__._TensorBase.q_per_channel_scales(self)->Tensor
torch._C.__init__._TensorBase.q_per_channel_zero_points(self)->Tensor
torch._C.__init__._TensorBase.q_scale(self)->_float
torch._C.__init__._TensorBase.q_zero_point(self)->_int
torch._C.__init__._TensorBase.qr(self,some:_bool=True)->namedtuple_Q_R
torch._C.__init__._TensorBase.qscheme(self)->_qscheme
torch._C.__init__._TensorBase.quantile(self,q:Tensor,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear')->Tensor
torch._C.__init__._TensorBase.quantile(self,q:_float,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear')->Tensor
torch._C.__init__._TensorBase.rad2deg(self)->Tensor
torch._C.__init__._TensorBase.rad2deg_(self)->Tensor
torch._C.__init__._TensorBase.random_(self,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.random_(self,from_:_int,to:Optional[_int],*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.random_(self,to:_int,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.ravel(self)->Tensor
torch._C.__init__._TensorBase.reciprocal(self)->Tensor
torch._C.__init__._TensorBase.reciprocal_(self)->Tensor
torch._C.__init__._TensorBase.record_stream(self,s:Stream)->None
torch._C.__init__._TensorBase.refine_names(self,names:Sequence[Union[str,ellipsis,None]])->Tensor
torch._C.__init__._TensorBase.relu(self)->Tensor
torch._C.__init__._TensorBase.relu_(self)->Tensor
torch._C.__init__._TensorBase.remainder(self,other:Number)->Tensor
torch._C.__init__._TensorBase.remainder(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.remainder_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.remainder_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.rename(self,names:Optional[Sequence[Union[str,ellipsis,None]]])->Tensor
torch._C.__init__._TensorBase.rename_(self,names:Optional[Sequence[Union[str,ellipsis,None]]])->Tensor
torch._C.__init__._TensorBase.renorm(self,p:Number,dim:_int,maxnorm:Number)->Tensor
torch._C.__init__._TensorBase.renorm_(self,p:Number,dim:_int,maxnorm:Number)->Tensor
torch._C.__init__._TensorBase.repeat(self,*repeats:_int)->Tensor
torch._C.__init__._TensorBase.repeat(self,repeats:_size)->Tensor
torch._C.__init__._TensorBase.repeat_interleave(self,repeats:Tensor,dim:Optional[_int]=None,*,output_size:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.repeat_interleave(self,repeats:_int,dim:Optional[_int]=None,*,output_size:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.requires_grad_(self,mode:_bool=True)->Tensor
torch._C.__init__._TensorBase.reshape(self,*shape:_int)->Tensor
torch._C.__init__._TensorBase.reshape(self,shape:_size)->Tensor
torch._C.__init__._TensorBase.reshape_as(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.resize_(self,*size:_int,memory_format:Optional[memory_format]=None)->Tensor
torch._C.__init__._TensorBase.resize_(self,size:_size,*,memory_format:Optional[memory_format]=None)->Tensor
torch._C.__init__._TensorBase.resize_as_(self,the_template:Tensor,*,memory_format:Optional[memory_format]=None)->Tensor
torch._C.__init__._TensorBase.resolve_conj(self)->Tensor
torch._C.__init__._TensorBase.resolve_neg(self)->Tensor
torch._C.__init__._TensorBase.retain_grad(self)->None
torch._C.__init__._TensorBase.roll(self,shifts:Union[_int,_size],dims:Union[_int,_size]=())->Tensor
torch._C.__init__._TensorBase.rot90(self,k:_int=1,dims:_size=(0,1))->Tensor
torch._C.__init__._TensorBase.round(self)->Tensor
torch._C.__init__._TensorBase.round(self,*,decimals:_int)->Tensor
torch._C.__init__._TensorBase.round_(self)->Tensor
torch._C.__init__._TensorBase.round_(self,*,decimals:_int)->Tensor
torch._C.__init__._TensorBase.rsqrt(self)->Tensor
torch._C.__init__._TensorBase.rsqrt_(self)->Tensor
torch._C.__init__._TensorBase.scatter(self,dim:Union[str,ellipsis,None],index:Tensor,src:Tensor)->Tensor
torch._C.__init__._TensorBase.scatter(self,dim:Union[str,ellipsis,None],index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.scatter(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch._C.__init__._TensorBase.scatter(self,dim:_int,index:Tensor,src:Tensor,*,reduce:str)->Tensor
torch._C.__init__._TensorBase.scatter(self,dim:_int,index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.scatter(self,dim:_int,index:Tensor,value:Number,*,reduce:str)->Tensor
torch._C.__init__._TensorBase.scatter_(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch._C.__init__._TensorBase.scatter_(self,dim:_int,index:Tensor,src:Tensor,*,reduce:str)->Tensor
torch._C.__init__._TensorBase.scatter_(self,dim:_int,index:Tensor,value:Number)->Tensor
torch._C.__init__._TensorBase.scatter_(self,dim:_int,index:Tensor,value:Number,*,reduce:str)->Tensor
torch._C.__init__._TensorBase.scatter_add(self,dim:Union[str,ellipsis,None],index:Tensor,src:Tensor)->Tensor
torch._C.__init__._TensorBase.scatter_add(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch._C.__init__._TensorBase.scatter_add_(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch._C.__init__._TensorBase.scatter_reduce(self,dim:_int,index:Tensor,reduce:str,*,output_size:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.select(self,dim:Union[str,ellipsis,None],index:_int)->Tensor
torch._C.__init__._TensorBase.select(self,dim:_int,index:_int)->Tensor
torch._C.__init__._TensorBase.select_scatter(self,src:Tensor,dim:_int,index:_int)->Tensor
torch._C.__init__._TensorBase.set_(self,storage:Union[Storage,_TypedStorage])->Tensor
torch._C.__init__._TensorBase.set_(self,storage:Union[Storage,_TypedStorage],offset:_int,size:_size,stride:_size)->Tensor
torch._C.__init__._TensorBase.sgn(self)->Tensor
torch._C.__init__._TensorBase.sgn_(self)->Tensor
torch._C.__init__._TensorBase.short(self)->Tensor
torch._C.__init__._TensorBase.sigmoid(self)->Tensor
torch._C.__init__._TensorBase.sigmoid_(self)->Tensor
torch._C.__init__._TensorBase.sign(self)->Tensor
torch._C.__init__._TensorBase.sign_(self)->Tensor
torch._C.__init__._TensorBase.signbit(self)->Tensor
torch._C.__init__._TensorBase.sin(self)->Tensor
torch._C.__init__._TensorBase.sin_(self)->Tensor
torch._C.__init__._TensorBase.sinc(self)->Tensor
torch._C.__init__._TensorBase.sinc_(self)->Tensor
torch._C.__init__._TensorBase.sinh(self)->Tensor
torch._C.__init__._TensorBase.sinh_(self)->Tensor
torch._C.__init__._TensorBase.size(self)->Size
torch._C.__init__._TensorBase.size(self,dim:_int)->_int
torch._C.__init__._TensorBase.slice_scatter(self,src:Tensor,dim:_int=0,start:Optional[_int]=None,end:Optional[_int]=None,step:_int=1)->Tensor
torch._C.__init__._TensorBase.slogdet(self)->namedtuple_sign_logabsdet
torch._C.__init__._TensorBase.smm(self,mat2:Tensor)->Tensor
torch._C.__init__._TensorBase.softmax(self,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.softmax(self,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.solve(self,A:Tensor)->namedtuple_solution_LU
torch._C.__init__._TensorBase.sort(self,*,stable:Optional[_bool],dim:Union[str,ellipsis,None],descending:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.sort(self,*,stable:Optional[_bool],dim:_int=-1,descending:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.sort(self,dim:Union[str,ellipsis,None],descending:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.sort(self,dim:_int=-1,descending:_bool=False)->namedtuple_values_indices
torch._C.__init__._TensorBase.sparse_dim(self)->_int
torch._C.__init__._TensorBase.sparse_mask(self,mask:Tensor)->Tensor
torch._C.__init__._TensorBase.sparse_resize_(self,size:_size,sparse_dim:_int,dense_dim:_int)->Tensor
torch._C.__init__._TensorBase.sparse_resize_and_clear_(self,size:_size,sparse_dim:_int,dense_dim:_int)->Tensor
torch._C.__init__._TensorBase.split(self,split_size:Tuple[_int,...],dim:_int=0)->Sequence[Tensor]
torch._C.__init__._TensorBase.split(self,split_size:_int,dim:_int=0)->Sequence[Tensor]
torch._C.__init__._TensorBase.split_with_sizes(self,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.sqrt(self)->Tensor
torch._C.__init__._TensorBase.sqrt_(self)->Tensor
torch._C.__init__._TensorBase.square(self)->Tensor
torch._C.__init__._TensorBase.square_(self)->Tensor
torch._C.__init__._TensorBase.squeeze(self)->Tensor
torch._C.__init__._TensorBase.squeeze(self,dim:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.squeeze(self,dim:_int)->Tensor
torch._C.__init__._TensorBase.squeeze_(self)->Tensor
torch._C.__init__._TensorBase.squeeze_(self,dim:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.squeeze_(self,dim:_int)->Tensor
torch._C.__init__._TensorBase.sspaddmm(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.std(self,dim:Optional[Union[_int,_size]],*,correction:Optional[_int],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.std(self,dim:Sequence[Union[str,ellipsis,None]],*,correction:Optional[_int],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.std(self,dim:Sequence[Union[str,ellipsis,None]],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.std(self,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.std(self,unbiased:_bool=True)->Tensor
torch._C.__init__._TensorBase.storage_offset(self)->_int
torch._C.__init__._TensorBase.storage_type(self)->Storage
torch._C.__init__._TensorBase.stride(self)->Tuple[_int]
torch._C.__init__._TensorBase.stride(self,_int)->_int
torch._C.__init__._TensorBase.sub(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.sub_(self,other:Union[Tensor,Number],*,alpha:Optional[Number]=1)->Tensor
torch._C.__init__._TensorBase.subtract(self,other:Number,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.subtract(self,other:Tensor,*,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.subtract_(self,other:Number,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.subtract_(self,other:Tensor,*,alpha:Number=1)->Tensor
torch._C.__init__._TensorBase.sum(self,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.sum(self,dim:Sequence[Union[str,ellipsis,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.sum(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.sum_to_size(self,*size:_int)->Tensor
torch._C.__init__._TensorBase.sum_to_size(self,size:_size)->Tensor
torch._C.__init__._TensorBase.svd(self,some:_bool=True,compute_uv:_bool=True)->namedtuple_U_S_V
torch._C.__init__._TensorBase.swapaxes(self,axis0:_int,axis1:_int)->Tensor
torch._C.__init__._TensorBase.swapaxes_(self,axis0:_int,axis1:_int)->Tensor
torch._C.__init__._TensorBase.swapdims(self,dim0:_int,dim1:_int)->Tensor
torch._C.__init__._TensorBase.swapdims_(self,dim0:_int,dim1:_int)->Tensor
torch._C.__init__._TensorBase.symeig(self,eigenvectors:_bool=False,upper:_bool=True)->namedtuple_eigenvalues_eigenvectors
torch._C.__init__._TensorBase.t(self)->Tensor
torch._C.__init__._TensorBase.t_(self)->Tensor
torch._C.__init__._TensorBase.take(self,index:Tensor)->Tensor
torch._C.__init__._TensorBase.take_along_dim(self,indices:Tensor,dim:Optional[_int]=None)->Tensor
torch._C.__init__._TensorBase.tan(self)->Tensor
torch._C.__init__._TensorBase.tan_(self)->Tensor
torch._C.__init__._TensorBase.tanh(self)->Tensor
torch._C.__init__._TensorBase.tanh_(self)->Tensor
torch._C.__init__._TensorBase.tensor_split(self,indices:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.tensor_split(self,sections:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.tensor_split(self,tensor_indices_or_sections:Tensor,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.tile(self,*dims:_int)->Tensor
torch._C.__init__._TensorBase.tile(self,dims:_size)->Tensor
torch._C.__init__._TensorBase.to(self,device:Optional[Union[_device,str]]=None,dtype:Optional[_dtype]=None,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch._C.__init__._TensorBase.to(self,dtype:_dtype,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch._C.__init__._TensorBase.to(self,other:Tensor,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch._C.__init__._TensorBase.to_dense(self,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.to_mkldnn(self,dtype:Optional[_dtype]=None)->Tensor
torch._C.__init__._TensorBase.to_sparse(self)->Tensor
torch._C.__init__._TensorBase.to_sparse(self,sparse_dim:_int)->Tensor
torch._C.__init__._TensorBase.tolist(self)->List
torch._C.__init__._TensorBase.topk(self,k:_int,dim:_int=-1,largest:_bool=True,sorted:_bool=True)->namedtuple_values_indices
torch._C.__init__._TensorBase.trace(self)->Tensor
torch._C.__init__._TensorBase.transpose(self,dim0:Union[str,ellipsis,None],dim1:Union[str,ellipsis,None])->Tensor
torch._C.__init__._TensorBase.transpose(self,dim0:_int,dim1:_int)->Tensor
torch._C.__init__._TensorBase.transpose_(self,dim0:_int,dim1:_int)->Tensor
torch._C.__init__._TensorBase.triangular_solve(self,A:Tensor,upper:_bool=True,transpose:_bool=False,unitriangular:_bool=False)->namedtuple_solution_cloned_coefficient
torch._C.__init__._TensorBase.tril(self,diagonal:_int=0)->Tensor
torch._C.__init__._TensorBase.tril_(self,diagonal:_int=0)->Tensor
torch._C.__init__._TensorBase.triu(self,diagonal:_int=0)->Tensor
torch._C.__init__._TensorBase.triu_(self,diagonal:_int=0)->Tensor
torch._C.__init__._TensorBase.true_divide(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch._C.__init__._TensorBase.true_divide_(self,other:Union[Tensor,Number])->Tensor
torch._C.__init__._TensorBase.trunc(self)->Tensor
torch._C.__init__._TensorBase.trunc_(self)->Tensor
torch._C.__init__._TensorBase.type(self,dtype:None=None,non_blocking:_bool=False)->str
torch._C.__init__._TensorBase.type(self,dtype:Union[str,_dtype],non_blocking:_bool=False)->Tensor
torch._C.__init__._TensorBase.type_as(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.unbind(self,dim:Union[str,ellipsis,None])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.unbind(self,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.unflatten(self,dim:Union[str,ellipsis,None],sizes:_size,names:Sequence[Union[str,ellipsis,None]])->Tensor
torch._C.__init__._TensorBase.unflatten(self,dim:_int,sizes:_size,names:Optional[Sequence[Union[str,ellipsis,None]]]=None)->Tensor
torch._C.__init__._TensorBase.unfold(self,dimension:_int,size:_int,step:_int)->Tensor
torch._C.__init__._TensorBase.uniform_(self,from_:_float=0,to:_float=1,*,generator:Optional[Generator]=None)->Tensor
torch._C.__init__._TensorBase.unsafe_chunk(self,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.unsafe_split(self,split_size:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.unsafe_split_with_sizes(self,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.unsqueeze(self,dim:_int)->Tensor
torch._C.__init__._TensorBase.unsqueeze_(self,dim:_int)->Tensor
torch._C.__init__._TensorBase.values(self)->Tensor
torch._C.__init__._TensorBase.var(self,dim:Optional[Union[_int,_size]],*,correction:Optional[_int],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.var(self,dim:Sequence[Union[str,ellipsis,None]],*,correction:Optional[_int],keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.var(self,dim:Sequence[Union[str,ellipsis,None]],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.var(self,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch._C.__init__._TensorBase.var(self,unbiased:_bool=True)->Tensor
torch._C.__init__._TensorBase.vdot(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.view(self,*size:_int)->Tensor
torch._C.__init__._TensorBase.view(self,dtype:_dtype)->Tensor
torch._C.__init__._TensorBase.view(self,size:_size)->Tensor
torch._C.__init__._TensorBase.view_as(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.vsplit(self,*indices:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.vsplit(self,indices:_size)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.vsplit(self,sections:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C.__init__._TensorBase.where(self,condition:Tensor,other:Tensor)->Tensor
torch._C.__init__._TensorBase.xlogy(self,other:Number)->Tensor
torch._C.__init__._TensorBase.xlogy(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.xlogy_(self,other:Number)->Tensor
torch._C.__init__._TensorBase.xlogy_(self,other:Tensor)->Tensor
torch._C.__init__._TensorBase.zero_(self)->Tensor
torch._C.__init__._TensorMeta(type)
torch._C.__init__._UpgraderEntry(self,bumped_at_version:_int,upgrader_name:str,old_schema:str)
torch._C.__init__._UpgraderEntry.__init__(self,bumped_at_version:_int,upgrader_name:str,old_schema:str)
torch._C.__init__._UpgraderRange
torch._C.__init__.__is_forward_AD_enabled()->_bool
torch._C.__init__.__set_forward_AD_enabled(enabled:_bool)->None
torch._C.__init__._add_docstr(obj:T,doc_obj:str)->T
torch._C.__init__._assign_output_shapes(graph:Graph,inputs:List[Tensor])->Graph
torch._C.__init__._autograd_init()->_bool
torch._C.__init__._backport_for_mobile(filename_input:Union[str,Path],filename_output:Union[str,Path],to_version:_int)->None
torch._C.__init__._backport_for_mobile_from_buffer(buffer:BinaryIO,filename_output:Union[str,Path],to_version:_int)->None
torch._C.__init__._backport_for_mobile_from_buffer_to_buffer(buffer:BinaryIO,to_version:_int)->bytes
torch._C.__init__._backport_for_mobile_to_buffer(filename_input:Union[str,Path],to_version:_int)->bytes
torch._C.__init__._broadcast(tensor:Tensor,devices:List[_int])->List[Tensor]
torch._C.__init__._broadcast_coalesced(tensors:List[Tensor],devices:List[_int],buffer_size:_int)->List[List[Tensor]]
torch._C.__init__._broadcast_out(tensor:Tensor,out_tensors:List[Tensor])->List[Tensor]
torch._C.__init__._c10d_init()->_bool
torch._C.__init__._check_onnx_proto(proto:str)->None
torch._C.__init__._clone_module_with_class(module:'torch.jit.ScriptModule',ignored_methods:List[AnyStr],ignored_attributes:List[AnyStr])->'torch.jit.ScriptModule'
torch._C.__init__._collect_all(futures:List[Future])->Future
torch._C.__init__._compile_graph_to_code_table(name:str,graph:Graph)->IValue
torch._C.__init__._crash_if_aten_asan()->_int
torch._C.__init__._crash_if_csrc_asan()->_int
torch._C.__init__._crash_if_csrc_ubsan()->_int
torch._C.__init__._create_function_from_graph(qualname:str,graph:Graph)->Graph
torch._C.__init__._create_function_from_trace(qualname:str,func:Callable[...,Any],input_tuple:Tuple[Any,...],var_lookup_fn:Callable[[Tensor],str],strict:_bool,force_outplace:_bool,argument_names:List[str])->Tuple[Graph, Stack]
torch._C.__init__._create_graph_by_tracing(func:Callable[...,Any],inputs:Any,var_name_lookup_fn:Callable[[Tensor],str],strict:Any,force_outplace:Any,self:Any=None,argument_names:List[str]=[])->Tuple[Graph, Stack]
torch._C.__init__._create_module_with_type(ty:JitType)->ScriptModule
torch._C.__init__._create_object_with_type(ty:ClassType)->ScriptObject
torch._C.__init__._cuda_canDeviceAccessPeer(device:_int,peer_device:_int)->_bool
torch._C.__init__._cuda_cudaCachingAllocator_raw_alloc(size:_int,cuda_stream:_int)->_int
torch._C.__init__._cuda_cudaCachingAllocator_raw_delete(ptr:_int)->None
torch._C.__init__._cuda_cudaHostAllocator()->_int
torch._C.__init__._cuda_emptyCache()->None
torch._C.__init__._cuda_getArchFlags()->Optional[str]
torch._C.__init__._cuda_getCompiledVersion()->_int
torch._C.__init__._cuda_getCurrentBlasHandle()->_int
torch._C.__init__._cuda_getCurrentStream(device:_int)->_int
torch._C.__init__._cuda_getDefaultStream(device:_int)->_int
torch._C.__init__._cuda_getDevice()->_int
torch._C.__init__._cuda_getDeviceCount()->_int
torch._C.__init__._cuda_get_sync_debug_mode()->_int
torch._C.__init__._cuda_init()->None
torch._C.__init__._cuda_ipc_collect()->None
torch._C.__init__._cuda_lock_mutex()->None
torch._C.__init__._cuda_memorySnapshot()->List[Dict[str, Any]]
torch._C.__init__._cuda_memoryStats(device:_int)->Dict[str, Any]
torch._C.__init__._cuda_resetAccumulatedMemoryStats(device:_int)->None
torch._C.__init__._cuda_resetPeakMemoryStats(device:_int)->None
torch._C.__init__._cuda_setDevice(device:_int)->None
torch._C.__init__._cuda_setMemoryFraction(fraction:_float,device:_int)->None
torch._C.__init__._cuda_setStream(cuda_stream:_int)->None
torch._C.__init__._cuda_set_sync_debug_mode(warn_level:Union[_int,str])->None
torch._C.__init__._cuda_sleep(cycles:_int)->None
torch._C.__init__._cuda_synchronize()->None
torch._C.__init__._cuda_unlock_mutex()->None
torch._C.__init__._cxx_flags()->str
torch._C.__init__._debug_get_fusion_group_inlining()->_bool
torch._C.__init__._debug_set_autodiff_subgraph_inlining(disabled:_bool)->None
torch._C.__init__._debug_set_fusion_group_inlining(enable:_bool)
torch._C.__init__._demangle(str)->str
torch._C.__init__._disable_minidumps()->None
torch._C.__init__._disabled_torch_function_impl(func:Callable,types:Iterable[Type],args:Tuple,kwargs:Dict)->Any
torch._C.__init__._dist_autograd_init()->_bool
torch._C.__init__._dump_upgraders_map()->Dict[str, str]
torch._C.__init__._enable_minidumps(directory:str)->None
torch._C.__init__._enable_minidumps_on_exceptions()->None
torch._C.__init__._enter_dual_level()->_int
torch._C.__init__._enter_python_mode(cls:Type)->None
torch._C.__init__._error_if_any_worker_fails()->None
torch._C.__init__._exit_dual_level(level:_int)->None
torch._C.__init__._exit_python_mode()->None
torch._C.__init__._export_operator_list(module:LiteScriptModule)
torch._C.__init__._export_opnames(module:ScriptModule)->List[str]
torch._C.__init__._faulty_agent_init()->_bool
torch._C.__init__._freeze_module(module:ScriptModule,preserved_attrs:List[str]=[],freeze_interfaces:_bool=True,preserveParameters:_bool=True)->ScriptModule
torch._C.__init__._from_dlpack(data:Any)->Tensor
torch._C.__init__._gather(tensors:List[Tensor],dim:_int,destination_index:Optional[_int])->Tensor
torch._C.__init__._gather_out(tensors:List[Tensor],out_tensor:Tensor,dim:_int)->Tensor
torch._C.__init__._generate_upgraders_graph()->Dict[str, Graph]
torch._C.__init__._get_backcompat_broadcast_warn()->_bool
torch._C.__init__._get_backcompat_keepdim_warn()->_bool
torch._C.__init__._get_cublas_allow_fp16_reduced_precision_reduction()->_bool
torch._C.__init__._get_cublas_allow_tf32()->_bool
torch._C.__init__._get_cudnn_allow_tf32()->_bool
torch._C.__init__._get_cudnn_benchmark()->_bool
torch._C.__init__._get_cudnn_deterministic()->_bool
torch._C.__init__._get_cudnn_enabled()->_bool
torch._C.__init__._get_custom_class_python_wrapper(name:str,attr:str)->Any
torch._C.__init__._get_default_device()->str
torch._C.__init__._get_deterministic_algorithms()->_bool
torch._C.__init__._get_deterministic_algorithms_warn_only()->_bool
torch._C.__init__._get_graph_executor_optimize()->_bool
torch._C.__init__._get_linalg_preferred_backend()->torch._C._LinalgBackend
torch._C.__init__._get_max_operator_version()->_int
torch._C.__init__._get_mkldnn_enabled()->_bool
torch._C.__init__._get_mobile_model_contained_types(filename:Union[str,Path])
torch._C.__init__._get_mobile_model_contained_types_from_buffer(buffer:BinaryIO)
torch._C.__init__._get_model_bytecode_version(filename:Union[str,Path])->_int
torch._C.__init__._get_model_bytecode_version_from_buffer(buffer:BinaryIO)->_int
torch._C.__init__._get_model_ops_and_info(filename:Union[str,Path])
torch._C.__init__._get_model_ops_and_info_from_buffer(buffer:BinaryIO)
torch._C.__init__._get_operation_overload(op_name:str,op_overload_name:str)->Callable
torch._C.__init__._get_operator_version_map()->Dict[str, List[_UpgraderEntry]]
torch._C.__init__._get_qengine()->_int
torch._C.__init__._get_schema(op_name:str,overload_name:str)->FunctionSchema
torch._C.__init__._get_tracing_state()->TracingState
torch._C.__init__._get_upgrader_ranges(name:str)->List[_UpgraderRange]
torch._C.__init__._get_upgraders_map_size()->_int
torch._C.__init__._get_warnAlways()->_bool
torch._C.__init__._graph_pool_handle()->Tuple[_int, _int]
torch._C.__init__._has_distributed()->_bool
torch._C.__init__._has_torch_function(args:Iterable[Any])->_bool
torch._C.__init__._has_torch_function_unary(Any)->_bool
torch._C.__init__._has_torch_function_variadic(*args:Any)->_bool
torch._C.__init__._import_ir_module_from_package(cu:CompilationUnit,reader:PyTorchFileReader,storage_context:DeserializationStorageContext,map_location:Union[_device,str,None],ts_id:str)->ScriptModule
torch._C.__init__._infer_size(arg1:Size,arg2:Size)->Size
torch._C.__init__._initExtension(shm_manager_path:str)->None
torch._C.__init__._init_names(arg:Sequence[Type])->None
torch._C.__init__._is_torch_function_enabled()->_bool
torch._C.__init__._is_tracing()->_bool
torch._C.__init__._is_upgraders_enabled()->_bool
torch._C.__init__._is_xnnpack_enabled()->_bool
torch._C.__init__._ivalue_tags_match(lhs:ScriptModule,rhs:ScriptModule)->_bool
torch._C.__init__._jit_assert_is_instance(obj:Any,type:JitType)
torch._C.__init__._jit_can_fuse_on_cpu()->_bool
torch._C.__init__._jit_can_fuse_on_cpu_legacy()->_bool
torch._C.__init__._jit_can_fuse_on_gpu()->_bool
torch._C.__init__._jit_cat_wo_conditionals(optimize_cat:_bool)
torch._C.__init__._jit_check_alias_annotation(g:Graph,args:Tuple[Any,...],unqualified_op_name:str)
torch._C.__init__._jit_clear_class_registry()->None
torch._C.__init__._jit_decay_packed_param_input_types(graph:Graph)->None
torch._C.__init__._jit_erase_non_input_shape_information(Graph)->None
torch._C.__init__._jit_flatten(arg:Any)->Tuple[List[Tensor], IODescriptor]
torch._C.__init__._jit_get_emit_hooks()->Tuple[Callable, Callable]
torch._C.__init__._jit_get_inline_everything_mode()->_bool
torch._C.__init__._jit_get_logging_option()->str
torch._C.__init__._jit_get_operation(op_name:str)->Callable
torch._C.__init__._jit_get_schemas_for_operator(name:str)->List[FunctionSchema]
torch._C.__init__._jit_get_trigger_value(trigger_name:str)->_int
torch._C.__init__._jit_init()->_bool
torch._C.__init__._jit_is_script_object(obj:Any)->_bool
torch._C.__init__._jit_nvfuser_enabled()->_bool
torch._C.__init__._jit_onnx_convert_pattern_from_subblock(block:Block,n:Node,env:Dict[Value,Value])->List[Value]
torch._C.__init__._jit_onnx_list_model_parameters(module:ScriptModule)->Tuple[ScriptModule, List[IValue]]
torch._C.__init__._jit_opt_conditionals(opt_conds:_bool)
torch._C.__init__._jit_override_can_fuse_on_cpu(override:_bool)
torch._C.__init__._jit_override_can_fuse_on_cpu_legacy(override:_bool)
torch._C.__init__._jit_override_can_fuse_on_gpu(override:_bool)
torch._C.__init__._jit_pass_canonicalize(graph:Graph)
torch._C.__init__._jit_pass_canonicalize_graph_fuser_ops(graph:Graph)->None
torch._C.__init__._jit_pass_common_expression_hoisting(Graph)->None
torch._C.__init__._jit_pass_concat_frozen_linear(graph:Graph)
torch._C.__init__._jit_pass_constant_propagation(Graph)->None
torch._C.__init__._jit_pass_convert_frozen_ops_to_mkldnn(graph:Graph)
torch._C.__init__._jit_pass_custom_pattern_based_rewrite_graph(pattern:str,fused_node_name:str,graph:Graph)->None
torch._C.__init__._jit_pass_dce(Graph)->None
torch._C.__init__._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph:Graph)->None
torch._C.__init__._jit_pass_erase_number_types(graph:Graph)->None
torch._C.__init__._jit_pass_erase_shape_information(graph:Graph)
torch._C.__init__._jit_pass_filter_non_tensor_arguments(params:Dict[str,IValue])->Dict[str, Tensor]
torch._C.__init__._jit_pass_fixup_onnx_controlflow_node(n:Node,opset_version:_int)->Node
torch._C.__init__._jit_pass_fold_convbn(module:'torch.jit.ScriptModule')
torch._C.__init__._jit_pass_fold_frozen_conv_add_or_sub(graph:Graph)
torch._C.__init__._jit_pass_fold_frozen_conv_bn(graph:Graph)
torch._C.__init__._jit_pass_fold_frozen_conv_mul_or_div(graph:Graph)
torch._C.__init__._jit_pass_fuse_addmm(graph:Graph)->None
torch._C.__init__._jit_pass_fuse_frozen_conv_add_relu(graph:Graph)
torch._C.__init__._jit_pass_inline(Graph)->None
torch._C.__init__._jit_pass_inline_fork_wait(graph:Graph)->None
torch._C.__init__._jit_pass_insert_observers(module:'torch.jit.ScriptModule',method_name:str,qconfig_dict:Dict[str,Any],inplace:_bool,quant_type:_int)
torch._C.__init__._jit_pass_insert_quant_dequant(module:'torch.jit.ScriptModule',method_name:str,inplace:_bool,debug:_bool,quant_type:_int)
torch._C.__init__._jit_pass_lint(Graph)->None
torch._C.__init__._jit_pass_lower_all_tuples(graph:Graph)->None
torch._C.__init__._jit_pass_lower_graph(graph:Graph,m:Module)->Tuple[Graph, List[IValue]]
torch._C.__init__._jit_pass_metal_optimize_for_mobile(module:'torch.jit.ScriptModule',preserved_methods:List[AnyStr])->'torch.jit.ScriptModule'
torch._C.__init__._jit_pass_onnx(graph:Graph,_jit_pass_onnx:_onnx.OperatorExportTypes)->Graph
torch._C.__init__._jit_pass_onnx_assign_output_shape(graph:Graph,tensors:List[Tensor],desc:IODescriptor,onnx_shape_inference:_bool=False)->None
torch._C.__init__._jit_pass_onnx_block(old_block:Block,new_block:Block,operator_export_type:_onnx.OperatorExportTypes,env:Dict[Value,Value],is_sub_block:_bool)->Dict[Value, Value]
torch._C.__init__._jit_pass_onnx_cast_all_constant_to_floating(graph:Graph)->None
torch._C.__init__._jit_pass_onnx_constant_fold(graph:Graph,paramsDict:Dict[str,IValue],opset_version:_int)->Dict[str, IValue]
torch._C.__init__._jit_pass_onnx_eliminate_unused_items(graph:Graph,paramsDict:Dict[str,IValue])->Dict[str, IValue]
torch._C.__init__._jit_pass_onnx_eval_peephole(graph:Graph,paramsDict:Dict[str,IValue])->Dict[str, IValue]
torch._C.__init__._jit_pass_onnx_function_extraction(graph:Graph,module_names:Set[str],param_names:List[str])->Dict[Node, Dict[str, str]]
torch._C.__init__._jit_pass_onnx_function_substitution(graph:Graph)->None
torch._C.__init__._jit_pass_onnx_graph_shape_type_inference(graph:Graph,paramsDict:Dict[str,IValue],opset_version:_int)->None
torch._C.__init__._jit_pass_onnx_lint(graph:Graph)->None
torch._C.__init__._jit_pass_onnx_node_shape_type_inference(n:Node,paramsDict:Dict[str,IValue],opset_version:_int)->None
torch._C.__init__._jit_pass_onnx_peephole(graph:Graph,opset_version:_int,fixed_batch_size:_bool)->None
torch._C.__init__._jit_pass_onnx_preprocess(graph:Graph)->None
torch._C.__init__._jit_pass_onnx_preprocess_caffe2(graph:Graph)->None
torch._C.__init__._jit_pass_onnx_quantization_insert_permutes(graph:Graph,paramsDict:Dict[str,IValue])->Dict[str, IValue]
torch._C.__init__._jit_pass_onnx_remove_inplace_ops_for_onnx(graph:Graph,module:Module)->None
torch._C.__init__._jit_pass_onnx_remove_print(graph:Graph)->None
torch._C.__init__._jit_pass_onnx_scalar_type_analysis(graph:Graph,lowprecision_cast:_bool,opset_version:_int)->None
torch._C.__init__._jit_pass_onnx_set_dynamic_input_shape(graph:Graph,dynamic_axes:Dict[str,Dict[_int,str]],input_names:List[str])->None
torch._C.__init__._jit_pass_onnx_unpack_quantized_weights(graph:Graph,paramsDict:Dict[str,IValue])->Dict[str, IValue]
torch._C.__init__._jit_pass_optimize_for_inference(module:'torch.jit.ScriptModule',other_methods:List[str]=[])->None
torch._C.__init__._jit_pass_optimize_for_mobile(module:'torch.jit.ScriptModule',optimization_blocklist:Set[MobileOptimizerType],preserved_methods:List[AnyStr])->'torch.jit.ScriptModule'
torch._C.__init__._jit_pass_optimize_frozen_graph(Graph,optimize_numerics:_bool=True)->None
torch._C.__init__._jit_pass_peephole(graph:Graph,addmm_fusion_enabled:_bool)->None
torch._C.__init__._jit_pass_prepare_division_for_onnx(graph:Graph)->None
torch._C.__init__._jit_pass_propagate_shapes_on_graph(Graph)->None
torch._C.__init__._jit_pass_quant_finalize(module:'torch.jit.ScriptModule',quant_type:_int,preserved_attrs:Sequence[str])
torch._C.__init__._jit_pass_remove_dropout(module:'torch.jit.ScriptModule')
torch._C.__init__._jit_pass_remove_inplace_ops(graph:Graph)->None
torch._C.__init__._jit_pass_transpose_frozen_linear(graph:Graph)
torch._C.__init__._jit_pass_vulkan_optimize_for_mobile(module:'torch.jit.ScriptModule',preserved_methods:List[AnyStr])->'torch.jit.ScriptModule'
torch._C.__init__._jit_script_class_compile(qual_name:str,definition:ClassDef,defaults:Dict[str,Dict[str,Any]],rcb:ResolutionCallback)
torch._C.__init__._jit_script_compile(qual_name:str,definition:Def,rcb:ResolutionCallback,defaults:Dict[str,Any])
torch._C.__init__._jit_script_compile_overload(qualname:str,overload_decl:Decl,implementation_def:Def,rcb:ResolutionCallback,implementation_defaults:Dict[str,Any],signature:Any)
torch._C.__init__._jit_script_interface_compile(name:str,class_def:ClassDef,rcb:ResolutionCallback,is_module:_bool)
torch._C.__init__._jit_set_emit_hooks(ModuleHook:Optional[Callable],FunctionHook:Optional[Callable])->None
torch._C.__init__._jit_set_fusion_strategy(strategy:List[Tuple[str,_int]])->List[Tuple[str, _int]]
torch._C.__init__._jit_set_inline_everything_mode(enabled:_bool)->None
torch._C.__init__._jit_set_logging_option(option:str)->None
torch._C.__init__._jit_set_logging_stream(stream_name:str)->None
torch._C.__init__._jit_set_num_profiled_runs(num:_size)->_size
torch._C.__init__._jit_set_nvfuser_enabled(enable:_bool)->_bool
torch._C.__init__._jit_set_profiling_executor(profiling_flag:_bool)->_bool
torch._C.__init__._jit_set_profiling_mode(profiling_flag:_bool)->_bool
torch._C.__init__._jit_set_symbolic_shapes_test_mode(override:_bool)
torch._C.__init__._jit_set_te_must_use_llvm_cpu(use_llvm:_bool)
torch._C.__init__._jit_set_texpr_fuser_enabled(enable:_bool)
torch._C.__init__._jit_symbolic_shapes_test_mode_enabled()->_bool
torch._C.__init__._jit_texpr_fuser_enabled()->_bool
torch._C.__init__._jit_try_infer_type(obj:Any)->InferredType
torch._C.__init__._jit_unflatten(vars:List[Tensor],desc:IODescriptor)->Any
torch._C.__init__._last_executed_optimized_graph()->Graph
torch._C.__init__._llvm_enabled()->_bool
torch._C.__init__._load_for_lite_interpreter(filename:Union[str,Path],map_location:Union[_device,str,None])
torch._C.__init__._load_for_lite_interpreter_from_buffer(buffer:BinaryIO,map_location:Union[_device,str,None])
torch._C.__init__._log_api_usage_once(str)->None
torch._C.__init__._logging_set_logger(logger:LoggerBase)->LoggerBase
torch._C.__init__._make_dual(tensor:Tensor,tangent:Tensor,level:_int)->Tensor
torch._C.__init__._multiprocessing_init()->None
torch._C.__init__._nccl_all_gather(input:Sequence[Tensor],output:Sequence[Tensor],streams:Optional[Sequence[_CudaStreamBase]],comms:Optional[Sequence[object]])->None
torch._C.__init__._nccl_all_reduce(input:Sequence[Tensor],output:Sequence[Tensor],op:_int,streams:Optional[Sequence[_CudaStreamBase]],comms:Optional[Sequence[object]])->None
torch._C.__init__._nccl_broadcast(input:Sequence[Tensor],root:_int,streams:Optional[Sequence[_CudaStreamBase]],comms:Optional[Sequence[object]])->None
torch._C.__init__._nccl_init_rank(nranks:_int,comm_id:bytes,rank:_int)->object
torch._C.__init__._nccl_reduce(input:Sequence[Tensor],output:Tensor,root:_int,op:_int,streams:Optional[Sequence[_CudaStreamBase]],comms:Optional[Sequence[object]])->None
torch._C.__init__._nccl_reduce_scatter(input:Sequence[Tensor],output:Sequence[Tensor],op:_int,streams:Optional[Sequence[_CudaStreamBase]],comms:Optional[Sequence[object]])->None
torch._C.__init__._nccl_unique_id()->bytes
torch._C.__init__._nccl_version()->_int
torch._C.__init__._parallel_info()->str
torch._C.__init__._parse_source_def(src:str)->Def
torch._C.__init__._propagate_and_assign_input_shapes(graph:Graph,inputs:Tuple[Tensor,...],param_count_list:List[_int],with_grad:_bool,propagate:_bool)->Graph
torch._C.__init__._register_default_hooks(pack_hook:Callable,unpack_hook:Callable)->None
torch._C.__init__._register_py_class_for_device(device:str,cls:Any)->None
torch._C.__init__._remove_worker_pids(loader_id:_int)->None
torch._C.__init__._replace_overloaded_method_decl(overload_decl:Decl,implementation_def:Def,new_name:str)->Def
torch._C.__init__._reset_default_hooks()->None
torch._C.__init__._resolve_type_from_object(obj:Any,range:SourceRange,rcb:ResolutionCallback)->JitType
torch._C.__init__._rpc_init()->_bool
torch._C.__init__._run_emit_module_hook(m:ScriptModule)
torch._C.__init__._scatter(tensor:Tensor,devices:List[_int],chunk_sizes:Optional[List[_int]],dim:_int,streams:Optional[List[Stream]])->List[Tensor]
torch._C.__init__._scatter_out(tensor:Tensor,out_tensors:List[Tensor],dim:_int,streams:Optional[List[Stream]])->List[Tensor]
torch._C.__init__._set_backcompat_broadcast_warn(arg:_bool)->None
torch._C.__init__._set_backcompat_keepdim_warn(arg:_bool)->None
torch._C.__init__._set_cublas_allow_fp16_reduced_precision_reduction(arg:_bool)->None
torch._C.__init__._set_cublas_allow_tf32(arg:_bool)->None
torch._C.__init__._set_cudnn_allow_tf32(arg:_bool)->None
torch._C.__init__._set_cudnn_benchmark(arg:_bool)->None
torch._C.__init__._set_cudnn_deterministic(arg:_bool)->None
torch._C.__init__._set_cudnn_enabled(arg:_bool)->None
torch._C.__init__._set_default_dtype(d:_dtype)->None
torch._C.__init__._set_default_mobile_cpu_allocator()->None
torch._C.__init__._set_default_tensor_type(type)->None
torch._C.__init__._set_deterministic_algorithms(mode:_bool,*,warn_only:_bool=...)->None
torch._C.__init__._set_grad_enabled(enabled:_bool)->None
torch._C.__init__._set_graph_executor_optimize(optimize:_bool)
torch._C.__init__._set_linalg_preferred_backend(arg:torch._C._LinalgBackend)
torch._C.__init__._set_mkldnn_enabled(arg:_bool)->None
torch._C.__init__._set_print_stack_traces_on_fatal_signal(print:_bool)->None
torch._C.__init__._set_qengine(qegine:_int)->None
torch._C.__init__._set_warnAlways(arg:_bool)->None
torch._C.__init__._set_worker_pids(key:_int,child_pids:Tuple[_int,...])->None
torch._C.__init__._set_worker_signal_handlers(*arg:Any)->None
torch._C.__init__._show_config()->str
torch._C.__init__._supported_qengines()->List[_int]
torch._C.__init__._test_only_add_entry_to_op_version(op_name:str,entry:_UpgraderEntry)->None
torch._C.__init__._test_only_populate_upgraders(content:Dict[str,str])->None
torch._C.__init__._test_only_remove_entry_to_op_version(op_name:str)->None
torch._C.__init__._test_only_remove_upgraders(content:Dict[str,str])->None
torch._C.__init__._to_dlpack(data:Tensor)->Any
torch._C.__init__._tracer_warn_use_python()
torch._C.__init__._unpack_dual(tensor:Tensor,level:_int)->Tensor
torch._C.__init__._unset_default_mobile_cpu_allocator()->None
torch._C.__init__._valgrind_supported_platform()->_bool
torch._C.__init__._valgrind_toggle()->None
torch._C.__init__._valgrind_toggle_and_dump_stats()->None
torch._C.__init__._vmapmode_decrement_nesting()->_int
torch._C.__init__._vmapmode_increment_nesting()->_int
torch._C.__init__.autocast_decrement_nesting()->_int
torch._C.__init__.autocast_increment_nesting()->_int
torch._C.__init__.clear_autocast_cache()->None
torch._C.__init__.device(self,type:str,index:_int)
torch._C.__init__.device.__get__(self,instance,owner=None)->device
torch._C.__init__.device.__init__(self,type:str,index:_int)
torch._C.__init__.device.__reduce__(self)->Tuple[Any, ...]
torch._C.__init__.dtype
torch._C.__init__.finfo(self)
torch._C.__init__.finfo.__init__(self)
torch._C.__init__.fork(*args:Any,**kwargs:Any)->Future
torch._C.__init__.get_autocast_cpu_dtype()->_dtype
torch._C.__init__.get_autocast_gpu_dtype()->_dtype
torch._C.__init__.get_default_dtype()->_dtype
torch._C.__init__.get_device(input:Tensor)->_int
torch._C.__init__.get_num_interop_threads()->_int
torch._C.__init__.get_num_thread()->_int
torch._C.__init__.iinfo(self,dtype:_dtype)
torch._C.__init__.iinfo.__init__(self,dtype:_dtype)
torch._C.__init__.import_ir_module(cu:CompilationUnit,filename:Union[str,Path],map_location:Union[_device,str,None],extra_files:Dict[str,Any])->ScriptModule
torch._C.__init__.import_ir_module_from_buffer(cu:CompilationUnit,buffer:BinaryIO,map_location:Union[_device,str,None],extra_files:Dict[str,Any])->ScriptModule
torch._C.__init__.is_anomaly_enabled()->_bool
torch._C.__init__.is_autocast_cache_enabled()->_bool
torch._C.__init__.is_autocast_cpu_enabled()->_bool
torch._C.__init__.is_autocast_enabled()->_bool
torch._C.__init__.is_grad_enabled()->_bool
torch._C.__init__.is_inference_mode_enabled()->_bool
torch._C.__init__.layout
torch._C.__init__.memory_format
torch._C.__init__.merge_type_from_type_comment(decl:Decl,type_annotation_decl:Decl,is_method:_bool)->Decl
torch._C.__init__.parse_ir(input:str)->Graph
torch._C.__init__.parse_schema(schema:str)->FunctionSchema
torch._C.__init__.parse_type_comment(comment:str)->Decl
torch._C.__init__.qscheme
torch._C.__init__.set_anomaly_enabled(enabled:_bool)->None
torch._C.__init__.set_autocast_cache_enabled(enabled:_bool)->None
torch._C.__init__.set_autocast_cpu_dtype(dtype:_dtype)->None
torch._C.__init__.set_autocast_cpu_enabled(enabled:_bool)->None
torch._C.__init__.set_autocast_enabled(enabled:_bool)->None
torch._C.__init__.set_autocast_gpu_dtype(dtype:_dtype)->None
torch._C.__init__.set_flush_denormal(arg:_bool)->_bool
torch._C.__init__.set_num_interop_threads(nthreads:_int)->None
torch._C.__init__.set_num_threads(nthreads:_int)->None
torch._C.__init__.unify_type_list(types:List[JitType])->JitType
torch._C.__init__.wait(fut:Future)->Any
torch.__init__.AliasDb
torch.__init__.AliasDb.__str__(self)->str
torch.__init__.Block
torch.__init__.CallStack(self,name:str,range:SourceRange)
torch.__init__.Graph
torch.__init__.Graph.alias_db(self)->AliasDb
torch.__init__.Graph.inputs(self)->List[Value]
torch.__init__.ScriptFunction(self,*args,**kwargs)
torch.__init__.ScriptFunction.graph(self)->Graph
torch.__init__._CUDAGraph
torch.__init__._CUDAGraph.pool(self)->Tuple[_int, _int]
torch.__init__._add_docstr(obj:T,doc_obj:str)->T
torch.__init__._broadcast(tensor:Tensor,devices:List[_int])->List[Tensor]
torch.__init__._broadcast_out(tensor:Tensor,out_tensors:List[Tensor])->List[Tensor]
torch.__init__._c10d_init()->_bool
torch.__init__._crash_if_csrc_asan()->_int
torch.__init__._crash_if_csrc_ubsan()->_int
torch.__init__._cxx_flags()->str
torch.__init__._from_dlpack(data:Any)->Tensor
torch.__init__._has_torch_function(args:Iterable[Any])->_bool
torch.__init__._has_torch_function_unary(Any)->_bool
torch.__init__._has_torch_function_variadic(*args:Any)->_bool
torch.__init__._is_tracing()->_bool
torch.__init__._nccl_broadcast(input:Sequence[Tensor],root:_int,streams:Optional[Sequence[_CudaStreamBase]],comms:Optional[Sequence[object]])->None
torch.__init__._nccl_init_rank(nranks:_int,comm_id:bytes,rank:_int)->object
torch.__init__._rpc_init()->_bool
torch.__init__._show_config()->str
torch.__init__._to_dlpack(data:Tensor)->Any
torch.__init__._unpack_dual(tensor:Tensor,level:_int)->Tensor
torch.__init__.fork(*args:Any,**kwargs:Any)->Future
torch.__init__.layout
torch.__init__.wait(fut:Future)->Any


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_VariableFunctions.pyi----------------------------------------
torch._C._VariableFunctions.__and__(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.__and__(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.__lshift__(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.__lshift__(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.__or__(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.__or__(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.__rshift__(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.__rshift__(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.__xor__(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.__xor__(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions._adaptive_avg_pool2d(input:Tensor,output_size:Union[_int,_size])->Tensor
torch._C._VariableFunctions._adaptive_avg_pool3d(input:Tensor,output_size:Union[_int,_size])->Tensor
torch._C._VariableFunctions._add_batch_dim(input:Tensor,batch_dim:_int,level:_int)->Tensor
torch._C._VariableFunctions._add_relu(input:Tensor,other:Number,alpha:Number=1)->Tensor
torch._C._VariableFunctions._add_relu(input:Tensor,other:Tensor,*,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._add_relu_(input:Tensor,other:Number,alpha:Number=1)->Tensor
torch._C._VariableFunctions._add_relu_(input:Tensor,other:Tensor,*,alpha:Number=1)->Tensor
torch._C._VariableFunctions._aminmax(input:Tensor)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._aminmax(input:Tensor,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._amp_foreach_non_finite_check_and_unscale_(self:Union[Tuple[Tensor,...],List[Tensor]],found_inf:Tensor,inv_scale:Tensor)->None
torch._C._VariableFunctions._amp_update_scale_(input:Tensor,growth_tracker:Tensor,found_inf:Tensor,scale_growth_factor:_float,scale_backoff_factor:_float,growth_interval:_int)->Tensor
torch._C._VariableFunctions._assert_async(input:Tensor)->None
torch._C._VariableFunctions._batch_norm_impl_index(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tuple[Tensor, Tensor, Tensor, Tensor, _int]
torch._C._VariableFunctions._cast_Byte(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Char(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Double(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Float(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Half(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Int(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Long(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cast_Short(input:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._choose_qparams_per_tensor(input:Tensor,reduce_range:_bool=False)->Tuple[_float, _int]
torch._C._VariableFunctions._coalesce(input:Tensor)->Tensor
torch._C._VariableFunctions._compute_linear_combination(input:Tensor,coefficients:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._conj(input:Tensor)->Tensor
torch._C._VariableFunctions._conj_physical(input:Tensor)->Tensor
torch._C._VariableFunctions._convert_indices_from_coo_to_csr(input:Tensor,size:_int,*,out_int32:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._convert_indices_from_csr_to_coo(crow_indices:Tensor,col_indices:Tensor,*,out_int32:_bool=False,transpose:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int,benchmark:_bool,deterministic:_bool,cudnn_enabled:_bool)->Tensor
torch._C._VariableFunctions._convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int,benchmark:_bool,deterministic:_bool,cudnn_enabled:_bool,allow_tf32:_bool)->Tensor
torch._C._VariableFunctions._convolution_mode(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:str,dilation:_size,groups:_int)->Tensor
torch._C._VariableFunctions._copy_from(input:Tensor,dst:Tensor,non_blocking:_bool=False)->Tensor
torch._C._VariableFunctions._copy_from_and_resize(input:Tensor,dst:Tensor)->Tensor
torch._C._VariableFunctions._ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int=0,zero_infinity:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._cudnn_ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int,deterministic:_bool,zero_infinity:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._cudnn_init_dropout_state(dropout:_float,train:_bool,dropout_seed:_int,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._cudnn_rnn(input:Tensor,weight:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,weight_buf:Optional[Tensor],hx:Tensor,cx:Optional[Tensor],mode:_int,hidden_size:_int,proj_size:_int,num_layers:_int,batch_first:_bool,dropout:_float,train:_bool,bidirectional:_bool,batch_sizes:_size,dropout_state:Optional[Tensor])->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions._cudnn_rnn_flatten_weight(weight_arr:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,input_size:_int,mode:_int,hidden_size:_int,proj_size:_int,num_layers:_int,batch_first:_bool,bidirectional:_bool)->Tensor
torch._C._VariableFunctions._cufft_clear_plan_cache(device_index:_int)->None
torch._C._VariableFunctions._cufft_get_plan_cache_max_size(device_index:_int)->_int
torch._C._VariableFunctions._cufft_get_plan_cache_size(device_index:_int)->_int
torch._C._VariableFunctions._cufft_set_plan_cache_max_size(device_index:_int,max_size:_int)->None
torch._C._VariableFunctions._cummax_helper(input:Tensor,values:Tensor,indices:Tensor,dim:_int)->None
torch._C._VariableFunctions._cummin_helper(input:Tensor,values:Tensor,indices:Tensor,dim:_int)->None
torch._C._VariableFunctions._debug_has_internal_overlap(input:Tensor)->_int
torch._C._VariableFunctions._det_lu_based_helper(input:Tensor)->namedtuple_det_lu_pivs
torch._C._VariableFunctions._det_lu_based_helper_backward_helper(det_grad:Tensor,det:Tensor,input:Tensor,lu:Tensor,pivs:Tensor)->Tensor
torch._C._VariableFunctions._dim_arange(like:Tensor,dim:_int)->Tensor
torch._C._VariableFunctions._dirichlet_grad(x:Tensor,alpha:Tensor,total:Tensor)->Tensor
torch._C._VariableFunctions._efficientzerotensor(*size:_int,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._efficientzerotensor(size:_size,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None,include_last_offset:_bool=False,padding_idx:_int=-1)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions._embedding_bag_forward_only(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None,include_last_offset:_bool=False,padding_idx:_int=-1)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions._empty_affine_quantized(*size:_int,scale:_float=1,zero_point:_int=0,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._empty_affine_quantized(size:_size,*,scale:_float=1,zero_point:_int=0,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._empty_per_channel_affine_quantized(*size:_int,scales:Tensor,zero_points:Tensor,axis:_int,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._empty_per_channel_affine_quantized(size:_size,*,scales:Tensor,zero_points:Tensor,axis:_int,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions._euclidean_dist(x1:Tensor,x2:Tensor)->Tensor
torch._C._VariableFunctions._fake_quantize_learnable_per_channel_affine(input:Tensor,scale:Tensor,zero_point:Tensor,axis:_int,quant_min:_int,quant_max:_int,grad_factor:_float=1.0)->Tensor
torch._C._VariableFunctions._fake_quantize_learnable_per_tensor_affine(input:Tensor,scale:Tensor,zero_point:Tensor,quant_min:_int,quant_max:_int,grad_factor:_float=1.0)->Tensor
torch._C._VariableFunctions._fake_quantize_per_tensor_affine_cachemask_tensor_qparams(input:Tensor,scale:Tensor,zero_point:Tensor,fake_quant_enabled:Tensor,quant_min:_int,quant_max:_int)->namedtuple_output_mask
torch._C._VariableFunctions._fft_c2c(input:Tensor,dim:_size,normalization:_int,forward:_bool,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._fft_c2r(input:Tensor,dim:_size,normalization:_int,last_dim_size:_int,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._fft_r2c(input:Tensor,dim:_size,normalization:_int,onesided:_bool,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._foreach_abs(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_abs_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_acos(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_acos_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_add(tensors1:Union[Tuple[Tensor,...],List[Tensor]],tensors2:Union[Tuple[Tensor,...],List[Tensor]],*,alpha:Number=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_add(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_add(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_add_(self:Union[Tuple[Tensor,...],List[Tensor]],other:Union[Tuple[Tensor,...],List[Tensor]],*,alpha:Number=1)->None
torch._C._VariableFunctions._foreach_add_(self:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->None
torch._C._VariableFunctions._foreach_add_(self:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->None
torch._C._VariableFunctions._foreach_addcdiv(input:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_addcdiv(input:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],value:Number=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_addcdiv_(self:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->None
torch._C._VariableFunctions._foreach_addcdiv_(self:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],value:Number=1)->None
torch._C._VariableFunctions._foreach_addcmul(input:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_addcmul(input:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],value:Number=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_addcmul_(self:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->None
torch._C._VariableFunctions._foreach_addcmul_(self:Union[Tuple[Tensor,...],List[Tensor]],tensor1:Union[Tuple[Tensor,...],List[Tensor]],tensor2:Union[Tuple[Tensor,...],List[Tensor]],value:Number=1)->None
torch._C._VariableFunctions._foreach_asin(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_asin_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_atan(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_atan_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_ceil(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_ceil_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_cos(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_cos_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_cosh(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_cosh_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_div(tensors1:Union[Tuple[Tensor,...],List[Tensor]],tensors2:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_div(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_div(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_div_(self:Union[Tuple[Tensor,...],List[Tensor]],other:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_div_(self:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->None
torch._C._VariableFunctions._foreach_div_(self:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->None
torch._C._VariableFunctions._foreach_erf(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_erf_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_erfc(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_erfc_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_exp(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_exp_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_expm1(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_expm1_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_floor(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_floor_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_frac(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_frac_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_lgamma(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_lgamma_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_log(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_log10(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_log10_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_log1p(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_log1p_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_log2(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_log2_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_log_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_maximum(tensors1:Union[Tuple[Tensor,...],List[Tensor]],tensors2:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_minimum(tensors1:Union[Tuple[Tensor,...],List[Tensor]],tensors2:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_mul(tensors1:Union[Tuple[Tensor,...],List[Tensor]],tensors2:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_mul(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_mul(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_mul_(self:Union[Tuple[Tensor,...],List[Tensor]],other:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_mul_(self:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->None
torch._C._VariableFunctions._foreach_mul_(self:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->None
torch._C._VariableFunctions._foreach_neg(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_neg_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_norm(tensors:Union[Tuple[Tensor,...],List[Tensor]],ord:Number=2)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_reciprocal(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_reciprocal_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_round(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_round_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_sigmoid(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sigmoid_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_sin(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sin_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_sinh(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sinh_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_sqrt(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sqrt_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_sub(tensors1:Union[Tuple[Tensor,...],List[Tensor]],tensors2:Union[Tuple[Tensor,...],List[Tensor]],*,alpha:Number=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sub(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sub(tensors:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_sub_(self:Union[Tuple[Tensor,...],List[Tensor]],other:Union[Tuple[Tensor,...],List[Tensor]],*,alpha:Number=1)->None
torch._C._VariableFunctions._foreach_sub_(self:Union[Tuple[Tensor,...],List[Tensor]],scalar:Number)->None
torch._C._VariableFunctions._foreach_sub_(self:Union[Tuple[Tensor,...],List[Tensor]],scalars:Sequence[Number])->None
torch._C._VariableFunctions._foreach_tan(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_tan_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_tanh(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_tanh_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_trunc(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._foreach_trunc_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._foreach_zero_(self:Union[Tuple[Tensor,...],List[Tensor]])->None
torch._C._VariableFunctions._fused_dropout(input:Tensor,p:_float,generator:Optional[Generator]=None)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._fused_moving_avg_obs_fq_helper(input:Tensor,observer_on:Tensor,fake_quant_on:Tensor,running_min:Tensor,running_max:Tensor,scale:Tensor,zero_point:Tensor,averaging_const:_float,quant_min:_int,quant_max:_int,ch_axis:_int,per_row_fake_quant:_bool=False,symmetric_quant:_bool=False)->namedtuple_output_mask
torch._C._VariableFunctions._grid_sampler_2d_cpu_fallback(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch._C._VariableFunctions._has_compatible_shallow_copy_type(input:Tensor,from_:Tensor)->_bool
torch._C._VariableFunctions._histogramdd_bin_edges(input:Tensor,bins:_size,*,range:Optional[Sequence[_float]]=None,weight:Optional[Tensor]=None,density:_bool=False)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._histogramdd_from_bin_cts(input:Tensor,bins:_size,*,range:Optional[Sequence[_float]]=None,weight:Optional[Tensor]=None,density:_bool=False)->Tensor
torch._C._VariableFunctions._histogramdd_from_bin_tensors(input:Tensor,bins:Union[Tuple[Tensor,...],List[Tensor]],*,weight:Optional[Tensor]=None,density:_bool=False)->Tensor
torch._C._VariableFunctions._index_copy_(input:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch._C._VariableFunctions._index_put_impl_(input:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False,unsafe:_bool=False)->Tensor
torch._C._VariableFunctions._is_zerotensor(input:Tensor)->_bool
torch._C._VariableFunctions._linalg_check_errors(info:Tensor,api_name:str,*,is_matrix:_bool)->None
torch._C._VariableFunctions._linalg_inv_out_helper_(input:Tensor,infos_lu:Tensor,infos_getri:Tensor)->Tensor
torch._C._VariableFunctions._linalg_qr_helper(input:Tensor,mode:str)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._linalg_svd(A:Tensor,full_matrices:_bool=False,compute_uv:_bool=True,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_U_S_Vh
torch._C._VariableFunctions._log_softmax(input:Tensor,dim:_int,half_to_float:_bool,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._log_softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,input_dtype:_dtype,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._logcumsumexp(input:Tensor,dim:_int,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._lu_with_info(input:Tensor,pivot:_bool=True,check_errors:_bool=True)->namedtuple_LU_pivots_info
torch._C._VariableFunctions._make_dual(primal:Tensor,tangent:Tensor,level:_int)->Tensor
torch._C._VariableFunctions._make_per_channel_quantized_tensor(input:Tensor,scale:Tensor,zero_point:Tensor,axis:_int)->Tensor
torch._C._VariableFunctions._make_per_tensor_quantized_tensor(input:Tensor,scale:_float,zero_point:_int)->Tensor
torch._C._VariableFunctions._masked_scale(input:Tensor,mask:Tensor,scale:_float)->Tensor
torch._C._VariableFunctions._masked_softmax(input:Tensor,mask:Tensor)->Tensor
torch._C._VariableFunctions._mkldnn_reshape(input:Tensor,shape:_size)->Tensor
torch._C._VariableFunctions._mkldnn_transpose(input:Tensor,dim0:_int,dim1:_int)->Tensor
torch._C._VariableFunctions._mkldnn_transpose_(input:Tensor,dim0:_int,dim1:_int)->Tensor
torch._C._VariableFunctions._native_multi_head_self_attention(query:Tensor,qkv_weight:Tensor,qkv_bias:Tensor,proj_weight:Tensor,proj_bias:Tensor,mask:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._neg_view(input:Tensor)->Tensor
torch._C._VariableFunctions._nnpack_available()->_bool
torch._C._VariableFunctions._nnpack_spatial_convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:Union[_int,_size],stride:Union[_int,_size]=1)->Tensor
torch._C._VariableFunctions._pack_padded_sequence(input:Tensor,lengths:Tensor,batch_first:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._pad_packed_sequence(data:Tensor,batch_sizes:Tensor,batch_first:_bool,padding_value:Number,total_length:_int)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._pin_memory(input:Tensor,device:Optional[Union[_device,str,None]]=None)->Tensor
torch._C._VariableFunctions._remove_batch_dim(input:Tensor,level:_int,batch_size:_int,out_dim:_int)->Tensor
torch._C._VariableFunctions._reshape_from_tensor(input:Tensor,shape:Tensor)->Tensor
torch._C._VariableFunctions._rowwise_prune(weight:Tensor,mask:Tensor,compressed_indices_dtype:_dtype)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._s_where(condition:Tensor,input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions._sample_dirichlet(input:Tensor,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions._saturate_weight_to_fp16(weight:Tensor)->Tensor
torch._C._VariableFunctions._shape_as_tensor(input:Tensor)->Tensor
torch._C._VariableFunctions._sobol_engine_draw(quasi:Tensor,n:_int,sobolstate:Tensor,dimension:_int,num_generated:_int,dtype:Optional[_dtype])->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._sobol_engine_ff_(input:Tensor,n:_int,sobolstate:Tensor,dimension:_int,num_generated:_int)->Tensor
torch._C._VariableFunctions._sobol_engine_initialize_state_(input:Tensor,dimension:_int)->Tensor
torch._C._VariableFunctions._sobol_engine_scramble_(input:Tensor,ltm:Tensor,dimension:_int)->Tensor
torch._C._VariableFunctions._softmax(input:Tensor,dim:_int,half_to_float:_bool,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,input_dtype:_dtype,*,grad_input:Tensor=None)->Tensor
torch._C._VariableFunctions._sparse_broadcast_to(input:Tensor,size:_size)->Tensor
torch._C._VariableFunctions._sparse_coo_tensor_unsafe(indices:Tensor,values:Tensor,size:List[int],dtype:Optional[_dtype]=None,device:Optional[_device]=None,requires_grad:bool=False)->Tensor
torch._C._VariableFunctions._sparse_csr_tensor_unsafe(crow_indices:Union[Tensor,List],col_indices:Union[Tensor,List],values:Union[Tensor,List],size:List[int],dtype:Optional[_dtype]=None,device:Optional[_device]=None,requires_grad:bool=False)->Tensor
torch._C._VariableFunctions._sparse_log_softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,input:Tensor)->Tensor
torch._C._VariableFunctions._sparse_mask_helper(t:Tensor,mask_indices:Tensor)->Tensor
torch._C._VariableFunctions._sparse_mm(sparse:Tensor,dense:Tensor)->Tensor
torch._C._VariableFunctions._sparse_softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,input:Tensor)->Tensor
torch._C._VariableFunctions._sparse_sparse_matmul(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions._sparse_sum(input:Tensor)->Tensor
torch._C._VariableFunctions._sparse_sum(input:Tensor,*,dtype:_dtype)->Tensor
torch._C._VariableFunctions._sparse_sum(input:Tensor,dim:Union[_int,_size])->Tensor
torch._C._VariableFunctions._sparse_sum(input:Tensor,dim:Union[_int,_size],*,dtype:_dtype)->Tensor
torch._C._VariableFunctions._stack(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions._standard_gamma(input:Tensor,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions._standard_gamma_grad(input:Tensor,output:Tensor)->Tensor
torch._C._VariableFunctions._test_serialization_subcmul(input:Tensor,other:Tensor,alpha:Number=1)->Tensor
torch._C._VariableFunctions._to_cpu(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions._torch_cuda_cu_linker_symbol_op(input:Tensor)->Tensor
torch._C._VariableFunctions._trilinear(i1:Tensor,i2:Tensor,i3:Tensor,expand1:_size,expand2:_size,expand3:_size,sumdim:_size,unroll_dim:_int=1)->Tensor
torch._C._VariableFunctions._unique(input:Tensor,sorted:_bool=True,return_inverse:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions._unique2(input:Tensor,sorted:_bool=True,return_inverse:_bool=False,return_counts:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions._unpack_dual(dual:Tensor,level:_int)->namedtuple_primal_tangent
torch._C._VariableFunctions._use_cudnn_ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int)->_bool
torch._C._VariableFunctions._use_cudnn_rnn_flatten_weight()->_bool
torch._C._VariableFunctions._validate_sparse_coo_tensor_args(indices:Tensor,values:Tensor,size:_size)->None
torch._C._VariableFunctions._validate_sparse_csr_tensor_args(crow_indices:Tensor,col_indices:Tensor,values:Tensor,size:_size)->None
torch._C._VariableFunctions._weight_norm(v:Tensor,g:Tensor,dim:_int=0)->Tensor
torch._C._VariableFunctions._weight_norm_cuda_interface(v:Tensor,g:Tensor,dim:_int=0)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.abs(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.abs_(input:Tensor)->Tensor
torch._C._VariableFunctions.absolute(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.acos(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.acos_(input:Tensor)->Tensor
torch._C._VariableFunctions.acosh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.acosh_(input:Tensor)->Tensor
torch._C._VariableFunctions.adaptive_avg_pool1d(input:Tensor,output_size:Union[_int,_size])->Tensor
torch._C._VariableFunctions.adaptive_max_pool1d(input:Tensor,output_size:Union[_int,_size])->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.add(input:Union[Tensor,Number],other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.add(self:Tensor,alpha:Number,other:Tensor)->Tensor
torch._C._VariableFunctions.add(self:Tensor,alpha:Number,other:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor)->Tensor
torch._C._VariableFunctions.addbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor)->Tensor
torch._C._VariableFunctions.addbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addbmm(input:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.addcdiv(input:Tensor,tensor1:Tensor,tensor2:Tensor,*,value:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.addcdiv(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor)->Tensor
torch._C._VariableFunctions.addcdiv(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addcmul(input:Tensor,tensor1:Tensor,tensor2:Tensor,*,value:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.addcmul(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor)->Tensor
torch._C._VariableFunctions.addcmul(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.addmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.addmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addmm(input:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.addmv(beta:Number,self:Tensor,alpha:Number,mat:Tensor,vec:Tensor)->Tensor
torch._C._VariableFunctions.addmv(beta:Number,self:Tensor,alpha:Number,mat:Tensor,vec:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addmv(beta:Number,self:Tensor,mat:Tensor,vec:Tensor)->Tensor
torch._C._VariableFunctions.addmv(beta:Number,self:Tensor,mat:Tensor,vec:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addmv(input:Tensor,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.addmv_(input:Tensor,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch._C._VariableFunctions.addr(beta:Number,self:Tensor,alpha:Number,vec1:Tensor,vec2:Tensor)->Tensor
torch._C._VariableFunctions.addr(beta:Number,self:Tensor,alpha:Number,vec1:Tensor,vec2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addr(beta:Number,self:Tensor,vec1:Tensor,vec2:Tensor)->Tensor
torch._C._VariableFunctions.addr(beta:Number,self:Tensor,vec1:Tensor,vec2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.addr(input:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.adjoint(input:Tensor)->Tensor
torch._C._VariableFunctions.affine_grid_generator(theta:Tensor,size:_size,align_corners:_bool)->Tensor
torch._C._VariableFunctions.all(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.all(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.all(input:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.allclose(input:Tensor,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->_bool
torch._C._VariableFunctions.alpha_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.alpha_dropout_(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.amax(input:Tensor,dim:Union[_int,_size]=(),keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.amin(input:Tensor,dim:Union[_int,_size]=(),keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.aminmax(input:Tensor,*,dim:Optional[_int]=None,keepdim:_bool=False,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_min_max
torch._C._VariableFunctions.angle(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.any(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.any(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.any(input:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arange(end:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.arange(start:Number,end:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.arange(start:Number,end:Number,step:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.arccos(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arccos_(input:Tensor)->Tensor
torch._C._VariableFunctions.arccosh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arccosh_(input:Tensor)->Tensor
torch._C._VariableFunctions.arcsin(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arcsin_(input:Tensor)->Tensor
torch._C._VariableFunctions.arcsinh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arcsinh_(input:Tensor)->Tensor
torch._C._VariableFunctions.arctan(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arctan2(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arctan_(input:Tensor)->Tensor
torch._C._VariableFunctions.arctanh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.arctanh_(input:Tensor)->Tensor
torch._C._VariableFunctions.argmax(input:Tensor,dim:Optional[_int]=None,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.argmin(input:Tensor,dim:Optional[_int]=None,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.argsort(input:Tensor,dim:Union[str,ellipsis,None],descending:_bool=False)->Tensor
torch._C._VariableFunctions.argsort(input:Tensor,dim:_int=-1,descending:_bool=False)->Tensor
torch._C._VariableFunctions.argwhere(input:Tensor)->Tensor
torch._C._VariableFunctions.as_strided(input:Tensor,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.as_strided_(input:Tensor,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.as_tensor(data:Any,dtype:_dtype=None,device:Optional[_device]=None)->Tensor
torch._C._VariableFunctions.asarray(obj:Any,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,copy:Optional[_bool]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.asin(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.asin_(input:Tensor)->Tensor
torch._C._VariableFunctions.asinh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.asinh_(input:Tensor)->Tensor
torch._C._VariableFunctions.atan(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.atan2(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.atan_(input:Tensor)->Tensor
torch._C._VariableFunctions.atanh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.atanh_(input:Tensor)->Tensor
torch._C._VariableFunctions.avg_pool1d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,ceil_mode:_bool=False,count_include_pad:_bool=True)->Tensor
torch._C._VariableFunctions.baddbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor)->Tensor
torch._C._VariableFunctions.baddbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.baddbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor)->Tensor
torch._C._VariableFunctions.baddbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.baddbmm(input:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bartlett_window(window_length:_int,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.bartlett_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tensor
torch._C._VariableFunctions.batch_norm_backward_elemt(grad_out:Tensor,input:Tensor,mean:Tensor,invstd:Tensor,weight:Optional[Tensor],mean_dy:Tensor,mean_dy_xmu:Tensor,count:Tensor)->Tensor
torch._C._VariableFunctions.batch_norm_backward_reduce(grad_out:Tensor,input:Tensor,mean:Tensor,invstd:Tensor,weight:Optional[Tensor],input_g:_bool,weight_g:_bool,bias_g:_bool)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions.batch_norm_elemt(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],mean:Tensor,invstd:Tensor,eps:_float,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.batch_norm_gather_stats(input:Tensor,mean:Tensor,invstd:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float,eps:_float,count:_int)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.batch_norm_gather_stats_with_counts(input:Tensor,mean:Tensor,invstd:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float,eps:_float,counts:Tensor)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.batch_norm_stats(input:Tensor,eps:_float)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.batch_norm_update_stats(input:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.bernoulli(input:Tensor,*,generator:Optional[Generator]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bernoulli(input:Tensor,p:_float,*,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions.bilinear(input1:Tensor,input2:Tensor,weight:Tensor,bias:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.binary_cross_entropy_with_logits(input:Tensor,target:Tensor,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str=...,pos_weight:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bincount(input:Tensor,weights:Optional[Tensor]=None,minlength:_int=0)->Tensor
torch._C._VariableFunctions.binomial(count:Tensor,prob:Tensor,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions.bitwise_and(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_and(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_left_shift(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_left_shift(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_left_shift(self:Number,other:Tensor)->Tensor
torch._C._VariableFunctions.bitwise_not(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_or(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_or(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_right_shift(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_right_shift(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_right_shift(self:Number,other:Tensor)->Tensor
torch._C._VariableFunctions.bitwise_xor(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bitwise_xor(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.blackman_window(window_length:_int,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.blackman_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.bmm(input:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.broadcast_to(input:Tensor,size:_size)->Tensor
torch._C._VariableFunctions.bucketize(input:Tensor,boundaries:Tensor,*,out_int32:_bool=False,right:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.bucketize(self:Number,boundaries:Tensor,*,out_int32:_bool=False,right:_bool=False)->Tensor
torch._C._VariableFunctions.can_cast(from_:_dtype,to:_dtype)->_bool
torch._C._VariableFunctions.cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:Union[str,ellipsis,None],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ceil(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ceil_(input:Tensor)->Tensor
torch._C._VariableFunctions.celu(input:Tensor,alpha:Number=1.0)->Tensor
torch._C._VariableFunctions.celu_(input:Tensor,alpha:Number=1.0)->Tensor
torch._C._VariableFunctions.channel_shuffle(input:Tensor,groups:_int)->Tensor
torch._C._VariableFunctions.cholesky(input:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cholesky_inverse(input:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cholesky_solve(input:Tensor,input2:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.choose_qparams_optimized(input:Tensor,numel:_int,n_bins:_int,ratio:_float,bit_width:_int)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.chunk(input:Tensor,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.clamp(input:Tensor,min:Optional[Number]=None,max:Optional[Number]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp(input:Tensor,min:Optional[Tensor]=None,max:Optional[Tensor]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp_(input:Tensor,min:Optional[Number]=None,max:Optional[Number]=None)->Tensor
torch._C._VariableFunctions.clamp_(input:Tensor,min:Optional[Tensor]=None,max:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp_max(input:Tensor,max:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp_max(input:Tensor,max:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp_max_(input:Tensor,max:Number)->Tensor
torch._C._VariableFunctions.clamp_max_(input:Tensor,max:Tensor)->Tensor
torch._C._VariableFunctions.clamp_min(input:Tensor,min:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp_min(input:Tensor,min:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clamp_min_(input:Tensor,min:Number)->Tensor
torch._C._VariableFunctions.clamp_min_(input:Tensor,min:Tensor)->Tensor
torch._C._VariableFunctions.clip(input:Tensor,min:Optional[Number]=None,max:Optional[Number]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clip(input:Tensor,min:Optional[Tensor]=None,max:Optional[Tensor]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clip_(input:Tensor,min:Optional[Number]=None,max:Optional[Number]=None)->Tensor
torch._C._VariableFunctions.clip_(input:Tensor,min:Optional[Tensor]=None,max:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.clone(input:Tensor,*,memory_format:Optional[memory_format]=None)->Tensor
torch._C._VariableFunctions.column_stack(tensors:Union[Tuple[Tensor,...],List[Tensor]],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.combinations(input:Tensor,r:_int=2,with_replacement:_bool=False)->Tensor
torch._C._VariableFunctions.complex(real:Tensor,imag:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.concat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:Union[str,ellipsis,None],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.concat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.conj(input:Tensor)->Tensor
torch._C._VariableFunctions.conj_physical(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.conj_physical_(input:Tensor)->Tensor
torch._C._VariableFunctions.constant_pad_nd(input:Tensor,pad:_size,value:Number=0)->Tensor
torch._C._VariableFunctions.conv1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch._C._VariableFunctions.conv1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:str='valid',dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch._C._VariableFunctions.conv2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch._C._VariableFunctions.conv2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:str='valid',dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch._C._VariableFunctions.conv3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch._C._VariableFunctions.conv3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:str='valid',dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch._C._VariableFunctions.conv_tbc(input:Tensor,weight:Tensor,bias:Tensor,pad:_int=0)->Tensor
torch._C._VariableFunctions.conv_transpose1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch._C._VariableFunctions.conv_transpose2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch._C._VariableFunctions.conv_transpose3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch._C._VariableFunctions.convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int)->Tensor
torch._C._VariableFunctions.copysign(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.copysign(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.corrcoef(input:Tensor)->Tensor
torch._C._VariableFunctions.cos(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cos_(input:Tensor)->Tensor
torch._C._VariableFunctions.cosh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cosh_(input:Tensor)->Tensor
torch._C._VariableFunctions.cosine_embedding_loss(input1:Tensor,input2:Tensor,target:Tensor,margin:float=...,size_average:Optional[bool]=...,reduce:Optional[bool]=...,reduction:str=...)->Tensor
torch._C._VariableFunctions.cosine_similarity(x1:Tensor,x2:Tensor,dim:_int=1,eps:_float=1e-08)->Tensor
torch._C._VariableFunctions.count_nonzero(input:Tensor,dim:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.count_nonzero(input:Tensor,dim:_size)->Tensor
torch._C._VariableFunctions.cov(input:Tensor,*,correction:_int=1,fweights:Optional[Tensor]=None,aweights:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cross(input:Tensor,other:Tensor,dim:Optional[_int]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:Tensor,target_lengths:Tensor,blank:int=...,reduction:str=...,zero_infinity:bool=...)->Tensor
torch._C._VariableFunctions.cudnn_affine_grid_generator(theta:Tensor,N:_int,C:_int,H:_int,W:_int)->Tensor
torch._C._VariableFunctions.cudnn_batch_norm(input:Tensor,weight:Tensor,bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,exponential_average_factor:_float,epsilon:_float)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions.cudnn_convolution(input:Tensor,weight:Tensor,padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool,allow_tf32:_bool)->Tensor
torch._C._VariableFunctions.cudnn_convolution_add_relu(input:Tensor,weight:Tensor,z:Tensor,alpha:Optional[Number],bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,groups:_int)->Tensor
torch._C._VariableFunctions.cudnn_convolution_relu(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,groups:_int)->Tensor
torch._C._VariableFunctions.cudnn_convolution_transpose(input:Tensor,weight:Tensor,padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool,allow_tf32:_bool)->Tensor
torch._C._VariableFunctions.cudnn_grid_sampler(input:Tensor,grid:Tensor)->Tensor
torch._C._VariableFunctions.cudnn_is_acceptable(input:Tensor)->_bool
torch._C._VariableFunctions.cummax(input:Tensor,dim:Union[str,ellipsis,None],*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.cummax(input:Tensor,dim:_int,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.cummin(input:Tensor,dim:Union[str,ellipsis,None],*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.cummin(input:Tensor,dim:_int,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.cumprod(input:Tensor,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cumprod(input:Tensor,dim:_int,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cumsum(input:Tensor,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cumsum(input:Tensor,dim:_int,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.cumulative_trapezoid(y:Tensor,*,dx:Number=1,dim:_int=-1)->Tensor
torch._C._VariableFunctions.cumulative_trapezoid(y:Tensor,x:Tensor,*,dim:_int=-1)->Tensor
torch._C._VariableFunctions.deg2rad(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.deg2rad_(input:Tensor)->Tensor
torch._C._VariableFunctions.dequantize(input:Tensor)->Tensor
torch._C._VariableFunctions.dequantize(tensors:Union[Tuple[Tensor,...],List[Tensor]])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.det(input:Tensor)->Tensor
torch._C._VariableFunctions.detach(input:Tensor)->Tensor
torch._C._VariableFunctions.detach_(input:Tensor)->Tensor
torch._C._VariableFunctions.diag(input:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.diag_embed(input:Tensor,offset:_int=0,dim1:_int=-2,dim2:_int=-1)->Tensor
torch._C._VariableFunctions.diagflat(input:Tensor,offset:_int=0)->Tensor
torch._C._VariableFunctions.diagonal(input:Tensor,*,outdim:Union[str,ellipsis,None],dim1:Union[str,ellipsis,None],dim2:Union[str,ellipsis,None],offset:_int=0)->Tensor
torch._C._VariableFunctions.diagonal(input:Tensor,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch._C._VariableFunctions.diagonal_scatter(input:Tensor,src:Tensor,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch._C._VariableFunctions.diff(input:Tensor,n:_int=1,dim:_int=-1,prepend:Optional[Tensor]=None,append:Optional[Tensor]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.digamma(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.dist(input:Tensor,other:Tensor,p:Number=2)->Tensor
torch._C._VariableFunctions.div(input:Union[Tensor,Number],other:Union[Tensor,Number],*,rounding_mode:Optional[str]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.divide(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.divide(input:Tensor,other:Number,*,rounding_mode:Optional[str])->Tensor
torch._C._VariableFunctions.divide(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.divide(input:Tensor,other:Tensor,*,rounding_mode:Optional[str],out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.dot(input:Tensor,tensor:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.dropout_(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.dsmm(input:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.dsplit(input:Tensor,indices:_size)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.dsplit(input:Tensor,sections:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.dstack(tensors:Union[Tuple[Tensor,...],List[Tensor]],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.eig(input:Tensor,eigenvectors:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_eigenvalues_eigenvectors
torch._C._VariableFunctions.embedding(weight:Tensor,indices:Tensor,padding_idx:_int=-1,scale_grad_by_freq:_bool=False,sparse:_bool=False)->Tensor
torch._C._VariableFunctions.embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool,mode:_int,sparse:_bool,per_sample_weights:Optional[Tensor],include_last_offset:_bool,padding_idx:Optional[_int])->Tuple[Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions.embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None,include_last_offset:_bool=False)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions.embedding_renorm_(input:Tensor,indices:Tensor,max_norm:_float,norm_type:_float)->Tensor
torch._C._VariableFunctions.empty(*size:_int,memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.empty(*size:_int,names:Optional[Sequence[Union[str,ellipsis,None]]],memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.empty(size:_size,*,memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.empty(size:_size,*,names:Optional[Sequence[Union[str,ellipsis,None]]],memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.empty_like(input:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.empty_quantized(size:_size,qtensor:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.empty_strided(size:_size,stride:_size,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.eq(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.eq(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.equal(input:Tensor,other:Tensor)->_bool
torch._C._VariableFunctions.erf(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.erf_(input:Tensor)->Tensor
torch._C._VariableFunctions.erfc(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.erfc_(input:Tensor)->Tensor
torch._C._VariableFunctions.erfinv(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.exp(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.exp2(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.exp2_(input:Tensor)->Tensor
torch._C._VariableFunctions.exp_(input:Tensor)->Tensor
torch._C._VariableFunctions.expm1(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.expm1_(input:Tensor)->Tensor
torch._C._VariableFunctions.eye(n:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.eye(n:_int,m:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.fake_quantize_per_channel_affine(input:Tensor,scale:Tensor,zero_point:Tensor,axis:_int,quant_min:_int,quant_max:_int)->Tensor
torch._C._VariableFunctions.fake_quantize_per_tensor_affine(input:Tensor,scale:Tensor,zero_point:Tensor,quant_min:_int,quant_max:_int)->Tensor
torch._C._VariableFunctions.fake_quantize_per_tensor_affine(input:Tensor,scale:_float,zero_point:_int,quant_min:_int,quant_max:_int)->Tensor
torch._C._VariableFunctions.fbgemm_linear_fp16_weight(input:Tensor,packed_weight:Tensor,bias:Tensor)->Tensor
torch._C._VariableFunctions.fbgemm_linear_fp16_weight_fp32_activation(input:Tensor,packed_weight:Tensor,bias:Tensor)->Tensor
torch._C._VariableFunctions.fbgemm_linear_int8_weight(input:Tensor,weight:Tensor,packed:Tensor,col_offsets:Tensor,weight_scale:Number,weight_zero_point:Number,bias:Tensor)->Tensor
torch._C._VariableFunctions.fbgemm_linear_int8_weight_fp32_activation(input:Tensor,weight:Tensor,packed:Tensor,col_offsets:Tensor,weight_scale:Number,weight_zero_point:Number,bias:Tensor)->Tensor
torch._C._VariableFunctions.fbgemm_linear_quantize_weight(input:Tensor)->Tuple[Tensor, Tensor, _float, _int]
torch._C._VariableFunctions.fbgemm_pack_gemm_matrix_fp16(input:Tensor)->Tensor
torch._C._VariableFunctions.fbgemm_pack_quantized_matrix(input:Tensor)->Tensor
torch._C._VariableFunctions.fbgemm_pack_quantized_matrix(input:Tensor,K:_int,N:_int)->Tensor
torch._C._VariableFunctions.feature_alpha_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.feature_alpha_dropout_(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.feature_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.feature_dropout_(input:Tensor,p:_float,train:_bool)->Tensor
torch._C._VariableFunctions.fill_(input:Tensor,value:Number)->Tensor
torch._C._VariableFunctions.fill_(input:Tensor,value:Tensor)->Tensor
torch._C._VariableFunctions.fix(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.fix_(input:Tensor)->Tensor
torch._C._VariableFunctions.flatten(input:Tensor,dims:Sequence[Union[str,ellipsis,None]],out_dim:Union[str,ellipsis,None])->Tensor
torch._C._VariableFunctions.flatten(input:Tensor,start_dim:Union[str,ellipsis,None],end_dim:Union[str,ellipsis,None],out_dim:Union[str,ellipsis,None])->Tensor
torch._C._VariableFunctions.flatten(input:Tensor,start_dim:_int,end_dim:_int,out_dim:Union[str,ellipsis,None])->Tensor
torch._C._VariableFunctions.flatten(input:Tensor,start_dim:_int=0,end_dim:_int=-1)->Tensor
torch._C._VariableFunctions.flip(input:Tensor,dims:_size)->Tensor
torch._C._VariableFunctions.fliplr(input:Tensor)->Tensor
torch._C._VariableFunctions.flipud(input:Tensor)->Tensor
torch._C._VariableFunctions.float_power(input:Tensor,exponent:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.float_power(input:Tensor,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.float_power(self:Number,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.floor(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.floor_(input:Tensor)->Tensor
torch._C._VariableFunctions.floor_divide(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.fmax(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.fmin(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.fmod(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.fmod(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.frac(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.frac_(input:Tensor)->Tensor
torch._C._VariableFunctions.frexp(input:Tensor,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_mantissa_exponent
torch._C._VariableFunctions.frobenius_norm(input:Tensor)->Tensor
torch._C._VariableFunctions.frobenius_norm(input:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.from_file(filename:str,shared:Optional[_bool]=None,size:Optional[_int]=0,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.from_numpy(ndarray)->Tensor
torch._C._VariableFunctions.frombuffer(buffer:Any,*,dtype:_dtype,count:int=-1,offset:int=0,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.full(size:_size,fill_value:Number,*,names:List[Union[str,None]],layout:_layout=strided,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.full(size:_size,fill_value:Number,*,out:Optional[Tensor]=None,layout:_layout=strided,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.full_like(input:Tensor,fill_value:Number,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.fused_moving_avg_obs_fake_quant(input:Tensor,observer_on:Tensor,fake_quant_on:Tensor,running_min:Tensor,running_max:Tensor,scale:Tensor,zero_point:Tensor,averaging_const:_float,quant_min:_int,quant_max:_int,ch_axis:_int,per_row_fake_quant:_bool=False,symmetric_quant:_bool=False)->Tensor
torch._C._VariableFunctions.gather(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,*,sparse_grad:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.gather(input:Tensor,dim:_int,index:Tensor,*,sparse_grad:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.gcd(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.gcd_(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.ge(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ge(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.geqrf(input:Tensor,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_a_tau
torch._C._VariableFunctions.ger(input:Tensor,vec2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.get_default_dtype()->_dtype
torch._C._VariableFunctions.get_num_interop_threads()->_int
torch._C._VariableFunctions.get_num_threads()->_int
torch._C._VariableFunctions.gradient(input:Tensor,*,dim:_size,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.gradient(input:Tensor,*,spacing:Number,dim:_size,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.gradient(input:Tensor,*,spacing:Optional[Number]=None,dim:Optional[_int]=None,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.gradient(input:Tensor,*,spacing:Sequence[Number],dim:Optional[_int]=None,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.gradient(input:Tensor,*,spacing:Sequence[Number],dim:_size,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.gradient(input:Tensor,*,spacing:Union[Tuple[Tensor,...],List[Tensor]],dim:Optional[_int]=None,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.gradient(input:Tensor,*,spacing:Union[Tuple[Tensor,...],List[Tensor]],dim:_size,edge_order:_int=1)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.greater(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.greater(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.greater_equal(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.greater_equal(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.grid_sampler(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch._C._VariableFunctions.grid_sampler_2d(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch._C._VariableFunctions.grid_sampler_3d(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch._C._VariableFunctions.group_norm(input:Tensor,num_groups:_int,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:_float=1e-05,cudnn_enabled:_bool=True)->Tensor
torch._C._VariableFunctions.gru(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.gru(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.gru_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.gt(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.gt(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.hamming_window(window_length:_int,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.hamming_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.hamming_window(window_length:_int,periodic:_bool,alpha:_float,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.hamming_window(window_length:_int,periodic:_bool,alpha:_float,beta:_float,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.hann_window(window_length:_int,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.hann_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.hardshrink(input:Tensor,lambd:Number=0.5,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.heaviside(input:Tensor,values:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.hinge_embedding_loss(input:Tensor,target:Tensor,margin:float=...,size_average:Optional[bool]=...,reduce:Optional[bool]=...,reduction:str=...)->Tensor
torch._C._VariableFunctions.histc(input:Tensor,bins:_int=100,min:Number=0,max:Number=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.histogram(input:Tensor,bins:Tensor,*,weight:Optional[Tensor]=None,density:_bool=False,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_hist_bin_edges
torch._C._VariableFunctions.histogram(input:Tensor,bins:_int=100,*,range:Optional[Sequence[_float]]=None,weight:Optional[Tensor]=None,density:_bool=False,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_hist_bin_edges
torch._C._VariableFunctions.hsmm(input:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.hsplit(input:Tensor,indices:_size)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.hsplit(input:Tensor,sections:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.hspmm(mat1:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.hstack(tensors:Union[Tuple[Tensor,...],List[Tensor]],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.hypot(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.i0(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.i0_(input:Tensor)->Tensor
torch._C._VariableFunctions.igamma(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.igammac(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.imag(input:Tensor)->Tensor
torch._C._VariableFunctions.index_add(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,source:Tensor,*,alpha:Number=1)->Tensor
torch._C._VariableFunctions.index_add(input:Tensor,dim:_int,index:Tensor,source:Tensor,*,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.index_copy(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,source:Tensor)->Tensor
torch._C._VariableFunctions.index_copy(input:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch._C._VariableFunctions.index_fill(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,value:Number)->Tensor
torch._C._VariableFunctions.index_fill(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,value:Tensor)->Tensor
torch._C._VariableFunctions.index_fill(input:Tensor,dim:_int,index:Tensor,value:Number)->Tensor
torch._C._VariableFunctions.index_fill(input:Tensor,dim:_int,index:Tensor,value:Tensor)->Tensor
torch._C._VariableFunctions.index_put(input:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch._C._VariableFunctions.index_put_(input:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch._C._VariableFunctions.index_select(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.index_select(input:Tensor,dim:_int,index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.init_num_threads()->None
torch._C._VariableFunctions.inner(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.instance_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],use_input_stats:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tensor
torch._C._VariableFunctions.int_repr(input:Tensor)->Tensor
torch._C._VariableFunctions.inverse(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.is_complex(input:Tensor)->_bool
torch._C._VariableFunctions.is_conj(input:Tensor)->_bool
torch._C._VariableFunctions.is_distributed(input:Tensor)->_bool
torch._C._VariableFunctions.is_floating_point(input:Tensor)->_bool
torch._C._VariableFunctions.is_grad_enabled()->_bool
torch._C._VariableFunctions.is_inference(input:Tensor)->_bool
torch._C._VariableFunctions.is_inference_mode_enabled()->_bool
torch._C._VariableFunctions.is_neg(input:Tensor)->_bool
torch._C._VariableFunctions.is_nonzero(input:Tensor)->_bool
torch._C._VariableFunctions.is_same_size(input:Tensor,other:Tensor)->_bool
torch._C._VariableFunctions.is_signed(input:Tensor)->_bool
torch._C._VariableFunctions.is_vulkan_available()->_bool
torch._C._VariableFunctions.isclose(input:Tensor,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->Tensor
torch._C._VariableFunctions.isfinite(input:Tensor)->Tensor
torch._C._VariableFunctions.isin(element:Number,test_elements:Tensor,*,assume_unique:_bool=False,invert:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.isin(elements:Tensor,test_element:Number,*,assume_unique:_bool=False,invert:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.isin(elements:Tensor,test_elements:Tensor,*,assume_unique:_bool=False,invert:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.isinf(input:Tensor)->Tensor
torch._C._VariableFunctions.isnan(input:Tensor)->Tensor
torch._C._VariableFunctions.isneginf(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.isposinf(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.isreal(input:Tensor)->Tensor
torch._C._VariableFunctions.istft(input:Tensor,n_fft:_int,hop_length:Optional[_int]=None,win_length:Optional[_int]=None,window:Optional[Tensor]=None,center:_bool=True,normalized:_bool=False,onesided:Optional[_bool]=None,length:Optional[_int]=None,return_complex:_bool=False)->Tensor
torch._C._VariableFunctions.kaiser_window(window_length:_int,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.kaiser_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.kaiser_window(window_length:_int,periodic:_bool,beta:_float,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.kl_div(input:Tensor,target:Tensor,size_average:Optional[bool]=...,reduce:Optional[bool]=...,reduction:str=...,log_target:bool=...)->Tensor
torch._C._VariableFunctions.kron(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.kthvalue(input:Tensor,k:_int,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.kthvalue(input:Tensor,k:_int,dim:_int=-1,keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.layer_norm(input:Tensor,normalized_shape:_size,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:_float=1e-05,cudnn_enable:_bool=True)->Tensor
torch._C._VariableFunctions.lcm(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lcm_(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.ldexp(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ldexp_(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.le(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.le(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lerp(input:Tensor,end:Tensor,weight:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lerp(input:Tensor,end:Tensor,weight:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.less(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.less(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.less_equal(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.less_equal(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lgamma(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.linspace(start:Number,end:Number,steps:Optional[_int]=None,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.log(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.log10(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.log10_(input:Tensor)->Tensor
torch._C._VariableFunctions.log1p(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.log1p_(input:Tensor)->Tensor
torch._C._VariableFunctions.log2(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.log2_(input:Tensor)->Tensor
torch._C._VariableFunctions.log_(input:Tensor)->Tensor
torch._C._VariableFunctions.log_softmax(input:Tensor,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.log_softmax(input:Tensor,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.logaddexp(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logaddexp2(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logcumsumexp(input:Tensor,dim:Union[str,ellipsis,None],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logcumsumexp(input:Tensor,dim:_int,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logdet(input:Tensor)->Tensor
torch._C._VariableFunctions.logical_and(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logical_not(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logical_or(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logical_xor(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logit(input:Tensor,eps:Optional[_float]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logit_(input:Tensor,eps:Optional[_float]=None)->Tensor
torch._C._VariableFunctions.logspace(start:Number,end:Number,steps:Optional[_int]=None,base:_float=10.0,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.logsumexp(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.logsumexp(input:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lstm(data:Tensor,batch_sizes:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.lstm(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.lstm_cell(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.lstsq(input:Tensor,A:Tensor,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_solution_QR
torch._C._VariableFunctions.lt(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lt(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lu_solve(input:Tensor,LU_data:Tensor,LU_pivots:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.lu_unpack(LU_data:Tensor,LU_pivots:Tensor,unpack_data:_bool=True,unpack_pivots:_bool=True,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_P_L_U
torch._C._VariableFunctions.margin_ranking_loss(input1:Tensor,input2:Tensor,target:Tensor,margin:float=...,size_average:Optional[bool]=...,reduce:Optional[bool]=...,reduction:str=...)->Tensor
torch._C._VariableFunctions.masked_fill(input:Tensor,mask:Tensor,value:Number)->Tensor
torch._C._VariableFunctions.masked_fill(input:Tensor,mask:Tensor,value:Tensor)->Tensor
torch._C._VariableFunctions.masked_scatter(input:Tensor,mask:Tensor,source:Tensor)->Tensor
torch._C._VariableFunctions.masked_select(input:Tensor,mask:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.matmul(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.matrix_exp(input:Tensor)->Tensor
torch._C._VariableFunctions.matrix_power(input:Tensor,n:_int,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.matrix_rank(input:Tensor,symmetric:_bool=False)->Tensor
torch._C._VariableFunctions.matrix_rank(input:Tensor,tol:_float,symmetric:_bool=False)->Tensor
torch._C._VariableFunctions.max(input:Tensor)->Tensor
torch._C._VariableFunctions.max(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.max(input:Tensor,dim:_int,keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.max(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.max_pool1d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.max_pool1d_with_indices(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.max_pool2d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.max_pool3d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.maximum(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.mean(input:Tensor,*,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.mean(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.mean(input:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.median(input:Tensor)->Tensor
torch._C._VariableFunctions.median(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.median(input:Tensor,dim:_int,keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.min(input:Tensor)->Tensor
torch._C._VariableFunctions.min(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.min(input:Tensor,dim:_int,keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.min(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.minimum(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.miopen_batch_norm(input:Tensor,weight:Tensor,bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,exponential_average_factor:_float,epsilon:_float)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.miopen_convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch._C._VariableFunctions.miopen_convolution_transpose(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch._C._VariableFunctions.miopen_depthwise_convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch._C._VariableFunctions.miopen_rnn(input:Tensor,weight:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,hx:Tensor,cx:Optional[Tensor],mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,dropout:_float,train:_bool,bidirectional:_bool,batch_sizes:_size,dropout_state:Optional[Tensor])->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]
torch._C._VariableFunctions.mkldnn_adaptive_avg_pool2d(input:Tensor,output_size:Union[_int,_size])->Tensor
torch._C._VariableFunctions.mkldnn_convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int)->Tensor
torch._C._VariableFunctions.mkldnn_linear_backward_weights(grad_output:Tensor,input:Tensor,weight:Tensor,bias_defined:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.mkldnn_max_pool2d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.mkldnn_max_pool3d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.mm(input:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.mode(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.mode(input:Tensor,dim:_int=-1,keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.moveaxis(input:Tensor,source:_int,destination:_int)->Tensor
torch._C._VariableFunctions.moveaxis(input:Tensor,source:_size,destination:_size)->Tensor
torch._C._VariableFunctions.movedim(input:Tensor,source:_int,destination:_int)->Tensor
torch._C._VariableFunctions.movedim(input:Tensor,source:_size,destination:_size)->Tensor
torch._C._VariableFunctions.msort(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.mul(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.multinomial(input:Tensor,num_samples:_int,replacement:_bool=False,*,generator:Optional[Generator]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.multiply(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.multiply(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.mv(input:Tensor,vec:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.mvlgamma(input:Tensor,p:_int,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nan_to_num(input:Tensor,nan:Optional[_float]=None,posinf:Optional[_float]=None,neginf:Optional[_float]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nan_to_num_(input:Tensor,nan:Optional[_float]=None,posinf:Optional[_float]=None,neginf:Optional[_float]=None)->Tensor
torch._C._VariableFunctions.nanmean(input:Tensor,dim:Union[_int,_size]=(),keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nanmedian(input:Tensor)->Tensor
torch._C._VariableFunctions.nanmedian(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.nanmedian(input:Tensor,dim:_int,keepdim:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.nanquantile(input:Tensor,q:Tensor,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear',out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nanquantile(input:Tensor,q:_float,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear',out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nansum(input:Tensor,*,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.nansum(input:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.narrow(input:Tensor,dim:_int,start:Tensor,length:_int)->Tensor
torch._C._VariableFunctions.narrow(input:Tensor,dim:_int,start:_int,length:_int)->Tensor
torch._C._VariableFunctions.narrow_copy(input:Tensor,dim:_int,start:_int,length:_int,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.native_batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.native_channel_shuffle(input:Tensor,groups:_int)->Tensor
torch._C._VariableFunctions.native_dropout(input:Tensor,p:_float,train:Optional[_bool])->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.native_group_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],N:_int,C:_int,HxW:_int,group:_int,eps:_float)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.native_layer_norm(input:Tensor,normalized_shape:_size,weight:Optional[Tensor],bias:Optional[Tensor],eps:_float)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.native_norm(input:Tensor,p:Number=2)->Tensor
torch._C._VariableFunctions.native_norm(input:Tensor,p:Optional[Number],dim:Union[_int,_size],keepdim:_bool,dtype:Optional[_dtype])->Tensor
torch._C._VariableFunctions.ne(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ne(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.neg(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.neg_(input:Tensor)->Tensor
torch._C._VariableFunctions.negative(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.negative_(input:Tensor)->Tensor
torch._C._VariableFunctions.nextafter(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nonzero(input:Tensor,*,as_tuple:Literal[False]=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nonzero(input:Tensor,*,as_tuple:Literal[True])->Tuple[Tensor, ...]
torch._C._VariableFunctions.norm_except_dim(v:Tensor,pow:_int=2,dim:_int=0)->Tensor
torch._C._VariableFunctions.normal(mean:Tensor,std:Tensor,*,generator:Optional[Generator]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.normal(mean:Tensor,std:_float=1,*,generator:Optional[Generator]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.normal(mean:_float,std:Tensor,*,generator:Optional[Generator]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.normal(mean:_float,std:_float,size:_size,*,generator:Optional[Generator]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.not_equal(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.not_equal(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nuclear_norm(input:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.nuclear_norm(input:Tensor,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.numel(self:Tensor)->_int
torch._C._VariableFunctions.ones(*size:_int,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.ones(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.ones(size:_size,*,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.ones(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.ones_like(input:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.orgqr(input:Tensor,input2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.ormqr(input:Tensor,input2:Tensor,input3:Tensor,left:_bool=True,transpose:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.outer(input:Tensor,vec2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.pairwise_distance(x1:Tensor,x2:Tensor,p:_float=2,eps:_float=1e-06,keepdim:_bool=False)->Tensor
torch._C._VariableFunctions.pdist(input:Tensor,p:_float=2)->Tensor
torch._C._VariableFunctions.permute(input:Tensor,dims:_size)->Tensor
torch._C._VariableFunctions.pinverse(input:Tensor,rcond:_float=1e-15)->Tensor
torch._C._VariableFunctions.pixel_shuffle(input:Tensor,upscale_factor:_int)->Tensor
torch._C._VariableFunctions.pixel_unshuffle(input:Tensor,downscale_factor:_int)->Tensor
torch._C._VariableFunctions.poisson(input:Tensor,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions.poisson_nll_loss(input:Tensor,target:Tensor,log_input:_bool,full:_bool,eps:_float,reduction:_int)->Tensor
torch._C._VariableFunctions.polar(abs:Tensor,angle:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.polygamma(n:_int,input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.positive(input:Tensor)->Tensor
torch._C._VariableFunctions.pow(input:Tensor,exponent:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.pow(input:Tensor,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.pow(self:Number,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.prelu(input:Tensor,weight:Tensor)->Tensor
torch._C._VariableFunctions.prod(input:Tensor,*,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.prod(input:Tensor,dim:Union[str,ellipsis,None],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.prod(input:Tensor,dim:_int,keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.promote_types(type1:_dtype,type2:_dtype)->_dtype
torch._C._VariableFunctions.put(input:Tensor,index:Tensor,source:Tensor,accumulate:_bool=False)->Tensor
torch._C._VariableFunctions.q_per_channel_axis(input:Tensor)->_int
torch._C._VariableFunctions.q_per_channel_scales(input:Tensor)->Tensor
torch._C._VariableFunctions.q_per_channel_zero_points(input:Tensor)->Tensor
torch._C._VariableFunctions.q_scale(input:Tensor)->_float
torch._C._VariableFunctions.q_zero_point(input:Tensor)->_int
torch._C._VariableFunctions.qr(input:Tensor,some:_bool=True,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_Q_R
torch._C._VariableFunctions.quantile(input:Tensor,q:Tensor,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear',out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.quantile(input:Tensor,q:_float,dim:Optional[_int]=None,keepdim:_bool=False,*,interpolation:str='linear',out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.quantize_per_channel(input:Tensor,scales:Tensor,zero_points:Tensor,axis:_int,dtype:_dtype)->Tensor
torch._C._VariableFunctions.quantize_per_tensor(input:Tensor,scale:Tensor,zero_point:Tensor,dtype:_dtype)->Tensor
torch._C._VariableFunctions.quantize_per_tensor(input:Tensor,scale:_float,zero_point:_int,dtype:_dtype)->Tensor
torch._C._VariableFunctions.quantize_per_tensor(tensors:Union[Tuple[Tensor,...],List[Tensor]],scales:Tensor,zero_points:Tensor,dtype:_dtype)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.quantize_per_tensor_dynamic(input:Tensor,dtype:_dtype,reduce_range:_bool)->Tensor
torch._C._VariableFunctions.quantized_batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],mean:Tensor,var:Tensor,eps:_float,output_scale:_float,output_zero_point:_int)->Tensor
torch._C._VariableFunctions.quantized_gru_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch._C._VariableFunctions.quantized_lstm_cell(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.quantized_max_pool1d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.quantized_max_pool2d(input:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch._C._VariableFunctions.quantized_rnn_relu_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch._C._VariableFunctions.quantized_rnn_tanh_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch._C._VariableFunctions.rad2deg(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.rad2deg_(input:Tensor)->Tensor
torch._C._VariableFunctions.rand(*size:_int,generator:Optional[Generator],names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(*size:_int,generator:Optional[Generator],out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(*size:_int,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(size:_size,*,generator:Optional[Generator],names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(size:_size,*,generator:Optional[Generator],out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(size:_size,*,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.rand_like(input:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randint(high:_int,size:_size,*,generator:Optional[Generator]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randint(low:_int,high:_int,size:_size,*,generator:Optional[Generator]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randint_like(input:Tensor,high:_int,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randint_like(input:Tensor,low:_int,high:_int,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(*size:_int,generator:Optional[Generator],names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(*size:_int,generator:Optional[Generator],out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(*size:_int,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(size:_size,*,generator:Optional[Generator],names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(size:_size,*,generator:Optional[Generator],out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(size:_size,*,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randn_like(input:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randperm(n:_int,*,generator:Optional[Generator],out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.randperm(n:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.range(start:Number,end:Number,step:Number=1,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.ravel(input:Tensor)->Tensor
torch._C._VariableFunctions.real(input:Tensor)->Tensor
torch._C._VariableFunctions.reciprocal(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.reciprocal_(input:Tensor)->Tensor
torch._C._VariableFunctions.relu(input:Tensor)->Tensor
torch._C._VariableFunctions.relu_(input:Tensor)->Tensor
torch._C._VariableFunctions.remainder(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.remainder(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.remainder(self:Number,other:Tensor)->Tensor
torch._C._VariableFunctions.renorm(input:Tensor,p:Number,dim:_int,maxnorm:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.repeat_interleave(input:Tensor,repeats:Tensor,dim:Optional[_int]=None,*,output_size:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.repeat_interleave(input:Tensor,repeats:_int,dim:Optional[_int]=None,*,output_size:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.repeat_interleave(repeats:Tensor,*,output_size:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.reshape(input:Tensor,shape:_size)->Tensor
torch._C._VariableFunctions.resize_as_(input:Tensor,the_template:Tensor,*,memory_format:Optional[memory_format]=None)->Tensor
torch._C._VariableFunctions.resize_as_sparse_(input:Tensor,the_template:Tensor)->Tensor
torch._C._VariableFunctions.resolve_conj(input:Tensor)->Tensor
torch._C._VariableFunctions.resolve_neg(input:Tensor)->Tensor
torch._C._VariableFunctions.result_type(scalar1:Number,scalar2:Number)->_dtype
torch._C._VariableFunctions.result_type(scalar:Number,tensor:Tensor)->_dtype
torch._C._VariableFunctions.result_type(tensor:Tensor,other:Number)->_dtype
torch._C._VariableFunctions.result_type(tensor:Tensor,other:Tensor)->_dtype
torch._C._VariableFunctions.rnn_relu(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.rnn_relu(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.rnn_relu_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.rnn_tanh(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.rnn_tanh(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.rnn_tanh_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.roll(input:Tensor,shifts:Union[_int,_size],dims:Union[_int,_size]=())->Tensor
torch._C._VariableFunctions.rot90(input:Tensor,k:_int=1,dims:_size=(0,1))->Tensor
torch._C._VariableFunctions.round(input:Tensor,*,decimals:_int,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.round(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.round_(input:Tensor)->Tensor
torch._C._VariableFunctions.round_(input:Tensor,*,decimals:_int)->Tensor
torch._C._VariableFunctions.row_stack(tensors:Union[Tuple[Tensor,...],List[Tensor]],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.rrelu(input:Tensor,lower:Number=0.125,upper:Number=0.3333333333333333,training:_bool=False,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions.rrelu_(input:Tensor,lower:Number=0.125,upper:Number=0.3333333333333333,training:_bool=False,generator:Optional[Generator]=None)->Tensor
torch._C._VariableFunctions.rsqrt(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.rsqrt_(input:Tensor)->Tensor
torch._C._VariableFunctions.rsub(input:Tensor,other:Number,alpha:Number=1)->Tensor
torch._C._VariableFunctions.rsub(input:Tensor,other:Tensor,*,alpha:Number=1)->Tensor
torch._C._VariableFunctions.saddmm(input:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.scalar_tensor(s:Number,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.scatter(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,src:Tensor)->Tensor
torch._C._VariableFunctions.scatter(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,value:Number)->Tensor
torch._C._VariableFunctions.scatter(input:Tensor,dim:_int,index:Tensor,src:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.scatter(input:Tensor,dim:_int,index:Tensor,src:Tensor,*,reduce:str,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.scatter(input:Tensor,dim:_int,index:Tensor,value:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.scatter(input:Tensor,dim:_int,index:Tensor,value:Number,*,reduce:str,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.scatter_add(input:Tensor,dim:Union[str,ellipsis,None],index:Tensor,src:Tensor)->Tensor
torch._C._VariableFunctions.scatter_add(input:Tensor,dim:_int,index:Tensor,src:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.scatter_reduce(input:Tensor,dim:_int,index:Tensor,reduce:str,*,output_size:Optional[_int]=None)->Tensor
torch._C._VariableFunctions.searchsorted(sorted_sequence:Tensor,input:Tensor,*,out_int32:_bool=False,right:_bool=False,side:Optional[str]=None,sorter:Optional[Tensor]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.searchsorted(sorted_sequence:Tensor,self:Number,*,out_int32:_bool=False,right:_bool=False,side:Optional[str]=None,sorter:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.segment_reduce(data:Tensor,reduce:str,*,lengths:Optional[Tensor]=None,indices:Optional[Tensor]=None,axis:_int=0,unsafe:_bool=False,initial:Optional[Number]=None)->Tensor
torch._C._VariableFunctions.select(input:Tensor,dim:Union[str,ellipsis,None],index:_int)->Tensor
torch._C._VariableFunctions.select(input:Tensor,dim:_int,index:_int)->Tensor
torch._C._VariableFunctions.select_scatter(input:Tensor,src:Tensor,dim:_int,index:_int)->Tensor
torch._C._VariableFunctions.selu(input:Tensor)->Tensor
torch._C._VariableFunctions.selu_(input:Tensor)->Tensor
torch._C._VariableFunctions.set_flush_denormal(mode:_bool)->_bool
torch._C._VariableFunctions.set_num_interop_threads(num:_int)->None
torch._C._VariableFunctions.set_num_threads(num:_int)->None
torch._C._VariableFunctions.sgn(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sigmoid(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sigmoid_(input:Tensor)->Tensor
torch._C._VariableFunctions.sign(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.signbit(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sin(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sin_(input:Tensor)->Tensor
torch._C._VariableFunctions.sinc(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sinc_(input:Tensor)->Tensor
torch._C._VariableFunctions.sinh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sinh_(input:Tensor)->Tensor
torch._C._VariableFunctions.slice_scatter(input:Tensor,src:Tensor,dim:_int=0,start:Optional[_int]=None,end:Optional[_int]=None,step:_int=1)->Tensor
torch._C._VariableFunctions.slogdet(input:Tensor)->namedtuple_sign_logabsdet
torch._C._VariableFunctions.smm(input:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.softmax(input:Tensor,dim:Union[str,ellipsis,None],*,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.softmax(input:Tensor,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.solve(input:Tensor,A:Tensor,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_solution_LU
torch._C._VariableFunctions.sort(input:Tensor,*,stable:Optional[_bool],dim:Union[str,ellipsis,None],descending:_bool=False,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.sort(input:Tensor,*,stable:Optional[_bool],dim:_int=-1,descending:_bool=False,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.sort(input:Tensor,dim:Union[str,ellipsis,None],descending:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.sort(input:Tensor,dim:_int=-1,descending:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.sparse_coo_tensor(indices:Tensor,values:Union[Tensor,List],size:Optional[_size]=None,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.sparse_csr_tensor(crow_indices:Union[Tensor,List],col_indices:Union[Tensor,List],values:Union[Tensor,List],size:Optional[_size]=None,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.split_with_sizes(input:Tensor,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.spmm(input:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.sqrt(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sqrt_(input:Tensor)->Tensor
torch._C._VariableFunctions.square(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.square_(input:Tensor)->Tensor
torch._C._VariableFunctions.squeeze(input:Tensor)->Tensor
torch._C._VariableFunctions.squeeze(input:Tensor,dim:Union[str,ellipsis,None])->Tensor
torch._C._VariableFunctions.squeeze(input:Tensor,dim:_int)->Tensor
torch._C._VariableFunctions.sspaddmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.sspaddmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor)->Tensor
torch._C._VariableFunctions.sspaddmm(input:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.stack(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.std(input:Tensor,dim:Optional[Union[_int,_size]],*,correction:Optional[_int],keepdim:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.std(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],*,correction:Optional[_int],keepdim:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.std(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.std(input:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.std(input:Tensor,unbiased:_bool=True)->Tensor
torch._C._VariableFunctions.std_mean(input:Tensor,dim:Optional[Union[_int,_size]],*,correction:Optional[_int],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.std_mean(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],*,correction:Optional[_int],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.std_mean(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.std_mean(input:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.std_mean(input:Tensor,unbiased:_bool=True)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.sub(input:Union[Tensor,Number],other:Union[Tensor,Number],*,alpha:Optional[Number]=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sub(self:Tensor,alpha:Number,other:Tensor)->Tensor
torch._C._VariableFunctions.sub(self:Tensor,alpha:Number,other:Tensor,*,out:Tensor)->Tensor
torch._C._VariableFunctions.subtract(input:Tensor,other:Number,alpha:Number=1)->Tensor
torch._C._VariableFunctions.subtract(input:Tensor,other:Tensor,*,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sum(input:Tensor,*,dtype:Optional[_dtype]=None)->Tensor
torch._C._VariableFunctions.sum(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.sum(input:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.svd(input:Tensor,some:_bool=True,compute_uv:_bool=True,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_U_S_V
torch._C._VariableFunctions.swapaxes(input:Tensor,axis0:_int,axis1:_int)->Tensor
torch._C._VariableFunctions.swapdims(input:Tensor,dim0:_int,dim1:_int)->Tensor
torch._C._VariableFunctions.symeig(input:Tensor,eigenvectors:_bool=False,upper:_bool=True,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_eigenvalues_eigenvectors
torch._C._VariableFunctions.t(input:Tensor)->Tensor
torch._C._VariableFunctions.take(input:Tensor,index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.take_along_dim(input:Tensor,indices:Tensor,dim:Optional[_int]=None,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.tan(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.tan_(input:Tensor)->Tensor
torch._C._VariableFunctions.tanh(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.tanh_(input:Tensor)->Tensor
torch._C._VariableFunctions.tensor(data:Any,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.tensor_split(input:Tensor,indices:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.tensor_split(input:Tensor,sections:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.tensor_split(input:Tensor,tensor_indices_or_sections:Tensor,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.threshold(input:Tensor,threshold:Number,value:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.threshold_(input:Tensor,threshold:Number,value:Number)->Tensor
torch._C._VariableFunctions.tile(input:Tensor,dims:_size)->Tensor
torch._C._VariableFunctions.topk(input:Tensor,k:_int,dim:_int=-1,largest:_bool=True,sorted:_bool=True,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_values_indices
torch._C._VariableFunctions.trace(input:Tensor)->Tensor
torch._C._VariableFunctions.transpose(input:Tensor,dim0:Union[str,ellipsis,None],dim1:Union[str,ellipsis,None])->Tensor
torch._C._VariableFunctions.transpose(input:Tensor,dim0:_int,dim1:_int)->Tensor
torch._C._VariableFunctions.trapezoid(y:Tensor,*,dx:Number=1,dim:_int=-1)->Tensor
torch._C._VariableFunctions.trapezoid(y:Tensor,x:Tensor,*,dim:_int=-1)->Tensor
torch._C._VariableFunctions.trapz(y:Tensor,*,dx:_float=1,dim:_int=-1)->Tensor
torch._C._VariableFunctions.trapz(y:Tensor,x:Tensor,*,dim:_int=-1)->Tensor
torch._C._VariableFunctions.triangular_solve(input:Tensor,A:Tensor,upper:_bool=True,transpose:_bool=False,unitriangular:_bool=False,*,out:Union[Tensor,Tuple[Tensor,...],List[Tensor]]=None)->namedtuple_solution_cloned_coefficient
torch._C._VariableFunctions.tril(input:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.tril_indices(row:_int,col:_int,offset:_int=0,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.triplet_margin_loss(anchor:Tensor,positive:Tensor,negative:Tensor,margin:float=...,p:float=...,eps:float=...,swap:bool=...,size_average:Optional[bool]=...,reduce:Optional[bool]=...,reduction:str=...)->Tensor
torch._C._VariableFunctions.triu(input:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.triu_indices(row:_int,col:_int,offset:_int=0,*,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.true_divide(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.trunc(input:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.trunc_(input:Tensor)->Tensor
torch._C._VariableFunctions.unbind(input:Tensor,dim:Union[str,ellipsis,None])->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.unbind(input:Tensor,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.unique_dim(input:Tensor,dim:_int,sorted:_bool=True,return_inverse:_bool=False,return_counts:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch._C._VariableFunctions.unsafe_chunk(input:Tensor,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.unsafe_split(input:Tensor,split_size:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.unsafe_split_with_sizes(input:Tensor,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.unsqueeze(input:Tensor,dim:_int)->Tensor
torch._C._VariableFunctions.vander(x:Tensor,N:Optional[_int]=None,increasing:_bool=False)->Tensor
torch._C._VariableFunctions.var(input:Tensor,dim:Optional[Union[_int,_size]],*,correction:Optional[_int],keepdim:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.var(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],*,correction:Optional[_int],keepdim:_bool=False,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.var(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.var(input:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.var(input:Tensor,unbiased:_bool=True)->Tensor
torch._C._VariableFunctions.var_mean(input:Tensor,dim:Optional[Union[_int,_size]],*,correction:Optional[_int],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.var_mean(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],*,correction:Optional[_int],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.var_mean(input:Tensor,dim:Sequence[Union[str,ellipsis,None]],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.var_mean(input:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.var_mean(input:Tensor,unbiased:_bool=True)->Tuple[Tensor, Tensor]
torch._C._VariableFunctions.vdot(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.view_as_complex(input:Tensor)->Tensor
torch._C._VariableFunctions.view_as_real(input:Tensor)->Tensor
torch._C._VariableFunctions.vsplit(input:Tensor,indices:_size)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.vsplit(input:Tensor,sections:_int)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.vstack(tensors:Union[Tuple[Tensor,...],List[Tensor]],*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.where(condition:Tensor)->Union[Tuple[Tensor, ...], List[Tensor]]
torch._C._VariableFunctions.where(condition:Tensor,input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.where(condition:Tensor,input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.where(condition:Tensor,self:Number,other:Number)->Tensor
torch._C._VariableFunctions.where(condition:Tensor,self:Number,other:Tensor)->Tensor
torch._C._VariableFunctions.xlogy(input:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.xlogy(input:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.xlogy(self:Number,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch._C._VariableFunctions.xlogy_(input:Tensor,other:Number)->Tensor
torch._C._VariableFunctions.xlogy_(input:Tensor,other:Tensor)->Tensor
torch._C._VariableFunctions.zero_(input:Tensor)->Tensor
torch._C._VariableFunctions.zeros(*size:_int,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.zeros(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.zeros(size:_size,*,names:Optional[Sequence[Union[str,ellipsis,None]]],dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.zeros(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor
torch._C._VariableFunctions.zeros_like(input:Tensor,*,memory_format:Optional[memory_format]=None,dtype:_dtype=None,layout:Optional[_layout]=strided,device:Union[_device,str,None]=None,pin_memory:_bool=False,requires_grad:_bool=False)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_distributed_rpc_testing.pyi----------------------------------------
torch._C._distributed_rpc_testing.FaultyTensorPipeAgent(self,store:Store,name:str,rank:int,world_size:int,options:FaultyTensorPipeRpcBackendOptions,reverse_device_maps:Dict[str,Dict[torch.device,torch.device]],devices:List[torch.device])
torch._C._distributed_rpc_testing.FaultyTensorPipeAgent.__init__(self,store:Store,name:str,rank:int,world_size:int,options:FaultyTensorPipeRpcBackendOptions,reverse_device_maps:Dict[str,Dict[torch.device,torch.device]],devices:List[torch.device])
torch._C._distributed_rpc_testing.FaultyTensorPipeRpcBackendOptions(self,num_worker_threads:int,rpc_timeout:float,init_method:str,messages_to_fail:List[str],messages_to_delay:Dict[str,float],num_fail_sends:int)
torch._C._distributed_rpc_testing.FaultyTensorPipeRpcBackendOptions.__init__(self,num_worker_threads:int,rpc_timeout:float,init_method:str,messages_to_fail:List[str],messages_to_delay:Dict[str,float],num_fail_sends:int)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_onnx.pyi----------------------------------------
torch._C._onnx.OperatorExportTypes(Enum)
torch._C._onnx.TensorProtoDataType(Enum)
torch._C._onnx.TrainingMode(Enum)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_nvtx.pyi----------------------------------------
torch._C._nvtx.markA(message:str)->None
torch._C._nvtx.rangePop()->int
torch._C._nvtx.rangePushA(message:str)->int
torch._nvtx.markA(message:str)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_functions.pyi----------------------------------------
torch._C._functions.DelayedError(self,msg:AnyStr,num_inputs:int)
torch._C._functions.DelayedError.__init__(self,msg:AnyStr,num_inputs:int)
torch._C._functions.UndefinedGrad(self)
torch._C._functions.UndefinedGrad.__init__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_C/_autograd.pyi----------------------------------------
torch._C._autograd.DeviceType(Enum)
torch._C._autograd.ProfilerActivity(Enum)
torch._C._autograd.ProfilerConfig(self,state:ProfilerState,report_input_shapes:bool,profile_memory:bool,with_stack:bool,with_flops:bool,with_modules:bool)
torch._C._autograd.ProfilerConfig.__init__(self,state:ProfilerState,report_input_shapes:bool,profile_memory:bool,with_stack:bool,with_flops:bool,with_modules:bool)
torch._C._autograd.ProfilerEvent
torch._C._autograd.ProfilerEvent.cpu_elapsed_us(self,other:ProfilerEvent)->float
torch._C._autograd.ProfilerEvent.cpu_memory_usage(self)->int
torch._C._autograd.ProfilerEvent.cuda_elapsed_us(self,other:ProfilerEvent)->float
torch._C._autograd.ProfilerEvent.cuda_memory_usage(self)->int
torch._C._autograd.ProfilerEvent.device(self)->int
torch._C._autograd.ProfilerEvent.flops(self)->float
torch._C._autograd.ProfilerEvent.handle(self)->int
torch._C._autograd.ProfilerEvent.has_cuda(self)->bool
torch._C._autograd.ProfilerEvent.is_async(self)->bool
torch._C._autograd.ProfilerEvent.is_remote(self)->bool
torch._C._autograd.ProfilerEvent.kind(self)->int
torch._C._autograd.ProfilerEvent.name(self)->str
torch._C._autograd.ProfilerEvent.node_id(self)->int
torch._C._autograd.ProfilerEvent.sequence_nr(self)->int
torch._C._autograd.ProfilerEvent.shapes(self)->List[List[int]]
torch._C._autograd.ProfilerEvent.thread_id(self)->int
torch._C._autograd.ProfilerState(Enum)
torch._C._autograd.SavedTensor
torch._C._autograd._KinetoEvent
torch._C._autograd._KinetoEvent.device_index(self)->int
torch._C._autograd._KinetoEvent.duration_us(self)->int
torch._C._autograd._KinetoEvent.is_async(self)->bool
torch._C._autograd._KinetoEvent.name(self)->str
torch._C._autograd._KinetoEvent.start_us(self)->int
torch._C._autograd._ProfilerResult
torch._C._autograd._ProfilerResult.events(self)->List[_KinetoEvent]
torch._C._autograd._ProfilerResult.legacy_events(self)->List[List[ProfilerEvent]]
torch._C._autograd._ProfilerResult.save(self,path:str)->None
torch._C._autograd._add_metadata_json(key:str,value:str)->None
torch._C._autograd._disable_profiler()->_ProfilerResult
torch._C._autograd._disable_profiler_legacy()->List[List[ProfilerEvent]]
torch._C._autograd._enable_profiler(config:ProfilerConfig,activities:Set[ProfilerActivity])->None
torch._C._autograd._enable_profiler_legacy(config:ProfilerConfig)->None
torch._C._autograd._enable_record_function(enable:bool)->None
torch._C._autograd._pop_saved_tensors_default_hooks()->None
torch._C._autograd._prepare_profiler(config:ProfilerConfig,activities:Set[ProfilerActivity])->None
torch._C._autograd._profiler_enabled()->bool
torch._C._autograd._push_saved_tensors_default_hooks(pack_hook:Callable,unpack_hook:Callable)->None
torch._C._autograd._record_function_with_args_enter(name:str,args:List[Any])->torch.Tensor
torch._C._autograd._record_function_with_args_exit(handle:torch.Tensor)->None
torch._C._autograd._set_empty_test_observer(is_global:bool,sampling_prob:float)->None
torch._C._autograd._supported_activities()->Set[ProfilerActivity]
torch._C._autograd.kineto_available()->bool


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/init.py----------------------------------------
A:torch.nn.init.l->norm_cdf((a - mean) / std)
A:torch.nn.init.u->norm_cdf((b - mean) / std)
A:torch.nn.init.dimensions->tensor.dim()
A:torch.nn.init.sizes->tensor.size()
A:torch.nn.init.min_dim->min(out_chans_per_grp, sizes[1])
A:torch.nn.init.num_input_fmaps->tensor.size(1)
A:torch.nn.init.num_output_fmaps->tensor.size(0)
A:torch.nn.init.(fan_in, fan_out)->_calculate_fan_in_and_fan_out(tensor)
A:torch.nn.init.mode->mode.lower().lower()
A:torch.nn.init.fan->_calculate_correct_fan(tensor, mode)
A:torch.nn.init.gain->calculate_gain(nonlinearity, a)
A:torch.nn.init.rows->tensor.size(0)
A:torch.nn.init.flattened->tensor.new(rows, cols).normal_(0, 1)
A:torch.nn.init.(q, r)->torch.linalg.qr(flattened)
A:torch.nn.init.d->torch.diag(r, 0)
A:torch.nn.init.ph->torch.diag(r, 0).sign()
A:torch.nn.init.num_zeros->int(math.ceil(sparsity * rows))
A:torch.nn.init.row_indices->torch.randperm(rows)
A:torch.nn.init.deprecated_init.__doc__->'\n    {old_name}(...)\n\n    .. warning::\n        This method is now deprecated in favor of :func:`torch.nn.init.{new_name}`.\n\n    See :func:`~torch.nn.init.{new_name}` for details.'.format(old_name=old_name, new_name=new_name)
A:torch.nn.init.uniform->_make_deprecate(uniform_)
A:torch.nn.init.normal->_make_deprecate(normal_)
A:torch.nn.init.constant->_make_deprecate(constant_)
A:torch.nn.init.eye->_make_deprecate(eye_)
A:torch.nn.init.dirac->_make_deprecate(dirac_)
A:torch.nn.init.xavier_uniform->_make_deprecate(xavier_uniform_)
A:torch.nn.init.xavier_normal->_make_deprecate(xavier_normal_)
A:torch.nn.init.kaiming_uniform->_make_deprecate(kaiming_uniform_)
A:torch.nn.init.kaiming_normal->_make_deprecate(kaiming_normal_)
A:torch.nn.init.orthogonal->_make_deprecate(orthogonal_)
A:torch.nn.init.sparse->_make_deprecate(sparse_)
torch.nn.init._calculate_correct_fan(tensor,mode)
torch.nn.init._calculate_fan_in_and_fan_out(tensor)
torch.nn.init._make_deprecate(meth)
torch.nn.init._no_grad_fill_(tensor,val)
torch.nn.init._no_grad_normal_(tensor,mean,std)
torch.nn.init._no_grad_trunc_normal_(tensor,mean,std,a,b)
torch.nn.init._no_grad_uniform_(tensor,a,b)
torch.nn.init._no_grad_zero_(tensor)
torch.nn.init.calculate_gain(nonlinearity,param=None)
torch.nn.init.constant_(tensor:Tensor,val:float)->Tensor
torch.nn.init.dirac_(tensor,groups=1)
torch.nn.init.eye_(tensor)
torch.nn.init.kaiming_normal_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.kaiming_uniform_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.normal_(tensor:Tensor,mean:float=0.0,std:float=1.0)->Tensor
torch.nn.init.ones_(tensor:Tensor)->Tensor
torch.nn.init.orthogonal_(tensor,gain=1)
torch.nn.init.sparse_(tensor,sparsity,std=0.01)
torch.nn.init.trunc_normal_(tensor:Tensor,mean:float=0.0,std:float=1.0,a:float=-2.0,b:float=2.0)->Tensor
torch.nn.init.uniform_(tensor:Tensor,a:float=0.0,b:float=1.0)->Tensor
torch.nn.init.xavier_normal_(tensor:Tensor,gain:float=1.0)->Tensor
torch.nn.init.xavier_uniform_(tensor:Tensor,gain:float=1.0)->Tensor
torch.nn.init.zeros_(tensor:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/common_types.py----------------------------------------
A:torch.nn.common_types.T->TypeVar('T')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parameter.py----------------------------------------
A:torch.nn.parameter.data->torch.empty(0, **factory_kwargs)
A:torch.nn.parameter.result->type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)
A:torch.nn.parameter.self.data->torch.empty(shape, device=device, dtype=dtype)
torch.nn.Parameter(cls,data=None,requires_grad=True)
torch.nn.Parameter.__deepcopy__(self,memo)
torch.nn.Parameter.__reduce_ex__(self,proto)
torch.nn.Parameter.__repr__(self)
torch.nn.UninitializedBuffer(cls,requires_grad=False,device=None,dtype=None)
torch.nn.UninitializedParameter(cls,requires_grad=True,device=None,dtype=None)
torch.nn.parameter.Parameter(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__deepcopy__(self,memo)
torch.nn.parameter.Parameter.__new__(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__reduce_ex__(self,proto)
torch.nn.parameter.Parameter.__repr__(self)
torch.nn.parameter.UninitializedBuffer(cls,requires_grad=False,device=None,dtype=None)
torch.nn.parameter.UninitializedBuffer.__new__(cls,requires_grad=False,device=None,dtype=None)
torch.nn.parameter.UninitializedParameter(cls,requires_grad=True,device=None,dtype=None)
torch.nn.parameter.UninitializedParameter.__new__(cls,requires_grad=True,device=None,dtype=None)
torch.nn.parameter.UninitializedTensorMixin
torch.nn.parameter.UninitializedTensorMixin.__reduce_ex__(self,proto)
torch.nn.parameter.UninitializedTensorMixin.__repr__(self)
torch.nn.parameter.UninitializedTensorMixin.__torch_function__(cls,func,types,args=(),kwargs=None)
torch.nn.parameter.UninitializedTensorMixin.materialize(self,shape,device=None,dtype=None)
torch.nn.parameter.UninitializedTensorMixin.shape(self)
torch.nn.parameter.UninitializedTensorMixin.share_memory_(self)
torch.nn.parameter.is_lazy(param)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parameter.pyi----------------------------------------
torch.nn.UninitializedBuffer.materialize(self,shape:Tuple[int,...],device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None)
torch.nn.UninitializedParameter.materialize(self,shape:Tuple[int,...],device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None)
torch.nn.parameter.Parameter.__init__(self,data:Tensor=...,requires_grad:builtins.bool=...)
torch.nn.parameter.UninitializedBuffer.__init__(self,data:Tensor=...,requires_grad:builtins.bool=...)
torch.nn.parameter.UninitializedBuffer.materialize(self,shape:Tuple[int,...],device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None)
torch.nn.parameter.UninitializedParameter.__init__(self,data:Tensor=...,requires_grad:builtins.bool=...)
torch.nn.parameter.UninitializedParameter.materialize(self,shape:Tuple[int,...],device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/grad.py----------------------------------------
A:torch.nn.grad.input_size->list(input_size)
A:torch.nn.grad.stride->_triple(stride)
A:torch.nn.grad.padding->_triple(padding)
A:torch.nn.grad.dilation->_triple(dilation)
A:torch.nn.grad.grad_input_padding->_grad_input_padding(grad_output, input_size, stride, padding, kernel_size, dilation)
A:torch.nn.grad.grad_output->grad_output.contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4]).contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4])
A:torch.nn.grad.input->input.contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4]).contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4])
A:torch.nn.grad.grad_weight->grad_weight.contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4]).contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4])
torch.nn.grad._grad_input_padding(grad_output,input_size,stride,padding,kernel_size,dilation=None)
torch.nn.grad.conv1d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv1d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/functional.py----------------------------------------
A:torch.nn.functional.conv1d->_add_docstr(torch.conv1d, '\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n'.format(**reproducibility_notes, **tf32_notes) + "\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or\n      a one-element tuple `(sW,)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a one-element tuple `(padW,)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n    dilation: the spacing between kernel elements. Can be a single number or\n      a one-element tuple `(dW,)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(33, 16, 30)\n    >>> filters = torch.randn(20, 16, 5)\n    >>> F.conv1d(inputs, filters)\n")
A:torch.nn.functional.conv2d->_add_docstr(torch.conv2d, '\nconv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n'.format(**reproducibility_notes, **tf32_notes) + "\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a tuple `(padH, padW)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8, 4, 3, 3)\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> F.conv2d(inputs, filters, padding=1)\n")
A:torch.nn.functional.conv3d->_add_docstr(torch.conv3d, '\nconv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 3D convolution over an input image composed of several input\nplanes.\n\n{tf32_note}\n\nSee :class:`~torch.nn.Conv3d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n'.format(**reproducibility_notes, **tf32_notes) + "\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'},\n      single number or a tuple `(padT, padH, padW)`. Default: 0\n      ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n      the input so the output has the same shape as the input. However, this mode\n      doesn't support any stride values other than 1.\n\n      .. warning::\n          For ``padding='same'``, if the ``weight`` is even-length and\n          ``dilation`` is odd in any dimension, a full :func:`pad` operation\n          may be needed internally. Lowering performance.\n\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3, 3, 3)\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> F.conv3d(inputs, filters)\n")
A:torch.nn.functional.conv_transpose1d->_add_docstr(torch.conv_transpose1d, '\nconv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called "deconvolution".\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n'.format(**reproducibility_notes, **tf32_notes) + '\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sW,)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padW,)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dW,)``. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> weights = torch.randn(16, 33, 5)\n    >>> F.conv_transpose1d(inputs, weights)\n')
A:torch.nn.functional.conv_transpose2d->_add_docstr(torch.conv_transpose2d, '\nconv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution".\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n'.format(**reproducibility_notes, **tf32_notes) + '\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n      Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dH, dW)``. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> weights = torch.randn(4, 8, 3, 3)\n    >>> F.conv_transpose2d(inputs, weights, padding=1)\n')
A:torch.nn.functional.conv_transpose3d->_add_docstr(torch.conv_transpose3d, '\nconv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution"\n\n{tf32_note}\n\nSee :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n\nNote:\n    {cudnn_reproducibility_note}\n'.format(**reproducibility_notes, **tf32_notes) + '\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sT, sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padT, padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple\n      ``(out_padT, out_padH, out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> weights = torch.randn(16, 33, 3, 3, 3)\n    >>> F.conv_transpose3d(inputs, weights)\n')
A:torch.nn.functional.conv_tbc->_add_docstr(torch.conv_tbc, '\nApplies a 1-dimensional sequence convolution over an input sequence.\nInput and output dimensions are (Time, Batch, Channels) - hence TBC.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n    bias: bias of shape (:math:`\\text{out\\_channels}`)\n    pad: number of timesteps to pad. Default: 0\n')
A:torch.nn.functional.avg_pool1d->_add_docstr(torch.avg_pool1d, '\navg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple `(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])\n\n')
A:torch.nn.functional.avg_pool2d->_add_docstr(torch._C._nn.avg_pool2d, '\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\n\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n    divisor_override: if specified, it will be used as divisor, otherwise\n         size of the pooling region will be used. Default: None\n')
A:torch.nn.functional.avg_pool3d->_add_docstr(torch._C._nn.avg_pool3d, '\navg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\nsize :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n\nSee :class:`~torch.nn.AvgPool3d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kT, kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`, Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation\n    divisor_override: if specified, it will be used as divisor, otherwise\n        size of the pooling region will be used. Default: None\n')
A:torch.nn.functional._output_ratio->_triple(output_ratio)
A:torch.nn.functional._random_samples->torch.rand(n_batch, input.size(-4), 3, dtype=input.dtype, device=input.device)
A:torch.nn.functional.fractional_max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool2d_with_indices, if_false=_fractional_max_pool2d, module_name=__name__, func_name='fractional_max_pool2d')
A:torch.nn.functional.fractional_max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool3d_with_indices, if_false=_fractional_max_pool3d, module_name=__name__, func_name='fractional_max_pool3d')
A:torch.nn.functional.stride->torch.jit.annotate(List[int], [])
A:torch.nn.functional.max_pool1d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool1d_with_indices, if_false=_max_pool1d, module_name=__name__, func_name='max_pool1d')
A:torch.nn.functional.max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool2d_with_indices, if_false=_max_pool2d, module_name=__name__, func_name='max_pool2d')
A:torch.nn.functional.max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool3d_with_indices, if_false=_max_pool3d, module_name=__name__, func_name='max_pool3d')
A:torch.nn.functional.input_size->input.reshape(-1).size()
A:torch.nn.functional.default_size->torch.jit.annotate(List[int], [])
A:torch.nn.functional.kernel_size->_triple(kernel_size)
A:torch.nn.functional._stride->_triple(stride)
A:torch.nn.functional.padding->_triple(padding)
A:torch.nn.functional.output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.(kw, kh)->modules.utils._pair(kernel_size)
A:torch.nn.functional.out->input.reshape(-1).new_empty(out_shape)
A:torch.nn.functional.adaptive_max_pool1d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool1d_with_indices, if_false=_adaptive_max_pool1d, module_name=__name__, func_name='adaptive_max_pool1d')
A:torch.nn.functional.adaptive_max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool2d_with_indices, if_false=_adaptive_max_pool2d, module_name=__name__, func_name='adaptive_max_pool2d')
A:torch.nn.functional.adaptive_max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool3d_with_indices, if_false=_adaptive_max_pool3d, module_name=__name__, func_name='adaptive_max_pool3d')
A:torch.nn.functional.adaptive_avg_pool1d->_add_docstr(torch.adaptive_avg_pool1d, '\nadaptive_avg_pool1d(input, output_size) -> Tensor\n\nApplies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\n\nSee :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n\nArgs:\n    output_size: the target output size (single integer)\n')
A:torch.nn.functional._output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.inp_dim->input.reshape(-1).dim()
A:torch.nn.functional.result->torch.rrelu(input, lower, upper, training)
A:torch.nn.functional.threshold_->_add_docstr(_VF.threshold_, '\nthreshold_(input, threshold, value) -> Tensor\n\nIn-place version of :func:`~threshold`.\n')
A:torch.nn.functional.relu_->_add_docstr(torch.relu_, '\nrelu_(input) -> Tensor\n\nIn-place version of :func:`~relu`.\n')
A:torch.nn.functional.hardtanh_->_add_docstr(torch._C._nn.hardtanh_, '\nhardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n\nIn-place version of :func:`~hardtanh`.\n')
A:torch.nn.functional.elu_->_add_docstr(torch._C._nn.elu_, '\nelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~elu`.\n')
A:torch.nn.functional.selu_->_add_docstr(torch.selu_, '\nselu_(input) -> Tensor\n\nIn-place version of :func:`~selu`.\n')
A:torch.nn.functional.celu_->_add_docstr(torch.celu_, '\ncelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~celu`.\n')
A:torch.nn.functional.leaky_relu_->_add_docstr(torch._C._nn.leaky_relu_, '\nleaky_relu_(input, negative_slope=0.01) -> Tensor\n\nIn-place version of :func:`~leaky_relu`.\n')
A:torch.nn.functional.prelu->_add_docstr(torch.prelu, 'prelu(input, weight) -> Tensor\n\nApplies element-wise the function\n:math:`\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)` where weight is a\nlearnable parameter.\n\nSee :class:`~torch.nn.PReLU` for more details.\n')
A:torch.nn.functional.rrelu_->_add_docstr(torch.rrelu_, '\nrrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n\nIn-place version of :func:`~rrelu`.\n')
A:torch.nn.functional.logsigmoid->_add_docstr(torch._C._nn.log_sigmoid, '\nlogsigmoid(input) -> Tensor\n\nApplies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n\nSee :class:`~torch.nn.LogSigmoid` for more details.\n')
A:torch.nn.functional.gelu->_add_docstr(torch._C._nn.gelu, '\ngelu(input) -> Tensor\n\nApplies element-wise the function\n:math:`\\text{GELU}(x) = x * \\Phi(x)`\n\nwhere :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\nSee `Gaussian Error Linear Units (GELUs) <https://arxiv.org/abs/1606.08415>`_.\n')
A:torch.nn.functional.hardshrink->_add_docstr(torch.hardshrink, '\nhardshrink(input, lambd=0.5) -> Tensor\n\nApplies the hard shrinkage function element-wise\n\nSee :class:`~torch.nn.Hardshrink` for more details.\n')
A:torch.nn.functional.softplus->_add_docstr(torch._C._nn.softplus, '\nsoftplus(input, beta=1, threshold=20) -> Tensor\n\nApplies element-wise, the function :math:`\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))`.\n\nFor numerical stability the implementation reverts to the linear function\nwhen :math:`input \\times \\beta > threshold`.\n\nSee :class:`~torch.nn.Softplus` for more details.\n')
A:torch.nn.functional.dim->input.reshape(-1).dim()
A:torch.nn.functional.ret->loss.sum()
A:torch.nn.functional.y_soft->gumbels.softmax(dim)
A:torch.nn.functional.y_hard->torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)
A:torch.nn.functional.softshrink->_add_docstr(torch._C._nn.softshrink, '\nsoftshrink(input, lambd=0.5) -> Tensor\n\nApplies the soft shrinkage function elementwise\n\nSee :class:`~torch.nn.Softshrink` for more details.\n')
A:torch.nn.functional.linear->_add_docstr(torch._C._nn.linear, '\nlinear(input, weight, bias=None) -> Tensor\n\nApplies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n\nThis operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nShape:\n\n    - Input: :math:`(*, in\\_features)` where `*` means any number of\n      additional dimensions, including none\n    - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n    - Bias: :math:`(out\\_features)` or :math:`()`\n    - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight\n')
A:torch.nn.functional.bilinear->_add_docstr(torch.bilinear, '\nbilinear(input1, input2, weight, bias=None) -> Tensor\n\nApplies a bilinear transformation to the incoming data:\n:math:`y = x_1^T A x_2 + b`\n\nShape:\n\n    - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}`\n      and :math:`*` means any number of additional dimensions.\n      All but the last dimension of the inputs should be the same.\n    - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`\n    - weight: :math:`(\\text{out\\_features}, \\text{in1\\_features},\n      \\text{in2\\_features})`\n    - bias: :math:`(\\text{out\\_features})`\n    - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n      and all but the last dimension are the same shape as the input.\n')
A:torch.nn.functional.input->input.reshape(-1).reshape(-1)
A:torch.nn.functional.type_str->str(type(offsets))
A:torch.nn.functional.offsets->torch.arange(0, input.numel(), input.size(1), dtype=input.dtype, device=input.device)
A:torch.nn.functional.per_sample_weights->per_sample_weights.reshape(-1).reshape(-1)
A:torch.nn.functional.(ret, _, _, _)->torch.embedding_bag(weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset, padding_idx)
A:torch.nn.functional.embedding_bag.__doc__->embedding_bag.__doc__.format(**reproducibility_notes)
A:torch.nn.functional.div->div.mul(alpha).add(k).pow(beta).mul(alpha).add(k).pow(beta)
A:torch.nn.functional.sizes->input.reshape(-1).reshape(-1).size()
A:torch.nn.functional.ctc_loss.__doc__->ctc_loss.__doc__.format(**reproducibility_notes)
A:torch.nn.functional.reduction->_Reduction.legacy_get_string(size_average, reduce)
A:torch.nn.functional.var->var.clone().clone()
A:torch.nn.functional.reduction_enum->_Reduction.get_enum(reduction)
A:torch.nn.functional.reduced->torch.kl_div(input, target, reduction_enum, log_target=log_target)
A:torch.nn.functional.new_size->_infer_size(target.size(), weight.size())
A:torch.nn.functional.weight->weight.expand(new_size).expand(new_size)
A:torch.nn.functional.(expanded_input, expanded_target)->torch.broadcast_tensors(input, target)
A:torch.nn.functional.C->input.reshape(-1).reshape(-1).size(class_dim)
A:torch.nn.functional.pixel_shuffle->_add_docstr(torch.pixel_shuffle, '\npixel_shuffle(input, upscale_factor) -> Tensor\n\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is the :attr:`upscale_factor`.\n\nSee :class:`~torch.nn.PixelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    upscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n')
A:torch.nn.functional.pixel_unshuffle->_add_docstr(torch.pixel_unshuffle, '\npixel_unshuffle(input, downscale_factor) -> Tensor\n\nReverses the :class:`~torch.nn.PixelShuffle` operation by rearranging elements in a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)` to a tensor of shape\n:math:`(*, C \\times r^2, H, W)`, where r is the :attr:`downscale_factor`.\n\nSee :class:`~torch.nn.PixelUnshuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    downscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 1, 12, 12)\n    >>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 9, 4, 4])\n')
A:torch.nn.functional.channel_shuffle->_add_docstr(torch.channel_shuffle, '\nchannel_shuffle(input, groups) -> Tensor\n\nDivide the channels in a tensor of shape :math:`(*, C , H, W)`\ninto g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\nwhile keeping the original tensor shape.\n\nSee :class:`~torch.nn.ChannelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    groups (int): number of groups to divide channels in and rearrange.\n\nExamples::\n\n    >>> input = torch.randn(1, 4, 2, 2)\n    >>> print(input)\n    [[[[1, 2],\n       [3, 4]],\n      [[5, 6],\n       [7, 8]],\n      [[9, 10],\n       [11, 12]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n    >>> output = torch.nn.functional.channel_shuffle(input, 2)\n    >>> print(output)\n    [[[[1, 2],\n       [3, 4]],\n      [[9, 10],\n       [11, 12]],\n      [[5, 6],\n       [7, 8]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n')
A:torch.nn.functional.native_channel_shuffle->_add_docstr(torch.native_channel_shuffle, '\nnative_channel_shuffle(input, groups) -> Tensor\n\nNative kernel level implementation of the `channel_shuffle`.\nThis function might become private in future releases, use with caution.\n\nDivide the channels in a tensor of shape :math:`(*, C , H, W)`\ninto g groups and rearrange them as :math:`(*, C \\frac g, g, H, W)`,\nwhile keeping the original tensor shape.\n\nSee :class:`~torch.nn.ChannelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    groups (int): number of groups to divide channels in and rearrange.\n\nExamples::\n\n    >>> input = torch.randn(1, 4, 2, 2)\n    >>> print(input)\n    [[[[1, 2],\n       [3, 4]],\n      [[5, 6],\n       [7, 8]],\n      [[9, 10],\n       [11, 12]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n    >>> output = torch.nn.functional.native_channel_shuffle(input, 2)\n    >>> print(output)\n    [[[[1, 2],\n       [3, 4]],\n      [[9, 10],\n       [11, 12]],\n      [[5, 6],\n       [7, 8]],\n      [[13, 14],\n       [15, 16]],\n     ]]\n')
A:torch.nn.functional.upsample.__doc__->upsample.__doc__.format(**reproducibility_notes)
A:torch.nn.functional.interpolate.__doc__->interpolate.__doc__.format(**reproducibility_notes)
A:torch.nn.functional.upsample_nearest.__doc__->upsample_nearest.__doc__.format(**reproducibility_notes)
A:torch.nn.functional.upsample_bilinear.__doc__->upsample_bilinear.__doc__.format(**reproducibility_notes)
A:torch.nn.functional.pairwise_distance->_add_docstr(torch.pairwise_distance, '\npairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False) -> Tensor\n\nSee :class:`torch.nn.PairwiseDistance` for details\n')
A:torch.nn.functional.pdist->_add_docstr(torch.pdist, "\npdist(input, p=2) -> Tensor\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape :math:`N \\times M` then the output will have shape\n:math:`\\frac{1}{2} N (N - 1)`.\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n'minkowski', p=p)` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\nequivalent to `scipy.spatial.distance.pdist(input, 'hamming') * M`.\nWhen :math:`p = \\infty`, the closest scipy function is\n`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.\n\nArgs:\n    input: input tensor of shape :math:`N \\times M`.\n    p: p value for the p-norm distance to calculate between each vector pair\n        :math:`\\in [0, \\infty]`.\n")
A:torch.nn.functional.cosine_similarity->_add_docstr(torch.cosine_similarity, '\ncosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\nto a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\nsqueezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 fewer dimension.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\n\nSupports :ref:`type promotion <type-promotion-doc>`.\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input.\n    dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)\n')
A:torch.nn.functional.one_hot->_add_docstr(torch._C._nn.one_hot, '\none_hot(tensor, num_classes=-1) -> LongTensor\n\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.\n\nSee also `One-hot on Wikipedia`_ .\n\n.. _One-hot on Wikipedia:\n    https://en.wikipedia.org/wiki/One-hot\n\nArguments:\n    tensor (LongTensor): class values of any shape.\n    num_classes (int):  Total number of classes. If set to -1, the number\n        of classes will be inferred as one greater than the largest class\n        value in the input tensor.\n\nReturns:\n    LongTensor that has one more dimension with 1 values at the\n    index of last dimension indicated by the input, and 0 everywhere\n    else.\n\nExamples:\n    >>> F.one_hot(torch.arange(0, 5) % 3)\n    tensor([[1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 1, 0]])\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n    tensor([[1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0]])\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n    tensor([[[1, 0, 0],\n             [0, 1, 0]],\n            [[0, 0, 1],\n             [1, 0, 0]],\n            [[0, 1, 0],\n             [0, 0, 1]]])\n')
A:torch.nn.functional.positive_dist->distance_function(anchor, positive)
A:torch.nn.functional.negative_dist->torch.min(negative_dist, swap_dist)
A:torch.nn.functional.swap_dist->distance_function(positive, negative)
A:torch.nn.functional.output->torch.bmm(attn, v)
A:torch.nn.functional.denom->input.reshape(-1).reshape(-1).norm(p, dim, keepdim=True).clamp_min_(eps).expand_as(input)
A:torch.nn.functional.ndim->len(paddable_shape)
A:torch.nn.functional.out_d0->max(padding[-2], 0)
A:torch.nn.functional.in_d0->max(-padding[-2], 0)
A:torch.nn.functional.out_h0->max(padding[-4], 0)
A:torch.nn.functional.in_h0->max(-padding[-4], 0)
A:torch.nn.functional.out_w0->max(padding[-6], 0)
A:torch.nn.functional.in_w0->max(-padding[-6], 0)
A:torch.nn.functional.i0->max(padding[-6], 0)
A:torch.nn.functional.E->q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1).size(-1)
A:torch.nn.functional.(w_q, w_kv)->w.split([E, E * 2])
A:torch.nn.functional.(b_q, b_kv)->b.split([E, E * 2])
A:torch.nn.functional.(w_q, w_k, w_v)->w.chunk(3)
A:torch.nn.functional.(b_q, b_k, b_v)->in_proj_bias.chunk(3)
A:torch.nn.functional.attn->dropout(attn, p=dropout_p)
A:torch.nn.functional.is_batched->_mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)
A:torch.nn.functional.query->query.unsqueeze(1).unsqueeze(1)
A:torch.nn.functional.key->key.unsqueeze(1).unsqueeze(1)
A:torch.nn.functional.value->value.unsqueeze(1).unsqueeze(1)
A:torch.nn.functional.key_padding_mask->key_padding_mask.view(bsz, 1, 1, src_len).expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len).view(bsz, 1, 1, src_len).expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
A:torch.nn.functional.head_dim->embed_dim.div(num_heads, rounding_mode='trunc')
A:torch.nn.functional.(q, k, v)->_in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)
A:torch.nn.functional.attn_mask->attn_mask.masked_fill(key_padding_mask, float('-inf')).masked_fill(key_padding_mask, float('-inf'))
A:torch.nn.functional.k->torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)
A:torch.nn.functional.v->torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)
A:torch.nn.functional.q->q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1).contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
A:torch.nn.functional.src_len->torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1).size(1)
A:torch.nn.functional.new_attn_mask->torch.zeros_like(attn_mask, dtype=q.dtype)
A:torch.nn.functional.(attn_output, attn_output_weights)->_scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
A:torch.nn.functional.attn_output->attn_output.squeeze(1).squeeze(1)
A:torch.nn.functional.attn_output_weights->attn_output_weights.squeeze(0).squeeze(0)
torch.nn.functional._adaptive_max_pool1d(input:Tensor,output_size:BroadcastingList1[int],return_indices:bool=False)->Tensor
torch.nn.functional._adaptive_max_pool2d(input:Tensor,output_size:BroadcastingList2[int],return_indices:bool=False)->Tensor
torch.nn.functional._adaptive_max_pool3d(input:Tensor,output_size:BroadcastingList3[int],return_indices:bool=False)->Tensor
torch.nn.functional._fractional_max_pool2d(input:Tensor,kernel_size:BroadcastingList2[int],output_size:Optional[BroadcastingList2[int]]=None,output_ratio:Optional[BroadcastingList2[float]]=None,return_indices:bool=False,_random_samples:Optional[Tensor]=None)->Tensor
torch.nn.functional._fractional_max_pool3d(input:Tensor,kernel_size:BroadcastingList3[int],output_size:Optional[BroadcastingList3[int]]=None,output_ratio:Optional[BroadcastingList3[float]]=None,return_indices:bool=False,_random_samples:Optional[Tensor]=None)->Tensor
torch.nn.functional._get_softmax_dim(name:str,ndim:int,stacklevel:int)->int
torch.nn.functional._in_projection(q:Tensor,k:Tensor,v:Tensor,w_q:Tensor,w_k:Tensor,w_v:Tensor,b_q:Optional[Tensor]=None,b_k:Optional[Tensor]=None,b_v:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor]
torch.nn.functional._in_projection_packed(q:Tensor,k:Tensor,v:Tensor,w:Tensor,b:Optional[Tensor]=None)->List[Tensor]
torch.nn.functional._max_pool1d(input:Tensor,kernel_size:BroadcastingList1[int],stride:Optional[BroadcastingList1[int]]=None,padding:BroadcastingList1[int]=0,dilation:BroadcastingList1[int]=1,ceil_mode:bool=False,return_indices:bool=False)->Tensor
torch.nn.functional._max_pool2d(input:Tensor,kernel_size:BroadcastingList2[int],stride:Optional[BroadcastingList2[int]]=None,padding:BroadcastingList2[int]=0,dilation:BroadcastingList2[int]=1,ceil_mode:bool=False,return_indices:bool=False)->Tensor
torch.nn.functional._max_pool3d(input:Tensor,kernel_size:BroadcastingList3[int],stride:Optional[BroadcastingList3[int]]=None,padding:BroadcastingList3[int]=0,dilation:BroadcastingList3[int]=1,ceil_mode:bool=False,return_indices:bool=False)->Tensor
torch.nn.functional._mha_shape_check(query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor],attn_mask:Optional[Tensor],num_heads:int)
torch.nn.functional._no_grad_embedding_renorm_(weight:Tensor,input:Tensor,max_norm:float,norm_type:float)->Tensor
torch.nn.functional._pad(input:Tensor,pad:List[int],mode:str='constant',value:float=0.0)->Tensor
torch.nn.functional._pad_circular(input:Tensor,padding:List[int])->Tensor
torch.nn.functional._scaled_dot_product_attention(q:Tensor,k:Tensor,v:Tensor,attn_mask:Optional[Tensor]=None,dropout_p:float=0.0)->Tuple[Tensor, Tensor]
torch.nn.functional._threshold(input:Tensor,threshold:float,value:float,inplace:bool=False)->Tensor
torch.nn.functional._unpool_output_size(input:Tensor,kernel_size:List[int],stride:List[int],padding:List[int],output_size:Optional[List[int]])->List[int]
torch.nn.functional._verify_batch_size(size:List[int])->None
torch.nn.functional._verify_spatial_size(size:List[int])->None
torch.nn.functional.adaptive_avg_pool2d(input:Tensor,output_size:BroadcastingList2[int])->Tensor
torch.nn.functional.adaptive_avg_pool3d(input:Tensor,output_size:BroadcastingList3[int])->Tensor
torch.nn.functional.adaptive_max_pool1d_with_indices(input:Tensor,output_size:BroadcastingList1[int],return_indices:bool=False)->Tuple[Tensor, Tensor]
torch.nn.functional.adaptive_max_pool2d_with_indices(input:Tensor,output_size:BroadcastingList2[int],return_indices:bool=False)->Tuple[Tensor, Tensor]
torch.nn.functional.adaptive_max_pool3d_with_indices(input:Tensor,output_size:BroadcastingList3[int],return_indices:bool=False)->Tuple[Tensor, Tensor]
torch.nn.functional.affine_grid(theta:Tensor,size:List[int],align_corners:Optional[bool]=None)->Tensor
torch.nn.functional.alpha_dropout(input:Tensor,p:float=0.5,training:bool=False,inplace:bool=False)->Tensor
torch.nn.functional.assert_int_or_pair(arg:List[int],arg_name:str,message:str)->None
torch.nn.functional.batch_norm(input:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,training:bool=False,momentum:float=0.1,eps:float=1e-05)->Tensor
torch.nn.functional.binary_cross_entropy(input:Tensor,target:Tensor,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.binary_cross_entropy_with_logits(input:Tensor,target:Tensor,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean',pos_weight:Optional[Tensor]=None)->Tensor
torch.nn.functional.celu(input:Tensor,alpha:float=1.0,inplace:bool=False)->Tensor
torch.nn.functional.cosine_embedding_loss(input1:Tensor,input2:Tensor,target:Tensor,margin:float=0,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.cross_entropy(input:Tensor,target:Tensor,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,ignore_index:int=-100,reduce:Optional[bool]=None,reduction:str='mean',label_smoothing:float=0.0)->Tensor
torch.nn.functional.ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:Tensor,target_lengths:Tensor,blank:int=0,reduction:str='mean',zero_infinity:bool=False)->Tensor
torch.nn.functional.dropout(input:Tensor,p:float=0.5,training:bool=True,inplace:bool=False)->Tensor
torch.nn.functional.dropout2d(input:Tensor,p:float=0.5,training:bool=True,inplace:bool=False)->Tensor
torch.nn.functional.dropout3d(input:Tensor,p:float=0.5,training:bool=True,inplace:bool=False)->Tensor
torch.nn.functional.elu(input:Tensor,alpha:float=1.0,inplace:bool=False)->Tensor
torch.nn.functional.embedding(input:Tensor,weight:Tensor,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False)->Tensor
torch.nn.functional.embedding_bag(input:Tensor,weight:Tensor,offsets:Optional[Tensor]=None,max_norm:Optional[float]=None,norm_type:float=2,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,per_sample_weights:Optional[Tensor]=None,include_last_offset:bool=False,padding_idx:Optional[int]=None)->Tensor
torch.nn.functional.feature_alpha_dropout(input:Tensor,p:float=0.5,training:bool=False,inplace:bool=False)->Tensor
torch.nn.functional.fold(input:Tensor,output_size:BroadcastingList2[int],kernel_size:BroadcastingList2[int],dilation:BroadcastingList2[int]=1,padding:BroadcastingList2[int]=0,stride:BroadcastingList2[int]=1)->Tensor
torch.nn.functional.fractional_max_pool2d_with_indices(input:Tensor,kernel_size:BroadcastingList2[int],output_size:Optional[BroadcastingList2[int]]=None,output_ratio:Optional[BroadcastingList2[float]]=None,return_indices:bool=False,_random_samples:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.nn.functional.fractional_max_pool3d_with_indices(input:Tensor,kernel_size:BroadcastingList3[int],output_size:Optional[BroadcastingList3[int]]=None,output_ratio:Optional[BroadcastingList3[float]]=None,return_indices:bool=False,_random_samples:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.nn.functional.gaussian_nll_loss(input:Tensor,target:Tensor,var:Tensor,full:bool=False,eps:float=1e-06,reduction:str='mean')->Tensor
torch.nn.functional.glu(input:Tensor,dim:int=-1)->Tensor
torch.nn.functional.grid_sample(input:Tensor,grid:Tensor,mode:str='bilinear',padding_mode:str='zeros',align_corners:Optional[bool]=None)->Tensor
torch.nn.functional.group_norm(input:Tensor,num_groups:int,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:float=1e-05)->Tensor
torch.nn.functional.gumbel_softmax(logits:Tensor,tau:float=1,hard:bool=False,eps:float=1e-10,dim:int=-1)->Tensor
torch.nn.functional.hardsigmoid(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.hardswish(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.hardtanh(input:Tensor,min_val:float=-1.0,max_val:float=1.0,inplace:bool=False)->Tensor
torch.nn.functional.hinge_embedding_loss(input:Tensor,target:Tensor,margin:float=1.0,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.huber_loss(input:Tensor,target:Tensor,reduction:str='mean',delta:float=1.0)->Tensor
torch.nn.functional.instance_norm(input:Tensor,running_mean:Optional[Tensor]=None,running_var:Optional[Tensor]=None,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,use_input_stats:bool=True,momentum:float=0.1,eps:float=1e-05)->Tensor
torch.nn.functional.interpolate(input:Tensor,size:Optional[int]=None,scale_factor:Optional[List[float]]=None,mode:str='nearest',align_corners:Optional[bool]=None,recompute_scale_factor:Optional[bool]=None,antialias:bool=False)->Tensor
torch.nn.functional.kl_div(input:Tensor,target:Tensor,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean',log_target:bool=False)->Tensor
torch.nn.functional.l1_loss(input:Tensor,target:Tensor,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.layer_norm(input:Tensor,normalized_shape:List[int],weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:float=1e-05)->Tensor
torch.nn.functional.leaky_relu(input:Tensor,negative_slope:float=0.01,inplace:bool=False)->Tensor
torch.nn.functional.local_response_norm(input:Tensor,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1.0)->Tensor
torch.nn.functional.log_softmax(input:Tensor,dim:Optional[int]=None,_stacklevel:int=3,dtype:Optional[DType]=None)->Tensor
torch.nn.functional.lp_pool1d(input:Tensor,norm_type:float,kernel_size:int,stride:Optional[BroadcastingList1[int]]=None,ceil_mode:bool=False)->Tensor
torch.nn.functional.lp_pool2d(input:Tensor,norm_type:float,kernel_size:int,stride:Optional[BroadcastingList2[int]]=None,ceil_mode:bool=False)->Tensor
torch.nn.functional.margin_ranking_loss(input1:Tensor,input2:Tensor,target:Tensor,margin:float=0,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.max_pool1d_with_indices(input:Tensor,kernel_size:BroadcastingList1[int],stride:Optional[BroadcastingList1[int]]=None,padding:BroadcastingList1[int]=0,dilation:BroadcastingList1[int]=1,ceil_mode:bool=False,return_indices:bool=False)->Tuple[Tensor, Tensor]
torch.nn.functional.max_pool2d_with_indices(input:Tensor,kernel_size:BroadcastingList2[int],stride:Optional[BroadcastingList2[int]]=None,padding:BroadcastingList2[int]=0,dilation:BroadcastingList2[int]=1,ceil_mode:bool=False,return_indices:bool=False)->Tuple[Tensor, Tensor]
torch.nn.functional.max_pool3d_with_indices(input:Tensor,kernel_size:BroadcastingList3[int],stride:Optional[BroadcastingList3[int]]=None,padding:BroadcastingList3[int]=0,dilation:BroadcastingList3[int]=1,ceil_mode:bool=False,return_indices:bool=False)->Tuple[Tensor, Tensor]
torch.nn.functional.max_unpool1d(input:Tensor,indices:Tensor,kernel_size:BroadcastingList1[int],stride:Optional[BroadcastingList1[int]]=None,padding:BroadcastingList1[int]=0,output_size:Optional[BroadcastingList1[int]]=None)->Tensor
torch.nn.functional.max_unpool2d(input:Tensor,indices:Tensor,kernel_size:BroadcastingList2[int],stride:Optional[BroadcastingList2[int]]=None,padding:BroadcastingList2[int]=0,output_size:Optional[BroadcastingList2[int]]=None)->Tensor
torch.nn.functional.max_unpool3d(input:Tensor,indices:Tensor,kernel_size:BroadcastingList3[int],stride:Optional[BroadcastingList3[int]]=None,padding:BroadcastingList3[int]=0,output_size:Optional[BroadcastingList3[int]]=None)->Tensor
torch.nn.functional.mish(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.mse_loss(input:Tensor,target:Tensor,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.multi_head_attention_forward(query:Tensor,key:Tensor,value:Tensor,embed_dim_to_check:int,num_heads:int,in_proj_weight:Tensor,in_proj_bias:Optional[Tensor],bias_k:Optional[Tensor],bias_v:Optional[Tensor],add_zero_attn:bool,dropout_p:float,out_proj_weight:Tensor,out_proj_bias:Optional[Tensor],training:bool=True,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,use_separate_proj_weight:bool=False,q_proj_weight:Optional[Tensor]=None,k_proj_weight:Optional[Tensor]=None,v_proj_weight:Optional[Tensor]=None,static_k:Optional[Tensor]=None,static_v:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.functional.multi_margin_loss(input:Tensor,target:Tensor,p:int=1,margin:float=1.0,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.multilabel_margin_loss(input:Tensor,target:Tensor,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.multilabel_soft_margin_loss(input:Tensor,target:Tensor,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.nll_loss(input:Tensor,target:Tensor,weight:Optional[Tensor]=None,size_average:Optional[bool]=None,ignore_index:int=-100,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.normalize(input:Tensor,p:float=2.0,dim:int=1,eps:float=1e-12,out:Optional[Tensor]=None)->Tensor
torch.nn.functional.poisson_nll_loss(input:Tensor,target:Tensor,log_input:bool=True,full:bool=False,size_average:Optional[bool]=None,eps:float=1e-08,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.relu(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.relu6(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.rrelu(input:Tensor,lower:float=1.0/8,upper:float=1.0/3,training:bool=False,inplace:bool=False)->Tensor
torch.nn.functional.selu(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.sigmoid(input)
torch.nn.functional.silu(input:Tensor,inplace:bool=False)->Tensor
torch.nn.functional.smooth_l1_loss(input:Tensor,target:Tensor,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean',beta:float=1.0)->Tensor
torch.nn.functional.soft_margin_loss(input:Tensor,target:Tensor,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.softmax(input:Tensor,dim:Optional[int]=None,_stacklevel:int=3,dtype:Optional[DType]=None)->Tensor
torch.nn.functional.softmin(input:Tensor,dim:Optional[int]=None,_stacklevel:int=3,dtype:Optional[DType]=None)->Tensor
torch.nn.functional.softsign(input)
torch.nn.functional.tanh(input)
torch.nn.functional.tanhshrink(input)
torch.nn.functional.triplet_margin_loss(anchor:Tensor,positive:Tensor,negative:Tensor,margin:float=1.0,p:float=2,eps:float=1e-06,swap:bool=False,size_average:Optional[bool]=None,reduce:Optional[bool]=None,reduction:str='mean')->Tensor
torch.nn.functional.triplet_margin_with_distance_loss(anchor:Tensor,positive:Tensor,negative:Tensor,*,distance_function:Optional[Callable[[Tensor,Tensor],Tensor]]=None,margin:float=1.0,swap:bool=False,reduction:str='mean')->Tensor
torch.nn.functional.unfold(input:Tensor,kernel_size:BroadcastingList2[int],dilation:BroadcastingList2[int]=1,padding:BroadcastingList2[int]=0,stride:BroadcastingList2[int]=1)->Tensor
torch.nn.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.functional.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/functional.pyi----------------------------------------
torch.nn.functional.adaptive_avg_pool1d(input:Tensor,output_size:_size_1_t)->Tensor
torch.nn.functional.bilinear(input1:Tensor,input2:Tensor,weight:Tensor,bias:Optional[Tensor]=...)->Tensor
torch.nn.functional.gelu(input:Any)
torch.nn.functional.hardshrink(input:Tensor,lambd:float=...)->Tensor
torch.nn.functional.linear(input:Tensor,weight:Tensor,bias:Optional[Tensor]=...)->Tensor
torch.nn.functional.pad(input:Tensor,pad:Sequence[int],mode:str=...,value:float=...)->Tensor
torch.nn.functional.pairwise_distance(x1:Tensor,x2:Tensor,p:float=...,eps:float=...,keepdim:bool=...)->Tensor
torch.nn.functional.prelu(input:Tensor,weight:Tensor)->Tensor
torch.nn.functional.threshold(input:Tensor,threshold:float,value:float,inplace:bool=...)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/_reduction.py----------------------------------------
torch.nn._reduction.get_enum(reduction:str)->int
torch.nn._reduction.legacy_get_enum(size_average:Optional[bool],reduce:Optional[bool],emit_warning:bool=True)->int
torch.nn._reduction.legacy_get_string(size_average:Optional[bool],reduce:Optional[bool],emit_warning:bool=True)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/__init__.py----------------------------------------
A:torch.nn.__init__.r->dict(kwargs.get('factory_kwargs', {}))
torch.nn.__init__.factory_kwargs(kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/cpp.py----------------------------------------
A:torch.nn.cpp.self._parameters->OrderedDictWrapper(cpp_module, '_parameters')
A:torch.nn.cpp.param.data->fn(param.data)
A:torch.nn.cpp.param._grad.data->fn(param._grad.data)
A:torch.nn.cpp.buf.data->fn(buf.data)
torch.nn.cpp.ModuleWrapper(self,cpp_module)
torch.nn.cpp.ModuleWrapper.__init__(self,cpp_module)
torch.nn.cpp.ModuleWrapper.__repr__(self)
torch.nn.cpp.ModuleWrapper._apply(self,fn)
torch.nn.cpp.ModuleWrapper.training(self)
torch.nn.cpp.ModuleWrapper.training(self,mode)
torch.nn.cpp.OrderedDictWrapper(self,cpp_module,attr)
torch.nn.cpp.OrderedDictWrapper.__contains__(self,key)
torch.nn.cpp.OrderedDictWrapper.__getitem__(self,key)
torch.nn.cpp.OrderedDictWrapper.__init__(self,cpp_module,attr)
torch.nn.cpp.OrderedDictWrapper.__iter__(self)
torch.nn.cpp.OrderedDictWrapper.__len__(self)
torch.nn.cpp.OrderedDictWrapper.cpp_dict(self)
torch.nn.cpp.OrderedDictWrapper.items(self)
torch.nn.cpp.OrderedDictWrapper.keys(self)
torch.nn.cpp.OrderedDictWrapper.values(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/init.py----------------------------------------
A:torch.nn.utils.init.final_device->kwargs.pop('device', 'cpu')
torch.nn.utils.init.skip_init(module_cls,*args,**kwargs)
torch.nn.utils.skip_init(module_cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/parametrize.py----------------------------------------
A:torch.nn.utils.parametrize.new->module.right_inverse(new)
A:torch.nn.utils.parametrize.self.is_tensor->isinstance(new, Tensor)
A:torch.nn.utils.parametrize.originali->Parameter(originali)
A:torch.nn.utils.parametrize.Z->parametrization.right_inverse(X)
A:torch.nn.utils.parametrize.value->module.right_inverse(value)
A:torch.nn.utils.parametrize.original_i->getattr(self, f'original{i}')
A:torch.nn.utils.parametrize.x->self[curr_idx](x)
A:torch.nn.utils.parametrize.param_cls->type(f'Parametrized{cls.__name__}', (cls,), {'__getstate__': getstate})
A:torch.nn.utils.parametrize.tensor->parametrization()
A:torch.nn.utils.parametrize.Y->getattr(module, tensor_name)
A:torch.nn.utils.parametrize.X->parametrization(Y)
A:torch.nn.utils.parametrize.original->getattr(module, tensor_name)
A:torch.nn.utils.parametrize.parametrizations->getattr(module, 'parametrizations', None)
A:torch.nn.utils.parametrize.module.parametrizations->ModuleDict()
A:torch.nn.utils.parametrize.t->getattr(module, tensor_name)
torch.nn.utils.parametrize.ParametrizationList(self,modules:Sequence[Module],original:Union[Tensor,Parameter],unsafe:bool=False)
torch.nn.utils.parametrize.ParametrizationList.__init__(self,modules:Sequence[Module],original:Union[Tensor,Parameter],unsafe:bool=False)
torch.nn.utils.parametrize.ParametrizationList.forward(self)->Tensor
torch.nn.utils.parametrize.ParametrizationList.right_inverse(self,value:Tensor)->None
torch.nn.utils.parametrize._inject_new_class(module:Module)->None
torch.nn.utils.parametrize._inject_property(module:Module,tensor_name:str)->None
torch.nn.utils.parametrize._register_parameter_or_buffer(module,name,X)
torch.nn.utils.parametrize.cached()
torch.nn.utils.parametrize.is_parametrized(module:Module,tensor_name:Optional[str]=None)->bool
torch.nn.utils.parametrize.register_parametrization(module:Module,tensor_name:str,parametrization:Module,*,unsafe:bool=False)->Module
torch.nn.utils.parametrize.remove_parametrizations(module:Module,tensor_name:str,leave_parametrized:bool=True)->Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/weight_norm.py----------------------------------------
A:torch.nn.utils.weight_norm.g->getattr(module, self.name + '_g')
A:torch.nn.utils.weight_norm.v->getattr(module, self.name + '_v')
A:torch.nn.utils.weight_norm.fn->WeightNorm(name, dim)
A:torch.nn.utils.weight_norm.weight->self.compute_weight(module)
A:torch.nn.utils.weight_norm.T_module->TypeVar('T_module', bound=Module)
torch.nn.utils.remove_weight_norm(module:T_module,name:str='weight')->T_module
torch.nn.utils.weight_norm(module:T_module,name:str='weight',dim:int=0)->T_module
torch.nn.utils.weight_norm.WeightNorm(self,name:str,dim:int)
torch.nn.utils.weight_norm.WeightNorm.__init__(self,name:str,dim:int)
torch.nn.utils.weight_norm.WeightNorm.apply(module,name:str,dim:int)->'WeightNorm'
torch.nn.utils.weight_norm.WeightNorm.compute_weight(self,module:Module)->Any
torch.nn.utils.weight_norm.WeightNorm.remove(self,module:Module)->None
torch.nn.utils.weight_norm.remove_weight_norm(module:T_module,name:str='weight')->T_module
torch.nn.utils.weight_norm.weight_norm(module:T_module,name:str='weight',dim:int=0)->T_module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/memory_format.py----------------------------------------
A:torch.nn.utils.memory_format.weight_data->module.weight.detach().clone().contiguous(memory_format=memory_format)
A:torch.nn.utils.memory_format.module.weight.data->module.weight.detach().clone().contiguous(memory_format=memory_format).resize_(weight_data.size(), memory_format=memory_format)
torch.nn.utils.convert_conv2d_weight_memory_format(module,memory_format)
torch.nn.utils.memory_format.convert_conv2d_weight_memory_format(module,memory_format)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/fusion.py----------------------------------------
A:torch.nn.utils.fusion.fused_conv->copy.deepcopy(conv)
A:torch.nn.utils.fusion.(fused_conv.weight, fused_conv.bias)->fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias, bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias, transpose)
A:torch.nn.utils.fusion.conv_b->torch.zeros_like(bn_rm)
A:torch.nn.utils.fusion.bn_w->torch.ones_like(bn_rm)
A:torch.nn.utils.fusion.bn_b->torch.zeros_like(bn_rm)
A:torch.nn.utils.fusion.bn_var_rsqrt->torch.rsqrt(bn_rv + bn_eps)
A:torch.nn.utils.fusion.fused_linear->copy.deepcopy(linear)
A:torch.nn.utils.fusion.(fused_linear.weight, fused_linear.bias)->fuse_linear_bn_weights(fused_linear.weight, fused_linear.bias, bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)
A:torch.nn.utils.fusion.linear_b->torch.zeros_like(bn_rm)
torch.nn.utils.fuse_conv_bn_eval(conv,bn,transpose=False)
torch.nn.utils.fuse_conv_bn_weights(conv_w,conv_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b,transpose=False)
torch.nn.utils.fusion.fuse_conv_bn_eval(conv,bn,transpose=False)
torch.nn.utils.fusion.fuse_conv_bn_weights(conv_w,conv_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b,transpose=False)
torch.nn.utils.fusion.fuse_linear_bn_eval(linear,bn)
torch.nn.utils.fusion.fuse_linear_bn_weights(linear_w,linear_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/prune.py----------------------------------------
A:torch.nn.utils.prune.mask->make_mask(t, self.dim, topk.indices)
A:torch.nn.utils.prune.orig->getattr(module, name + '_orig')
A:torch.nn.utils.prune.method->pruning_method(**kwargs)
A:torch.nn.utils.prune.container->PruningContainer()
A:torch.nn.utils.prune.default_mask->torch.nn.utils.parameters_to_vector([getattr(module, name + '_mask', torch.ones_like(getattr(module, name))) for (module, name) in parameters])
A:torch.nn.utils.prune.weight->self.apply_mask(module)
A:torch.nn.utils.prune.new_mask->new_mask.to(dtype=t.dtype).to(dtype=t.dtype)
A:torch.nn.utils.prune.n_dims->len(t.shape)
A:torch.nn.utils.prune.partial_mask->pruning_method(**kwargs).compute_mask(t[slc], default_mask=mask[slc])
A:torch.nn.utils.prune.new_mask[slc]->pruning_method(**kwargs).compute_mask(t[slc], default_mask=mask[slc]).to(dtype=new_mask.dtype)
A:torch.nn.utils.prune.tensor_size->t.nelement()
A:torch.nn.utils.prune.nparams_toprune->_compute_nparams_toprune(self.amount, tensor_size)
A:torch.nn.utils.prune.prob->torch.rand(nchannels)
A:torch.nn.utils.prune.topk->torch.topk(norm, k=nparams_tokeep, largest=True)
A:torch.nn.utils.prune.norm->torch.norm(t, p=n, dim=dims)
A:torch.nn.utils.prune.relevant_importance_scores->torch.nn.utils.parameters_to_vector([importance_scores.get((module, name), getattr(module, name)) for (module, name) in parameters])
A:torch.nn.utils.prune.final_mask->PruningContainer().compute_mask(relevant_importance_scores, default_mask)
A:torch.nn.utils.prune.param->getattr(module, name)
A:torch.nn.utils.prune.num_param->getattr(module, name).numel()
A:torch.nn.utils.prune.param_mask->final_mask[pointer:pointer + num_param].view_as(param)
A:torch.nn.utils.prune.dims->list(range(t.dim()))
torch.nn.utils.prune.BasePruningMethod(self)
torch.nn.utils.prune.BasePruningMethod.__init__(self)
torch.nn.utils.prune.BasePruningMethod.apply(cls,module,name,*args,importance_scores=None,**kwargs)
torch.nn.utils.prune.BasePruningMethod.apply_mask(self,module)
torch.nn.utils.prune.BasePruningMethod.compute_mask(self,t,default_mask)
torch.nn.utils.prune.BasePruningMethod.prune(self,t,default_mask=None,importance_scores=None)
torch.nn.utils.prune.BasePruningMethod.remove(self,module)
torch.nn.utils.prune.CustomFromMask(self,mask)
torch.nn.utils.prune.CustomFromMask.__init__(self,mask)
torch.nn.utils.prune.CustomFromMask.apply(cls,module,name,mask)
torch.nn.utils.prune.CustomFromMask.compute_mask(self,t,default_mask)
torch.nn.utils.prune.Identity(BasePruningMethod)
torch.nn.utils.prune.Identity.apply(cls,module,name)
torch.nn.utils.prune.Identity.compute_mask(self,t,default_mask)
torch.nn.utils.prune.L1Unstructured(self,amount)
torch.nn.utils.prune.L1Unstructured.__init__(self,amount)
torch.nn.utils.prune.L1Unstructured.apply(cls,module,name,amount,importance_scores=None)
torch.nn.utils.prune.L1Unstructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune.LnStructured(self,amount,n,dim=-1)
torch.nn.utils.prune.LnStructured.__init__(self,amount,n,dim=-1)
torch.nn.utils.prune.LnStructured.apply(cls,module,name,amount,n,dim,importance_scores=None)
torch.nn.utils.prune.LnStructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune.PruningContainer(self,*args)
torch.nn.utils.prune.PruningContainer.__getitem__(self,idx)
torch.nn.utils.prune.PruningContainer.__init__(self,*args)
torch.nn.utils.prune.PruningContainer.__iter__(self)
torch.nn.utils.prune.PruningContainer.__len__(self)
torch.nn.utils.prune.PruningContainer.add_pruning_method(self,method)
torch.nn.utils.prune.PruningContainer.compute_mask(self,t,default_mask)
torch.nn.utils.prune.RandomStructured(self,amount,dim=-1)
torch.nn.utils.prune.RandomStructured.__init__(self,amount,dim=-1)
torch.nn.utils.prune.RandomStructured.apply(cls,module,name,amount,dim=-1)
torch.nn.utils.prune.RandomStructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune.RandomUnstructured(self,amount)
torch.nn.utils.prune.RandomUnstructured.__init__(self,amount)
torch.nn.utils.prune.RandomUnstructured.apply(cls,module,name,amount)
torch.nn.utils.prune.RandomUnstructured.compute_mask(self,t,default_mask)
torch.nn.utils.prune._compute_norm(t,n,dim)
torch.nn.utils.prune._compute_nparams_toprune(amount,tensor_size)
torch.nn.utils.prune._validate_pruning_amount(amount,tensor_size)
torch.nn.utils.prune._validate_pruning_amount_init(amount)
torch.nn.utils.prune._validate_pruning_dim(t,dim)
torch.nn.utils.prune._validate_structured_pruning(t)
torch.nn.utils.prune.custom_from_mask(module,name,mask)
torch.nn.utils.prune.global_unstructured(parameters,pruning_method,importance_scores=None,**kwargs)
torch.nn.utils.prune.identity(module,name)
torch.nn.utils.prune.is_pruned(module)
torch.nn.utils.prune.l1_unstructured(module,name,amount,importance_scores=None)
torch.nn.utils.prune.ln_structured(module,name,amount,n,dim,importance_scores=None)
torch.nn.utils.prune.random_structured(module,name,amount,dim)
torch.nn.utils.prune.random_unstructured(module,name,amount)
torch.nn.utils.prune.remove(module,name)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/clip_grad.py----------------------------------------
A:torch.nn.utils.clip_grad.max_norm->float(max_norm)
A:torch.nn.utils.clip_grad.norm_type->float(norm_type)
A:torch.nn.utils.clip_grad.total_norm->torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
A:torch.nn.utils.clip_grad.clip_coef_clamped->torch.clamp(clip_coef, max=1.0)
A:torch.nn.utils.clip_grad.clip_value->float(clip_value)
torch.nn.utils.clip_grad.clip_grad_norm(parameters:_tensor_or_tensors,max_norm:float,norm_type:float=2.0,error_if_nonfinite:bool=False)->torch.Tensor
torch.nn.utils.clip_grad.clip_grad_norm_(parameters:_tensor_or_tensors,max_norm:float,norm_type:float=2.0,error_if_nonfinite:bool=False)->torch.Tensor
torch.nn.utils.clip_grad.clip_grad_value_(parameters:_tensor_or_tensors,clip_value:float)->None
torch.nn.utils.clip_grad_norm(parameters:_tensor_or_tensors,max_norm:float,norm_type:float=2.0,error_if_nonfinite:bool=False)->torch.Tensor
torch.nn.utils.clip_grad_norm_(parameters:_tensor_or_tensors,max_norm:float,norm_type:float=2.0,error_if_nonfinite:bool=False)->torch.Tensor
torch.nn.utils.clip_grad_value_(parameters:_tensor_or_tensors,clip_value:float)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/convert_parameters.py----------------------------------------
A:torch.nn.utils.convert_parameters.param_device->_check_param_device(param, param_device)
A:torch.nn.utils.convert_parameters.num_param->param.numel()
torch.nn.utils.convert_parameters._check_param_device(param:torch.Tensor,old_param_device:Optional[int])->int
torch.nn.utils.convert_parameters.parameters_to_vector(parameters:Iterable[torch.Tensor])->torch.Tensor
torch.nn.utils.convert_parameters.vector_to_parameters(vec:torch.Tensor,parameters:Iterable[torch.Tensor])->None
torch.nn.utils.parameters_to_vector(parameters:Iterable[torch.Tensor])->torch.Tensor
torch.nn.utils.vector_to_parameters(vec:torch.Tensor,parameters:Iterable[torch.Tensor])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/spectral_norm.py----------------------------------------
A:torch.nn.utils.spectral_norm.weight_mat->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig)
A:torch.nn.utils.spectral_norm.height->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size(0)
A:torch.nn.utils.spectral_norm.weight->state_dict.pop(weight_key)
A:torch.nn.utils.spectral_norm.u->normalize(weight.new_empty(h).normal_(0, 1), dim=0, eps=fn.eps)
A:torch.nn.utils.spectral_norm.v->SpectralNorm(name, n_power_iterations, dim, eps)._solve_v_and_rescale(weight_mat, u, sigma)
A:torch.nn.utils.spectral_norm.sigma->(weight_orig / weight).mean()
A:torch.nn.utils.spectral_norm.fn->SpectralNorm(name, n_power_iterations, dim, eps)
A:torch.nn.utils.spectral_norm.(h, w)->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size()
A:torch.nn.utils.spectral_norm.version->local_metadata.get('spectral_norm', {}).get(fn.name + '.version', None)
A:torch.nn.utils.spectral_norm.T_module->TypeVar('T_module', bound=Module)
torch.nn.utils.remove_spectral_norm(module:T_module,name:str='weight')->T_module
torch.nn.utils.spectral_norm(module:T_module,name:str='weight',n_power_iterations:int=1,eps:float=1e-12,dim:Optional[int]=None)->T_module
torch.nn.utils.spectral_norm.SpectralNorm(self,name:str='weight',n_power_iterations:int=1,dim:int=0,eps:float=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm.__init__(self,name:str='weight',n_power_iterations:int=1,dim:int=0,eps:float=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm._solve_v_and_rescale(self,weight_mat,u,target_sigma)
torch.nn.utils.spectral_norm.SpectralNorm.apply(module:Module,name:str,n_power_iterations:int,dim:int,eps:float)->'SpectralNorm'
torch.nn.utils.spectral_norm.SpectralNorm.compute_weight(self,module:Module,do_power_iteration:bool)->torch.Tensor
torch.nn.utils.spectral_norm.SpectralNorm.remove(self,module:Module)->None
torch.nn.utils.spectral_norm.SpectralNorm.reshape_weight_to_matrix(self,weight:torch.Tensor)->torch.Tensor
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook.__init__(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook.__init__(self,fn)
torch.nn.utils.spectral_norm.remove_spectral_norm(module:T_module,name:str='weight')->T_module
torch.nn.utils.spectral_norm.spectral_norm(module:T_module,name:str='weight',n_power_iterations:int=1,eps:float=1e-12,dim:Optional[int]=None)->T_module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/_stateless.py----------------------------------------
A:torch.nn.utils._stateless.param_cls->type(f'StatelessReplacer{cls.__name__}', (cls,), {'__getattribute__': _getattribute})
A:torch.nn.utils._stateless.out->module(args, **kwargs)
torch.nn.utils._stateless._apply_func_submodules(func:Callable[...,None],module:torch.nn.Module,path:List[str],args:Tuple)
torch.nn.utils._stateless._change_class(module)->None
torch.nn.utils._stateless._remove_swap(module,name:str)->None
torch.nn.utils._stateless._swap_parameters(module,tensor_name:str,tensor:Tensor)->None
torch.nn.utils._stateless.functional_call(module:torch.nn.Module,parameters_and_buffers:Dict[str,Tensor],args:Tuple,kwargs:Dict[str,Any]=None)
torch.nn.utils._stateless.reparametrize_module(module:torch.nn.Module,parameters_and_buffers:Dict[str,Tensor])->Iterator[None]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/rnn.py----------------------------------------
A:torch.nn.utils.rnn.PackedSequence_->namedtuple('PackedSequence_', ['data', 'batch_sizes', 'sorted_indices', 'unsorted_indices'])
A:torch.nn.utils.rnn.ex->torch.tensor((), dtype=self.data.dtype, device=self.data.device).to(*args, **kwargs)
A:torch.nn.utils.rnn.data->self.data.to(*args, **kwargs)
A:torch.nn.utils.rnn.sorted_indices->sorted_indices.to(input.device).to(input.device)
A:torch.nn.utils.rnn.unsorted_indices->invert_permutation(sorted_indices)
A:torch.nn.utils.rnn.(data, batch_sizes, sorted_indices, unsorted_indices)->_packed_sequence_init_args(data, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.utils.rnn.output->torch.empty_like(permutation, memory_format=torch.legacy_contiguous_format)
A:torch.nn.utils.rnn.lengths->torch.as_tensor([v.size(0) for v in sequences])
A:torch.nn.utils.rnn.(lengths, sorted_indices)->torch.sort(lengths, descending=True)
A:torch.nn.utils.rnn.input->input.index_select(batch_dim, sorted_indices).index_select(batch_dim, sorted_indices)
A:torch.nn.utils.rnn.(data, batch_sizes)->_VF._pack_padded_sequence(input, lengths, batch_first)
A:torch.nn.utils.rnn.max_seq_length->sequence.batch_sizes.size(0)
A:torch.nn.utils.rnn.(padded_output, lengths)->_VF._pad_packed_sequence(sequence.data, sequence.batch_sizes, batch_first, padding_value, max_seq_length)
A:torch.nn.utils.rnn.sequences->sequences.unbind(0).unbind(0)
A:torch.nn.utils.rnn.idx->torch.arange(max_length)
A:torch.nn.utils.rnn.(padded_sequences, lengths)->pad_packed_sequence(packed_sequences, batch_first=True)
A:torch.nn.utils.rnn.unpacked_sequences->unpad_sequence(padded_sequences, lengths, batch_first=True)
torch.nn.utils.rnn.PackedSequence(cls,data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.PackedSequence.__new__(cls,data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.PackedSequence.byte(self)
torch.nn.utils.rnn.PackedSequence.char(self)
torch.nn.utils.rnn.PackedSequence.cpu(self,*args,**kwargs)
torch.nn.utils.rnn.PackedSequence.cuda(self,*args,**kwargs)
torch.nn.utils.rnn.PackedSequence.double(self)
torch.nn.utils.rnn.PackedSequence.float(self)
torch.nn.utils.rnn.PackedSequence.half(self)
torch.nn.utils.rnn.PackedSequence.int(self)
torch.nn.utils.rnn.PackedSequence.is_cuda(self)
torch.nn.utils.rnn.PackedSequence.is_pinned(self)
torch.nn.utils.rnn.PackedSequence.long(self)
torch.nn.utils.rnn.PackedSequence.pin_memory(self)
torch.nn.utils.rnn.PackedSequence.short(self)
torch.nn.utils.rnn.PackedSequence.to(self,*args,**kwargs)
torch.nn.utils.rnn._packed_sequence_init(data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn._packed_sequence_init_args(data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.bind(optional,fn)
torch.nn.utils.rnn.invert_permutation(permutation)
torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)
torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)
torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)
torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0.0)
torch.nn.utils.rnn.unpack_sequence(packed_sequences)
torch.nn.utils.rnn.unpad_sequence(padded_sequences,lengths,batch_first=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/utils/parametrizations.py----------------------------------------
A:torch.nn.utils.parametrizations.Id->torch.eye(n, dtype=A.dtype, device=A.device)
A:torch.nn.utils.parametrizations.(X, tau)->torch.geqrf(A)
A:torch.nn.utils.parametrizations.Q->_make_orthogonal(Q)
A:torch.nn.utils.parametrizations.matrix_exp->auto()
A:torch.nn.utils.parametrizations.cayley->auto()
A:torch.nn.utils.parametrizations.householder->auto()
A:torch.nn.utils.parametrizations.X->torch.cat([X, X.new_zeros(n, n - k).expand(*X.shape[:-2], -1, -1)], dim=-1)
A:torch.nn.utils.parametrizations.A->torch.cat([X, X.new_zeros(n, n - k).expand(*X.shape[:-2], -1, -1)], dim=-1).tril(diagonal=-1)
A:torch.nn.utils.parametrizations.(A, tau)->torch.geqrf(Q)
A:torch.nn.utils.parametrizations.N->torch.randn(*Q.size()[:-2] + (n, n - k), dtype=Q.dtype, device=Q.device)
A:torch.nn.utils.parametrizations.neg_Id->torch.zeros_like(Q_init)
A:torch.nn.utils.parametrizations.weight->getattr(module, name, None)
A:torch.nn.utils.parametrizations.orth_enum->getattr(_OrthMaps, orthogonal_map, None)
A:torch.nn.utils.parametrizations.orth->_Orthogonal(weight, orth_enum, use_trivialization=use_trivialization)
A:torch.nn.utils.parametrizations.weight_mat->self._reshape_weight_to_matrix(weight)
A:torch.nn.utils.parametrizations.(h, w)->self._reshape_weight_to_matrix(weight).size()
A:torch.nn.utils.parametrizations.u->self._u.clone(memory_format=torch.contiguous_format)
A:torch.nn.utils.parametrizations.v->self._v.clone(memory_format=torch.contiguous_format)
A:torch.nn.utils.parametrizations.self._u->F.normalize(torch.mv(weight_mat, self._v), dim=0, eps=self.eps, out=self._u)
A:torch.nn.utils.parametrizations.self._v->F.normalize(torch.mv(weight_mat.t(), self._u), dim=0, eps=self.eps, out=self._v)
A:torch.nn.utils.parametrizations.sigma->torch.dot(u, torch.mv(weight_mat, v))
torch.nn.utils.parametrizations._OrthMaps(Enum)
torch.nn.utils.parametrizations._Orthogonal(self,weight,orthogonal_map:_OrthMaps,*,use_trivialization=True)
torch.nn.utils.parametrizations._Orthogonal.__init__(self,weight,orthogonal_map:_OrthMaps,*,use_trivialization=True)
torch.nn.utils.parametrizations._Orthogonal.forward(self,X:torch.Tensor)->torch.Tensor
torch.nn.utils.parametrizations._Orthogonal.right_inverse(self,Q:torch.Tensor)->torch.Tensor
torch.nn.utils.parametrizations._SpectralNorm(self,weight:torch.Tensor,n_power_iterations:int=1,dim:int=0,eps:float=1e-12)
torch.nn.utils.parametrizations._SpectralNorm.__init__(self,weight:torch.Tensor,n_power_iterations:int=1,dim:int=0,eps:float=1e-12)
torch.nn.utils.parametrizations._SpectralNorm._power_method(self,weight_mat:torch.Tensor,n_power_iterations:int)->None
torch.nn.utils.parametrizations._SpectralNorm._reshape_weight_to_matrix(self,weight:torch.Tensor)->torch.Tensor
torch.nn.utils.parametrizations._SpectralNorm.forward(self,weight:torch.Tensor)->torch.Tensor
torch.nn.utils.parametrizations._SpectralNorm.right_inverse(self,value:torch.Tensor)->torch.Tensor
torch.nn.utils.parametrizations._is_orthogonal(Q,eps=None)
torch.nn.utils.parametrizations._make_orthogonal(A)
torch.nn.utils.parametrizations.orthogonal(module:Module,name:str='weight',orthogonal_map:Optional[str]=None,*,use_trivialization:bool=True)->Module
torch.nn.utils.parametrizations.spectral_norm(module:Module,name:str='weight',n_power_iterations:int=1,eps:float=1e-12,dim:Optional[int]=None)->Module


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/backends/thnn.py----------------------------------------
torch.nn.backends.thnn._get_thnn_function_backend()


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/backends/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/replicate.py----------------------------------------
A:torch.nn.parallel.replicate.gen->module.modules()
A:torch.nn.parallel.replicate.memo->set()
A:torch.nn.parallel.replicate.tensor_copies->_functions.Broadcast.apply(devices, *tensors)
A:torch.nn.parallel.replicate.num_replicas->len(devices)
A:torch.nn.parallel.replicate.params->list(network.parameters())
A:torch.nn.parallel.replicate.param_copies->_broadcast_coalesced_reshape(params, devices, detach)
A:torch.nn.parallel.replicate.buffers->list(network.buffers())
A:torch.nn.parallel.replicate.buffer_copies_rg->_broadcast_coalesced_reshape(buffers_rg, devices, detach=detach)
A:torch.nn.parallel.replicate.buffer_copies_not_rg->_broadcast_coalesced_reshape(buffers_not_rg, devices, detach=True)
A:torch.nn.parallel.replicate.modules->list(network.modules())
A:torch.nn.parallel.replicate.replica->module._replicate_for_data_parallel()
A:torch.nn.parallel.replicate.replica._former_parameters->OrderedDict()
torch.nn.parallel.replicate(network,devices,detach=False)
torch.nn.parallel.replicate._broadcast_coalesced_reshape(tensors,devices,detach=False)
torch.nn.parallel.replicate._init_script_module()
torch.nn.parallel.replicate._is_jit_enabled()
torch.nn.parallel.replicate._is_script_method(module)
torch.nn.parallel.replicate._is_script_module(module)
torch.nn.parallel.replicate._replicatable_module(module,memo=None)
torch.nn.parallel.replicate.replicate(network,devices,detach=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/replicate.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/parallel_apply.py----------------------------------------
A:torch.nn.parallel.parallel_apply.lock->threading.Lock()
A:torch.nn.parallel.parallel_apply.device->get_a_var(input).get_device()
A:torch.nn.parallel.parallel_apply.output->module(*input, **kwargs)
A:torch.nn.parallel.parallel_apply.results[i]->ExceptionWrapper(where='in replica {} on device {}'.format(i, device))
torch.nn.parallel.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)
torch.nn.parallel.parallel_apply.get_a_var(obj)
torch.nn.parallel.parallel_apply.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/parallel_apply.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/data_parallel.py----------------------------------------
A:torch.nn.parallel.data_parallel.dev_props->_get_devices_properties(device_ids)
A:torch.nn.parallel.data_parallel.(min_pos, min_val)->min(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.(max_pos, max_val)->max(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.device_type->_get_available_device_type()
A:torch.nn.parallel.data_parallel.device_ids->_get_all_device_indices()
A:torch.nn.parallel.data_parallel.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.self.src_device_obj->torch.device(device_type, self.device_ids[0])
A:torch.nn.parallel.data_parallel.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.data_parallel.replicas->replicate(module, used_device_ids)
A:torch.nn.parallel.data_parallel.outputs->parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
A:torch.nn.parallel.data_parallel.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.src_device_obj->torch.device(device_type, device_ids[0])
A:torch.nn.parallel.data_parallel.(inputs, module_kwargs)->scatter_kwargs(inputs, module_kwargs, device_ids, dim)
torch.nn.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.DataParallel.gather(self,outputs,output_device)
torch.nn.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.DataParallel.replicate(self,module,device_ids)
torch.nn.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)
torch.nn.parallel.data_parallel.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.data_parallel.DataParallel.gather(self,outputs,output_device)
torch.nn.parallel.data_parallel.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.data_parallel.DataParallel.replicate(self,module,device_ids)
torch.nn.parallel.data_parallel.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel._check_balance(device_ids)
torch.nn.parallel.data_parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/data_parallel.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/__init__.py----------------------------------------
torch.nn.parallel.__init__.DistributedDataParallelCPU(*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/distributed.py----------------------------------------
A:torch.nn.parallel.distributed.(output_tensor_list, treespec)->tree_flatten(output)
A:torch.nn.parallel.distributed.output->obj.to(target_gpu)
A:torch.nn.parallel.distributed.PRE_FORWARD->auto()
A:torch.nn.parallel.distributed.POST_FORWARD->auto()
A:torch.nn.parallel.distributed.ret->tuple((inp.clone() if isinstance(inp, torch.Tensor) else inp for inp in inputs))
A:torch.nn.parallel.distributed.work->self.reducer._run_comm_hook(grad_bucket)
A:torch.nn.parallel.distributed.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.distributed.self.process_group->_get_default_group()
A:torch.nn.parallel.distributed.self.broadcast_bucket_size->int(250 * 1024 * 1024)
A:torch.nn.parallel.distributed.self.bucket_bytes_cap->int(bucket_cap_mb * 1024 * 1024)
A:torch.nn.parallel.distributed.(parameters, expect_sparse_gradient)->self._build_params_for_reducer()
A:torch.nn.parallel.distributed.param_to_name_mapping->self._build_param_to_name_mapping(parameters)
A:torch.nn.parallel.distributed.(bucket_indices, per_bucket_size_limits)->torch.distributed._compute_bucket_assignment_by_size(parameters, [dist._DEFAULT_FIRST_BUCKET_BYTES, self.bucket_bytes_cap], expect_sparse_gradient)
A:torch.nn.parallel.distributed.self.reducer->torch.distributed.Reducer(parameters, list(reversed(bucket_indices)), list(reversed(per_bucket_size_limits)), self.process_group, expect_sparse_gradient, self.bucket_bytes_cap, self.find_unused_parameters, self.gradient_as_bucket_view, param_to_name_mapping, dist._DEFAULT_FIRST_BUCKET_BYTES)
A:torch.nn.parallel.distributed.self.logger->torch.distributed.Logger(self.reducer)
A:torch.nn.parallel.distributed.attrs->copy.copy(self.__dict__)
A:torch.nn.parallel.distributed.memo->set()
A:torch.nn.parallel.distributed.parameters->list((parameter for (_, parameter) in modules_and_parameters))
A:torch.nn.parallel.distributed.expect_sparse_gradient->list((produces_sparse_gradient(module) for (module, _) in modules_and_parameters))
A:torch.nn.parallel.distributed.param_set->set(parameters)
A:torch.nn.parallel.distributed.buffer_hook_registered->hasattr(self, 'buffer_hook')
A:torch.nn.parallel.distributed.(inputs, kwargs)->self.to_kwargs(inputs, kwargs, self.device_ids[0])
A:torch.nn.parallel.distributed.(output_tensor_list, treespec, output_is_rref)->_tree_flatten_with_rref(output)
A:torch.nn.parallel.distributed.passthrough_tensor_list->_DDPSink.apply(self.reducer, state_dict, *output_tensor_list)
A:torch.nn.parallel.distributed.stream->_get_stream(target_gpu)
A:torch.nn.parallel.distributed.current_stream->torch.cuda.current_stream()
A:torch.nn.parallel.distributed.res->to_map(inputs)
A:torch.nn.parallel.distributed.inputs->tuple(inputs)
A:torch.nn.parallel.distributed.kwargs->tuple(kwargs)
A:torch.nn.parallel.distributed.all_active_procs->torch.zeros(1, device=self.device)
A:torch.nn.parallel.distributed.requires_sync_tensor->torch.zeros(1, device=self.device)
A:torch.nn.parallel.distributed.authoritative_rank->self._find_common_rank(self._distributed_rank, True)
A:torch.nn.parallel.distributed.self._authoritative_rank->self._find_common_rank(self._distributed_rank, is_last_joiner)
A:torch.nn.parallel.distributed.grad_buckets->self.reducer._get_zeros_like_grad_buckets()
A:torch.nn.parallel.distributed.locally_used_param_map->self.reducer._get_local_used_map()
A:torch.nn.parallel.distributed.divide_by_initial_world_size->tuple(kwargs).get('divide_by_initial_world_size', True)
A:torch.nn.parallel.distributed.self.buffer_hook->_BufferCommHook(buffer_comm_hook=hook, buffer_comm_hook_state=state, buffer_comm_hook_location=comm_hook_location)
A:torch.nn.parallel.distributed.overlapped_optim->_as_overlapped_optim(optim, optim_params, *args, **kwargs)
A:torch.nn.parallel.distributed.rank_to_use->torch.tensor([input_rank if rank_cond else -1], device=self.device)
A:torch.nn.parallel.distributed.futs->hook(state, self.named_module_buffers)
A:torch.nn.parallel.distributed.sig->inspect.signature(hook)
A:torch.nn.parallel.distributed.ddp_logging_data->self.logger._get_ddp_logging_data()
torch.nn.parallel.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False,gradient_as_bucket_view=False,static_graph=False)
torch.nn.parallel.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.DistributedDataParallel._assign_modules_buffers(self)
torch.nn.parallel.DistributedDataParallel._build_param_to_name_mapping(self,parameters)
torch.nn.parallel.DistributedDataParallel._build_params_for_reducer(self)
torch.nn.parallel.DistributedDataParallel._check_and_sync_module_buffers(self)
torch.nn.parallel.DistributedDataParallel._check_comm_hook(self,hook)
torch.nn.parallel.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.DistributedDataParallel._check_global_requires_backward_grad_sync(self,is_joined_rank)
torch.nn.parallel.DistributedDataParallel._check_sync_bufs_post_fwd(self)
torch.nn.parallel.DistributedDataParallel._check_sync_bufs_pre_fwd(self)
torch.nn.parallel.DistributedDataParallel._ddp_init_helper(self,parameters,expect_sparse_gradient,param_to_name_mapping)
torch.nn.parallel.DistributedDataParallel._default_broadcast_coalesced(self,bufs=None,bucket_size=None,authoritative_rank=0)
torch.nn.parallel.DistributedDataParallel._distributed_broadcast_coalesced(self,tensors,buffer_size,authoritative_rank=0)
torch.nn.parallel.DistributedDataParallel._distributed_rank(self)
torch.nn.parallel.DistributedDataParallel._find_common_rank(self,input_rank,rank_cond)
torch.nn.parallel.DistributedDataParallel._get_ddp_logging_data(self)
torch.nn.parallel.DistributedDataParallel._get_parameters(self,m,recurse=True)
torch.nn.parallel.DistributedDataParallel._log_and_throw(self,err_type,err_msg)
torch.nn.parallel.DistributedDataParallel._match_all_reduce_for_bwd_pass(self)
torch.nn.parallel.DistributedDataParallel._match_unused_params_allreduce(self)
torch.nn.parallel.DistributedDataParallel._passing_sync_batchnorm_handle(self,module)
torch.nn.parallel.DistributedDataParallel._recursive_to(self,inputs,target_gpu)
torch.nn.parallel.DistributedDataParallel._register_buffer_comm_hook(self,state,hook:callable,comm_hook_location=_BufferCommHookLocation.POST_FORWARD)
torch.nn.parallel.DistributedDataParallel._register_builtin_comm_hook(self,comm_hook_type)
torch.nn.parallel.DistributedDataParallel._register_fused_optim(self,optim:Type,*args,optim_params=None,**kwargs)
torch.nn.parallel.DistributedDataParallel._schedule_shadow_all_reduce_for_fwd_pass(self)
torch.nn.parallel.DistributedDataParallel._set_ddp_runtime_logging_sample_rate(self,sample_rate)
torch.nn.parallel.DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(module,params_and_buffers_to_ignore)
torch.nn.parallel.DistributedDataParallel._set_static_graph(self)
torch.nn.parallel.DistributedDataParallel._sync_buffers(self)
torch.nn.parallel.DistributedDataParallel._sync_final_model(self,is_last_joiner)
torch.nn.parallel.DistributedDataParallel._sync_module_buffers(self,authoritative_rank)
torch.nn.parallel.DistributedDataParallel._sync_params_and_buffers(self,authoritative_rank=0)
torch.nn.parallel.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.DistributedDataParallel.join(self,divide_by_initial_world_size:bool=True,enable:bool=True,throw_on_early_termination:bool=False)
torch.nn.parallel.DistributedDataParallel.join_device(self)
torch.nn.parallel.DistributedDataParallel.join_hook(self,**kwargs)
torch.nn.parallel.DistributedDataParallel.join_process_group(self)
torch.nn.parallel.DistributedDataParallel.no_sync(self)
torch.nn.parallel.DistributedDataParallel.register_comm_hook(self,state:object,hook:callable)
torch.nn.parallel.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.DistributedDataParallel.to_kwargs(self,inputs,kwargs,device_id)
torch.nn.parallel.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.DistributedDataParallel.will_sync_module_buffers(self)
torch.nn.parallel.distributed.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False,gradient_as_bucket_view=False,static_graph=False)
torch.nn.parallel.distributed.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.distributed.DistributedDataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False,gradient_as_bucket_view=False,static_graph=False)
torch.nn.parallel.distributed.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.distributed.DistributedDataParallel._assign_modules_buffers(self)
torch.nn.parallel.distributed.DistributedDataParallel._build_param_to_name_mapping(self,parameters)
torch.nn.parallel.distributed.DistributedDataParallel._build_params_for_reducer(self)
torch.nn.parallel.distributed.DistributedDataParallel._check_and_sync_module_buffers(self)
torch.nn.parallel.distributed.DistributedDataParallel._check_comm_hook(self,hook)
torch.nn.parallel.distributed.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.distributed.DistributedDataParallel._check_global_requires_backward_grad_sync(self,is_joined_rank)
torch.nn.parallel.distributed.DistributedDataParallel._check_sync_bufs_post_fwd(self)
torch.nn.parallel.distributed.DistributedDataParallel._check_sync_bufs_pre_fwd(self)
torch.nn.parallel.distributed.DistributedDataParallel._ddp_init_helper(self,parameters,expect_sparse_gradient,param_to_name_mapping)
torch.nn.parallel.distributed.DistributedDataParallel._default_broadcast_coalesced(self,bufs=None,bucket_size=None,authoritative_rank=0)
torch.nn.parallel.distributed.DistributedDataParallel._distributed_broadcast_coalesced(self,tensors,buffer_size,authoritative_rank=0)
torch.nn.parallel.distributed.DistributedDataParallel._distributed_rank(self)
torch.nn.parallel.distributed.DistributedDataParallel._find_common_rank(self,input_rank,rank_cond)
torch.nn.parallel.distributed.DistributedDataParallel._get_ddp_logging_data(self)
torch.nn.parallel.distributed.DistributedDataParallel._get_parameters(self,m,recurse=True)
torch.nn.parallel.distributed.DistributedDataParallel._log_and_throw(self,err_type,err_msg)
torch.nn.parallel.distributed.DistributedDataParallel._match_all_reduce_for_bwd_pass(self)
torch.nn.parallel.distributed.DistributedDataParallel._match_unused_params_allreduce(self)
torch.nn.parallel.distributed.DistributedDataParallel._passing_sync_batchnorm_handle(self,module)
torch.nn.parallel.distributed.DistributedDataParallel._recursive_to(self,inputs,target_gpu)
torch.nn.parallel.distributed.DistributedDataParallel._register_buffer_comm_hook(self,state,hook:callable,comm_hook_location=_BufferCommHookLocation.POST_FORWARD)
torch.nn.parallel.distributed.DistributedDataParallel._register_builtin_comm_hook(self,comm_hook_type)
torch.nn.parallel.distributed.DistributedDataParallel._register_fused_optim(self,optim:Type,*args,optim_params=None,**kwargs)
torch.nn.parallel.distributed.DistributedDataParallel._schedule_shadow_all_reduce_for_fwd_pass(self)
torch.nn.parallel.distributed.DistributedDataParallel._set_ddp_runtime_logging_sample_rate(self,sample_rate)
torch.nn.parallel.distributed.DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(module,params_and_buffers_to_ignore)
torch.nn.parallel.distributed.DistributedDataParallel._set_static_graph(self)
torch.nn.parallel.distributed.DistributedDataParallel._sync_buffers(self)
torch.nn.parallel.distributed.DistributedDataParallel._sync_final_model(self,is_last_joiner)
torch.nn.parallel.distributed.DistributedDataParallel._sync_module_buffers(self,authoritative_rank)
torch.nn.parallel.distributed.DistributedDataParallel._sync_params_and_buffers(self,authoritative_rank=0)
torch.nn.parallel.distributed.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.distributed.DistributedDataParallel.join(self,divide_by_initial_world_size:bool=True,enable:bool=True,throw_on_early_termination:bool=False)
torch.nn.parallel.distributed.DistributedDataParallel.join_device(self)
torch.nn.parallel.distributed.DistributedDataParallel.join_hook(self,**kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.join_process_group(self)
torch.nn.parallel.distributed.DistributedDataParallel.no_sync(self)
torch.nn.parallel.distributed.DistributedDataParallel.register_comm_hook(self,state:object,hook:callable)
torch.nn.parallel.distributed.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.distributed.DistributedDataParallel.to_kwargs(self,inputs,kwargs,device_id)
torch.nn.parallel.distributed.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.distributed.DistributedDataParallel.will_sync_module_buffers(self)
torch.nn.parallel.distributed._BufferCommHook
torch.nn.parallel.distributed._BufferCommHookLocation(Enum)
torch.nn.parallel.distributed._DDPJoinHook(self,ddp,divide_by_initial_world_size)
torch.nn.parallel.distributed._DDPJoinHook.__init__(self,ddp,divide_by_initial_world_size)
torch.nn.parallel.distributed._DDPJoinHook.main_hook(self)
torch.nn.parallel.distributed._DDPJoinHook.post_hook(self,is_last_joiner:bool)
torch.nn.parallel.distributed._DDPSink(Function)
torch.nn.parallel.distributed._DDPSink.backward(ctx,*grad_outputs)
torch.nn.parallel.distributed._DDPSink.forward(ctx,reducer,state_dict,*inputs)
torch.nn.parallel.distributed._dump_DDP_relevant_env_vars()
torch.nn.parallel.distributed._find_tensors(obj)
torch.nn.parallel.distributed._tree_flatten_with_rref(output)
torch.nn.parallel.distributed._tree_unflatten_with_rref(output,treespec,output_is_rref)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/distributed.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/comm.py----------------------------------------
A:torch.nn.parallel.comm.tensor->_handle_complex(tensor)
A:torch.nn.parallel.comm.destination->_get_device_index(destination, allow_cpu=True, optional=True)
A:torch.nn.parallel.comm.input_size->inputs[0].size()
A:torch.nn.parallel.comm.got->'x'.join((str(x) for x in inp.size()))
A:torch.nn.parallel.comm.expected->'x'.join((str(x) for x in input_size))
A:torch.nn.parallel.comm.result->reduce_add(tensor_at_gpus, destination)
A:torch.nn.parallel.comm.destination_device->torch.device(inputs[root_index].device.type, destination)
A:torch.nn.parallel.comm.flat_result->reduce_add(flat_tensors, destination)
torch.nn.parallel.comm.broadcast(tensor,devices=None,*,out=None)
torch.nn.parallel.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)
torch.nn.parallel.comm.gather(tensors,dim=0,destination=None,*,out=None)
torch.nn.parallel.comm.reduce_add(inputs,destination=None)
torch.nn.parallel.comm.reduce_add_coalesced(inputs,destination=None,buffer_size=10485760)
torch.nn.parallel.comm.scatter(tensor,devices=None,chunk_sizes=None,dim=0,streams=None,*,out=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/scatter_gather.py----------------------------------------
A:torch.nn.parallel.scatter_gather.res->gather_map(outputs)
A:torch.nn.parallel.scatter_gather.inputs->tuple(inputs)
A:torch.nn.parallel.scatter_gather.kwargs->tuple(kwargs)
torch.nn.parallel.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter_gather.is_namedtuple(obj)
torch.nn.parallel.scatter_gather.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)
torch.nn.parallel.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/scatter_gather.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/common_types.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/parallel/_functions.py----------------------------------------
A:torch.nn.parallel._functions.ctx.num_inputs->len(inputs)
A:torch.nn.parallel._functions.ctx.input_device->inputs[0].get_device()
A:torch.nn.parallel._functions.outputs->comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
A:torch.nn.parallel._functions.target_device->_get_device_index(target_device, True)
A:torch.nn.parallel._functions.ctx.input_gpus->tuple((i.get_device() for i in inputs))
A:torch.nn.parallel._functions.inputs->tuple((t.view(1) for t in inputs))
A:torch.nn.parallel._functions.ctx.input_sizes->tuple((i.size(ctx.dim) for i in inputs))
A:torch.nn.parallel._functions.scattered_grads->tuple((g[0] for g in scattered_grads))
A:torch.nn.parallel._functions.main_stream->torch.cuda.current_stream()
A:torch.nn.parallel._functions._streams[device]->torch.cuda.Stream(device)
torch.nn.parallel._functions.Broadcast(Function)
torch.nn.parallel._functions.Broadcast.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.Broadcast.forward(ctx,target_gpus,*inputs)
torch.nn.parallel._functions.Gather(Function)
torch.nn.parallel._functions.Gather.backward(ctx,grad_output)
torch.nn.parallel._functions.Gather.forward(ctx,target_device,dim,*inputs)
torch.nn.parallel._functions.ReduceAddCoalesced(Function)
torch.nn.parallel._functions.ReduceAddCoalesced.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.ReduceAddCoalesced.forward(ctx,destination,num_inputs,*grads)
torch.nn.parallel._functions.Scatter(Function)
torch.nn.parallel._functions.Scatter.backward(ctx,*grad_output)
torch.nn.parallel._functions.Scatter.forward(ctx,target_gpus,chunk_sizes,dim,input)
torch.nn.parallel._functions._get_stream(device:int)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/container.py----------------------------------------
A:torch.nn.modules.container.T->TypeVar('T', bound=Module)
A:torch.nn.modules.container.size->len(self)
A:torch.nn.modules.container.idx->self._get_abs_string_index(idx)
A:torch.nn.modules.container.key->self._get_item_by_idx(self._modules.keys(), idx)
A:torch.nn.modules.container.keys->super(ParameterList, self).__dir__()
A:torch.nn.modules.container.input->module(input)
A:torch.nn.modules.container.self._modules->OrderedDict(list(zip(str_indices, self._modules.values())))
A:torch.nn.modules.container.combined->ModuleList()
A:torch.nn.modules.container.offset->len(self)
A:torch.nn.modules.container.size_str->'x'.join((str(size) for size in p.size()))
A:torch.nn.modules.container.parastr->'Parameter containing: [{} of size {}{}]'.format(torch.typename(p), size_str, device_str)
A:torch.nn.modules.container.tmpstr->'\n'.join(child_lines)
A:torch.nn.modules.container.copy->other.copy()
torch.nn.Container(self,**kwargs:Any)
torch.nn.ModuleDict(self,modules:Optional[Mapping[str,Module]]=None)
torch.nn.ModuleDict.__contains__(self,key:str)->bool
torch.nn.ModuleDict.__delitem__(self,key:str)->None
torch.nn.ModuleDict.__getitem__(self,key:str)->Module
torch.nn.ModuleDict.__iter__(self)->Iterator[str]
torch.nn.ModuleDict.__len__(self)->int
torch.nn.ModuleDict.__setitem__(self,key:str,module:Module)->None
torch.nn.ModuleDict.clear(self)->None
torch.nn.ModuleDict.items(self)->Iterable[Tuple[str, Module]]
torch.nn.ModuleDict.keys(self)->Iterable[str]
torch.nn.ModuleDict.pop(self,key:str)->Module
torch.nn.ModuleDict.update(self,modules:Mapping[str,Module])->None
torch.nn.ModuleDict.values(self)->Iterable[Module]
torch.nn.ModuleList(self,modules:Optional[Iterable[Module]]=None)
torch.nn.ModuleList.__add__(self,other:Iterable[Module])->'ModuleList'
torch.nn.ModuleList.__delitem__(self,idx:Union[int,slice])->None
torch.nn.ModuleList.__dir__(self)
torch.nn.ModuleList.__getitem__(self,idx:int)->Union[Module, 'ModuleList']
torch.nn.ModuleList.__iadd__(self,modules:Iterable[Module])->'ModuleList'
torch.nn.ModuleList.__iter__(self)->Iterator[Module]
torch.nn.ModuleList.__len__(self)->int
torch.nn.ModuleList.__setitem__(self,idx:int,module:Module)->None
torch.nn.ModuleList._get_abs_string_index(self,idx)
torch.nn.ModuleList.append(self,module:Module)->'ModuleList'
torch.nn.ModuleList.extend(self,modules:Iterable[Module])->'ModuleList'
torch.nn.ModuleList.insert(self,index:int,module:Module)->None
torch.nn.ParameterDict(self,parameters:Optional[Mapping[str,'Parameter']]=None)
torch.nn.ParameterDict.__contains__(self,key:str)->bool
torch.nn.ParameterDict.__delitem__(self,key:str)->None
torch.nn.ParameterDict.__getitem__(self,key:str)->'Parameter'
torch.nn.ParameterDict.__ior__(self,other:'ParameterDict')->'ParameterDict'
torch.nn.ParameterDict.__iter__(self)->Iterator[str]
torch.nn.ParameterDict.__len__(self)->int
torch.nn.ParameterDict.__or__(self,other:'ParameterDict')->'ParameterDict'
torch.nn.ParameterDict.__reversed__(self)->Iterator[str]
torch.nn.ParameterDict.__ror__(self,other:'ParameterDict')->'ParameterDict'
torch.nn.ParameterDict.__setattr__(self,key:Any,value:Any)->None
torch.nn.ParameterDict.__setitem__(self,key:str,parameter:'Parameter')->None
torch.nn.ParameterDict.__setstate__(self,state)
torch.nn.ParameterDict._replicate_for_data_parallel(self)
torch.nn.ParameterDict.clear(self)->None
torch.nn.ParameterDict.copy(self)->'ParameterDict'
torch.nn.ParameterDict.extra_repr(self)->str
torch.nn.ParameterDict.fromkeys(self,keys:Iterable['str'],default:Optional['Parameter']=None)->'ParameterDict'
torch.nn.ParameterDict.get(self,key:str,default:Optional['Parameter']=None)->'Parameter | None'
torch.nn.ParameterDict.items(self)->Iterable[Tuple[str, 'Parameter']]
torch.nn.ParameterDict.keys(self)->Iterable[str]
torch.nn.ParameterDict.pop(self,key:str)->'Parameter'
torch.nn.ParameterDict.popitem(self)->Tuple[str, 'Parameter']
torch.nn.ParameterDict.setdefault(self,key:str,default:Optional['Parameter']=None)->'Parameter'
torch.nn.ParameterDict.update(self,parameters:Mapping[str,'Parameter'])->None
torch.nn.ParameterDict.values(self)->Iterable['Parameter']
torch.nn.ParameterList(self,parameters:Optional[Iterable['Parameter']]=None)
torch.nn.ParameterList.__dir__(self)
torch.nn.ParameterList.__getitem__(self,idx)
torch.nn.ParameterList.__iadd__(self,parameters:Iterable['Parameter'])->'ParameterList'
torch.nn.ParameterList.__iter__(self)->Iterator['Parameter']
torch.nn.ParameterList.__len__(self)->int
torch.nn.ParameterList.__setattr__(self,key:Any,value:Any)->None
torch.nn.ParameterList.__setitem__(self,idx:int,param:'Parameter')->None
torch.nn.ParameterList.__setstate__(self,state)
torch.nn.ParameterList._get_abs_string_index(self,idx)
torch.nn.ParameterList._replicate_for_data_parallel(self)
torch.nn.ParameterList.append(self,parameter:'Parameter')->'ParameterList'
torch.nn.ParameterList.extend(self,parameters:Iterable['Parameter'])->'ParameterList'
torch.nn.ParameterList.extra_repr(self)->str
torch.nn.Sequential(self,*args)
torch.nn.Sequential.__delitem__(self,idx:Union[slice,int])->None
torch.nn.Sequential.__dir__(self)
torch.nn.Sequential.__getitem__(self,idx)->Union['Sequential', T]
torch.nn.Sequential.__iter__(self)->Iterator[Module]
torch.nn.Sequential.__len__(self)->int
torch.nn.Sequential.__setitem__(self,idx:int,module:Module)->None
torch.nn.Sequential._get_item_by_idx(self,iterator,idx)->T
torch.nn.Sequential.append(self,module:Module)->'Sequential'
torch.nn.Sequential.forward(self,input)
torch.nn.modules.container.Container(self,**kwargs:Any)
torch.nn.modules.container.Container.__init__(self,**kwargs:Any)
torch.nn.modules.container.ModuleDict(self,modules:Optional[Mapping[str,Module]]=None)
torch.nn.modules.container.ModuleDict.__contains__(self,key:str)->bool
torch.nn.modules.container.ModuleDict.__delitem__(self,key:str)->None
torch.nn.modules.container.ModuleDict.__getitem__(self,key:str)->Module
torch.nn.modules.container.ModuleDict.__init__(self,modules:Optional[Mapping[str,Module]]=None)
torch.nn.modules.container.ModuleDict.__iter__(self)->Iterator[str]
torch.nn.modules.container.ModuleDict.__len__(self)->int
torch.nn.modules.container.ModuleDict.__setitem__(self,key:str,module:Module)->None
torch.nn.modules.container.ModuleDict.clear(self)->None
torch.nn.modules.container.ModuleDict.items(self)->Iterable[Tuple[str, Module]]
torch.nn.modules.container.ModuleDict.keys(self)->Iterable[str]
torch.nn.modules.container.ModuleDict.pop(self,key:str)->Module
torch.nn.modules.container.ModuleDict.update(self,modules:Mapping[str,Module])->None
torch.nn.modules.container.ModuleDict.values(self)->Iterable[Module]
torch.nn.modules.container.ModuleList(self,modules:Optional[Iterable[Module]]=None)
torch.nn.modules.container.ModuleList.__add__(self,other:Iterable[Module])->'ModuleList'
torch.nn.modules.container.ModuleList.__delitem__(self,idx:Union[int,slice])->None
torch.nn.modules.container.ModuleList.__dir__(self)
torch.nn.modules.container.ModuleList.__getitem__(self,idx:int)->Union[Module, 'ModuleList']
torch.nn.modules.container.ModuleList.__iadd__(self,modules:Iterable[Module])->'ModuleList'
torch.nn.modules.container.ModuleList.__init__(self,modules:Optional[Iterable[Module]]=None)
torch.nn.modules.container.ModuleList.__iter__(self)->Iterator[Module]
torch.nn.modules.container.ModuleList.__len__(self)->int
torch.nn.modules.container.ModuleList.__setitem__(self,idx:int,module:Module)->None
torch.nn.modules.container.ModuleList._get_abs_string_index(self,idx)
torch.nn.modules.container.ModuleList.append(self,module:Module)->'ModuleList'
torch.nn.modules.container.ModuleList.extend(self,modules:Iterable[Module])->'ModuleList'
torch.nn.modules.container.ModuleList.insert(self,index:int,module:Module)->None
torch.nn.modules.container.ParameterDict(self,parameters:Optional[Mapping[str,'Parameter']]=None)
torch.nn.modules.container.ParameterDict.__contains__(self,key:str)->bool
torch.nn.modules.container.ParameterDict.__delitem__(self,key:str)->None
torch.nn.modules.container.ParameterDict.__getitem__(self,key:str)->'Parameter'
torch.nn.modules.container.ParameterDict.__init__(self,parameters:Optional[Mapping[str,'Parameter']]=None)
torch.nn.modules.container.ParameterDict.__ior__(self,other:'ParameterDict')->'ParameterDict'
torch.nn.modules.container.ParameterDict.__iter__(self)->Iterator[str]
torch.nn.modules.container.ParameterDict.__len__(self)->int
torch.nn.modules.container.ParameterDict.__or__(self,other:'ParameterDict')->'ParameterDict'
torch.nn.modules.container.ParameterDict.__reversed__(self)->Iterator[str]
torch.nn.modules.container.ParameterDict.__ror__(self,other:'ParameterDict')->'ParameterDict'
torch.nn.modules.container.ParameterDict.__setattr__(self,key:Any,value:Any)->None
torch.nn.modules.container.ParameterDict.__setitem__(self,key:str,parameter:'Parameter')->None
torch.nn.modules.container.ParameterDict.__setstate__(self,state)
torch.nn.modules.container.ParameterDict._replicate_for_data_parallel(self)
torch.nn.modules.container.ParameterDict.clear(self)->None
torch.nn.modules.container.ParameterDict.copy(self)->'ParameterDict'
torch.nn.modules.container.ParameterDict.extra_repr(self)->str
torch.nn.modules.container.ParameterDict.fromkeys(self,keys:Iterable['str'],default:Optional['Parameter']=None)->'ParameterDict'
torch.nn.modules.container.ParameterDict.get(self,key:str,default:Optional['Parameter']=None)->'Parameter | None'
torch.nn.modules.container.ParameterDict.items(self)->Iterable[Tuple[str, 'Parameter']]
torch.nn.modules.container.ParameterDict.keys(self)->Iterable[str]
torch.nn.modules.container.ParameterDict.pop(self,key:str)->'Parameter'
torch.nn.modules.container.ParameterDict.popitem(self)->Tuple[str, 'Parameter']
torch.nn.modules.container.ParameterDict.setdefault(self,key:str,default:Optional['Parameter']=None)->'Parameter'
torch.nn.modules.container.ParameterDict.update(self,parameters:Mapping[str,'Parameter'])->None
torch.nn.modules.container.ParameterDict.values(self)->Iterable['Parameter']
torch.nn.modules.container.ParameterList(self,parameters:Optional[Iterable['Parameter']]=None)
torch.nn.modules.container.ParameterList.__dir__(self)
torch.nn.modules.container.ParameterList.__getitem__(self,idx)
torch.nn.modules.container.ParameterList.__iadd__(self,parameters:Iterable['Parameter'])->'ParameterList'
torch.nn.modules.container.ParameterList.__init__(self,parameters:Optional[Iterable['Parameter']]=None)
torch.nn.modules.container.ParameterList.__iter__(self)->Iterator['Parameter']
torch.nn.modules.container.ParameterList.__len__(self)->int
torch.nn.modules.container.ParameterList.__setattr__(self,key:Any,value:Any)->None
torch.nn.modules.container.ParameterList.__setitem__(self,idx:int,param:'Parameter')->None
torch.nn.modules.container.ParameterList.__setstate__(self,state)
torch.nn.modules.container.ParameterList._get_abs_string_index(self,idx)
torch.nn.modules.container.ParameterList._replicate_for_data_parallel(self)
torch.nn.modules.container.ParameterList.append(self,parameter:'Parameter')->'ParameterList'
torch.nn.modules.container.ParameterList.extend(self,parameters:Iterable['Parameter'])->'ParameterList'
torch.nn.modules.container.ParameterList.extra_repr(self)->str
torch.nn.modules.container.Sequential(self,*args)
torch.nn.modules.container.Sequential.__delitem__(self,idx:Union[slice,int])->None
torch.nn.modules.container.Sequential.__dir__(self)
torch.nn.modules.container.Sequential.__getitem__(self,idx)->Union['Sequential', T]
torch.nn.modules.container.Sequential.__init__(self,*args)
torch.nn.modules.container.Sequential.__iter__(self)->Iterator[Module]
torch.nn.modules.container.Sequential.__len__(self)->int
torch.nn.modules.container.Sequential.__setitem__(self,idx:int,module:Module)->None
torch.nn.modules.container.Sequential._get_item_by_idx(self,iterator,idx)->T
torch.nn.modules.container.Sequential.append(self,module:Module)->'Sequential'
torch.nn.modules.container.Sequential.forward(self,input)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/flatten.py----------------------------------------
torch.nn.Flatten(self,start_dim:int=1,end_dim:int=-1)
torch.nn.Flatten.extra_repr(self)->str
torch.nn.Flatten.forward(self,input:Tensor)->Tensor
torch.nn.Unflatten(self,dim:Union[int,str],unflattened_size:Union[_size,NamedShape])
torch.nn.Unflatten._require_tuple_int(self,input)
torch.nn.Unflatten._require_tuple_tuple(self,input)
torch.nn.Unflatten.extra_repr(self)->str
torch.nn.Unflatten.forward(self,input:Tensor)->Tensor
torch.nn.modules.flatten.Flatten(self,start_dim:int=1,end_dim:int=-1)
torch.nn.modules.flatten.Flatten.__init__(self,start_dim:int=1,end_dim:int=-1)
torch.nn.modules.flatten.Flatten.extra_repr(self)->str
torch.nn.modules.flatten.Flatten.forward(self,input:Tensor)->Tensor
torch.nn.modules.flatten.Unflatten(self,dim:Union[int,str],unflattened_size:Union[_size,NamedShape])
torch.nn.modules.flatten.Unflatten.__init__(self,dim:Union[int,str],unflattened_size:Union[_size,NamedShape])
torch.nn.modules.flatten.Unflatten._require_tuple_int(self,input)
torch.nn.modules.flatten.Unflatten._require_tuple_tuple(self,input)
torch.nn.modules.flatten.Unflatten.extra_repr(self)->str
torch.nn.modules.flatten.Unflatten.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/dropout.py----------------------------------------
torch.nn.AlphaDropout(_DropoutNd)
torch.nn.AlphaDropout.forward(self,input:Tensor)->Tensor
torch.nn.Dropout(_DropoutNd)
torch.nn.Dropout.forward(self,input:Tensor)->Tensor
torch.nn.Dropout2d(_DropoutNd)
torch.nn.Dropout2d.forward(self,input:Tensor)->Tensor
torch.nn.Dropout3d(_DropoutNd)
torch.nn.Dropout3d.forward(self,input:Tensor)->Tensor
torch.nn.FeatureAlphaDropout(_DropoutNd)
torch.nn.FeatureAlphaDropout.forward(self,input:Tensor)->Tensor
torch.nn.dropout._DropoutNd(self,p:float=0.5,inplace:bool=False)
torch.nn.dropout._DropoutNd.extra_repr(self)->str
torch.nn.modules.dropout.AlphaDropout(_DropoutNd)
torch.nn.modules.dropout.AlphaDropout.forward(self,input:Tensor)->Tensor
torch.nn.modules.dropout.Dropout(_DropoutNd)
torch.nn.modules.dropout.Dropout.forward(self,input:Tensor)->Tensor
torch.nn.modules.dropout.Dropout2d(_DropoutNd)
torch.nn.modules.dropout.Dropout2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.dropout.Dropout3d(_DropoutNd)
torch.nn.modules.dropout.Dropout3d.forward(self,input:Tensor)->Tensor
torch.nn.modules.dropout.FeatureAlphaDropout(_DropoutNd)
torch.nn.modules.dropout.FeatureAlphaDropout.forward(self,input:Tensor)->Tensor
torch.nn.modules.dropout._DropoutNd(self,p:float=0.5,inplace:bool=False)
torch.nn.modules.dropout._DropoutNd.__init__(self,p:float=0.5,inplace:bool=False)
torch.nn.modules.dropout._DropoutNd.extra_repr(self)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/transformer.py----------------------------------------
A:torch.nn.modules.transformer.encoder_layer->TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)
A:torch.nn.modules.transformer.encoder_norm->LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
A:torch.nn.modules.transformer.self.encoder->TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
A:torch.nn.modules.transformer.decoder_layer->TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, **factory_kwargs)
A:torch.nn.modules.transformer.decoder_norm->LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
A:torch.nn.modules.transformer.self.decoder->TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)
A:torch.nn.modules.transformer.memory->self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)
A:torch.nn.modules.transformer.output->self.norm(output)
A:torch.nn.modules.transformer.self.layers->_get_clones(decoder_layer, num_layers)
A:torch.nn.modules.transformer.self.self_attn->MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs)
A:torch.nn.modules.transformer.self.linear1->Linear(d_model, dim_feedforward, **factory_kwargs)
A:torch.nn.modules.transformer.self.dropout->Dropout(dropout)
A:torch.nn.modules.transformer.self.linear2->Linear(dim_feedforward, d_model, **factory_kwargs)
A:torch.nn.modules.transformer.self.norm1->LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
A:torch.nn.modules.transformer.self.norm2->LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
A:torch.nn.modules.transformer.self.dropout1->Dropout(dropout)
A:torch.nn.modules.transformer.self.dropout2->Dropout(dropout)
A:torch.nn.modules.transformer.self.activation->_get_activation_fn(activation)
A:torch.nn.modules.transformer.x->self.linear2(self.dropout(self.activation(self.linear1(x))))
A:torch.nn.modules.transformer.self.multihead_attn->MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs)
A:torch.nn.modules.transformer.self.norm3->LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)
A:torch.nn.modules.transformer.self.dropout3->Dropout(dropout)
torch.nn.Transformer(self,d_model:int=512,nhead:int=8,num_encoder_layers:int=6,num_decoder_layers:int=6,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,custom_encoder:Optional[Any]=None,custom_decoder:Optional[Any]=None,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.Transformer._reset_parameters(self)
torch.nn.Transformer.forward(self,src:Tensor,tgt:Tensor,src_mask:Optional[Tensor]=None,tgt_mask:Optional[Tensor]=None,memory_mask:Optional[Tensor]=None,src_key_padding_mask:Optional[Tensor]=None,tgt_key_padding_mask:Optional[Tensor]=None,memory_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.Transformer.generate_square_subsequent_mask(sz:int)->Tensor
torch.nn.TransformerDecoder(self,decoder_layer,num_layers,norm=None)
torch.nn.TransformerDecoder.forward(self,tgt:Tensor,memory:Tensor,tgt_mask:Optional[Tensor]=None,memory_mask:Optional[Tensor]=None,tgt_key_padding_mask:Optional[Tensor]=None,memory_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.TransformerDecoderLayer(self,d_model:int,nhead:int,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.TransformerDecoderLayer.__setstate__(self,state)
torch.nn.TransformerDecoderLayer._ff_block(self,x:Tensor)->Tensor
torch.nn.TransformerDecoderLayer._mha_block(self,x:Tensor,mem:Tensor,attn_mask:Optional[Tensor],key_padding_mask:Optional[Tensor])->Tensor
torch.nn.TransformerDecoderLayer._sa_block(self,x:Tensor,attn_mask:Optional[Tensor],key_padding_mask:Optional[Tensor])->Tensor
torch.nn.TransformerDecoderLayer.forward(self,tgt:Tensor,memory:Tensor,tgt_mask:Optional[Tensor]=None,memory_mask:Optional[Tensor]=None,tgt_key_padding_mask:Optional[Tensor]=None,memory_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.TransformerEncoder(self,encoder_layer,num_layers,norm=None)
torch.nn.TransformerEncoder.forward(self,src:Tensor,mask:Optional[Tensor]=None,src_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.TransformerEncoderLayer(self,d_model:int,nhead:int,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.TransformerEncoderLayer.__setstate__(self,state)
torch.nn.TransformerEncoderLayer._ff_block(self,x:Tensor)->Tensor
torch.nn.TransformerEncoderLayer._sa_block(self,x:Tensor,attn_mask:Optional[Tensor],key_padding_mask:Optional[Tensor])->Tensor
torch.nn.TransformerEncoderLayer.forward(self,src:Tensor,src_mask:Optional[Tensor]=None,src_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.modules.transformer.Transformer(self,d_model:int=512,nhead:int=8,num_encoder_layers:int=6,num_decoder_layers:int=6,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,custom_encoder:Optional[Any]=None,custom_decoder:Optional[Any]=None,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.modules.transformer.Transformer.__init__(self,d_model:int=512,nhead:int=8,num_encoder_layers:int=6,num_decoder_layers:int=6,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,custom_encoder:Optional[Any]=None,custom_decoder:Optional[Any]=None,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.modules.transformer.Transformer._reset_parameters(self)
torch.nn.modules.transformer.Transformer.forward(self,src:Tensor,tgt:Tensor,src_mask:Optional[Tensor]=None,tgt_mask:Optional[Tensor]=None,memory_mask:Optional[Tensor]=None,src_key_padding_mask:Optional[Tensor]=None,tgt_key_padding_mask:Optional[Tensor]=None,memory_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.modules.transformer.Transformer.generate_square_subsequent_mask(sz:int)->Tensor
torch.nn.modules.transformer.TransformerDecoder(self,decoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerDecoder.__init__(self,decoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerDecoder.forward(self,tgt:Tensor,memory:Tensor,tgt_mask:Optional[Tensor]=None,memory_mask:Optional[Tensor]=None,tgt_key_padding_mask:Optional[Tensor]=None,memory_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.modules.transformer.TransformerDecoderLayer(self,d_model:int,nhead:int,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.modules.transformer.TransformerDecoderLayer.__init__(self,d_model:int,nhead:int,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.modules.transformer.TransformerDecoderLayer.__setstate__(self,state)
torch.nn.modules.transformer.TransformerDecoderLayer._ff_block(self,x:Tensor)->Tensor
torch.nn.modules.transformer.TransformerDecoderLayer._mha_block(self,x:Tensor,mem:Tensor,attn_mask:Optional[Tensor],key_padding_mask:Optional[Tensor])->Tensor
torch.nn.modules.transformer.TransformerDecoderLayer._sa_block(self,x:Tensor,attn_mask:Optional[Tensor],key_padding_mask:Optional[Tensor])->Tensor
torch.nn.modules.transformer.TransformerDecoderLayer.forward(self,tgt:Tensor,memory:Tensor,tgt_mask:Optional[Tensor]=None,memory_mask:Optional[Tensor]=None,tgt_key_padding_mask:Optional[Tensor]=None,memory_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.modules.transformer.TransformerEncoder(self,encoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerEncoder.__init__(self,encoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerEncoder.forward(self,src:Tensor,mask:Optional[Tensor]=None,src_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.modules.transformer.TransformerEncoderLayer(self,d_model:int,nhead:int,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.modules.transformer.TransformerEncoderLayer.__init__(self,d_model:int,nhead:int,dim_feedforward:int=2048,dropout:float=0.1,activation:Union[str,Callable[[Tensor],Tensor]]=F.relu,layer_norm_eps:float=1e-05,batch_first:bool=False,norm_first:bool=False,device=None,dtype=None)
torch.nn.modules.transformer.TransformerEncoderLayer.__setstate__(self,state)
torch.nn.modules.transformer.TransformerEncoderLayer._ff_block(self,x:Tensor)->Tensor
torch.nn.modules.transformer.TransformerEncoderLayer._sa_block(self,x:Tensor,attn_mask:Optional[Tensor],key_padding_mask:Optional[Tensor])->Tensor
torch.nn.modules.transformer.TransformerEncoderLayer.forward(self,src:Tensor,src_mask:Optional[Tensor]=None,src_key_padding_mask:Optional[Tensor]=None)->Tensor
torch.nn.modules.transformer._get_activation_fn(activation)
torch.nn.modules.transformer._get_clones(module,N)
torch.nn.transformer._get_activation_fn(activation)
torch.nn.transformer._get_clones(module,N)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/upsampling.py----------------------------------------
A:torch.nn.modules.upsampling.self.scale_factor->tuple((float(factor) for factor in scale_factor))
torch.nn.Upsample(self,size:Optional[_size_any_t]=None,scale_factor:Optional[_ratio_any_t]=None,mode:str='nearest',align_corners:Optional[bool]=None,recompute_scale_factor:Optional[bool]=None)
torch.nn.Upsample.extra_repr(self)->str
torch.nn.Upsample.forward(self,input:Tensor)->Tensor
torch.nn.UpsamplingBilinear2d(self,size:Optional[_size_2_t]=None,scale_factor:Optional[_ratio_2_t]=None)
torch.nn.UpsamplingNearest2d(self,size:Optional[_size_2_t]=None,scale_factor:Optional[_ratio_2_t]=None)
torch.nn.modules.upsampling.Upsample(self,size:Optional[_size_any_t]=None,scale_factor:Optional[_ratio_any_t]=None,mode:str='nearest',align_corners:Optional[bool]=None,recompute_scale_factor:Optional[bool]=None)
torch.nn.modules.upsampling.Upsample.__init__(self,size:Optional[_size_any_t]=None,scale_factor:Optional[_ratio_any_t]=None,mode:str='nearest',align_corners:Optional[bool]=None,recompute_scale_factor:Optional[bool]=None)
torch.nn.modules.upsampling.Upsample.extra_repr(self)->str
torch.nn.modules.upsampling.Upsample.forward(self,input:Tensor)->Tensor
torch.nn.modules.upsampling.UpsamplingBilinear2d(self,size:Optional[_size_2_t]=None,scale_factor:Optional[_ratio_2_t]=None)
torch.nn.modules.upsampling.UpsamplingBilinear2d.__init__(self,size:Optional[_size_2_t]=None,scale_factor:Optional[_ratio_2_t]=None)
torch.nn.modules.upsampling.UpsamplingNearest2d(self,size:Optional[_size_2_t]=None,scale_factor:Optional[_ratio_2_t]=None)
torch.nn.modules.upsampling.UpsamplingNearest2d.__init__(self,size:Optional[_size_2_t]=None,scale_factor:Optional[_ratio_2_t]=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/activation.py----------------------------------------
A:torch.nn.modules.activation.self.q_proj_weight->Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
A:torch.nn.modules.activation.self.k_proj_weight->Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
A:torch.nn.modules.activation.self.v_proj_weight->Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
A:torch.nn.modules.activation.self.in_proj_weight->Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
A:torch.nn.modules.activation.self.in_proj_bias->Parameter(torch.empty(3 * embed_dim, **factory_kwargs))
A:torch.nn.modules.activation.self.out_proj->NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
A:torch.nn.modules.activation.self.bias_k->Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
A:torch.nn.modules.activation.self.bias_v->Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))
A:torch.nn.modules.activation.(attn_output, attn_output_weights)->F.multi_head_attention_forward(query, key, value, self.embed_dim, self.num_heads, self.in_proj_weight, self.in_proj_bias, self.bias_k, self.bias_v, self.add_zero_attn, self.dropout, self.out_proj.weight, self.out_proj.bias, training=self.training, key_padding_mask=key_padding_mask, need_weights=need_weights, attn_mask=attn_mask, average_attn_weights=average_attn_weights)
A:torch.nn.modules.activation.self.weight->Parameter(torch.empty(num_parameters, **factory_kwargs).fill_(init))
torch.nn.CELU(self,alpha:float=1.0,inplace:bool=False)
torch.nn.CELU.extra_repr(self)->str
torch.nn.CELU.forward(self,input:Tensor)->Tensor
torch.nn.ELU(self,alpha:float=1.0,inplace:bool=False)
torch.nn.ELU.extra_repr(self)->str
torch.nn.ELU.forward(self,input:Tensor)->Tensor
torch.nn.GELU(Module)
torch.nn.GELU.forward(self,input:Tensor)->Tensor
torch.nn.GLU(self,dim:int=-1)
torch.nn.GLU.extra_repr(self)->str
torch.nn.GLU.forward(self,input:Tensor)->Tensor
torch.nn.Hardshrink(self,lambd:float=0.5)
torch.nn.Hardshrink.extra_repr(self)->str
torch.nn.Hardshrink.forward(self,input:Tensor)->Tensor
torch.nn.Hardsigmoid(self,inplace:bool=False)
torch.nn.Hardsigmoid.forward(self,input:Tensor)->Tensor
torch.nn.Hardswish(self,inplace:bool=False)
torch.nn.Hardswish.forward(self,input:Tensor)->Tensor
torch.nn.Hardtanh(self,min_val:float=-1.0,max_val:float=1.0,inplace:bool=False,min_value:Optional[float]=None,max_value:Optional[float]=None)
torch.nn.Hardtanh.extra_repr(self)->str
torch.nn.Hardtanh.forward(self,input:Tensor)->Tensor
torch.nn.LeakyReLU(self,negative_slope:float=0.01,inplace:bool=False)
torch.nn.LeakyReLU.extra_repr(self)->str
torch.nn.LeakyReLU.forward(self,input:Tensor)->Tensor
torch.nn.LogSigmoid(Module)
torch.nn.LogSigmoid.forward(self,input:Tensor)->Tensor
torch.nn.LogSoftmax(self,dim:Optional[int]=None)
torch.nn.LogSoftmax.__setstate__(self,state)
torch.nn.LogSoftmax.extra_repr(self)
torch.nn.LogSoftmax.forward(self,input:Tensor)->Tensor
torch.nn.Mish(self,inplace:bool=False)
torch.nn.Mish.extra_repr(self)->str
torch.nn.Mish.forward(self,input:Tensor)->Tensor
torch.nn.MultiheadAttention(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None,batch_first=False,device=None,dtype=None)
torch.nn.MultiheadAttention.__setstate__(self,state)
torch.nn.MultiheadAttention._reset_parameters(self)
torch.nn.MultiheadAttention.forward(self,query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.PReLU(self,num_parameters:int=1,init:float=0.25,device=None,dtype=None)
torch.nn.PReLU.extra_repr(self)->str
torch.nn.PReLU.forward(self,input:Tensor)->Tensor
torch.nn.RReLU(self,lower:float=1.0/8,upper:float=1.0/3,inplace:bool=False)
torch.nn.RReLU.extra_repr(self)
torch.nn.RReLU.forward(self,input:Tensor)->Tensor
torch.nn.ReLU(self,inplace:bool=False)
torch.nn.ReLU.extra_repr(self)->str
torch.nn.ReLU.forward(self,input:Tensor)->Tensor
torch.nn.ReLU6(self,inplace:bool=False)
torch.nn.ReLU6.extra_repr(self)->str
torch.nn.SELU(self,inplace:bool=False)
torch.nn.SELU.extra_repr(self)->str
torch.nn.SELU.forward(self,input:Tensor)->Tensor
torch.nn.SiLU(self,inplace:bool=False)
torch.nn.SiLU.extra_repr(self)->str
torch.nn.SiLU.forward(self,input:Tensor)->Tensor
torch.nn.Sigmoid(Module)
torch.nn.Sigmoid.forward(self,input:Tensor)->Tensor
torch.nn.Softmax(self,dim:Optional[int]=None)
torch.nn.Softmax.__setstate__(self,state)
torch.nn.Softmax.extra_repr(self)->str
torch.nn.Softmax.forward(self,input:Tensor)->Tensor
torch.nn.Softmax2d(Module)
torch.nn.Softmax2d.forward(self,input:Tensor)->Tensor
torch.nn.Softmin(self,dim:Optional[int]=None)
torch.nn.Softmin.__setstate__(self,state)
torch.nn.Softmin.extra_repr(self)
torch.nn.Softmin.forward(self,input:Tensor)->Tensor
torch.nn.Softplus(self,beta:int=1,threshold:int=20)
torch.nn.Softplus.extra_repr(self)->str
torch.nn.Softplus.forward(self,input:Tensor)->Tensor
torch.nn.Softshrink(self,lambd:float=0.5)
torch.nn.Softshrink.extra_repr(self)->str
torch.nn.Softshrink.forward(self,input:Tensor)->Tensor
torch.nn.Softsign(Module)
torch.nn.Softsign.forward(self,input:Tensor)->Tensor
torch.nn.Tanh(Module)
torch.nn.Tanh.forward(self,input:Tensor)->Tensor
torch.nn.Tanhshrink(Module)
torch.nn.Tanhshrink.forward(self,input:Tensor)->Tensor
torch.nn.Threshold(self,threshold:float,value:float,inplace:bool=False)
torch.nn.Threshold.extra_repr(self)
torch.nn.Threshold.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.CELU(self,alpha:float=1.0,inplace:bool=False)
torch.nn.modules.activation.CELU.__init__(self,alpha:float=1.0,inplace:bool=False)
torch.nn.modules.activation.CELU.extra_repr(self)->str
torch.nn.modules.activation.CELU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.ELU(self,alpha:float=1.0,inplace:bool=False)
torch.nn.modules.activation.ELU.__init__(self,alpha:float=1.0,inplace:bool=False)
torch.nn.modules.activation.ELU.extra_repr(self)->str
torch.nn.modules.activation.ELU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.GELU(Module)
torch.nn.modules.activation.GELU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.GLU(self,dim:int=-1)
torch.nn.modules.activation.GLU.__init__(self,dim:int=-1)
torch.nn.modules.activation.GLU.extra_repr(self)->str
torch.nn.modules.activation.GLU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Hardshrink(self,lambd:float=0.5)
torch.nn.modules.activation.Hardshrink.__init__(self,lambd:float=0.5)
torch.nn.modules.activation.Hardshrink.extra_repr(self)->str
torch.nn.modules.activation.Hardshrink.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Hardsigmoid(self,inplace:bool=False)
torch.nn.modules.activation.Hardsigmoid.__init__(self,inplace:bool=False)
torch.nn.modules.activation.Hardsigmoid.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Hardswish(self,inplace:bool=False)
torch.nn.modules.activation.Hardswish.__init__(self,inplace:bool=False)
torch.nn.modules.activation.Hardswish.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Hardtanh(self,min_val:float=-1.0,max_val:float=1.0,inplace:bool=False,min_value:Optional[float]=None,max_value:Optional[float]=None)
torch.nn.modules.activation.Hardtanh.__init__(self,min_val:float=-1.0,max_val:float=1.0,inplace:bool=False,min_value:Optional[float]=None,max_value:Optional[float]=None)
torch.nn.modules.activation.Hardtanh.extra_repr(self)->str
torch.nn.modules.activation.Hardtanh.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.LeakyReLU(self,negative_slope:float=0.01,inplace:bool=False)
torch.nn.modules.activation.LeakyReLU.__init__(self,negative_slope:float=0.01,inplace:bool=False)
torch.nn.modules.activation.LeakyReLU.extra_repr(self)->str
torch.nn.modules.activation.LeakyReLU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.LogSigmoid(Module)
torch.nn.modules.activation.LogSigmoid.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.LogSoftmax(self,dim:Optional[int]=None)
torch.nn.modules.activation.LogSoftmax.__init__(self,dim:Optional[int]=None)
torch.nn.modules.activation.LogSoftmax.__setstate__(self,state)
torch.nn.modules.activation.LogSoftmax.extra_repr(self)
torch.nn.modules.activation.LogSoftmax.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Mish(self,inplace:bool=False)
torch.nn.modules.activation.Mish.__init__(self,inplace:bool=False)
torch.nn.modules.activation.Mish.extra_repr(self)->str
torch.nn.modules.activation.Mish.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.MultiheadAttention(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None,batch_first=False,device=None,dtype=None)
torch.nn.modules.activation.MultiheadAttention.__init__(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None,batch_first=False,device=None,dtype=None)
torch.nn.modules.activation.MultiheadAttention.__setstate__(self,state)
torch.nn.modules.activation.MultiheadAttention._reset_parameters(self)
torch.nn.modules.activation.MultiheadAttention.forward(self,query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.modules.activation.PReLU(self,num_parameters:int=1,init:float=0.25,device=None,dtype=None)
torch.nn.modules.activation.PReLU.__init__(self,num_parameters:int=1,init:float=0.25,device=None,dtype=None)
torch.nn.modules.activation.PReLU.extra_repr(self)->str
torch.nn.modules.activation.PReLU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.RReLU(self,lower:float=1.0/8,upper:float=1.0/3,inplace:bool=False)
torch.nn.modules.activation.RReLU.__init__(self,lower:float=1.0/8,upper:float=1.0/3,inplace:bool=False)
torch.nn.modules.activation.RReLU.extra_repr(self)
torch.nn.modules.activation.RReLU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.ReLU(self,inplace:bool=False)
torch.nn.modules.activation.ReLU.__init__(self,inplace:bool=False)
torch.nn.modules.activation.ReLU.extra_repr(self)->str
torch.nn.modules.activation.ReLU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.ReLU6(self,inplace:bool=False)
torch.nn.modules.activation.ReLU6.__init__(self,inplace:bool=False)
torch.nn.modules.activation.ReLU6.extra_repr(self)->str
torch.nn.modules.activation.SELU(self,inplace:bool=False)
torch.nn.modules.activation.SELU.__init__(self,inplace:bool=False)
torch.nn.modules.activation.SELU.extra_repr(self)->str
torch.nn.modules.activation.SELU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.SiLU(self,inplace:bool=False)
torch.nn.modules.activation.SiLU.__init__(self,inplace:bool=False)
torch.nn.modules.activation.SiLU.extra_repr(self)->str
torch.nn.modules.activation.SiLU.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Sigmoid(Module)
torch.nn.modules.activation.Sigmoid.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Softmax(self,dim:Optional[int]=None)
torch.nn.modules.activation.Softmax.__init__(self,dim:Optional[int]=None)
torch.nn.modules.activation.Softmax.__setstate__(self,state)
torch.nn.modules.activation.Softmax.extra_repr(self)->str
torch.nn.modules.activation.Softmax.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Softmax2d(Module)
torch.nn.modules.activation.Softmax2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Softmin(self,dim:Optional[int]=None)
torch.nn.modules.activation.Softmin.__init__(self,dim:Optional[int]=None)
torch.nn.modules.activation.Softmin.__setstate__(self,state)
torch.nn.modules.activation.Softmin.extra_repr(self)
torch.nn.modules.activation.Softmin.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Softplus(self,beta:int=1,threshold:int=20)
torch.nn.modules.activation.Softplus.__init__(self,beta:int=1,threshold:int=20)
torch.nn.modules.activation.Softplus.extra_repr(self)->str
torch.nn.modules.activation.Softplus.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Softshrink(self,lambd:float=0.5)
torch.nn.modules.activation.Softshrink.__init__(self,lambd:float=0.5)
torch.nn.modules.activation.Softshrink.extra_repr(self)->str
torch.nn.modules.activation.Softshrink.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Softsign(Module)
torch.nn.modules.activation.Softsign.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Tanh(Module)
torch.nn.modules.activation.Tanh.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Tanhshrink(Module)
torch.nn.modules.activation.Tanhshrink.forward(self,input:Tensor)->Tensor
torch.nn.modules.activation.Threshold(self,threshold:float,value:float,inplace:bool=False)
torch.nn.modules.activation.Threshold.__init__(self,threshold:float,value:float,inplace:bool=False)
torch.nn.modules.activation.Threshold.extra_repr(self)
torch.nn.modules.activation.Threshold.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/distance.py----------------------------------------
torch.nn.CosineSimilarity(self,dim:int=1,eps:float=1e-08)
torch.nn.CosineSimilarity.forward(self,x1:Tensor,x2:Tensor)->Tensor
torch.nn.PairwiseDistance(self,p:float=2.0,eps:float=1e-06,keepdim:bool=False)
torch.nn.PairwiseDistance.forward(self,x1:Tensor,x2:Tensor)->Tensor
torch.nn.modules.distance.CosineSimilarity(self,dim:int=1,eps:float=1e-08)
torch.nn.modules.distance.CosineSimilarity.__init__(self,dim:int=1,eps:float=1e-08)
torch.nn.modules.distance.CosineSimilarity.forward(self,x1:Tensor,x2:Tensor)->Tensor
torch.nn.modules.distance.PairwiseDistance(self,p:float=2.0,eps:float=1e-06,keepdim:bool=False)
torch.nn.modules.distance.PairwiseDistance.__init__(self,p:float=2.0,eps:float=1e-06,keepdim:bool=False)
torch.nn.modules.distance.PairwiseDistance.forward(self,x1:Tensor,x2:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/module.py----------------------------------------
A:torch.nn.modules.module.T->TypeVar('T', bound='Module')
A:torch.nn.modules.module.s->'\n'.join(s)
A:torch.nn.modules.module.first->'\n'.join(s).pop(0)
A:torch.nn.modules.module.handle->torch.utils.hooks.RemovableHandle(self._load_state_dict_pre_hooks)
A:torch.nn.modules.module.mod->getattr(mod, item)
A:torch.nn.modules.module.(module_path, _, param_name)->target.rpartition('.')
A:torch.nn.modules.module.(module_path, _, buffer_name)->target.rpartition('.')
A:torch.nn.modules.module.param_applied->fn(param)
A:torch.nn.modules.module.should_use_set_data->compute_should_use_set_data(param.grad, grad_applied)
A:torch.nn.modules.module.out_param->Parameter(param_applied, param.requires_grad)
A:torch.nn.modules.module.grad_applied->fn(param.grad)
A:torch.nn.modules.module.out_param.grad->fn(param.grad).requires_grad_(param.grad.requires_grad)
A:torch.nn.modules.module.self._buffers[key]->fn(buf)
A:torch.nn.modules.module.(device, dtype, non_blocking, convert_to_format)->torch._C._nn._parse_to(*args, **kwargs)
A:torch.nn.modules.module.tracing_state->torch._C._get_tracing_state()
A:torch.nn.modules.module.result->torch.utils.hooks.BackwardHook(self, full_backward_hooks).setup_output_hook(result)
A:torch.nn.modules.module.(full_backward_hooks, non_full_backward_hooks)->self._get_backward_hooks()
A:torch.nn.modules.module.bw_hook->torch.utils.hooks.BackwardHook(self, full_backward_hooks)
A:torch.nn.modules.module.input->torch.utils.hooks.BackwardHook(self, full_backward_hooks).setup_input_hook(input)
A:torch.nn.modules.module.hook_result->hook(self, destination, prefix, local_metadata)
A:torch.nn.modules.module.var->next((v for v in var.values() if isinstance(v, torch.Tensor)))
A:torch.nn.modules.module.wrapper->functools.partial(hook, self)
A:torch.nn.modules.module.self._forward_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._state_dict_hooks->OrderedDict()
A:torch.nn.modules.module.self._load_state_dict_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._non_persistent_buffers_set->set()
A:torch.nn.modules.module.params->self.__dict__.get('_parameters')
A:torch.nn.modules.module.modules->list(self._modules.keys())
A:torch.nn.modules.module.buffers->list(self._buffers.keys())
A:torch.nn.modules.module.destination[extra_state_key]->self.get_extra_state()
A:torch.nn.modules.module.T_destination->TypeVar('T_destination', bound=Mapping[str, Tensor])
A:torch.nn.modules.module.destination->OrderedDict()
A:torch.nn.modules.module.destination._metadata->OrderedDict()
A:torch.nn.modules.module.destination._metadata[prefix[:-1]]local_metadata->dict(version=self._version)
A:torch.nn.modules.module.hook->functools.partial(hook, self)
A:torch.nn.modules.module.local_name_params->itertools.chain(self._parameters.items(), persistent_buffers.items())
A:torch.nn.modules.module.is_param_lazy->torch.nn.parameter.is_lazy(param)
A:torch.nn.modules.module.metadata->getattr(state_dict, '_metadata', None)
A:torch.nn.modules.module.state_dict->state_dict.copy().copy()
A:torch.nn.modules.module.memo->set()
A:torch.nn.modules.module.members->get_members_fn(module)
A:torch.nn.modules.module.gen->self._named_members(lambda module: module._buffers.items(), prefix=prefix, recurse=recurse)
A:torch.nn.modules.module.extra_repr->self.extra_repr()
A:torch.nn.modules.module.extra_lines->self.extra_repr().split('\n')
A:torch.nn.modules.module.mod_str->_addindent(mod_str, 2)
A:torch.nn.modules.module.module_attrs->dir(self.__class__)
A:torch.nn.modules.module.attrs->list(self.__dict__.keys())
A:torch.nn.modules.module.parameters->list(self._parameters.keys())
A:torch.nn.modules.module.replica->self.__new__(type(self))
A:torch.nn.modules.module.replica.__dict__->self.__dict__.copy()
A:torch.nn.modules.module.replica._parameters->OrderedDict()
A:torch.nn.modules.module.replica._buffers->self.__new__(type(self))._buffers.copy()
A:torch.nn.modules.module.replica._modules->self.__new__(type(self))._modules.copy()
torch.nn.Module(self)
torch.nn.Module.__delattr__(self,name)
torch.nn.Module.__dir__(self)
torch.nn.Module.__getattr__(self,name:str)->Union[Tensor, 'Module']
torch.nn.Module.__repr__(self)
torch.nn.Module.__setattr__(self,name:str,value:Union[Tensor,'Module'])->None
torch.nn.Module.__setstate__(self,state)
torch.nn.Module._apply(self,fn)
torch.nn.Module._call_impl(self,*input,**kwargs)
torch.nn.Module._get_backward_hooks(self)
torch.nn.Module._get_name(self)
torch.nn.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.Module._maybe_warn_non_full_backward_hook(self,inputs,result,grad_fn)
torch.nn.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.Module._register_load_state_dict_pre_hook(self,hook,with_module=False)
torch.nn.Module._register_state_dict_hook(self,hook)
torch.nn.Module._replicate_for_data_parallel(self)
torch.nn.Module._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.Module._slow_forward(self,*input,**kwargs)
torch.nn.Module.add_module(self,name:str,module:Optional['Module'])->None
torch.nn.Module.apply(self:T,fn:Callable[['Module'],None])->T
torch.nn.Module.bfloat16(self:T)->T
torch.nn.Module.buffers(self,recurse:bool=True)->Iterator[Tensor]
torch.nn.Module.children(self)->Iterator['Module']
torch.nn.Module.cpu(self:T)->T
torch.nn.Module.cuda(self:T,device:Optional[Union[int,device]]=None)->T
torch.nn.Module.double(self:T)->T
torch.nn.Module.eval(self:T)->T
torch.nn.Module.extra_repr(self)->str
torch.nn.Module.float(self:T)->T
torch.nn.Module.get_buffer(self,target:str)->'Tensor'
torch.nn.Module.get_extra_state(self)->Any
torch.nn.Module.get_parameter(self,target:str)->'Parameter'
torch.nn.Module.get_submodule(self,target:str)->'Module'
torch.nn.Module.half(self:T)->T
torch.nn.Module.load_state_dict(self,state_dict:'OrderedDict[str,Tensor]',strict:bool=True)
torch.nn.Module.modules(self)->Iterator['Module']
torch.nn.Module.named_buffers(self,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Tensor]]
torch.nn.Module.named_children(self)->Iterator[Tuple[str, 'Module']]
torch.nn.Module.named_modules(self,memo:Optional[Set['Module']]=None,prefix:str='',remove_duplicate:bool=True)
torch.nn.Module.named_parameters(self,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Parameter]]
torch.nn.Module.parameters(self,recurse:bool=True)->Iterator[Parameter]
torch.nn.Module.register_backward_hook(self,hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.Module.register_buffer(self,name:str,tensor:Optional[Tensor],persistent:bool=True)->None
torch.nn.Module.register_forward_hook(self,hook:Callable[...,None])->RemovableHandle
torch.nn.Module.register_forward_pre_hook(self,hook:Callable[...,None])->RemovableHandle
torch.nn.Module.register_full_backward_hook(self,hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.Module.register_module(self,name:str,module:Optional['Module'])->None
torch.nn.Module.register_parameter(self,name:str,param:Optional[Parameter])->None
torch.nn.Module.requires_grad_(self:T,requires_grad:bool=True)->T
torch.nn.Module.set_extra_state(self,state:Any)
torch.nn.Module.share_memory(self:T)->T
torch.nn.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.Module.to(self,*args,**kwargs)
torch.nn.Module.to_empty(self:T,*,device:Union[str,device])->T
torch.nn.Module.train(self:T,mode:bool=True)->T
torch.nn.Module.type(self:T,dst_type:Union[dtype,str])->T
torch.nn.Module.xpu(self:T,device:Optional[Union[int,device]]=None)->T
torch.nn.Module.zero_grad(self,set_to_none:bool=False)->None
torch.nn.module._IncompatibleKeys(namedtuple('IncompatibleKeys',['missing_keys','unexpected_keys']))
torch.nn.module._IncompatibleKeys.__repr__(self)
torch.nn.module._addindent(s_,numSpaces)
torch.nn.module._forward_unimplemented(self,*input:Any)->None
torch.nn.module.register_module_backward_hook(hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.module.register_module_forward_hook(hook:Callable[...,None])->RemovableHandle
torch.nn.module.register_module_forward_pre_hook(hook:Callable[...,None])->RemovableHandle
torch.nn.module.register_module_full_backward_hook(hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.modules.module.Module(self)
torch.nn.modules.module.Module.__delattr__(self,name)
torch.nn.modules.module.Module.__dir__(self)
torch.nn.modules.module.Module.__getattr__(self,name:str)->Union[Tensor, 'Module']
torch.nn.modules.module.Module.__init__(self)
torch.nn.modules.module.Module.__repr__(self)
torch.nn.modules.module.Module.__setattr__(self,name:str,value:Union[Tensor,'Module'])->None
torch.nn.modules.module.Module.__setstate__(self,state)
torch.nn.modules.module.Module._apply(self,fn)
torch.nn.modules.module.Module._call_impl(self,*input,**kwargs)
torch.nn.modules.module.Module._get_backward_hooks(self)
torch.nn.modules.module.Module._get_name(self)
torch.nn.modules.module.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.module.Module._maybe_warn_non_full_backward_hook(self,inputs,result,grad_fn)
torch.nn.modules.module.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self,hook,with_module=False)
torch.nn.modules.module.Module._register_state_dict_hook(self,hook)
torch.nn.modules.module.Module._replicate_for_data_parallel(self)
torch.nn.modules.module.Module._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.modules.module.Module._slow_forward(self,*input,**kwargs)
torch.nn.modules.module.Module.add_module(self,name:str,module:Optional['Module'])->None
torch.nn.modules.module.Module.apply(self:T,fn:Callable[['Module'],None])->T
torch.nn.modules.module.Module.bfloat16(self:T)->T
torch.nn.modules.module.Module.buffers(self,recurse:bool=True)->Iterator[Tensor]
torch.nn.modules.module.Module.children(self)->Iterator['Module']
torch.nn.modules.module.Module.cpu(self:T)->T
torch.nn.modules.module.Module.cuda(self:T,device:Optional[Union[int,device]]=None)->T
torch.nn.modules.module.Module.double(self:T)->T
torch.nn.modules.module.Module.eval(self:T)->T
torch.nn.modules.module.Module.extra_repr(self)->str
torch.nn.modules.module.Module.float(self:T)->T
torch.nn.modules.module.Module.get_buffer(self,target:str)->'Tensor'
torch.nn.modules.module.Module.get_extra_state(self)->Any
torch.nn.modules.module.Module.get_parameter(self,target:str)->'Parameter'
torch.nn.modules.module.Module.get_submodule(self,target:str)->'Module'
torch.nn.modules.module.Module.half(self:T)->T
torch.nn.modules.module.Module.load_state_dict(self,state_dict:'OrderedDict[str,Tensor]',strict:bool=True)
torch.nn.modules.module.Module.modules(self)->Iterator['Module']
torch.nn.modules.module.Module.named_buffers(self,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Tensor]]
torch.nn.modules.module.Module.named_children(self)->Iterator[Tuple[str, 'Module']]
torch.nn.modules.module.Module.named_modules(self,memo:Optional[Set['Module']]=None,prefix:str='',remove_duplicate:bool=True)
torch.nn.modules.module.Module.named_parameters(self,prefix:str='',recurse:bool=True)->Iterator[Tuple[str, Parameter]]
torch.nn.modules.module.Module.parameters(self,recurse:bool=True)->Iterator[Parameter]
torch.nn.modules.module.Module.register_backward_hook(self,hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.modules.module.Module.register_buffer(self,name:str,tensor:Optional[Tensor],persistent:bool=True)->None
torch.nn.modules.module.Module.register_forward_hook(self,hook:Callable[...,None])->RemovableHandle
torch.nn.modules.module.Module.register_forward_pre_hook(self,hook:Callable[...,None])->RemovableHandle
torch.nn.modules.module.Module.register_full_backward_hook(self,hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.modules.module.Module.register_module(self,name:str,module:Optional['Module'])->None
torch.nn.modules.module.Module.register_parameter(self,name:str,param:Optional[Parameter])->None
torch.nn.modules.module.Module.requires_grad_(self:T,requires_grad:bool=True)->T
torch.nn.modules.module.Module.set_extra_state(self,state:Any)
torch.nn.modules.module.Module.share_memory(self:T)->T
torch.nn.modules.module.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.modules.module.Module.to(self,*args,**kwargs)
torch.nn.modules.module.Module.to_empty(self:T,*,device:Union[str,device])->T
torch.nn.modules.module.Module.train(self:T,mode:bool=True)->T
torch.nn.modules.module.Module.type(self:T,dst_type:Union[dtype,str])->T
torch.nn.modules.module.Module.xpu(self:T,device:Optional[Union[int,device]]=None)->T
torch.nn.modules.module.Module.zero_grad(self,set_to_none:bool=False)->None
torch.nn.modules.module._IncompatibleKeys(namedtuple('IncompatibleKeys',['missing_keys','unexpected_keys']))
torch.nn.modules.module._IncompatibleKeys.__repr__(self)
torch.nn.modules.module._addindent(s_,numSpaces)
torch.nn.modules.module._forward_unimplemented(self,*input:Any)->None
torch.nn.modules.module.register_module_backward_hook(hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle
torch.nn.modules.module.register_module_forward_hook(hook:Callable[...,None])->RemovableHandle
torch.nn.modules.module.register_module_forward_pre_hook(hook:Callable[...,None])->RemovableHandle
torch.nn.modules.module.register_module_full_backward_hook(hook:Callable[['Module',_grad_t,_grad_t],Union[None,Tensor]])->RemovableHandle


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/sparse.py----------------------------------------
A:torch.nn.modules.sparse.self.weight->Parameter(_weight)
A:torch.nn.modules.sparse.embedding->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, padding_idx=padding_idx, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, sparse=sparse)
A:torch.nn.modules.sparse.embeddingbag->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, mode=mode, sparse=sparse, include_last_offset=include_last_offset, padding_idx=padding_idx)
torch.nn.Embedding(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[Tensor]=None,device=None,dtype=None)
torch.nn.Embedding._fill_padding_idx_with_zero(self)->None
torch.nn.Embedding.extra_repr(self)->str
torch.nn.Embedding.forward(self,input:Tensor)->Tensor
torch.nn.Embedding.from_pretrained(cls,embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.Embedding.reset_parameters(self)->None
torch.nn.EmbeddingBag(self,num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,_weight:Optional[Tensor]=None,include_last_offset:bool=False,padding_idx:Optional[int]=None,device=None,dtype=None)
torch.nn.EmbeddingBag._fill_padding_idx_with_zero(self)->None
torch.nn.EmbeddingBag.extra_repr(self)->str
torch.nn.EmbeddingBag.forward(self,input:Tensor,offsets:Optional[Tensor]=None,per_sample_weights:Optional[Tensor]=None)->Tensor
torch.nn.EmbeddingBag.from_pretrained(cls,embeddings:Tensor,freeze:bool=True,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,include_last_offset:bool=False,padding_idx:Optional[int]=None)->'EmbeddingBag'
torch.nn.EmbeddingBag.reset_parameters(self)->None
torch.nn.modules.sparse.Embedding(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[Tensor]=None,device=None,dtype=None)
torch.nn.modules.sparse.Embedding.__init__(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[Tensor]=None,device=None,dtype=None)
torch.nn.modules.sparse.Embedding._fill_padding_idx_with_zero(self)->None
torch.nn.modules.sparse.Embedding.extra_repr(self)->str
torch.nn.modules.sparse.Embedding.forward(self,input:Tensor)->Tensor
torch.nn.modules.sparse.Embedding.from_pretrained(cls,embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.modules.sparse.Embedding.reset_parameters(self)->None
torch.nn.modules.sparse.EmbeddingBag(self,num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,_weight:Optional[Tensor]=None,include_last_offset:bool=False,padding_idx:Optional[int]=None,device=None,dtype=None)
torch.nn.modules.sparse.EmbeddingBag.__init__(self,num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,_weight:Optional[Tensor]=None,include_last_offset:bool=False,padding_idx:Optional[int]=None,device=None,dtype=None)
torch.nn.modules.sparse.EmbeddingBag._fill_padding_idx_with_zero(self)->None
torch.nn.modules.sparse.EmbeddingBag.extra_repr(self)->str
torch.nn.modules.sparse.EmbeddingBag.forward(self,input:Tensor,offsets:Optional[Tensor]=None,per_sample_weights:Optional[Tensor]=None)->Tensor
torch.nn.modules.sparse.EmbeddingBag.from_pretrained(cls,embeddings:Tensor,freeze:bool=True,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,include_last_offset:bool=False,padding_idx:Optional[int]=None)->'EmbeddingBag'
torch.nn.modules.sparse.EmbeddingBag.reset_parameters(self)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/lazy.py----------------------------------------
A:torch.nn.modules.lazy.self._load_hook->self._register_load_state_dict_pre_hook(self._lazy_load_hook)
A:torch.nn.modules.lazy.self._initialize_hook->self.register_forward_pre_hook(self._infer_parameters)
A:torch.nn.modules.lazy.param->param.detach().detach()
A:torch.nn.modules.lazy.buf->buf.detach().detach()
A:torch.nn.modules.lazy.params->self._parameters.values()
A:torch.nn.modules.lazy.buffers->self._buffers.values()
torch.nn.lazy.LazyModuleMixin(self:_LazyProtocol,*args,**kwargs)
torch.nn.lazy.LazyModuleMixin._infer_parameters(self:_LazyProtocol,module,input)
torch.nn.lazy.LazyModuleMixin._lazy_load_hook(self:_LazyProtocol,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.lazy.LazyModuleMixin._replicate_for_data_parallel(self:_LazyProtocol)
torch.nn.lazy.LazyModuleMixin._save_to_state_dict(self:_LazyProtocol,destination,prefix,keep_vars)
torch.nn.lazy.LazyModuleMixin.has_uninitialized_params(self:_LazyProtocol)
torch.nn.lazy.LazyModuleMixin.initialize_parameters(self:_LazyProtocol,*args,**kwargs)
torch.nn.lazy._LazyProtocol(Protocol)
torch.nn.lazy._LazyProtocol._buffers(self)
torch.nn.lazy._LazyProtocol._get_name(self)
torch.nn.lazy._LazyProtocol._infer_parameters(self,module,input)
torch.nn.lazy._LazyProtocol._initialize_hook(self)
torch.nn.lazy._LazyProtocol._lazy_load_hook(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.lazy._LazyProtocol._load_hook(self)
torch.nn.lazy._LazyProtocol._non_persistent_buffers_set(self)
torch.nn.lazy._LazyProtocol._parameters(self)
torch.nn.lazy._LazyProtocol._register_load_state_dict_pre_hook(self,hook)
torch.nn.lazy._LazyProtocol.register_forward_pre_hook(self,hook)
torch.nn.modules.lazy.LazyModuleMixin(self:_LazyProtocol,*args,**kwargs)
torch.nn.modules.lazy.LazyModuleMixin.__init__(self:_LazyProtocol,*args,**kwargs)
torch.nn.modules.lazy.LazyModuleMixin._infer_parameters(self:_LazyProtocol,module,input)
torch.nn.modules.lazy.LazyModuleMixin._lazy_load_hook(self:_LazyProtocol,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.lazy.LazyModuleMixin._replicate_for_data_parallel(self:_LazyProtocol)
torch.nn.modules.lazy.LazyModuleMixin._save_to_state_dict(self:_LazyProtocol,destination,prefix,keep_vars)
torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params(self:_LazyProtocol)
torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters(self:_LazyProtocol,*args,**kwargs)
torch.nn.modules.lazy._LazyProtocol(Protocol)
torch.nn.modules.lazy._LazyProtocol._buffers(self)
torch.nn.modules.lazy._LazyProtocol._get_name(self)
torch.nn.modules.lazy._LazyProtocol._infer_parameters(self,module,input)
torch.nn.modules.lazy._LazyProtocol._initialize_hook(self)
torch.nn.modules.lazy._LazyProtocol._lazy_load_hook(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.lazy._LazyProtocol._load_hook(self)
torch.nn.modules.lazy._LazyProtocol._non_persistent_buffers_set(self)
torch.nn.modules.lazy._LazyProtocol._parameters(self)
torch.nn.modules.lazy._LazyProtocol._register_load_state_dict_pre_hook(self,hook)
torch.nn.modules.lazy._LazyProtocol.register_forward_pre_hook(self,hook)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/utils.py----------------------------------------
A:torch.nn.modules.utils._single->_ntuple(1, '_single')
A:torch.nn.modules.utils._pair->_ntuple(2, '_pair')
A:torch.nn.modules.utils._triple->_ntuple(3, '_triple')
A:torch.nn.modules.utils._quadruple->_ntuple(4, '_quadruple')
A:torch.nn.modules.utils.keys->sorted(state_dict.keys())
A:torch.nn.modules.utils.state_dict[newkey]->state_dict.pop(key)
A:torch.nn.modules.utils.metadata[newkey]->metadata.pop(key)
torch.nn.modules.utils._list_with_default(out_size:List[int],defaults:List[int])->List[int]
torch.nn.modules.utils._ntuple(n,name='parse')
torch.nn.modules.utils._reverse_repeat_tuple(t,n)
torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(state_dict:Dict[str,Any],prefix:str)->None
torch.nn.utils._list_with_default(out_size:List[int],defaults:List[int])->List[int]
torch.nn.utils._ntuple(n,name='parse')
torch.nn.utils._reverse_repeat_tuple(t,n)
torch.nn.utils.consume_prefix_in_state_dict_if_present(state_dict:Dict[str,Any],prefix:str)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/linear.py----------------------------------------
A:torch.nn.modules.linear.self.weight->UninitializedParameter(**factory_kwargs)
A:torch.nn.modules.linear.self.bias->UninitializedParameter(**factory_kwargs)
A:torch.nn.modules.linear.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
torch.nn.Bilinear(self,in1_features:int,in2_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.Bilinear.extra_repr(self)->str
torch.nn.Bilinear.forward(self,input1:Tensor,input2:Tensor)->Tensor
torch.nn.Bilinear.reset_parameters(self)->None
torch.nn.Identity(self,*args,**kwargs)
torch.nn.Identity.forward(self,input:Tensor)->Tensor
torch.nn.LazyLinear(self,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.LazyLinear.initialize_parameters(self,input)->None
torch.nn.LazyLinear.reset_parameters(self)->None
torch.nn.Linear(self,in_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.Linear.extra_repr(self)->str
torch.nn.Linear.forward(self,input:Tensor)->Tensor
torch.nn.Linear.reset_parameters(self)->None
torch.nn.linear.NonDynamicallyQuantizableLinear(self,in_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.Bilinear(self,in1_features:int,in2_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.Bilinear.__init__(self,in1_features:int,in2_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.Bilinear.extra_repr(self)->str
torch.nn.modules.linear.Bilinear.forward(self,input1:Tensor,input2:Tensor)->Tensor
torch.nn.modules.linear.Bilinear.reset_parameters(self)->None
torch.nn.modules.linear.Identity(self,*args,**kwargs)
torch.nn.modules.linear.Identity.__init__(self,*args,**kwargs)
torch.nn.modules.linear.Identity.forward(self,input:Tensor)->Tensor
torch.nn.modules.linear.LazyLinear(self,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.LazyLinear.__init__(self,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.LazyLinear.initialize_parameters(self,input)->None
torch.nn.modules.linear.LazyLinear.reset_parameters(self)->None
torch.nn.modules.linear.Linear(self,in_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.Linear.__init__(self,in_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.Linear.extra_repr(self)->str
torch.nn.modules.linear.Linear.forward(self,input:Tensor)->Tensor
torch.nn.modules.linear.Linear.reset_parameters(self)->None
torch.nn.modules.linear.NonDynamicallyQuantizableLinear(self,in_features:int,out_features:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.linear.NonDynamicallyQuantizableLinear.__init__(self,in_features:int,out_features:int,bias:bool=True,device=None,dtype=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/pixelshuffle.py----------------------------------------
torch.nn.PixelShuffle(self,upscale_factor:int)
torch.nn.PixelShuffle.extra_repr(self)->str
torch.nn.PixelShuffle.forward(self,input:Tensor)->Tensor
torch.nn.PixelUnshuffle(self,downscale_factor:int)
torch.nn.PixelUnshuffle.extra_repr(self)->str
torch.nn.PixelUnshuffle.forward(self,input:Tensor)->Tensor
torch.nn.modules.pixelshuffle.PixelShuffle(self,upscale_factor:int)
torch.nn.modules.pixelshuffle.PixelShuffle.__init__(self,upscale_factor:int)
torch.nn.modules.pixelshuffle.PixelShuffle.extra_repr(self)->str
torch.nn.modules.pixelshuffle.PixelShuffle.forward(self,input:Tensor)->Tensor
torch.nn.modules.pixelshuffle.PixelUnshuffle(self,downscale_factor:int)
torch.nn.modules.pixelshuffle.PixelUnshuffle.__init__(self,downscale_factor:int)
torch.nn.modules.pixelshuffle.PixelUnshuffle.extra_repr(self)->str
torch.nn.modules.pixelshuffle.PixelUnshuffle.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/adaptive.py----------------------------------------
A:torch.nn.modules.adaptive._ASMoutput->namedtuple('_ASMoutput', ['output', 'loss'])
A:torch.nn.modules.adaptive.cutoffs->list(cutoffs)
A:torch.nn.modules.adaptive.self.head->Linear(self.in_features, self.head_size, bias=self.head_bias, **factory_kwargs)
A:torch.nn.modules.adaptive.self.tail->ModuleList()
A:torch.nn.modules.adaptive.hsz->int(self.in_features // self.div_value ** (i + 1))
A:torch.nn.modules.adaptive.projection->Sequential(Linear(self.in_features, hsz, bias=False, **factory_kwargs), Linear(hsz, osz, bias=False, **factory_kwargs))
A:torch.nn.modules.adaptive.targ_dim->target_.dim()
A:torch.nn.modules.adaptive.batch_size->target.size(0)
A:torch.nn.modules.adaptive.output->torch.argmax(head_output, dim=1)
A:torch.nn.modules.adaptive.gather_inds->target.new_empty(batch_size)
A:torch.nn.modules.adaptive.row_indices->target_mask.nonzero().squeeze()
A:torch.nn.modules.adaptive.input_subset->input.index_select(0, row_indices)
A:torch.nn.modules.adaptive.cluster_output->self.tail[i](input)
A:torch.nn.modules.adaptive.cluster_logprob->log_softmax(cluster_output, dim=1)
A:torch.nn.modules.adaptive.local_logprob->log_softmax(cluster_output, dim=1).gather(1, relative_target.unsqueeze(1))
A:torch.nn.modules.adaptive.head_output->self.head(input)
A:torch.nn.modules.adaptive.head_logprob->log_softmax(head_output, dim=1)
A:torch.nn.modules.adaptive.loss->(-output).mean()
A:torch.nn.modules.adaptive.out->input.new_empty((head_output.size(0), self.n_classes))
A:torch.nn.modules.adaptive.log_prob->self._get_full_log_prob(input[not_in_shortlist], head_output[not_in_shortlist])
A:torch.nn.modules.adaptive.output[not_in_shortlist]->torch.argmax(log_prob, dim=1)
torch.nn.AdaptiveLogSoftmaxWithLoss(self,in_features:int,n_classes:int,cutoffs:Sequence[int],div_value:float=4.0,head_bias:bool=False,device=None,dtype=None)
torch.nn.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.AdaptiveLogSoftmaxWithLoss.forward(self,input_:Tensor,target_:Tensor)->_ASMoutput
torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob(self,input:Tensor)->Tensor
torch.nn.AdaptiveLogSoftmaxWithLoss.predict(self,input:Tensor)->Tensor
torch.nn.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)->None
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss(self,in_features:int,n_classes:int,cutoffs:Sequence[int],div_value:float=4.0,head_bias:bool=False,device=None,dtype=None)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.__init__(self,in_features:int,n_classes:int,cutoffs:Sequence[int],div_value:float=4.0,head_bias:bool=False,device=None,dtype=None)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.forward(self,input_:Tensor,target_:Tensor)->_ASMoutput
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.log_prob(self,input:Tensor)->Tensor
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.predict(self,input:Tensor)->Tensor
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/batchnorm.py----------------------------------------
A:torch.nn.modules.batchnorm.self.weight->UninitializedParameter(**factory_kwargs)
A:torch.nn.modules.batchnorm.self.bias->UninitializedParameter(**factory_kwargs)
A:torch.nn.modules.batchnorm.version->local_metadata.get('version', None)
A:torch.nn.modules.batchnorm.state_dict[num_batches_tracked_key]->torch.tensor(0, dtype=torch.long)
A:torch.nn.modules.batchnorm.self.running_mean->UninitializedBuffer(**factory_kwargs)
A:torch.nn.modules.batchnorm.self.running_var->UninitializedBuffer(**factory_kwargs)
A:torch.nn.modules.batchnorm.self.num_batches_tracked->torch.tensor(0, dtype=torch.long, **{k: v for (k, v) in factory_kwargs.items() if k != 'dtype'})
A:torch.nn.modules.batchnorm.world_size->torch.distributed.get_world_size(process_group)
A:torch.nn.modules.batchnorm.module_output->torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group)
torch.nn.BatchNorm1d(_BatchNorm)
torch.nn.BatchNorm1d._check_input_dim(self,input)
torch.nn.BatchNorm2d(_BatchNorm)
torch.nn.BatchNorm2d._check_input_dim(self,input)
torch.nn.BatchNorm3d(_BatchNorm)
torch.nn.BatchNorm3d._check_input_dim(self,input)
torch.nn.LazyBatchNorm1d(_LazyNormBase,_BatchNorm)
torch.nn.LazyBatchNorm1d._check_input_dim(self,input)
torch.nn.LazyBatchNorm2d(_LazyNormBase,_BatchNorm)
torch.nn.LazyBatchNorm2d._check_input_dim(self,input)
torch.nn.LazyBatchNorm3d(_LazyNormBase,_BatchNorm)
torch.nn.LazyBatchNorm3d._check_input_dim(self,input)
torch.nn.SyncBatchNorm(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,process_group:Optional[Any]=None,device=None,dtype=None)
torch.nn.SyncBatchNorm._check_input_dim(self,input)
torch.nn.SyncBatchNorm._check_non_zero_input_channels(self,input)
torch.nn.SyncBatchNorm.convert_sync_batchnorm(cls,module,process_group=None)
torch.nn.SyncBatchNorm.forward(self,input:Tensor)->Tensor
torch.nn.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,device=None,dtype=None)
torch.nn.batchnorm._BatchNorm.forward(self,input:Tensor)->Tensor
torch.nn.batchnorm._LazyNormBase(self,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,device=None,dtype=None)
torch.nn.batchnorm._LazyNormBase.initialize_parameters(self,input)->None
torch.nn.batchnorm._LazyNormBase.reset_parameters(self)->None
torch.nn.batchnorm._NormBase(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,device=None,dtype=None)
torch.nn.batchnorm._NormBase._check_input_dim(self,input)
torch.nn.batchnorm._NormBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.batchnorm._NormBase.extra_repr(self)
torch.nn.batchnorm._NormBase.reset_parameters(self)->None
torch.nn.batchnorm._NormBase.reset_running_stats(self)->None
torch.nn.modules.batchnorm.BatchNorm1d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm1d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm2d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm2d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm3d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm3d._check_input_dim(self,input)
torch.nn.modules.batchnorm.LazyBatchNorm1d(_LazyNormBase,_BatchNorm)
torch.nn.modules.batchnorm.LazyBatchNorm1d._check_input_dim(self,input)
torch.nn.modules.batchnorm.LazyBatchNorm2d(_LazyNormBase,_BatchNorm)
torch.nn.modules.batchnorm.LazyBatchNorm2d._check_input_dim(self,input)
torch.nn.modules.batchnorm.LazyBatchNorm3d(_LazyNormBase,_BatchNorm)
torch.nn.modules.batchnorm.LazyBatchNorm3d._check_input_dim(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,process_group:Optional[Any]=None,device=None,dtype=None)
torch.nn.modules.batchnorm.SyncBatchNorm.__init__(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,process_group:Optional[Any]=None,device=None,dtype=None)
torch.nn.modules.batchnorm.SyncBatchNorm._check_input_dim(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm._check_non_zero_input_channels(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm.convert_sync_batchnorm(cls,module,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm.forward(self,input:Tensor)->Tensor
torch.nn.modules.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,device=None,dtype=None)
torch.nn.modules.batchnorm._BatchNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,device=None,dtype=None)
torch.nn.modules.batchnorm._BatchNorm.forward(self,input:Tensor)->Tensor
torch.nn.modules.batchnorm._LazyNormBase(self,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,device=None,dtype=None)
torch.nn.modules.batchnorm._LazyNormBase.__init__(self,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,device=None,dtype=None)
torch.nn.modules.batchnorm._LazyNormBase.initialize_parameters(self,input)->None
torch.nn.modules.batchnorm._LazyNormBase.reset_parameters(self)->None
torch.nn.modules.batchnorm._NormBase(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,device=None,dtype=None)
torch.nn.modules.batchnorm._NormBase.__init__(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,device=None,dtype=None)
torch.nn.modules.batchnorm._NormBase._check_input_dim(self,input)
torch.nn.modules.batchnorm._NormBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.batchnorm._NormBase.extra_repr(self)
torch.nn.modules.batchnorm._NormBase.reset_parameters(self)->None
torch.nn.modules.batchnorm._NormBase.reset_running_stats(self)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/padding.py----------------------------------------
A:torch.nn.modules.padding.self.padding->_ntuple(6)(padding)
torch.nn.ConstantPad1d(self,padding:_size_2_t,value:float)
torch.nn.ConstantPad2d(self,padding:_size_4_t,value:float)
torch.nn.ConstantPad3d(self,padding:_size_6_t,value:float)
torch.nn.ReflectionPad1d(self,padding:_size_2_t)
torch.nn.ReflectionPad2d(self,padding:_size_4_t)
torch.nn.ReflectionPad3d(self,padding:_size_6_t)
torch.nn.ReplicationPad1d(self,padding:_size_2_t)
torch.nn.ReplicationPad2d(self,padding:_size_4_t)
torch.nn.ReplicationPad3d(self,padding:_size_6_t)
torch.nn.ZeroPad2d(self,padding:_size_4_t)
torch.nn.ZeroPad2d.extra_repr(self)->str
torch.nn.modules.padding.ConstantPad1d(self,padding:_size_2_t,value:float)
torch.nn.modules.padding.ConstantPad1d.__init__(self,padding:_size_2_t,value:float)
torch.nn.modules.padding.ConstantPad2d(self,padding:_size_4_t,value:float)
torch.nn.modules.padding.ConstantPad2d.__init__(self,padding:_size_4_t,value:float)
torch.nn.modules.padding.ConstantPad3d(self,padding:_size_6_t,value:float)
torch.nn.modules.padding.ConstantPad3d.__init__(self,padding:_size_6_t,value:float)
torch.nn.modules.padding.ReflectionPad1d(self,padding:_size_2_t)
torch.nn.modules.padding.ReflectionPad1d.__init__(self,padding:_size_2_t)
torch.nn.modules.padding.ReflectionPad2d(self,padding:_size_4_t)
torch.nn.modules.padding.ReflectionPad2d.__init__(self,padding:_size_4_t)
torch.nn.modules.padding.ReflectionPad3d(self,padding:_size_6_t)
torch.nn.modules.padding.ReflectionPad3d.__init__(self,padding:_size_6_t)
torch.nn.modules.padding.ReplicationPad1d(self,padding:_size_2_t)
torch.nn.modules.padding.ReplicationPad1d.__init__(self,padding:_size_2_t)
torch.nn.modules.padding.ReplicationPad2d(self,padding:_size_4_t)
torch.nn.modules.padding.ReplicationPad2d.__init__(self,padding:_size_4_t)
torch.nn.modules.padding.ReplicationPad3d(self,padding:_size_6_t)
torch.nn.modules.padding.ReplicationPad3d.__init__(self,padding:_size_6_t)
torch.nn.modules.padding.ZeroPad2d(self,padding:_size_4_t)
torch.nn.modules.padding.ZeroPad2d.__init__(self,padding:_size_4_t)
torch.nn.modules.padding.ZeroPad2d.extra_repr(self)->str
torch.nn.modules.padding._ConstantPadNd(self,value:float)
torch.nn.modules.padding._ConstantPadNd.__init__(self,value:float)
torch.nn.modules.padding._ConstantPadNd.extra_repr(self)->str
torch.nn.modules.padding._ConstantPadNd.forward(self,input:Tensor)->Tensor
torch.nn.modules.padding._ReflectionPadNd(Module)
torch.nn.modules.padding._ReflectionPadNd.extra_repr(self)->str
torch.nn.modules.padding._ReflectionPadNd.forward(self,input:Tensor)->Tensor
torch.nn.modules.padding._ReplicationPadNd(Module)
torch.nn.modules.padding._ReplicationPadNd.extra_repr(self)->str
torch.nn.modules.padding._ReplicationPadNd.forward(self,input:Tensor)->Tensor
torch.nn.padding._ConstantPadNd(self,value:float)
torch.nn.padding._ConstantPadNd.extra_repr(self)->str
torch.nn.padding._ConstantPadNd.forward(self,input:Tensor)->Tensor
torch.nn.padding._ReflectionPadNd(Module)
torch.nn.padding._ReflectionPadNd.extra_repr(self)->str
torch.nn.padding._ReflectionPadNd.forward(self,input:Tensor)->Tensor
torch.nn.padding._ReplicationPadNd(Module)
torch.nn.padding._ReplicationPadNd.extra_repr(self)->str
torch.nn.padding._ReplicationPadNd.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/fold.py----------------------------------------
torch.nn.Fold(self,output_size:_size_any_t,kernel_size:_size_any_t,dilation:_size_any_t=1,padding:_size_any_t=0,stride:_size_any_t=1)
torch.nn.Fold.extra_repr(self)->str
torch.nn.Fold.forward(self,input:Tensor)->Tensor
torch.nn.Unfold(self,kernel_size:_size_any_t,dilation:_size_any_t=1,padding:_size_any_t=0,stride:_size_any_t=1)
torch.nn.Unfold.extra_repr(self)->str
torch.nn.Unfold.forward(self,input:Tensor)->Tensor
torch.nn.modules.fold.Fold(self,output_size:_size_any_t,kernel_size:_size_any_t,dilation:_size_any_t=1,padding:_size_any_t=0,stride:_size_any_t=1)
torch.nn.modules.fold.Fold.__init__(self,output_size:_size_any_t,kernel_size:_size_any_t,dilation:_size_any_t=1,padding:_size_any_t=0,stride:_size_any_t=1)
torch.nn.modules.fold.Fold.extra_repr(self)->str
torch.nn.modules.fold.Fold.forward(self,input:Tensor)->Tensor
torch.nn.modules.fold.Unfold(self,kernel_size:_size_any_t,dilation:_size_any_t=1,padding:_size_any_t=0,stride:_size_any_t=1)
torch.nn.modules.fold.Unfold.__init__(self,kernel_size:_size_any_t,dilation:_size_any_t=1,padding:_size_any_t=0,stride:_size_any_t=1)
torch.nn.modules.fold.Unfold.extra_repr(self)->str
torch.nn.modules.fold.Unfold.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/instancenorm.py----------------------------------------
A:torch.nn.modules.instancenorm.version->local_metadata.get('version', None)
torch.nn.InstanceNorm1d(_InstanceNorm)
torch.nn.InstanceNorm1d._check_input_dim(self,input)
torch.nn.InstanceNorm1d._get_no_batch_dim(self)
torch.nn.InstanceNorm2d(_InstanceNorm)
torch.nn.InstanceNorm2d._check_input_dim(self,input)
torch.nn.InstanceNorm2d._get_no_batch_dim(self)
torch.nn.InstanceNorm3d(_InstanceNorm)
torch.nn.InstanceNorm3d._check_input_dim(self,input)
torch.nn.InstanceNorm3d._get_no_batch_dim(self)
torch.nn.LazyInstanceNorm1d(_LazyNormBase,_InstanceNorm)
torch.nn.LazyInstanceNorm1d._check_input_dim(self,input)
torch.nn.LazyInstanceNorm1d._get_no_batch_dim(self)
torch.nn.LazyInstanceNorm2d(_LazyNormBase,_InstanceNorm)
torch.nn.LazyInstanceNorm2d._check_input_dim(self,input)
torch.nn.LazyInstanceNorm2d._get_no_batch_dim(self)
torch.nn.LazyInstanceNorm3d(_LazyNormBase,_InstanceNorm)
torch.nn.LazyInstanceNorm3d._check_input_dim(self,input)
torch.nn.LazyInstanceNorm3d._get_no_batch_dim(self)
torch.nn.instancenorm._InstanceNorm(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False,device=None,dtype=None)
torch.nn.instancenorm._InstanceNorm._apply_instance_norm(self,input)
torch.nn.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm._get_no_batch_dim(self)
torch.nn.instancenorm._InstanceNorm._handle_no_batch_input(self,input)
torch.nn.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.instancenorm._InstanceNorm.forward(self,input:Tensor)->Tensor
torch.nn.modules.instancenorm.InstanceNorm1d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm1d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm1d._get_no_batch_dim(self)
torch.nn.modules.instancenorm.InstanceNorm2d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm2d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm2d._get_no_batch_dim(self)
torch.nn.modules.instancenorm.InstanceNorm3d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm3d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm3d._get_no_batch_dim(self)
torch.nn.modules.instancenorm.LazyInstanceNorm1d(_LazyNormBase,_InstanceNorm)
torch.nn.modules.instancenorm.LazyInstanceNorm1d._check_input_dim(self,input)
torch.nn.modules.instancenorm.LazyInstanceNorm1d._get_no_batch_dim(self)
torch.nn.modules.instancenorm.LazyInstanceNorm2d(_LazyNormBase,_InstanceNorm)
torch.nn.modules.instancenorm.LazyInstanceNorm2d._check_input_dim(self,input)
torch.nn.modules.instancenorm.LazyInstanceNorm2d._get_no_batch_dim(self)
torch.nn.modules.instancenorm.LazyInstanceNorm3d(_LazyNormBase,_InstanceNorm)
torch.nn.modules.instancenorm.LazyInstanceNorm3d._check_input_dim(self,input)
torch.nn.modules.instancenorm.LazyInstanceNorm3d._get_no_batch_dim(self)
torch.nn.modules.instancenorm._InstanceNorm(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False,device=None,dtype=None)
torch.nn.modules.instancenorm._InstanceNorm.__init__(self,num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False,device=None,dtype=None)
torch.nn.modules.instancenorm._InstanceNorm._apply_instance_norm(self,input)
torch.nn.modules.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm._get_no_batch_dim(self)
torch.nn.modules.instancenorm._InstanceNorm._handle_no_batch_input(self,input)
torch.nn.modules.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.instancenorm._InstanceNorm.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/channelshuffle.py----------------------------------------
torch.nn.ChannelShuffle(self,groups:int)
torch.nn.ChannelShuffle.extra_repr(self)->str
torch.nn.ChannelShuffle.forward(self,input:Tensor)->Tensor
torch.nn.modules.channelshuffle.ChannelShuffle(self,groups:int)
torch.nn.modules.channelshuffle.ChannelShuffle.__init__(self,groups:int)
torch.nn.modules.channelshuffle.ChannelShuffle.extra_repr(self)->str
torch.nn.modules.channelshuffle.ChannelShuffle.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/pooling.py----------------------------------------
A:torch.nn.modules.pooling.self.kernel_size->_triple(kernel_size)
A:torch.nn.modules.pooling.self.stride->_single(stride if stride is not None else kernel_size)
A:torch.nn.modules.pooling.self.padding->_single(padding)
torch.nn.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool1d.forward(self,input:Tensor)->Tensor
torch.nn.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool2d.forward(self,input:Tensor)->Tensor
torch.nn.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool3d.forward(self,input:Tensor)->Tensor
torch.nn.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool1d.forward(self,input:Tensor)->Tensor
torch.nn.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool2d.forward(self,input:Tensor)
torch.nn.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool3d.forward(self,input:Tensor)
torch.nn.AvgPool1d(self,kernel_size:_size_1_t,stride:_size_1_t=None,padding:_size_1_t=0,ceil_mode:bool=False,count_include_pad:bool=True)
torch.nn.AvgPool1d.forward(self,input:Tensor)->Tensor
torch.nn.AvgPool2d(self,kernel_size:_size_2_t,stride:Optional[_size_2_t]=None,padding:_size_2_t=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:Optional[int]=None)
torch.nn.AvgPool2d.forward(self,input:Tensor)->Tensor
torch.nn.AvgPool3d(self,kernel_size:_size_3_t,stride:Optional[_size_3_t]=None,padding:_size_3_t=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:Optional[int]=None)
torch.nn.AvgPool3d.__setstate__(self,d)
torch.nn.AvgPool3d.forward(self,input:Tensor)->Tensor
torch.nn.FractionalMaxPool2d(self,kernel_size:_size_2_t,output_size:Optional[_size_2_t]=None,output_ratio:Optional[_ratio_2_t]=None,return_indices:bool=False,_random_samples=None)
torch.nn.FractionalMaxPool2d.forward(self,input:Tensor)
torch.nn.FractionalMaxPool3d(self,kernel_size:_size_3_t,output_size:Optional[_size_3_t]=None,output_ratio:Optional[_ratio_3_t]=None,return_indices:bool=False,_random_samples=None)
torch.nn.FractionalMaxPool3d.forward(self,input:Tensor)
torch.nn.LPPool1d(_LPPoolNd)
torch.nn.LPPool1d.forward(self,input:Tensor)->Tensor
torch.nn.LPPool2d(_LPPoolNd)
torch.nn.LPPool2d.forward(self,input:Tensor)->Tensor
torch.nn.MaxPool1d(_MaxPoolNd)
torch.nn.MaxPool1d.forward(self,input:Tensor)
torch.nn.MaxPool2d(_MaxPoolNd)
torch.nn.MaxPool2d.forward(self,input:Tensor)
torch.nn.MaxPool3d(_MaxPoolNd)
torch.nn.MaxPool3d.forward(self,input:Tensor)
torch.nn.MaxUnpool1d(self,kernel_size:_size_1_t,stride:Optional[_size_1_t]=None,padding:_size_1_t=0)
torch.nn.MaxUnpool1d.forward(self,input:Tensor,indices:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.MaxUnpool2d(self,kernel_size:_size_2_t,stride:Optional[_size_2_t]=None,padding:_size_2_t=0)
torch.nn.MaxUnpool2d.forward(self,input:Tensor,indices:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.MaxUnpool3d(self,kernel_size:_size_3_t,stride:Optional[_size_3_t]=None,padding:_size_3_t=0)
torch.nn.MaxUnpool3d.forward(self,input:Tensor,indices:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.pooling.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool1d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool3d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool1d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool2d.forward(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool3d.forward(self,input:Tensor)
torch.nn.modules.pooling.AvgPool1d(self,kernel_size:_size_1_t,stride:_size_1_t=None,padding:_size_1_t=0,ceil_mode:bool=False,count_include_pad:bool=True)
torch.nn.modules.pooling.AvgPool1d.__init__(self,kernel_size:_size_1_t,stride:_size_1_t=None,padding:_size_1_t=0,ceil_mode:bool=False,count_include_pad:bool=True)
torch.nn.modules.pooling.AvgPool1d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.AvgPool2d(self,kernel_size:_size_2_t,stride:Optional[_size_2_t]=None,padding:_size_2_t=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:Optional[int]=None)
torch.nn.modules.pooling.AvgPool2d.__init__(self,kernel_size:_size_2_t,stride:Optional[_size_2_t]=None,padding:_size_2_t=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:Optional[int]=None)
torch.nn.modules.pooling.AvgPool2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.AvgPool3d(self,kernel_size:_size_3_t,stride:Optional[_size_3_t]=None,padding:_size_3_t=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:Optional[int]=None)
torch.nn.modules.pooling.AvgPool3d.__init__(self,kernel_size:_size_3_t,stride:Optional[_size_3_t]=None,padding:_size_3_t=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:Optional[int]=None)
torch.nn.modules.pooling.AvgPool3d.__setstate__(self,d)
torch.nn.modules.pooling.AvgPool3d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.FractionalMaxPool2d(self,kernel_size:_size_2_t,output_size:Optional[_size_2_t]=None,output_ratio:Optional[_ratio_2_t]=None,return_indices:bool=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.__init__(self,kernel_size:_size_2_t,output_size:Optional[_size_2_t]=None,output_ratio:Optional[_ratio_2_t]=None,return_indices:bool=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.forward(self,input:Tensor)
torch.nn.modules.pooling.FractionalMaxPool3d(self,kernel_size:_size_3_t,output_size:Optional[_size_3_t]=None,output_ratio:Optional[_ratio_3_t]=None,return_indices:bool=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool3d.__init__(self,kernel_size:_size_3_t,output_size:Optional[_size_3_t]=None,output_ratio:Optional[_ratio_3_t]=None,return_indices:bool=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool3d.forward(self,input:Tensor)
torch.nn.modules.pooling.LPPool1d(_LPPoolNd)
torch.nn.modules.pooling.LPPool1d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.LPPool2d(_LPPoolNd)
torch.nn.modules.pooling.LPPool2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.pooling.MaxPool1d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool1d.forward(self,input:Tensor)
torch.nn.modules.pooling.MaxPool2d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool2d.forward(self,input:Tensor)
torch.nn.modules.pooling.MaxPool3d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool3d.forward(self,input:Tensor)
torch.nn.modules.pooling.MaxUnpool1d(self,kernel_size:_size_1_t,stride:Optional[_size_1_t]=None,padding:_size_1_t=0)
torch.nn.modules.pooling.MaxUnpool1d.__init__(self,kernel_size:_size_1_t,stride:Optional[_size_1_t]=None,padding:_size_1_t=0)
torch.nn.modules.pooling.MaxUnpool1d.forward(self,input:Tensor,indices:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.pooling.MaxUnpool2d(self,kernel_size:_size_2_t,stride:Optional[_size_2_t]=None,padding:_size_2_t=0)
torch.nn.modules.pooling.MaxUnpool2d.__init__(self,kernel_size:_size_2_t,stride:Optional[_size_2_t]=None,padding:_size_2_t=0)
torch.nn.modules.pooling.MaxUnpool2d.forward(self,input:Tensor,indices:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.pooling.MaxUnpool3d(self,kernel_size:_size_3_t,stride:Optional[_size_3_t]=None,padding:_size_3_t=0)
torch.nn.modules.pooling.MaxUnpool3d.__init__(self,kernel_size:_size_3_t,stride:Optional[_size_3_t]=None,padding:_size_3_t=0)
torch.nn.modules.pooling.MaxUnpool3d.forward(self,input:Tensor,indices:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.pooling._AdaptiveAvgPoolNd(self,output_size:_size_any_opt_t)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.__init__(self,output_size:_size_any_opt_t)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.extra_repr(self)->str
torch.nn.modules.pooling._AdaptiveMaxPoolNd(self,output_size:_size_any_opt_t,return_indices:bool=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.__init__(self,output_size:_size_any_opt_t,return_indices:bool=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.extra_repr(self)->str
torch.nn.modules.pooling._AvgPoolNd(Module)
torch.nn.modules.pooling._AvgPoolNd.extra_repr(self)->str
torch.nn.modules.pooling._LPPoolNd(self,norm_type:float,kernel_size:_size_any_t,stride:Optional[_size_any_t]=None,ceil_mode:bool=False)
torch.nn.modules.pooling._LPPoolNd.__init__(self,norm_type:float,kernel_size:_size_any_t,stride:Optional[_size_any_t]=None,ceil_mode:bool=False)
torch.nn.modules.pooling._LPPoolNd.extra_repr(self)->str
torch.nn.modules.pooling._MaxPoolNd(self,kernel_size:_size_any_t,stride:Optional[_size_any_t]=None,padding:_size_any_t=0,dilation:_size_any_t=1,return_indices:bool=False,ceil_mode:bool=False)
torch.nn.modules.pooling._MaxPoolNd.__init__(self,kernel_size:_size_any_t,stride:Optional[_size_any_t]=None,padding:_size_any_t=0,dilation:_size_any_t=1,return_indices:bool=False,ceil_mode:bool=False)
torch.nn.modules.pooling._MaxPoolNd.extra_repr(self)->str
torch.nn.modules.pooling._MaxUnpoolNd(Module)
torch.nn.modules.pooling._MaxUnpoolNd.extra_repr(self)->str
torch.nn.pooling._AdaptiveAvgPoolNd(self,output_size:_size_any_opt_t)
torch.nn.pooling._AdaptiveAvgPoolNd.extra_repr(self)->str
torch.nn.pooling._AdaptiveMaxPoolNd(self,output_size:_size_any_opt_t,return_indices:bool=False)
torch.nn.pooling._AdaptiveMaxPoolNd.extra_repr(self)->str
torch.nn.pooling._AvgPoolNd(Module)
torch.nn.pooling._AvgPoolNd.extra_repr(self)->str
torch.nn.pooling._LPPoolNd(self,norm_type:float,kernel_size:_size_any_t,stride:Optional[_size_any_t]=None,ceil_mode:bool=False)
torch.nn.pooling._LPPoolNd.extra_repr(self)->str
torch.nn.pooling._MaxPoolNd(self,kernel_size:_size_any_t,stride:Optional[_size_any_t]=None,padding:_size_any_t=0,dilation:_size_any_t=1,return_indices:bool=False,ceil_mode:bool=False)
torch.nn.pooling._MaxPoolNd.extra_repr(self)->str
torch.nn.pooling._MaxUnpoolNd(Module)
torch.nn.pooling._MaxUnpoolNd.extra_repr(self)->str


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/_functions.py----------------------------------------
A:torch.nn.modules._functions.input->input.contiguous().contiguous()
A:torch.nn.modules._functions.weight->weight.contiguous().contiguous()
A:torch.nn.modules._functions.size->int(input.numel() // input.size(1))
A:torch.nn.modules._functions.(mean, invstd)->torch.batch_norm_gather_stats_with_counts(input, mean_all, invstd_all, running_mean, running_var, momentum, eps, count_all.view(-1))
A:torch.nn.modules._functions.count->torch.full((1,), input.numel() // input.size(1), dtype=mean.dtype, device=mean.device)
A:torch.nn.modules._functions.combined->torch.cat([sum_dy, sum_dy_xmu], dim=0)
A:torch.nn.modules._functions.combined_size->torch.cat([sum_dy, sum_dy_xmu], dim=0).numel()
A:torch.nn.modules._functions.combined_flat->torch.empty(1, combined_size * world_size, dtype=combined.dtype, device=combined.device)
A:torch.nn.modules._functions.(mean_all, invstd_all, count_all)->torch.split(combined, num_channels, dim=1)
A:torch.nn.modules._functions.out->torch.batch_norm_elemt(input, weight, bias, mean, invstd, eps)
A:torch.nn.modules._functions.grad_output->grad_output.contiguous().contiguous()
A:torch.nn.modules._functions.(sum_dy, sum_dy_xmu, grad_weight, grad_bias)->torch.batch_norm_backward_reduce(grad_output, saved_input, mean, invstd, weight, self.needs_input_grad[0], self.needs_input_grad[1], self.needs_input_grad[2])
A:torch.nn.modules._functions.(sum_dy, sum_dy_xmu)->torch.split(combined, num_channels)
A:torch.nn.modules._functions.grad_input->grad_output.contiguous().contiguous().new()
A:torch.nn.modules._functions.output->input.contiguous().contiguous().new()
A:torch.nn.modules._functions.batch_size->input.contiguous().contiguous().size(0)
A:torch.nn.modules._functions.channels->input.contiguous().contiguous().size(1)
A:torch.nn.modules._functions.input_height->input.contiguous().contiguous().size(2)
A:torch.nn.modules._functions.input_width->input.contiguous().contiguous().size(3)
A:torch.nn.modules._functions.pre_pad->int((ctx.size - 1) / 2 + 1)
A:torch.nn.modules._functions.scale_first->ctx.scale.select(1, 0)
A:torch.nn.modules._functions.scale_previous->ctx.scale.select(1, c - 1)
A:torch.nn.modules._functions.scale_current->ctx.scale.select(1, c)
A:torch.nn.modules._functions.square_next->input_square.select(1, c + pre_pad - 1)
A:torch.nn.modules._functions.square_previous->input_square.select(1, c - pre_pad)
A:torch.nn.modules._functions.paddded_ratio->input.contiguous().contiguous().new(channels + ctx.size - 1, input_height, input_width)
A:torch.nn.modules._functions.accum_ratio->input.contiguous().contiguous().new(input_height, input_width)
A:torch.nn.modules._functions.inversePrePad->int(ctx.size - (ctx.size - 1) / 2)
A:torch.nn.modules._functions.padded_ratio_center->input.contiguous().contiguous().new(channels + ctx.size - 1, input_height, input_width).narrow(0, inversePrePad, channels)
torch.nn._functions.BackwardHookFunction(torch.autograd.Function)
torch.nn._functions.BackwardHookFunction.backward(ctx,*args)
torch.nn._functions.BackwardHookFunction.forward(ctx,*args)
torch.nn._functions.CrossMapLRN2d(Function)
torch.nn._functions.CrossMapLRN2d.backward(ctx,grad_output)
torch.nn._functions.CrossMapLRN2d.forward(ctx,input,size,alpha=0.0001,beta=0.75,k=1)
torch.nn._functions.SyncBatchNorm(Function)
torch.nn._functions.SyncBatchNorm.backward(self,grad_output)
torch.nn._functions.SyncBatchNorm.forward(self,input,weight,bias,running_mean,running_var,eps,momentum,process_group,world_size)
torch.nn.modules._functions.BackwardHookFunction(torch.autograd.Function)
torch.nn.modules._functions.BackwardHookFunction.backward(ctx,*args)
torch.nn.modules._functions.BackwardHookFunction.forward(ctx,*args)
torch.nn.modules._functions.CrossMapLRN2d(Function)
torch.nn.modules._functions.CrossMapLRN2d.backward(ctx,grad_output)
torch.nn.modules._functions.CrossMapLRN2d.forward(ctx,input,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules._functions.SyncBatchNorm(Function)
torch.nn.modules._functions.SyncBatchNorm.backward(self,grad_output)
torch.nn.modules._functions.SyncBatchNorm.forward(self,input,weight,bias,running_mean,running_var,eps,momentum,process_group,world_size)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/rnn.py----------------------------------------
A:torch.nn.modules.rnn.self.dropout->float(dropout)
A:torch.nn.modules.rnn.w_ih->Parameter(torch.empty((gate_size, layer_input_size), **factory_kwargs))
A:torch.nn.modules.rnn.w_hh->Parameter(torch.empty((gate_size, real_hidden_size), **factory_kwargs))
A:torch.nn.modules.rnn.b_ih->Parameter(torch.empty(gate_size, **factory_kwargs))
A:torch.nn.modules.rnn.b_hh->Parameter(torch.empty(gate_size, **factory_kwargs))
A:torch.nn.modules.rnn.w_hr->Parameter(torch.empty((proj_size, hidden_size), **factory_kwargs))
A:torch.nn.modules.rnn.idx->self._flat_weights_names.index(attr)
A:torch.nn.modules.rnn.unique_data_ptrs->set((p.data_ptr() for p in self._flat_weights))
A:torch.nn.modules.rnn.ret->ret.squeeze(0).squeeze(0)
A:torch.nn.modules.rnn.mini_batch->int(batch_sizes[0])
A:torch.nn.modules.rnn.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.nn.modules.rnn.replica->super(RNNBase, self)._replicate_for_data_parallel()
A:torch.nn.modules.rnn.self.nonlinearity->kwargs.pop('nonlinearity', 'tanh')
A:torch.nn.modules.rnn.max_batch_size->int(max_batch_size)
A:torch.nn.modules.rnn.input->input.unsqueeze(0).unsqueeze(0)
A:torch.nn.modules.rnn.hx->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.result->_VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias, self.num_layers, self.dropout, self.training, self.bidirectional)
A:torch.nn.modules.rnn.output_packed->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.modules.rnn.output->output.squeeze(batch_dim).squeeze(batch_dim)
A:torch.nn.modules.rnn.hidden->hidden.squeeze(1).squeeze(1)
A:torch.nn.modules.rnn.h_zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, real_hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.c_zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.self.weight_ih->Parameter(torch.empty((num_chunks * hidden_size, input_size), **factory_kwargs))
A:torch.nn.modules.rnn.self.weight_hh->Parameter(torch.empty((num_chunks * hidden_size, hidden_size), **factory_kwargs))
A:torch.nn.modules.rnn.self.bias_ih->Parameter(torch.empty(num_chunks * hidden_size, **factory_kwargs))
A:torch.nn.modules.rnn.self.bias_hh->Parameter(torch.empty(num_chunks * hidden_size, **factory_kwargs))
A:torch.nn.modules.rnn.zeros->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
torch.nn.GRU(self,*args,**kwargs)
torch.nn.GRU.forward(self,input,hx=None)
torch.nn.GRUCell(self,input_size:int,hidden_size:int,bias:bool=True,device=None,dtype=None)
torch.nn.GRUCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.LSTM(self,*args,**kwargs)
torch.nn.LSTM.check_forward_args(self,input:Tensor,hidden:Tuple[Tensor,Tensor],batch_sizes:Optional[Tensor])
torch.nn.LSTM.forward(self,input,hx=None)
torch.nn.LSTM.get_expected_cell_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.nn.LSTM.permute_hidden(self,hx:Tuple[Tensor,Tensor],permutation:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.nn.LSTMCell(self,input_size:int,hidden_size:int,bias:bool=True,device=None,dtype=None)
torch.nn.LSTMCell.forward(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.nn.RNN(self,*args,**kwargs)
torch.nn.RNN.forward(self,input,hx=None)
torch.nn.RNNBase(self,mode:str,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False,proj_size:int=0,device=None,dtype=None)
torch.nn.RNNBase.__setattr__(self,attr,value)
torch.nn.RNNBase.__setstate__(self,d)
torch.nn.RNNBase._apply(self,fn)
torch.nn.RNNBase._replicate_for_data_parallel(self)
torch.nn.RNNBase.all_weights(self)->List[List[Parameter]]
torch.nn.RNNBase.check_forward_args(self,input:Tensor,hidden:Tensor,batch_sizes:Optional[Tensor])
torch.nn.RNNBase.check_hidden_size(self,hx:Tensor,expected_hidden_size:Tuple[int,int,int],msg:str='Expectedhiddensize{},got{}')->None
torch.nn.RNNBase.check_input(self,input:Tensor,batch_sizes:Optional[Tensor])->None
torch.nn.RNNBase.extra_repr(self)->str
torch.nn.RNNBase.flatten_parameters(self)->None
torch.nn.RNNBase.get_expected_hidden_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.nn.RNNBase.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])
torch.nn.RNNBase.reset_parameters(self)->None
torch.nn.RNNCell(self,input_size:int,hidden_size:int,bias:bool=True,nonlinearity:str='tanh',device=None,dtype=None)
torch.nn.RNNCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.RNNCellBase(self,input_size:int,hidden_size:int,bias:bool,num_chunks:int,device=None,dtype=None)
torch.nn.RNNCellBase.extra_repr(self)->str
torch.nn.RNNCellBase.reset_parameters(self)->None
torch.nn.modules.rnn.GRU(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.forward(self,input,hx=None)
torch.nn.modules.rnn.GRUCell(self,input_size:int,hidden_size:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.rnn.GRUCell.__init__(self,input_size:int,hidden_size:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.rnn.GRUCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.check_forward_args(self,input:Tensor,hidden:Tuple[Tensor,Tensor],batch_sizes:Optional[Tensor])
torch.nn.modules.rnn.LSTM.forward(self,input,hx=None)
torch.nn.modules.rnn.LSTM.get_expected_cell_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.nn.modules.rnn.LSTM.permute_hidden(self,hx:Tuple[Tensor,Tensor],permutation:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.nn.modules.rnn.LSTMCell(self,input_size:int,hidden_size:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.rnn.LSTMCell.__init__(self,input_size:int,hidden_size:int,bias:bool=True,device=None,dtype=None)
torch.nn.modules.rnn.LSTMCell.forward(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.nn.modules.rnn.RNN(self,*args,**kwargs)
torch.nn.modules.rnn.RNN.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.RNN.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNBase(self,mode:str,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False,proj_size:int=0,device=None,dtype=None)
torch.nn.modules.rnn.RNNBase.__init__(self,mode:str,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False,proj_size:int=0,device=None,dtype=None)
torch.nn.modules.rnn.RNNBase.__setattr__(self,attr,value)
torch.nn.modules.rnn.RNNBase.__setstate__(self,d)
torch.nn.modules.rnn.RNNBase._apply(self,fn)
torch.nn.modules.rnn.RNNBase._replicate_for_data_parallel(self)
torch.nn.modules.rnn.RNNBase.all_weights(self)->List[List[Parameter]]
torch.nn.modules.rnn.RNNBase.check_forward_args(self,input:Tensor,hidden:Tensor,batch_sizes:Optional[Tensor])
torch.nn.modules.rnn.RNNBase.check_hidden_size(self,hx:Tensor,expected_hidden_size:Tuple[int,int,int],msg:str='Expectedhiddensize{},got{}')->None
torch.nn.modules.rnn.RNNBase.check_input(self,input:Tensor,batch_sizes:Optional[Tensor])->None
torch.nn.modules.rnn.RNNBase.extra_repr(self)->str
torch.nn.modules.rnn.RNNBase.flatten_parameters(self)->None
torch.nn.modules.rnn.RNNBase.get_expected_hidden_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.nn.modules.rnn.RNNBase.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])
torch.nn.modules.rnn.RNNBase.reset_parameters(self)->None
torch.nn.modules.rnn.RNNCell(self,input_size:int,hidden_size:int,bias:bool=True,nonlinearity:str='tanh',device=None,dtype=None)
torch.nn.modules.rnn.RNNCell.__init__(self,input_size:int,hidden_size:int,bias:bool=True,nonlinearity:str='tanh',device=None,dtype=None)
torch.nn.modules.rnn.RNNCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.modules.rnn.RNNCellBase(self,input_size:int,hidden_size:int,bias:bool,num_chunks:int,device=None,dtype=None)
torch.nn.modules.rnn.RNNCellBase.__init__(self,input_size:int,hidden_size:int,bias:bool,num_chunks:int,device=None,dtype=None)
torch.nn.modules.rnn.RNNCellBase.extra_repr(self)->str
torch.nn.modules.rnn.RNNCellBase.reset_parameters(self)->None
torch.nn.modules.rnn.apply_permutation(tensor:Tensor,permutation:Tensor,dim:int=1)->Tensor
torch.nn.rnn.apply_permutation(tensor:Tensor,permutation:Tensor,dim:int=1)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/loss.py----------------------------------------
torch.nn.BCELoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.BCELoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.BCEWithLogitsLoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean',pos_weight:Optional[Tensor]=None)
torch.nn.BCEWithLogitsLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.CTCLoss(self,blank:int=0,reduction:str='mean',zero_infinity:bool=False)
torch.nn.CTCLoss.forward(self,log_probs:Tensor,targets:Tensor,input_lengths:Tensor,target_lengths:Tensor)->Tensor
torch.nn.CosineEmbeddingLoss(self,margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.CosineEmbeddingLoss.forward(self,input1:Tensor,input2:Tensor,target:Tensor)->Tensor
torch.nn.CrossEntropyLoss(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean',label_smoothing:float=0.0)
torch.nn.CrossEntropyLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.GaussianNLLLoss(self,*,full:bool=False,eps:float=1e-06,reduction:str='mean')
torch.nn.GaussianNLLLoss.forward(self,input:Tensor,target:Tensor,var:Tensor)->Tensor
torch.nn.HingeEmbeddingLoss(self,margin:float=1.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.HingeEmbeddingLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.HuberLoss(self,reduction:str='mean',delta:float=1.0)
torch.nn.HuberLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.KLDivLoss(self,size_average=None,reduce=None,reduction:str='mean',log_target:bool=False)
torch.nn.KLDivLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.L1Loss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.L1Loss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.MSELoss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.MSELoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.MarginRankingLoss(self,margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.MarginRankingLoss.forward(self,input1:Tensor,input2:Tensor,target:Tensor)->Tensor
torch.nn.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.MultiLabelMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.MultiLabelSoftMarginLoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.MultiLabelSoftMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.MultiMarginLoss(self,p:int=1,margin:float=1.0,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.MultiMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.NLLLoss(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')
torch.nn.NLLLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.NLLLoss2d(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')
torch.nn.PoissonNLLLoss(self,log_input:bool=True,full:bool=False,size_average=None,eps:float=1e-08,reduce=None,reduction:str='mean')
torch.nn.PoissonNLLLoss.forward(self,log_input:Tensor,target:Tensor)->Tensor
torch.nn.SmoothL1Loss(self,size_average=None,reduce=None,reduction:str='mean',beta:float=1.0)
torch.nn.SmoothL1Loss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.SoftMarginLoss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.SoftMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.TripletMarginLoss(self,margin:float=1.0,p:float=2.0,eps:float=1e-06,swap:bool=False,size_average=None,reduce=None,reduction:str='mean')
torch.nn.TripletMarginLoss.forward(self,anchor:Tensor,positive:Tensor,negative:Tensor)->Tensor
torch.nn.TripletMarginWithDistanceLoss(self,*,distance_function:Optional[Callable[[Tensor,Tensor],Tensor]]=None,margin:float=1.0,swap:bool=False,reduction:str='mean')
torch.nn.TripletMarginWithDistanceLoss.forward(self,anchor:Tensor,positive:Tensor,negative:Tensor)->Tensor
torch.nn.loss._Loss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.loss._WeightedLoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.BCELoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.BCELoss.__init__(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.BCELoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.BCEWithLogitsLoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean',pos_weight:Optional[Tensor]=None)
torch.nn.modules.loss.BCEWithLogitsLoss.__init__(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean',pos_weight:Optional[Tensor]=None)
torch.nn.modules.loss.BCEWithLogitsLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.CTCLoss(self,blank:int=0,reduction:str='mean',zero_infinity:bool=False)
torch.nn.modules.loss.CTCLoss.__init__(self,blank:int=0,reduction:str='mean',zero_infinity:bool=False)
torch.nn.modules.loss.CTCLoss.forward(self,log_probs:Tensor,targets:Tensor,input_lengths:Tensor,target_lengths:Tensor)->Tensor
torch.nn.modules.loss.CosineEmbeddingLoss(self,margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.__init__(self,margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.forward(self,input1:Tensor,input2:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.CrossEntropyLoss(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean',label_smoothing:float=0.0)
torch.nn.modules.loss.CrossEntropyLoss.__init__(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean',label_smoothing:float=0.0)
torch.nn.modules.loss.CrossEntropyLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.GaussianNLLLoss(self,*,full:bool=False,eps:float=1e-06,reduction:str='mean')
torch.nn.modules.loss.GaussianNLLLoss.__init__(self,*,full:bool=False,eps:float=1e-06,reduction:str='mean')
torch.nn.modules.loss.GaussianNLLLoss.forward(self,input:Tensor,target:Tensor,var:Tensor)->Tensor
torch.nn.modules.loss.HingeEmbeddingLoss(self,margin:float=1.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.__init__(self,margin:float=1.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.HuberLoss(self,reduction:str='mean',delta:float=1.0)
torch.nn.modules.loss.HuberLoss.__init__(self,reduction:str='mean',delta:float=1.0)
torch.nn.modules.loss.HuberLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.KLDivLoss(self,size_average=None,reduce=None,reduction:str='mean',log_target:bool=False)
torch.nn.modules.loss.KLDivLoss.__init__(self,size_average=None,reduce=None,reduction:str='mean',log_target:bool=False)
torch.nn.modules.loss.KLDivLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.L1Loss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.L1Loss.__init__(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.L1Loss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.MSELoss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MSELoss.__init__(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MSELoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.MarginRankingLoss(self,margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MarginRankingLoss.__init__(self,margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MarginRankingLoss.forward(self,input1:Tensor,input2:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.__init__(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.MultiLabelSoftMarginLoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.__init__(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.MultiMarginLoss(self,p:int=1,margin:float=1.0,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MultiMarginLoss.__init__(self,p:int=1,margin:float=1.0,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.MultiMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.NLLLoss(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')
torch.nn.modules.loss.NLLLoss.__init__(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')
torch.nn.modules.loss.NLLLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.NLLLoss2d(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')
torch.nn.modules.loss.NLLLoss2d.__init__(self,weight:Optional[Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')
torch.nn.modules.loss.PoissonNLLLoss(self,log_input:bool=True,full:bool=False,size_average=None,eps:float=1e-08,reduce=None,reduction:str='mean')
torch.nn.modules.loss.PoissonNLLLoss.__init__(self,log_input:bool=True,full:bool=False,size_average=None,eps:float=1e-08,reduce=None,reduction:str='mean')
torch.nn.modules.loss.PoissonNLLLoss.forward(self,log_input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.SmoothL1Loss(self,size_average=None,reduce=None,reduction:str='mean',beta:float=1.0)
torch.nn.modules.loss.SmoothL1Loss.__init__(self,size_average=None,reduce=None,reduction:str='mean',beta:float=1.0)
torch.nn.modules.loss.SmoothL1Loss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.SoftMarginLoss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.SoftMarginLoss.__init__(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.SoftMarginLoss.forward(self,input:Tensor,target:Tensor)->Tensor
torch.nn.modules.loss.TripletMarginLoss(self,margin:float=1.0,p:float=2.0,eps:float=1e-06,swap:bool=False,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.TripletMarginLoss.__init__(self,margin:float=1.0,p:float=2.0,eps:float=1e-06,swap:bool=False,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss.TripletMarginLoss.forward(self,anchor:Tensor,positive:Tensor,negative:Tensor)->Tensor
torch.nn.modules.loss.TripletMarginWithDistanceLoss(self,*,distance_function:Optional[Callable[[Tensor,Tensor],Tensor]]=None,margin:float=1.0,swap:bool=False,reduction:str='mean')
torch.nn.modules.loss.TripletMarginWithDistanceLoss.__init__(self,*,distance_function:Optional[Callable[[Tensor,Tensor],Tensor]]=None,margin:float=1.0,swap:bool=False,reduction:str='mean')
torch.nn.modules.loss.TripletMarginWithDistanceLoss.forward(self,anchor:Tensor,positive:Tensor,negative:Tensor)->Tensor
torch.nn.modules.loss._Loss(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss._Loss.__init__(self,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss._WeightedLoss(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')
torch.nn.modules.loss._WeightedLoss.__init__(self,weight:Optional[Tensor]=None,size_average=None,reduce=None,reduction:str='mean')


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/normalization.py----------------------------------------
A:torch.nn.modules.normalization.self.normalized_shape->tuple(normalized_shape)
A:torch.nn.modules.normalization.self.weight->Parameter(torch.empty(num_channels, **factory_kwargs))
A:torch.nn.modules.normalization.self.bias->Parameter(torch.empty(num_channels, **factory_kwargs))
torch.nn.CrossMapLRN2d(self,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1)
torch.nn.CrossMapLRN2d.extra_repr(self)->str
torch.nn.CrossMapLRN2d.forward(self,input:Tensor)->Tensor
torch.nn.GroupNorm(self,num_groups:int,num_channels:int,eps:float=1e-05,affine:bool=True,device=None,dtype=None)
torch.nn.GroupNorm.extra_repr(self)->str
torch.nn.GroupNorm.forward(self,input:Tensor)->Tensor
torch.nn.GroupNorm.reset_parameters(self)->None
torch.nn.LayerNorm(self,normalized_shape:_shape_t,eps:float=1e-05,elementwise_affine:bool=True,device=None,dtype=None)
torch.nn.LayerNorm.extra_repr(self)->str
torch.nn.LayerNorm.forward(self,input:Tensor)->Tensor
torch.nn.LayerNorm.reset_parameters(self)->None
torch.nn.LocalResponseNorm(self,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1.0)
torch.nn.LocalResponseNorm.extra_repr(self)
torch.nn.LocalResponseNorm.forward(self,input:Tensor)->Tensor
torch.nn.modules.normalization.CrossMapLRN2d(self,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1)
torch.nn.modules.normalization.CrossMapLRN2d.__init__(self,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1)
torch.nn.modules.normalization.CrossMapLRN2d.extra_repr(self)->str
torch.nn.modules.normalization.CrossMapLRN2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.normalization.GroupNorm(self,num_groups:int,num_channels:int,eps:float=1e-05,affine:bool=True,device=None,dtype=None)
torch.nn.modules.normalization.GroupNorm.__init__(self,num_groups:int,num_channels:int,eps:float=1e-05,affine:bool=True,device=None,dtype=None)
torch.nn.modules.normalization.GroupNorm.extra_repr(self)->str
torch.nn.modules.normalization.GroupNorm.forward(self,input:Tensor)->Tensor
torch.nn.modules.normalization.GroupNorm.reset_parameters(self)->None
torch.nn.modules.normalization.LayerNorm(self,normalized_shape:_shape_t,eps:float=1e-05,elementwise_affine:bool=True,device=None,dtype=None)
torch.nn.modules.normalization.LayerNorm.__init__(self,normalized_shape:_shape_t,eps:float=1e-05,elementwise_affine:bool=True,device=None,dtype=None)
torch.nn.modules.normalization.LayerNorm.extra_repr(self)->str
torch.nn.modules.normalization.LayerNorm.forward(self,input:Tensor)->Tensor
torch.nn.modules.normalization.LayerNorm.reset_parameters(self)->None
torch.nn.modules.normalization.LocalResponseNorm(self,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1.0)
torch.nn.modules.normalization.LocalResponseNorm.__init__(self,size:int,alpha:float=0.0001,beta:float=0.75,k:float=1.0)
torch.nn.modules.normalization.LocalResponseNorm.extra_repr(self)
torch.nn.modules.normalization.LocalResponseNorm.forward(self,input:Tensor)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/modules/conv.py----------------------------------------
A:torch.nn.modules.conv.self._reversed_padding_repeated_twice->_reverse_repeat_tuple(self.padding, 2)
A:torch.nn.modules.conv.self.weight->UninitializedParameter(**factory_kwargs)
A:torch.nn.modules.conv.self.bias->UninitializedParameter(**factory_kwargs)
A:torch.nn.modules.conv.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
A:torch.nn.modules.conv.kernel_size_->_triple(kernel_size)
A:torch.nn.modules.conv.stride_->_triple(stride)
A:torch.nn.modules.conv.dilation_->_triple(dilation)
A:torch.nn.modules.conv.ret->_single(self.output_padding)
A:torch.nn.modules.conv.min_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.max_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.res->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.modules.conv.stride->_triple(stride)
A:torch.nn.modules.conv.padding->_triple(padding)
A:torch.nn.modules.conv.dilation->_triple(dilation)
A:torch.nn.modules.conv.output_padding->self._output_padding(input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)
A:torch.nn.modules.conv.self.in_channels->self._get_in_channels(input)
A:torch.nn.modules.conv.num_spatial_dims->self._get_num_spatial_dims()
torch.nn.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:Union[str,_size_1_t]=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.Conv1d._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])
torch.nn.Conv1d.forward(self,input:Tensor)->Tensor
torch.nn.Conv2d(self,in_channels:int,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:Union[str,_size_2_t]=0,dilation:_size_2_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.Conv2d._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])
torch.nn.Conv2d.forward(self,input:Tensor)->Tensor
torch.nn.Conv3d(self,in_channels:int,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:Union[str,_size_3_t]=0,dilation:_size_3_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.Conv3d._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])
torch.nn.Conv3d.forward(self,input:Tensor)->Tensor
torch.nn.ConvTranspose1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,output_padding:_size_1_t=0,groups:int=1,bias:bool=True,dilation:_size_1_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.ConvTranspose1d.forward(self,input:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.ConvTranspose2d(self,in_channels:int,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,output_padding:_size_2_t=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.ConvTranspose2d.forward(self,input:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.ConvTranspose3d(self,in_channels:int,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,output_padding:_size_3_t=0,groups:int=1,bias:bool=True,dilation:_size_3_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.ConvTranspose3d.forward(self,input:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.LazyConv1d(self,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.LazyConv1d._get_num_spatial_dims(self)->int
torch.nn.LazyConv2d(self,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,dilation:_size_2_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.LazyConv2d._get_num_spatial_dims(self)->int
torch.nn.LazyConv3d(self,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,dilation:_size_3_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.LazyConv3d._get_num_spatial_dims(self)->int
torch.nn.LazyConvTranspose1d(self,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,output_padding:_size_1_t=0,groups:int=1,bias:bool=True,dilation:_size_1_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.LazyConvTranspose1d._get_num_spatial_dims(self)->int
torch.nn.LazyConvTranspose2d(self,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,output_padding:_size_2_t=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.LazyConvTranspose2d._get_num_spatial_dims(self)->int
torch.nn.LazyConvTranspose3d(self,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,output_padding:_size_3_t=0,groups:int=1,bias:bool=True,dilation:_size_3_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.LazyConvTranspose3d._get_num_spatial_dims(self)->int
torch.nn.conv._ConvNd(self,in_channels:int,out_channels:int,kernel_size:Tuple[int,...],stride:Tuple[int,...],padding:Tuple[int,...],dilation:Tuple[int,...],transposed:bool,output_padding:Tuple[int,...],groups:int,bias:bool,padding_mode:str,device=None,dtype=None)
torch.nn.conv._ConvNd.__setstate__(self,state)
torch.nn.conv._ConvNd._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])->Tensor
torch.nn.conv._ConvNd.extra_repr(self)
torch.nn.conv._ConvNd.reset_parameters(self)->None
torch.nn.conv._ConvTransposeMixin(self,*args,**kwargs)
torch.nn.conv._ConvTransposeNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,device=None,dtype=None)
torch.nn.conv._ConvTransposeNd._output_padding(self,input:Tensor,output_size:Optional[List[int]],stride:List[int],padding:List[int],kernel_size:List[int],dilation:Optional[List[int]]=None)->List[int]
torch.nn.conv._LazyConvXdMixin(LazyModuleMixin)
torch.nn.conv._LazyConvXdMixin._get_in_channels(self,input:Tensor)->int
torch.nn.conv._LazyConvXdMixin._get_num_spatial_dims(self)->int
torch.nn.conv._LazyConvXdMixin.initialize_parameters(self,input)->None
torch.nn.conv._LazyConvXdMixin.reset_parameters(self)->None
torch.nn.modules.conv.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:Union[str,_size_1_t]=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.Conv1d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:Union[str,_size_1_t]=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.Conv1d._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])
torch.nn.modules.conv.Conv1d.forward(self,input:Tensor)->Tensor
torch.nn.modules.conv.Conv2d(self,in_channels:int,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:Union[str,_size_2_t]=0,dilation:_size_2_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.Conv2d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:Union[str,_size_2_t]=0,dilation:_size_2_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.Conv2d._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])
torch.nn.modules.conv.Conv2d.forward(self,input:Tensor)->Tensor
torch.nn.modules.conv.Conv3d(self,in_channels:int,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:Union[str,_size_3_t]=0,dilation:_size_3_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.Conv3d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:Union[str,_size_3_t]=0,dilation:_size_3_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.Conv3d._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])
torch.nn.modules.conv.Conv3d.forward(self,input:Tensor)->Tensor
torch.nn.modules.conv.ConvTranspose1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,output_padding:_size_1_t=0,groups:int=1,bias:bool=True,dilation:_size_1_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.ConvTranspose1d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,output_padding:_size_1_t=0,groups:int=1,bias:bool=True,dilation:_size_1_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.ConvTranspose1d.forward(self,input:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.conv.ConvTranspose2d(self,in_channels:int,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,output_padding:_size_2_t=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.ConvTranspose2d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,output_padding:_size_2_t=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.ConvTranspose2d.forward(self,input:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.conv.ConvTranspose3d(self,in_channels:int,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,output_padding:_size_3_t=0,groups:int=1,bias:bool=True,dilation:_size_3_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.ConvTranspose3d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,output_padding:_size_3_t=0,groups:int=1,bias:bool=True,dilation:_size_3_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.ConvTranspose3d.forward(self,input:Tensor,output_size:Optional[List[int]]=None)->Tensor
torch.nn.modules.conv.LazyConv1d(self,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConv1d.__init__(self,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConv1d._get_num_spatial_dims(self)->int
torch.nn.modules.conv.LazyConv2d(self,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,dilation:_size_2_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConv2d.__init__(self,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,dilation:_size_2_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConv2d._get_num_spatial_dims(self)->int
torch.nn.modules.conv.LazyConv3d(self,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,dilation:_size_3_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConv3d.__init__(self,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,dilation:_size_3_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConv3d._get_num_spatial_dims(self)->int
torch.nn.modules.conv.LazyConvTranspose1d(self,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,output_padding:_size_1_t=0,groups:int=1,bias:bool=True,dilation:_size_1_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConvTranspose1d.__init__(self,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,output_padding:_size_1_t=0,groups:int=1,bias:bool=True,dilation:_size_1_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConvTranspose1d._get_num_spatial_dims(self)->int
torch.nn.modules.conv.LazyConvTranspose2d(self,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,output_padding:_size_2_t=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConvTranspose2d.__init__(self,out_channels:int,kernel_size:_size_2_t,stride:_size_2_t=1,padding:_size_2_t=0,output_padding:_size_2_t=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConvTranspose2d._get_num_spatial_dims(self)->int
torch.nn.modules.conv.LazyConvTranspose3d(self,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,output_padding:_size_3_t=0,groups:int=1,bias:bool=True,dilation:_size_3_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConvTranspose3d.__init__(self,out_channels:int,kernel_size:_size_3_t,stride:_size_3_t=1,padding:_size_3_t=0,output_padding:_size_3_t=0,groups:int=1,bias:bool=True,dilation:_size_3_t=1,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.modules.conv.LazyConvTranspose3d._get_num_spatial_dims(self)->int
torch.nn.modules.conv._ConvNd(self,in_channels:int,out_channels:int,kernel_size:Tuple[int,...],stride:Tuple[int,...],padding:Tuple[int,...],dilation:Tuple[int,...],transposed:bool,output_padding:Tuple[int,...],groups:int,bias:bool,padding_mode:str,device=None,dtype=None)
torch.nn.modules.conv._ConvNd.__init__(self,in_channels:int,out_channels:int,kernel_size:Tuple[int,...],stride:Tuple[int,...],padding:Tuple[int,...],dilation:Tuple[int,...],transposed:bool,output_padding:Tuple[int,...],groups:int,bias:bool,padding_mode:str,device=None,dtype=None)
torch.nn.modules.conv._ConvNd.__setstate__(self,state)
torch.nn.modules.conv._ConvNd._conv_forward(self,input:Tensor,weight:Tensor,bias:Optional[Tensor])->Tensor
torch.nn.modules.conv._ConvNd.extra_repr(self)
torch.nn.modules.conv._ConvNd.reset_parameters(self)->None
torch.nn.modules.conv._ConvTransposeMixin(self,*args,**kwargs)
torch.nn.modules.conv._ConvTransposeMixin.__init__(self,*args,**kwargs)
torch.nn.modules.conv._ConvTransposeNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,device=None,dtype=None)
torch.nn.modules.conv._ConvTransposeNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,device=None,dtype=None)
torch.nn.modules.conv._ConvTransposeNd._output_padding(self,input:Tensor,output_size:Optional[List[int]],stride:List[int],padding:List[int],kernel_size:List[int],dilation:Optional[List[int]]=None)->List[int]
torch.nn.modules.conv._LazyConvXdMixin(LazyModuleMixin)
torch.nn.modules.conv._LazyConvXdMixin._get_in_channels(self,input:Tensor)->int
torch.nn.modules.conv._LazyConvXdMixin._get_num_spatial_dims(self)->int
torch.nn.modules.conv._LazyConvXdMixin.initialize_parameters(self,input)->None
torch.nn.modules.conv._LazyConvXdMixin.reset_parameters(self)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantizable/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantizable/modules/activation.py----------------------------------------
A:torch.nn.quantizable.modules.activation.self.linear_Q->torch.nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.activation.self.linear_K->torch.nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.activation.self.linear_V->torch.nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.activation.self.out_proj->torch.nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.activation.self.q_scaling_product->torch.nn.quantized.FloatFunctional()
A:torch.nn.quantizable.modules.activation.self.quant_attn_output->torch.ao.quantization.QuantStub()
A:torch.nn.quantizable.modules.activation.self.quant_attn_output_weights->torch.ao.quantization.QuantStub()
A:torch.nn.quantizable.modules.activation.self.dequant_q->torch.ao.quantization.DeQuantStub()
A:torch.nn.quantizable.modules.activation.self.dequant_k->torch.ao.quantization.DeQuantStub()
A:torch.nn.quantizable.modules.activation.self.dequant_v->torch.ao.quantization.DeQuantStub()
A:torch.nn.quantizable.modules.activation.observed->torch.ao.quantization.prepare(observed, inplace=True)
A:torch.nn.quantizable.modules.activation.bias->torch.nn.Parameter(bias[_start:], bias.requires_grad)
A:torch.nn.quantizable.modules.activation.observed.linear_Q.weight->torch.nn.Parameter(other.q_proj_weight)
A:torch.nn.quantizable.modules.activation.observed.linear_K.weight->torch.nn.Parameter(other.k_proj_weight)
A:torch.nn.quantizable.modules.activation.observed.linear_V.weight->torch.nn.Parameter(other.v_proj_weight)
A:torch.nn.quantizable.modules.activation.observed.linear_Q.bias->torch.nn.Parameter(other.in_proj_bias[0:other.embed_dim])
A:torch.nn.quantizable.modules.activation.observed.linear_K.bias->torch.nn.Parameter(other.in_proj_bias[other.embed_dim:other.embed_dim * 2])
A:torch.nn.quantizable.modules.activation.observed.linear_V.bias->torch.nn.Parameter(other.in_proj_bias[other.embed_dim * 2:])
A:torch.nn.quantizable.modules.activation.fp->self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout, self.in_proj_bias is not None, self.bias_k is not None, self.add_zero_attn, self.kdim, self.vdim, self.batch_first)
A:torch.nn.quantizable.modules.activation.fp.bias_k->torch.nn.Parameter(self.bias_k.dequantize())
A:torch.nn.quantizable.modules.activation.fp.bias_v->torch.nn.Parameter(self.bias_v.dequantize())
A:torch.nn.quantizable.modules.activation.(w, b)->self.out_proj._weight_bias()
A:torch.nn.quantizable.modules.activation.fp.out_proj.weight->torch.nn.Parameter(w.dequantize())
A:torch.nn.quantizable.modules.activation.fp.out_proj.bias->torch.nn.Parameter(b)
A:torch.nn.quantizable.modules.activation.(wQ, bQ)->self.linear_Q._weight_bias()
A:torch.nn.quantizable.modules.activation.wQ->wQ.dequantize().dequantize()
A:torch.nn.quantizable.modules.activation.(wK, bK)->self.linear_K._weight_bias()
A:torch.nn.quantizable.modules.activation.wK->wK.dequantize().dequantize()
A:torch.nn.quantizable.modules.activation.(wV, bV)->self.linear_V._weight_bias()
A:torch.nn.quantizable.modules.activation.wV->wV.dequantize().dequantize()
A:torch.nn.quantizable.modules.activation.fp.q_proj_weight->torch.nn.Parameter(wQ)
A:torch.nn.quantizable.modules.activation.fp.k_proj_weight->torch.nn.Parameter(wK)
A:torch.nn.quantizable.modules.activation.fp.v_proj_weight->torch.nn.Parameter(wV)
A:torch.nn.quantizable.modules.activation.converted->torch.ao.quantization.convert(other, mapping=None, inplace=False, remove_qconfig=True, convert_custom_config_dict=None)
A:torch.nn.quantizable.modules.activation.bias_k->torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)
A:torch.nn.quantizable.modules.activation.(sc, zp)->torch._choose_qparams_per_tensor(bias_k, reduce_range=False)
A:torch.nn.quantizable.modules.activation.bias_v->torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)
A:torch.nn.quantizable.modules.activation.(tgt_len, bsz, embed_dim_to_check)->query.size()
A:torch.nn.quantizable.modules.activation.q->self.dequant_q(q)
A:torch.nn.quantizable.modules.activation.k->self.dequant_k(k)
A:torch.nn.quantizable.modules.activation.v->self.dequant_v(v)
A:torch.nn.quantizable.modules.activation.attn_mask->torch.nn.functional.pad(attn_mask, (0, 1))
A:torch.nn.quantizable.modules.activation.key_padding_mask->torch.nn.functional.pad(key_padding_mask, (0, 1))
A:torch.nn.quantizable.modules.activation.src_len->self.dequant_k(k).size(1)
A:torch.nn.quantizable.modules.activation.k_zeros->torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)
A:torch.nn.quantizable.modules.activation.v_zeros->torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)
A:torch.nn.quantizable.modules.activation.attn_output_weights->attn_output_weights.mean(dim=1).mean(dim=1)
A:torch.nn.quantizable.modules.activation.attn_output->self.out_proj(attn_output)
torch.nn.quantizable.MultiheadAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,bias:bool=True,add_bias_kv:bool=False,add_zero_attn:bool=False,kdim:int=None,vdim:int=None,batch_first:bool=False,device=None,dtype=None)
torch.nn.quantizable.MultiheadAttention._forward_impl(self,query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.quantizable.MultiheadAttention._get_name(self)
torch.nn.quantizable.MultiheadAttention.dequantize(self)
torch.nn.quantizable.MultiheadAttention.forward(self,query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.quantizable.MultiheadAttention.from_float(cls,other)
torch.nn.quantizable.MultiheadAttention.from_observed(cls,other)
torch.nn.quantizable.modules.activation.MultiheadAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,bias:bool=True,add_bias_kv:bool=False,add_zero_attn:bool=False,kdim:int=None,vdim:int=None,batch_first:bool=False,device=None,dtype=None)
torch.nn.quantizable.modules.activation.MultiheadAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,bias:bool=True,add_bias_kv:bool=False,add_zero_attn:bool=False,kdim:int=None,vdim:int=None,batch_first:bool=False,device=None,dtype=None)
torch.nn.quantizable.modules.activation.MultiheadAttention._forward_impl(self,query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.quantizable.modules.activation.MultiheadAttention._get_name(self)
torch.nn.quantizable.modules.activation.MultiheadAttention.dequantize(self)
torch.nn.quantizable.modules.activation.MultiheadAttention.forward(self,query:Tensor,key:Tensor,value:Tensor,key_padding_mask:Optional[Tensor]=None,need_weights:bool=True,attn_mask:Optional[Tensor]=None,average_attn_weights:bool=True)->Tuple[Tensor, Optional[Tensor]]
torch.nn.quantizable.modules.activation.MultiheadAttention.from_float(cls,other)
torch.nn.quantizable.modules.activation.MultiheadAttention.from_observed(cls,other)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantizable/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantizable/modules/rnn.py----------------------------------------
A:torch.nn.quantizable.modules.rnn.self.igates->torch.nn.Linear(input_dim, 4 * hidden_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.rnn.self.hgates->torch.nn.Linear(hidden_dim, 4 * hidden_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.rnn.self.gates->torch.nn.quantized.FloatFunctional()
A:torch.nn.quantizable.modules.rnn.self.fgate_cx->torch.nn.quantized.FloatFunctional()
A:torch.nn.quantizable.modules.rnn.self.igate_cgate->torch.nn.quantized.FloatFunctional()
A:torch.nn.quantizable.modules.rnn.self.fgate_cx_igate_cgate->torch.nn.quantized.FloatFunctional()
A:torch.nn.quantizable.modules.rnn.self.ogate_cy->torch.nn.quantized.FloatFunctional()
A:torch.nn.quantizable.modules.rnn.hidden->self.cell(xx, hidden)
A:torch.nn.quantizable.modules.rnn.igates->self.igates(x)
A:torch.nn.quantizable.modules.rnn.hgates->self.hgates(hx)
A:torch.nn.quantizable.modules.rnn.gates->self.gates.add(igates, hgates)
A:torch.nn.quantizable.modules.rnn.(input_gate, forget_gate, cell_gate, out_gate)->self.gates.add(igates, hgates).chunk(4, 1)
A:torch.nn.quantizable.modules.rnn.input_gate->torch.sigmoid(input_gate)
A:torch.nn.quantizable.modules.rnn.forget_gate->torch.sigmoid(forget_gate)
A:torch.nn.quantizable.modules.rnn.cell_gate->torch.tanh(cell_gate)
A:torch.nn.quantizable.modules.rnn.out_gate->torch.sigmoid(out_gate)
A:torch.nn.quantizable.modules.rnn.fgate_cx->self.fgate_cx.mul(forget_gate, cx)
A:torch.nn.quantizable.modules.rnn.igate_cgate->self.igate_cgate.mul(input_gate, cell_gate)
A:torch.nn.quantizable.modules.rnn.fgate_cx_igate_cgate->self.fgate_cx_igate_cgate.add(fgate_cx, igate_cgate)
A:torch.nn.quantizable.modules.rnn.tanh_cy->torch.tanh(cy)
A:torch.nn.quantizable.modules.rnn.hy->self.ogate_cy.mul(out_gate, tanh_cy)
A:torch.nn.quantizable.modules.rnn.h->torch.stack([hidden_fw[0], hidden_bw[0]], 0)
A:torch.nn.quantizable.modules.rnn.c->torch.stack([hidden_fw[1], hidden_bw[1]], 0)
A:torch.nn.quantizable.modules.rnn.cell->LSTMCell.from_params(*args, **kwargs)
A:torch.nn.quantizable.modules.rnn.cell.igates.weight->torch.nn.Parameter(wi)
A:torch.nn.quantizable.modules.rnn.cell.igates.bias->torch.nn.Parameter(bi)
A:torch.nn.quantizable.modules.rnn.cell.hgates.weight->torch.nn.Parameter(wh)
A:torch.nn.quantizable.modules.rnn.cell.hgates.bias->torch.nn.Parameter(bh)
A:torch.nn.quantizable.modules.rnn.observed->torch.ao.quantization.prepare(observed, inplace=True)
A:torch.nn.quantizable.modules.rnn.self.cell->LSTMCell(input_dim, hidden_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.rnn.result_tensor->torch.stack(result, 0)
A:torch.nn.quantizable.modules.rnn.layer->cls(input_size, hidden_size, bias, batch_first, bidirectional)
A:torch.nn.quantizable.modules.rnn.self.layer_fw->_LSTMSingleLayer(input_dim, hidden_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.rnn.self.layer_bw->_LSTMSingleLayer(input_dim, hidden_dim, bias=bias, **factory_kwargs)
A:torch.nn.quantizable.modules.rnn.x->x.transpose(0, 1).transpose(0, 1)
A:torch.nn.quantizable.modules.rnn.(result_fw, hidden_fw)->self.layer_fw(x, hidden_fw)
A:torch.nn.quantizable.modules.rnn.x_reversed->x.transpose(0, 1).transpose(0, 1).flip(0)
A:torch.nn.quantizable.modules.rnn.(result_bw, hidden_bw)->self.layer_bw(x_reversed, hidden_bw)
A:torch.nn.quantizable.modules.rnn.result_bw->result_bw.flip(0).flip(0)
A:torch.nn.quantizable.modules.rnn.result->torch.cat([result_fw, result_bw], result_fw.dim() - 1)
A:torch.nn.quantizable.modules.rnn.(h, c)->torch.jit._unwrap_optional(hidden_fw)
A:torch.nn.quantizable.modules.rnn.input_size->kwargs.get('input_size', other.input_size)
A:torch.nn.quantizable.modules.rnn.hidden_size->kwargs.get('hidden_size', other.hidden_size)
A:torch.nn.quantizable.modules.rnn.bias->kwargs.get('bias', other.bias)
A:torch.nn.quantizable.modules.rnn.batch_first->kwargs.get('batch_first', other.batch_first)
A:torch.nn.quantizable.modules.rnn.bidirectional->kwargs.get('bidirectional', other.bidirectional)
A:torch.nn.quantizable.modules.rnn.layer.qconfig->getattr(other, 'qconfig', qconfig)
A:torch.nn.quantizable.modules.rnn.wi->getattr(other, f'weight_ih_l{layer_idx}_reverse')
A:torch.nn.quantizable.modules.rnn.wh->getattr(other, f'weight_hh_l{layer_idx}_reverse')
A:torch.nn.quantizable.modules.rnn.bi->getattr(other, f'bias_ih_l{layer_idx}_reverse', None)
A:torch.nn.quantizable.modules.rnn.bh->getattr(other, f'bias_hh_l{layer_idx}_reverse', None)
A:torch.nn.quantizable.modules.rnn.layer.layer_fw->_LSTMSingleLayer.from_params(wi, wh, bi, bh)
A:torch.nn.quantizable.modules.rnn.layer.layer_bw->_LSTMSingleLayer.from_params(wi, wh, bi, bh)
A:torch.nn.quantizable.modules.rnn.self.dropout->float(dropout)
A:torch.nn.quantizable.modules.rnn.self.layers->torch.nn.ModuleList(layers)
A:torch.nn.quantizable.modules.rnn.max_batch_size->x.transpose(0, 1).transpose(0, 1).size(1)
A:torch.nn.quantizable.modules.rnn.zeros->torch.quantize_per_tensor(zeros, scale=1.0, zero_point=0, dtype=x.dtype)
A:torch.nn.quantizable.modules.rnn.hidden_non_opt->torch.jit._unwrap_optional(hidden)
A:torch.nn.quantizable.modules.rnn.hx->hidden_non_opt[0].reshape(self.num_layers, num_directions, max_batch_size, self.hidden_size).unbind(0)
A:torch.nn.quantizable.modules.rnn.cx->hidden_non_opt[1].reshape(self.num_layers, num_directions, max_batch_size, self.hidden_size).unbind(0)
A:torch.nn.quantizable.modules.rnn.(x, hxcx[idx])->layer(x, hxcx[idx])
A:torch.nn.quantizable.modules.rnn.hx_tensor->hx_tensor.reshape(-1, *hx_tensor.shape[-2:]).reshape(-1, *hx_tensor.shape[-2:])
A:torch.nn.quantizable.modules.rnn.cx_tensor->cx_tensor.reshape(-1, *cx_tensor.shape[-2:]).reshape(-1, *cx_tensor.shape[-2:])
A:torch.nn.quantizable.modules.rnn.observed.qconfig->getattr(other, 'qconfig', qconfig)
A:torch.nn.quantizable.modules.rnn.observed.layers[idx]->_LSTMLayer.from_float(other, idx, qconfig, batch_first=False)
torch.nn.quantizable.LSTM(self,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False,device=None,dtype=None)
torch.nn.quantizable.LSTM._get_name(self)
torch.nn.quantizable.LSTM.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)
torch.nn.quantizable.LSTM.from_float(cls,other,qconfig=None)
torch.nn.quantizable.LSTM.from_observed(cls,other)
torch.nn.quantizable.LSTMCell(self,input_dim:int,hidden_dim:int,bias:bool=True,device=None,dtype=None)
torch.nn.quantizable.LSTMCell._get_name(self)
torch.nn.quantizable.LSTMCell.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.nn.quantizable.LSTMCell.from_float(cls,other)
torch.nn.quantizable.LSTMCell.from_params(cls,wi,wh,bi=None,bh=None)
torch.nn.quantizable.LSTMCell.initialize_hidden(self,batch_size:int,is_quantized:bool=False)->Tuple[Tensor, Tensor]
torch.nn.quantizable.modules.rnn.LSTM(self,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False,device=None,dtype=None)
torch.nn.quantizable.modules.rnn.LSTM.__init__(self,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False,device=None,dtype=None)
torch.nn.quantizable.modules.rnn.LSTM._get_name(self)
torch.nn.quantizable.modules.rnn.LSTM.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)
torch.nn.quantizable.modules.rnn.LSTM.from_float(cls,other,qconfig=None)
torch.nn.quantizable.modules.rnn.LSTM.from_observed(cls,other)
torch.nn.quantizable.modules.rnn.LSTMCell(self,input_dim:int,hidden_dim:int,bias:bool=True,device=None,dtype=None)
torch.nn.quantizable.modules.rnn.LSTMCell.__init__(self,input_dim:int,hidden_dim:int,bias:bool=True,device=None,dtype=None)
torch.nn.quantizable.modules.rnn.LSTMCell._get_name(self)
torch.nn.quantizable.modules.rnn.LSTMCell.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.nn.quantizable.modules.rnn.LSTMCell.from_float(cls,other)
torch.nn.quantizable.modules.rnn.LSTMCell.from_params(cls,wi,wh,bi=None,bh=None)
torch.nn.quantizable.modules.rnn.LSTMCell.initialize_hidden(self,batch_size:int,is_quantized:bool=False)->Tuple[Tensor, Tensor]
torch.nn.quantizable.modules.rnn._LSTMLayer(self,input_dim:int,hidden_dim:int,bias:bool=True,batch_first:bool=False,bidirectional:bool=False,device=None,dtype=None)
torch.nn.quantizable.modules.rnn._LSTMLayer.__init__(self,input_dim:int,hidden_dim:int,bias:bool=True,batch_first:bool=False,bidirectional:bool=False,device=None,dtype=None)
torch.nn.quantizable.modules.rnn._LSTMLayer.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)
torch.nn.quantizable.modules.rnn._LSTMLayer.from_float(cls,other,layer_idx=0,qconfig=None,**kwargs)
torch.nn.quantizable.modules.rnn._LSTMSingleLayer(self,input_dim:int,hidden_dim:int,bias:bool=True,device=None,dtype=None)
torch.nn.quantizable.modules.rnn._LSTMSingleLayer.__init__(self,input_dim:int,hidden_dim:int,bias:bool=True,device=None,dtype=None)
torch.nn.quantizable.modules.rnn._LSTMSingleLayer.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)
torch.nn.quantizable.modules.rnn._LSTMSingleLayer.from_params(cls,*args,**kwargs)
torch.nn.quantizable.rnn._LSTMLayer(self,input_dim:int,hidden_dim:int,bias:bool=True,batch_first:bool=False,bidirectional:bool=False,device=None,dtype=None)
torch.nn.quantizable.rnn._LSTMLayer.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)
torch.nn.quantizable.rnn._LSTMLayer.from_float(cls,other,layer_idx=0,qconfig=None,**kwargs)
torch.nn.quantizable.rnn._LSTMSingleLayer(self,input_dim:int,hidden_dim:int,bias:bool=True,device=None,dtype=None)
torch.nn.quantizable.rnn._LSTMSingleLayer.forward(self,x:Tensor,hidden:Optional[Tuple[Tensor,Tensor]]=None)
torch.nn.quantizable.rnn._LSTMSingleLayer.from_params(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/modules/embedding_ops.py----------------------------------------
A:torch.nn.qat.modules.embedding_ops.self.weight_fake_quant->qconfig.weight(factory_kwargs=factory_kwargs)
A:torch.nn.qat.modules.embedding_ops.qat_embedding_bag->cls(mod.num_embeddings, mod.embedding_dim, mod.max_norm, mod.norm_type, mod.scale_grad_by_freq, mod.mode, mod.sparse, mod.weight, mod.include_last_offset, mod.padding_idx, qconfig=qconfig)
A:torch.nn.qat.modules.embedding_ops.embedding_bag->torch.nn.EmbeddingBag(self.num_embeddings, self.embedding_dim, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.mode, self.sparse, None, self.include_last_offset, self.padding_idx, self.device, self.dtype)
A:torch.nn.qat.modules.embedding_ops.embedding_bag.weight->torch.nn.Parameter(self.weight.detach())
torch.nn.qat.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None,device=None,dtype=None,qconfig=None)
torch.nn.qat.Embedding.forward(self,input)->Tensor
torch.nn.qat.Embedding.from_float(cls,mod)
torch.nn.qat.Embedding.to_float(self)
torch.nn.qat.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None,include_last_offset=False,padding_idx=None,qconfig=None,device=None,dtype=None)
torch.nn.qat.EmbeddingBag.forward(self,input,offsets=None,per_sample_weights=None)->Tensor
torch.nn.qat.EmbeddingBag.from_float(cls,mod)
torch.nn.qat.EmbeddingBag.to_float(self)
torch.nn.qat.modules.embedding_ops.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None,device=None,dtype=None,qconfig=None)
torch.nn.qat.modules.embedding_ops.Embedding.__init__(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None,device=None,dtype=None,qconfig=None)
torch.nn.qat.modules.embedding_ops.Embedding.forward(self,input)->Tensor
torch.nn.qat.modules.embedding_ops.Embedding.from_float(cls,mod)
torch.nn.qat.modules.embedding_ops.Embedding.to_float(self)
torch.nn.qat.modules.embedding_ops.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None,include_last_offset=False,padding_idx=None,qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.embedding_ops.EmbeddingBag.__init__(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None,include_last_offset=False,padding_idx=None,qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.embedding_ops.EmbeddingBag.forward(self,input,offsets=None,per_sample_weights=None)->Tensor
torch.nn.qat.modules.embedding_ops.EmbeddingBag.from_float(cls,mod)
torch.nn.qat.modules.embedding_ops.EmbeddingBag.to_float(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/modules/linear.py----------------------------------------
A:torch.nn.qat.modules.linear.self.weight_fake_quant->qconfig.weight(factory_kwargs=factory_kwargs)
A:torch.nn.qat.modules.linear.qat_linear->cls(mod.in_features, mod.out_features, bias=mod.bias is not None, qconfig=qconfig)
A:torch.nn.qat.modules.linear.linear->torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)
A:torch.nn.qat.modules.linear.linear.weight->torch.nn.Parameter(self.weight.detach())
A:torch.nn.qat.modules.linear.linear.bias->torch.nn.Parameter(self.bias.detach())
torch.nn.qat.Linear(self,in_features,out_features,bias=True,qconfig=None,device=None,dtype=None)
torch.nn.qat.Linear.forward(self,input)
torch.nn.qat.Linear.from_float(cls,mod)
torch.nn.qat.Linear.to_float(self)
torch.nn.qat.modules.linear.Linear(self,in_features,out_features,bias=True,qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.linear.Linear.__init__(self,in_features,out_features,bias=True,qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.linear.Linear.forward(self,input)
torch.nn.qat.modules.linear.Linear.from_float(cls,mod)
torch.nn.qat.modules.linear.Linear.to_float(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/modules/conv.py----------------------------------------
A:torch.nn.qat.modules.conv.self.weight_fake_quant->qconfig.weight(factory_kwargs=factory_kwargs)
A:torch.nn.qat.modules.conv.qat_conv->cls(mod.in_channels, mod.out_channels, mod.kernel_size, stride=mod.stride, padding=mod.padding, dilation=mod.dilation, groups=mod.groups, bias=mod.bias is not None, padding_mode=mod.padding_mode, qconfig=qconfig)
A:torch.nn.qat.modules.conv.conv->torch.nn.Conv3d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, self.dilation, self.groups, self.bias is not None, self.padding_mode)
A:torch.nn.qat.modules.conv.conv.weight->torch.nn.Parameter(self.weight.detach())
A:torch.nn.qat.modules.conv.conv.bias->torch.nn.Parameter(self.bias.detach())
torch.nn.qat.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None,device=None,dtype=None)
torch.nn.qat.Conv2d.forward(self,input)
torch.nn.qat.Conv2d.from_float(cls,mod)
torch.nn.qat.Conv2d.to_float(self)
torch.nn.qat.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None,device=None,dtype=None)
torch.nn.qat.Conv3d.forward(self,input)
torch.nn.qat.Conv3d.from_float(cls,mod)
torch.nn.qat.Conv3d.to_float(self)
torch.nn.qat.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.conv.Conv2d.forward(self,input)
torch.nn.qat.modules.conv.Conv2d.from_float(cls,mod)
torch.nn.qat.modules.conv.Conv2d.to_float(self)
torch.nn.qat.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None,device=None,dtype=None)
torch.nn.qat.modules.conv.Conv3d.forward(self,input)
torch.nn.qat.modules.conv.Conv3d.from_float(cls,mod)
torch.nn.qat.modules.conv.Conv3d.to_float(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/dynamic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/dynamic/modules/linear.py----------------------------------------
torch.nn.qat.dynamic.Linear(self,in_features,out_features,bias=True,qconfig=None,device=None,dtype=None)
torch.nn.qat.dynamic.modules.linear.Linear(self,in_features,out_features,bias=True,qconfig=None,device=None,dtype=None)
torch.nn.qat.dynamic.modules.linear.Linear.__init__(self,in_features,out_features,bias=True,qconfig=None,device=None,dtype=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/qat/dynamic/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/functional.py----------------------------------------
A:torch.nn.quantized.functional.stride->torch.jit.annotate(List[int], [])
A:torch.nn.quantized.functional.padding->_triple(padding)
A:torch.nn.quantized.functional.dilation->_triple(dilation)
A:torch.nn.quantized.functional.packed_params->torch.ops.quantized.conv3d_prepack(weight, bias, stride, padding, dilation, groups)
A:torch.nn.quantized.functional.scale->input.q_scale()
A:torch.nn.quantized.functional.zero_point->input.q_zero_point()
A:torch.nn.quantized.functional._packed_params->torch.ops.quantized.linear_prepack(weight, bias)
A:torch.nn.quantized.functional.output->torch._empty_affine_quantized(input.shape, scale=scale, zero_point=int(zero_point), dtype=input.dtype)
A:torch.nn.quantized.functional.result->torch._C._nn.leaky_relu(input, negative_slope)
torch.nn.quantized.functional.adaptive_avg_pool2d(input:Tensor,output_size:BroadcastingList2[int])->Tensor
torch.nn.quantized.functional.adaptive_avg_pool3d(input:Tensor,output_size:BroadcastingList2[int])->Tensor
torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.functional.celu(input:Tensor,scale:float,zero_point:int,alpha:float=1.0)->Tensor
torch.nn.quantized.functional.clamp(input:Tensor,min_:float,max_:float)->Tensor
torch.nn.quantized.functional.conv1d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.elu(input:Tensor,scale:float,zero_point:int,alpha:float=1.0)->Tensor
torch.nn.quantized.functional.hardsigmoid(input:Tensor,inplace:bool=False)->Tensor
torch.nn.quantized.functional.hardswish(input:Tensor,scale:float,zero_point:int)->Tensor
torch.nn.quantized.functional.hardtanh(input:Tensor,min_val:float=-1.0,max_val:float=1.0,inplace:bool=False)->Tensor
torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.functional.leaky_relu(input:Tensor,negative_slope:float=0.01,inplace:bool=False,scale:Optional[float]=None,zero_point:Optional[int]=None)
torch.nn.quantized.functional.linear(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,scale:Optional[float]=None,zero_point:Optional[int]=None)->Tensor
torch.nn.quantized.functional.max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.quantized.functional.max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.quantized.functional.threshold(input:Tensor,threshold:float,value:float)->Tensor
torch.nn.quantized.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.quantized.functional.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/embedding_ops.py----------------------------------------
A:torch.nn.quantized.modules.embedding_ops.scales->torch.ones(num_embeddings, dtype=torch.float)
A:torch.nn.quantized.modules.embedding_ops.zero_points->torch.zeros(num_embeddings, dtype=torch.float)
A:torch.nn.quantized.modules.embedding_ops.wq->torch._empty_per_channel_affine_quantized([num_embeddings, embedding_dim], scales=scales, zero_points=zero_points, axis=0, dtype=self.dtype)
A:torch.nn.quantized.modules.embedding_ops.self._packed_weight->torch.ops.quantized.embedding_bag_prepack(weight)
A:torch.nn.quantized.modules.embedding_ops.destination[prefix + '_packed_weight']->self._weight()
A:torch.nn.quantized.modules.embedding_ops.qweight->_quantize_weight(mod.weight.float(), weight_observer)
A:torch.nn.quantized.modules.embedding_ops.self._packed_params->EmbeddingPackedParams(num_embeddings, embedding_dim, dtype)
A:torch.nn.quantized.modules.embedding_ops.extra_repr_str->'num_embeddings={}, embedding_dim={}, dtype={}, qscheme={}'.format(self.num_embeddings, self.embedding_dim, self._packed_params.dtype, self.weight().qscheme())
A:torch.nn.quantized.modules.embedding_ops.weight_observer->torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.weight()
A:torch.nn.quantized.modules.embedding_ops.qembedding->Embedding(mod.num_embeddings, mod.embedding_dim)
A:torch.nn.quantized.modules.embedding_ops.qembedding_bag->EmbeddingBag(mod.num_embeddings, mod.embedding_dim, dtype=dtype)
torch.nn.quantized.Embedding(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[Tensor]=None,dtype=torch.quint8)
torch.nn.quantized.Embedding.__repr__(self)
torch.nn.quantized.Embedding._get_name(self)
torch.nn.quantized.Embedding.extra_repr(self)
torch.nn.quantized.Embedding.forward(self,indices:Tensor)->Tensor
torch.nn.quantized.Embedding.from_float(cls,mod)
torch.nn.quantized.Embedding.set_weight(self,w:torch.Tensor)->None
torch.nn.quantized.Embedding.weight(self)
torch.nn.quantized.EmbeddingBag(self,num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='sum',sparse:bool=False,_weight:Optional[Tensor]=None,include_last_offset:bool=False,dtype=torch.quint8)
torch.nn.quantized.EmbeddingBag._get_name(self)
torch.nn.quantized.EmbeddingBag.forward(self,indices:Tensor,offsets:Optional[Tensor]=None,per_sample_weights:Optional[Tensor]=None,compressed_indices_mapping:Optional[Tensor]=None)->Tensor
torch.nn.quantized.EmbeddingBag.from_float(cls,mod)
torch.nn.quantized.EmbeddingPackedParams(self,num_embeddings,embedding_dim,dtype=torch.quint8)
torch.nn.quantized.EmbeddingPackedParams.__repr__(self)
torch.nn.quantized.EmbeddingPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.EmbeddingPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.EmbeddingPackedParams._weight(self)
torch.nn.quantized.EmbeddingPackedParams.forward(self,x)
torch.nn.quantized.EmbeddingPackedParams.set_weight(self,weight:torch.Tensor)->None
torch.nn.quantized.modules.embedding_ops.Embedding(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[Tensor]=None,dtype=torch.quint8)
torch.nn.quantized.modules.embedding_ops.Embedding.__init__(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[Tensor]=None,dtype=torch.quint8)
torch.nn.quantized.modules.embedding_ops.Embedding.__repr__(self)
torch.nn.quantized.modules.embedding_ops.Embedding._get_name(self)
torch.nn.quantized.modules.embedding_ops.Embedding.extra_repr(self)
torch.nn.quantized.modules.embedding_ops.Embedding.forward(self,indices:Tensor)->Tensor
torch.nn.quantized.modules.embedding_ops.Embedding.from_float(cls,mod)
torch.nn.quantized.modules.embedding_ops.Embedding.set_weight(self,w:torch.Tensor)->None
torch.nn.quantized.modules.embedding_ops.Embedding.weight(self)
torch.nn.quantized.modules.embedding_ops.EmbeddingBag(self,num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='sum',sparse:bool=False,_weight:Optional[Tensor]=None,include_last_offset:bool=False,dtype=torch.quint8)
torch.nn.quantized.modules.embedding_ops.EmbeddingBag.__init__(self,num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='sum',sparse:bool=False,_weight:Optional[Tensor]=None,include_last_offset:bool=False,dtype=torch.quint8)
torch.nn.quantized.modules.embedding_ops.EmbeddingBag._get_name(self)
torch.nn.quantized.modules.embedding_ops.EmbeddingBag.forward(self,indices:Tensor,offsets:Optional[Tensor]=None,per_sample_weights:Optional[Tensor]=None,compressed_indices_mapping:Optional[Tensor]=None)->Tensor
torch.nn.quantized.modules.embedding_ops.EmbeddingBag.from_float(cls,mod)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams(self,num_embeddings,embedding_dim,dtype=torch.quint8)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams.__init__(self,num_embeddings,embedding_dim,dtype=torch.quint8)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams.__repr__(self)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams._weight(self)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams.forward(self,x)
torch.nn.quantized.modules.embedding_ops.EmbeddingPackedParams.set_weight(self,weight:torch.Tensor)->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/dropout.py----------------------------------------
torch.nn.quantized.Dropout(torch.nn.Dropout)
torch.nn.quantized.Dropout._get_name(self)
torch.nn.quantized.Dropout.forward(self,input)
torch.nn.quantized.Dropout.from_float(cls,mod)
torch.nn.quantized.modules.dropout.Dropout(torch.nn.Dropout)
torch.nn.quantized.modules.dropout.Dropout._get_name(self)
torch.nn.quantized.modules.dropout.Dropout.forward(self,input)
torch.nn.quantized.modules.dropout.Dropout.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/activation.py----------------------------------------
A:torch.nn.quantized.modules.activation.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.activation.(output_scale, output_zero_point)->mod.activation_post_process.calculate_qparams()
torch.nn.quantized.ELU(self,scale,zero_point,alpha=1.0)
torch.nn.quantized.ELU._get_name(self)
torch.nn.quantized.ELU.forward(self,input)
torch.nn.quantized.ELU.from_float(mod)
torch.nn.quantized.Hardswish(self,scale,zero_point)
torch.nn.quantized.Hardswish._get_name(self)
torch.nn.quantized.Hardswish.forward(self,input)
torch.nn.quantized.Hardswish.from_float(mod)
torch.nn.quantized.LeakyReLU(self,scale:float,zero_point:int,negative_slope:float=0.01,inplace:bool=False,device=None,dtype=None)
torch.nn.quantized.LeakyReLU._get_name(self)
torch.nn.quantized.LeakyReLU.forward(self,input)
torch.nn.quantized.LeakyReLU.from_float(cls,mod)
torch.nn.quantized.ReLU6(self,inplace=False)
torch.nn.quantized.ReLU6._get_name(self)
torch.nn.quantized.ReLU6.forward(self,input)
torch.nn.quantized.ReLU6.from_float(mod)
torch.nn.quantized.Sigmoid(self,output_scale:float,output_zero_point:int)
torch.nn.quantized.Sigmoid.forward(self,input)
torch.nn.quantized.Sigmoid.from_float(cls,mod)
torch.nn.quantized.modules.activation.ELU(self,scale,zero_point,alpha=1.0)
torch.nn.quantized.modules.activation.ELU.__init__(self,scale,zero_point,alpha=1.0)
torch.nn.quantized.modules.activation.ELU._get_name(self)
torch.nn.quantized.modules.activation.ELU.forward(self,input)
torch.nn.quantized.modules.activation.ELU.from_float(mod)
torch.nn.quantized.modules.activation.Hardswish(self,scale,zero_point)
torch.nn.quantized.modules.activation.Hardswish.__init__(self,scale,zero_point)
torch.nn.quantized.modules.activation.Hardswish._get_name(self)
torch.nn.quantized.modules.activation.Hardswish.forward(self,input)
torch.nn.quantized.modules.activation.Hardswish.from_float(mod)
torch.nn.quantized.modules.activation.LeakyReLU(self,scale:float,zero_point:int,negative_slope:float=0.01,inplace:bool=False,device=None,dtype=None)
torch.nn.quantized.modules.activation.LeakyReLU.__init__(self,scale:float,zero_point:int,negative_slope:float=0.01,inplace:bool=False,device=None,dtype=None)
torch.nn.quantized.modules.activation.LeakyReLU._get_name(self)
torch.nn.quantized.modules.activation.LeakyReLU.forward(self,input)
torch.nn.quantized.modules.activation.LeakyReLU.from_float(cls,mod)
torch.nn.quantized.modules.activation.ReLU6(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU6.__init__(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU6._get_name(self)
torch.nn.quantized.modules.activation.ReLU6.forward(self,input)
torch.nn.quantized.modules.activation.ReLU6.from_float(mod)
torch.nn.quantized.modules.activation.Sigmoid(self,output_scale:float,output_zero_point:int)
torch.nn.quantized.modules.activation.Sigmoid.__init__(self,output_scale:float,output_zero_point:int)
torch.nn.quantized.modules.activation.Sigmoid.forward(self,input)
torch.nn.quantized.modules.activation.Sigmoid.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/utils.py----------------------------------------
A:torch.nn.quantized.modules.utils.(wt_scale, wt_zp)->observer.calculate_qparams()
A:torch.nn.quantized.modules.utils.qweight->torch.quantize_per_channel(float_wt, wt_scale.to(torch.float), wt_zp.to(torch.float), observer.ch_axis, observer.dtype)
A:torch.nn.quantized.modules.utils.extra_repr->self.extra_repr()
A:torch.nn.quantized.modules.utils.extra_lines->self.extra_repr().split('\n')
A:torch.nn.quantized.modules.utils.mod_str->_addindent(mod_str, 2)
A:torch.nn.quantized.modules.utils._pair_from_first->_ntuple_from_first(2)
torch.nn.quantized.modules.utils.ReferenceableQuantizedModule(torch.nn.Module,metaclass=abc.ABCMeta)
torch.nn.quantized.modules.utils.ReferenceableQuantizedModule.from_reference(cls,ref_module,output_scale,output_zero_point)
torch.nn.quantized.modules.utils._ntuple_from_first(n)
torch.nn.quantized.modules.utils._quantize_weight(float_wt,observer)
torch.nn.quantized.modules.utils.hide_packed_params_repr(self,params)
torch.nn.quantized.utils.ReferenceableQuantizedModule(torch.nn.Module,metaclass=abc.ABCMeta)
torch.nn.quantized.utils.ReferenceableQuantizedModule.from_reference(cls,ref_module,output_scale,output_zero_point)
torch.nn.quantized.utils._ntuple_from_first(n)
torch.nn.quantized.utils._quantize_weight(float_wt,observer)
torch.nn.quantized.utils.hide_packed_params_repr(self,params)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/linear.py----------------------------------------
A:torch.nn.quantized.modules.linear.wq->torch.zeros([1, 1], dtype=torch.float)
A:torch.nn.quantized.modules.linear.self._packed_params->LinearPackedParams(dtype)
A:torch.nn.quantized.modules.linear.destination[prefix + '_packed_params']->self._weight_bias()
A:torch.nn.quantized.modules.linear.version->local_metadata.get('version', None)
A:torch.nn.quantized.modules.linear.(qweight, bias)->self._weight_bias()
A:torch.nn.quantized.modules.linear.new_instance->type(self).__new__(type(self))
A:torch.nn.quantized.modules.linear.state->self.__getstate__()
A:torch.nn.quantized.modules.linear.bias->state_dict.pop(prefix + 'bias')
A:torch.nn.quantized.modules.linear.qweight->ref_qlinear.get_quantized_weight()
A:torch.nn.quantized.modules.linear.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.linear.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.linear.self.scale->float(state_dict[prefix + 'scale'])
A:torch.nn.quantized.modules.linear.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.nn.quantized.modules.linear.weight->state_dict.pop(prefix + 'weight')
A:torch.nn.quantized.modules.linear.supported_modules->', '.join([float_mod.__name__ for float_mod in cls._FLOAT_MODULE])
A:torch.nn.quantized.modules.linear.error_msg->'nnq.{}.from_float only works for {}, but got: {}'.format(cls.__name__, supported_modules, type(mod))
A:torch.nn.quantized.modules.linear.weight_post_process->mod.qconfig.weight()
A:torch.nn.quantized.modules.linear.(act_scale, act_zp)->activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.linear.qlinear->cls(ref_qlinear.in_features, ref_qlinear.out_features)
A:torch.nn.quantized.modules.linear.qlinear.scale->float(output_scale)
A:torch.nn.quantized.modules.linear.qlinear.zero_point->int(output_zero_point)
torch.nn.quantized.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.Linear.__repr__(self)
torch.nn.quantized.Linear._get_name(self)
torch.nn.quantized.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.Linear._weight_bias(self)
torch.nn.quantized.Linear.bias(self)
torch.nn.quantized.Linear.extra_repr(self)
torch.nn.quantized.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized.Linear.from_float(cls,mod)
torch.nn.quantized.Linear.from_reference(cls,ref_qlinear,output_scale,output_zero_point)
torch.nn.quantized.Linear.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.Linear.weight(self)
torch.nn.quantized.LinearPackedParams(self,dtype=torch.qint8)
torch.nn.quantized.LinearPackedParams.__copy__(self)
torch.nn.quantized.LinearPackedParams.__deepcopy__(self,memo)
torch.nn.quantized.LinearPackedParams.__getstate__(self)
torch.nn.quantized.LinearPackedParams.__repr__(self)
torch.nn.quantized.LinearPackedParams.__setstate__(self,state)
torch.nn.quantized.LinearPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.LinearPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.LinearPackedParams._weight_bias(self)
torch.nn.quantized.LinearPackedParams.forward(self,x)
torch.nn.quantized.LinearPackedParams.set_weight_bias(self,weight:torch.Tensor,bias:Optional[torch.Tensor])->None
torch.nn.quantized.modules.linear.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.modules.linear.Linear.__init__(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.modules.linear.Linear.__repr__(self)
torch.nn.quantized.modules.linear.Linear._get_name(self)
torch.nn.quantized.modules.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.linear.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.linear.Linear._weight_bias(self)
torch.nn.quantized.modules.linear.Linear.bias(self)
torch.nn.quantized.modules.linear.Linear.extra_repr(self)
torch.nn.quantized.modules.linear.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized.modules.linear.Linear.from_float(cls,mod)
torch.nn.quantized.modules.linear.Linear.from_reference(cls,ref_qlinear,output_scale,output_zero_point)
torch.nn.quantized.modules.linear.Linear.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.linear.Linear.weight(self)
torch.nn.quantized.modules.linear.LinearPackedParams(self,dtype=torch.qint8)
torch.nn.quantized.modules.linear.LinearPackedParams.__copy__(self)
torch.nn.quantized.modules.linear.LinearPackedParams.__deepcopy__(self,memo)
torch.nn.quantized.modules.linear.LinearPackedParams.__getstate__(self)
torch.nn.quantized.modules.linear.LinearPackedParams.__init__(self,dtype=torch.qint8)
torch.nn.quantized.modules.linear.LinearPackedParams.__repr__(self)
torch.nn.quantized.modules.linear.LinearPackedParams.__setstate__(self,state)
torch.nn.quantized.modules.linear.LinearPackedParams._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.linear.LinearPackedParams._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.linear.LinearPackedParams._weight_bias(self)
torch.nn.quantized.modules.linear.LinearPackedParams.forward(self,x)
torch.nn.quantized.modules.linear.LinearPackedParams.set_weight_bias(self,weight:torch.Tensor,bias:Optional[torch.Tensor])->None


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/functional_modules.py----------------------------------------
A:torch.nn.quantized.modules.functional_modules.self.activation_post_process->torch.nn.Identity()
A:torch.nn.quantized.modules.functional_modules.r->self.activation_post_process(r)
A:torch.nn.quantized.modules.functional_modules.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.functional_modules.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.functional_modules.self.scale->float(state_dict.pop(prefix + 'scale'))
A:torch.nn.quantized.modules.functional_modules.self.zero_point->int(state_dict.pop(prefix + 'zero_point'))
A:torch.nn.quantized.modules.functional_modules.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.functional_modules.new_mod->QFunctional()
A:torch.nn.quantized.modules.functional_modules.new_mod.scale->float(scale)
A:torch.nn.quantized.modules.functional_modules.new_mod.zero_point->int(zero_point)
torch.nn.quantized.FXFloatFunctional(torch.nn.Module)
torch.nn.quantized.FXFloatFunctional.add(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.FXFloatFunctional.add_relu(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.FXFloatFunctional.add_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.FXFloatFunctional.cat(self,x:List[Tensor],dim:int=0)->Tensor
torch.nn.quantized.FXFloatFunctional.forward(self,x)
torch.nn.quantized.FXFloatFunctional.mul(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.FXFloatFunctional.mul_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.FloatFunctional(self)
torch.nn.quantized.FloatFunctional.add(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.FloatFunctional.add_relu(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.FloatFunctional.add_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.FloatFunctional.cat(self,x:List[Tensor],dim:int=0)->Tensor
torch.nn.quantized.FloatFunctional.forward(self,x)
torch.nn.quantized.FloatFunctional.mul(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.FloatFunctional.mul_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.QFunctional(self)
torch.nn.quantized.QFunctional._get_name(self)
torch.nn.quantized.QFunctional._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.QFunctional._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.QFunctional.add(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.QFunctional.add_relu(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.QFunctional.add_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.QFunctional.cat(self,x:List[Tensor],dim:int=0)->Tensor
torch.nn.quantized.QFunctional.extra_repr(self)
torch.nn.quantized.QFunctional.forward(self,x)
torch.nn.quantized.QFunctional.from_float(cls,mod)
torch.nn.quantized.QFunctional.mul(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.QFunctional.mul_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.modules.functional_modules.FXFloatFunctional(torch.nn.Module)
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.add(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.add_relu(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.add_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.cat(self,x:List[Tensor],dim:int=0)->Tensor
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.mul(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.FXFloatFunctional.mul_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.modules.functional_modules.FloatFunctional(self)
torch.nn.quantized.modules.functional_modules.FloatFunctional.__init__(self)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.FloatFunctional.add_relu(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.FloatFunctional.add_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.modules.functional_modules.FloatFunctional.cat(self,x:List[Tensor],dim:int=0)->Tensor
torch.nn.quantized.modules.functional_modules.FloatFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.FloatFunctional.mul(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.FloatFunctional.mul_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.modules.functional_modules.QFunctional(self)
torch.nn.quantized.modules.functional_modules.QFunctional.__init__(self)
torch.nn.quantized.modules.functional_modules.QFunctional._get_name(self)
torch.nn.quantized.modules.functional_modules.QFunctional._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.functional_modules.QFunctional._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.functional_modules.QFunctional.add(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.QFunctional.add_relu(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.QFunctional.add_scalar(self,x:Tensor,y:float)->Tensor
torch.nn.quantized.modules.functional_modules.QFunctional.cat(self,x:List[Tensor],dim:int=0)->Tensor
torch.nn.quantized.modules.functional_modules.QFunctional.extra_repr(self)
torch.nn.quantized.modules.functional_modules.QFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.QFunctional.from_float(cls,mod)
torch.nn.quantized.modules.functional_modules.QFunctional.mul(self,x:Tensor,y:Tensor)->Tensor
torch.nn.quantized.modules.functional_modules.QFunctional.mul_scalar(self,x:Tensor,y:float)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/batchnorm.py----------------------------------------
A:torch.nn.quantized.modules.batchnorm.(scale, zero_point)->activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.batchnorm.new_mod->cls(mod.num_features, mod.eps)
torch.nn.quantized.BatchNorm2d(self,num_features,eps=1e-05,momentum=0.1,device=None,dtype=None)
torch.nn.quantized.BatchNorm2d._get_name(self)
torch.nn.quantized.BatchNorm2d.forward(self,input)
torch.nn.quantized.BatchNorm2d.from_float(cls,mod)
torch.nn.quantized.BatchNorm3d(self,num_features,eps=1e-05,momentum=0.1,device=None,dtype=None)
torch.nn.quantized.BatchNorm3d._get_name(self)
torch.nn.quantized.BatchNorm3d.forward(self,input)
torch.nn.quantized.BatchNorm3d.from_float(cls,mod)
torch.nn.quantized.modules.batchnorm.BatchNorm2d(self,num_features,eps=1e-05,momentum=0.1,device=None,dtype=None)
torch.nn.quantized.modules.batchnorm.BatchNorm2d.__init__(self,num_features,eps=1e-05,momentum=0.1,device=None,dtype=None)
torch.nn.quantized.modules.batchnorm.BatchNorm2d._get_name(self)
torch.nn.quantized.modules.batchnorm.BatchNorm2d.forward(self,input)
torch.nn.quantized.modules.batchnorm.BatchNorm2d.from_float(cls,mod)
torch.nn.quantized.modules.batchnorm.BatchNorm3d(self,num_features,eps=1e-05,momentum=0.1,device=None,dtype=None)
torch.nn.quantized.modules.batchnorm.BatchNorm3d.__init__(self,num_features,eps=1e-05,momentum=0.1,device=None,dtype=None)
torch.nn.quantized.modules.batchnorm.BatchNorm3d._get_name(self)
torch.nn.quantized.modules.batchnorm.BatchNorm3d.forward(self,input)
torch.nn.quantized.modules.batchnorm.BatchNorm3d.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/__init__.py----------------------------------------
A:torch.nn.quantized.modules.__init__.factory_kwargs->torch.nn.factory_kwargs(factory_kwargs)
A:torch.nn.quantized.modules.__init__.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
torch.nn.quantized.__init__.DeQuantize(self)
torch.nn.quantized.__init__.DeQuantize.forward(self,Xq)
torch.nn.quantized.__init__.DeQuantize.from_float(mod)
torch.nn.quantized.__init__.Quantize(self,scale,zero_point,dtype,factory_kwargs=None)
torch.nn.quantized.__init__.Quantize.extra_repr(self)
torch.nn.quantized.__init__.Quantize.forward(self,X)
torch.nn.quantized.__init__.Quantize.from_float(mod)
torch.nn.quantized.modules.__init__.DeQuantize(self)
torch.nn.quantized.modules.__init__.DeQuantize.__init__(self)
torch.nn.quantized.modules.__init__.DeQuantize.forward(self,Xq)
torch.nn.quantized.modules.__init__.DeQuantize.from_float(mod)
torch.nn.quantized.modules.__init__.Quantize(self,scale,zero_point,dtype,factory_kwargs=None)
torch.nn.quantized.modules.__init__.Quantize.__init__(self,scale,zero_point,dtype,factory_kwargs=None)
torch.nn.quantized.modules.__init__.Quantize.extra_repr(self)
torch.nn.quantized.modules.__init__.Quantize.forward(self,X)
torch.nn.quantized.modules.__init__.Quantize.from_float(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/normalization.py----------------------------------------
A:torch.nn.quantized.modules.normalization.(scale, zero_point)->mod.activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.normalization.new_mod->cls(mod.num_features, mod.weight, mod.bias, float(scale), int(zero_point), mod.eps, mod.affine)
torch.nn.quantized.GroupNorm(self,num_groups,num_channels,weight,bias,scale,zero_point,eps=1e-05,affine=True,device=None,dtype=None)
torch.nn.quantized.GroupNorm._get_name(self)
torch.nn.quantized.GroupNorm.forward(self,input)
torch.nn.quantized.GroupNorm.from_float(cls,mod)
torch.nn.quantized.InstanceNorm1d(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.InstanceNorm1d._get_name(self)
torch.nn.quantized.InstanceNorm1d.forward(self,input)
torch.nn.quantized.InstanceNorm1d.from_float(cls,mod)
torch.nn.quantized.InstanceNorm2d(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.InstanceNorm2d._get_name(self)
torch.nn.quantized.InstanceNorm2d.forward(self,input)
torch.nn.quantized.InstanceNorm2d.from_float(cls,mod)
torch.nn.quantized.InstanceNorm3d(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.InstanceNorm3d._get_name(self)
torch.nn.quantized.InstanceNorm3d.forward(self,input)
torch.nn.quantized.InstanceNorm3d.from_float(cls,mod)
torch.nn.quantized.LayerNorm(self,normalized_shape,weight,bias,scale,zero_point,eps=1e-05,elementwise_affine=True,device=None,dtype=None)
torch.nn.quantized.LayerNorm._get_name(self)
torch.nn.quantized.LayerNorm.forward(self,input)
torch.nn.quantized.LayerNorm.from_float(cls,mod)
torch.nn.quantized.modules.normalization.GroupNorm(self,num_groups,num_channels,weight,bias,scale,zero_point,eps=1e-05,affine=True,device=None,dtype=None)
torch.nn.quantized.modules.normalization.GroupNorm.__init__(self,num_groups,num_channels,weight,bias,scale,zero_point,eps=1e-05,affine=True,device=None,dtype=None)
torch.nn.quantized.modules.normalization.GroupNorm._get_name(self)
torch.nn.quantized.modules.normalization.GroupNorm.forward(self,input)
torch.nn.quantized.modules.normalization.GroupNorm.from_float(cls,mod)
torch.nn.quantized.modules.normalization.InstanceNorm1d(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.modules.normalization.InstanceNorm1d.__init__(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.modules.normalization.InstanceNorm1d._get_name(self)
torch.nn.quantized.modules.normalization.InstanceNorm1d.forward(self,input)
torch.nn.quantized.modules.normalization.InstanceNorm1d.from_float(cls,mod)
torch.nn.quantized.modules.normalization.InstanceNorm2d(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.modules.normalization.InstanceNorm2d.__init__(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.modules.normalization.InstanceNorm2d._get_name(self)
torch.nn.quantized.modules.normalization.InstanceNorm2d.forward(self,input)
torch.nn.quantized.modules.normalization.InstanceNorm2d.from_float(cls,mod)
torch.nn.quantized.modules.normalization.InstanceNorm3d(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.modules.normalization.InstanceNorm3d.__init__(self,num_features,weight,bias,scale,zero_point,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False,device=None,dtype=None)
torch.nn.quantized.modules.normalization.InstanceNorm3d._get_name(self)
torch.nn.quantized.modules.normalization.InstanceNorm3d.forward(self,input)
torch.nn.quantized.modules.normalization.InstanceNorm3d.from_float(cls,mod)
torch.nn.quantized.modules.normalization.LayerNorm(self,normalized_shape,weight,bias,scale,zero_point,eps=1e-05,elementwise_affine=True,device=None,dtype=None)
torch.nn.quantized.modules.normalization.LayerNorm.__init__(self,normalized_shape,weight,bias,scale,zero_point,eps=1e-05,elementwise_affine=True,device=None,dtype=None)
torch.nn.quantized.modules.normalization.LayerNorm._get_name(self)
torch.nn.quantized.modules.normalization.LayerNorm.forward(self,input)
torch.nn.quantized.modules.normalization.LayerNorm.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/modules/conv.py----------------------------------------
A:torch.nn.quantized.modules.conv.N->len(padding)
A:torch.nn.quantized.modules.conv.qweight->_quantize_weight(mod.weight.float(), weight_post_process)
A:torch.nn.quantized.modules.conv.(w, b)->torch.ops.quantized.conv3d_unpack(self._packed_params)
A:torch.nn.quantized.modules.conv.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.conv.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.conv.self.scale->float(state_dict[prefix + 'scale'])
A:torch.nn.quantized.modules.conv.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.nn.quantized.modules.conv.new_instance->type(self).__new__(type(self))
A:torch.nn.quantized.modules.conv.state->self.__getstate__()
A:torch.nn.quantized.modules.conv.weight_post_process->mod.qconfig.weight()
A:torch.nn.quantized.modules.conv.qconv->cls(mod.in_channels, mod.out_channels, mod.kernel_size, mod.stride, mod.padding, mod.output_padding, mod.groups, mod.bias is not None, mod.dilation, mod.padding_mode)
A:torch.nn.quantized.modules.conv.(act_scale, act_zp)->mod.activation_post_process.calculate_qparams()
A:torch.nn.quantized.modules.conv.qconv.scale->float(act_scale)
A:torch.nn.quantized.modules.conv.qconv.zero_point->int(act_zp)
A:torch.nn.quantized.modules.conv.(mod.weight, mod.bias)->fuse_conv_bn_weights(mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var, mod.bn.eps, mod.bn.weight, mod.bn.bias)
A:torch.nn.quantized.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.quantized.modules.conv.stride->_triple(stride)
A:torch.nn.quantized.modules.conv.dilation->_triple(dilation)
A:torch.nn.quantized.modules.conv.self._packed_params->torch.ops.quantized.conv_transpose3d_prepack(w, b, self.stride, self.padding, self.output_padding, self.dilation, self.groups)
A:torch.nn.quantized.modules.conv._reversed_padding_repeated_twice->_reverse_repeat_padding(self.padding)
A:torch.nn.quantized.modules.conv.input->torch.nn.functional.pad(input, _reversed_padding_repeated_twice, mode=self.padding_mode)
A:torch.nn.quantized.modules.conv.padding->_triple(padding)
A:torch.nn.quantized.modules.conv.MOD->TypeVar('MOD', bound=nn.modules.conv._ConvNd)
A:torch.nn.quantized.modules.conv.res->torch.jit.annotate(List[int], [])
A:torch.nn.quantized.modules.conv.output_padding->_triple(output_padding)
A:torch.nn.quantized.modules.conv.(w, _)->self._weight_bias()
A:torch.nn.quantized.modules.conv.(_, b)->self._weight_bias()
torch.nn.quantized.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.quantized.Conv1d._get_name(self)
torch.nn.quantized.Conv1d._weight_bias(self)
torch.nn.quantized.Conv1d.bias(self)
torch.nn.quantized.Conv1d.forward(self,input)
torch.nn.quantized.Conv1d.from_float(cls,mod)
torch.nn.quantized.Conv1d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.Conv1d.weight(self)
torch.nn.quantized.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.Conv2d._get_name(self)
torch.nn.quantized.Conv2d._weight_bias(self)
torch.nn.quantized.Conv2d.bias(self)
torch.nn.quantized.Conv2d.forward(self,input)
torch.nn.quantized.Conv2d.from_float(cls,mod)
torch.nn.quantized.Conv2d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.Conv2d.weight(self)
torch.nn.quantized.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.Conv3d._get_name(self)
torch.nn.quantized.Conv3d._weight_bias(self)
torch.nn.quantized.Conv3d.bias(self)
torch.nn.quantized.Conv3d.forward(self,input)
torch.nn.quantized.Conv3d.from_float(cls,mod)
torch.nn.quantized.Conv3d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.Conv3d.weight(self)
torch.nn.quantized.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.ConvTranspose1d._get_name(self)
torch.nn.quantized.ConvTranspose1d._weight_bias(self)
torch.nn.quantized.ConvTranspose1d.bias(self)
torch.nn.quantized.ConvTranspose1d.forward(self,input)
torch.nn.quantized.ConvTranspose1d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.ConvTranspose1d.weight(self)
torch.nn.quantized.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.ConvTranspose2d._get_name(self)
torch.nn.quantized.ConvTranspose2d._weight_bias(self)
torch.nn.quantized.ConvTranspose2d.bias(self)
torch.nn.quantized.ConvTranspose2d.forward(self,input)
torch.nn.quantized.ConvTranspose2d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.ConvTranspose2d.weight(self)
torch.nn.quantized.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.ConvTranspose3d._get_name(self)
torch.nn.quantized.ConvTranspose3d._weight_bias(self)
torch.nn.quantized.ConvTranspose3d.bias(self)
torch.nn.quantized.ConvTranspose3d.forward(self,input)
torch.nn.quantized.ConvTranspose3d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.ConvTranspose3d.weight(self)
torch.nn.quantized._ConvNd(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized._ConvNd.__copy__(self)
torch.nn.quantized._ConvNd.__deepcopy__(self,memo)
torch.nn.quantized._ConvNd.__getstate__(self)
torch.nn.quantized._ConvNd.__setstate__(self,state)
torch.nn.quantized._ConvNd._init(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode='zeros',device=None,dtype=None)->None
torch.nn.quantized._ConvNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized._ConvNd._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized._ConvNd._weight_bias(self)
torch.nn.quantized._ConvNd.bias(self)
torch.nn.quantized._ConvNd.extra_repr(self)
torch.nn.quantized._ConvNd.from_float(cls,mod)
torch.nn.quantized._ConvNd.from_reference(cls,ref_qconv,output_scale,output_zero_point)
torch.nn.quantized._ConvNd.get_qconv(cls,mod,activation_post_process,weight_post_process=None)
torch.nn.quantized._ConvNd.set_weight_bias(self,qweight,bias_float)
torch.nn.quantized.conv._ConvTransposeNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,device=None,dtype=None)
torch.nn.quantized.conv._ConvTransposeNd._input_padding(self,kernel_size:List[int],dilation:List[int],padding:List[int])->List[int]
torch.nn.quantized.conv._ConvTransposeNd.from_float(cls,mod)
torch.nn.quantized.conv._reverse_repeat_padding(padding:List[int])->List[int]
torch.nn.quantized.modules.conv.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.Conv1d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.Conv1d._get_name(self)
torch.nn.quantized.modules.conv.Conv1d._weight_bias(self)
torch.nn.quantized.modules.conv.Conv1d.bias(self)
torch.nn.quantized.modules.conv.Conv1d.forward(self,input)
torch.nn.quantized.modules.conv.Conv1d.from_float(cls,mod)
torch.nn.quantized.modules.conv.Conv1d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.conv.Conv1d.weight(self)
torch.nn.quantized.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.Conv2d._get_name(self)
torch.nn.quantized.modules.conv.Conv2d._weight_bias(self)
torch.nn.quantized.modules.conv.Conv2d.bias(self)
torch.nn.quantized.modules.conv.Conv2d.forward(self,input)
torch.nn.quantized.modules.conv.Conv2d.from_float(cls,mod)
torch.nn.quantized.modules.conv.Conv2d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.conv.Conv2d.weight(self)
torch.nn.quantized.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.Conv3d._get_name(self)
torch.nn.quantized.modules.conv.Conv3d._weight_bias(self)
torch.nn.quantized.modules.conv.Conv3d.bias(self)
torch.nn.quantized.modules.conv.Conv3d.forward(self,input)
torch.nn.quantized.modules.conv.Conv3d.from_float(cls,mod)
torch.nn.quantized.modules.conv.Conv3d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.conv.Conv3d.weight(self)
torch.nn.quantized.modules.conv.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.ConvTranspose1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.ConvTranspose1d._get_name(self)
torch.nn.quantized.modules.conv.ConvTranspose1d._weight_bias(self)
torch.nn.quantized.modules.conv.ConvTranspose1d.bias(self)
torch.nn.quantized.modules.conv.ConvTranspose1d.forward(self,input)
torch.nn.quantized.modules.conv.ConvTranspose1d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.conv.ConvTranspose1d.weight(self)
torch.nn.quantized.modules.conv.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.ConvTranspose2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.ConvTranspose2d._get_name(self)
torch.nn.quantized.modules.conv.ConvTranspose2d._weight_bias(self)
torch.nn.quantized.modules.conv.ConvTranspose2d.bias(self)
torch.nn.quantized.modules.conv.ConvTranspose2d.forward(self,input)
torch.nn.quantized.modules.conv.ConvTranspose2d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.conv.ConvTranspose2d.weight(self)
torch.nn.quantized.modules.conv.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.ConvTranspose3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv.ConvTranspose3d._get_name(self)
torch.nn.quantized.modules.conv.ConvTranspose3d._weight_bias(self)
torch.nn.quantized.modules.conv.ConvTranspose3d.bias(self)
torch.nn.quantized.modules.conv.ConvTranspose3d.forward(self,input)
torch.nn.quantized.modules.conv.ConvTranspose3d.set_weight_bias(self,w:torch.Tensor,b:Optional[torch.Tensor])->None
torch.nn.quantized.modules.conv.ConvTranspose3d.weight(self)
torch.nn.quantized.modules.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv._ConvNd.__copy__(self)
torch.nn.quantized.modules.conv._ConvNd.__deepcopy__(self,memo)
torch.nn.quantized.modules.conv._ConvNd.__getstate__(self)
torch.nn.quantized.modules.conv._ConvNd.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.modules.conv._ConvNd.__setstate__(self,state)
torch.nn.quantized.modules.conv._ConvNd._init(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode='zeros',device=None,dtype=None)->None
torch.nn.quantized.modules.conv._ConvNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.conv._ConvNd._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.conv._ConvNd._weight_bias(self)
torch.nn.quantized.modules.conv._ConvNd.bias(self)
torch.nn.quantized.modules.conv._ConvNd.extra_repr(self)
torch.nn.quantized.modules.conv._ConvNd.from_float(cls,mod)
torch.nn.quantized.modules.conv._ConvNd.from_reference(cls,ref_qconv,output_scale,output_zero_point)
torch.nn.quantized.modules.conv._ConvNd.get_qconv(cls,mod,activation_post_process,weight_post_process=None)
torch.nn.quantized.modules.conv._ConvNd.set_weight_bias(self,qweight,bias_float)
torch.nn.quantized.modules.conv._ConvTransposeNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,device=None,dtype=None)
torch.nn.quantized.modules.conv._ConvTransposeNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,device=None,dtype=None)
torch.nn.quantized.modules.conv._ConvTransposeNd._input_padding(self,kernel_size:List[int],dilation:List[int],padding:List[int])->List[int]
torch.nn.quantized.modules.conv._ConvTransposeNd.from_float(cls,mod)
torch.nn.quantized.modules.conv._reverse_repeat_padding(padding:List[int])->List[int]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/_reference/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/_reference/modules/utils.py----------------------------------------
A:torch.nn.quantized._reference.modules.utils.weight->torch.quantize_per_channel(weight, weight_scale, weight_zero_point, weight_axis.item(), weight_dtype)
A:torch.nn.quantized._reference.modules.utils.weight_quant->_quantize_weight(weight, weight_qscheme, weight_dtype, weight_scale, weight_zero_point, weight_axis)
A:torch.nn.quantized._reference.modules.utils.weight_dequant->_quantize_weight(weight, weight_qscheme, weight_dtype, weight_scale, weight_zero_point, weight_axis).dequantize()
torch.nn.quantized._reference.modules.utils._get_weight_qparam_keys(state_dict:Dict[str,Any],prefix:str)
torch.nn.quantized._reference.modules.utils._quantize_and_dequantize_weight(weight:torch.Tensor,weight_qscheme:torch.qscheme,weight_dtype:torch.dtype,weight_scale:torch.Tensor,weight_zero_point:torch.Tensor,weight_axis:torch.Tensor)
torch.nn.quantized._reference.modules.utils._quantize_weight(weight:torch.Tensor,weight_qscheme:torch.qscheme,weight_dtype:torch.dtype,weight_scale:torch.Tensor,weight_zero_point:torch.Tensor,weight_axis:torch.Tensor)
torch.nn.quantized._reference.modules.utils._save_weight_qparams(destination,prefix,weight_qscheme,weight_dtype,weight_scale,weight_zero_point,weight_axis)
torch.nn.quantized._reference.utils._get_weight_qparam_keys(state_dict:Dict[str,Any],prefix:str)
torch.nn.quantized._reference.utils._quantize_and_dequantize_weight(weight:torch.Tensor,weight_qscheme:torch.qscheme,weight_dtype:torch.dtype,weight_scale:torch.Tensor,weight_zero_point:torch.Tensor,weight_axis:torch.Tensor)
torch.nn.quantized._reference.utils._quantize_weight(weight:torch.Tensor,weight_qscheme:torch.qscheme,weight_dtype:torch.dtype,weight_scale:torch.Tensor,weight_zero_point:torch.Tensor,weight_axis:torch.Tensor)
torch.nn.quantized._reference.utils._save_weight_qparams(destination,prefix,weight_qscheme,weight_dtype,weight_scale,weight_zero_point,weight_axis)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/_reference/modules/linear.py----------------------------------------
A:torch.nn.quantized._reference.modules.linear.weight_dequant->self.get_weight()
A:torch.nn.quantized._reference.modules.linear.result->torch.nn.functional.linear(x, weight_dequant, self.bias)
A:torch.nn.quantized._reference.modules.linear.qref_linear->Linear(float_linear.in_features, float_linear.out_features, float_linear.bias is not None, device=float_linear.weight.device, dtype=float_linear.weight.dtype, weight_qparams=weight_qparams)
A:torch.nn.quantized._reference.modules.linear.qref_linear.weight->torch.nn.Parameter(float_linear.weight.detach())
A:torch.nn.quantized._reference.modules.linear.qref_linear.bias->torch.nn.Parameter(float_linear.bias.detach())
torch.nn.quantized._reference.Linear(self,in_features:int,out_features:int,bias_:bool=True,device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.Linear._get_name(self)
torch.nn.quantized._reference.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized._reference.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized._reference.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.Linear.from_float(cls,float_linear,weight_qparams)
torch.nn.quantized._reference.Linear.get_quantized_weight(self)
torch.nn.quantized._reference.Linear.get_weight(self)
torch.nn.quantized._reference.modules.linear.Linear(self,in_features:int,out_features:int,bias_:bool=True,device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.linear.Linear.__init__(self,in_features:int,out_features:int,bias_:bool=True,device:Optional[torch.device]=None,dtype:Optional[torch.dtype]=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.linear.Linear._get_name(self)
torch.nn.quantized._reference.modules.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized._reference.modules.linear.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized._reference.modules.linear.Linear.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.modules.linear.Linear.from_float(cls,float_linear,weight_qparams)
torch.nn.quantized._reference.modules.linear.Linear.get_quantized_weight(self)
torch.nn.quantized._reference.modules.linear.Linear.get_weight(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/_reference/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/_reference/modules/conv.py----------------------------------------
A:torch.nn.quantized._reference.modules.conv.qref_conv->cls(float_conv.in_channels, float_conv.out_channels, float_conv.kernel_size, float_conv.stride, float_conv.padding, float_conv.dilation, float_conv.groups, float_conv.bias is not None, float_conv.padding_mode, device=float_conv.weight.device, dtype=float_conv.weight.dtype, weight_qparams=weight_qparams)
A:torch.nn.quantized._reference.modules.conv.qref_conv.weight->torch.nn.Parameter(float_conv.weight.detach())
A:torch.nn.quantized._reference.modules.conv.qref_conv.bias->torch.nn.Parameter(float_conv.bias.detach())
A:torch.nn.quantized._reference.modules.conv.weight_dequant->self.get_weight()
A:torch.nn.quantized._reference.modules.conv.result->torch.nn.functional.conv3d(x, weight_dequant, self.bias, self.stride, self.padding, self.dilation, self.groups)
torch.nn.quantized._reference.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.Conv1d._get_name(self)
torch.nn.quantized._reference.Conv1d.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.Conv1d.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.Conv2d._get_name(self)
torch.nn.quantized._reference.Conv2d.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.Conv2d.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.Conv3d._get_name(self)
torch.nn.quantized._reference.Conv3d.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.Conv3d.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.conv._ConvNd(torch.nn.modules.conv._ConvNd)
torch.nn.quantized._reference.conv._ConvNd._init_weight_qparams(self,weight_qparams,device)
torch.nn.quantized._reference.conv._ConvNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized._reference.conv._ConvNd._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized._reference.conv._ConvNd.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.conv._ConvNd.get_quantized_weight(self)
torch.nn.quantized._reference.conv._ConvNd.get_weight(self)
torch.nn.quantized._reference.modules.conv.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.conv.Conv1d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.conv.Conv1d._get_name(self)
torch.nn.quantized._reference.modules.conv.Conv1d.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.modules.conv.Conv1d.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.conv.Conv2d._get_name(self)
torch.nn.quantized._reference.modules.conv.Conv2d.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.modules.conv.Conv2d.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None,weight_qparams:Optional[Dict[str,Any]]=None)
torch.nn.quantized._reference.modules.conv.Conv3d._get_name(self)
torch.nn.quantized._reference.modules.conv.Conv3d.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.quantized._reference.modules.conv.Conv3d.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.modules.conv._ConvNd(torch.nn.modules.conv._ConvNd)
torch.nn.quantized._reference.modules.conv._ConvNd._init_weight_qparams(self,weight_qparams,device)
torch.nn.quantized._reference.modules.conv._ConvNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized._reference.modules.conv._ConvNd._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized._reference.modules.conv._ConvNd.from_float(cls,float_conv,weight_qparams)
torch.nn.quantized._reference.modules.conv._ConvNd.get_quantized_weight(self)
torch.nn.quantized._reference.modules.conv._ConvNd.get_weight(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/dynamic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/dynamic/modules/linear.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.linear.Y->torch.ops.quantized.linear_dynamic_fp16(x, self._packed_params._packed_params)
A:torch.nn.quantized.dynamic.modules.linear.extra_repr_str->'in_features={}, out_features={}, dtype={}'.format(self.in_features, self.out_features, self._packed_params.dtype)
A:torch.nn.quantized.dynamic.modules.linear.version->local_metadata.get('version', None)
A:torch.nn.quantized.dynamic.modules.linear.weight_observer->torch.ao.quantization.qconfig.default_dynamic_qconfig.weight()
A:torch.nn.quantized.dynamic.modules.linear.qweight->mod.weight.float()
A:torch.nn.quantized.dynamic.modules.linear.qlinear->cls(mod.in_features, mod.out_features, dtype=dtype)
torch.nn.quantized.dynamic.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.Linear._get_name(self)
torch.nn.quantized.dynamic.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.Linear.extra_repr(self)
torch.nn.quantized.dynamic.Linear.forward(self,x)
torch.nn.quantized.dynamic.Linear.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.linear.Linear(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.linear.Linear.__init__(self,in_features,out_features,bias_=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.linear.Linear._get_name(self)
torch.nn.quantized.dynamic.modules.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.modules.linear.Linear.extra_repr(self)
torch.nn.quantized.dynamic.modules.linear.Linear.forward(self,x)
torch.nn.quantized.dynamic.modules.linear.Linear.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/dynamic/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/dynamic/modules/rnn.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.rnn.self.dropout->float(dropout)
A:torch.nn.quantized.dynamic.modules.rnn.w_ih->torch.quantize_per_tensor(w_ih, scale=0.1, zero_point=0, dtype=torch.qint8)
A:torch.nn.quantized.dynamic.modules.rnn.w_hh->torch.quantize_per_tensor(w_hh, scale=0.1, zero_point=0, dtype=torch.qint8)
A:torch.nn.quantized.dynamic.modules.rnn.b_ih->torch.randn(gate_size).to(torch.float)
A:torch.nn.quantized.dynamic.modules.rnn.b_hh->torch.randn(gate_size).to(torch.float)
A:torch.nn.quantized.dynamic.modules.rnn.packed_ih->torch.ops.quantized.linear_prepack_fp16(weight_ih.float(), bias_ih)
A:torch.nn.quantized.dynamic.modules.rnn.packed_hh->torch.ops.quantized.linear_prepack_fp16(weight_hh.float(), bias_hh)
A:torch.nn.quantized.dynamic.modules.rnn.cell_params->torch.ops.quantized.make_quantized_cell_params_fp16(packed_ih, packed_hh)
A:torch.nn.quantized.dynamic.modules.rnn.self._all_weight_values->torch.nn.ModuleList(_all_weight_values)
A:torch.nn.quantized.dynamic.modules.rnn.extra_repr->self.extra_repr()
A:torch.nn.quantized.dynamic.modules.rnn.extra_lines->self.extra_repr().split('\n')
A:torch.nn.quantized.dynamic.modules.rnn.mod_str->torch.nn.modules.module._addindent(mod_str, 2)
A:torch.nn.quantized.dynamic.modules.rnn.mini_batch->int(batch_sizes[0])
A:torch.nn.quantized.dynamic.modules.rnn.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.nn.quantized.dynamic.modules.rnn.version->local_metadata.get('version', None)
A:torch.nn.quantized.dynamic.modules.rnn.qRNNBase->GRU(mod.input_size, mod.hidden_size, mod.num_layers, mod.bias, mod.batch_first, mod.dropout, mod.bidirectional, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.weight_name->'weight_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.nn.quantized.dynamic.modules.rnn.bias_name->'bias_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.nn.quantized.dynamic.modules.rnn.weight->getattr(mod, weight_name)
A:torch.nn.quantized.dynamic.modules.rnn.bias->getattr(mod, bias_name)
A:torch.nn.quantized.dynamic.modules.rnn.(weight_ih, bias_ih)->retrieve_weight_bias('ih')
A:torch.nn.quantized.dynamic.modules.rnn.(weight_hh, bias_hh)->retrieve_weight_bias('hh')
A:torch.nn.quantized.dynamic.modules.rnn.weight_observer->weight_observer_method()
A:torch.nn.quantized.dynamic.modules.rnn.qweight->_quantize_weight(weight.float(), weight_observer)
A:torch.nn.quantized.dynamic.modules.rnn.packed_weight->torch.ops.quantized.linear_prepack_fp16(weight.float(), bias)
A:torch.nn.quantized.dynamic.modules.rnn.qRNNBase._all_weight_values->torch.nn.ModuleList(_all_weight_values)
A:torch.nn.quantized.dynamic.modules.rnn.key_name1->'bias_ih_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)
A:torch.nn.quantized.dynamic.modules.rnn.key_name2->'bias_hh_l{layer_idx}{suffix}'.format(layer_idx=layer, suffix=suffix)
A:torch.nn.quantized.dynamic.modules.rnn.zeros->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.quantized.dynamic.modules.rnn.hx->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.quantized.dynamic.modules.rnn.result->torch.quantized_gru(input, batch_sizes, hx, _all_params, self.bias, self.num_layers, self.dropout, self.training, self.bidirectional)
A:torch.nn.quantized.dynamic.modules.rnn.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.max_batch_size->int(max_batch_size)
A:torch.nn.quantized.dynamic.modules.rnn.(output_, hidden)->self.forward_impl(input_, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.output->PackedSequence(output_, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.self.bias_ih->torch.randn(num_chunks * hidden_size).to(dtype=torch.float)
A:torch.nn.quantized.dynamic.modules.rnn.self.bias_hh->torch.randn(num_chunks * hidden_size).to(dtype=torch.float)
A:torch.nn.quantized.dynamic.modules.rnn.weight_ih->torch.quantize_per_tensor(weight_ih, scale=1, zero_point=0, dtype=torch.qint8)
A:torch.nn.quantized.dynamic.modules.rnn.weight_hh->torch.quantize_per_tensor(weight_hh, scale=1, zero_point=0, dtype=torch.qint8)
A:torch.nn.quantized.dynamic.modules.rnn.packed_weight_ih->torch.ops.quantized.linear_prepack_fp16(weight_ih, self.bias_ih)
A:torch.nn.quantized.dynamic.modules.rnn.packed_weight_hh->torch.ops.quantized.linear_prepack_fp16(weight_hh, self.bias_hh)
A:torch.nn.quantized.dynamic.modules.rnn.qRNNCellBase->RNNCell(mod.input_size, mod.hidden_size, bias=mod.bias, nonlinearity=mod.nonlinearity, dtype=dtype)
A:torch.nn.quantized.dynamic.modules.rnn.qRNNCellBase._packed_weight_ih->process_weights(mod.weight_ih, mod.bias_ih, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.qRNNCellBase._packed_weight_hh->process_weights(mod.weight_hh, mod.bias_hh, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.self._packed_weight_ih->state_dict.pop(prefix + '_packed_weight_ih')
A:torch.nn.quantized.dynamic.modules.rnn.self._packed_weight_hh->state_dict.pop(prefix + '_packed_weight_hh')
A:torch.nn.quantized.dynamic.modules.rnn.ret->torch.ops.quantized.quantized_rnn_relu_cell_dynamic(input, hx, self._packed_weight_ih, self._packed_weight_hh, self.bias_ih, self.bias_hh)
torch.nn.quantized.dynamic.GRU(self,*args,**kwargs)
torch.nn.quantized.dynamic.GRU._get_name(self)
torch.nn.quantized.dynamic.GRU.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.GRU.forward(self,input,hx=None)
torch.nn.quantized.dynamic.GRU.forward_impl(self,input:Tensor,hx:Optional[Tensor],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.GRU.forward_packed(self,input:PackedSequence,hx:Optional[Tensor]=None)->Tuple[PackedSequence, Tensor]
torch.nn.quantized.dynamic.GRU.forward_tensor(self,input:Tensor,hx:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.GRU.from_float(cls,mod)
torch.nn.quantized.dynamic.GRU.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])->Tensor
torch.nn.quantized.dynamic.GRUCell(self,input_size,hidden_size,bias=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.GRUCell._get_name(self)
torch.nn.quantized.dynamic.GRUCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.quantized.dynamic.GRUCell.from_float(cls,mod)
torch.nn.quantized.dynamic.LSTM(self,*args,**kwargs)
torch.nn.quantized.dynamic.LSTM._get_name(self)
torch.nn.quantized.dynamic.LSTM.check_forward_args(self,input:Tensor,hidden:Tuple[Tensor,Tensor],batch_sizes:Optional[Tensor])->None
torch.nn.quantized.dynamic.LSTM.forward(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.forward_impl(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.quantized.dynamic.LSTM.forward_packed(self,input:PackedSequence,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[PackedSequence, Tuple[Tensor, Tensor]]
torch.nn.quantized.dynamic.LSTM.forward_tensor(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.quantized.dynamic.LSTM.from_float(cls,mod)
torch.nn.quantized.dynamic.LSTM.permute_hidden(self,hx:Tuple[Tensor,Tensor],permutation:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.LSTMCell(self,*args,**kwargs)
torch.nn.quantized.dynamic.LSTMCell._get_name(self)
torch.nn.quantized.dynamic.LSTMCell.forward(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.LSTMCell.from_float(cls,mod)
torch.nn.quantized.dynamic.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh',dtype=torch.qint8)
torch.nn.quantized.dynamic.RNNCell._get_name(self)
torch.nn.quantized.dynamic.RNNCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.quantized.dynamic.RNNCell.from_float(cls,mod)
torch.nn.quantized.dynamic.RNNCellBase(self,input_size,hidden_size,bias=True,num_chunks=4,dtype=torch.qint8)
torch.nn.quantized.dynamic.RNNCellBase._get_name(self)
torch.nn.quantized.dynamic.RNNCellBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.RNNCellBase._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.dynamic.RNNCellBase._weight_bias(self)
torch.nn.quantized.dynamic.RNNCellBase.check_forward_hidden(self,input:Tensor,hx:Tensor,hidden_label:str='')->None
torch.nn.quantized.dynamic.RNNCellBase.check_forward_input(self,input)
torch.nn.quantized.dynamic.RNNCellBase.extra_repr(self)
torch.nn.quantized.dynamic.RNNCellBase.from_float(cls,mod)
torch.nn.quantized.dynamic.RNNCellBase.get_bias(self)
torch.nn.quantized.dynamic.RNNCellBase.get_weight(self)
torch.nn.quantized.dynamic.modules.rnn.GRU(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.GRU.__init__(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.GRU._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.GRU.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.GRU.forward(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.GRU.forward_impl(self,input:Tensor,hx:Optional[Tensor],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.modules.rnn.GRU.forward_packed(self,input:PackedSequence,hx:Optional[Tensor]=None)->Tuple[PackedSequence, Tensor]
torch.nn.quantized.dynamic.modules.rnn.GRU.forward_tensor(self,input:Tensor,hx:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.modules.rnn.GRU.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.GRU.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])->Tensor
torch.nn.quantized.dynamic.modules.rnn.GRUCell(self,input_size,hidden_size,bias=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.GRUCell.__init__(self,input_size,hidden_size,bias=True,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.GRUCell._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.GRUCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.quantized.dynamic.modules.rnn.GRUCell.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTM._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.LSTM.check_forward_args(self,input:Tensor,hidden:Tuple[Tensor,Tensor],batch_sizes:Optional[Tensor])->None
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_impl(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]],batch_sizes:Optional[Tensor],max_batch_size:int,sorted_indices:Optional[Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_packed(self,input:PackedSequence,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[PackedSequence, Tuple[Tensor, Tensor]]
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_tensor(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tuple[Tensor, Tensor]]
torch.nn.quantized.dynamic.modules.rnn.LSTM.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.LSTM.permute_hidden(self,hx:Tuple[Tensor,Tensor],permutation:Optional[Tensor])->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.modules.rnn.LSTMCell(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTMCell.__init__(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTMCell._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.LSTMCell.forward(self,input:Tensor,hx:Optional[Tuple[Tensor,Tensor]]=None)->Tuple[Tensor, Tensor]
torch.nn.quantized.dynamic.modules.rnn.LSTMCell.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter(self,param)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter.__init__(self,param)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.modules.rnn.PackedParameter._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.dynamic.modules.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__init__(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__repr__(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.modules.rnn.RNNBase._weight_bias(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_forward_args(self,input:Tensor,hidden:Tensor,batch_sizes:Optional[Tensor])->None
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_hidden_size(self,hx:Tensor,expected_hidden_size:Tuple[int,int,int],msg:str='Expectedhiddensize{},got{}')->None
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_input(self,input:Tensor,batch_sizes:Optional[Tensor])->None
torch.nn.quantized.dynamic.modules.rnn.RNNBase.extra_repr(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.get_bias(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.get_expected_hidden_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.nn.quantized.dynamic.modules.rnn.RNNBase.get_weight(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])->Tensor
torch.nn.quantized.dynamic.modules.rnn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh',dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNCell.__init__(self,input_size,hidden_size,bias=True,nonlinearity='tanh',dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNCell._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.RNNCell.forward(self,input:Tensor,hx:Optional[Tensor]=None)->Tensor
torch.nn.quantized.dynamic.modules.rnn.RNNCell.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase(self,input_size,hidden_size,bias=True,num_chunks=4,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.__init__(self,input_size,hidden_size,bias=True,num_chunks=4,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase._weight_bias(self)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.check_forward_hidden(self,input:Tensor,hx:Tensor,hidden_label:str='')->None
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.check_forward_input(self,input)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.extra_repr(self)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.get_bias(self)
torch.nn.quantized.dynamic.modules.rnn.RNNCellBase.get_weight(self)
torch.nn.quantized.dynamic.modules.rnn.apply_permutation(tensor:Tensor,permutation:Tensor,dim:int=1)->Tensor
torch.nn.quantized.dynamic.rnn.PackedParameter(self,param)
torch.nn.quantized.dynamic.rnn.PackedParameter._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.rnn.PackedParameter._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.dynamic.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.rnn.RNNBase.__repr__(self)
torch.nn.quantized.dynamic.rnn.RNNBase._get_name(self)
torch.nn.quantized.dynamic.rnn.RNNBase._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.dynamic.rnn.RNNBase._weight_bias(self)
torch.nn.quantized.dynamic.rnn.RNNBase.check_forward_args(self,input:Tensor,hidden:Tensor,batch_sizes:Optional[Tensor])->None
torch.nn.quantized.dynamic.rnn.RNNBase.check_hidden_size(self,hx:Tensor,expected_hidden_size:Tuple[int,int,int],msg:str='Expectedhiddensize{},got{}')->None
torch.nn.quantized.dynamic.rnn.RNNBase.check_input(self,input:Tensor,batch_sizes:Optional[Tensor])->None
torch.nn.quantized.dynamic.rnn.RNNBase.extra_repr(self)
torch.nn.quantized.dynamic.rnn.RNNBase.from_float(cls,mod)
torch.nn.quantized.dynamic.rnn.RNNBase.get_bias(self)
torch.nn.quantized.dynamic.rnn.RNNBase.get_expected_hidden_size(self,input:Tensor,batch_sizes:Optional[Tensor])->Tuple[int, int, int]
torch.nn.quantized.dynamic.rnn.RNNBase.get_weight(self)
torch.nn.quantized.dynamic.rnn.RNNBase.permute_hidden(self,hx:Tensor,permutation:Optional[Tensor])->Tensor
torch.nn.quantized.dynamic.rnn.apply_permutation(tensor:Tensor,permutation:Tensor,dim:int=1)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/quantized/dynamic/modules/conv.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.quantized.dynamic.modules.conv.stride->_triple(stride)
A:torch.nn.quantized.dynamic.modules.conv.dilation->_triple(dilation)
A:torch.nn.quantized.dynamic.modules.conv._reversed_padding_repeated_twice->_reverse_repeat_padding(self.padding)
A:torch.nn.quantized.dynamic.modules.conv.input->torch.nn.functional.pad(input, _reversed_padding_repeated_twice, mode=self.padding_mode)
A:torch.nn.quantized.dynamic.modules.conv.padding->_triple(padding)
torch.nn.quantized.dynamic.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None,reduce_range=True)
torch.nn.quantized.dynamic.Conv1d._get_name(self)
torch.nn.quantized.dynamic.Conv1d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.Conv2d._get_name(self)
torch.nn.quantized.dynamic.Conv2d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.Conv3d._get_name(self)
torch.nn.quantized.dynamic.Conv3d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.ConvTranspose1d._get_name(self)
torch.nn.quantized.dynamic.ConvTranspose1d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.ConvTranspose2d._get_name(self)
torch.nn.quantized.dynamic.ConvTranspose2d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.ConvTranspose3d._get_name(self)
torch.nn.quantized.dynamic.ConvTranspose3d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.modules.conv.Conv1d(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None,reduce_range=True)
torch.nn.quantized.dynamic.modules.conv.Conv1d.__init__(self,in_channels:int,out_channels:int,kernel_size:_size_1_t,stride:_size_1_t=1,padding:_size_1_t=0,dilation:_size_1_t=1,groups:int=1,bias:bool=True,padding_mode:str='zeros',device=None,dtype=None,reduce_range=True)
torch.nn.quantized.dynamic.modules.conv.Conv1d._get_name(self)
torch.nn.quantized.dynamic.modules.conv.Conv1d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.Conv2d._get_name(self)
torch.nn.quantized.dynamic.modules.conv.Conv2d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.Conv3d._get_name(self)
torch.nn.quantized.dynamic.modules.conv.Conv3d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.modules.conv.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose1d._get_name(self)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose1d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.modules.conv.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose2d._get_name(self)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose2d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor
torch.nn.quantized.dynamic.modules.conv.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros',device=None,dtype=None)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose3d._get_name(self)
torch.nn.quantized.dynamic.modules.conv.ConvTranspose3d.forward(self,input:Tensor,reduce_range:bool=True)->Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/modules/fused.py----------------------------------------
torch.nn.intrinsic.BNReLU2d(self,batch_norm,relu)
torch.nn.intrinsic.BNReLU3d(self,batch_norm,relu)
torch.nn.intrinsic.ConvBn1d(self,conv,bn)
torch.nn.intrinsic.ConvBn2d(self,conv,bn)
torch.nn.intrinsic.ConvBn3d(self,conv,bn)
torch.nn.intrinsic.ConvBnReLU1d(self,conv,bn,relu)
torch.nn.intrinsic.ConvBnReLU2d(self,conv,bn,relu)
torch.nn.intrinsic.ConvBnReLU3d(self,conv,bn,relu)
torch.nn.intrinsic.ConvReLU1d(self,conv,relu)
torch.nn.intrinsic.ConvReLU2d(self,conv,relu)
torch.nn.intrinsic.ConvReLU3d(self,conv,relu)
torch.nn.intrinsic.LinearReLU(self,linear,relu)
torch.nn.intrinsic._FusedModule(torch.nn.Sequential)
torch.nn.intrinsic.modules.fused.BNReLU2d(self,batch_norm,relu)
torch.nn.intrinsic.modules.fused.BNReLU2d.__init__(self,batch_norm,relu)
torch.nn.intrinsic.modules.fused.BNReLU3d(self,batch_norm,relu)
torch.nn.intrinsic.modules.fused.BNReLU3d.__init__(self,batch_norm,relu)
torch.nn.intrinsic.modules.fused.ConvBn1d(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn1d.__init__(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn2d(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn2d.__init__(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn3d(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn3d.__init__(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBnReLU1d(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU1d.__init__(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU2d(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU2d.__init__(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU3d(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU3d.__init__(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvReLU1d(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU1d.__init__(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU2d(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU2d.__init__(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU3d(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU3d.__init__(self,conv,relu)
torch.nn.intrinsic.modules.fused.LinearReLU(self,linear,relu)
torch.nn.intrinsic.modules.fused.LinearReLU.__init__(self,linear,relu)
torch.nn.intrinsic.modules.fused._FusedModule(torch.nn.Sequential)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/qat/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/qat/modules/conv_fused.py----------------------------------------
A:torch.nn.intrinsic.qat.modules.conv_fused.MOD->TypeVar('MOD', bound=nn.modules.conv._ConvNd)
A:torch.nn.intrinsic.qat.modules.conv_fused.self.bn->_BN_CLASS_MAP[dim](out_channels, eps, momentum, True, True)
A:torch.nn.intrinsic.qat.modules.conv_fused.self.weight_fake_quant->self.qconfig.weight()
A:torch.nn.intrinsic.qat.modules.conv_fused.self.bias->Parameter(torch.empty(out_channels))
A:torch.nn.intrinsic.qat.modules.conv_fused.(fan_in, _)->torch.nn.init._calculate_fan_in_and_fan_out(self.weight)
A:torch.nn.intrinsic.qat.modules.conv_fused.running_std->torch.sqrt(self.bn.running_var + self.bn.eps)
A:torch.nn.intrinsic.qat.modules.conv_fused.scaled_weight->self.weight_fake_quant(self.weight * scale_factor.reshape(weight_shape))
A:torch.nn.intrinsic.qat.modules.conv_fused.zero_bias->torch.zeros(self.out_channels, device=scaled_weight.device)
A:torch.nn.intrinsic.qat.modules.conv_fused.conv->type(self)._FLOAT_CONV_MODULE(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, self.dilation, self.groups, self.bias is not None, self.padding_mode)
A:torch.nn.intrinsic.qat.modules.conv_fused.version->local_metadata.get('version', None)
A:torch.nn.intrinsic.qat.modules.conv_fused.qat_convbn->cls(conv.in_channels, conv.out_channels, conv.kernel_size, conv.stride, conv.padding, conv.dilation, conv.groups, conv.bias is not None, conv.padding_mode, bn.eps, bn.momentum, False, qconfig)
A:torch.nn.intrinsic.qat.modules.conv_fused.cls->type(self)
A:torch.nn.intrinsic.qat.modules.conv_fused.conv.weight->torch.nn.Parameter(self.weight.detach())
A:torch.nn.intrinsic.qat.modules.conv_fused.conv.bias->torch.nn.Parameter(self.bias.detach())
A:torch.nn.intrinsic.qat.modules.conv_fused.bn->type(self)._FLOAT_BN_MODULE(self.bn.num_features, self.bn.eps, self.bn.momentum, self.bn.affine, self.bn.track_running_stats)
A:torch.nn.intrinsic.qat.modules.conv_fused.bn.weight->Parameter(self.bn.weight.detach())
A:torch.nn.intrinsic.qat.modules.conv_fused.bn.bias->Parameter(self.bn.bias.detach())
A:torch.nn.intrinsic.qat.modules.conv_fused.relu->type(self)._FLOAT_RELU_MODULE()
A:torch.nn.intrinsic.qat.modules.conv_fused.result->type(self)._FLOAT_MODULE(*modules)
A:torch.nn.intrinsic.qat.modules.conv_fused.kernel_size->_triple(kernel_size)
A:torch.nn.intrinsic.qat.modules.conv_fused.stride->_triple(stride)
A:torch.nn.intrinsic.qat.modules.conv_fused.padding->_triple(padding)
A:torch.nn.intrinsic.qat.modules.conv_fused.dilation->_triple(dilation)
torch.nn.intrinsic.qat.ConvBn1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBn2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBn3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU1d.forward(self,input)
torch.nn.intrinsic.qat.ConvBnReLU1d.from_float(cls,mod)
torch.nn.intrinsic.qat.ConvBnReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU2d.forward(self,input)
torch.nn.intrinsic.qat.ConvBnReLU2d.from_float(cls,mod)
torch.nn.intrinsic.qat.ConvBnReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU3d.forward(self,input)
torch.nn.intrinsic.qat.ConvBnReLU3d.from_float(cls,mod)
torch.nn.intrinsic.qat.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.qat.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.qat.ConvReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.ConvReLU3d.forward(self,input)
torch.nn.intrinsic.qat.ConvReLU3d.from_float(cls,mod)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None,dim=2)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd._forward(self,input)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.extra_repr(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.forward(self,input)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.freeze_bn_stats(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.from_float(cls,mod)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.reset_bn_parameters(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.reset_parameters(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.reset_running_stats(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.to_float(self)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.train(self,mode=True)
torch.nn.intrinsic.qat.conv_fused._ConvBnNd.update_bn_stats(self)
torch.nn.intrinsic.qat.freeze_bn_stats(mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=None,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None,dim=2)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode,eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None,dim=2)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd._forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.extra_repr(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.freeze_bn_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.reset_bn_parameters(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.reset_parameters(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.reset_running_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.to_float(self)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.train(self,mode=True)
torch.nn.intrinsic.qat.modules.conv_fused._ConvBnNd.update_bn_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused.freeze_bn_stats(mod)
torch.nn.intrinsic.qat.modules.conv_fused.update_bn_stats(mod)
torch.nn.intrinsic.qat.update_bn_stats(mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/qat/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/qat/modules/linear_relu.py----------------------------------------
A:torch.nn.intrinsic.qat.modules.linear_relu.linear->torch.nn.Linear(self.in_features, self.out_features, self.bias is not None)
A:torch.nn.intrinsic.qat.modules.linear_relu.linear.weight->torch.nn.Parameter(self.weight.detach())
A:torch.nn.intrinsic.qat.modules.linear_relu.linear.bias->torch.nn.Parameter(self.bias.detach())
A:torch.nn.intrinsic.qat.modules.linear_relu.relu->torch.nn.ReLU()
torch.nn.intrinsic.qat.LinearReLU(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.LinearReLU.forward(self,input)
torch.nn.intrinsic.qat.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.qat.LinearReLU.to_float(self)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.forward(self,input)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.to_float(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/modules/conv_relu.py----------------------------------------
A:torch.nn.intrinsic.quantized.modules.conv_relu._reversed_padding_repeated_twice->_reverse_repeat_padding(self.padding)
A:torch.nn.intrinsic.quantized.modules.conv_relu.input->torch.nn.functional.pad(input, _reversed_padding_repeated_twice, mode=self.padding_mode)
A:torch.nn.intrinsic.quantized.modules.conv_relu.(mod.weight, mod.bias)->fuse_conv_bn_weights(mod.weight, mod.bias, mod.bn.running_mean, mod.bn.running_var, mod.bn.eps, mod.bn.weight, mod.bn.bias)
torch.nn.intrinsic.quantized.ConvReLU1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.ConvReLU1d._get_name(self)
torch.nn.intrinsic.quantized.ConvReLU1d.forward(self,input)
torch.nn.intrinsic.quantized.ConvReLU1d.from_float(cls,mod)
torch.nn.intrinsic.quantized.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.ConvReLU2d._get_name(self)
torch.nn.intrinsic.quantized.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.ConvReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.ConvReLU3d._get_name(self)
torch.nn.intrinsic.quantized.ConvReLU3d.forward(self,input)
torch.nn.intrinsic.quantized.ConvReLU3d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU1d._get_name(self)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU1d.forward(self,input)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU1d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d._get_name(self)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d._get_name(self)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.forward(self,input)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/modules/linear_relu.py----------------------------------------
torch.nn.intrinsic.quantized.LinearReLU(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.LinearReLU.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.intrinsic.quantized.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.quantized.LinearReLU.from_reference(cls,ref_linear_relu,output_scale,output_zero_point)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.from_reference(cls,ref_linear_relu,output_scale,output_zero_point)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/modules/bn_relu.py----------------------------------------
torch.nn.intrinsic.quantized.BNReLU2d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.intrinsic.quantized.BNReLU2d._get_name(self)
torch.nn.intrinsic.quantized.BNReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.BNReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.BNReLU3d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.intrinsic.quantized.BNReLU3d._get_name(self)
torch.nn.intrinsic.quantized.BNReLU3d.forward(self,input)
torch.nn.intrinsic.quantized.BNReLU3d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU2d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU2d.__init__(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU2d._get_name(self)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU3d(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU3d.__init__(self,num_features,eps=1e-05,momentum=0.1)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU3d._get_name(self)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU3d.forward(self,input)
torch.nn.intrinsic.quantized.modules.bn_relu.BNReLU3d.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/dynamic/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/dynamic/modules/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/nn/intrinsic/quantized/dynamic/modules/linear_relu.py----------------------------------------
A:torch.nn.intrinsic.quantized.dynamic.modules.linear_relu.Y->torch.ops.quantized.linear_relu_dynamic_fp16(x, self._packed_params._packed_params)
torch.nn.intrinsic.quantized.dynamic.LinearReLU(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.dynamic.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.dynamic.LinearReLU.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.intrinsic.quantized.dynamic.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.quantized.dynamic.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.dynamic.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True,dtype=torch.qint8)
torch.nn.intrinsic.quantized.dynamic.modules.linear_relu.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.dynamic.modules.linear_relu.LinearReLU.forward(self,x:torch.Tensor)->torch.Tensor
torch.nn.intrinsic.quantized.dynamic.modules.linear_relu.LinearReLU.from_float(cls,mod)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/poisson.py----------------------------------------
A:torch.distributions.poisson.(self.rate,)->broadcast_all(rate)
A:torch.distributions.poisson.batch_shape->torch.Size(batch_shape)
A:torch.distributions.poisson.new->self._get_checked_instance(Poisson, _instance)
A:torch.distributions.poisson.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.poisson.shape->self._extended_shape(sample_shape)
A:torch.distributions.poisson.(rate, value)->broadcast_all(self.rate, value)
torch.distributions.Poisson(self,rate,validate_args=None)
torch.distributions.Poisson._log_normalizer(self,x)
torch.distributions.Poisson._natural_params(self)
torch.distributions.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.Poisson.log_prob(self,value)
torch.distributions.Poisson.mean(self)
torch.distributions.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.Poisson.variance(self)
torch.distributions.poisson.Poisson(self,rate,validate_args=None)
torch.distributions.poisson.Poisson.__init__(self,rate,validate_args=None)
torch.distributions.poisson.Poisson._log_normalizer(self,x)
torch.distributions.poisson.Poisson._natural_params(self)
torch.distributions.poisson.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.poisson.Poisson.log_prob(self,value)
torch.distributions.poisson.Poisson.mean(self)
torch.distributions.poisson.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.poisson.Poisson.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/pareto.py----------------------------------------
A:torch.distributions.pareto.(self.scale, self.alpha)->broadcast_all(scale, alpha)
A:torch.distributions.pareto.base_dist->Exponential(self.alpha, validate_args=validate_args)
A:torch.distributions.pareto.new->self._get_checked_instance(Pareto, _instance)
A:torch.distributions.pareto.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.pareto.new.alpha->self.alpha.expand(batch_shape)
A:torch.distributions.pareto.a->self.alpha.clamp(min=2)
torch.distributions.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.Pareto.entropy(self)
torch.distributions.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.Pareto.mean(self)
torch.distributions.Pareto.support(self)
torch.distributions.Pareto.variance(self)
torch.distributions.pareto.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.__init__(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.entropy(self)
torch.distributions.pareto.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.pareto.Pareto.mean(self)
torch.distributions.pareto.Pareto.support(self)
torch.distributions.pareto.Pareto.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/cauchy.py----------------------------------------
A:torch.distributions.cauchy.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.cauchy.batch_shape->torch.Size(batch_shape)
A:torch.distributions.cauchy.new->self._get_checked_instance(Cauchy, _instance)
A:torch.distributions.cauchy.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.cauchy.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.cauchy.shape->self._extended_shape(sample_shape)
A:torch.distributions.cauchy.eps->self.loc.new(shape).cauchy_()
torch.distributions.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.Cauchy.cdf(self,value)
torch.distributions.Cauchy.entropy(self)
torch.distributions.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.Cauchy.icdf(self,value)
torch.distributions.Cauchy.log_prob(self,value)
torch.distributions.Cauchy.mean(self)
torch.distributions.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.Cauchy.variance(self)
torch.distributions.cauchy.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.__init__(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.cdf(self,value)
torch.distributions.cauchy.Cauchy.entropy(self)
torch.distributions.cauchy.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.cauchy.Cauchy.icdf(self,value)
torch.distributions.cauchy.Cauchy.log_prob(self,value)
torch.distributions.cauchy.Cauchy.mean(self)
torch.distributions.cauchy.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.cauchy.Cauchy.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/geometric.py----------------------------------------
A:torch.distributions.geometric.(self.probs,)->broadcast_all(probs)
A:torch.distributions.geometric.(self.logits,)->broadcast_all(logits)
A:torch.distributions.geometric.batch_shape->torch.Size(batch_shape)
A:torch.distributions.geometric.new->self._get_checked_instance(Geometric, _instance)
A:torch.distributions.geometric.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.geometric.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.geometric.shape->self._extended_shape(sample_shape)
A:torch.distributions.geometric.u->self.probs.new(shape).uniform_(tiny, 1)
A:torch.distributions.geometric.(value, probs)->broadcast_all(value, self.probs)
A:torch.distributions.geometric.probs->probs.clone(memory_format=torch.contiguous_format).clone(memory_format=torch.contiguous_format)
torch.distributions.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.Geometric.entropy(self)
torch.distributions.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.Geometric.log_prob(self,value)
torch.distributions.Geometric.logits(self)
torch.distributions.Geometric.mean(self)
torch.distributions.Geometric.probs(self)
torch.distributions.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.Geometric.variance(self)
torch.distributions.geometric.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.entropy(self)
torch.distributions.geometric.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.geometric.Geometric.log_prob(self,value)
torch.distributions.geometric.Geometric.logits(self)
torch.distributions.geometric.Geometric.mean(self)
torch.distributions.geometric.Geometric.probs(self)
torch.distributions.geometric.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.geometric.Geometric.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/half_normal.py----------------------------------------
A:torch.distributions.half_normal.base_dist->Normal(0, scale, validate_args=False)
A:torch.distributions.half_normal.new->self._get_checked_instance(HalfNormal, _instance)
torch.distributions.HalfNormal(self,scale,validate_args=None)
torch.distributions.HalfNormal.cdf(self,value)
torch.distributions.HalfNormal.entropy(self)
torch.distributions.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.HalfNormal.icdf(self,prob)
torch.distributions.HalfNormal.log_prob(self,value)
torch.distributions.HalfNormal.mean(self)
torch.distributions.HalfNormal.scale(self)
torch.distributions.HalfNormal.variance(self)
torch.distributions.half_normal.HalfNormal(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.__init__(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.cdf(self,value)
torch.distributions.half_normal.HalfNormal.entropy(self)
torch.distributions.half_normal.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.half_normal.HalfNormal.icdf(self,prob)
torch.distributions.half_normal.HalfNormal.log_prob(self,value)
torch.distributions.half_normal.HalfNormal.mean(self)
torch.distributions.half_normal.HalfNormal.scale(self)
torch.distributions.half_normal.HalfNormal.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/one_hot_categorical.py----------------------------------------
A:torch.distributions.one_hot_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.one_hot_categorical.new->self._get_checked_instance(OneHotCategorical, _instance)
A:torch.distributions.one_hot_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.one_hot_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.one_hot_categorical.sample_shape->torch.Size(sample_shape)
A:torch.distributions.one_hot_categorical.indices->self._categorical.sample(sample_shape)
A:torch.distributions.one_hot_categorical.values->values.expand((n,) + self.batch_shape + (n,)).expand((n,) + self.batch_shape + (n,))
A:torch.distributions.one_hot_categorical.samples->self.sample(sample_shape)
torch.distributions.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.OneHotCategorical._param(self)
torch.distributions.OneHotCategorical.entropy(self)
torch.distributions.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.OneHotCategorical.log_prob(self,value)
torch.distributions.OneHotCategorical.logits(self)
torch.distributions.OneHotCategorical.mean(self)
torch.distributions.OneHotCategorical.param_shape(self)
torch.distributions.OneHotCategorical.probs(self)
torch.distributions.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.OneHotCategorical.variance(self)
torch.distributions.OneHotCategoricalStraightThrough(OneHotCategorical)
torch.distributions.OneHotCategoricalStraightThrough.rsample(self,sample_shape=torch.Size())
torch.distributions.one_hot_categorical.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.one_hot_categorical.OneHotCategorical._param(self)
torch.distributions.one_hot_categorical.OneHotCategorical.entropy(self)
torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.one_hot_categorical.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.one_hot_categorical.OneHotCategorical.log_prob(self,value)
torch.distributions.one_hot_categorical.OneHotCategorical.logits(self)
torch.distributions.one_hot_categorical.OneHotCategorical.mean(self)
torch.distributions.one_hot_categorical.OneHotCategorical.param_shape(self)
torch.distributions.one_hot_categorical.OneHotCategorical.probs(self)
torch.distributions.one_hot_categorical.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.one_hot_categorical.OneHotCategorical.variance(self)
torch.distributions.one_hot_categorical.OneHotCategoricalStraightThrough(OneHotCategorical)
torch.distributions.one_hot_categorical.OneHotCategoricalStraightThrough.rsample(self,sample_shape=torch.Size())


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/wishart.py----------------------------------------
A:torch.distributions.wishart._log_2->math.log(2)
A:torch.distributions.wishart.param->next((p for p in (covariance_matrix, precision_matrix, scale_tril) if p is not None))
A:torch.distributions.wishart.batch_shape->torch.Size(batch_shape)
A:torch.distributions.wishart.self.df->df.expand(batch_shape)
A:torch.distributions.wishart.self.scale_tril->next((p for p in (covariance_matrix, precision_matrix, scale_tril) if p is not None)).expand(batch_shape + (-1, -1))
A:torch.distributions.wishart.self.covariance_matrix->next((p for p in (covariance_matrix, precision_matrix, scale_tril) if p is not None)).expand(batch_shape + (-1, -1))
A:torch.distributions.wishart.self.precision_matrix->next((p for p in (covariance_matrix, precision_matrix, scale_tril) if p is not None)).expand(batch_shape + (-1, -1))
A:torch.distributions.wishart.self.arg_constraints['df']->torch.distributions.constraints.greater_than(event_shape[-1] - 1)
A:torch.distributions.wishart.self._unbroadcasted_scale_tril->_precision_to_scale_tril(precision_matrix)
A:torch.distributions.wishart.self._dist_chi2->torch.distributions.chi2.Chi2(df=self.df.unsqueeze(-1) - torch.arange(self._event_shape[-1], dtype=self._unbroadcasted_scale_tril.dtype, device=self._unbroadcasted_scale_tril.device).expand(batch_shape + (-1,)))
A:torch.distributions.wishart.new->self._get_checked_instance(Wishart, _instance)
A:torch.distributions.wishart.new._unbroadcasted_scale_tril->self._unbroadcasted_scale_tril.expand(cov_shape)
A:torch.distributions.wishart.new.df->self.df.expand(df_shape)
A:torch.distributions.wishart.new.covariance_matrix->self.covariance_matrix.expand(cov_shape)
A:torch.distributions.wishart.new.scale_tril->self.scale_tril.expand(cov_shape)
A:torch.distributions.wishart.new.precision_matrix->self.precision_matrix.expand(cov_shape)
A:torch.distributions.wishart.new._dist_chi2->torch.distributions.chi2.Chi2(df=new.df.unsqueeze(-1) - torch.arange(self.event_shape[-1], dtype=new._unbroadcasted_scale_tril.dtype, device=new._unbroadcasted_scale_tril.device).expand(batch_shape + (-1,)))
A:torch.distributions.wishart.identity->torch.eye(self._event_shape[-1], device=self._unbroadcasted_scale_tril.device, dtype=self._unbroadcasted_scale_tril.dtype)
A:torch.distributions.wishart.diag_V->V.diagonal(dim1=-2, dim2=-1)
A:torch.distributions.wishart.noise->self._dist_chi2.rsample(sample_shape).sqrt().diag_embed(dim1=-2, dim2=-1)
A:torch.distributions.wishart.(i, j)->torch.tril_indices(p, p, offset=-1)
A:torch.distributions.wishart.noise[..., i, j]->torch.randn(torch.Size(sample_shape) + self._batch_shape + (int(p * (p - 1) / 2),), dtype=noise.dtype, device=noise.device)
A:torch.distributions.wishart.sample_shape->torch.Size(sample_shape)
A:torch.distributions.wishart.sample->torch.where(is_singular, sample_new, sample)
A:torch.distributions.wishart.is_singular->is_singular.amax(self._batch_dims).amax(self._batch_dims)
A:torch.distributions.wishart.sample_new->self._bartlett_sampling(is_singular[is_singular].shape)
A:torch.distributions.wishart.is_singular_new->is_singular_new.amax(self._batch_dims).amax(self._batch_dims)
torch.distributions.Wishart(self,df:Union[torch.Tensor,Number],covariance_matrix:torch.Tensor=None,precision_matrix:torch.Tensor=None,scale_tril:torch.Tensor=None,validate_args=None)
torch.distributions.Wishart._bartlett_sampling(self,sample_shape=torch.Size())
torch.distributions.Wishart._log_normalizer(self,x,y)
torch.distributions.Wishart._natural_params(self)
torch.distributions.Wishart.covariance_matrix(self)
torch.distributions.Wishart.entropy(self)
torch.distributions.Wishart.expand(self,batch_shape,_instance=None)
torch.distributions.Wishart.log_prob(self,value)
torch.distributions.Wishart.mean(self)
torch.distributions.Wishart.precision_matrix(self)
torch.distributions.Wishart.rsample(self,sample_shape=torch.Size(),max_try_correction=None)
torch.distributions.Wishart.scale_tril(self)
torch.distributions.Wishart.variance(self)
torch.distributions.wishart.Wishart(self,df:Union[torch.Tensor,Number],covariance_matrix:torch.Tensor=None,precision_matrix:torch.Tensor=None,scale_tril:torch.Tensor=None,validate_args=None)
torch.distributions.wishart.Wishart.__init__(self,df:Union[torch.Tensor,Number],covariance_matrix:torch.Tensor=None,precision_matrix:torch.Tensor=None,scale_tril:torch.Tensor=None,validate_args=None)
torch.distributions.wishart.Wishart._bartlett_sampling(self,sample_shape=torch.Size())
torch.distributions.wishart.Wishart._log_normalizer(self,x,y)
torch.distributions.wishart.Wishart._natural_params(self)
torch.distributions.wishart.Wishart.covariance_matrix(self)
torch.distributions.wishart.Wishart.entropy(self)
torch.distributions.wishart.Wishart.expand(self,batch_shape,_instance=None)
torch.distributions.wishart.Wishart.log_prob(self,value)
torch.distributions.wishart.Wishart.mean(self)
torch.distributions.wishart.Wishart.precision_matrix(self)
torch.distributions.wishart.Wishart.rsample(self,sample_shape=torch.Size(),max_try_correction=None)
torch.distributions.wishart.Wishart.scale_tril(self)
torch.distributions.wishart.Wishart.variance(self)
torch.distributions.wishart._mvdigamma(x:torch.Tensor,p:int)->torch.Tensor


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/logistic_normal.py----------------------------------------
A:torch.distributions.logistic_normal.base_dist->base_dist.expand([1]).expand([1])
A:torch.distributions.logistic_normal.new->self._get_checked_instance(LogisticNormal, _instance)
torch.distributions.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogisticNormal.loc(self)
torch.distributions.LogisticNormal.scale(self)
torch.distributions.logistic_normal.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.logistic_normal.LogisticNormal.loc(self)
torch.distributions.logistic_normal.LogisticNormal.scale(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/utils.py----------------------------------------
A:torch.distributions.utils.options->dict(dtype=value.dtype, device=value.device)
A:torch.distributions.utils.ps_clamped->clamp_probs(probs)
A:torch.distributions.utils.value->self.wrapped(instance)
A:torch.distributions.utils.arange->torch.arange(n, device=vec.device)
A:torch.distributions.utils.mat->vec.new_zeros(vec.shape[:-1] + torch.Size((n, n)))
torch.distributions.utils._lazy_property_and_property(self,wrapped)
torch.distributions.utils._lazy_property_and_property.__init__(self,wrapped)
torch.distributions.utils._standard_normal(shape,dtype,device)
torch.distributions.utils._sum_rightmost(value,dim)
torch.distributions.utils.broadcast_all(*values)
torch.distributions.utils.clamp_probs(probs)
torch.distributions.utils.lazy_property(self,wrapped)
torch.distributions.utils.lazy_property.__get__(self,instance,obj_type=None)
torch.distributions.utils.lazy_property.__init__(self,wrapped)
torch.distributions.utils.logits_to_probs(logits,is_binary=False)
torch.distributions.utils.probs_to_logits(probs,is_binary=False)
torch.distributions.utils.tril_matrix_to_vec(mat,diag=0)
torch.distributions.utils.vec_to_tril_matrix(vec,diag=0)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/weibull.py----------------------------------------
A:torch.distributions.weibull.(self.scale, self.concentration)->broadcast_all(scale, concentration)
A:torch.distributions.weibull.self.concentration_reciprocal->self.concentration.reciprocal()
A:torch.distributions.weibull.base_dist->self.base_dist.expand(batch_shape)
A:torch.distributions.weibull.new->self._get_checked_instance(Weibull, _instance)
A:torch.distributions.weibull.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.weibull.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.weibull.new.concentration_reciprocal->self._get_checked_instance(Weibull, _instance).concentration.reciprocal()
torch.distributions.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.Weibull.entropy(self)
torch.distributions.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.Weibull.mean(self)
torch.distributions.Weibull.variance(self)
torch.distributions.weibull.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.__init__(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.entropy(self)
torch.distributions.weibull.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.weibull.Weibull.mean(self)
torch.distributions.weibull.Weibull.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/log_normal.py----------------------------------------
A:torch.distributions.log_normal.base_dist->Normal(loc, scale, validate_args=validate_args)
A:torch.distributions.log_normal.new->self._get_checked_instance(LogNormal, _instance)
torch.distributions.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.LogNormal.entropy(self)
torch.distributions.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogNormal.loc(self)
torch.distributions.LogNormal.mean(self)
torch.distributions.LogNormal.scale(self)
torch.distributions.LogNormal.variance(self)
torch.distributions.log_normal.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.entropy(self)
torch.distributions.log_normal.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.log_normal.LogNormal.loc(self)
torch.distributions.log_normal.LogNormal.mean(self)
torch.distributions.log_normal.LogNormal.scale(self)
torch.distributions.log_normal.LogNormal.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/lowrank_multivariate_normal.py----------------------------------------
A:torch.distributions.lowrank_multivariate_normal.m->W.size(-1)
A:torch.distributions.lowrank_multivariate_normal.K->torch.matmul(Dinvsqrt_W, Dinvsqrt_W.mT).contiguous()
A:torch.distributions.lowrank_multivariate_normal.Wt_Dinv_x->_batch_mv(Wt_Dinv, x)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term1->(x.pow(2) / D).sum(-1)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term2->_batch_mahalanobis(capacitance_tril, Wt_Dinv_x)
A:torch.distributions.lowrank_multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.cov_diag_->cov_diag.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.(loc_, self.cov_factor, cov_diag_)->torch.broadcast_tensors(loc_, cov_factor, cov_diag_)
A:torch.distributions.lowrank_multivariate_normal.self._capacitance_tril->_batch_capacitance_tril(cov_factor, cov_diag)
A:torch.distributions.lowrank_multivariate_normal.new->self._get_checked_instance(LowRankMultivariateNormal, _instance)
A:torch.distributions.lowrank_multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.lowrank_multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_diag->self.cov_diag.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_factor->self.cov_factor.expand(loc_shape + self.cov_factor.shape[-1:])
A:torch.distributions.lowrank_multivariate_normal.cov_diag_sqrt_unsqueeze->self._unbroadcasted_cov_diag.sqrt().unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.A->torch.linalg.solve_triangular(self._capacitance_tril, Wt_Dinv, upper=False)
A:torch.distributions.lowrank_multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.lowrank_multivariate_normal.eps_W->_standard_normal(W_shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.eps_D->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.M->_batch_lowrank_mahalanobis(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, diff, self._capacitance_tril)
A:torch.distributions.lowrank_multivariate_normal.log_det->_batch_lowrank_logdet(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, self._capacitance_tril)
torch.distributions.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.LowRankMultivariateNormal.entropy(self)
torch.distributions.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.LowRankMultivariateNormal.mean(self)
torch.distributions.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.__init__(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal._batch_capacitance_tril(W,D)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_logdet(W,D,capacitance_tril)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_mahalanobis(W,D,x,capacitance_tril)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/kumaraswamy.py----------------------------------------
A:torch.distributions.kumaraswamy.(self.concentration1, self.concentration0)->broadcast_all(concentration1, concentration0)
A:torch.distributions.kumaraswamy.finfo->torch.finfo(self.concentration0.dtype)
A:torch.distributions.kumaraswamy.base_dist->Uniform(torch.full_like(self.concentration0, 0), torch.full_like(self.concentration0, 1), validate_args=validate_args)
A:torch.distributions.kumaraswamy.new->self._get_checked_instance(Kumaraswamy, _instance)
A:torch.distributions.kumaraswamy.new.concentration1->self.concentration1.expand(batch_shape)
A:torch.distributions.kumaraswamy.new.concentration0->self.concentration0.expand(batch_shape)
torch.distributions.Kumaraswamy(self,concentration1,concentration0,validate_args=None)
torch.distributions.Kumaraswamy.entropy(self)
torch.distributions.Kumaraswamy.expand(self,batch_shape,_instance=None)
torch.distributions.Kumaraswamy.mean(self)
torch.distributions.Kumaraswamy.variance(self)
torch.distributions.kumaraswamy.Kumaraswamy(self,concentration1,concentration0,validate_args=None)
torch.distributions.kumaraswamy.Kumaraswamy.__init__(self,concentration1,concentration0,validate_args=None)
torch.distributions.kumaraswamy.Kumaraswamy.entropy(self)
torch.distributions.kumaraswamy.Kumaraswamy.expand(self,batch_shape,_instance=None)
torch.distributions.kumaraswamy.Kumaraswamy.mean(self)
torch.distributions.kumaraswamy.Kumaraswamy.variance(self)
torch.distributions.kumaraswamy._moments(a,b,n)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/transformed_distribution.py----------------------------------------
A:torch.distributions.transformed_distribution.base_event_dim->len(base_distribution.event_shape)
A:torch.distributions.transformed_distribution.transform->ComposeTransform(self.transforms)
A:torch.distributions.transformed_distribution.shape->t.inverse_shape(shape)
A:torch.distributions.transformed_distribution.expanded_base_shape->ComposeTransform(self.transforms).inverse_shape(shape)
A:torch.distributions.transformed_distribution.base_distribution->Independent(base_distribution, reinterpreted_batch_ndims)
A:torch.distributions.transformed_distribution.new->self._get_checked_instance(TransformedDistribution, _instance)
A:torch.distributions.transformed_distribution.batch_shape->torch.Size(batch_shape)
A:torch.distributions.transformed_distribution.new.base_dist->self.base_dist.expand(base_batch_shape)
A:torch.distributions.transformed_distribution.support->torch.distributions.constraints.independent(support, len(self.event_shape) - support.event_dim)
A:torch.distributions.transformed_distribution.x->ComposeTransform(self.transforms).inv(y)
A:torch.distributions.transformed_distribution.event_dim->len(self.event_shape)
A:torch.distributions.transformed_distribution.value->transform(value)
torch.distributions.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.TransformedDistribution.cdf(self,value)
torch.distributions.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.TransformedDistribution.has_rsample(self)
torch.distributions.TransformedDistribution.icdf(self,value)
torch.distributions.TransformedDistribution.log_prob(self,value)
torch.distributions.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.support(self)
torch.distributions.transformed_distribution.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution.__init__(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.transformed_distribution.TransformedDistribution.has_rsample(self)
torch.distributions.transformed_distribution.TransformedDistribution.icdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.log_prob(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.support(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/exp_family.py----------------------------------------
A:torch.distributions.exp_family.lg_normal->self._log_normalizer(*nparams)
A:torch.distributions.exp_family.gradients->torch.autograd.grad(lg_normal.sum(), nparams, create_graph=True)
torch.distributions.ExponentialFamily(Distribution)
torch.distributions.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.ExponentialFamily._natural_params(self)
torch.distributions.ExponentialFamily.entropy(self)
torch.distributions.exp_family.ExponentialFamily(Distribution)
torch.distributions.exp_family.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.exp_family.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.exp_family.ExponentialFamily._natural_params(self)
torch.distributions.exp_family.ExponentialFamily.entropy(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/laplace.py----------------------------------------
A:torch.distributions.laplace.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.laplace.batch_shape->torch.Size(batch_shape)
A:torch.distributions.laplace.new->self._get_checked_instance(Laplace, _instance)
A:torch.distributions.laplace.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.laplace.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.laplace.shape->self._extended_shape(sample_shape)
A:torch.distributions.laplace.finfo->torch.finfo(self.loc.dtype)
A:torch.distributions.laplace.u->self.loc.new(shape).uniform_(finfo.eps - 1, 1)
torch.distributions.Laplace(self,loc,scale,validate_args=None)
torch.distributions.Laplace.cdf(self,value)
torch.distributions.Laplace.entropy(self)
torch.distributions.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.Laplace.icdf(self,value)
torch.distributions.Laplace.log_prob(self,value)
torch.distributions.Laplace.mean(self)
torch.distributions.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.Laplace.stddev(self)
torch.distributions.Laplace.variance(self)
torch.distributions.laplace.Laplace(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.__init__(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.cdf(self,value)
torch.distributions.laplace.Laplace.entropy(self)
torch.distributions.laplace.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.laplace.Laplace.icdf(self,value)
torch.distributions.laplace.Laplace.log_prob(self,value)
torch.distributions.laplace.Laplace.mean(self)
torch.distributions.laplace.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.laplace.Laplace.stddev(self)
torch.distributions.laplace.Laplace.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/multivariate_normal.py----------------------------------------
A:torch.distributions.multivariate_normal.n->bx.permute(permute_dims).size(-1)
A:torch.distributions.multivariate_normal.bx_batch_dims->len(bx_batch_shape)
A:torch.distributions.multivariate_normal.bx->bx.permute(permute_dims).permute(permute_dims)
A:torch.distributions.multivariate_normal.flat_L->bL.reshape(-1, n, n)
A:torch.distributions.multivariate_normal.flat_x->bx.permute(permute_dims).permute(permute_dims).reshape(-1, flat_L.size(0), n)
A:torch.distributions.multivariate_normal.flat_x_swap->bx.permute(permute_dims).permute(permute_dims).reshape(-1, flat_L.size(0), n).permute(1, 2, 0)
A:torch.distributions.multivariate_normal.M_swap->torch.linalg.solve_triangular(flat_L, flat_x_swap, upper=False).pow(2).sum(-2)
A:torch.distributions.multivariate_normal.M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff)
A:torch.distributions.multivariate_normal.permuted_M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff).reshape(bx.shape[:-1])
A:torch.distributions.multivariate_normal.permute_inv_dims->list(range(outer_batch_dims))
A:torch.distributions.multivariate_normal.reshaped_M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff).reshape(bx.shape[:-1]).permute(permute_inv_dims)
A:torch.distributions.multivariate_normal.Lf->torch.linalg.cholesky(torch.flip(P, (-2, -1)))
A:torch.distributions.multivariate_normal.L_inv->torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)
A:torch.distributions.multivariate_normal.Id->torch.eye(P.shape[-1], dtype=P.dtype, device=P.device)
A:torch.distributions.multivariate_normal.L->torch.linalg.solve_triangular(L_inv, Id, upper=False)
A:torch.distributions.multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multivariate_normal.self.scale_tril->scale_tril.expand(batch_shape + (-1, -1))
A:torch.distributions.multivariate_normal.self.covariance_matrix->covariance_matrix.expand(batch_shape + (-1, -1))
A:torch.distributions.multivariate_normal.self.precision_matrix->precision_matrix.expand(batch_shape + (-1, -1))
A:torch.distributions.multivariate_normal.self.loc->loc.expand(batch_shape + (-1,))
A:torch.distributions.multivariate_normal.self._unbroadcasted_scale_tril->_precision_to_scale_tril(precision_matrix)
A:torch.distributions.multivariate_normal.new->self._get_checked_instance(MultivariateNormal, _instance)
A:torch.distributions.multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.multivariate_normal.new.covariance_matrix->self.covariance_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.scale_tril->self.scale_tril.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.precision_matrix->self.precision_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.multivariate_normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.multivariate_normal.half_log_det->self._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)
torch.distributions.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.MultivariateNormal.covariance_matrix(self)
torch.distributions.MultivariateNormal.entropy(self)
torch.distributions.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.MultivariateNormal.log_prob(self,value)
torch.distributions.MultivariateNormal.mean(self)
torch.distributions.MultivariateNormal.precision_matrix(self)
torch.distributions.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.MultivariateNormal.scale_tril(self)
torch.distributions.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.__init__(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.entropy(self)
torch.distributions.multivariate_normal.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.multivariate_normal.MultivariateNormal.log_prob(self,value)
torch.distributions.multivariate_normal.MultivariateNormal.mean(self)
torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.multivariate_normal.MultivariateNormal.scale_tril(self)
torch.distributions.multivariate_normal.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal._batch_mahalanobis(bL,bx)
torch.distributions.multivariate_normal._batch_mv(bmat,bvec)
torch.distributions.multivariate_normal._precision_to_scale_tril(P)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/gamma.py----------------------------------------
A:torch.distributions.gamma.(self.concentration, self.rate)->broadcast_all(concentration, rate)
A:torch.distributions.gamma.batch_shape->torch.Size(batch_shape)
A:torch.distributions.gamma.new->self._get_checked_instance(Gamma, _instance)
A:torch.distributions.gamma.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.gamma.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.gamma.shape->self._extended_shape(sample_shape)
A:torch.distributions.gamma.value->torch.as_tensor(value, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.Gamma._log_normalizer(self,x,y)
torch.distributions.Gamma._natural_params(self)
torch.distributions.Gamma.entropy(self)
torch.distributions.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.Gamma.log_prob(self,value)
torch.distributions.Gamma.mean(self)
torch.distributions.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.Gamma.variance(self)
torch.distributions.gamma.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma.__init__(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma._log_normalizer(self,x,y)
torch.distributions.gamma.Gamma._natural_params(self)
torch.distributions.gamma.Gamma.entropy(self)
torch.distributions.gamma.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.gamma.Gamma.log_prob(self,value)
torch.distributions.gamma.Gamma.mean(self)
torch.distributions.gamma.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.gamma.Gamma.variance(self)
torch.distributions.gamma._standard_gamma(concentration)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/mixture_same_family.py----------------------------------------
A:torch.distributions.mixture_same_family.self._event_ndims->len(event_shape)
A:torch.distributions.mixture_same_family.batch_shape->torch.Size(batch_shape)
A:torch.distributions.mixture_same_family.new->self._get_checked_instance(MixtureSameFamily, _instance)
A:torch.distributions.mixture_same_family.new._component_distribution->self._component_distribution.expand(batch_shape_comp)
A:torch.distributions.mixture_same_family.new._mixture_distribution->self._mixture_distribution.expand(batch_shape)
A:torch.distributions.mixture_same_family.probs->self._pad_mixture_dimensions(self.mixture_distribution.probs)
A:torch.distributions.mixture_same_family.mean_cond_var->torch.sum(probs * self.component_distribution.variance, dim=-1 - self._event_ndims)
A:torch.distributions.mixture_same_family.var_cond_mean->torch.sum(probs * (self.component_distribution.mean - self._pad(self.mean)).pow(2.0), dim=-1 - self._event_ndims)
A:torch.distributions.mixture_same_family.x->x.reshape(xs[:-1] + torch.Size(pad_ndims * [1]) + xs[-1:] + torch.Size(self._event_ndims * [1])).reshape(xs[:-1] + torch.Size(pad_ndims * [1]) + xs[-1:] + torch.Size(self._event_ndims * [1]))
A:torch.distributions.mixture_same_family.cdf_x->self.component_distribution.cdf(x)
A:torch.distributions.mixture_same_family.log_prob_x->self.component_distribution.log_prob(x)
A:torch.distributions.mixture_same_family.log_mix_prob->torch.log_softmax(self.mixture_distribution.logits, dim=-1)
A:torch.distributions.mixture_same_family.sample_len->len(sample_shape)
A:torch.distributions.mixture_same_family.batch_len->len(self.batch_shape)
A:torch.distributions.mixture_same_family.mix_sample->self.mixture_distribution.sample(sample_shape)
A:torch.distributions.mixture_same_family.comp_samples->self.component_distribution.sample(sample_shape)
A:torch.distributions.mixture_same_family.mix_sample_r->mix_sample_r.repeat(torch.Size([1] * len(mix_shape)) + torch.Size([1]) + es).repeat(torch.Size([1] * len(mix_shape)) + torch.Size([1]) + es)
A:torch.distributions.mixture_same_family.samples->torch.gather(comp_samples, gather_dim, mix_sample_r)
A:torch.distributions.mixture_same_family.dist_batch_ndims->self.batch_shape.numel()
A:torch.distributions.mixture_same_family.cat_batch_ndims->self.mixture_distribution.batch_shape.numel()
A:torch.distributions.mixture_same_family.args_string->'\n  {},\n  {}'.format(self.mixture_distribution, self.component_distribution)
torch.distributions.MixtureSameFamily(self,mixture_distribution,component_distribution,validate_args=None)
torch.distributions.MixtureSameFamily.__repr__(self)
torch.distributions.MixtureSameFamily._pad(self,x)
torch.distributions.MixtureSameFamily._pad_mixture_dimensions(self,x)
torch.distributions.MixtureSameFamily.cdf(self,x)
torch.distributions.MixtureSameFamily.component_distribution(self)
torch.distributions.MixtureSameFamily.expand(self,batch_shape,_instance=None)
torch.distributions.MixtureSameFamily.log_prob(self,x)
torch.distributions.MixtureSameFamily.mean(self)
torch.distributions.MixtureSameFamily.mixture_distribution(self)
torch.distributions.MixtureSameFamily.sample(self,sample_shape=torch.Size())
torch.distributions.MixtureSameFamily.support(self)
torch.distributions.MixtureSameFamily.variance(self)
torch.distributions.mixture_same_family.MixtureSameFamily(self,mixture_distribution,component_distribution,validate_args=None)
torch.distributions.mixture_same_family.MixtureSameFamily.__init__(self,mixture_distribution,component_distribution,validate_args=None)
torch.distributions.mixture_same_family.MixtureSameFamily.__repr__(self)
torch.distributions.mixture_same_family.MixtureSameFamily._pad(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily._pad_mixture_dimensions(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily.cdf(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution(self)
torch.distributions.mixture_same_family.MixtureSameFamily.expand(self,batch_shape,_instance=None)
torch.distributions.mixture_same_family.MixtureSameFamily.log_prob(self,x)
torch.distributions.mixture_same_family.MixtureSameFamily.mean(self)
torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution(self)
torch.distributions.mixture_same_family.MixtureSameFamily.sample(self,sample_shape=torch.Size())
torch.distributions.mixture_same_family.MixtureSameFamily.support(self)
torch.distributions.mixture_same_family.MixtureSameFamily.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/distribution.py----------------------------------------
A:torch.distributions.distribution.value->getattr(self, param)
A:torch.distributions.distribution.valid->support.check(value)
A:torch.distributions.distribution.sample_shape->torch.Size(sample_shape)
A:torch.distributions.distribution.actual_shape->getattr(self, param).size()
A:torch.distributions.distribution.args_string->', '.join(['{}: {}'.format(p, self.__dict__[p] if self.__dict__[p].numel() == 1 else self.__dict__[p].size()) for p in param_names])
torch.distributions.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.Distribution.__repr__(self)
torch.distributions.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.Distribution._validate_sample(self,value)
torch.distributions.Distribution.arg_constraints(self)->Dict[str, constraints.Constraint]
torch.distributions.Distribution.batch_shape(self)
torch.distributions.Distribution.cdf(self,value)
torch.distributions.Distribution.entropy(self)
torch.distributions.Distribution.enumerate_support(self,expand=True)
torch.distributions.Distribution.event_shape(self)
torch.distributions.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.Distribution.icdf(self,value)
torch.distributions.Distribution.log_prob(self,value)
torch.distributions.Distribution.mean(self)
torch.distributions.Distribution.perplexity(self)
torch.distributions.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample_n(self,n)
torch.distributions.Distribution.set_default_validate_args(value)
torch.distributions.Distribution.stddev(self)
torch.distributions.Distribution.support(self)->Optional[Any]
torch.distributions.Distribution.variance(self)
torch.distributions.distribution.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__init__(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__repr__(self)
torch.distributions.distribution.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.distribution.Distribution._validate_sample(self,value)
torch.distributions.distribution.Distribution.arg_constraints(self)->Dict[str, constraints.Constraint]
torch.distributions.distribution.Distribution.batch_shape(self)
torch.distributions.distribution.Distribution.cdf(self,value)
torch.distributions.distribution.Distribution.entropy(self)
torch.distributions.distribution.Distribution.enumerate_support(self,expand=True)
torch.distributions.distribution.Distribution.event_shape(self)
torch.distributions.distribution.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.distribution.Distribution.icdf(self,value)
torch.distributions.distribution.Distribution.log_prob(self,value)
torch.distributions.distribution.Distribution.mean(self)
torch.distributions.distribution.Distribution.perplexity(self)
torch.distributions.distribution.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample_n(self,n)
torch.distributions.distribution.Distribution.set_default_validate_args(value)
torch.distributions.distribution.Distribution.stddev(self)
torch.distributions.distribution.Distribution.support(self)->Optional[Any]
torch.distributions.distribution.Distribution.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/transforms.py----------------------------------------
A:torch.distributions.transforms.inv->ComposeTransform([p.inv for p in reversed(self.parts)])
A:torch.distributions.transforms.self._inv->weakref.ref(inv)
A:torch.distributions.transforms.y->y.clamp(min=finfo.tiny, max=1.0 - finfo.eps).clamp(min=finfo.tiny, max=1.0 - finfo.eps)
A:torch.distributions.transforms.x->x.clamp(min=-1 + eps, max=1 - eps).clamp(min=-1 + eps, max=1 - eps)
A:torch.distributions.transforms.event_dim->max(event_dim, part.codomain.event_dim)
A:torch.distributions.transforms.domain->torch.distributions.constraints.independent(constraints.real, 2)
A:torch.distributions.transforms.codomain->torch.distributions.constraints.interval(-1.0, 1.0)
A:torch.distributions.transforms.inv._inv->weakref.ref(self)
A:torch.distributions.transforms.shape->part.inverse_shape(shape)
A:torch.distributions.transforms.identity_transform->ComposeTransform([])
A:torch.distributions.transforms.self.base_transform->base_transform.with_cache(cache_size)
A:torch.distributions.transforms.result->result.view(result_size).sum(-1).view(result_size).sum(-1)
A:torch.distributions.transforms.self.in_shape->torch.Size(in_shape)
A:torch.distributions.transforms.self.out_shape->torch.Size(out_shape)
A:torch.distributions.transforms.(self.exponent,)->broadcast_all(exponent)
A:torch.distributions.transforms.finfo->torch.finfo(y.dtype)
A:torch.distributions.transforms.r->vec_to_tril_matrix(x, diag=-1)
A:torch.distributions.transforms.z1m_cumprod_sqrt->(1 - z).sqrt().cumprod(-1)
A:torch.distributions.transforms.y_cumsum_shifted->pad(y_cumsum[..., :-1], [1, 0], value=1)
A:torch.distributions.transforms.y_vec->tril_matrix_to_vec(y, diag=-1)
A:torch.distributions.transforms.y_cumsum_vec->tril_matrix_to_vec(y_cumsum_shifted, diag=-1)
A:torch.distributions.transforms.y1m_cumsum_tril->tril_matrix_to_vec(y1m_cumsum, diag=-2)
A:torch.distributions.transforms.D->round((0.25 + 2 * N) ** 0.5 + 0.5)
A:torch.distributions.transforms.probs->(logprobs - logprobs.max(-1, True)[0]).exp()
A:torch.distributions.transforms.z->_clipped_sigmoid(x - offset.log())
A:torch.distributions.transforms.z_cumprod->(1 - z).cumprod(-1)
A:torch.distributions.transforms.sf->torch.clamp(sf, min=torch.finfo(y.dtype).tiny)
A:torch.distributions.transforms.detJ->(-x + F.logsigmoid(x) + y[..., :-1].log()).sum(-1)
A:torch.distributions.transforms.self.transforms->list(tseq)
A:torch.distributions.transforms.self.lengths->list(lengths)
A:torch.distributions.transforms.xslice->x.clamp(min=-1 + eps, max=1 - eps).clamp(min=-1 + eps, max=1 - eps).narrow(self.dim, start, length)
A:torch.distributions.transforms.yslice->y.clamp(min=finfo.tiny, max=1.0 - finfo.eps).clamp(min=finfo.tiny, max=1.0 - finfo.eps).narrow(self.dim, start, length)
A:torch.distributions.transforms.logdetjac->_sum_rightmost(logdetjac, self.event_dim - trans.event_dim)
A:torch.distributions.transforms.yslices->self._slice(y)
A:torch.distributions.transforms.xslices->self._slice(x)
torch.distributions.AbsTransform(Transform)
torch.distributions.AbsTransform.__eq__(self,other)
torch.distributions.AbsTransform._call(self,x)
torch.distributions.AbsTransform._inverse(self,y)
torch.distributions.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.AffineTransform.__eq__(self,other)
torch.distributions.AffineTransform._call(self,x)
torch.distributions.AffineTransform._inverse(self,y)
torch.distributions.AffineTransform.codomain(self)
torch.distributions.AffineTransform.domain(self)
torch.distributions.AffineTransform.event_dim(self)
torch.distributions.AffineTransform.forward_shape(self,shape)
torch.distributions.AffineTransform.inverse_shape(self,shape)
torch.distributions.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.AffineTransform.sign(self)
torch.distributions.AffineTransform.with_cache(self,cache_size=1)
torch.distributions.CatTransform(self,tseq,dim=0,lengths=None,cache_size=0)
torch.distributions.CatTransform._call(self,x)
torch.distributions.CatTransform._inverse(self,y)
torch.distributions.CatTransform.bijective(self)
torch.distributions.CatTransform.codomain(self)
torch.distributions.CatTransform.domain(self)
torch.distributions.CatTransform.event_dim(self)
torch.distributions.CatTransform.length(self)
torch.distributions.CatTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.CatTransform.with_cache(self,cache_size=1)
torch.distributions.ComposeTransform(self,parts:List[Transform],cache_size=0)
torch.distributions.ComposeTransform.__eq__(self,other)
torch.distributions.ComposeTransform.__repr__(self)
torch.distributions.ComposeTransform.bijective(self)
torch.distributions.ComposeTransform.codomain(self)
torch.distributions.ComposeTransform.domain(self)
torch.distributions.ComposeTransform.forward_shape(self,shape)
torch.distributions.ComposeTransform.inv(self)
torch.distributions.ComposeTransform.inverse_shape(self,shape)
torch.distributions.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ComposeTransform.sign(self)
torch.distributions.ComposeTransform.with_cache(self,cache_size=1)
torch.distributions.CorrCholeskyTransform(Transform)
torch.distributions.CorrCholeskyTransform._call(self,x)
torch.distributions.CorrCholeskyTransform._inverse(self,y)
torch.distributions.CorrCholeskyTransform.forward_shape(self,shape)
torch.distributions.CorrCholeskyTransform.inverse_shape(self,shape)
torch.distributions.CorrCholeskyTransform.log_abs_det_jacobian(self,x,y,intermediates=None)
torch.distributions.ExpTransform(Transform)
torch.distributions.ExpTransform.__eq__(self,other)
torch.distributions.ExpTransform._call(self,x)
torch.distributions.ExpTransform._inverse(self,y)
torch.distributions.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.IndependentTransform(self,base_transform,reinterpreted_batch_ndims,cache_size=0)
torch.distributions.IndependentTransform.__repr__(self)
torch.distributions.IndependentTransform._call(self,x)
torch.distributions.IndependentTransform._inverse(self,y)
torch.distributions.IndependentTransform.bijective(self)
torch.distributions.IndependentTransform.codomain(self)
torch.distributions.IndependentTransform.domain(self)
torch.distributions.IndependentTransform.forward_shape(self,shape)
torch.distributions.IndependentTransform.inverse_shape(self,shape)
torch.distributions.IndependentTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.IndependentTransform.sign(self)
torch.distributions.IndependentTransform.with_cache(self,cache_size=1)
torch.distributions.LowerCholeskyTransform(Transform)
torch.distributions.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.LowerCholeskyTransform._call(self,x)
torch.distributions.LowerCholeskyTransform._inverse(self,y)
torch.distributions.PowerTransform(self,exponent,cache_size=0)
torch.distributions.PowerTransform.__eq__(self,other)
torch.distributions.PowerTransform._call(self,x)
torch.distributions.PowerTransform._inverse(self,y)
torch.distributions.PowerTransform.forward_shape(self,shape)
torch.distributions.PowerTransform.inverse_shape(self,shape)
torch.distributions.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.PowerTransform.with_cache(self,cache_size=1)
torch.distributions.ReshapeTransform(self,in_shape,out_shape,cache_size=0)
torch.distributions.ReshapeTransform._call(self,x)
torch.distributions.ReshapeTransform._inverse(self,y)
torch.distributions.ReshapeTransform.codomain(self)
torch.distributions.ReshapeTransform.domain(self)
torch.distributions.ReshapeTransform.forward_shape(self,shape)
torch.distributions.ReshapeTransform.inverse_shape(self,shape)
torch.distributions.ReshapeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ReshapeTransform.with_cache(self,cache_size=1)
torch.distributions.SigmoidTransform(Transform)
torch.distributions.SigmoidTransform.__eq__(self,other)
torch.distributions.SigmoidTransform._call(self,x)
torch.distributions.SigmoidTransform._inverse(self,y)
torch.distributions.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SoftmaxTransform(Transform)
torch.distributions.SoftmaxTransform.__eq__(self,other)
torch.distributions.SoftmaxTransform._call(self,x)
torch.distributions.SoftmaxTransform._inverse(self,y)
torch.distributions.SoftmaxTransform.forward_shape(self,shape)
torch.distributions.SoftmaxTransform.inverse_shape(self,shape)
torch.distributions.StackTransform(self,tseq,dim=0,cache_size=0)
torch.distributions.StackTransform._call(self,x)
torch.distributions.StackTransform._inverse(self,y)
torch.distributions.StackTransform._slice(self,z)
torch.distributions.StackTransform.bijective(self)
torch.distributions.StackTransform.codomain(self)
torch.distributions.StackTransform.domain(self)
torch.distributions.StackTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.StackTransform.with_cache(self,cache_size=1)
torch.distributions.StickBreakingTransform(Transform)
torch.distributions.StickBreakingTransform.__eq__(self,other)
torch.distributions.StickBreakingTransform._call(self,x)
torch.distributions.StickBreakingTransform._inverse(self,y)
torch.distributions.StickBreakingTransform.forward_shape(self,shape)
torch.distributions.StickBreakingTransform.inverse_shape(self,shape)
torch.distributions.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.TanhTransform(Transform)
torch.distributions.TanhTransform.__eq__(self,other)
torch.distributions.TanhTransform._call(self,x)
torch.distributions.TanhTransform._inverse(self,y)
torch.distributions.TanhTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform(self,cache_size=0)
torch.distributions.Transform.__eq__(self,other)
torch.distributions.Transform.__ne__(self,other)
torch.distributions.Transform.__repr__(self)
torch.distributions.Transform._call(self,x)
torch.distributions.Transform._inv_call(self,y)
torch.distributions.Transform._inverse(self,y)
torch.distributions.Transform.event_dim(self)
torch.distributions.Transform.forward_shape(self,shape)
torch.distributions.Transform.inv(self)
torch.distributions.Transform.inverse_shape(self,shape)
torch.distributions.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform.sign(self)
torch.distributions.Transform.with_cache(self,cache_size=1)
torch.distributions._InverseTransform(self,transform:Transform)
torch.distributions._InverseTransform.__eq__(self,other)
torch.distributions._InverseTransform.__repr__(self)
torch.distributions._InverseTransform.bijective(self)
torch.distributions._InverseTransform.codomain(self)
torch.distributions._InverseTransform.domain(self)
torch.distributions._InverseTransform.forward_shape(self,shape)
torch.distributions._InverseTransform.inv(self)
torch.distributions._InverseTransform.inverse_shape(self,shape)
torch.distributions._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions._InverseTransform.sign(self)
torch.distributions._InverseTransform.with_cache(self,cache_size=1)
torch.distributions._clipped_sigmoid(x)
torch.distributions.transforms.AbsTransform(Transform)
torch.distributions.transforms.AbsTransform.__eq__(self,other)
torch.distributions.transforms.AbsTransform._call(self,x)
torch.distributions.transforms.AbsTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform.__eq__(self,other)
torch.distributions.transforms.AffineTransform.__init__(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform._call(self,x)
torch.distributions.transforms.AffineTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform.codomain(self)
torch.distributions.transforms.AffineTransform.domain(self)
torch.distributions.transforms.AffineTransform.event_dim(self)
torch.distributions.transforms.AffineTransform.forward_shape(self,shape)
torch.distributions.transforms.AffineTransform.inverse_shape(self,shape)
torch.distributions.transforms.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.AffineTransform.sign(self)
torch.distributions.transforms.AffineTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.CatTransform(self,tseq,dim=0,lengths=None,cache_size=0)
torch.distributions.transforms.CatTransform.__init__(self,tseq,dim=0,lengths=None,cache_size=0)
torch.distributions.transforms.CatTransform._call(self,x)
torch.distributions.transforms.CatTransform._inverse(self,y)
torch.distributions.transforms.CatTransform.bijective(self)
torch.distributions.transforms.CatTransform.codomain(self)
torch.distributions.transforms.CatTransform.domain(self)
torch.distributions.transforms.CatTransform.event_dim(self)
torch.distributions.transforms.CatTransform.length(self)
torch.distributions.transforms.CatTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.CatTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.ComposeTransform(self,parts:List[Transform],cache_size=0)
torch.distributions.transforms.ComposeTransform.__eq__(self,other)
torch.distributions.transforms.ComposeTransform.__init__(self,parts:List[Transform],cache_size=0)
torch.distributions.transforms.ComposeTransform.__repr__(self)
torch.distributions.transforms.ComposeTransform.bijective(self)
torch.distributions.transforms.ComposeTransform.codomain(self)
torch.distributions.transforms.ComposeTransform.domain(self)
torch.distributions.transforms.ComposeTransform.forward_shape(self,shape)
torch.distributions.transforms.ComposeTransform.inv(self)
torch.distributions.transforms.ComposeTransform.inverse_shape(self,shape)
torch.distributions.transforms.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ComposeTransform.sign(self)
torch.distributions.transforms.ComposeTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.CorrCholeskyTransform(Transform)
torch.distributions.transforms.CorrCholeskyTransform._call(self,x)
torch.distributions.transforms.CorrCholeskyTransform._inverse(self,y)
torch.distributions.transforms.CorrCholeskyTransform.forward_shape(self,shape)
torch.distributions.transforms.CorrCholeskyTransform.inverse_shape(self,shape)
torch.distributions.transforms.CorrCholeskyTransform.log_abs_det_jacobian(self,x,y,intermediates=None)
torch.distributions.transforms.ExpTransform(Transform)
torch.distributions.transforms.ExpTransform.__eq__(self,other)
torch.distributions.transforms.ExpTransform._call(self,x)
torch.distributions.transforms.ExpTransform._inverse(self,y)
torch.distributions.transforms.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.IndependentTransform(self,base_transform,reinterpreted_batch_ndims,cache_size=0)
torch.distributions.transforms.IndependentTransform.__init__(self,base_transform,reinterpreted_batch_ndims,cache_size=0)
torch.distributions.transforms.IndependentTransform.__repr__(self)
torch.distributions.transforms.IndependentTransform._call(self,x)
torch.distributions.transforms.IndependentTransform._inverse(self,y)
torch.distributions.transforms.IndependentTransform.bijective(self)
torch.distributions.transforms.IndependentTransform.codomain(self)
torch.distributions.transforms.IndependentTransform.domain(self)
torch.distributions.transforms.IndependentTransform.forward_shape(self,shape)
torch.distributions.transforms.IndependentTransform.inverse_shape(self,shape)
torch.distributions.transforms.IndependentTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.IndependentTransform.sign(self)
torch.distributions.transforms.IndependentTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.LowerCholeskyTransform(Transform)
torch.distributions.transforms.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.transforms.LowerCholeskyTransform._call(self,x)
torch.distributions.transforms.LowerCholeskyTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform.__eq__(self,other)
torch.distributions.transforms.PowerTransform.__init__(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform._call(self,x)
torch.distributions.transforms.PowerTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform.forward_shape(self,shape)
torch.distributions.transforms.PowerTransform.inverse_shape(self,shape)
torch.distributions.transforms.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.PowerTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.ReshapeTransform(self,in_shape,out_shape,cache_size=0)
torch.distributions.transforms.ReshapeTransform.__init__(self,in_shape,out_shape,cache_size=0)
torch.distributions.transforms.ReshapeTransform._call(self,x)
torch.distributions.transforms.ReshapeTransform._inverse(self,y)
torch.distributions.transforms.ReshapeTransform.codomain(self)
torch.distributions.transforms.ReshapeTransform.domain(self)
torch.distributions.transforms.ReshapeTransform.forward_shape(self,shape)
torch.distributions.transforms.ReshapeTransform.inverse_shape(self,shape)
torch.distributions.transforms.ReshapeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ReshapeTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.SigmoidTransform(Transform)
torch.distributions.transforms.SigmoidTransform.__eq__(self,other)
torch.distributions.transforms.SigmoidTransform._call(self,x)
torch.distributions.transforms.SigmoidTransform._inverse(self,y)
torch.distributions.transforms.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SoftmaxTransform(Transform)
torch.distributions.transforms.SoftmaxTransform.__eq__(self,other)
torch.distributions.transforms.SoftmaxTransform._call(self,x)
torch.distributions.transforms.SoftmaxTransform._inverse(self,y)
torch.distributions.transforms.SoftmaxTransform.forward_shape(self,shape)
torch.distributions.transforms.SoftmaxTransform.inverse_shape(self,shape)
torch.distributions.transforms.StackTransform(self,tseq,dim=0,cache_size=0)
torch.distributions.transforms.StackTransform.__init__(self,tseq,dim=0,cache_size=0)
torch.distributions.transforms.StackTransform._call(self,x)
torch.distributions.transforms.StackTransform._inverse(self,y)
torch.distributions.transforms.StackTransform._slice(self,z)
torch.distributions.transforms.StackTransform.bijective(self)
torch.distributions.transforms.StackTransform.codomain(self)
torch.distributions.transforms.StackTransform.domain(self)
torch.distributions.transforms.StackTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.StackTransform.with_cache(self,cache_size=1)
torch.distributions.transforms.StickBreakingTransform(Transform)
torch.distributions.transforms.StickBreakingTransform.__eq__(self,other)
torch.distributions.transforms.StickBreakingTransform._call(self,x)
torch.distributions.transforms.StickBreakingTransform._inverse(self,y)
torch.distributions.transforms.StickBreakingTransform.forward_shape(self,shape)
torch.distributions.transforms.StickBreakingTransform.inverse_shape(self,shape)
torch.distributions.transforms.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.TanhTransform(Transform)
torch.distributions.transforms.TanhTransform.__eq__(self,other)
torch.distributions.transforms.TanhTransform._call(self,x)
torch.distributions.transforms.TanhTransform._inverse(self,y)
torch.distributions.transforms.TanhTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform(self,cache_size=0)
torch.distributions.transforms.Transform.__eq__(self,other)
torch.distributions.transforms.Transform.__init__(self,cache_size=0)
torch.distributions.transforms.Transform.__ne__(self,other)
torch.distributions.transforms.Transform.__repr__(self)
torch.distributions.transforms.Transform._call(self,x)
torch.distributions.transforms.Transform._inv_call(self,y)
torch.distributions.transforms.Transform._inverse(self,y)
torch.distributions.transforms.Transform.event_dim(self)
torch.distributions.transforms.Transform.forward_shape(self,shape)
torch.distributions.transforms.Transform.inv(self)
torch.distributions.transforms.Transform.inverse_shape(self,shape)
torch.distributions.transforms.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform.sign(self)
torch.distributions.transforms.Transform.with_cache(self,cache_size=1)
torch.distributions.transforms._InverseTransform(self,transform:Transform)
torch.distributions.transforms._InverseTransform.__eq__(self,other)
torch.distributions.transforms._InverseTransform.__init__(self,transform:Transform)
torch.distributions.transforms._InverseTransform.__repr__(self)
torch.distributions.transforms._InverseTransform.bijective(self)
torch.distributions.transforms._InverseTransform.codomain(self)
torch.distributions.transforms._InverseTransform.domain(self)
torch.distributions.transforms._InverseTransform.forward_shape(self,shape)
torch.distributions.transforms._InverseTransform.inv(self)
torch.distributions.transforms._InverseTransform.inverse_shape(self,shape)
torch.distributions.transforms._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms._InverseTransform.sign(self)
torch.distributions.transforms._InverseTransform.with_cache(self,cache_size=1)
torch.distributions.transforms._clipped_sigmoid(x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/constraint_registry.py----------------------------------------
A:torch.distributions.constraint_registry.constraint->type(constraint)
A:torch.distributions.constraint_registry.biject_to->ConstraintRegistry()
A:torch.distributions.constraint_registry.transform_to->ConstraintRegistry()
A:torch.distributions.constraint_registry.base_transform->transform_to(constraint.base_constraint)
torch.distributions.constraint_registry.ConstraintRegistry(self)
torch.distributions.constraint_registry.ConstraintRegistry.__init__(self)
torch.distributions.constraint_registry.ConstraintRegistry.register(self,constraint,factory=None)
torch.distributions.constraint_registry._biject_to_cat(constraint)
torch.distributions.constraint_registry._biject_to_independent(constraint)
torch.distributions.constraint_registry._biject_to_simplex(constraint)
torch.distributions.constraint_registry._biject_to_stack(constraint)
torch.distributions.constraint_registry._transform_to_cat(constraint)
torch.distributions.constraint_registry._transform_to_corr_cholesky(constraint)
torch.distributions.constraint_registry._transform_to_greater_than(constraint)
torch.distributions.constraint_registry._transform_to_independent(constraint)
torch.distributions.constraint_registry._transform_to_interval(constraint)
torch.distributions.constraint_registry._transform_to_less_than(constraint)
torch.distributions.constraint_registry._transform_to_lower_cholesky(constraint)
torch.distributions.constraint_registry._transform_to_positive(constraint)
torch.distributions.constraint_registry._transform_to_real(constraint)
torch.distributions.constraint_registry._transform_to_simplex(constraint)
torch.distributions.constraint_registry._transform_to_stack(constraint)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/relaxed_categorical.py----------------------------------------
A:torch.distributions.relaxed_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.relaxed_categorical.new->self._get_checked_instance(RelaxedOneHotCategorical, _instance)
A:torch.distributions.relaxed_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.relaxed_categorical.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_categorical.uniforms->clamp_probs(torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device))
A:torch.distributions.relaxed_categorical.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_categorical.score->(score - score.logsumexp(dim=-1, keepdim=True)).sum(-1)
A:torch.distributions.relaxed_categorical.base_dist->ExpRelaxedCategorical(temperature, probs, logits, validate_args=validate_args)
torch.distributions.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedOneHotCategorical.logits(self)
torch.distributions.RelaxedOneHotCategorical.probs(self)
torch.distributions.RelaxedOneHotCategorical.temperature(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical._new(self,*args,**kwargs)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.log_prob(self,value)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.logits(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.param_shape(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.probs(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/lkj_cholesky.py----------------------------------------
A:torch.distributions.lkj_cholesky.(self.concentration,)->broadcast_all(concentration)
A:torch.distributions.lkj_cholesky.batch_shape->torch.Size(batch_shape)
A:torch.distributions.lkj_cholesky.event_shape->torch.Size((dim, dim))
A:torch.distributions.lkj_cholesky.offset->torch.cat([offset.new_zeros((1,)), offset])
A:torch.distributions.lkj_cholesky.self._beta->Beta(beta_conc1, beta_conc0)
A:torch.distributions.lkj_cholesky.new->self._get_checked_instance(LKJCholesky, _instance)
A:torch.distributions.lkj_cholesky.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.lkj_cholesky.new._beta->self._beta.expand(batch_shape + (self.dim,))
A:torch.distributions.lkj_cholesky.y->self._beta.sample(sample_shape).unsqueeze(-1)
A:torch.distributions.lkj_cholesky.u_normal->torch.randn(self._extended_shape(sample_shape), dtype=y.dtype, device=y.device).tril(-1)
A:torch.distributions.lkj_cholesky.diag_elems->torch.clamp(1 - torch.sum(w ** 2, dim=-1), min=eps).sqrt()
A:torch.distributions.lkj_cholesky.order->torch.arange(2, self.dim + 1)
A:torch.distributions.lkj_cholesky.unnormalized_log_pdf->torch.sum(order * diag_elems.log(), dim=-1)
A:torch.distributions.lkj_cholesky.numerator->torch.mvlgamma(alpha - 0.5, dm1)
torch.distributions.LKJCholesky(self,dim,concentration=1.0,validate_args=None)
torch.distributions.LKJCholesky.expand(self,batch_shape,_instance=None)
torch.distributions.LKJCholesky.log_prob(self,value)
torch.distributions.LKJCholesky.sample(self,sample_shape=torch.Size())
torch.distributions.lkj_cholesky.LKJCholesky(self,dim,concentration=1.0,validate_args=None)
torch.distributions.lkj_cholesky.LKJCholesky.__init__(self,dim,concentration=1.0,validate_args=None)
torch.distributions.lkj_cholesky.LKJCholesky.expand(self,batch_shape,_instance=None)
torch.distributions.lkj_cholesky.LKJCholesky.log_prob(self,value)
torch.distributions.lkj_cholesky.LKJCholesky.sample(self,sample_shape=torch.Size())


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/von_mises.py----------------------------------------
A:torch.distributions.von_mises.coef->list(coef)
A:torch.distributions.von_mises.result->torch.where(x < 3.75, small, large)
A:torch.distributions.von_mises.small->small.log().log()
A:torch.distributions.von_mises.done->torch.zeros(x.shape, dtype=torch.bool, device=loc.device)
A:torch.distributions.von_mises.u->torch.rand((3,) + x.shape, dtype=loc.dtype, device=loc.device)
A:torch.distributions.von_mises.(u1, u2, u3)->torch.rand((3,) + x.shape, dtype=loc.dtype, device=loc.device).unbind()
A:torch.distributions.von_mises.z->torch.cos(math.pi * u1)
A:torch.distributions.von_mises.x->torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.von_mises.(self.loc, self.concentration)->broadcast_all(loc, concentration)
A:torch.distributions.von_mises.event_shape->torch.Size()
A:torch.distributions.von_mises.shape->self._extended_shape(sample_shape)
A:torch.distributions.von_mises.validate_args->self.__dict__.get('_validate_args')
A:torch.distributions.von_mises.loc->self.loc.expand(batch_shape)
A:torch.distributions.von_mises.concentration->self.concentration.expand(batch_shape)
torch.distributions.VonMises(self,loc,concentration,validate_args=None)
torch.distributions.VonMises.expand(self,batch_shape)
torch.distributions.VonMises.log_prob(self,value)
torch.distributions.VonMises.mean(self)
torch.distributions.VonMises.sample(self,sample_shape=torch.Size())
torch.distributions.VonMises.variance(self)
torch.distributions.von_mises.VonMises(self,loc,concentration,validate_args=None)
torch.distributions.von_mises.VonMises.__init__(self,loc,concentration,validate_args=None)
torch.distributions.von_mises.VonMises.expand(self,batch_shape)
torch.distributions.von_mises.VonMises.log_prob(self,value)
torch.distributions.von_mises.VonMises.mean(self)
torch.distributions.von_mises.VonMises.sample(self,sample_shape=torch.Size())
torch.distributions.von_mises.VonMises.variance(self)
torch.distributions.von_mises._eval_poly(y,coef)
torch.distributions.von_mises._log_modified_bessel_fn(x,order=0)
torch.distributions.von_mises._rejection_sample(loc,concentration,proposal_r,x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/normal.py----------------------------------------
A:torch.distributions.normal.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.normal.new->self._get_checked_instance(Normal, _instance)
A:torch.distributions.normal.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.normal.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
torch.distributions.Normal(self,loc,scale,validate_args=None)
torch.distributions.Normal._log_normalizer(self,x,y)
torch.distributions.Normal._natural_params(self)
torch.distributions.Normal.cdf(self,value)
torch.distributions.Normal.entropy(self)
torch.distributions.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.Normal.icdf(self,value)
torch.distributions.Normal.log_prob(self,value)
torch.distributions.Normal.mean(self)
torch.distributions.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.Normal.stddev(self)
torch.distributions.Normal.variance(self)
torch.distributions.normal.Normal(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal.__init__(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal._log_normalizer(self,x,y)
torch.distributions.normal.Normal._natural_params(self)
torch.distributions.normal.Normal.cdf(self,value)
torch.distributions.normal.Normal.entropy(self)
torch.distributions.normal.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.normal.Normal.icdf(self,value)
torch.distributions.normal.Normal.log_prob(self,value)
torch.distributions.normal.Normal.mean(self)
torch.distributions.normal.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.stddev(self)
torch.distributions.normal.Normal.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/multinomial.py----------------------------------------
A:torch.distributions.multinomial.self._categorical->Categorical(probs=probs, logits=logits)
A:torch.distributions.multinomial.self._binomial->Binomial(total_count=total_count, probs=self.probs)
A:torch.distributions.multinomial.new->self._get_checked_instance(Multinomial, _instance)
A:torch.distributions.multinomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multinomial.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.multinomial.sample_shape->torch.Size(sample_shape)
A:torch.distributions.multinomial.samples->samples.permute(*shifted_idx).permute(*shifted_idx)
A:torch.distributions.multinomial.shifted_idx->list(range(samples.dim()))
A:torch.distributions.multinomial.counts->samples.permute(*shifted_idx).permute(*shifted_idx).new(self._extended_shape(sample_shape)).zero_()
A:torch.distributions.multinomial.n->torch.tensor(self.total_count)
A:torch.distributions.multinomial.cat_entropy->self._categorical.entropy()
A:torch.distributions.multinomial.binomial_probs->torch.exp(self._binomial.log_prob(support))
A:torch.distributions.multinomial.weights->torch.lgamma(support + 1)
A:torch.distributions.multinomial.term2->(binomial_probs * weights).sum([0, -1])
A:torch.distributions.multinomial.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.multinomial.logits->logits.clone(memory_format=torch.contiguous_format).clone(memory_format=torch.contiguous_format)
A:torch.distributions.multinomial.log_factorial_n->torch.lgamma(value.sum(-1) + 1)
A:torch.distributions.multinomial.log_factorial_xs->torch.lgamma(value + 1).sum(-1)
A:torch.distributions.multinomial.log_powers->(logits * value).sum(-1)
torch.distributions.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Multinomial._new(self,*args,**kwargs)
torch.distributions.Multinomial.entropy(self)
torch.distributions.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.Multinomial.log_prob(self,value)
torch.distributions.Multinomial.logits(self)
torch.distributions.Multinomial.mean(self)
torch.distributions.Multinomial.param_shape(self)
torch.distributions.Multinomial.probs(self)
torch.distributions.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.Multinomial.support(self)
torch.distributions.Multinomial.variance(self)
torch.distributions.multinomial.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial._new(self,*args,**kwargs)
torch.distributions.multinomial.Multinomial.entropy(self)
torch.distributions.multinomial.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.multinomial.Multinomial.log_prob(self,value)
torch.distributions.multinomial.Multinomial.logits(self)
torch.distributions.multinomial.Multinomial.mean(self)
torch.distributions.multinomial.Multinomial.param_shape(self)
torch.distributions.multinomial.Multinomial.probs(self)
torch.distributions.multinomial.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.multinomial.Multinomial.support(self)
torch.distributions.multinomial.Multinomial.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/negative_binomial.py----------------------------------------
A:torch.distributions.negative_binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.negative_binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.negative_binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.negative_binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.negative_binomial.new->self._get_checked_instance(NegativeBinomial, _instance)
A:torch.distributions.negative_binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.negative_binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.negative_binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.negative_binomial.rate->self._gamma.sample(sample_shape=sample_shape)
torch.distributions.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.NegativeBinomial._gamma(self)
torch.distributions.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.NegativeBinomial.log_prob(self,value)
torch.distributions.NegativeBinomial.logits(self)
torch.distributions.NegativeBinomial.mean(self)
torch.distributions.NegativeBinomial.param_shape(self)
torch.distributions.NegativeBinomial.probs(self)
torch.distributions.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.NegativeBinomial.variance(self)
torch.distributions.negative_binomial.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial.__init__(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial._gamma(self)
torch.distributions.negative_binomial.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.negative_binomial.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.negative_binomial.NegativeBinomial.log_prob(self,value)
torch.distributions.negative_binomial.NegativeBinomial.logits(self)
torch.distributions.negative_binomial.NegativeBinomial.mean(self)
torch.distributions.negative_binomial.NegativeBinomial.param_shape(self)
torch.distributions.negative_binomial.NegativeBinomial.probs(self)
torch.distributions.negative_binomial.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.negative_binomial.NegativeBinomial.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/chi2.py----------------------------------------
A:torch.distributions.chi2.new->self._get_checked_instance(Chi2, _instance)
torch.distributions.Chi2(self,df,validate_args=None)
torch.distributions.Chi2.df(self)
torch.distributions.Chi2.expand(self,batch_shape,_instance=None)
torch.distributions.chi2.Chi2(self,df,validate_args=None)
torch.distributions.chi2.Chi2.__init__(self,df,validate_args=None)
torch.distributions.chi2.Chi2.df(self)
torch.distributions.chi2.Chi2.expand(self,batch_shape,_instance=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/kl.py----------------------------------------
A:torch.distributions.kl.n->bmat.size(-1)
A:torch.distributions.kl.m->bmat.size(-2)
A:torch.distributions.kl.flat_trace->bmat.reshape(-1, m * n).pow(2).sum(-1)
A:torch.distributions.kl.fun->_dispatch_kl(type(p), type(q))
A:torch.distributions.kl.kl[inf_idxs]->_infinite_like(kl[inf_idxs])
A:torch.distributions.kl.sum_p_concentration->p.concentration.sum(-1)
A:torch.distributions.kl.sum_q_concentration->q.concentration.sum(-1)
A:torch.distributions.kl.t2->(4 * p.scale * q.scale).log()
A:torch.distributions.kl.lg_normal->p._log_normalizer(*p_nparams)
A:torch.distributions.kl.gradients->torch.autograd.grad(lg_normal.sum(), p_nparams, create_graph=True)
A:torch.distributions.kl.t3->((p.high + p.low - 2 * q.loc) / 2).pow(2)
A:torch.distributions.kl.loc_abs_diff->(p.loc - q.loc).abs()
A:torch.distributions.kl.term3->_batch_mahalanobis(q._unbroadcasted_scale_tril, q.loc - p.loc)
A:torch.distributions.kl.A->torch.linalg.solve_triangular(q._capacitance_tril, qWt_qDinv, upper=False)
A:torch.distributions.kl.term21->_batch_trace_XXT(torch.linalg.solve_triangular(q_scale_tril, p_cov_factor, upper=False))
A:torch.distributions.kl.term22->_batch_trace_XXT(torch.linalg.solve_triangular(q_scale_tril, p_cov_diag, upper=False))
A:torch.distributions.kl.term23->_batch_trace_XXT(A * p._unbroadcasted_cov_diag.sqrt().unsqueeze(-2))
A:torch.distributions.kl.term24->_batch_trace_XXT(A.matmul(p._unbroadcasted_cov_factor))
A:torch.distributions.kl.combined_batch_shape->torch._C._infer_size(q._unbroadcasted_scale_tril.shape[:-2], p._unbroadcasted_scale_tril.shape[:-2])
A:torch.distributions.kl.q_scale_tril->q._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_cov_factor->p._unbroadcasted_cov_factor.expand(combined_batch_shape + (n, p.cov_factor.size(-1)))
A:torch.distributions.kl.p_cov_diag->torch.diag_embed(p._unbroadcasted_cov_diag.sqrt()).expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_scale_tril->p._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.term2->_batch_trace_XXT(torch.linalg.solve_triangular(q_scale_tril, p_scale_tril, upper=False))
A:torch.distributions.kl.var_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t1->((p.scale + q.scale).pow(2) + (p.loc - q.loc).pow(2)).log()
A:torch.distributions.kl.result->kl_divergence(p.base_dist, q.base_dist)
A:torch.distributions.kl.var_normal->q.scale.pow(2)
A:torch.distributions.kl.rate_sqr->p.rate.pow(2)
A:torch.distributions.kl.beta_sqr->p.rate.pow(2)
A:torch.distributions.kl.var_scale_sqr_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t4->(p.alpha * common_term - q.loc).pow(2)
torch.distributions.kl._Match(self,*types)
torch.distributions.kl._Match.__eq__(self,other)
torch.distributions.kl._Match.__init__(self,*types)
torch.distributions.kl._Match.__le__(self,other)
torch.distributions.kl._batch_trace_XXT(bmat)
torch.distributions.kl._dispatch_kl(type_p,type_q)
torch.distributions.kl._infinite_like(tensor)
torch.distributions.kl._kl_bernoulli_bernoulli(p,q)
torch.distributions.kl._kl_bernoulli_poisson(p,q)
torch.distributions.kl._kl_beta_beta(p,q)
torch.distributions.kl._kl_beta_continuous_bernoulli(p,q)
torch.distributions.kl._kl_beta_exponential(p,q)
torch.distributions.kl._kl_beta_gamma(p,q)
torch.distributions.kl._kl_beta_infinity(p,q)
torch.distributions.kl._kl_beta_normal(p,q)
torch.distributions.kl._kl_beta_uniform(p,q)
torch.distributions.kl._kl_binomial_binomial(p,q)
torch.distributions.kl._kl_categorical_categorical(p,q)
torch.distributions.kl._kl_cauchy_cauchy(p,q)
torch.distributions.kl._kl_continuous_bernoulli_continuous_bernoulli(p,q)
torch.distributions.kl._kl_continuous_bernoulli_exponential(p,q)
torch.distributions.kl._kl_continuous_bernoulli_infinity(p,q)
torch.distributions.kl._kl_continuous_bernoulli_normal(p,q)
torch.distributions.kl._kl_continuous_bernoulli_uniform(p,q)
torch.distributions.kl._kl_dirichlet_dirichlet(p,q)
torch.distributions.kl._kl_expfamily_expfamily(p,q)
torch.distributions.kl._kl_exponential_exponential(p,q)
torch.distributions.kl._kl_exponential_gamma(p,q)
torch.distributions.kl._kl_exponential_gumbel(p,q)
torch.distributions.kl._kl_exponential_infinity(p,q)
torch.distributions.kl._kl_exponential_normal(p,q)
torch.distributions.kl._kl_gamma_exponential(p,q)
torch.distributions.kl._kl_gamma_gamma(p,q)
torch.distributions.kl._kl_gamma_gumbel(p,q)
torch.distributions.kl._kl_gamma_infinity(p,q)
torch.distributions.kl._kl_gamma_normal(p,q)
torch.distributions.kl._kl_geometric_geometric(p,q)
torch.distributions.kl._kl_gumbel_gumbel(p,q)
torch.distributions.kl._kl_gumbel_infinity(p,q)
torch.distributions.kl._kl_gumbel_normal(p,q)
torch.distributions.kl._kl_halfnormal_halfnormal(p,q)
torch.distributions.kl._kl_independent_independent(p,q)
torch.distributions.kl._kl_laplace_infinity(p,q)
torch.distributions.kl._kl_laplace_laplace(p,q)
torch.distributions.kl._kl_laplace_normal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_normal_gumbel(p,q)
torch.distributions.kl._kl_normal_infinity(p,q)
torch.distributions.kl._kl_normal_laplace(p,q)
torch.distributions.kl._kl_normal_normal(p,q)
torch.distributions.kl._kl_onehotcategorical_onehotcategorical(p,q)
torch.distributions.kl._kl_pareto_exponential(p,q)
torch.distributions.kl._kl_pareto_gamma(p,q)
torch.distributions.kl._kl_pareto_infinity(p,q)
torch.distributions.kl._kl_pareto_normal(p,q)
torch.distributions.kl._kl_pareto_pareto(p,q)
torch.distributions.kl._kl_poisson_infinity(p,q)
torch.distributions.kl._kl_poisson_poisson(p,q)
torch.distributions.kl._kl_transformed_transformed(p,q)
torch.distributions.kl._kl_uniform_beta(p,q)
torch.distributions.kl._kl_uniform_continuous_bernoulli(p,q)
torch.distributions.kl._kl_uniform_exponetial(p,q)
torch.distributions.kl._kl_uniform_gamma(p,q)
torch.distributions.kl._kl_uniform_gumbel(p,q)
torch.distributions.kl._kl_uniform_normal(p,q)
torch.distributions.kl._kl_uniform_pareto(p,q)
torch.distributions.kl._kl_uniform_uniform(p,q)
torch.distributions.kl._x_log_x(tensor)
torch.distributions.kl.kl_divergence(p,q)
torch.distributions.kl.register_kl(type_p,type_q)
torch.distributions.kl_divergence(p,q)
torch.distributions.register_kl(type_p,type_q)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/bernoulli.py----------------------------------------
A:torch.distributions.bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.bernoulli.new->self._get_checked_instance(Bernoulli, _instance)
A:torch.distributions.bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.bernoulli.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.Bernoulli._log_normalizer(self,x)
torch.distributions.Bernoulli._natural_params(self)
torch.distributions.Bernoulli._new(self,*args,**kwargs)
torch.distributions.Bernoulli.entropy(self)
torch.distributions.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.Bernoulli.log_prob(self,value)
torch.distributions.Bernoulli.logits(self)
torch.distributions.Bernoulli.mean(self)
torch.distributions.Bernoulli.param_shape(self)
torch.distributions.Bernoulli.probs(self)
torch.distributions.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.Bernoulli.variance(self)
torch.distributions.bernoulli.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli._log_normalizer(self,x)
torch.distributions.bernoulli.Bernoulli._natural_params(self)
torch.distributions.bernoulli.Bernoulli._new(self,*args,**kwargs)
torch.distributions.bernoulli.Bernoulli.entropy(self)
torch.distributions.bernoulli.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.bernoulli.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.bernoulli.Bernoulli.log_prob(self,value)
torch.distributions.bernoulli.Bernoulli.logits(self)
torch.distributions.bernoulli.Bernoulli.mean(self)
torch.distributions.bernoulli.Bernoulli.param_shape(self)
torch.distributions.bernoulli.Bernoulli.probs(self)
torch.distributions.bernoulli.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.bernoulli.Bernoulli.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/gumbel.py----------------------------------------
A:torch.distributions.gumbel.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.gumbel.finfo->torch.finfo(self.loc.dtype)
A:torch.distributions.gumbel.base_dist->Uniform(torch.full_like(self.loc, finfo.tiny), torch.full_like(self.loc, 1 - finfo.eps))
A:torch.distributions.gumbel.new->self._get_checked_instance(Gumbel, _instance)
A:torch.distributions.gumbel.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.gumbel.new.scale->self.scale.expand(batch_shape)
torch.distributions.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.Gumbel.entropy(self)
torch.distributions.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.Gumbel.log_prob(self,value)
torch.distributions.Gumbel.mean(self)
torch.distributions.Gumbel.stddev(self)
torch.distributions.Gumbel.variance(self)
torch.distributions.gumbel.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.__init__(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.entropy(self)
torch.distributions.gumbel.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.gumbel.Gumbel.log_prob(self,value)
torch.distributions.gumbel.Gumbel.mean(self)
torch.distributions.gumbel.Gumbel.stddev(self)
torch.distributions.gumbel.Gumbel.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/dirichlet.py----------------------------------------
A:torch.distributions.dirichlet.total->self.concentration.expand(shape).sum(-1, True).expand_as(concentration)
A:torch.distributions.dirichlet.grad->torch._dirichlet_grad(x, concentration, total)
A:torch.distributions.dirichlet.x->torch._sample_dirichlet(concentration)
A:torch.distributions.dirichlet.new->self._get_checked_instance(Dirichlet, _instance)
A:torch.distributions.dirichlet.batch_shape->torch.Size(batch_shape)
A:torch.distributions.dirichlet.new.concentration->self.concentration.expand(batch_shape + self.event_shape)
A:torch.distributions.dirichlet.shape->self._extended_shape(sample_shape)
A:torch.distributions.dirichlet.concentration->self.concentration.expand(shape)
A:torch.distributions.dirichlet.con0->self.concentration.sum(-1, True)
A:torch.distributions.dirichlet.k->self.concentration.size(-1)
A:torch.distributions.dirichlet.a0->self.concentration.sum(-1)
torch.distributions.Dirichlet(self,concentration,validate_args=None)
torch.distributions.Dirichlet._log_normalizer(self,x)
torch.distributions.Dirichlet._natural_params(self)
torch.distributions.Dirichlet.entropy(self)
torch.distributions.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.Dirichlet.log_prob(self,value)
torch.distributions.Dirichlet.mean(self)
torch.distributions.Dirichlet.rsample(self,sample_shape=())
torch.distributions.Dirichlet.variance(self)
torch.distributions.dirichlet.Dirichlet(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet.__init__(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet._log_normalizer(self,x)
torch.distributions.dirichlet.Dirichlet._natural_params(self)
torch.distributions.dirichlet.Dirichlet.entropy(self)
torch.distributions.dirichlet.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.dirichlet.Dirichlet.log_prob(self,value)
torch.distributions.dirichlet.Dirichlet.mean(self)
torch.distributions.dirichlet.Dirichlet.rsample(self,sample_shape=())
torch.distributions.dirichlet.Dirichlet.variance(self)
torch.distributions.dirichlet._Dirichlet(Function)
torch.distributions.dirichlet._Dirichlet.backward(ctx,grad_output)
torch.distributions.dirichlet._Dirichlet.forward(ctx,concentration)
torch.distributions.dirichlet._Dirichlet_backward(x,concentration,grad_output)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/relaxed_bernoulli.py----------------------------------------
A:torch.distributions.relaxed_bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.relaxed_bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.relaxed_bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.relaxed_bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_bernoulli.new->self._get_checked_instance(RelaxedBernoulli, _instance)
A:torch.distributions.relaxed_bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_bernoulli.probs->clamp_probs(self.probs.expand(shape))
A:torch.distributions.relaxed_bernoulli.uniforms->clamp_probs(torch.rand(shape, dtype=probs.dtype, device=probs.device))
A:torch.distributions.relaxed_bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_bernoulli.base_dist->LogitRelaxedBernoulli(temperature, probs, logits)
torch.distributions.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedBernoulli.logits(self)
torch.distributions.RelaxedBernoulli.probs(self)
torch.distributions.RelaxedBernoulli.temperature(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli._new(self,*args,**kwargs)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob(self,value)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_bernoulli.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/independent.py----------------------------------------
A:torch.distributions.independent.new->self._get_checked_instance(Independent, _instance)
A:torch.distributions.independent.batch_shape->torch.Size(batch_shape)
A:torch.distributions.independent.new.base_dist->self.base_dist.expand(batch_shape + self.event_shape[:self.reinterpreted_batch_ndims])
A:torch.distributions.independent.result->torch.distributions.constraints.independent(result, self.reinterpreted_batch_ndims)
A:torch.distributions.independent.log_prob->self.base_dist.log_prob(value)
A:torch.distributions.independent.entropy->self.base_dist.entropy()
torch.distributions.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.Independent.__repr__(self)
torch.distributions.Independent.entropy(self)
torch.distributions.Independent.enumerate_support(self,expand=True)
torch.distributions.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.Independent.has_enumerate_support(self)
torch.distributions.Independent.has_rsample(self)
torch.distributions.Independent.log_prob(self,value)
torch.distributions.Independent.mean(self)
torch.distributions.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.Independent.support(self)
torch.distributions.Independent.variance(self)
torch.distributions.independent.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.__init__(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.__repr__(self)
torch.distributions.independent.Independent.entropy(self)
torch.distributions.independent.Independent.enumerate_support(self,expand=True)
torch.distributions.independent.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.independent.Independent.has_enumerate_support(self)
torch.distributions.independent.Independent.has_rsample(self)
torch.distributions.independent.Independent.log_prob(self,value)
torch.distributions.independent.Independent.mean(self)
torch.distributions.independent.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.support(self)
torch.distributions.independent.Independent.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/uniform.py----------------------------------------
A:torch.distributions.uniform.(self.low, self.high)->broadcast_all(low, high)
A:torch.distributions.uniform.batch_shape->torch.Size(batch_shape)
A:torch.distributions.uniform.new->self._get_checked_instance(Uniform, _instance)
A:torch.distributions.uniform.new.low->self.low.expand(batch_shape)
A:torch.distributions.uniform.new.high->self.high.expand(batch_shape)
A:torch.distributions.uniform.shape->self._extended_shape(sample_shape)
A:torch.distributions.uniform.rand->torch.rand(shape, dtype=self.low.dtype, device=self.low.device)
A:torch.distributions.uniform.lb->self.low.le(value).type_as(self.low)
A:torch.distributions.uniform.ub->self.high.gt(value).type_as(self.low)
torch.distributions.Uniform(self,low,high,validate_args=None)
torch.distributions.Uniform.cdf(self,value)
torch.distributions.Uniform.entropy(self)
torch.distributions.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.Uniform.icdf(self,value)
torch.distributions.Uniform.log_prob(self,value)
torch.distributions.Uniform.mean(self)
torch.distributions.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.Uniform.stddev(self)
torch.distributions.Uniform.support(self)
torch.distributions.Uniform.variance(self)
torch.distributions.uniform.Uniform(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.__init__(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.cdf(self,value)
torch.distributions.uniform.Uniform.entropy(self)
torch.distributions.uniform.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.uniform.Uniform.icdf(self,value)
torch.distributions.uniform.Uniform.log_prob(self,value)
torch.distributions.uniform.Uniform.mean(self)
torch.distributions.uniform.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.uniform.Uniform.stddev(self)
torch.distributions.uniform.Uniform.support(self)
torch.distributions.uniform.Uniform.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/categorical.py----------------------------------------
A:torch.distributions.categorical.new->self._get_checked_instance(Categorical, _instance)
A:torch.distributions.categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.categorical.new.probs->self.probs.expand(param_shape)
A:torch.distributions.categorical.new.logits->self.logits.expand(param_shape)
A:torch.distributions.categorical.sample_shape->torch.Size(sample_shape)
A:torch.distributions.categorical.probs_2d->self.probs.reshape(-1, self._num_events)
A:torch.distributions.categorical.value->value.long().unsqueeze(-1).long().unsqueeze(-1)
A:torch.distributions.categorical.(value, log_pmf)->torch.broadcast_tensors(value, self.logits)
A:torch.distributions.categorical.logits->torch.clamp(self.logits, min=min_real)
A:torch.distributions.categorical.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.Categorical._new(self,*args,**kwargs)
torch.distributions.Categorical.entropy(self)
torch.distributions.Categorical.enumerate_support(self,expand=True)
torch.distributions.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.Categorical.log_prob(self,value)
torch.distributions.Categorical.logits(self)
torch.distributions.Categorical.mean(self)
torch.distributions.Categorical.param_shape(self)
torch.distributions.Categorical.probs(self)
torch.distributions.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.Categorical.support(self)
torch.distributions.Categorical.variance(self)
torch.distributions.categorical.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical._new(self,*args,**kwargs)
torch.distributions.categorical.Categorical.entropy(self)
torch.distributions.categorical.Categorical.enumerate_support(self,expand=True)
torch.distributions.categorical.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.categorical.Categorical.log_prob(self,value)
torch.distributions.categorical.Categorical.logits(self)
torch.distributions.categorical.Categorical.mean(self)
torch.distributions.categorical.Categorical.param_shape(self)
torch.distributions.categorical.Categorical.probs(self)
torch.distributions.categorical.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.categorical.Categorical.support(self)
torch.distributions.categorical.Categorical.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/continuous_bernoulli.py----------------------------------------
A:torch.distributions.continuous_bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.continuous_bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.continuous_bernoulli.self.probs->clamp_probs(self.probs)
A:torch.distributions.continuous_bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.continuous_bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.continuous_bernoulli.new->self._get_checked_instance(ContinuousBernoulli, _instance)
A:torch.distributions.continuous_bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.continuous_bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.continuous_bernoulli.cut_probs->self._cut_probs()
A:torch.distributions.continuous_bernoulli.cut_probs_below_half->torch.where(torch.le(cut_probs, 0.5), cut_probs, torch.zeros_like(cut_probs))
A:torch.distributions.continuous_bernoulli.cut_probs_above_half->torch.where(torch.ge(cut_probs, 0.5), cut_probs, torch.ones_like(cut_probs))
A:torch.distributions.continuous_bernoulli.x->torch.pow(self.probs - 0.5, 2)
A:torch.distributions.continuous_bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.continuous_bernoulli.u->torch.rand(shape, dtype=self.probs.dtype, device=self.probs.device)
A:torch.distributions.continuous_bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.continuous_bernoulli.unbounded_cdfs->torch.where(self._outside_unstable_region(), cdfs, value)
A:torch.distributions.continuous_bernoulli.log_probs0->torch.log1p(-self.probs)
A:torch.distributions.continuous_bernoulli.log_probs1->torch.log(self.probs)
A:torch.distributions.continuous_bernoulli.out_unst_reg->torch.max(torch.le(x, self._lims[0] - 0.5), torch.gt(x, self._lims[1] - 0.5))
A:torch.distributions.continuous_bernoulli.cut_nat_params->torch.where(out_unst_reg, x, (self._lims[0] - 0.5) * torch.ones_like(x))
torch.distributions.ContinuousBernoulli(self,probs=None,logits=None,lims=(0.499,0.501),validate_args=None)
torch.distributions.ContinuousBernoulli._cont_bern_log_norm(self)
torch.distributions.ContinuousBernoulli._cut_probs(self)
torch.distributions.ContinuousBernoulli._log_normalizer(self,x)
torch.distributions.ContinuousBernoulli._natural_params(self)
torch.distributions.ContinuousBernoulli._new(self,*args,**kwargs)
torch.distributions.ContinuousBernoulli._outside_unstable_region(self)
torch.distributions.ContinuousBernoulli.cdf(self,value)
torch.distributions.ContinuousBernoulli.entropy(self)
torch.distributions.ContinuousBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.ContinuousBernoulli.icdf(self,value)
torch.distributions.ContinuousBernoulli.log_prob(self,value)
torch.distributions.ContinuousBernoulli.logits(self)
torch.distributions.ContinuousBernoulli.mean(self)
torch.distributions.ContinuousBernoulli.param_shape(self)
torch.distributions.ContinuousBernoulli.probs(self)
torch.distributions.ContinuousBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.ContinuousBernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.ContinuousBernoulli.stddev(self)
torch.distributions.ContinuousBernoulli.variance(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli(self,probs=None,logits=None,lims=(0.499,0.501),validate_args=None)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.__init__(self,probs=None,logits=None,lims=(0.499,0.501),validate_args=None)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._cont_bern_log_norm(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._cut_probs(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._log_normalizer(self,x)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._natural_params(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._new(self,*args,**kwargs)
torch.distributions.continuous_bernoulli.ContinuousBernoulli._outside_unstable_region(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf(self,value)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf(self,value)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob(self,value)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev(self)
torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/studentT.py----------------------------------------
A:torch.distributions.studentT.m->self.df.clone(memory_format=torch.contiguous_format)
A:torch.distributions.studentT.(self.df, self.loc, self.scale)->broadcast_all(df, loc, scale)
A:torch.distributions.studentT.self._chi2->Chi2(self.df)
A:torch.distributions.studentT.batch_shape->torch.Size(batch_shape)
A:torch.distributions.studentT.new->self._get_checked_instance(StudentT, _instance)
A:torch.distributions.studentT.new.df->self.df.expand(batch_shape)
A:torch.distributions.studentT.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.studentT.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.studentT.new._chi2->self._chi2.expand(batch_shape)
A:torch.distributions.studentT.shape->self._extended_shape(sample_shape)
A:torch.distributions.studentT.X->_standard_normal(shape, dtype=self.df.dtype, device=self.df.device)
A:torch.distributions.studentT.Z->self._chi2.rsample(sample_shape)
torch.distributions.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.StudentT.entropy(self)
torch.distributions.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.StudentT.log_prob(self,value)
torch.distributions.StudentT.mean(self)
torch.distributions.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.StudentT.variance(self)
torch.distributions.studentT.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.__init__(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.entropy(self)
torch.distributions.studentT.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.studentT.StudentT.log_prob(self,value)
torch.distributions.studentT.StudentT.mean(self)
torch.distributions.studentT.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.studentT.StudentT.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/binomial.py----------------------------------------
A:torch.distributions.binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.binomial.new->self._get_checked_instance(Binomial, _instance)
A:torch.distributions.binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.binomial.shape->self._extended_shape(sample_shape)
A:torch.distributions.binomial.log_factorial_n->torch.lgamma(self.total_count + 1)
A:torch.distributions.binomial.log_factorial_k->torch.lgamma(value + 1)
A:torch.distributions.binomial.log_factorial_nmk->torch.lgamma(self.total_count - value + 1)
A:torch.distributions.binomial.total_count->int(self.total_count.max())
A:torch.distributions.binomial.log_prob->self.log_prob(self.enumerate_support(False))
A:torch.distributions.binomial.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Binomial._new(self,*args,**kwargs)
torch.distributions.Binomial.entropy(self)
torch.distributions.Binomial.enumerate_support(self,expand=True)
torch.distributions.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.Binomial.log_prob(self,value)
torch.distributions.Binomial.logits(self)
torch.distributions.Binomial.mean(self)
torch.distributions.Binomial.param_shape(self)
torch.distributions.Binomial.probs(self)
torch.distributions.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.Binomial.support(self)
torch.distributions.Binomial.variance(self)
torch.distributions.binomial.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial._new(self,*args,**kwargs)
torch.distributions.binomial.Binomial.entropy(self)
torch.distributions.binomial.Binomial.enumerate_support(self,expand=True)
torch.distributions.binomial.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.binomial.Binomial.log_prob(self,value)
torch.distributions.binomial.Binomial.logits(self)
torch.distributions.binomial.Binomial.mean(self)
torch.distributions.binomial.Binomial.param_shape(self)
torch.distributions.binomial.Binomial.probs(self)
torch.distributions.binomial.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.binomial.Binomial.support(self)
torch.distributions.binomial.Binomial.variance(self)
torch.distributions.binomial._clamp_by_zero(x)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/half_cauchy.py----------------------------------------
A:torch.distributions.half_cauchy.base_dist->Cauchy(0, scale, validate_args=False)
A:torch.distributions.half_cauchy.new->self._get_checked_instance(HalfCauchy, _instance)
A:torch.distributions.half_cauchy.value->torch.as_tensor(value, dtype=self.base_dist.scale.dtype, device=self.base_dist.scale.device)
torch.distributions.HalfCauchy(self,scale,validate_args=None)
torch.distributions.HalfCauchy.cdf(self,value)
torch.distributions.HalfCauchy.entropy(self)
torch.distributions.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.HalfCauchy.icdf(self,prob)
torch.distributions.HalfCauchy.log_prob(self,value)
torch.distributions.HalfCauchy.mean(self)
torch.distributions.HalfCauchy.scale(self)
torch.distributions.HalfCauchy.variance(self)
torch.distributions.half_cauchy.HalfCauchy(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.__init__(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.cdf(self,value)
torch.distributions.half_cauchy.HalfCauchy.entropy(self)
torch.distributions.half_cauchy.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.half_cauchy.HalfCauchy.icdf(self,prob)
torch.distributions.half_cauchy.HalfCauchy.log_prob(self,value)
torch.distributions.half_cauchy.HalfCauchy.mean(self)
torch.distributions.half_cauchy.HalfCauchy.scale(self)
torch.distributions.half_cauchy.HalfCauchy.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/exponential.py----------------------------------------
A:torch.distributions.exponential.(self.rate,)->broadcast_all(rate)
A:torch.distributions.exponential.new->self._get_checked_instance(Exponential, _instance)
A:torch.distributions.exponential.batch_shape->torch.Size(batch_shape)
A:torch.distributions.exponential.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.exponential.shape->self._extended_shape(sample_shape)
A:torch.distributions.exponential.u->torch.rand(shape, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Exponential(self,rate,validate_args=None)
torch.distributions.Exponential._log_normalizer(self,x)
torch.distributions.Exponential._natural_params(self)
torch.distributions.Exponential.cdf(self,value)
torch.distributions.Exponential.entropy(self)
torch.distributions.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.Exponential.icdf(self,value)
torch.distributions.Exponential.log_prob(self,value)
torch.distributions.Exponential.mean(self)
torch.distributions.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.Exponential.stddev(self)
torch.distributions.Exponential.variance(self)
torch.distributions.exponential.Exponential(self,rate,validate_args=None)
torch.distributions.exponential.Exponential.__init__(self,rate,validate_args=None)
torch.distributions.exponential.Exponential._log_normalizer(self,x)
torch.distributions.exponential.Exponential._natural_params(self)
torch.distributions.exponential.Exponential.cdf(self,value)
torch.distributions.exponential.Exponential.entropy(self)
torch.distributions.exponential.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.exponential.Exponential.icdf(self,value)
torch.distributions.exponential.Exponential.log_prob(self,value)
torch.distributions.exponential.Exponential.mean(self)
torch.distributions.exponential.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.exponential.Exponential.stddev(self)
torch.distributions.exponential.Exponential.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/beta.py----------------------------------------
A:torch.distributions.beta.concentration1_concentration0->torch.stack([concentration1, concentration0], -1)
A:torch.distributions.beta.(concentration1, concentration0)->broadcast_all(concentration1, concentration0)
A:torch.distributions.beta.self._dirichlet->Dirichlet(concentration1_concentration0, validate_args=validate_args)
A:torch.distributions.beta.new->self._get_checked_instance(Beta, _instance)
A:torch.distributions.beta.batch_shape->torch.Size(batch_shape)
A:torch.distributions.beta.new._dirichlet->self._dirichlet.expand(batch_shape)
A:torch.distributions.beta.heads_tails->torch.stack([value, 1.0 - value], -1)
torch.distributions.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.Beta._log_normalizer(self,x,y)
torch.distributions.Beta._natural_params(self)
torch.distributions.Beta.concentration0(self)
torch.distributions.Beta.concentration1(self)
torch.distributions.Beta.entropy(self)
torch.distributions.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.Beta.log_prob(self,value)
torch.distributions.Beta.mean(self)
torch.distributions.Beta.rsample(self,sample_shape=())
torch.distributions.Beta.variance(self)
torch.distributions.beta.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta.__init__(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta._log_normalizer(self,x,y)
torch.distributions.beta.Beta._natural_params(self)
torch.distributions.beta.Beta.concentration0(self)
torch.distributions.beta.Beta.concentration1(self)
torch.distributions.beta.Beta.entropy(self)
torch.distributions.beta.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.beta.Beta.log_prob(self,value)
torch.distributions.beta.Beta.mean(self)
torch.distributions.beta.Beta.rsample(self,sample_shape=())
torch.distributions.beta.Beta.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/constraints.py----------------------------------------
A:torch.distributions.constraints.result->result.all(-1).all(-1)
A:torch.distributions.constraints.is_normalized->value.sum(-1).eq(1)
A:torch.distributions.constraints.value_tril->value.tril()
A:torch.distributions.constraints.row_norm->torch.linalg.norm(value.detach(), dim=-1)
A:torch.distributions.constraints.unit_row_norm->(row_norm - 1.0).abs().le(tol).all(dim=-1)
A:torch.distributions.constraints.square_check->super().check(value)
A:torch.distributions.constraints.sym_check->super().check(value)
A:torch.distributions.constraints.self.cseq->list(cseq)
A:torch.distributions.constraints.self.lengths->list(lengths)
A:torch.distributions.constraints.v->value.narrow(self.dim, start, length)
A:torch.distributions.constraints.dim->max((c.event_dim for c in self.cseq))
A:torch.distributions.constraints.dependent->_Dependent()
A:torch.distributions.constraints.boolean->_Boolean()
A:torch.distributions.constraints.one_hot->_OneHot()
A:torch.distributions.constraints.nonnegative_integer->_IntegerGreaterThan(0)
A:torch.distributions.constraints.positive_integer->_IntegerGreaterThan(1)
A:torch.distributions.constraints.real->_Real()
A:torch.distributions.constraints.real_vector->independent(real, 1)
A:torch.distributions.constraints.positive->_GreaterThan(0.0)
A:torch.distributions.constraints.nonnegative->_GreaterThanEq(0.0)
A:torch.distributions.constraints.unit_interval->_Interval(0.0, 1.0)
A:torch.distributions.constraints.simplex->_Simplex()
A:torch.distributions.constraints.lower_triangular->_LowerTriangular()
A:torch.distributions.constraints.lower_cholesky->_LowerCholesky()
A:torch.distributions.constraints.corr_cholesky->_CorrCholesky()
A:torch.distributions.constraints.square->_Square()
A:torch.distributions.constraints.symmetric->_Symmetric()
A:torch.distributions.constraints.positive_semidefinite->_PositiveSemidefinite()
A:torch.distributions.constraints.positive_definite->_PositiveDefinite()
torch.distributions.constraints.Constraint(object)
torch.distributions.constraints.Constraint.__repr__(self)
torch.distributions.constraints.Constraint.check(self,value)
torch.distributions.constraints._Boolean(Constraint)
torch.distributions.constraints._Boolean.check(self,value)
torch.distributions.constraints._Cat(self,cseq,dim=0,lengths=None)
torch.distributions.constraints._Cat.__init__(self,cseq,dim=0,lengths=None)
torch.distributions.constraints._Cat.check(self,value)
torch.distributions.constraints._Cat.event_dim(self)
torch.distributions.constraints._Cat.is_discrete(self)
torch.distributions.constraints._CorrCholesky(Constraint)
torch.distributions.constraints._CorrCholesky.check(self,value)
torch.distributions.constraints._Dependent(self,*,is_discrete=NotImplemented,event_dim=NotImplemented)
torch.distributions.constraints._Dependent.__init__(self,*,is_discrete=NotImplemented,event_dim=NotImplemented)
torch.distributions.constraints._Dependent.check(self,x)
torch.distributions.constraints._Dependent.event_dim(self)
torch.distributions.constraints._Dependent.is_discrete(self)
torch.distributions.constraints._DependentProperty(self,fn=None,*,is_discrete=NotImplemented,event_dim=NotImplemented)
torch.distributions.constraints._DependentProperty.__init__(self,fn=None,*,is_discrete=NotImplemented,event_dim=NotImplemented)
torch.distributions.constraints._GreaterThan(self,lower_bound)
torch.distributions.constraints._GreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThan.__repr__(self)
torch.distributions.constraints._GreaterThan.check(self,value)
torch.distributions.constraints._GreaterThanEq(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__repr__(self)
torch.distributions.constraints._GreaterThanEq.check(self,value)
torch.distributions.constraints._HalfOpenInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__repr__(self)
torch.distributions.constraints._HalfOpenInterval.check(self,value)
torch.distributions.constraints._IndependentConstraint(self,base_constraint,reinterpreted_batch_ndims)
torch.distributions.constraints._IndependentConstraint.__init__(self,base_constraint,reinterpreted_batch_ndims)
torch.distributions.constraints._IndependentConstraint.__repr__(self)
torch.distributions.constraints._IndependentConstraint.check(self,value)
torch.distributions.constraints._IndependentConstraint.event_dim(self)
torch.distributions.constraints._IndependentConstraint.is_discrete(self)
torch.distributions.constraints._IntegerGreaterThan(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__repr__(self)
torch.distributions.constraints._IntegerGreaterThan.check(self,value)
torch.distributions.constraints._IntegerInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__repr__(self)
torch.distributions.constraints._IntegerInterval.check(self,value)
torch.distributions.constraints._IntegerLessThan(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__init__(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__repr__(self)
torch.distributions.constraints._IntegerLessThan.check(self,value)
torch.distributions.constraints._Interval(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__repr__(self)
torch.distributions.constraints._Interval.check(self,value)
torch.distributions.constraints._LessThan(self,upper_bound)
torch.distributions.constraints._LessThan.__init__(self,upper_bound)
torch.distributions.constraints._LessThan.__repr__(self)
torch.distributions.constraints._LessThan.check(self,value)
torch.distributions.constraints._LowerCholesky(Constraint)
torch.distributions.constraints._LowerCholesky.check(self,value)
torch.distributions.constraints._LowerTriangular(Constraint)
torch.distributions.constraints._LowerTriangular.check(self,value)
torch.distributions.constraints._Multinomial(self,upper_bound)
torch.distributions.constraints._Multinomial.__init__(self,upper_bound)
torch.distributions.constraints._Multinomial.check(self,x)
torch.distributions.constraints._OneHot(Constraint)
torch.distributions.constraints._OneHot.check(self,value)
torch.distributions.constraints._PositiveDefinite(_Symmetric)
torch.distributions.constraints._PositiveDefinite.check(self,value)
torch.distributions.constraints._PositiveSemidefinite(_Symmetric)
torch.distributions.constraints._PositiveSemidefinite.check(self,value)
torch.distributions.constraints._Real(Constraint)
torch.distributions.constraints._Real.check(self,value)
torch.distributions.constraints._Simplex(Constraint)
torch.distributions.constraints._Simplex.check(self,value)
torch.distributions.constraints._Square(Constraint)
torch.distributions.constraints._Square.check(self,value)
torch.distributions.constraints._Stack(self,cseq,dim=0)
torch.distributions.constraints._Stack.__init__(self,cseq,dim=0)
torch.distributions.constraints._Stack.check(self,value)
torch.distributions.constraints._Stack.event_dim(self)
torch.distributions.constraints._Stack.is_discrete(self)
torch.distributions.constraints._Symmetric(_Square)
torch.distributions.constraints._Symmetric.check(self,value)
torch.distributions.constraints.is_dependent(constraint)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/distributions/fishersnedecor.py----------------------------------------
A:torch.distributions.fishersnedecor.(self.df1, self.df2)->broadcast_all(df1, df2)
A:torch.distributions.fishersnedecor.self._gamma1->Gamma(self.df1 * 0.5, self.df1)
A:torch.distributions.fishersnedecor.self._gamma2->Gamma(self.df2 * 0.5, self.df2)
A:torch.distributions.fishersnedecor.batch_shape->torch.Size(batch_shape)
A:torch.distributions.fishersnedecor.new->self._get_checked_instance(FisherSnedecor, _instance)
A:torch.distributions.fishersnedecor.new.df1->self.df1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new.df2->self.df2.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma1->self._gamma1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma2->self._gamma2.expand(batch_shape)
A:torch.distributions.fishersnedecor.df2->self.df2.clone(memory_format=torch.contiguous_format)
A:torch.distributions.fishersnedecor.shape->self._extended_shape(sample_shape)
A:torch.distributions.fishersnedecor.X1->self._gamma1.rsample(sample_shape).view(shape)
A:torch.distributions.fishersnedecor.X2->self._gamma2.rsample(sample_shape).view(shape)
torch.distributions.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.FisherSnedecor.log_prob(self,value)
torch.distributions.FisherSnedecor.mean(self)
torch.distributions.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.FisherSnedecor.variance(self)
torch.distributions.fishersnedecor.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.__init__(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.fishersnedecor.FisherSnedecor.log_prob(self,value)
torch.distributions.fishersnedecor.FisherSnedecor.mean(self)
torch.distributions.fishersnedecor.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.fishersnedecor.FisherSnedecor.variance(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/linalg/__init__.py----------------------------------------
A:torch.linalg.__init__.cross->_add_docstr(_linalg.linalg_cross, '\nlinalg.cross(input, other, *, dim=-1, out=None) -> Tensor\n\n\nComputes the cross product of two 3-dimensional vectors.\n\nSupports input of float, double, cfloat and cdouble dtypes. Also supports batches\nof vectors, for which it computes the product along the dimension :attr:`dim`.\nIn this case, the output has the same batch dimensions as the inputs broadcast to\na common shape.\n\nArgs:\n    input (Tensor): the first input tensor.\n    other (Tensor): the second input tensor.\n    dim  (int, optional): the dimension along which to take the cross-product. Default: `-1`.\n\nKeyword args:\n    out (Tensor, optional): the output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: If after broadcasting :attr:`input`\\ `.size(\\ `:attr:`dim`\\ `) != 3`\n                  or :attr:`other`\\ `.size(\\ `:attr:`dim`\\ `) != 3`.\nExample:\n    >>> a = torch.randn(4, 3)\n    >>> a\n    tensor([[-0.3956,  1.1455,  1.6895],\n            [-0.5849,  1.3672,  0.3599],\n            [-1.1626,  0.7180, -0.0521],\n            [-0.1339,  0.9902, -2.0225]])\n    >>> b = torch.randn(4, 3)\n    >>> b\n    tensor([[-0.0257, -1.4725, -1.2251],\n            [-1.1479, -0.7005, -1.9757],\n            [-1.3904,  0.3726, -1.1836],\n            [-0.9688, -0.7153,  0.2159]])\n    >>> torch.linalg.cross(a, b)\n    tensor([[ 1.0844, -0.5281,  0.6120],\n            [-2.4490, -1.5687,  1.9792],\n            [-0.8304, -1.3037,  0.5650],\n            [-1.2329,  1.9883,  1.0551]])\n    >>> a = torch.randn(1, 3)  # a is broadcast to match shape of b\n    >>> a\n    tensor([[-0.9941, -0.5132,  0.5681]])\n    >>> torch.linalg.cross(a, b)\n    tensor([[ 1.4653, -1.2325,  1.4507],\n            [ 1.4119, -2.6163,  0.1073],\n            [ 0.3957, -1.9666, -1.0840],\n            [ 0.2956, -0.3357,  0.2139]])\n')
A:torch.linalg.__init__.cholesky->_add_docstr(_linalg.linalg_cholesky, '\nlinalg.cholesky(A, *, upper=False, out=None) -> Tensor\n\nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **Cholesky decomposition** of a complex Hermitian or real symmetric positive-definite matrix\n:math:`A \\in \\mathbb{K}^{n \\times n}` is defined as\n\n.. math::\n\n    A = LL^{\\text{H}}\\mathrlap{\\qquad L \\in \\mathbb{K}^{n \\times n}}\n\nwhere :math:`L` is a lower triangular matrix and\n:math:`L^{\\text{H}}` is the conjugate transpose when :math:`L` is complex, and the transpose when :math:`L` is real-valued.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n' + f"\n.. note:: {common_notes['sync_note']}\n" + '\n\n.. seealso::\n\n        :func:`torch.linalg.cholesky_ex` for a version of this operation that\n        skips the (slow) error checking by default and instead returns the debug\n        information. This makes it a faster way to check if a matrix is\n        positive-definite.\n\n        :func:`torch.linalg.eigh` for a different decomposition of a Hermitian matrix.\n        The eigenvalue decomposition gives more information about the matrix but it\n        slower to compute than the Cholesky decomposition.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                consisting of symmetric or Hermitian positive-definite matrices.\n\nKeyword args:\n    upper (bool, optional): whether to return an upper triangular matrix.\n        The tensor returned with upper=True is the conjugate transpose of the tensor\n        returned with upper=False.\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if the :attr:`A` matrix or any matrix in a batched :attr:`A` is not Hermitian\n                  (resp. symmetric) positive-definite. If :attr:`A` is a batch of matrices,\n                  the error message will include the batch index of the first matrix that fails\n                  to meet this condition.\n\nExamples::\n\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> A = A @ A.T.conj() + torch.eye(2) # creates a Hermitian positive-definite matrix\n    >>> A\n    tensor([[2.5266+0.0000j, 1.9586-2.0626j],\n            [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)\n    >>> L = torch.linalg.cholesky(A)\n    >>> L\n    tensor([[1.5895+0.0000j, 0.0000+0.0000j],\n            [1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)\n    >>> torch.dist(L @ L.T.conj(), A)\n    tensor(4.4692e-16, dtype=torch.float64)\n\n    >>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n    >>> A = A @ A.mT + torch.eye(2)  # batch of symmetric positive-definite matrices\n    >>> L = torch.linalg.cholesky(A)\n    >>> torch.dist(L @ L.mT, A)\n    tensor(5.8747e-16, dtype=torch.float64)\n')
A:torch.linalg.__init__.cholesky_ex->_add_docstr(_linalg.linalg_cholesky_ex, "\nlinalg.cholesky_ex(A, *, upper=False, check_errors=False, out=None) -> (Tensor, Tensor)\n\nComputes the Cholesky decomposition of a complex Hermitian or real\nsymmetric positive-definite matrix.\n\nThis function skips the (slow) error checking and error message construction\nof :func:`torch.linalg.cholesky`, instead directly returning the LAPACK\nerror codes as part of a named tuple ``(L, info)``. This makes this function\na faster way to check if a matrix is positive-definite, and it provides an\nopportunity to handle decomposition errors more gracefully or performantly\nthan :func:`torch.linalg.cholesky` does.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nIf :attr:`A` is not a Hermitian positive-definite matrix, or if it's a batch of matrices\nand one or more of them is not a Hermitian positive-definite matrix,\nthen ``info`` stores a positive integer for the corresponding matrix.\nThe positive integer indicates the order of the leading minor that is not positive-definite,\nand the decomposition could not be completed.\n``info`` filled with zeros indicates that the decomposition was successful.\nIf ``check_errors=True`` and ``info`` contains positive integers, then a RuntimeError is thrown.\n\n" + f"\n.. note:: {common_notes['sync_note_ex']}\n\n.. warning:: {common_notes['experimental_warning']}\n" + '\n\n.. seealso::\n        :func:`torch.linalg.cholesky` is a NumPy compatible variant that always checks for errors.\n\nArgs:\n    A (Tensor): the Hermitian `n \\times n` matrix or the batch of such matrices of size\n                    `(*, n, n)` where `*` is one or more batch dimensions.\n\nKeyword args:\n    upper (bool, optional): whether to return an upper triangular matrix.\n        The tensor returned with upper=True is the conjugate transpose of the tensor\n        returned with upper=False.\n    check_errors (bool, optional): controls whether to check the content of ``infos``. Default: `False`.\n    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> A = A @ A.t().conj()  # creates a Hermitian positive-definite matrix\n    >>> L, info = torch.linalg.cholesky_ex(A)\n    >>> A\n    tensor([[ 2.3792+0.0000j, -0.9023+0.9831j],\n            [-0.9023-0.9831j,  0.8757+0.0000j]], dtype=torch.complex128)\n    >>> L\n    tensor([[ 1.5425+0.0000j,  0.0000+0.0000j],\n            [-0.5850-0.6374j,  0.3567+0.0000j]], dtype=torch.complex128)\n    >>> info\n    tensor(0, dtype=torch.int32)\n\n')
A:torch.linalg.__init__.inv->_add_docstr(_linalg.linalg_inv, '\nlinalg.inv(A, *, out=None) -> Tensor\n\nComputes the inverse of a square matrix if it exists.\nThrows a `RuntimeError` if the matrix is not invertible.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nfor a matrix :math:`A \\in \\mathbb{K}^{n \\times n}`,\nits **inverse matrix** :math:`A^{-1} \\in \\mathbb{K}^{n \\times n}` (if it exists) is defined as\n\n.. math::\n\n    A^{-1}A = AA^{-1} = \\mathrm{I}_n\n\nwhere :math:`\\mathrm{I}_n` is the `n`-dimensional identity matrix.\n\nThe inverse matrix exists if and only if :math:`A` is `invertible`_. In this case,\nthe inverse is unique.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices\nthen the output has the same batch dimensions.\n\n' + f"\n.. note:: {common_notes['sync_note']}\n" + '\n\n.. note::\n    Consider using :func:`torch.linalg.solve` if possible for multiplying a matrix on the left by\n    the inverse, as::\n\n        torch.linalg.solve(A, B) == A.inv() @ B\n\n    It is always prefered to use :func:`~solve` when possible, as it is faster and more\n    numerically stable than computing the inverse explicitly.\n\n.. seealso::\n\n        :func:`torch.linalg.pinv` computes the pseudoinverse (Moore-Penrose inverse) of matrices\n        of any shape.\n\n        :func:`torch.linalg.solve` computes :attr:`A`\\ `.inv() @ \\ `:attr:`B` with a\n        numerically stable algorithm.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                consisting of invertible matrices.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if the matrix :attr:`A` or any matrix in the batch of matrices :attr:`A` is not invertible.\n\nExamples::\n\n    >>> A = torch.randn(4, 4)\n    >>> Ainv = torch.linalg.inv(A)\n    >>> torch.dist(A @ Ainv, torch.eye(4))\n    tensor(1.1921e-07)\n\n    >>> A = torch.randn(2, 3, 4, 4)  # Batch of matrices\n    >>> Ainv = torch.linalg.inv(A)\n    >>> torch.dist(A @ Ainv, torch.eye(4)))\n    tensor(1.9073e-06)\n\n    >>> A = torch.randn(4, 4, dtype=torch.complex128)  # Complex matrix\n    >>> Ainv = torch.linalg.inv(A)\n    >>> torch.dist(A @ Ainv, torch.eye(4))\n    tensor(7.5107e-16, dtype=torch.float64)\n\n.. _invertible:\n    https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem\n')
A:torch.linalg.__init__.inv_ex->_add_docstr(_linalg.linalg_inv_ex, "\nlinalg.inv_ex(A, *, check_errors=False, out=None) -> (Tensor, Tensor)\n\nComputes the inverse of a square matrix if it is invertible.\n\nReturns a namedtuple ``(inverse, info)``. ``inverse`` contains the result of\ninverting :attr:`A` and ``info`` stores the LAPACK error codes.\n\nIf :attr:`A` is not an invertible matrix, or if it's a batch of matrices\nand one or more of them is not an invertible matrix,\nthen ``info`` stores a positive integer for the corresponding matrix.\nThe positive integer indicates the diagonal element of the LU decomposition of\nthe input matrix that is exactly zero.\n``info`` filled with zeros indicates that the inversion was successful.\nIf ``check_errors=True`` and ``info`` contains positive integers, then a RuntimeError is thrown.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n" + f"\n.. note:: {common_notes['sync_note_ex']}\n\n.. warning:: {common_notes['experimental_warning']}\n" + '\n\n.. seealso::\n\n        :func:`torch.linalg.inv` is a NumPy compatible variant that always checks for errors.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                    consisting of square matrices.\n    check_errors (bool, optional): controls whether to check the content of ``info``. Default: `False`.\n\nKeyword args:\n    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> A = torch.randn(3, 3)\n    >>> Ainv, info = torch.linalg.inv_ex(A)\n    >>> torch.dist(torch.linalg.inv(A), Ainv)\n    tensor(0.)\n    >>> info\n    tensor(0, dtype=torch.int32)\n\n')
A:torch.linalg.__init__.det->_add_docstr(_linalg.linalg_det, '\nlinalg.det(A, *, out=None) -> Tensor\n\nComputes the determinant of a square matrix.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n.. seealso::\n\n        :func:`torch.linalg.slogdet` computes the sign (resp. angle) and natural logarithm of the\n        absolute value (resp. modulus) of the determinant of real-valued (resp. complex)\n        square matrices.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> A = torch.randn(3, 3)\n    >>> torch.linalg.det(A)\n    tensor(0.0934)\n\n    >>> A = torch.randn(3, 2, 2)\n    >>> torch.linalg.det(A)\n    tensor([1.1990, 0.4099, 0.7386])\n')
A:torch.linalg.__init__.slogdet->_add_docstr(_linalg.linalg_slogdet, '\nlinalg.slogdet(A, *, out=None) -> (Tensor, Tensor)\n\nComputes the sign and natural logarithm of the absolute value of the determinant of a square matrix.\n\nFor complex :attr:`A`, it returns the angle and the natural logarithm of the modulus of the\ndeterminant, that is, a logarithmic polar decomposition of the determinant.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n' + f"\n.. note:: This function is computed using :func:`torch.lu`.\n          {common_notes['sync_note']}\n" + '\n\n.. note:: The determinant can be recovered as `sign * exp(logabsdet)`.\n\n.. note:: When a matrix has a determinant of zero, it returns `(0, -inf)`.\n\n.. seealso::\n\n        :func:`torch.linalg.det` computes the determinant of square matrices.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    out (tuple, optional): output tuple of two tensors. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(sign, logabsdet)`.\n\n    `logabsdet` will always be real-valued, even when :attr:`A` is complex.\n\n    `sign` will have the same dtype as :attr:`A`.\n\nExamples::\n\n    >>> A = torch.randn(3, 3)\n    >>> A\n    tensor([[ 0.0032, -0.2239, -1.1219],\n            [-0.6690,  0.1161,  0.4053],\n            [-1.6218, -0.9273, -0.0082]])\n    >>> torch.linalg.det(A)\n    tensor(-0.7576)\n    >>> torch.linalg.logdet(A)\n    tensor(nan)\n    >>> torch.linalg.slogdet(A)\n    torch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n')
A:torch.linalg.__init__.eig->_add_docstr(_linalg.linalg_eig, '\nlinalg.eig(A, *, out=None) -> (Tensor, Tensor)\n\nComputes the eigenvalue decomposition of a square matrix if it exists.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **eigenvalue decomposition** of a square matrix\n:math:`A \\in \\mathbb{K}^{n \\times n}` (if it exists) is defined as\n\n.. math::\n\n    A = V \\operatorname{diag}(\\Lambda) V^{-1}\\mathrlap{\\qquad V \\in \\mathbb{C}^{n \\times n}, \\Lambda \\in \\mathbb{C}^n}\n\nThis decomposition exists if and only if :math:`A` is `diagonalizable`_.\nThis is the case when all its eigenvalues are different.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n.. note:: The eigenvalues and eigenvectors of a real matrix may be complex.\n\n' + f"\n.. note:: {common_notes['sync_note']}\n" + '\n\n.. warning:: This function assumes that :attr:`A` is `diagonalizable`_ (for example, when all the\n             eigenvalues are different). If it is not diagonalizable, the returned\n             eigenvalues will be correct but :math:`A \\neq V \\operatorname{diag}(\\Lambda)V^{-1}`.\n\n.. warning:: The returned eigenvectors are normalized to have norm `1`.\n             Even then, the eigenvectors of a matrix are not unique, nor are they continuous with respect to\n             :attr:`A`. Due to this lack of uniqueness, different hardware and software may compute\n             different eigenvectors.\n\n             This non-uniqueness is caused by the fact that multiplying an eigenvector by\n             by :math:`e^{i \\phi}, \\phi \\in \\mathbb{R}` produces another set of valid eigenvectors\n             of the matrix.  For this reason, the loss function shall not depend on the phase of the\n             eigenvectors, as this quantity is not well-defined.\n             This is checked when computing the gradients of this function. As such,\n             when inputs are on a CUDA device, this function synchronizes that device with the CPU\n             when computing the gradients.\n             This is checked when computing the gradients of this function. As such,\n             when inputs are on a CUDA device, the computation of the gradients\n             of this function synchronizes that device with the CPU.\n\n\n.. warning:: Gradients computed using the `eigenvectors` tensor will only be finite when\n             :attr:`A` has distinct eigenvalues.\n             Furthermore, if the distance between any two eigenvalues is close to zero,\n             the gradient will be numerically unstable, as it depends on the eigenvalues\n             :math:`\\lambda_i` through the computation of\n             :math:`\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}`.\n\n.. seealso::\n\n        :func:`torch.linalg.eigvals` computes only the eigenvalues.\n        Unlike :func:`torch.linalg.eig`, the gradients of :func:`~eigvals` are always\n        numerically stable.\n\n        :func:`torch.linalg.eigh` for a (faster) function that computes the eigenvalue decomposition\n        for Hermitian and symmetric matrices.\n\n        :func:`torch.linalg.svd` for a function that computes another type of spectral\n        decomposition that works on matrices of any shape.\n\n        :func:`torch.linalg.qr` for another (much faster) decomposition that works on matrices of\n        any shape.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                consisting of diagonalizable matrices.\n\nKeyword args:\n    out (tuple, optional): output tuple of two tensors. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(eigenvalues, eigenvectors)` which corresponds to :math:`\\Lambda` and :math:`V` above.\n\n    `eigenvalues` and `eigenvectors` will always be complex-valued, even when :attr:`A` is real. The eigenvectors\n    will be given by the columns of `eigenvectors`.\n\nExamples::\n\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> A\n    tensor([[ 0.9828+0.3889j, -0.4617+0.3010j],\n            [ 0.1662-0.7435j, -0.6139+0.0562j]], dtype=torch.complex128)\n    >>> L, V = torch.linalg.eig(A)\n    >>> L\n    tensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)\n    >>> V\n    tensor([[ 0.9218+0.0000j,  0.1882-0.2220j],\n            [-0.0270-0.3867j,  0.9567+0.0000j]], dtype=torch.complex128)\n    >>> torch.dist(V @ torch.diag(L) @ torch.linalg.inv(V), A)\n    tensor(7.7119e-16, dtype=torch.float64)\n\n    >>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n    >>> L, V = torch.linalg.eig(A)\n    >>> torch.dist(V @ torch.diag_embed(L) @ torch.linalg.inv(V), A)\n    tensor(3.2841e-16, dtype=torch.float64)\n\n.. _diagonalizable:\n    https://en.wikipedia.org/wiki/Diagonalizable_matrix#Definition\n')
A:torch.linalg.__init__.eigvals->_add_docstr(_linalg.linalg_eigvals, '\nlinalg.eigvals(A, *, out=None) -> Tensor\n\nComputes the eigenvalues of a square matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **eigenvalues** of a square matrix :math:`A \\in \\mathbb{K}^{n \\times n}` are defined\nas the roots (counted with multiplicity) of the polynomial `p` of degree `n` given by\n\n.. math::\n\n    p(\\lambda) = \\operatorname{det}(A - \\lambda \\mathrm{I}_n)\\mathrlap{\\qquad \\lambda \\in \\mathbb{C}}\n\nwhere :math:`\\mathrm{I}_n` is the `n`-dimensional identity matrix.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n.. note:: The eigenvalues of a real matrix may be complex, as the roots of a real polynomial may be complex.\n\n          The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.\n\n' + f"\n.. note:: {common_notes['sync_note']}\n" + '\n\n.. seealso::\n\n        :func:`torch.linalg.eig` computes the full eigenvalue decomposition.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nReturns:\n    A complex-valued tensor cointaining the eigenvalues even when :attr:`A` is real.\n\nExamples::\n\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> L = torch.linalg.eigvals(A)\n    >>> L\n    tensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)\n\n    >>> torch.dist(L, torch.linalg.eig(A).eigenvalues)\n    tensor(2.4576e-07)\n')
A:torch.linalg.__init__.eigh->_add_docstr(_linalg.linalg_eigh, "\nlinalg.eigh(A, UPLO='L', *, out=None) -> (Tensor, Tensor)\n\nComputes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **eigenvalue decomposition** of a complex Hermitian or real symmetric matrix\n:math:`A \\in \\mathbb{K}^{n \\times n}` is defined as\n\n.. math::\n\n    A = Q \\operatorname{diag}(\\Lambda) Q^{\\text{H}}\\mathrlap{\\qquad Q \\in \\mathbb{K}^{n \\times n}, \\Lambda \\in \\mathbb{R}^n}\n\nwhere :math:`Q^{\\text{H}}` is the conjugate transpose when :math:`Q` is complex, and the transpose when :math:`Q` is real-valued.\n:math:`Q` is orthogonal in the real case and unitary in the complex case.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n:attr:`A` is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:\n\n- If :attr:`UPLO`\\ `= 'L'` (default), only the lower triangular part of the matrix is used in the computation.\n- If :attr:`UPLO`\\ `= 'U'`, only the upper triangular part of the matrix is used.\n\nThe eigenvalues are returned in ascending order.\n\n" + f"\n.. note:: {common_notes['sync_note']}\n" + "\n\n.. note:: The eigenvalues of real symmetric or complex Hermitian matrices are always real.\n\n.. warning:: The eigenvectors of a symmetric matrix are not unique, nor are they continuous with\n             respect to :attr:`A`. Due to this lack of uniqueness, different hardware and\n             software may compute different eigenvectors.\n\n             This non-uniqueness is caused by the fact that multiplying an eigenvector by\n             `-1` in the real case or by :math:`e^{i \\phi}, \\phi \\in \\mathbb{R}` in the complex\n             case produces another set of valid eigenvectors of the matrix.\n             For this reason, the loss function shall not depend on the phase of the eigenvectors, as\n             this quantity is not well-defined.\n             This is checked for complex inputs when computing the gradients of this function. As such,\n             when inputs are complex and are on a CUDA device, the computation of the gradients\n             of this function synchronizes that device with the CPU.\n\n.. warning:: Gradients computed using the `eigenvectors` tensor will only be finite when\n             :attr:`A` has distinct eigenvalues.\n             Furthermore, if the distance between any two eigenvalues is close to zero,\n             the gradient will be numerically unstable, as it depends on the eigenvalues\n             :math:`\\lambda_i` through the computation of\n             :math:`\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}`.\n\n.. seealso::\n\n        :func:`torch.linalg.eigvalsh` computes only the eigenvalues of a Hermitian matrix.\n        Unlike :func:`torch.linalg.eigh`, the gradients of :func:`~eigvalsh` are always\n        numerically stable.\n\n        :func:`torch.linalg.cholesky` for a different decomposition of a Hermitian matrix.\n        The Cholesky decomposition gives less information about the matrix but is much faster\n        to compute than the eigenvalue decomposition.\n\n        :func:`torch.linalg.eig` for a (slower) function that computes the eigenvalue decomposition\n        of a not necessarily Hermitian square matrix.\n\n        :func:`torch.linalg.svd` for a (slower) function that computes the more general SVD\n        decomposition of matrices of any shape.\n\n        :func:`torch.linalg.qr` for another (much faster) decomposition that works on general\n        matrices.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                consisting of symmetric or Hermitian matrices.\n    UPLO ('L', 'U', optional): controls whether to use the upper or lower triangular part\n                               of :attr:`A` in the computations. Default: `'L'`.\n\nKeyword args:\n    out (tuple, optional): output tuple of two tensors. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(eigenvalues, eigenvectors)` which corresponds to :math:`\\Lambda` and :math:`Q` above.\n\n    `eigenvalues` will always be real-valued, even when :attr:`A` is complex.\n    It will also be ordered in ascending order.\n\n    `eigenvectors` will have the same dtype as :attr:`A` and will contain the eigenvectors as its columns.\n\nExamples::\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> A = A + A.T.conj()  # creates a Hermitian matrix\n    >>> A\n    tensor([[2.9228+0.0000j, 0.2029-0.0862j],\n            [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n    >>> L, Q = torch.linalg.eigh(A)\n    >>> L\n    tensor([0.3277, 2.9415], dtype=torch.float64)\n    >>> Q\n    tensor([[-0.0846+-0.0000j, -0.9964+0.0000j],\n            [ 0.9170+0.3898j, -0.0779-0.0331j]], dtype=torch.complex128)\n    >>> torch.dist(Q @ torch.diag(L.cdouble()) @ Q.T.conj(), A)\n    tensor(6.1062e-16, dtype=torch.float64)\n\n    >>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n    >>> A = A + A.mT  # creates a batch of symmetric matrices\n    >>> L, Q = torch.linalg.eigh(A)\n    >>> torch.dist(Q @ torch.diag_embed(L) @ Q.mH, A)\n    tensor(1.5423e-15, dtype=torch.float64)\n")
A:torch.linalg.__init__.eigvalsh->_add_docstr(_linalg.linalg_eigvalsh, "\nlinalg.eigvalsh(A, UPLO='L', *, out=None) -> Tensor\n\nComputes the eigenvalues of a complex Hermitian or real symmetric matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **eigenvalues** of a complex Hermitian or real symmetric  matrix :math:`A \\in \\mathbb{K}^{n \\times n}`\nare defined as the roots (counted with multiplicity) of the polynomial `p` of degree `n` given by\n\n.. math::\n\n    p(\\lambda) = \\operatorname{det}(A - \\lambda \\mathrm{I}_n)\\mathrlap{\\qquad \\lambda \\in \\mathbb{R}}\n\nwhere :math:`\\mathrm{I}_n` is the `n`-dimensional identity matrix.\nThe eigenvalues of a real symmetric or complex Hermitian matrix are always real.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nThe eigenvalues are returned in ascending order.\n\n:attr:`A` is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:\n\n- If :attr:`UPLO`\\ `= 'L'` (default), only the lower triangular part of the matrix is used in the computation.\n- If :attr:`UPLO`\\ `= 'U'`, only the upper triangular part of the matrix is used.\n\n" + f"\n.. note:: {common_notes['sync_note']}\n" + "\n\n.. seealso::\n\n        :func:`torch.linalg.eigh` computes the full eigenvalue decomposition.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                consisting of symmetric or Hermitian matrices.\n    UPLO ('L', 'U', optional): controls whether to use the upper or lower triangular part\n                               of :attr:`A` in the computations. Default: `'L'`.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nReturns:\n    A real-valued tensor cointaining the eigenvalues even when :attr:`A` is complex.\n    The eigenvalues are returned in ascending order.\n\nExamples::\n\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> A = A + A.T.conj()  # creates a Hermitian matrix\n    >>> A\n    tensor([[2.9228+0.0000j, 0.2029-0.0862j],\n            [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n    >>> torch.linalg.eigvalsh(A)\n    tensor([0.3277, 2.9415], dtype=torch.float64)\n\n    >>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n    >>> A = A + A.mT  # creates a batch of symmetric matrices\n    >>> torch.linalg.eigvalsh(A)\n    tensor([[ 2.5797,  3.4629],\n            [-4.1605,  1.3780],\n            [-3.1113,  2.7381]], dtype=torch.float64)\n")
A:torch.linalg.__init__.householder_product->_add_docstr(_linalg.linalg_householder_product, "\nhouseholder_product(A, tau, *, out=None) -> Tensor\n\nComputes the first `n` columns of a product of Householder matrices.\n\nLet :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`, and\nlet :math:`V \\in \\mathbb{K}^{m \\times n}` be a matrix with columns :math:`v_i \\in \\mathbb{K}^m`\nfor :math:`i=1,\\ldots,m` with :math:`m \\geq n`. Denote by :math:`w_i` the vector resulting from\nzeroing out the first :math:`i-1` compontents of :math:`v_i` and setting to `1` the :math:`i`-th.\nFor a vector :math:`\\tau \\in \\mathbb{K}^k` with :math:`k \\leq n`, this function computes the\nfirst :math:`n` columns of the matrix\n\n.. math::\n\n    H_1H_2 ... H_k \\qquad\\text{with}\\qquad H_i = \\mathrm{I}_m - \\tau_i w_i w_i^{\\text{H}}\n\nwhere :math:`\\mathrm{I}_m` is the `m`-dimensional identity matrix and :math:`w^{\\text{H}}` is the\nconjugate transpose when :math:`w` is complex, and the transpose when :math:`w` is real-valued.\nThe output matrix is the same size as the input matrix :attr:`A`.\n\nSee `Representation of Orthogonal or Unitary Matrices`_ for further details.\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions.\n\n.. seealso::\n\n        :func:`torch.geqrf` can be used together with this function to form the `Q` from the\n        :func:`~qr` decomposition.\n\n        :func:`torch.ormqr` is a related function that computes the matrix multiplication\n        of a product of Householder matrices with another matrix.\n        However, that function is not supported by autograd.\n\n.. warning::\n    Gradient computations are only well-defined if :math:`tau_i \\neq \\frac{1}{||v_i||^2}`.\n    If this condition is not met, no error will be thrown, but the gradient produced may contain `NaN`.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    tau (Tensor): tensor of shape `(*, k)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if :attr:`A` doesn't satisfy the requirement `m >= n`,\n                  or :attr:`tau` doesn't satisfy the requirement `n >= k`.\n\nExamples::\n\n    >>> A = torch.randn(2, 2)\n    >>> h, tau = torch.geqrf(A)\n    >>> Q = torch.linalg.householder_product(h, tau)\n    >>> torch.dist(Q, torch.linalg.qr(A).Q)\n    tensor(0.)\n\n    >>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\n    >>> tau = torch.randn(3, 1, dtype=torch.complex128)\n    >>> Q = torch.linalg.householder_product(h, tau)\n    >>> Q\n    tensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\n            [-0.6853+0.7953j,  2.0790+0.5620j]],\n\n            [[ 1.4581+1.6989j, -1.5360+0.1193j],\n            [ 1.3877-0.6691j,  1.3512+1.3024j]],\n\n            [[ 1.4766+0.5783j,  0.0361+0.6587j],\n            [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)\n\n.. _Representation of Orthogonal or Unitary Matrices:\n    https://www.netlib.org/lapack/lug/node128.html\n")
A:torch.linalg.__init__.lstsq->_add_docstr(_linalg.linalg_lstsq, "\ntorch.linalg.lstsq(A, B, rcond=None, *, driver=None) -> (Tensor, Tensor, Tensor, Tensor)\n\nComputes a solution to the least squares problem of a system of linear equations.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **least squares problem** for a linear system :math:`AX = B` with\n:math:`A \\in \\mathbb{K}^{m \\times n}, B \\in \\mathbb{K}^{m \\times k}` is defined as\n\n.. math::\n\n    \\min_{X \\in \\mathbb{K}^{n \\times k}} \\|AX - B\\|_F\n\nwhere :math:`\\|-\\|_F` denotes the Frobenius norm.\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions.\n\n:attr:`driver` chooses the LAPACK/MAGMA function that will be used.\nFor CPU inputs the valid values are `'gels'`, `'gelsy'`, `'gelsd`, `'gelss'`.\nFor CUDA input, the only valid driver is `'gels'`, which assumes that :attr:`A` is full-rank.\nTo choose the best driver on CPU consider:\n\n- If :attr:`A` is well-conditioned (its `condition number`_ is not too large), or you do not mind some precision loss.\n\n  - For a general matrix: `'gelsy'` (QR with pivoting) (default)\n  - If :attr:`A` is full-rank: `'gels'` (QR)\n\n- If :attr:`A` is not well-conditioned.\n\n  - `'gelsd'` (tridiagonal reduction and SVD)\n  - But if you run into memory issues: `'gelss'` (full SVD).\n\nSee also the `full description of these drivers`_\n\n:attr:`rcond` is used to determine the effective rank of the matrices in :attr:`A`\nwhen :attr:`driver` is one of (`'gelsy'`, `'gelsd'`, `'gelss'`).\nIn this case, if :math:`\\sigma_i` are the singular values of `A` in decreasing order,\n:math:`\\sigma_i` will be rounded down to zero if :math:`\\sigma_i \\leq \\text{rcond} \\cdot \\sigma_1`.\nIf :attr:`rcond`\\ `= None` (default), :attr:`rcond` is set to the machine precision of the dtype of :attr:`A`.\n\nThis function returns the solution to the problem and some extra information in a named tuple of\nfour tensors `(solution, residuals, rank, singular_values)`. For inputs :attr:`A`, :attr:`B`\nof shape `(*, m, n)`, `(*, m, k)` respectively, it cointains\n\n- `solution`: the least squares solution. It has shape `(*, n, k)`.\n- `residuals`: the squared residuals of the solutions, that is, :math:`\\|AX - B\\|_F^2`.\n  It has shape equal to the batch dimensions of :attr:`A`.\n  It is computed when `m > n` and every matrix in :attr:`A` is full-rank,\n  otherwise, it is an empty tensor.\n  If :attr:`A` is a batch of matrices and any matrix in the batch is not full rank,\n  then an empty tensor is returned. This behavior may change in a future PyTorch release.\n- `rank`: tensor of ranks of the matrices in :attr:`A`.\n  It has shape equal to the batch dimensions of :attr:`A`.\n  It is computed when :attr:`driver` is one of (`'gelsy'`, `'gelsd'`, `'gelss'`),\n  otherwise it is an empty tensor.\n- `singular_values`: tensor of singular values of the matrices in :attr:`A`.\n  It has shape `(*, min(m, n))`.\n  It is computed when :attr:`driver` is one of (`'gelsd'`, `'gelss'`),\n  otherwise it is an empty tensor.\n\n.. note::\n    This function computes `X = \\ `:attr:`A`\\ `.pinverse() @ \\ `:attr:`B` in a faster and\n    more numerically stable way than performing the computations separately.\n\n.. warning::\n    The default value of :attr:`rcond` may change in a future PyTorch release.\n    It is therefore recommended to use a fixed value to avoid potential\n    breaking changes.\n\nArgs:\n    A (Tensor): lhs tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    B (Tensor): rhs tensor of shape `(*, m, k)` where `*` is zero or more batch dimensions.\n    rcond (float, optional): used to determine the effective rank of :attr:`A`.\n                             If :attr:`rcond`\\ `= None`, :attr:`rcond` is set to the machine\n                             precision of the dtype of :attr:`A` times `max(m, n)`. Default: `None`.\n\nKeyword args:\n    driver (str, optional): name of the LAPACK/MAGMA method to be used.\n        If `None`, `'gelsy'` is used for CPU inputs and `'gels'` for CUDA inputs.\n        Default: `None`.\n\nReturns:\n    A named tuple `(solution, residuals, rank, singular_values)`.\n\nExamples::\n\n    >>> A = torch.tensor([[[10, 2, 3], [3, 10, 5], [5, 6, 12]]], dtype=torch.float) # shape (1, 3, 3)\n    >>> B = torch.tensor([[[2, 5, 1], [3, 2, 1], [5, 1, 9]],\n                          [[4, 2, 9], [2, 0, 3], [2, 5, 3]]], dtype=torch.float) # shape (2, 3, 3)\n    >>> X = torch.linalg.lstsq(A, B).solution # A is broadcasted to shape (2, 3, 3)\n    >>> torch.dist(X, torch.linalg.pinv(A) @ B)\n    tensor(2.0862e-07)\n\n    >>> S = torch.linalg.lstsq(A, B, driver='gelsd').singular_values\n    >>> torch.dist(S, torch.linalg.svdvals(A))\n    tensor(5.7220e-06)\n\n    >>> A[:, 0].zero_()  # Decrease the rank of A\n    >>> rank = torch.linalg.lstsq(A, B).rank\n    >>> rank\n    tensor([2])\n\n.. _condition number:\n    https://pytorch.org/docs/master/linalg.html#torch.linalg.cond\n.. _full description of these drivers:\n    https://www.netlib.org/lapack/lug/node27.html\n")
A:torch.linalg.__init__.matrix_power->_add_docstr(_linalg.linalg_matrix_power, '\nmatrix_power(A, n, *, out=None) -> Tensor\n\nComputes the `n`-th power of a square matrix for an integer `n`.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nIf :attr:`n`\\ `= 0`, it returns the identity matrix (or batch) of the same shape\nas :attr:`A`. If :attr:`n` is negative, it returns the inverse of each matrix\n(if invertible) raised to the power of `abs(n)`.\n\n.. note::\n    Consider using :func:`torch.linalg.solve` if possible for multiplying a matrix on the left by\n    a negative power as, if :attr:`n`\\ `> 0`::\n\n        matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B\n\n    It is always prefered to use :func:`~solve` when possible, as it is faster and more\n    numerically stable than computing :math:`A^{-n}` explicitly.\n\n.. seealso::\n\n        :func:`torch.linalg.solve` computes :attr:`A`\\ `.inverse() @ \\ `:attr:`B` with a\n        numerically stable algorithm.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, m)` where `*` is zero or more batch dimensions.\n    n (int): the exponent.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if :attr:`n`\\ `< 0` and the matrix :attr:`A` or any matrix in the\n                  batch of matrices :attr:`A` is not invertible.\n\nExamples::\n\n    >>> A = torch.randn(3, 3)\n    >>> torch.linalg.matrix_power(A, 0)\n    tensor([[1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.]])\n    >>> torch.linalg.matrix_power(A, 3)\n    tensor([[ 1.0756,  0.4980,  0.0100],\n            [-1.6617,  1.4994, -1.9980],\n            [-0.4509,  0.2731,  0.8001]])\n    >>> torch.linalg.matrix_power(A.expand(2, -1, -1), -2)\n    tensor([[[ 0.2640,  0.4571, -0.5511],\n            [-1.0163,  0.3491, -1.5292],\n            [-0.4899,  0.0822,  0.2773]],\n            [[ 0.2640,  0.4571, -0.5511],\n            [-1.0163,  0.3491, -1.5292],\n            [-0.4899,  0.0822,  0.2773]]])\n')
A:torch.linalg.__init__.matrix_rank->_add_docstr(_linalg.linalg_matrix_rank, '\nlinalg.matrix_rank(A, *, atol=None, rtol=None, hermitian=False, out=None) -> Tensor\n\nComputes the numerical rank of a matrix.\n\nThe matrix rank is computed as the number of singular values\n(or eigenvalues in absolute value when :attr:`hermitian`\\ `= True`)\nthat are greater than :math:`\\max(\\text{atol}, \\sigma_1 * \\text{rtol})` threshold,\nwhere :math:`\\sigma_1` is the largest singular value (or eigenvalue).\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nIf :attr:`hermitian`\\ `= True`, :attr:`A` is assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.\n\nIf :attr:`rtol` is not specified and :attr:`A` is a matrix of dimensions `(m, n)`,\nthe relative tolerance is set to be :math:`\\text{rtol} = \\max(m, n) \\varepsilon`\nand :math:`\\varepsilon` is the epsilon value for the dtype of :attr:`A` (see :class:`.finfo`).\nIf :attr:`rtol` is not specified and :attr:`atol` is specified to be larger than zero then\n:attr:`rtol` is set to zero.\n\nIf :attr:`atol` or :attr:`rtol` is a :class:`torch.Tensor`, its shape must be broadcastable to that\nof the singular values of :attr:`A` as returned by :func:`torch.linalg.svdvals`.\n\n.. note::\n    This function has NumPy compatible variant `linalg.matrix_rank(A, tol, hermitian=False)`.\n    However, use of the positional argument :attr:`tol` is deprecated in favor of :attr:`atol` and :attr:`rtol`.\n\n' + f"\n.. note:: The matrix rank is computed using a singular value decomposition\n          :func:`torch.linalg.svdvals` if :attr:`hermitian`\\ `= False` (default) and the eigenvalue\n          decomposition :func:`torch.linalg.eigvalsh` when :attr:`hermitian`\\ `= True`.\n          {common_notes['sync_note']}\n" + "\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    tol (float, Tensor, optional): [NumPy Compat] Alias for :attr:`atol`. Default: `None`.\n\nKeyword args:\n    atol (float, Tensor, optional): the absolute tolerance value. When `None` it's considered to be zero.\n                                    Default: `None`.\n    rtol (float, Tensor, optional): the relative tolerance value. See above for the value it takes when `None`.\n                                    Default: `None`.\n    hermitian(bool): indicates whether :attr:`A` is Hermitian if complex\n                     or symmetric if real. Default: `False`.\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> A = torch.eye(10)\n    >>> torch.linalg.matrix_rank(A)\n    tensor(10)\n    >>> B = torch.eye(10)\n    >>> B[0, 0] = 0\n    >>> torch.linalg.matrix_rank(B)\n    tensor(9)\n\n    >>> A = torch.randn(4, 3, 2)\n    >>> torch.linalg.matrix_rank(A)\n    tensor([2, 2, 2, 2])\n\n    >>> A = torch.randn(2, 4, 2, 3)\n    >>> torch.linalg.matrix_rank(A)\n    tensor([[2, 2, 2, 2],\n            [2, 2, 2, 2]])\n\n    >>> A = torch.randn(2, 4, 3, 3, dtype=torch.complex64)\n    >>> torch.linalg.matrix_rank(A)\n    tensor([[3, 3, 3, 3],\n            [3, 3, 3, 3]])\n    >>> torch.linalg.matrix_rank(A, hermitian=True)\n    tensor([[3, 3, 3, 3],\n            [3, 3, 3, 3]])\n    >>> torch.linalg.matrix_rank(A, atol=1.0, rtol=0.0)\n    tensor([[3, 2, 2, 2],\n            [1, 2, 1, 2]])\n    >>> torch.linalg.matrix_rank(A, atol=1.0, rtol=0.0, hermitian=True)\n    tensor([[2, 2, 2, 1],\n            [1, 2, 2, 2]])\n")
A:torch.linalg.__init__.norm->_add_docstr(_linalg.linalg_norm, "\nlinalg.norm(A, ord=None, dim=None, keepdim=False, *, out=None, dtype=None) -> Tensor\n\nComputes a vector or matrix norm.\n\nIf :attr:`A` is complex valued, it computes the norm of :attr:`A`\\ `.abs()`\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nWhether this function computes a vector or matrix norm is determined as follows:\n\n- If :attr:`dim` is an `int`, the vector norm will be computed.\n- If :attr:`dim` is a `2`-`tuple`, the matrix norm will be computed.\n- If :attr:`dim`\\ `= None` and :attr:`ord`\\ `= None`,\n  :attr:`A` will be flattened to 1D and the `2`-norm of the resulting vector will be computed.\n- If :attr:`dim`\\ `= None` and :attr:`ord` `!= None`, :attr:`A` must be 1D or 2D.\n\n:attr:`ord` defines the norm that is computed. The following norms are supported:\n\n======================     =========================  ========================================================\n:attr:`ord`                norm for matrices          norm for vectors\n======================     =========================  ========================================================\n`None` (default)           Frobenius norm             `2`-norm (see below)\n`'fro'`                    Frobenius norm             -- not supported --\n`'nuc'`                    nuclear norm               -- not supported --\n`inf`                      `max(sum(abs(x), dim=1))`  `max(abs(x))`\n`-inf`                     `min(sum(abs(x), dim=1))`  `min(abs(x))`\n`0`                        -- not supported --        `sum(x != 0)`\n`1`                        `max(sum(abs(x), dim=0))`  as below\n`-1`                       `min(sum(abs(x), dim=0))`  as below\n`2`                        largest singular value     as below\n`-2`                       smallest singular value    as below\nother `int` or `float`     -- not supported --        `sum(abs(x)^{ord})^{(1 / ord)}`\n======================     =========================  ========================================================\n\nwhere `inf` refers to `float('inf')`, NumPy's `inf` object, or any equivalent object.\n\n.. seealso::\n\n        :func:`torch.linalg.vector_norm` computes a vector norm.\n\n        :func:`torch.linalg.matrix_norm` computes a matrix norm.\n\n        The above functions are often clearer and more flexible than using :func:`torch.linalg.norm`.\n        For example, `torch.linalg.norm(A, ord=1, dim=(0, 1))` always\n        computes a matrix norm, but with `torch.linalg.vector_norm(A, ord=1, dim=(0, 1))` it is possible\n        to compute a vector norm over the two dimensions.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n)` or `(*, m, n)` where `*` is zero or more batch dimensions\n    ord (int, float, inf, -inf, 'fro', 'nuc', optional): order of norm. Default: `None`\n    dim (int, Tuple[int], optional): dimensions over which to compute\n        the vector or matrix norm. See above for the behavior when :attr:`dim`\\ `= None`.\n        Default: `None`\n    keepdim (bool, optional): If set to `True`, the reduced dimensions are retained\n        in the result as dimensions with size one. Default: `False`\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n    dtype (:class:`torch.dtype`, optional): If specified, the input tensor is cast to\n        :attr:`dtype` before performing the operation, and the returned tensor's type\n        will be :attr:`dtype`. Default: `None`\n\nReturns:\n    A real-valued tensor, even when :attr:`A` is complex.\n\nExamples::\n\n    >>> from torch import linalg as LA\n    >>> a = torch.arange(9, dtype=torch.float) - 4\n    >>> a\n    tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n    >>> B = a.reshape((3, 3))\n    >>> B\n    tensor([[-4., -3., -2.],\n            [-1.,  0.,  1.],\n            [ 2.,  3.,  4.]])\n\n    >>> LA.norm(a)\n    tensor(7.7460)\n    >>> LA.norm(B)\n    tensor(7.7460)\n    >>> LA.norm(B, 'fro')\n    tensor(7.7460)\n    >>> LA.norm(a, float('inf'))\n    tensor(4.)\n    >>> LA.norm(B, float('inf'))\n    tensor(9.)\n    >>> LA.norm(a, -float('inf'))\n    tensor(0.)\n    >>> LA.norm(B, -float('inf'))\n    tensor(2.)\n\n    >>> LA.norm(a, 1)\n    tensor(20.)\n    >>> LA.norm(B, 1)\n    tensor(7.)\n    >>> LA.norm(a, -1)\n    tensor(0.)\n    >>> LA.norm(B, -1)\n    tensor(6.)\n    >>> LA.norm(a, 2)\n    tensor(7.7460)\n    >>> LA.norm(B, 2)\n    tensor(7.3485)\n\n    >>> LA.norm(a, -2)\n    tensor(0.)\n    >>> LA.norm(B.double(), -2)\n    tensor(1.8570e-16, dtype=torch.float64)\n    >>> LA.norm(a, 3)\n    tensor(5.8480)\n    >>> LA.norm(a, -3)\n    tensor(0.)\n\nUsing the :attr:`dim` argument to compute vector norms::\n\n    >>> c = torch.tensor([[1., 2., 3.],\n    ...                   [-1, 1, 4]])\n    >>> LA.norm(c, dim=0)\n    tensor([1.4142, 2.2361, 5.0000])\n    >>> LA.norm(c, dim=1)\n    tensor([3.7417, 4.2426])\n    >>> LA.norm(c, ord=1, dim=1)\n    tensor([6., 6.])\n\nUsing the :attr:`dim` argument to compute matrix norms::\n\n    >>> A = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)\n    >>> LA.norm(A, dim=(1,2))\n    tensor([ 3.7417, 11.2250])\n    >>> LA.norm(A[0, :, :]), LA.norm(A[1, :, :])\n    (tensor(3.7417), tensor(11.2250))\n")
A:torch.linalg.__init__.vector_norm->_add_docstr(_linalg.linalg_vector_norm, "\nlinalg.vector_norm(A, ord=2, dim=None, keepdim=False, *, dtype=None, out=None) -> Tensor\n\nComputes a vector norm.\n\nIf :attr:`A` is complex valued, it computes the norm of :attr:`A`\\ `.abs()`\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nThis function does not necessarily treat multidimensonal :attr:`A` as a batch of\nvectors, instead:\n\n- If :attr:`dim`\\ `= None`, :attr:`A` will be flattened before the norm is computed.\n- If :attr:`dim` is an `int` or a `tuple`, the norm will be computed over these dimensions\n  and the other dimensions will be treated as batch dimensions.\n\nThis behavior is for consistency with :func:`torch.linalg.norm`.\n\n:attr:`ord` defines the vector norm that is computed. The following norms are supported:\n\n======================   ===============================\n:attr:`ord`              vector norm\n======================   ===============================\n`2` (default)            `2`-norm (see below)\n`inf`                    `max(abs(x))`\n`-inf`                   `min(abs(x))`\n`0`                      `sum(x != 0)`\nother `int` or `float`   `sum(abs(x)^{ord})^{(1 / ord)}`\n======================   ===============================\n\nwhere `inf` refers to `float('inf')`, NumPy's `inf` object, or any equivalent object.\n\n.. seealso::\n\n        :func:`torch.linalg.matrix_norm` computes a matrix norm.\n\nArgs:\n    A (Tensor): tensor, flattened by default, but this behavior can be\n        controlled using :attr:`dim`.\n    ord (int, float, inf, -inf, 'fro', 'nuc', optional): order of norm. Default: `2`\n    dim (int, Tuple[int], optional): dimensions over which to compute\n        the norm. See above for the behavior when :attr:`dim`\\ `= None`.\n        Default: `None`\n    keepdim (bool, optional): If set to `True`, the reduced dimensions are retained\n        in the result as dimensions with size one. Default: `False`\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n    dtype (:class:`torch.dtype`, optional): If specified, the input tensor is cast to\n        :attr:`dtype` before performing the operation, and the returned tensor's type\n        will be :attr:`dtype`. Default: `None`\n\nReturns:\n    A real-valued tensor, even when :attr:`A` is complex.\n\nExamples::\n\n    >>> from torch import linalg as LA\n    >>> a = torch.arange(9, dtype=torch.float) - 4\n    >>> a\n    tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n    >>> B = a.reshape((3, 3))\n    >>> B\n    tensor([[-4., -3., -2.],\n            [-1.,  0.,  1.],\n            [ 2.,  3.,  4.]])\n    >>> LA.vector_norm(a, ord=3.5)\n    tensor(5.4345)\n    >>> LA.vector_norm(B, ord=3.5)\n    tensor(5.4345)\n")
A:torch.linalg.__init__.matrix_norm->_add_docstr(_linalg.linalg_matrix_norm, "\nlinalg.matrix_norm(A, ord='fro', dim=(-2, -1), keepdim=False, *, dtype=None, out=None) -> Tensor\n\nComputes a matrix norm.\n\nIf :attr:`A` is complex valued, it computes the norm of :attr:`A`\\ `.abs()`\n\nSupport input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices: the norm will be computed over the\ndimensions specified by the 2-tuple :attr:`dim` and the other dimensions will\nbe treated as batch dimensions. The output will have the same batch dimensions.\n\n:attr:`ord` defines the matrix norm that is computed. The following norms are supported:\n\n======================   ========================================================\n:attr:`ord`              matrix norm\n======================   ========================================================\n`'fro'` (default)        Frobenius norm\n`'nuc'`                  nuclear norm\n`inf`                    `max(sum(abs(x), dim=1))`\n`-inf`                   `min(sum(abs(x), dim=1))`\n`1`                      `max(sum(abs(x), dim=0))`\n`-1`                     `min(sum(abs(x), dim=0))`\n`2`                      largest singular value\n`-2`                     smallest singular value\n======================   ========================================================\n\nwhere `inf` refers to `float('inf')`, NumPy's `inf` object, or any equivalent object.\n\nArgs:\n    A (Tensor): tensor with two or more dimensions. By default its\n        shape is interpreted as `(*, m, n)` where `*` is zero or more\n        batch dimensions, but this behavior can be controlled using :attr:`dim`.\n    ord (int, inf, -inf, 'fro', 'nuc', optional): order of norm. Default: `'fro'`\n    dim (Tuple[int, int], optional): dimensions over which to compute the norm. Default: `(-2, -1)`\n    keepdim (bool, optional): If set to `True`, the reduced dimensions are retained\n        in the result as dimensions with size one. Default: `False`\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n    dtype (:class:`torch.dtype`, optional): If specified, the input tensor is cast to\n        :attr:`dtype` before performing the operation, and the returned tensor's type\n        will be :attr:`dtype`. Default: `None`\n\nReturns:\n    A real-valued tensor, even when :attr:`A` is complex.\n\nExamples::\n\n    >>> from torch import linalg as LA\n    >>> A = torch.arange(9, dtype=torch.float).reshape(3, 3)\n    >>> A\n    tensor([[0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]])\n    >>> LA.matrix_norm(A)\n    tensor(14.2829)\n    >>> LA.matrix_norm(A, ord=-1)\n    tensor(9.)\n    >>> B = A.expand(2, -1, -1)\n    >>> B\n    tensor([[[0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]],\n\n            [[0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]]])\n    >>> LA.matrix_norm(B)\n    tensor([14.2829, 14.2829])\n    >>> LA.matrix_norm(B, dim=(0, 2))\n    tensor([ 3.1623, 10.0000, 17.2627])\n")
A:torch.linalg.__init__.matmul->_add_docstr(_linalg.linalg_matmul, '\nlinalg.matmul(input, other, *, out=None) -> Tensor\n\nAlias for :func:`torch.matmul`\n')
A:torch.linalg.__init__.diagonal->_add_docstr(_linalg.linalg_diagonal, '\nlinalg.diagonal(A, *, offset=0, dim1=-2, dim2=-1) -> Tensor\n\nAlias for :func:`torch.diagonal` with defaults :attr:`dim1`\\ `= -2`, :attr:`dim2`\\ `= -1`.\n')
A:torch.linalg.__init__.multi_dot->_add_docstr(_linalg.linalg_multi_dot, '\nlinalg.multi_dot(tensors, *, out=None)\n\nEfficiently multiplies two or more matrices by reordering the multiplications so that\nthe fewest arithmetic operations are performed.\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\nThis function does not support batched inputs.\n\nEvery tensor in :attr:`tensors` must be 2D, except for the first and last which\nmay be 1D. If the first tensor is a 1D vector of shape `(n,)` it is treated as a row vector\nof shape `(1, n)`, similarly if the last tensor is a 1D vector of shape `(n,)` it is treated\nas a column vector of shape `(n, 1)`.\n\nIf the first and last tensors are matrices, the output will be a matrix.\nHowever, if either is a 1D vector, then the output will be a 1D vector.\n\nDifferences with `numpy.linalg.multi_dot`:\n\n- Unlike `numpy.linalg.multi_dot`, the first and last tensors must either be 1D or 2D\n  whereas NumPy allows them to be nD\n\n.. warning:: This function does not broadcast.\n\n.. note:: This function is implemented by chaining :func:`torch.mm` calls after\n          computing the optimal matrix multiplication order.\n\n.. note:: The cost of multiplying two matrices with shapes `(a, b)` and `(b, c)` is\n          `a * b * c`. Given matrices `A`, `B`, `C` with shapes `(10, 100)`,\n          `(100, 5)`, `(5, 50)` respectively, we can calculate the cost of different\n          multiplication orders as follows:\n\n          .. math::\n\n             \\begin{align*}\n             \\operatorname{cost}((AB)C) &= 10 \\times 100 \\times 5 + 10 \\times 5 \\times 50 = 7500 \\\\\n             \\operatorname{cost}(A(BC)) &= 10 \\times 100 \\times 50 + 100 \\times 5 \\times 50 = 75000\n             \\end{align*}\n\n          In this case, multiplying `A` and `B` first followed by `C` is 10 times faster.\n\nArgs:\n    tensors (Sequence[Tensor]): two or more tensors to multiply. The first and last\n        tensors may be 1D or 2D. Every other tensor must be 2D.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> from torch.linalg import multi_dot\n\n    >>> multi_dot([torch.tensor([1, 2]), torch.tensor([2, 3])])\n    tensor(8)\n    >>> multi_dot([torch.tensor([[1, 2]]), torch.tensor([2, 3])])\n    tensor([8])\n    >>> multi_dot([torch.tensor([[1, 2]]), torch.tensor([[2], [3]])])\n    tensor([[8]])\n\n    >>> A = torch.arange(2 * 3).view(2, 3)\n    >>> B = torch.arange(3 * 2).view(3, 2)\n    >>> C = torch.arange(2 * 2).view(2, 2)\n    >>> multi_dot((A, B, C))\n    tensor([[ 26,  49],\n            [ 80, 148]])\n')
A:torch.linalg.__init__.svd->_add_docstr(_linalg.linalg_svd, "\nlinalg.svd(A, full_matrices=True, *, out=None) -> (Tensor, Tensor, Tensor)\n\nComputes the singular value decomposition (SVD) of a matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **full SVD** of a matrix\n:math:`A \\in \\mathbb{K}^{m \\times n}`, if `k = min(m,n)`, is defined as\n\n.. math::\n\n    A = U \\operatorname{diag}(S) V^{\\text{H}}\n    \\mathrlap{\\qquad U \\in \\mathbb{K}^{m \\times m}, S \\in \\mathbb{R}^k, V \\in \\mathbb{K}^{n \\times n}}\n\nwhere :math:`\\operatorname{diag}(S) \\in \\mathbb{K}^{m \\times n}`,\n:math:`V^{\\text{H}}` is the conjugate transpose when :math:`V` is complex, and the transpose when :math:`V` is real-valued.\nThe matrices  :math:`U`, :math:`V` (and thus :math:`V^{\\text{H}}`) are orthogonal in the real case, and unitary in the complex case.\n\nWhen `m > n` (resp. `m < n`) we can drop the last `m - n` (resp. `n - m`) columns of `U` (resp. `V`) to form the **reduced SVD**:\n\n.. math::\n\n    A = U \\operatorname{diag}(S) V^{\\text{H}}\n    \\mathrlap{\\qquad U \\in \\mathbb{K}^{m \\times k}, S \\in \\mathbb{R}^k, V \\in \\mathbb{K}^{k \\times n}}\n\nwhere :math:`\\operatorname{diag}(S) \\in \\mathbb{K}^{k \\times k}`.\nIn this case, :math:`U` and :math:`V` also have orthonormal columns.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nThe returned decomposition is a named tuple `(U, S, Vh)`\nwhich corresponds to :math:`U`, :math:`S`, :math:`V^{\\text{H}}` above.\n\nThe singular values are returned in descending order.\n\nThe parameter :attr:`full_matrices` chooses between the full (default) and reduced SVD.\n\nDifferences with `numpy.linalg.svd`:\n\n- Unlike `numpy.linalg.svd`, this function always returns a tuple of three tensors\n  and it doesn't support `compute_uv` argument.\n  Please use :func:`torch.linalg.svdvals`, which computes only the singular values,\n  instead of `compute_uv=False`.\n\n.. note:: When :attr:`full_matrices`\\ `= True`, the gradients with respect to `U[..., :, min(m, n):]`\n          and `Vh[..., min(m, n):, :]` will be ignored, as those vectors can be arbitrary bases\n          of the corresponding subspaces.\n\n.. warning:: The returned tensors `U` and `V` are not unique, nor are they continuous with\n             respect to :attr:`A`.\n             Due to this lack of uniqueness, different hardware and software may compute\n             different singular vectors.\n\n             This non-uniqueness is caused by the fact that multiplying any pair of singular\n             vectors :math:`u_k, v_k` by `-1` in the real case or by\n             :math:`e^{i \\phi}, \\phi \\in \\mathbb{R}` in the complex case produces another two\n             valid singular vectors of the matrix.\n             For this reason, the loss function shall not depend on this :math:`e^{i \\phi}` quantity,\n             as it is not well-defined.\n             This is checked for complex inputs when computing the gradients of this function. As such,\n             when inputs are complex and are on a CUDA device, the computation of the gradients\n             of this function synchronizes that device with the CPU.\n\n.. warning:: Gradients computed using `U` or `Vh` will only be finite when\n             :attr:`A` does not have repeated singular values. If :attr:`A` is rectangular,\n             additionally, zero must also not be one of its singular values.\n             Furthermore, if the distance between any two singular values is close to zero,\n             the gradient will be numerically unstable, as it depends on the singular values\n             :math:`\\sigma_i` through the computation of\n             :math:`\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}`.\n             In the rectangular case, the gradient will also be numerically unstable when\n             :attr:`A` has small singular values, as it also depends on the computation of\n             :math:`\\frac{1}{\\sigma_i}`.\n\n.. seealso::\n\n        :func:`torch.linalg.svdvals` computes only the singular values.\n        Unlike :func:`torch.linalg.svd`, the gradients of :func:`~svdvals` are always\n        numerically stable.\n\n        :func:`torch.linalg.eig` for a function that computes another type of spectral\n        decomposition of a matrix. The eigendecomposition works just on square matrices.\n\n        :func:`torch.linalg.eigh` for a (faster) function that computes the eigenvalue decomposition\n        for Hermitian and symmetric matrices.\n\n        :func:`torch.linalg.qr` for another (much faster) decomposition that works on general\n        matrices.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    full_matrices (bool, optional): controls whether to compute the full or reduced\n                                    SVD, and consequently,\n                                    the shape of the returned tensors\n                                    `U` and `Vh`. Default: `True`.\n\nKeyword args:\n    out (tuple, optional): output tuple of three tensors. Ignored if `None`.\n\nReturns:\n    A named tuple `(U, S, Vh)` which corresponds to :math:`U`, :math:`S`, :math:`V^{\\text{H}}` above.\n\n    `S` will always be real-valued, even when :attr:`A` is complex.\n    It will also be ordered in descending order.\n\n    `U` and `Vh` will have the same dtype as :attr:`A`. The left / right singular vectors will be given by\n    the columns of `U` and the rows of `Vh` respectively.\n\nExamples::\n\n    >>> A = torch.randn(5, 3)\n    >>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n    >>> U.shape, S.shape, Vh.shape\n    (torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))\n    >>> torch.dist(A, U @ torch.diag(S) @ Vh)\n    tensor(1.0486e-06)\n\n    >>> U, S, Vh = torch.linalg.svd(A)\n    >>> U.shape, S.shape, Vh.shape\n    (torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))\n    >>> torch.dist(A, U[:, :3] @ torch.diag(S) @ Vh)\n    tensor(1.0486e-06)\n\n    >>> A = torch.randn(7, 5, 3)\n    >>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n    >>> torch.dist(A, U @ torch.diag_embed(S) @ Vh)\n    tensor(3.0957e-06)\n\n.. _the resulting vectors will span the same subspace:\n    https://en.wikipedia.org/wiki/Singular_value_decomposition#Singular_values,_singular_vectors,_and_their_relation_to_the_SVD\n")
A:torch.linalg.__init__.svdvals->_add_docstr(_linalg.linalg_svdvals, "\nlinalg.svdvals(A, *, out=None) -> Tensor\n\nComputes the singular values of a matrix.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nThe singular values are returned in descending order.\n\n.. note:: This function is equivalent to NumPy's `linalg.svd(A, compute_uv=False)`.\n\n" + f"\n.. note:: {common_notes['sync_note']}\n" + '\n\n.. seealso::\n\n        :func:`torch.linalg.svd` computes the full singular value decomposition.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nReturns:\n    A real-valued tensor, even when :attr:`A` is complex.\n\nExamples::\n\n    >>> A = torch.randn(5, 3)\n    >>> S = torch.linalg.svdvals(A)\n    >>> S\n    tensor([2.5139, 2.1087, 1.1066])\n\n    >>> torch.dist(S, torch.linalg.svd(A, full_matrices=False).S)\n    tensor(2.4576e-07)\n')
A:torch.linalg.__init__.cond->_add_docstr(_linalg.linalg_cond, "\nlinalg.cond(A, p=None, *, out=None) -> Tensor\n\nComputes the condition number of a matrix with respect to a matrix norm.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **condition number** :math:`\\kappa` of a matrix\n:math:`A \\in \\mathbb{K}^{n \\times n}` is defined as\n\n.. math::\n\n    \\kappa(A) = \\|A\\|_p\\|A^{-1}\\|_p\n\nThe condition number of :attr:`A` measures the numerical stability of the linear system `AX = B`\nwith respect to a matrix norm.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\n:attr:`p` defines the matrix norm that is computed. The following norms are supported:\n\n=========    =================================\n:attr:`p`    matrix norm\n=========    =================================\n`None`       `2`-norm (largest singular value)\n`'fro'`      Frobenius norm\n`'nuc'`      nuclear norm\n`inf`        `max(sum(abs(x), dim=1))`\n`-inf`       `min(sum(abs(x), dim=1))`\n`1`          `max(sum(abs(x), dim=0))`\n`-1`         `min(sum(abs(x), dim=0))`\n`2`          largest singular value\n`-2`         smallest singular value\n=========    =================================\n\nwhere `inf` refers to `float('inf')`, NumPy's `inf` object, or any equivalent object.\n\nFor :attr:`p` is one of `('fro', 'nuc', inf, -inf, 1, -1)`, this function uses\n:func:`torch.linalg.norm` and :func:`torch.linalg.inv`.\nAs such, in this case, the matrix (or every matrix in the batch) :attr:`A` has to be square\nand invertible.\n\nFor :attr:`p` in `(2, -2)`, this function can be computed in terms of the singular values\n:math:`\\sigma_1 \\geq \\ldots \\geq \\sigma_n`\n\n.. math::\n\n    \\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_n}\\qquad \\kappa_{-2}(A) = \\frac{\\sigma_n}{\\sigma_1}\n\nIn these cases, it is computed using :func:`torch.linalg.svdvals`. For these norms, the matrix\n(or every matrix in the batch) :attr:`A` may have any shape.\n\n.. note :: When inputs are on a CUDA device, this function synchronizes that device with the CPU\n           if :attr:`p` is one of `('fro', 'nuc', inf, -inf, 1, -1)`.\n\n.. seealso::\n\n        :func:`torch.linalg.solve` for a function that solves linear systems of square matrices.\n\n        :func:`torch.linalg.lstsq` for a function that solves linear systems of general matrices.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions\n                    for :attr:`p` in `(2, -2)`, and of shape `(*, n, n)` where every matrix\n                    is invertible for :attr:`p` in `('fro', 'nuc', inf, -inf, 1, -1)`.\n    p (int, inf, -inf, 'fro', 'nuc', optional):\n        the type of the matrix norm to use in the computations (see above). Default: `None`\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nReturns:\n    A real-valued tensor, even when :attr:`A` is complex.\n\nRaises:\n    RuntimeError:\n        if :attr:`p` is one of `('fro', 'nuc', inf, -inf, 1, -1)`\n        and the :attr:`A` matrix or any matrix in the batch :attr:`A` is not square\n        or invertible.\n\nExamples::\n\n    >>> A = torch.randn(3, 4, 4, dtype=torch.complex64)\n    >>> torch.linalg.cond(A)\n    >>> A = torch.tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])\n    >>> torch.linalg.cond(A)\n    tensor([1.4142])\n    >>> torch.linalg.cond(A, 'fro')\n    tensor(3.1623)\n    >>> torch.linalg.cond(A, 'nuc')\n    tensor(9.2426)\n    >>> torch.linalg.cond(A, float('inf'))\n    tensor(2.)\n    >>> torch.linalg.cond(A, float('-inf'))\n    tensor(1.)\n    >>> torch.linalg.cond(A, 1)\n    tensor(2.)\n    >>> torch.linalg.cond(A, -1)\n    tensor(1.)\n    >>> torch.linalg.cond(A, 2)\n    tensor([1.4142])\n    >>> torch.linalg.cond(A, -2)\n    tensor([0.7071])\n\n    >>> A = torch.randn(2, 3, 3)\n    >>> torch.linalg.cond(A)\n    tensor([[9.5917],\n            [3.2538]])\n    >>> A = torch.randn(2, 3, 3, dtype=torch.complex64)\n    >>> torch.linalg.cond(A)\n    tensor([[4.6245],\n            [4.5671]])\n")
A:torch.linalg.__init__.pinv->_add_docstr(_linalg.linalg_pinv, "\nlinalg.pinv(A, *, atol=None, rtol=None, hermitian=False, out=None) -> Tensor\n\nComputes the pseudoinverse (Moore-Penrose inverse) of a matrix.\n\nThe pseudoinverse may be `defined algebraically`_\nbut it is more computationally convenient to understand it `through the SVD`_\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nIf :attr:`hermitian`\\ `= True`, :attr:`A` is assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.\n\nThe singular values (or the norm of the eigenvalues when :attr:`hermitian`\\ `= True`)\nthat are below :math:`\\max(\\text{atol}, \\sigma_1 \\cdot \\text{rtol})` threshold are\ntreated as zero and discarded in the computation,\nwhere :math:`\\sigma_1` is the largest singular value (or eigenvalue).\n\nIf :attr:`rtol` is not specified and :attr:`A` is a matrix of dimensions `(m, n)`,\nthe relative tolerance is set to be :math:`\\text{rtol} = \\max(m, n) \\varepsilon`\nand :math:`\\varepsilon` is the epsilon value for the dtype of :attr:`A` (see :class:`.finfo`).\nIf :attr:`rtol` is not specified and :attr:`atol` is specified to be larger than zero then\n:attr:`rtol` is set to zero.\n\nIf :attr:`atol` or :attr:`rtol` is a :class:`torch.Tensor`, its shape must be broadcastable to that\nof the singular values of :attr:`A` as returned by :func:`torch.linalg.svd`.\n\n.. note:: This function uses :func:`torch.linalg.svd` if :attr:`hermitian`\\ `= False` and\n          :func:`torch.linalg.eigh` if :attr:`hermitian`\\ `= True`.\n          For CUDA inputs, this function synchronizes that device with the CPU.\n\n.. note::\n    Consider using :func:`torch.linalg.lstsq` if possible for multiplying a matrix on the left by\n    the pseudoinverse, as::\n\n        torch.linalg.lstsq(A, B).solution == A.pinv() @ B\n\n    It is always prefered to use :func:`~lstsq` when possible, as it is faster and more\n    numerically stable than computing the pseudoinverse explicitly.\n\n.. note::\n    This function has NumPy compatible variant `linalg.pinv(A, rcond, hermitian=False)`.\n    However, use of the positional argument :attr:`rcond` is deprecated in favor of :attr:`rtol`.\n\n.. warning::\n    This function uses internally :func:`torch.linalg.svd` (or :func:`torch.linalg.eigh`\n    when :attr:`hermitian`\\ `= True`), so its derivative has the same problems as those of these\n    functions. See the warnings in :func:`torch.linalg.svd` and :func:`torch.linalg.eigh` for\n    more details.\n\n.. seealso::\n\n        :func:`torch.linalg.inv` computes the inverse of a square matrix.\n\n        :func:`torch.linalg.lstsq` computes :attr:`A`\\ `.pinv() @ \\ `:attr:`B` with a\n        numerically stable algorithm.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    rcond (float, Tensor, optional): [NumPy Compat]. Alias for :attr:`rtol`. Default: `None`.\n\nKeyword args:\n    atol (float, Tensor, optional): the absolute tolerance value. When `None` it's considered to be zero.\n                                    Default: `None`.\n    rtol (float, Tensor, optional): the relative tolerance value. See above for the value it takes when `None`.\n                                    Default: `None`.\n    hermitian(bool, optional): indicates whether :attr:`A` is Hermitian if complex\n                               or symmetric if real. Default: `False`.\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> A = torch.randn(3, 5)\n    >>> A\n    tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n            [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n            [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n    >>> torch.linalg.pinv(A)\n    tensor([[ 0.0600, -0.1933, -0.2090],\n            [-0.0903, -0.0817, -0.4752],\n            [-0.7124, -0.1631, -0.2272],\n            [ 0.1356,  0.3933, -0.5023],\n            [-0.0308, -0.1725, -0.5216]])\n\n    >>> A = torch.randn(2, 6, 3)\n    >>> Apinv = torch.linalg.pinv(A)\n    >>> torch.dist(Apinv @ A, torch.eye(3))\n    tensor(8.5633e-07)\n\n    >>> A = torch.randn(3, 3, dtype=torch.complex64)\n    >>> A = A + A.T.conj()  # creates a Hermitian matrix\n    >>> Apinv = torch.linalg.pinv(A, hermitian=True)\n    >>> torch.dist(Apinv @ A, torch.eye(3))\n    tensor(1.0830e-06)\n\n.. _defined algebraically:\n    https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Existence_and_uniqueness\n.. _through the SVD:\n    https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)\n")
A:torch.linalg.__init__.matrix_exp->_add_docstr(_linalg.linalg_matrix_exp, '\nlinalg.matrix_exp(A) -> Tensor\n\nComputes the matrix exponential of a square matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthis function computes the **matrix exponential** of :math:`A \\in \\mathbb{K}^{n \\times n}`, which is defined as\n\n.. math::\n    \\mathrm{matrix_exp}(A) = \\sum_{k=0}^\\infty \\frac{1}{k!}A^k \\in \\mathbb{K}^{n \\times n}.\n\nIf the matrix :math:`A` has eigenvalues :math:`\\lambda_i \\in \\mathbb{C}`,\nthe matrix :math:`\\mathrm{matrix_exp}(A)` has eigenvalues :math:`e^{\\lambda_i} \\in \\mathbb{C}`.\n\nSupports input of bfloat16, float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.\n\nExample::\n\n    >>> A = torch.empty(2, 2, 2)\n    >>> A[0, :, :] = torch.eye(2, 2)\n    >>> A[1, :, :] = 2 * torch.eye(2, 2)\n    >>> A\n    tensor([[[1., 0.],\n             [0., 1.]],\n\n            [[2., 0.],\n             [0., 2.]]])\n    >>> torch.linalg.matrix_exp(A)\n    tensor([[[2.7183, 0.0000],\n             [0.0000, 2.7183]],\n\n             [[7.3891, 0.0000],\n              [0.0000, 7.3891]]])\n\n    >>> import math\n    >>> A = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]]) # A is skew-symmetric\n    >>> torch.linalg.matrix_exp(A) # matrix_exp(A) = [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\n    tensor([[ 0.5000,  0.8660],\n            [-0.8660,  0.5000]])\n')
A:torch.linalg.__init__.solve->_add_docstr(_linalg.linalg_solve, '\nlinalg.solve(A, B, *, out=None) -> Tensor\n\nComputes the solution of a square system of linear equations with a unique solution.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthis function computes the solution :math:`X \\in \\mathbb{K}^{n \\times k}` of the **linear system** associated to\n:math:`A \\in \\mathbb{K}^{n \\times n}, B \\in \\mathbb{K}^{n \\times k}`, which is defined as\n\n.. math:: AX = B\n\nThis system of linear equations has one solution if and only if :math:`A` is `invertible`_.\nThis function assumes that :math:`A` is invertible.\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions.\n\nLetting `*` be zero or more batch dimensions,\n\n- If :attr:`A` has shape `(*, n, n)` and :attr:`B` has shape `(*, n)` (a batch of vectors) or shape\n  `(*, n, k)` (a batch of matrices or "multiple right-hand sides"), this function returns `X` of shape\n  `(*, n)` or `(*, n, k)` respectively.\n- Otherwise, if :attr:`A` has shape `(*, n, n)` and  :attr:`B` has shape `(n,)`  or `(n, k)`, :attr:`B`\n  is broadcasted to have shape `(*, n)` or `(*, n, k)` respectively.\n  This function then returns the solution of the resulting batch of systems of linear equations.\n\n.. note::\n    This function computes `X = \\ `:attr:`A`\\ `.inverse() @ \\ `:attr:`B` in a faster and\n    more numerically stable way than performing the computations separately.\n\n.. note::\n    It is possible to compute the solution of the system :math:`XA = B` by passing the inputs\n    :attr:`A` and :attr:`B` transposed and transposing the output returned by this function.\n\n' + f"\n.. note:: {common_notes['sync_note']}\n" + '\n\n.. seealso::\n\n        :func:`torch.linalg.solve_triangular` computes the solution of a triangular system of linear\n        equations with a unique solution.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.\n    B (Tensor): right-hand side tensor of shape `(*, n)` or  `(*, n, k)` or `(n,)` or `(n, k)`\n                according to the rules described above\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if the :attr:`A` matrix is not invertible or any matrix in a batched :attr:`A`\n                  is not invertible.\n\nExamples::\n\n    >>> A = torch.randn(3, 3)\n    >>> b = torch.randn(3)\n    >>> x = torch.linalg.solve(A, b)\n    >>> torch.allclose(A @ x, b)\n    True\n    >>> A = torch.randn(2, 3, 3)\n    >>> B = torch.randn(2, 3, 4)\n    >>> X = torch.linalg.solve(A, B)\n    >>> X.shape\n    torch.Size([2, 3, 4])\n    >>> torch.allclose(A @ X, B)\n    True\n\n    >>> A = torch.randn(2, 3, 3)\n    >>> b = torch.randn(3, 1)\n    >>> x = torch.linalg.solve(A, b) # b is broadcasted to size (2, 3, 1)\n    >>> x.shape\n    torch.Size([2, 3, 1])\n    >>> torch.allclose(A @ x, b)\n    True\n    >>> b = torch.randn(3)\n    >>> x = torch.linalg.solve(A, b) # b is broadcasted to size (2, 3)\n    >>> x.shape\n    torch.Size([2, 3])\n    >>> Ax = A @ x.unsqueeze(-1)\n    >>> torch.allclose(Ax, b.unsqueeze(-1).expand_as(Ax))\n    True\n\n.. _invertible:\n    https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem\n')
A:torch.linalg.__init__.solve_triangular->_add_docstr(_linalg.linalg_solve_triangular, '\nlinalg.solve_triangular(A, B, *, upper, left=True, unitriangular=False, out=None) -> Tensor\n\nComputes the solution of a triangular system of linear equations with a unique solution.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthis function computes the solution :math:`X \\in \\mathbb{K}^{n \\times k}` of the **linear system**\nassociated to the triangular matrix :math:`A \\in \\mathbb{K}^{n \\times n}` without zeros on the diagonal\n(that is, it is `invertible`_) and the rectangular matrix , :math:`B \\in \\mathbb{K}^{n \\times k}`,\nwhich is defined as\n\n.. math:: AX = B\n\nThe argument :attr:`upper` signals whether :math:`A` is upper or lower triangular.\n\nIf :attr:`left`\\ `= False`, this function returns the matrix :math:`X \\in \\mathbb{K}^{n \\times k}` that\nsolves the system\n\n.. math::\n\n    XA = B\\mathrlap{\\qquad A \\in \\mathbb{K}^{k \\times k}, B \\in \\mathbb{K}^{n \\times k}.}\n\nIf :attr:`upper`\\ `= True` (resp. `False`) just the upper (resp. lower) triangular half of :attr:`A`\nwill be accessed. The elements below the main diagonal will be considered to be zero and will not be accessed.\n\nIf :attr:`unitriangular`\\ `= True`, the diagonal of :attr:`A` is assumed to be ones and will not be accessed.\n\nThe result may contain `NaN` s if the diagonal of :attr:`A` contains zeros or elements that\nare very close to zero and :attr:`unitriangular`\\ `= False` (default) or if the input matrix\nhas very small eigenvalues.\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions.\n\n.. seealso::\n\n        :func:`torch.linalg.solve` computes the solution of a general square system of linear\n        equations with a unique solution.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` (or `(*, k, k)` if :attr:`left`\\ `= True`)\n                where `*` is zero or more batch dimensions.\n    B (Tensor): right-hand side tensor of shape `(*, n, k)`.\n\nKeyword args:\n    upper (bool): whether :attr:`A` is an upper or lower triangular matrix.\n    left (bool, optional): whether to solve the system :math:`AX=B` or :math:`XA = B`. Default: `True`.\n    unitriangular (bool, optional): if `True`, the diagonal elements of :attr:`A` are assumed to be\n                                    all equal to `1`. Default: `False`.\n    out (Tensor, optional): output tensor. `B` may be passed as `out` and the result is computed in-place on `B`.\n                            Ignored if `None`. Default: `None`.\n\nExamples::\n\n    >>> A = torch.randn(3, 3).triu_()\n    >>> b = torch.randn(3, 4)\n    >>> X = torch.linalg.solve_triangular(A, B, upper=True)\n    >>> torch.allclose(A @ X, B)\n    True\n\n    >>> A = torch.randn(2, 3, 3).tril_()\n    >>> B = torch.randn(2, 3, 4)\n    >>> X = torch.linalg.solve_triangular(A, B, upper=False)\n    >>> torch.allclose(A @ X, B)\n    True\n\n    >>> A = torch.randn(2, 4, 4).tril_()\n    >>> B = torch.randn(2, 3, 4)\n    >>> X = torch.linalg.solve_triangular(A, B, upper=False, left=False)\n    >>> torch.allclose(X @ A, B)\n    True\n\n.. _invertible:\n    https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem\n')
A:torch.linalg.__init__.lu_factor->_add_docstr(_linalg.linalg_lu_factor, '\nlinalg.lu_factor(A, *, bool pivot=True, out=None) -> (Tensor, Tensor)\n\nComputes a compact representation of the LU factorization with partial pivoting of a matrix.\n\nThis function computes a compact representation of the decomposition given by :func:`torch.linalg.lu`.\nIf the matrix is square, this representation may be used in :func:`torch.linalg.lu_solve`\nto solve system of linear equations that share the matrix :attr:`A`.\n\nThe returned decomposition is represented as a named tuple `(LU, pivots)`.\nThe ``LU`` matrix has the same shape as the input matrix ``A``. Its upper and lower triangular\nparts encode the non-constant elements of ``L`` and ``U`` of the LU decomposition of ``A``.\n\nThe returned permutation matrix is represented by a 1-indexed vector. `pivots[i] == j` represents\nthat in the `i`-th step of the algorithm, the `i`-th row was permuted with the `j-1`-th row.\n\nOn CUDA, one may use :attr:`pivot`\\ `= False`. In this case, this function returns the LU\ndecomposition without pivoting if it exists.\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions.\n\n' + f"\n.. note:: {common_notes['sync_note_has_ex'].format('torch.linalg.lu_factor_ex')}\n" + '\n.. warning:: The LU decomposition is almost never unique, as often there are different permutation\n             matrices that can yield different LU decompositions.\n             As such, different platforms, like SciPy, or inputs on different devices,\n             may produce different valid decompositions.\n\n.. warning:: Gradient computations are only supported if the input matrix is full-rank.\n             If this condition is not met, no error will be thrown, but the gradient may not be finite.\n             This is because the LU decomposition with pivoting is not differentiable at these points.\n\n.. seealso::\n\n        :func:`torch.linalg.lu_solve` solves a system of linear equations given the output of this\n        function provided the input matrix was square and invertible.\n\n        :func:`torch.linalg.lu` computes the LU decomposition with partial pivoting of a possibly\n        non-square matrix.\n\n        :func:`torch.linalg.solve` solves a system of linear equations. It is a composition\n        of :func:`~lu_factor` and :func:`~lu_solve`.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    pivot (bool, optional): Whether to compute the LU decomposition with partial pivoting, or the regular LU\n                            decomposition. :attr:`pivot`\\ `= False` not supported on CPU. Default: `True`.\n    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(LU, pivots)`.\n\nRaises:\n    RuntimeError: if the :attr:`A` matrix is not invertible or any matrix in a batched :attr:`A`\n                  is not invertible.\n\nExamples::\n\n    >>> A = torch.randn(2, 3, 3)\n    >>> B1 = torch.randn(2, 3, 4)\n    >>> B2 = torch.randn(2, 3, 7)\n    >>> A_factor = torch.linalg.lu_factor(A)\n    >>> X1 = torch.linalg.lu_solve(A_factor, B1)\n    >>> X2 = torch.linalg.lu_solve(A_factor, B2)\n    >>> torch.allclose(A @ X1, B1)\n    True\n    >>> torch.allclose(A @ X2, B2)\n    True\n\n.. _invertible:\n    https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem\n')
A:torch.linalg.__init__.lu_factor_ex->_add_docstr(_linalg.linalg_lu_factor_ex, "\nlinalg.lu_factor_ex(A, *, pivot=True, check_errors=False, out=None) -> (Tensor, Tensor, Tensor)\n\nThis is a version of :func:`~lu_factor` that does not perform error checks unless :attr:`check_errors`\\ `= True`.\nIt also returns the :attr:`info` tensor returned by `LAPACK's getrf`_.\n\n" + f"\n.. note:: {common_notes['sync_note_ex']}\n\n.. warning:: {common_notes['experimental_warning']}\n" + "\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    pivot (bool, optional): Whether to compute the LU decomposition with partial pivoting, or the regular LU\n                            decomposition. :attr:`pivot`\\ `= False` not supported on CPU. Default: `True`.\n    check_errors (bool, optional): controls whether to check the content of ``infos`` and raise\n                                   an error if it is non-zero. Default: `False`.\n    out (tuple, optional): tuple of three tensors to write the output to. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(LU, pivots, info)`.\n\n.. _LAPACK's getrf:\n    https://www.netlib.org/lapack/explore-html/dd/d9a/group__double_g_ecomputational_ga0019443faea08275ca60a734d0593e60.html\n")
A:torch.linalg.__init__.tensorinv->_add_docstr(_linalg.linalg_tensorinv, '\nlinalg.tensorinv(A, ind=2, *, out=None) -> Tensor\n\nComputes the multiplicative inverse of :func:`torch.tensordot`.\n\nIf `m` is the product of the first :attr:`ind` dimensions of :attr:`A` and `n` is the product of\nthe rest of the dimensions, this function expects `m` and `n` to be equal.\nIf this is the case, it computes a tensor `X` such that\n`tensordot(\\ `:attr:`A`\\ `, X, \\ `:attr:`ind`\\ `)` is the identity matrix in dimension `m`.\n`X` will have the shape of :attr:`A` but with the first :attr:`ind` dimensions pushed back to the end\n\n.. code:: text\n\n    X.shape == A.shape[ind:] + A.shape[:ind]\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\n.. note:: When :attr:`A` is a `2`-dimensional tensor and :attr:`ind`\\ `= 1`,\n          this function computes the (multiplicative) inverse of :attr:`A`\n          (see :func:`torch.linalg.inv`).\n\n.. note::\n    Consider using :func:`torch.linalg.tensorsolve` if possible for multiplying a tensor on the left\n    by the tensor inverse, as::\n\n        tensorsolve(A, B) == torch.tensordot(tensorinv(A), B)\n\n    It is always prefered to use :func:`~tensorsolve` when possible, as it is faster and more\n    numerically stable than computing the pseudoinverse explicitly.\n\n.. seealso::\n\n        :func:`torch.linalg.tensorsolve` computes\n        `torch.tensordot(tensorinv(\\ `:attr:`A`\\ `), \\ `:attr:`B`\\ `)`.\n\nArgs:\n    A (Tensor): tensor to invert. Its shape must satisfy\n                    `prod(\\ `:attr:`A`\\ `.shape[:\\ `:attr:`ind`\\ `]) ==\n                    prod(\\ `:attr:`A`\\ `.shape[\\ `:attr:`ind`\\ `:])`.\n    ind (int): index at which to compute the inverse of :func:`torch.tensordot`. Default: `2`.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if the reshaped :attr:`A` is not invertible or the product of the first\n                  :attr:`ind` dimensions is not equal to the product of the rest.\n\nExamples::\n\n    >>> A = torch.eye(4 * 6).reshape((4, 6, 8, 3))\n    >>> Ainv = torch.linalg.tensorinv(A, ind=2)\n    >>> Ainv.shape\n    torch.Size([8, 3, 4, 6])\n    >>> B = torch.randn(4, 6)\n    >>> torch.allclose(torch.tensordot(Ainv, B), torch.linalg.tensorsolve(A, B))\n    True\n\n    >>> A = torch.randn(4, 4)\n    >>> Atensorinv = torch.linalg.tensorinv(A, ind=1)\n    >>> Ainv = torch.linalg.inverse(A)\n    >>> torch.allclose(Atensorinv, Ainv)\n    True\n')
A:torch.linalg.__init__.tensorsolve->_add_docstr(_linalg.linalg_tensorsolve, '\nlinalg.tensorsolve(A, B, dims=None, *, out=None) -> Tensor\n\nComputes the solution `X` to the system `torch.tensordot(A, X) = B`.\n\nIf `m` is the product of the first :attr:`B`\\ `.ndim`  dimensions of :attr:`A` and\n`n` is the product of the rest of the dimensions, this function expects `m` and `n` to be equal.\n\nThe returned tensor `x` satisfies\n`tensordot(\\ `:attr:`A`\\ `, x, dims=x.ndim) == \\ `:attr:`B`.\n`x` has shape :attr:`A`\\ `[B.ndim:]`.\n\nIf :attr:`dims` is specified, :attr:`A` will be reshaped as\n\n.. code:: text\n\n    A = movedim(A, dims, range(len(dims) - A.ndim + 1, 0))\n\nSupports inputs of float, double, cfloat and cdouble dtypes.\n\n.. seealso::\n\n        :func:`torch.linalg.tensorinv` computes the multiplicative inverse of\n        :func:`torch.tensordot`.\n\nArgs:\n    A (Tensor): tensor to solve for. Its shape must satisfy\n                    `prod(\\ `:attr:`A`\\ `.shape[:\\ `:attr:`B`\\ `.ndim]) ==\n                    prod(\\ `:attr:`A`\\ `.shape[\\ `:attr:`B`\\ `.ndim:])`.\n    B (Tensor): tensor of shape :attr:`A`\\ `.shape[\\ `:attr:`B`\\ `.ndim]`.\n    dims (Tuple[int], optional): dimensions of :attr:`A` to be moved.\n        If `None`, no dimensions are moved. Default: `None`.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nRaises:\n    RuntimeError: if the reshaped :attr:`A`\\ `.view(m, m)` with `m` as above  is not\n                  invertible or the product of the first :attr:`ind` dimensions is not equal\n                  to the product of the rest of the dimensions.\n\nExamples::\n\n    >>> A = torch.eye(2 * 3 * 4).reshape((2 * 3, 4, 2, 3, 4))\n    >>> B = torch.randn(2 * 3, 4)\n    >>> X = torch.linalg.tensorsolve(A, B)\n    >>> X.shape\n    torch.Size([2, 3, 4])\n    >>> torch.allclose(torch.tensordot(A, X, dims=X.ndim), B)\n    True\n\n    >>> A = torch.randn(6, 4, 4, 3, 2)\n    >>> B = torch.randn(4, 3, 2)\n    >>> X = torch.linalg.tensorsolve(A, B, dims=(0, 2))\n    >>> X.shape\n    torch.Size([6, 4])\n    >>> A = A.permute(1, 3, 4, 0, 2)\n    >>> A.shape[B.ndim:]\n    torch.Size([6, 4])\n    >>> torch.allclose(torch.tensordot(A, X, dims=X.ndim), B, atol=1e-6)\n    True\n')
A:torch.linalg.__init__.qr->_add_docstr(_linalg.linalg_qr, "\nqr(A, mode='reduced', *, out=None) -> (Tensor, Tensor)\n\nComputes the QR decomposition of a matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **full QR decomposition** of a matrix\n:math:`A \\in \\mathbb{K}^{m \\times n}` is defined as\n\n.. math::\n\n    A = QR\\mathrlap{\\qquad Q \\in \\mathbb{K}^{m \\times m}, R \\in \\mathbb{K}^{m \\times n}}\n\nwhere :math:`Q` is orthogonal in the real case and unitary in the complex case, and :math:`R` is upper triangular.\n\nWhen `m > n` (tall matrix), as `R` is upper triangular, its last `m - n` rows are zero.\nIn this case, we can drop the last `m - n` columns of `Q` to form the\n**reduced QR decomposition**:\n\n.. math::\n\n    A = QR\\mathrlap{\\qquad Q \\in \\mathbb{K}^{m \\times n}, R \\in \\mathbb{K}^{n \\times n}}\n\nThe reduced QR decomposition agrees with the full QR decomposition when `n >= m` (wide matrix).\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nThe parameter :attr:`mode` chooses between the full and reduced QR decomposition.\nIf :attr:`A` has shape `(*, m, n)`, denoting `k = min(m, n)`\n\n- :attr:`mode`\\ `= 'reduced'` (default): Returns `(Q, R)` of shapes `(*, m, k)`, `(*, k, n)` respectively.\n- :attr:`mode`\\ `= 'complete'`: Returns `(Q, R)` of shapes `(*, m, m)`, `(*, m, n)` respectively.\n- :attr:`mode`\\ `= 'r'`: Computes only the reduced `R`. Returns `(Q, R)` with `Q` empty and `R` of shape `(*, k, n)`.\n\nDifferences with `numpy.linalg.qr`:\n\n- :attr:`mode`\\ `= 'raw'` is not implemented.\n- Unlike `numpy.linalg.qr`, this function always returns a tuple of two tensors.\n  When :attr:`mode`\\ `= 'r'`, the `Q` tensor is an empty tensor.\n  This behavior may change in a future PyTorch release.\n\n.. note:: The elements in the diagonal of `R` are not necessarily positive.\n\n.. note:: :attr:`mode`\\ `= 'r'` does not support backpropagation. Use :attr:`mode`\\ `= 'reduced'` instead.\n\n.. warning:: The QR decomposition is only unique up to the sign of the diagonal of `R` when the\n             first `k = min(m, n)` columns of :attr:`A` are linearly independent.\n             If this is not the case, different platforms, like NumPy,\n             or inputs on different devices, may produce different valid decompositions.\n\n.. warning:: Gradient computations are only supported if the first `k = min(m, n)` columns\n             of every matrix in :attr:`A` are linearly independent.\n             If this condition is not met, no error will be thrown, but the gradient produced\n             will be incorrect.\n             This is because the QR decomposition is not differentiable at these points.\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    mode (str, optional): one of `'reduced'`, `'complete'`, `'r'`.\n                          Controls the shape of the returned tensors. Default: `'reduced'`.\n\nKeyword args:\n    out (tuple, optional): output tuple of two tensors. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(Q, R)`.\n\nExamples::\n\n    >>> A = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n    >>> Q, R = torch.linalg.qr(A)\n    >>> Q\n    tensor([[-0.8571,  0.3943,  0.3314],\n            [-0.4286, -0.9029, -0.0343],\n            [ 0.2857, -0.1714,  0.9429]])\n    >>> R\n    tensor([[ -14.0000,  -21.0000,   14.0000],\n            [   0.0000, -175.0000,   70.0000],\n            [   0.0000,    0.0000,  -35.0000]])\n    >>> (Q @ R).round()\n    tensor([[  12.,  -51.,    4.],\n            [   6.,  167.,  -68.],\n            [  -4.,   24.,  -41.]])\n    >>> (Q.T @ Q).round()\n    tensor([[ 1.,  0.,  0.],\n            [ 0.,  1., -0.],\n            [ 0., -0.,  1.]])\n    >>> Q2, R2 = torch.linalg.qr(A, mode='r')\n    >>> Q2\n    tensor([])\n    >>> torch.equal(R, R2)\n    True\n    >>> A = torch.randn(3, 4, 5)\n    >>> Q, R = torch.linalg.qr(A, mode='complete')\n    >>> torch.dist(Q @ R, A)\n    tensor(1.6099e-06)\n    >>> torch.dist(Q.mT @ Q, torch.eye(4))\n    tensor(6.2158e-07)\n")


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/profiler/python_tracer.py----------------------------------------
A:torch.profiler.python_tracer.path_prefixes->sorted({os.path.abspath(i) for i in raw_paths})
torch.profiler.python_tracer._prefix_regex()->typing.List[str]


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/profiler/profiler.py----------------------------------------
A:torch.profiler.profiler.self.profiler->torch.autograd.profiler.profile(use_cuda=ProfilerActivity.CUDA in self.activities, use_cpu=ProfilerActivity.CPU in self.activities, record_shapes=self.record_shapes, with_flops=self.with_flops, profile_memory=self.profile_memory, with_stack=self.with_stack, with_modules=self.with_modules, use_kineto=True)
A:torch.profiler.profiler.dist_info->self._get_distributed_info()
A:torch.profiler.profiler.fp->tempfile.NamedTemporaryFile('w+t', suffix='.json', delete=False)
A:torch.profiler.profiler.retvalue->self.profiler.export_chrome_trace(fp.name)
A:torch.profiler.profiler.worker_name->'{}_{}'.format(socket.gethostname(), str(os.getpid()))
A:torch.profiler.profiler.file_name->'{}.{}.pt.trace.json'.format(worker_name, int(time.time() * 1000))
A:torch.profiler.profiler.self.current_action->self.schedule(self.step_num)
A:torch.profiler.profiler.self.step_rec_fn->torch.autograd.profiler.record_function('ProfilerStep#' + str(self.step_num))
A:torch.profiler.profiler.action_list->self.action_map.get((prev_action, current_action))
torch.profiler.ProfilerAction(Enum)
torch.profiler._KinetoProfile(self,*,activities:Optional[Iterable[ProfilerActivity]]=None,record_shapes:bool=False,profile_memory:bool=False,with_stack:bool=False,with_flops:bool=False,with_modules:bool=False)
torch.profiler._KinetoProfile._get_distributed_info(self)
torch.profiler._KinetoProfile.add_metadata(self,key:str,value:str)
torch.profiler._KinetoProfile.add_metadata_json(self,key:str,value:str)
torch.profiler._KinetoProfile.events(self)
torch.profiler._KinetoProfile.export_chrome_trace(self,path:str)
torch.profiler._KinetoProfile.export_stacks(self,path:str,metric:str='self_cpu_time_total')
torch.profiler._KinetoProfile.key_averages(self,group_by_input_shape:bool=False,group_by_stack_n:int=0)
torch.profiler._KinetoProfile.prepare_trace(self)
torch.profiler._KinetoProfile.start(self)
torch.profiler._KinetoProfile.start_trace(self)
torch.profiler._KinetoProfile.stop(self)
torch.profiler._KinetoProfile.stop_trace(self)
torch.profiler._default_schedule_fn(_:int)->ProfilerAction
torch.profiler.profile(self,*,activities:Optional[Iterable[ProfilerActivity]]=None,schedule:Optional[Callable[[int],ProfilerAction]]=None,on_trace_ready:Optional[Callable[...,Any]]=None,record_shapes:bool=False,profile_memory:bool=False,with_stack:bool=False,with_flops:bool=False,with_modules:bool=False,use_cuda:Optional[bool]=None)
torch.profiler.profile.__enter__(self)
torch.profiler.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.profiler.profile._trace_ready(self)
torch.profiler.profile._transit_action(self,prev_action,current_action)
torch.profiler.profile.start(self)
torch.profiler.profile.step(self)
torch.profiler.profile.stop(self)
torch.profiler.profiler.ProfilerAction(Enum)
torch.profiler.profiler._KinetoProfile(self,*,activities:Optional[Iterable[ProfilerActivity]]=None,record_shapes:bool=False,profile_memory:bool=False,with_stack:bool=False,with_flops:bool=False,with_modules:bool=False)
torch.profiler.profiler._KinetoProfile.__init__(self,*,activities:Optional[Iterable[ProfilerActivity]]=None,record_shapes:bool=False,profile_memory:bool=False,with_stack:bool=False,with_flops:bool=False,with_modules:bool=False)
torch.profiler.profiler._KinetoProfile._get_distributed_info(self)
torch.profiler.profiler._KinetoProfile.add_metadata(self,key:str,value:str)
torch.profiler.profiler._KinetoProfile.add_metadata_json(self,key:str,value:str)
torch.profiler.profiler._KinetoProfile.events(self)
torch.profiler.profiler._KinetoProfile.export_chrome_trace(self,path:str)
torch.profiler.profiler._KinetoProfile.export_stacks(self,path:str,metric:str='self_cpu_time_total')
torch.profiler.profiler._KinetoProfile.key_averages(self,group_by_input_shape:bool=False,group_by_stack_n:int=0)
torch.profiler.profiler._KinetoProfile.prepare_trace(self)
torch.profiler.profiler._KinetoProfile.start(self)
torch.profiler.profiler._KinetoProfile.start_trace(self)
torch.profiler.profiler._KinetoProfile.stop(self)
torch.profiler.profiler._KinetoProfile.stop_trace(self)
torch.profiler.profiler._default_schedule_fn(_:int)->ProfilerAction
torch.profiler.profiler.profile(self,*,activities:Optional[Iterable[ProfilerActivity]]=None,schedule:Optional[Callable[[int],ProfilerAction]]=None,on_trace_ready:Optional[Callable[...,Any]]=None,record_shapes:bool=False,profile_memory:bool=False,with_stack:bool=False,with_flops:bool=False,with_modules:bool=False,use_cuda:Optional[bool]=None)
torch.profiler.profiler.profile.__enter__(self)
torch.profiler.profiler.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.profiler.profiler.profile.__init__(self,*,activities:Optional[Iterable[ProfilerActivity]]=None,schedule:Optional[Callable[[int],ProfilerAction]]=None,on_trace_ready:Optional[Callable[...,Any]]=None,record_shapes:bool=False,profile_memory:bool=False,with_stack:bool=False,with_flops:bool=False,with_modules:bool=False,use_cuda:Optional[bool]=None)
torch.profiler.profiler.profile._trace_ready(self)
torch.profiler.profiler.profile._transit_action(self,prev_action,current_action)
torch.profiler.profiler.profile.start(self)
torch.profiler.profiler.profile.step(self)
torch.profiler.profiler.profile.stop(self)
torch.profiler.profiler.schedule(*,wait:int,warmup:int,active:int,repeat:int=0,skip_first:int=0)->Callable
torch.profiler.profiler.supported_activities()
torch.profiler.profiler.tensorboard_trace_handler(dir_name:str,worker_name:Optional[str]=None,use_gzip:bool=False)
torch.profiler.schedule(*,wait:int,warmup:int,active:int,repeat:int=0,skip_first:int=0)->Callable
torch.profiler.supported_activities()
torch.profiler.tensorboard_trace_handler(dir_name:str,worker_name:Optional[str]=None,use_gzip:bool=False)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/profiler/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/swa_utils.py----------------------------------------
A:torch.optim.swa_utils.self.module->self.module.to(device)
A:torch.optim.swa_utils.p_model_->p_model.detach().to(device)
A:torch.optim.swa_utils.module.running_mean->torch.zeros_like(module.running_mean)
A:torch.optim.swa_utils.module.running_var->torch.ones_like(module.running_var)
A:torch.optim.swa_utils.input->input.to(device).to(device)
A:torch.optim.swa_utils.swa_lrs->self._format_param(optimizer, swa_lr)
A:torch.optim.swa_utils.step->max(1, step)
A:torch.optim.swa_utils.prev_t->max(0, min(1, (step - 1) / max(1, self.anneal_epochs)))
A:torch.optim.swa_utils.prev_alpha->self.anneal_func(prev_t)
A:torch.optim.swa_utils.t->max(0, min(1, step / max(1, self.anneal_epochs)))
A:torch.optim.swa_utils.alpha->self.anneal_func(t)
torch.optim.swa_utils.AveragedModel(self,model,device=None,avg_fn=None,use_buffers=False)
torch.optim.swa_utils.AveragedModel.__init__(self,model,device=None,avg_fn=None,use_buffers=False)
torch.optim.swa_utils.AveragedModel.forward(self,*args,**kwargs)
torch.optim.swa_utils.AveragedModel.update_parameters(self,model)
torch.optim.swa_utils.SWALR(self,optimizer,swa_lr,anneal_epochs=10,anneal_strategy='cos',last_epoch=-1)
torch.optim.swa_utils.SWALR.__init__(self,optimizer,swa_lr,anneal_epochs=10,anneal_strategy='cos',last_epoch=-1)
torch.optim.swa_utils.SWALR._cosine_anneal(t)
torch.optim.swa_utils.SWALR._format_param(optimizer,swa_lrs)
torch.optim.swa_utils.SWALR._get_initial_lr(lr,swa_lr,alpha)
torch.optim.swa_utils.SWALR._linear_anneal(t)
torch.optim.swa_utils.SWALR.get_lr(self)
torch.optim.swa_utils.update_bn(loader,model,device=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/swa_utils.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/sgd.py----------------------------------------
A:torch.optim.sgd.defaults->dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov, maximize=maximize)
A:torch.optim.sgd.loss->closure()
torch.optim.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False,*,maximize=False)
torch.optim.SGD.__setstate__(self,state)
torch.optim.SGD.step(self,closure=None)
torch.optim.sgd.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False,*,maximize=False)
torch.optim.sgd.SGD.__init__(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False,*,maximize=False)
torch.optim.sgd.SGD.__setstate__(self,state)
torch.optim.sgd.SGD.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/sgd.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/rprop.py----------------------------------------
A:torch.optim.rprop.defaults->dict(lr=lr, etas=etas, step_sizes=step_sizes)
A:torch.optim.rprop.loss->closure()
A:torch.optim.rprop.state['prev']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rprop.state['step_size']->grad.new().resize_as_(grad).fill_(group['lr'])
torch.optim.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.Rprop.step(self,closure=None)
torch.optim.rprop.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.__init__(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/rprop.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/lbfgs.py----------------------------------------
A:torch.optim.lbfgs.d2->d2_square.sqrt()
A:torch.optim.lbfgs.d_norm->flat_grad.neg().abs().max()
A:torch.optim.lbfgs.g->g.clone(memory_format=torch.contiguous_format).clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.(f_new, g_new)->obj_func(x, t, d)
A:torch.optim.lbfgs.gtd_new->g_new.dot(d)
A:torch.optim.lbfgs.t->state.get('t')
A:torch.optim.lbfgs.g_prev->g_new.clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.bracket_g[high_pos]->g_new.clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.bracket_g[low_pos]->g_new.clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.defaults->dict(lr=lr, max_iter=max_iter, max_eval=max_eval, tolerance_grad=tolerance_grad, tolerance_change=tolerance_change, history_size=history_size, line_search_fn=line_search_fn)
A:torch.optim.lbfgs.self._numel_cache->reduce(lambda total, p: total + p.numel(), self._params, 0)
A:torch.optim.lbfgs.view->p.grad.view(-1)
A:torch.optim.lbfgs.numel->p.numel()
A:torch.optim.lbfgs.loss->float(closure())
A:torch.optim.lbfgs.flat_grad->self._gather_flat_grad()
A:torch.optim.lbfgs.closure->torch.enable_grad()(closure)
A:torch.optim.lbfgs.orig_loss->closure()
A:torch.optim.lbfgs.d->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.old_dirs->state.get('old_dirs')
A:torch.optim.lbfgs.old_stps->state.get('old_stps')
A:torch.optim.lbfgs.ro->state.get('ro')
A:torch.optim.lbfgs.H_diag->state.get('H_diag')
A:torch.optim.lbfgs.prev_flat_grad->self._gather_flat_grad().clone(memory_format=torch.contiguous_format)
A:torch.optim.lbfgs.prev_loss->state.get('prev_loss')
A:torch.optim.lbfgs.y->self._gather_flat_grad().sub(prev_flat_grad)
A:torch.optim.lbfgs.s->self._gather_flat_grad().neg().mul(t)
A:torch.optim.lbfgs.ys->self._gather_flat_grad().sub(prev_flat_grad).dot(s)
A:torch.optim.lbfgs.num_old->len(old_dirs)
A:torch.optim.lbfgs.q->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.dr->torch.mul(q, H_diag)
A:torch.optim.lbfgs.gtd->self._gather_flat_grad().dot(d)
A:torch.optim.lbfgs.x_init->self._clone_param()
A:torch.optim.lbfgs.(loss, flat_grad, t, ls_func_evals)->_strong_wolfe(obj_func, x_init, t, d, loss, flat_grad, gtd)
torch.optim.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.LBFGS._add_grad(self,step_size,update)
torch.optim.LBFGS._clone_param(self)
torch.optim.LBFGS._directional_evaluate(self,closure,x,t,d)
torch.optim.LBFGS._gather_flat_grad(self)
torch.optim.LBFGS._numel(self)
torch.optim.LBFGS._set_param(self,params_data)
torch.optim.LBFGS.step(self,closure)
torch.optim.lbfgs.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS.__init__(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS._add_grad(self,step_size,update)
torch.optim.lbfgs.LBFGS._clone_param(self)
torch.optim.lbfgs.LBFGS._directional_evaluate(self,closure,x,t,d)
torch.optim.lbfgs.LBFGS._gather_flat_grad(self)
torch.optim.lbfgs.LBFGS._numel(self)
torch.optim.lbfgs.LBFGS._set_param(self,params_data)
torch.optim.lbfgs.LBFGS.step(self,closure)
torch.optim.lbfgs._cubic_interpolate(x1,f1,g1,x2,f2,g2,bounds=None)
torch.optim.lbfgs._strong_wolfe(obj_func,x,t,d,f,g,gtd,c1=0.0001,c2=0.9,tolerance_change=1e-09,max_ls=25)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/lbfgs.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adadelta.py----------------------------------------
A:torch.optim.adadelta.defaults->dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)
A:torch.optim.adadelta.loss->closure()
A:torch.optim.adadelta.state['square_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adadelta.state['acc_delta']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.Adadelta.step(self,closure=None)
torch.optim.adadelta.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.__init__(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adadelta.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_functional.py----------------------------------------
A:torch.optim._functional.size->grad.coalesce().size()
A:torch.optim._functional.grad->grad.coalesce().coalesce()
A:torch.optim._functional.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim._functional.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim._functional.std->torch.view_as_real(square_avg).add(eps).sqrt_()
A:torch.optim._functional.std_values->torch.view_as_real(square_avg).add(eps).sqrt_()._values().sqrt_().add_(eps)
A:torch.optim._functional.is_complex->torch.is_complex(param)
A:torch.optim._functional.state_sum->torch.view_as_complex(state_sum)
A:torch.optim._functional.param->torch.view_as_complex(param)
A:torch.optim._functional.denom->grad_values.pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2).sqrt_().add_(eps)
A:torch.optim._functional.d_p->d_p.add(buf, alpha=momentum).add(buf, alpha=momentum)
A:torch.optim._functional.buf->torch.clone(d_p).detach()
A:torch.optim._functional.square_avg->torch.view_as_real(square_avg)
A:torch.optim._functional.acc_delta->torch.view_as_real(acc_delta)
A:torch.optim._functional.delta->torch.view_as_complex(delta)
A:torch.optim._functional.avg->torch.view_as_real(square_avg).sqrt().add_(eps)
A:torch.optim._functional.sign->grad.coalesce().coalesce().mul(prev).sign()
A:torch.optim._functional.norm_buf->torch.cat([exp_inf.mul_(beta2).unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)
A:torch.optim._functional.rect->math.sqrt((rho_t - 4) * (rho_t - 2) * rho_inf / ((rho_inf - 4) * (rho_inf - 2) * rho_t))
A:torch.optim._functional.old_exp_avg_values->exp_avg.sparse_mask(grad)._values()
A:torch.optim._functional.exp_avg_update_values->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1)
A:torch.optim._functional.old_exp_avg_sq_values->exp_avg_sq.sparse_mask(grad)._values()
A:torch.optim._functional.exp_avg_sq_update_values->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)
A:torch.optim._functional.numer->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1).add_(old_exp_avg_values)
torch.optim._functional._make_sparse(grad,grad_indices,values)
torch.optim._functional.adadelta(params:List[Tensor],grads:List[Tensor],square_avgs:List[Tensor],acc_deltas:List[Tensor],*,lr:float,rho:float,eps:float,weight_decay:float)
torch.optim._functional.adagrad(params:List[Tensor],grads:List[Tensor],state_sums:List[Tensor],state_steps:List[int],*,lr:float,weight_decay:float,lr_decay:float,eps:float)
torch.optim._functional.adam(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_avg_sqs:List[Tensor],max_exp_avg_sqs:List[Tensor],state_steps:List[int],*,amsgrad:bool,beta1:float,beta2:float,lr:float,weight_decay:float,eps:float,maximize:bool)
torch.optim._functional.adamax(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_infs:List[Tensor],state_steps:List[int],*,eps:float,beta1:float,beta2:float,lr:float,weight_decay:float)
torch.optim._functional.adamw(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_avg_sqs:List[Tensor],max_exp_avg_sqs:List[Tensor],state_steps:List[int],*,amsgrad:bool,beta1:float,beta2:float,lr:float,weight_decay:float,eps:float,maximize:bool)
torch.optim._functional.asgd(params:List[Tensor],grads:List[Tensor],axs:List[Tensor],mus:List[float],etas:List[float],*,weight_decay:float,lambd:float)
torch.optim._functional.nadam(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_avg_sqs:List[Tensor],mu_products:List[float],state_steps:List[int],*,beta1:float,beta2:float,lr:float,weight_decay:float,momentum_decay:float,eps:float)
torch.optim._functional.radam(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_avg_sqs:List[Tensor],state_steps:List[int],*,beta1:float,beta2:float,lr:float,weight_decay:float,eps:float)
torch.optim._functional.rmsprop(params:List[Tensor],grads:List[Tensor],square_avgs:List[Tensor],grad_avgs:List[Tensor],momentum_buffer_list:List[Tensor],*,lr:float,alpha:float,eps:float,weight_decay:float,momentum:float,centered:bool)
torch.optim._functional.rprop(params:List[Tensor],grads:List[Tensor],prevs:List[Tensor],step_sizes:List[Tensor],*,step_size_min:float,step_size_max:float,etaminus:float,etaplus:float)
torch.optim._functional.sgd(params:List[Tensor],d_p_list:List[Tensor],momentum_buffer_list:List[Optional[Tensor]],*,weight_decay:float,momentum:float,lr:float,dampening:float,nesterov:bool,maximize:bool)
torch.optim._functional.sparse_adam(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_avg_sqs:List[Tensor],state_steps:List[int],*,eps:float,beta1:float,beta2:float,lr:float)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adamax.py----------------------------------------
A:torch.optim.adamax.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
A:torch.optim.adamax.loss->closure()
A:torch.optim.adamax.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamax.state['exp_inf']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.Adamax.step(self,closure=None)
torch.optim.adamax.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adamax.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/rmsprop.py----------------------------------------
A:torch.optim.rmsprop.defaults->dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)
A:torch.optim.rmsprop.loss->closure()
A:torch.optim.rmsprop.state['square_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rmsprop.state['momentum_buffer']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.rmsprop.state['grad_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.RMSprop.__setstate__(self,state)
torch.optim.RMSprop.step(self,closure=None)
torch.optim.rmsprop.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__init__(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__setstate__(self,state)
torch.optim.rmsprop.RMSprop.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/rmsprop.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/nadam.py----------------------------------------
A:torch.optim.nadam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, momentum_decay=momentum_decay)
A:torch.optim.nadam.loss->closure()
A:torch.optim.nadam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.nadam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.NAdam(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0,momentum_decay=0.004)
torch.optim.NAdam.step(self,closure=None)
torch.optim.nadam.NAdam(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0,momentum_decay=0.004)
torch.optim.nadam.NAdam.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0,momentum_decay=0.004)
torch.optim.nadam.NAdam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/nadam.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/asgd.py----------------------------------------
A:torch.optim.asgd.defaults->dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0, weight_decay=weight_decay)
A:torch.optim.asgd.loss->closure()
A:torch.optim.asgd.state['ax']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.ASGD.step(self,closure=None)
torch.optim.asgd.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.__init__(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/asgd.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adagrad.py----------------------------------------
A:torch.optim.adagrad.defaults->dict(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay, initial_accumulator_value=initial_accumulator_value)
A:torch.optim.adagrad.state['sum']->torch.full_like(p, init_value, memory_format=torch.preserve_format)
A:torch.optim.adagrad.loss->closure()
torch.optim.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.Adagrad.share_memory(self)
torch.optim.Adagrad.step(self,closure=None)
torch.optim.adagrad.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.adagrad.Adagrad.__init__(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.adagrad.Adagrad.share_memory(self)
torch.optim.adagrad.Adagrad.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adagrad.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/__init__.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adamw.py----------------------------------------
A:torch.optim.adamw.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, maximize=maximize)
A:torch.optim.adamw.loss->closure()
A:torch.optim.adamw.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamw.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adamw.state['max_exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False,*,maximize:bool=False)
torch.optim.AdamW.__setstate__(self,state)
torch.optim.AdamW.step(self,closure=None)
torch.optim.adamw.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False,*,maximize:bool=False)
torch.optim.adamw.AdamW.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False,*,maximize:bool=False)
torch.optim.adamw.AdamW.__setstate__(self,state)
torch.optim.adamw.AdamW.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adamw.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/lr_scheduler.py----------------------------------------
A:torch.optim.lr_scheduler.instance_ref->weakref.ref(method.__self__)
A:torch.optim.lr_scheduler.instance->instance_ref()
A:torch.optim.lr_scheduler.wrapped->func.__get__(instance, cls)
A:torch.optim.lr_scheduler.self.optimizer.step->with_counter(self.optimizer.step)
A:torch.optim.lr_scheduler.values->self.get_lr()
A:torch.optim.lr_scheduler.self.lr_lambdas->list(lr_lambda)
A:torch.optim.lr_scheduler.state_dict['lr_lambdas'][idx]->fn.__dict__.copy()
A:torch.optim.lr_scheduler.lr_lambdas->state_dict.pop('lr_lambdas')
A:torch.optim.lr_scheduler.self.milestones->Counter(milestones)
A:torch.optim.lr_scheduler.milestones->list(sorted(self.milestones.elements()))
A:torch.optim.lr_scheduler.self._last_lr->self._schedulers[idx].get_last_lr()
A:torch.optim.lr_scheduler.idx->bisect_right(self._milestones, self.last_epoch)
A:torch.optim.lr_scheduler.state_dict['_schedulers'][idx]->s.state_dict()
A:torch.optim.lr_scheduler._schedulers->state_dict.pop('_schedulers')
A:torch.optim.lr_scheduler.self._schedulers->list(schedulers)
A:torch.optim.lr_scheduler.self.min_lrs->list(min_lr)
A:torch.optim.lr_scheduler.current->float(metrics)
A:torch.optim.lr_scheduler.old_lr->float(param_group['lr'])
A:torch.optim.lr_scheduler.new_lr->max(old_lr * self.factor, self.min_lrs[i])
A:torch.optim.lr_scheduler.base_lrs->self._format_param('base_lr', optimizer, base_lr)
A:torch.optim.lr_scheduler.self.max_lrs->self._format_param('max_lr', optimizer, max_lr)
A:torch.optim.lr_scheduler.step_size_up->float(step_size_up)
A:torch.optim.lr_scheduler.base_momentums->self._format_param('base_momentum', optimizer, base_momentum)
A:torch.optim.lr_scheduler.self.max_momentums->self._format_param('max_momentum', optimizer, max_momentum)
A:torch.optim.lr_scheduler.cycle->math.floor(1 + self.last_epoch / self.total_size)
A:torch.optim.lr_scheduler.n->int(math.log(epoch / self.T_0 * (self.T_mult - 1) + 1, self.T_mult))
A:torch.optim.lr_scheduler.self.last_epoch->math.floor(epoch)
A:torch.optim.lr_scheduler.max_lrs->self._format_param('max_lr', self.optimizer, max_lr)
A:torch.optim.lr_scheduler.max_momentums->self._format_param('max_momentum', optimizer, max_momentum)
A:torch.optim.lr_scheduler.computed_lr->self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)
A:torch.optim.lr_scheduler.computed_momentum->self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)
torch.optim.lr_scheduler.ChainedScheduler(self,schedulers)
torch.optim.lr_scheduler.ChainedScheduler.__init__(self,schedulers)
torch.optim.lr_scheduler.ChainedScheduler.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.ChainedScheduler.state_dict(self)
torch.optim.lr_scheduler.ChainedScheduler.step(self)
torch.optim.lr_scheduler.ConstantLR(self,optimizer,factor=1.0/3,total_iters=5,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.ConstantLR.__init__(self,optimizer,factor=1.0/3,total_iters=5,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.ConstantLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.ConstantLR.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingLR(self,optimizer,T_max,eta_min=0,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.CosineAnnealingLR.__init__(self,optimizer,T_max,eta_min=0,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.CosineAnnealingLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.CosineAnnealingLR.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self,optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.__init__(self,optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step(self,epoch=None)
torch.optim.lr_scheduler.CyclicLR(self,optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.CyclicLR.__init__(self,optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.CyclicLR._exp_range_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR._format_param(self,name,optimizer,param)
torch.optim.lr_scheduler.CyclicLR._triangular2_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR._triangular_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR.get_lr(self)
torch.optim.lr_scheduler.ExponentialLR(self,optimizer,gamma,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.ExponentialLR.__init__(self,optimizer,gamma,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.ExponentialLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.ExponentialLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR(self,optimizer,lr_lambda,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.LambdaLR.__init__(self,optimizer,lr_lambda,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.LambdaLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.LambdaLR.state_dict(self)
torch.optim.lr_scheduler.LinearLR(self,optimizer,start_factor=1.0/3,end_factor=1.0,total_iters=5,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.LinearLR.__init__(self,optimizer,start_factor=1.0/3,end_factor=1.0,total_iters=5,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.LinearLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.LinearLR.get_lr(self)
torch.optim.lr_scheduler.MultiStepLR(self,optimizer,milestones,gamma=0.1,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.MultiStepLR.__init__(self,optimizer,milestones,gamma=0.1,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.MultiStepLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.MultiStepLR.get_lr(self)
torch.optim.lr_scheduler.MultiplicativeLR(self,optimizer,lr_lambda,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.MultiplicativeLR.__init__(self,optimizer,lr_lambda,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.MultiplicativeLR.get_lr(self)
torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.MultiplicativeLR.state_dict(self)
torch.optim.lr_scheduler.OneCycleLR(self,optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,three_phase=False,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.OneCycleLR.__init__(self,optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,three_phase=False,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.OneCycleLR._annealing_cos(self,start,end,pct)
torch.optim.lr_scheduler.OneCycleLR._annealing_linear(self,start,end,pct)
torch.optim.lr_scheduler.OneCycleLR._format_param(self,name,optimizer,param)
torch.optim.lr_scheduler.OneCycleLR.get_lr(self)
torch.optim.lr_scheduler.ReduceLROnPlateau(self,optimizer,mode='min',factor=0.1,patience=10,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08,verbose=False)
torch.optim.lr_scheduler.ReduceLROnPlateau.__init__(self,optimizer,mode='min',factor=0.1,patience=10,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08,verbose=False)
torch.optim.lr_scheduler.ReduceLROnPlateau._init_is_better(self,mode,threshold,threshold_mode)
torch.optim.lr_scheduler.ReduceLROnPlateau._reduce_lr(self,epoch)
torch.optim.lr_scheduler.ReduceLROnPlateau._reset(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.in_cooldown(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.is_better(self,a,best)
torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.ReduceLROnPlateau.state_dict(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.step(self,metrics,epoch=None)
torch.optim.lr_scheduler.SequentialLR(self,optimizer,schedulers,milestones,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.SequentialLR.__init__(self,optimizer,schedulers,milestones,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.SequentialLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.SequentialLR.state_dict(self)
torch.optim.lr_scheduler.SequentialLR.step(self)
torch.optim.lr_scheduler.StepLR(self,optimizer,step_size,gamma=0.1,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.StepLR.__init__(self,optimizer,step_size,gamma=0.1,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler.StepLR._get_closed_form_lr(self)
torch.optim.lr_scheduler.StepLR.get_lr(self)
torch.optim.lr_scheduler._LRScheduler(self,optimizer,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler._LRScheduler.__init__(self,optimizer,last_epoch=-1,verbose=False)
torch.optim.lr_scheduler._LRScheduler.get_last_lr(self)
torch.optim.lr_scheduler._LRScheduler.get_lr(self)
torch.optim.lr_scheduler._LRScheduler.load_state_dict(self,state_dict)
torch.optim.lr_scheduler._LRScheduler.print_lr(self,is_verbose,group,lr,epoch=None)
torch.optim.lr_scheduler._LRScheduler.state_dict(self)
torch.optim.lr_scheduler._LRScheduler.step(self,epoch=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/lr_scheduler.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/radam.py----------------------------------------
A:torch.optim.radam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
A:torch.optim.radam.loss->closure()
A:torch.optim.radam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.radam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.RAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.RAdam.step(self,closure=None)
torch.optim.radam.RAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.radam.RAdam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.radam.RAdam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/radam.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/sparse_adam.py----------------------------------------
A:torch.optim.sparse_adam.params->list(params)
A:torch.optim.sparse_adam.defaults->dict(lr=lr, betas=betas, eps=eps)
A:torch.optim.sparse_adam.loss->closure()
A:torch.optim.sparse_adam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.sparse_adam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.SparseAdam.step(self,closure=None)
torch.optim.sparse_adam.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/sparse_adam.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/optimizer.py----------------------------------------
A:torch.optim.optimizer.required->_RequiredParameter()
A:torch.optim.optimizer.self.state->defaultdict(dict)
A:torch.optim.optimizer.param_groups->list(params)
A:torch.optim.optimizer.self._zero_grad_profile_name->'Optimizer.zero_grad#{}.zero_grad'.format(self.__class__.__name__)
A:torch.optim.optimizer.profile_name->'Optimizer.step#{}.step'.format(obj.__class__.__name__)
A:torch.optim.optimizer.hooked->getattr(self.__class__.step, 'hooked', None)
A:torch.optim.optimizer.self.__class__.step->profile_hook_step(self.__class__.step)
A:torch.optim.optimizer.state_dict->deepcopy(state_dict)
A:torch.optim.optimizer.value->value.to(param.device).to(param.device)
A:torch.optim.optimizer.state->defaultdict(dict)
A:torch.optim.optimizer.state[param]->cast(param, v)
A:torch.optim.optimizer.foreach->self.defaults.get('foreach', False)
A:torch.optim.optimizer.per_device_and_dtype_grads->defaultdict(lambda : defaultdict(list))
A:torch.optim.optimizer.param_group['params']->list(params)
A:torch.optim.optimizer.param_set->set()
torch.optim.Optimizer(self,params,defaults)
torch.optim.Optimizer.__getstate__(self)
torch.optim.Optimizer.__repr__(self)
torch.optim.Optimizer.__setstate__(self,state)
torch.optim.Optimizer._hook_for_profile(self)
torch.optim.Optimizer.add_param_group(self,param_group)
torch.optim.Optimizer.load_state_dict(self,state_dict)
torch.optim.Optimizer.state_dict(self)
torch.optim.Optimizer.step(self,closure)
torch.optim.Optimizer.zero_grad(self,set_to_none:bool=False)
torch.optim.optimizer.Optimizer(self,params,defaults)
torch.optim.optimizer.Optimizer.__getstate__(self)
torch.optim.optimizer.Optimizer.__init__(self,params,defaults)
torch.optim.optimizer.Optimizer.__repr__(self)
torch.optim.optimizer.Optimizer.__setstate__(self,state)
torch.optim.optimizer.Optimizer._hook_for_profile(self)
torch.optim.optimizer.Optimizer.add_param_group(self,param_group)
torch.optim.optimizer.Optimizer.load_state_dict(self,state_dict)
torch.optim.optimizer.Optimizer.state_dict(self)
torch.optim.optimizer.Optimizer.step(self,closure)
torch.optim.optimizer.Optimizer.zero_grad(self,set_to_none:bool=False)
torch.optim.optimizer._RequiredParameter(object)
torch.optim.optimizer._RequiredParameter.__repr__(self)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/optimizer.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adam.py----------------------------------------
A:torch.optim.adam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, maximize=maximize)
A:torch.optim.adam.loss->closure()
A:torch.optim.adam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim.adam.state['max_exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False,*,maximize:bool=False)
torch.optim.Adam.__setstate__(self,state)
torch.optim.Adam.step(self,closure=None)
torch.optim.adam.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False,*,maximize:bool=False)
torch.optim.adam.Adam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False,*,maximize:bool=False)
torch.optim.adam.Adam.__setstate__(self,state)
torch.optim.adam.Adam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/adam.pyi----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/rprop.py----------------------------------------
A:torch.optim._multi_tensor.rprop.defaults->dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=True)
A:torch.optim._multi_tensor.rprop.loss->closure()
A:torch.optim._multi_tensor.rprop.state['prev']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.rprop.state['step_size']->p.grad.new().resize_as_(p.grad).fill_(group['lr'])
A:torch.optim._multi_tensor.rprop.signs->torch._foreach_mul(grads, [s['prev'] for s in states])
A:torch.optim._multi_tensor.rprop.grads[i]->grads[i].clone(memory_format=torch.preserve_format).clone(memory_format=torch.preserve_format)
torch.optim._multi_tensor.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim._multi_tensor.Rprop.step(self,closure=None)
torch.optim._multi_tensor.rprop.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim._multi_tensor.rprop.Rprop.__init__(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim._multi_tensor.rprop.Rprop.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/adadelta.py----------------------------------------
A:torch.optim._multi_tensor.adadelta.defaults->dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay, foreach=True)
A:torch.optim._multi_tensor.adadelta.loss->closure()
A:torch.optim._multi_tensor.adadelta.state['square_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adadelta.state['acc_delta']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim._multi_tensor.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim._multi_tensor.Adadelta.step(self,closure=None)
torch.optim._multi_tensor.adadelta.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim._multi_tensor.adadelta.Adadelta.__init__(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim._multi_tensor.adadelta.Adadelta.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/_functional.py----------------------------------------
A:torch.optim._multi_tensor._functional.size->grad.coalesce().size()
A:torch.optim._multi_tensor._functional.grad->grad.coalesce().coalesce()
A:torch.optim._multi_tensor._functional.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim._multi_tensor._functional.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim._multi_tensor._functional.std_sparse->state_sum.sparse_mask(grad)
A:torch.optim._multi_tensor._functional.std_sparse_values->state_sum.sparse_mask(grad)._values().sqrt_().add_(eps)
A:torch.optim._multi_tensor._functional.std->torch._foreach_add(square_avgs, eps)
A:torch.optim._multi_tensor._functional.toAdd->torch._foreach_div(torch._foreach_mul(grads, minus_clr), std)
A:torch.optim._multi_tensor._functional.norm_buf->torch.cat([exp_inf.unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)
A:torch.optim._multi_tensor._functional.deltas->torch._foreach_add(acc_deltas, eps)
A:torch.optim._multi_tensor._functional.exp_avg_sq_sqrt->torch._foreach_sqrt(exp_avg_sq)
A:torch.optim._multi_tensor._functional.denom->torch._foreach_add(exp_avg_sq_sqrt, eps)
torch.optim._multi_tensor._functional._make_sparse(grad,grad_indices,values)
torch.optim._multi_tensor._functional.adadelta(params:List[Tensor],grads:List[Tensor],square_avgs:List[Tensor],acc_deltas:List[Tensor],*,lr:float,weight_decay:float,rho:float,eps:float)
torch.optim._multi_tensor._functional.adagrad(params:List[Tensor],grads:List[Tensor],state_sums:List[Tensor],state_steps:List[int],has_sparse_grad:bool,*,lr:float,weight_decay:float,lr_decay:float,eps:float)
torch.optim._multi_tensor._functional.adamax(params:List[Tensor],grads:List[Tensor],exp_avgs:List[Tensor],exp_infs:List[Tensor],states:List[Dict],*,beta1:float,beta2:float,lr:float,weight_decay:float,eps:float)
torch.optim._multi_tensor._functional.asgd(params:List[Tensor],grads:List[Tensor],states:List[Dict],lambd:float,lr:float,t0:float,alpha:float,weight_decay:float)
torch.optim._multi_tensor._functional.nadam(params:List[Tensor],grads:List[Tensor],exp_avg:List[Tensor],exp_avg_sq:List[Tensor],mu_products:List[Tensor],states:List[Dict],*,beta1:float,beta2:float,lr:float,weight_decay:float,momentum_decay:float,eps:float)
torch.optim._multi_tensor._functional.radam(params:List[Tensor],grads:List[Tensor],exp_avg:List[Tensor],exp_avg_sq:List[Tensor],states:List[Dict],*,beta1:float,beta2:float,lr:float,weight_decay:float,eps:float)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/adamax.py----------------------------------------
A:torch.optim._multi_tensor.adamax.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, foreach=True)
A:torch.optim._multi_tensor.adamax.loss->closure()
A:torch.optim._multi_tensor.adamax.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adamax.state['exp_inf']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim._multi_tensor.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim._multi_tensor.Adamax.step(self,closure=None)
torch.optim._multi_tensor.adamax.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim._multi_tensor.adamax.Adamax.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim._multi_tensor.adamax.Adamax.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/rmsprop.py----------------------------------------
A:torch.optim._multi_tensor.rmsprop.defaults->dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay, foreach=True)
A:torch.optim._multi_tensor.rmsprop.loss->closure()
A:torch.optim._multi_tensor.rmsprop.state['square_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.rmsprop.state['momentum_buffer']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.rmsprop.state['grad_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.rmsprop.avg->torch._foreach_sqrt(square_avg)
torch.optim._multi_tensor.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim._multi_tensor.RMSprop.__setstate__(self,state)
torch.optim._multi_tensor.RMSprop.step(self,closure=None)
torch.optim._multi_tensor.rmsprop.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim._multi_tensor.rmsprop.RMSprop.__init__(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim._multi_tensor.rmsprop.RMSprop.__setstate__(self,state)
torch.optim._multi_tensor.rmsprop.RMSprop.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/sgd.py----------------------------------------
A:torch.optim._multi_tensor.sgd.defaults->dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov, maximize=maximize, foreach=True)
A:torch.optim._multi_tensor.sgd.loss->closure()
A:torch.optim._multi_tensor.sgd.grads->torch._foreach_add(grads, params_with_grad, alpha=weight_decay)
A:torch.optim._multi_tensor.sgd.bufstates[i]['momentum_buffer']->torch.clone(grads[i]).detach()
torch.optim._multi_tensor.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False,*,maximize=False)
torch.optim._multi_tensor.SGD.__setstate__(self,state)
torch.optim._multi_tensor.SGD.step(self,closure=None)
torch.optim._multi_tensor.sgd.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False,*,maximize=False)
torch.optim._multi_tensor.sgd.SGD.__init__(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False,*,maximize=False)
torch.optim._multi_tensor.sgd.SGD.__setstate__(self,state)
torch.optim._multi_tensor.sgd.SGD.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/nadam.py----------------------------------------
A:torch.optim._multi_tensor.nadam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, momentum_decay=momentum_decay, foreach=True)
A:torch.optim._multi_tensor.nadam.loss->closure()
A:torch.optim._multi_tensor.nadam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.nadam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim._multi_tensor.NAdam(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0,momentum_decay=0.004)
torch.optim._multi_tensor.NAdam.step(self,closure=None)
torch.optim._multi_tensor.nadam.NAdam(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0,momentum_decay=0.004)
torch.optim._multi_tensor.nadam.NAdam.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0,momentum_decay=0.004)
torch.optim._multi_tensor.nadam.NAdam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/adagrad.py----------------------------------------
A:torch.optim._multi_tensor.adagrad.defaults->dict(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay, initial_accumulator_value=initial_accumulator_value, foreach=True)
A:torch.optim._multi_tensor.adagrad.state['sum']->torch.full_like(p, init_value, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adagrad.loss->closure()
torch.optim._multi_tensor.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim._multi_tensor.Adagrad.share_memory(self)
torch.optim._multi_tensor.Adagrad.step(self,closure=None)
torch.optim._multi_tensor.adagrad.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim._multi_tensor.adagrad.Adagrad.__init__(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim._multi_tensor.adagrad.Adagrad.share_memory(self)
torch.optim._multi_tensor.adagrad.Adagrad.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/adamw.py----------------------------------------
A:torch.optim._multi_tensor.adamw.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, maximize=maximize, foreach=True)
A:torch.optim._multi_tensor.adamw.loss->closure()
A:torch.optim._multi_tensor.adamw.grads->torch._foreach_neg(tuple(grads))
A:torch.optim._multi_tensor.adamw.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adamw.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adamw.state['max_exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adamw.max_exp_avg_sq->torch._foreach_maximum(max_exp_avg_sq, exp_avg_sq)
A:torch.optim._multi_tensor.adamw.max_exp_avg_sq_sqrt->torch._foreach_sqrt(max_exp_avg_sq)
A:torch.optim._multi_tensor.adamw.denom->torch._foreach_add(exp_avg_sq_sqrt, group['eps'])
A:torch.optim._multi_tensor.adamw.exp_avg_sq_sqrt->torch._foreach_sqrt(exp_avg_sq)
torch.optim._multi_tensor.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False,*,maximize:bool=False)
torch.optim._multi_tensor.AdamW.__setstate__(self,state)
torch.optim._multi_tensor.AdamW.step(self,closure=None)
torch.optim._multi_tensor.adamw.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False,*,maximize:bool=False)
torch.optim._multi_tensor.adamw.AdamW.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False,*,maximize:bool=False)
torch.optim._multi_tensor.adamw.AdamW.__setstate__(self,state)
torch.optim._multi_tensor.adamw.AdamW.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/radam.py----------------------------------------
A:torch.optim._multi_tensor.radam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, foreach=True)
A:torch.optim._multi_tensor.radam.loss->closure()
A:torch.optim._multi_tensor.radam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.radam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim._multi_tensor.RAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim._multi_tensor.RAdam.step(self,closure=None)
torch.optim._multi_tensor.radam.RAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim._multi_tensor.radam.RAdam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim._multi_tensor.radam.RAdam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/adam.py----------------------------------------
A:torch.optim._multi_tensor.adam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, maximize=maximize, foreach=True)
A:torch.optim._multi_tensor.adam.loss->closure()
A:torch.optim._multi_tensor.adam.grads->torch._foreach_add(grads, params_with_grad, alpha=group['weight_decay'])
A:torch.optim._multi_tensor.adam.state['exp_avg']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adam.state['exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adam.state['max_exp_avg_sq']->torch.zeros_like(p, memory_format=torch.preserve_format)
A:torch.optim._multi_tensor.adam.max_exp_avg_sq->torch._foreach_maximum(max_exp_avg_sq, exp_avg_sq)
A:torch.optim._multi_tensor.adam.max_exp_avg_sq_sqrt->torch._foreach_sqrt(max_exp_avg_sq)
A:torch.optim._multi_tensor.adam.denom->torch._foreach_add(exp_avg_sq_sqrt, group['eps'])
A:torch.optim._multi_tensor.adam.exp_avg_sq_sqrt->torch._foreach_sqrt(exp_avg_sq)
torch.optim._multi_tensor.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False,*,maximize:bool=False)
torch.optim._multi_tensor.Adam.__setstate__(self,state)
torch.optim._multi_tensor.Adam.step(self,closure=None)
torch.optim._multi_tensor.adam.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False,*,maximize:bool=False)
torch.optim._multi_tensor.adam.Adam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False,*,maximize:bool=False)
torch.optim._multi_tensor.adam.Adam.__setstate__(self,state)
torch.optim._multi_tensor.adam.Adam.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/optim/_multi_tensor/asgd.py----------------------------------------
A:torch.optim._multi_tensor.asgd.defaults->dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0, weight_decay=weight_decay, foreach=True)
A:torch.optim._multi_tensor.asgd.loss->closure()
A:torch.optim._multi_tensor.asgd.state['ax']->torch.zeros_like(p, memory_format=torch.preserve_format)
torch.optim._multi_tensor.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim._multi_tensor.ASGD.step(self,closure=None)
torch.optim._multi_tensor.asgd.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim._multi_tensor.asgd.ASGD.__init__(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim._multi_tensor.asgd.ASGD.step(self,closure=None)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cpu/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cpu/amp/autocast_mode.py----------------------------------------
torch.cpu.amp.autocast(self,enabled:bool=True,dtype:torch.dtype=torch.bfloat16,cache_enabled:bool=True)
torch.cpu.amp.autocast.__enter__(self)
torch.cpu.amp.autocast.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)
torch.cpu.amp.autocast_mode.autocast(self,enabled:bool=True,dtype:torch.dtype=torch.bfloat16,cache_enabled:bool=True)
torch.cpu.amp.autocast_mode.autocast.__enter__(self)
torch.cpu.amp.autocast_mode.autocast.__exit__(self,exc_type:Any,exc_val:Any,exc_tb:Any)
torch.cpu.amp.autocast_mode.autocast.__init__(self,enabled:bool=True,dtype:torch.dtype=torch.bfloat16,cache_enabled:bool=True)


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/cpu/amp/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/torch/torch1.11.0/_masked/__init__.py----------------------------------------
A:torch._masked.__init__.docstring_templates->dict(reduction_signature='{function_name}(input, {operation_args}, *, {operation_kwargs}) -> Tensor', reduction_descr='Returns {operation name} of all the elements in the :attr:`input`\ntensor along the given dimension(s) :attr:`dim` while the :attr:`input`\nelements are masked out according to the boolean tensor\n:attr:`mask`.', reduction_args='If :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of\nsize 1. Otherwise, :attr:`dim` is squeezed (see\n:func:`torch.squeeze`), resulting in the output tensor having 1 (or\n``len(dim)``) fewer dimension(s).\n\nThe boolean tensor :attr:`mask` defines the "validity" of\n:attr:`input` tensor elements: if :attr:`mask` element is True\nthen the corresponding element in :attr:`input` tensor will be\nincluded in {operation name} computation, otherwise the element is\nignored.\n\nWhen all elements of :attr:`input` along the given dimension\n:attr:`dim` are ignored (fully masked-out), the corresponding element\nof the output tensor will have undefined value: it may or may not\ncorrespond to the identity value of {operation name} operation; the\nchoice may correspond to the value that leads to the most efficient\nstorage of :attr:`output` tensor.\n\nThe mask of the output tensor can be computed as\n``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim,\ndtype=torch.bool)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon\'t need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    {args_declarations}\n\nKeyword args:\n    {kwargs_declarations}', reduction_example='Example::\n\n    >>> input = {example_input}\n    >>> input\n    {indent_example_input}\n    >>> mask = {example_mask}\n    >>> mask\n    {indent_example_mask}\n    >>> {full_function_name}(input, {example_args}, mask=mask)\n    {indent_example_output}\n', reduction_identity='The identity value of {operation name} operation, which is used to start the reduction, is ``{identity_int32}``.', reduction_identity_dtype='The identity value of {operation name} operation, which is used to start the\nreduction, depends on input dtype. For instance, for float32, uint8,\nand int32 dtypes, the identity values are ``{identity_float32}``, ``{identity_uint8}``, and ``{identity_int32}``, respectively.', normalization_signature='{function_name}(input, {operation_args}, *, {operation_kwargs}) -> Tensor', normalization_descr='Returns {operation name} of all the slices in the :attr:`input` tensor\nalong :attr:`dim` while the :attr:`input` elements are masked out\naccording to the boolean tensor :attr:`mask`.\n\n{definition}', normalization_args='The boolean tensor :attr:`mask` defines the "validity" of\n:attr:`input` tensor elements: if :attr:`mask` element is True then\nthe corresponding element in :attr:`input` tensor will be included in\n{operation name} computation, otherwise the element is ignored.\n\nThe values of masked-out elements of the output tensor have undefined\nvalue: it may or may not be set to zero or nan; the choice may correspond to\nthe value that leads to the most efficient storage of :attr:`output`\ntensor.\n\nThe mask of the {operation name} output tensor can be computed as\n``torch.broadcast_to(mask, input.shape)``.\n\nThe shapes of the :attr:`mask` tensor and the :attr:`input` tensor\ndon\'t need to match, but they must be :ref:`broadcastable\n<broadcasting-semantics>` and the dimensionality of the :attr:`mask`\ntensor must not be greater than of the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor\n    {args_declarations}\n\nKeyword args:\n    {kwargs_declarations}', normalization_example='Example::\n\n    >>> input = {example_input}\n    >>> input\n    {indent_example_input}\n    >>> mask = {example_mask}\n    >>> mask\n    {indent_example_mask}\n    >>> {full_function_name}(input, {example_args}, mask=mask)\n    {indent_example_output}\n')
A:torch._masked.__init__.args_and_kwargs->dict(sum=(('dim',), ('keepdim=False', 'dtype=None', 'mask=None')), prod=(('dim',), ('keepdim=False', 'dtype=None', 'mask=None')), amin=(('dim',), ('keepdim=False', 'dtype=None', 'mask=None')), amax=(('dim',), ('keepdim=False', 'dtype=None', 'mask=None')), mean=(('dim',), ('keepdim=False', 'dtype=None', 'mask=None')), norm=(('ord', 'dim'), ('keepdim=False', 'dtype=None', 'mask=None')), var=(('dim', 'unbiased'), ('keepdim=False', 'dtype=None', 'mask=None')), softmax=(('dim__as_int',), ('dtype=None', 'mask=None')), log_softmax=(('dim__as_int',), ('dtype=None', 'mask=None')), softmin=(('dim__as_int',), ('dtype=None', 'mask=None')), normalize=(('ord__required', 'dim__as_int'), ('eps=1e-12', 'dtype=None', 'mask=None')))
A:torch._masked.__init__.argument_declarations->dict(dim='dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n  Default: None that is equivalent to ``tuple(range(input.ndim))``.', dim__as_int='dim (int): the dimension along which {operation name} is computed.', ord='ord (int, float, optional): the order of vector norm. Default: 2.\n  See :func:`torch.linalg.vector_norm` for a list of supported norms.', ord__required='ord (int, float): the order of vector norm. Default: 2.\n  See :func:`torch.linalg.vector_norm` for a list of supported norms.', unbiased='unbiased (bool): when True, use Bessel’s correction, otherwise, compute\n  the uncorrected sample variance.', eps='eps (float, optional): small value to avoid division by zero. Default: {default}.', keepdim='keepdim (bool, optional): whether the output tensor has\n  :attr:`dim` retained or not. Default: {default}.', dtype='dtype (:class:`torch.dtype`, optional): the desired data type\n  of returned tensor.  If specified, the input tensor is\n  casted to :attr:`dtype` before the operation is\n  performed. Default: {default}.', mask='mask (:class:`torch.Tensor`, optional): the boolean tensor\n  containing the binary mask of validity of input tensor\n  elements.\n  Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.')
A:torch._masked.__init__.definitions->dict(softmax='Let ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Softmax of i-th element in ``x`` is\ndefined as ``exp(x[i])/sum(exp(x))``.', log_softmax='Let ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. LogSoftmax of i-th element in ``x`` is\ndefined as ``log(exp(x[i])/sum(exp(x)))``.', softmin='Let ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Softmin of i-th element in ``x`` is\ndefined as ``exp(-x[i])/sum(exp(-x))``.', normalize='Let ``x`` be a sequence of unmasked elements of one-dimensional slice\nof the :attr:`input` tensor. Normalize of i-th element in ``x`` is\ndefined as ``x[i]/max(norm(x, p), eps)``.')
A:torch._masked.__init__.reduction_names->dict(sum='sum', prod='product', amax='maximum', amin='minimum', mean='mean', norm='norm', var='variance')
A:torch._masked.__init__.normalization_names->dict(softmax='softmax', log_softmax='log_softmax', softmin='softmin', normalize='normalize')
A:torch._masked.__init__.operation_names->dict()
A:torch._masked.__init__.example_input->example_input.to(dtype=torch.float32).to(dtype=torch.float32)
A:torch._masked.__init__.example_mask->torch.tensor([[True, False, True], [False, False, False]])
A:torch._masked.__init__.example_output->func(example_input, *example_args, mask=example_mask)
A:torch._masked.__init__.templates->dict(((k, v.format_map(template_data)) for (k, v) in docstring_templates.items() if k.startswith(op_kind)))
A:torch._masked.__init__.doc_template->'\n\n'.join([f'{{{op_kind}_{sec}}}' for sec in doc_sections])
A:torch._masked.__init__.func.__doc__->'\n\n'.join([f'{{{op_kind}_{sec}}}' for sec in doc_sections]).format_map(templates)
A:torch._masked.__init__.ndim->max(ndim, 1)
A:torch._masked.__init__.mask->kwargs.get('mask')
A:torch._masked.__init__.inmask->_input_mask(input, mask=mask)
A:torch._masked.__init__.outmask->outmask.any(dim=d, keepdim=bool(keepdim)).any(dim=d, keepdim=bool(keepdim))
A:torch._masked.__init__.keepdim->kwargs.get('keepdim', False)
A:torch._masked.__init__.dim_->_canonical_dim(dim, input.ndim)
A:torch._masked.__init__.result->result.to(dtype=dtype).to(dtype=dtype)
A:torch._masked.__init__.identity->input.new_full([], _reduction_identity('norm', input, ord))
A:torch._masked.__init__.mask_input->torch.where(inmask, input, fill)
A:torch._masked.__init__.count->torch.maximum(count, count.new_zeros([]))
A:torch._masked.__init__.total->sum(x * x.conj(), dim, keepdim=keepdim, dtype=compute_dtype, mask=inmask)
A:torch._masked.__init__.sample_total->sum(input, dim, keepdim=True, dtype=dtype, mask=inmask)
A:torch._masked.__init__.sample_mean->torch.divide(sample_total, count)
A:torch._masked.__init__.x->torch.subtract(input, sample_mean)
A:torch._masked.__init__.fill->input.new_full([], _reduction_identity('amin', input))
A:torch._masked.__init__.nrm_->norm(input, ord, dim, keepdim=True, dtype=dtype, mask=mask)
A:torch._masked.__init__.denom->torch.maximum(nrm_, nrm_.new_full([], eps))
torch._masked.__init__._apply_docstring_templates(func)
torch._masked.__init__._canonical_dim(dim:DimOrDims,ndim:int)->Tuple[int, ...]
torch._masked.__init__._input_mask(input:Tensor,*args,**kwargs)->Tensor
torch._masked.__init__._output_mask(op,input:Tensor,*args,**kwargs)->Tensor
torch._masked.__init__._reduction_identity(op_name:str,input:Tensor,*args)
torch._masked.__init__.amax(input:Tensor,dim:DimOrDims=None,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.amin(input:Tensor,dim:DimOrDims=None,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.log_softmax(input:Tensor,dim:int,*,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.mean(input:Tensor,dim:DimOrDims=None,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.norm(input:Tensor,ord:Optional[float]=2.0,dim:DimOrDims=None,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.normalize(input:Tensor,ord:float,dim:int,*,eps:float=1e-12,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.prod(input:Tensor,dim:DimOrDims=None,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.softmax(input:Tensor,dim:int,*,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.softmin(input:Tensor,dim:int,*,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.sum(input:Tensor,dim:DimOrDims=None,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor
torch._masked.__init__.var(input:Tensor,dim:DimOrDims=None,unbiased:Optional[bool]=False,*,keepdim:Optional[bool]=False,dtype:Optional[DType]=None,mask:Optional[Tensor]=None)->Tensor

