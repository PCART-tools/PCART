
----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/functional.py----------------------------------------
A:torch.functional.sz->LU_data.size(-1)
A:torch.functional.U->LU_data.triu()
A:torch.functional.L->LU_data.tril()
A:torch.functional.P->P.index_select(1, torch.as_tensor(final_order, device=LU_pivots.device)).index_select(1, torch.as_tensor(final_order, device=LU_pivots.device))
A:torch.functional.final_order->list(range(sz))
A:torch.functional.P[idx]->P[idx].index_select(1, torch.as_tensor(final_order, device=LU_pivots.device)).index_select(1, torch.as_tensor(final_order, device=LU_pivots.device))
A:torch.functional.signal_dim->input.view(input.shape[-signal_dim:]).dim()
A:torch.functional.pad->int(n_fft // 2)
A:torch.functional.input->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:])
A:torch.functional.(output, inverse_indices, counts)->torch._C._VariableFunctions.unique_consecutive(input, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
A:torch.functional.dims->dims.item().item()
A:torch.functional.dims_a->list(range(-dims, 0))
A:torch.functional.dims_b->list(range(dims))
A:torch.functional.ndim->input.view(input.shape[-signal_dim:]).view(input.shape[-signal_dim:]).dim()
A:torch.functional.dim->tuple(range(ndim))
A:torch.functional.result->torch._lu_with_info(A, pivot=pivot, check_errors=not get_infos)
torch.align_tensors(*tensors)
torch.broadcast_tensors(*tensors)
torch.cartesian_prod(*tensors)
torch.chain_matmul(*matrices)
torch.einsum(equation,*operands)
torch.functional.align_tensors(*tensors)
torch.functional.broadcast_tensors(*tensors)
torch.functional.cartesian_prod(*tensors)
torch.functional.chain_matmul(*matrices)
torch.functional.einsum(equation,*operands)
torch.functional.isfinite(tensor)
torch.functional.isinf(tensor)
torch.functional.lu(A,pivot=True,get_infos=False,out=None)
torch.functional.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)
torch.functional.meshgrid(*tensors,**kwargs)
torch.functional.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)
torch.functional.split(tensor,split_size_or_sections,dim=0)
torch.functional.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.functional.tensordot(a,b,dims=2)
torch.functional.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.functional.unique_consecutive(input,return_inverse=False,return_counts=False,dim=None)
torch.isfinite(tensor)
torch.isinf(tensor)
torch.lu(A,pivot=True,get_infos=False,out=None)
torch.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)
torch.meshgrid(*tensors,**kwargs)
torch.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)
torch.split(tensor,split_size_or_sections,dim=0)
torch.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.tensordot(a,b,dims=2)
torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.unique_consecutive(input,return_inverse=False,return_counts=False,dim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/__init__.py----------------------------------------
A:torch.__init__.NVTOOLEXT_HOME->torch._dl.getenv('NVTOOLSEXT_PATH', 'C:\\Program Files\\NVIDIA Corporation\\NvToolsExt')
A:torch.__init__.py_dll_path->torch._dl.path.join(sys.exec_prefix, 'Library', 'bin')
A:torch.__init__.th_dll_path->torch._dl.path.join(_dl_flags.path.dirname(__file__), 'lib')
A:torch.__init__._dl_flags.environ['PATH']->';'.join(dll_paths)
A:torch.__init__.old_flags->sys.getdlopenflags()
A:torch.__init__.t->_import_dotted_name(t)
A:torch.__init__._tensor_classes->set()
A:torch.__init__.path->get_file_path('torch', 'bin', 'torch_shm_manager')
A:torch.__init__.globals()[name]->getattr(_C._VariableFunctions, name)
torch.__init__.BFloat16Storage(_C.BFloat16StorageBase,_StorageBase)
torch.__init__.BoolStorage(_C.BoolStorageBase,_StorageBase)
torch.__init__.ByteStorage(_C.ByteStorageBase,_StorageBase)
torch.__init__.CharStorage(_C.CharStorageBase,_StorageBase)
torch.__init__.DoubleStorage(_C.DoubleStorageBase,_StorageBase)
torch.__init__.FloatStorage(_C.FloatStorageBase,_StorageBase)
torch.__init__.HalfStorage(_C.HalfStorageBase,_StorageBase)
torch.__init__.IntStorage(_C.IntStorageBase,_StorageBase)
torch.__init__.LongStorage(_C.LongStorageBase,_StorageBase)
torch.__init__.QInt32Storage(_C.QInt32StorageBase,_StorageBase)
torch.__init__.QInt8Storage(_C.QInt8StorageBase,_StorageBase)
torch.__init__.QUInt8Storage(_C.QUInt8StorageBase,_StorageBase)
torch.__init__.ShortStorage(_C.ShortStorageBase,_StorageBase)
torch.__init__.compiled_with_cxx11_abi()
torch.__init__.is_storage(obj)
torch.__init__.is_tensor(obj)
torch.__init__.manager_path()
torch.__init__.set_default_dtype(d)
torch.__init__.set_default_tensor_type(t)
torch.__init__.typename(o)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/__init__.pyi----------------------------------------
torch.__init__.BoolTensor(Tensor)
torch.__init__.ByteTensor(Tensor)
torch.__init__.CharTensor(Tensor)
torch.__init__.DoubleTensor(Tensor)
torch.__init__.FloatTensor(Tensor)
torch.__init__.Generator(self,device:Union[_int,str])
torch.__init__.Generator.__init__(self,device:Union[_int,str])
torch.__init__.IntTensor(Tensor)
torch.__init__.LongTensor(Tensor)
torch.__init__.ShortTensor(Tensor)
torch.__init__.Size(tuple)
torch.__init__.Storage
torch.__init__.Tensor
torch.__init__.Tensor.__abs__(self)->Tensor
torch.__init__.Tensor.__add__(self,other:Any)->Tensor
torch.__init__.Tensor.__and__(self,other:Any)->Tensor
torch.__init__.Tensor.__and__(self,other:Number)->Tensor
torch.__init__.Tensor.__and__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__bool__(self)->builtins.bool
torch.__init__.Tensor.__div__(self,other:Any)->Tensor
torch.__init__.Tensor.__eq__(self,other:Any)->Tensor
torch.__init__.Tensor.__float__(self)->builtins.float
torch.__init__.Tensor.__floordiv__(self,other:Any)->Tensor
torch.__init__.Tensor.__ge__(self,other:Any)->Tensor
torch.__init__.Tensor.__getitem__(self,indices:Union[None,_int,slice,Tensor,List,Tuple])->Tensor
torch.__init__.Tensor.__gt__(self,other:Any)->Tensor
torch.__init__.Tensor.__iadd__(self,other:Any)->Tensor
torch.__init__.Tensor.__iand__(self,other:Any)->Tensor
torch.__init__.Tensor.__iand__(self,other:Number)->Tensor
torch.__init__.Tensor.__iand__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__idiv__(self,other:Any)->Tensor
torch.__init__.Tensor.__ilshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__ilshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__ilshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__imul__(self,other:Any)->Tensor
torch.__init__.Tensor.__index__(self)->builtins.int
torch.__init__.Tensor.__int__(self)->builtins.int
torch.__init__.Tensor.__invert__(self)->Tensor
torch.__init__.Tensor.__ior__(self,other:Any)->Tensor
torch.__init__.Tensor.__ior__(self,other:Number)->Tensor
torch.__init__.Tensor.__ior__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__irshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__irshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__irshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__isub__(self,other:Any)->Tensor
torch.__init__.Tensor.__itruediv__(self,other:Any)->Tensor
torch.__init__.Tensor.__ixor__(self,other:Any)->Tensor
torch.__init__.Tensor.__ixor__(self,other:Number)->Tensor
torch.__init__.Tensor.__ixor__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__le__(self,other:Any)->Tensor
torch.__init__.Tensor.__long__(self)->builtins.int
torch.__init__.Tensor.__lshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__lshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__lshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__lt__(self,other:Any)->Tensor
torch.__init__.Tensor.__matmul__(self,other:Any)->Tensor
torch.__init__.Tensor.__mod__(self,other:Any)->Tensor
torch.__init__.Tensor.__mul__(self,other:Any)->Tensor
torch.__init__.Tensor.__ne__(self,other:Any)->Tensor
torch.__init__.Tensor.__neg__(self)->Tensor
torch.__init__.Tensor.__nonzero__(self)->builtins.bool
torch.__init__.Tensor.__or__(self,other:Any)->Tensor
torch.__init__.Tensor.__or__(self,other:Number)->Tensor
torch.__init__.Tensor.__or__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__pow__(self,other:Any)->Tensor
torch.__init__.Tensor.__radd__(self,other:Any)->Tensor
torch.__init__.Tensor.__rfloordiv__(self,other:Any)->Tensor
torch.__init__.Tensor.__rmul__(self,other:Any)->Tensor
torch.__init__.Tensor.__rshift__(self,other:Any)->Tensor
torch.__init__.Tensor.__rshift__(self,other:Number)->Tensor
torch.__init__.Tensor.__rshift__(self,other:Tensor)->Tensor
torch.__init__.Tensor.__setitem__(self,indices:Union[None,_int,slice,Tensor,List,Tuple],val:Union[Tensor,Number])->None
torch.__init__.Tensor.__sub__(self,other:Any)->Tensor
torch.__init__.Tensor.__truediv__(self,other:Any)->Tensor
torch.__init__.Tensor.__xor__(self,other:Any)->Tensor
torch.__init__.Tensor.__xor__(self,other:Number)->Tensor
torch.__init__.Tensor.__xor__(self,other:Tensor)->Tensor
torch.__init__.Tensor._coalesced_(self,coalesced:_bool)->Tensor
torch.__init__.Tensor._dimI(self)->_int
torch.__init__.Tensor._dimV(self)->_int
torch.__init__.Tensor._indices(self)->Tensor
torch.__init__.Tensor._nnz(self)->_int
torch.__init__.Tensor._values(self)->Tensor
torch.__init__.Tensor.abs(self)->Tensor
torch.__init__.Tensor.abs_(self)->Tensor
torch.__init__.Tensor.acos(self)->Tensor
torch.__init__.Tensor.acos_(self)->Tensor
torch.__init__.Tensor.addbmm(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addbmm_(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addcdiv(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addcdiv_(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addcmul(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addcmul_(self,tensor1:Tensor,tensor2:Tensor,*,value:Number=1)->Tensor
torch.__init__.Tensor.addmm(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addmm_(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addmv(self,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addmv_(self,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addr(self,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.addr_(self,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.align_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.align_to(self,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.all(self)->Tensor
torch.__init__.Tensor.all(self,dim:Union[str,None],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.all(self,dim:_int,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.allclose(self,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->_bool
torch.__init__.Tensor.any(self)->Tensor
torch.__init__.Tensor.any(self,dim:Union[str,None],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.any(self,dim:_int,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.apply_(self,callable:Callable)->Tensor
torch.__init__.Tensor.argmax(self,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.argmin(self,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.argsort(self,dim:Union[str,None],descending:_bool=False)->Tensor
torch.__init__.Tensor.argsort(self,dim:_int=-1,descending:_bool=False)->Tensor
torch.__init__.Tensor.as_strided(self,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.Tensor.as_strided_(self,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.Tensor.asin(self)->Tensor
torch.__init__.Tensor.asin_(self)->Tensor
torch.__init__.Tensor.atan(self)->Tensor
torch.__init__.Tensor.atan2(self,other:Tensor)->Tensor
torch.__init__.Tensor.atan2_(self,other:Tensor)->Tensor
torch.__init__.Tensor.atan_(self)->Tensor
torch.__init__.Tensor.backward(self,gradient:Optional[Tensor]=None,keep_graph:_bool=False,create_graph:_bool=False)->None
torch.__init__.Tensor.baddbmm(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.baddbmm_(self,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.bernoulli(self,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bernoulli(self,p:_float,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bernoulli_(self,p:Tensor,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bernoulli_(self,p:_float=0.5,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.bincount(self,weights:Optional[Tensor]=None,minlength:_int=0)->Tensor
torch.__init__.Tensor.bitwise_not(self)->Tensor
torch.__init__.Tensor.bitwise_not_(self)->Tensor
torch.__init__.Tensor.bmm(self,mat2:Tensor)->Tensor
torch.__init__.Tensor.bool(self)->Tensor
torch.__init__.Tensor.byte(self)->Tensor
torch.__init__.Tensor.cauchy_(self,median:_float=0,sigma:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.ceil(self)->Tensor
torch.__init__.Tensor.ceil_(self)->Tensor
torch.__init__.Tensor.char(self)->Tensor
torch.__init__.Tensor.cholesky(self,upper:_bool=False)->Tensor
torch.__init__.Tensor.cholesky_inverse(self,upper:_bool=False)->Tensor
torch.__init__.Tensor.cholesky_solve(self,input2:Tensor,upper:_bool=False)->Tensor
torch.__init__.Tensor.chunk(self,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.clamp(self,min:_float=-inf,max:_float=inf,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.clamp_(self,min:_float=-inf,max:_float=inf)->Tensor
torch.__init__.Tensor.clamp_max(self,max:Number)->Tensor
torch.__init__.Tensor.clamp_max_(self,max:Number)->Tensor
torch.__init__.Tensor.clamp_min(self,min:Number)->Tensor
torch.__init__.Tensor.clamp_min_(self,min:Number)->Tensor
torch.__init__.Tensor.clone(self)->Tensor
torch.__init__.Tensor.coalesce(self)->Tensor
torch.__init__.Tensor.contiguous(self)->Tensor
torch.__init__.Tensor.cos(self)->Tensor
torch.__init__.Tensor.cos_(self)->Tensor
torch.__init__.Tensor.cosh(self)->Tensor
torch.__init__.Tensor.cosh_(self)->Tensor
torch.__init__.Tensor.cpu(self)->Tensor
torch.__init__.Tensor.cross(self,other:Tensor,dim:Optional[_int]=None)->Tensor
torch.__init__.Tensor.cuda(self,device:Optional[_device]=None,non_blocking:_bool=False)->Tensor
torch.__init__.Tensor.cumprod(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.cumprod(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.cumsum(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.cumsum(self,dim:_int,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.dense_dim(self)->_int
torch.__init__.Tensor.dequantize(self)->Tensor
torch.__init__.Tensor.det(self)->Tensor
torch.__init__.Tensor.detach(self)->Tensor
torch.__init__.Tensor.detach_(self)->Tensor
torch.__init__.Tensor.diag(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.diag_embed(self,offset:_int=0,dim1:_int=-2,dim2:_int=-1)->Tensor
torch.__init__.Tensor.diagflat(self,offset:_int=0)->Tensor
torch.__init__.Tensor.diagonal(self,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch.__init__.Tensor.digamma(self)->Tensor
torch.__init__.Tensor.digamma_(self)->Tensor
torch.__init__.Tensor.dim(self)->_int
torch.__init__.Tensor.dist(self,other:Tensor,p:Number=2)->Tensor
torch.__init__.Tensor.dot(self,tensor:Tensor)->Tensor
torch.__init__.Tensor.double(self)->Tensor
torch.__init__.Tensor.eig(self,eigenvectors:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.element_size(self)->_int
torch.__init__.Tensor.eq(self,other:Number)->Tensor
torch.__init__.Tensor.eq(self,other:Tensor)->Tensor
torch.__init__.Tensor.eq_(self,other:Number)->Tensor
torch.__init__.Tensor.eq_(self,other:Tensor)->Tensor
torch.__init__.Tensor.equal(self,other:Tensor)->_bool
torch.__init__.Tensor.erf(self)->Tensor
torch.__init__.Tensor.erf_(self)->Tensor
torch.__init__.Tensor.erfc(self)->Tensor
torch.__init__.Tensor.erfc_(self)->Tensor
torch.__init__.Tensor.erfinv(self)->Tensor
torch.__init__.Tensor.erfinv_(self)->Tensor
torch.__init__.Tensor.exp(self)->Tensor
torch.__init__.Tensor.exp_(self)->Tensor
torch.__init__.Tensor.expand(self,*size:_int,implicit:_bool=False)->Tensor
torch.__init__.Tensor.expand(self,size:_size,*,implicit:_bool=False)->Tensor
torch.__init__.Tensor.expand_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.expm1(self)->Tensor
torch.__init__.Tensor.expm1_(self)->Tensor
torch.__init__.Tensor.exponential_(self,lambd:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.fft(self,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.Tensor.fill_(self,value:Number)->Tensor
torch.__init__.Tensor.fill_(self,value:Tensor)->Tensor
torch.__init__.Tensor.fill_diagonal_(self,fill_value:Number,wrap:_bool=False)->Tensor
torch.__init__.Tensor.flatten(self,dims:List[Union[str,None]],out_dim:Union[str,None])->Tensor
torch.__init__.Tensor.flatten(self,start_dim:Union[str,None],end_dim:Union[str,None],out_dim:Union[str,None])->Tensor
torch.__init__.Tensor.flatten(self,start_dim:_int,end_dim:_int,out_dim:Union[str,None])->Tensor
torch.__init__.Tensor.flatten(self,start_dim:_int=0,end_dim:_int=-1)->Tensor
torch.__init__.Tensor.flip(self,*dims:_int)->Tensor
torch.__init__.Tensor.flip(self,dims:_size)->Tensor
torch.__init__.Tensor.float(self)->Tensor
torch.__init__.Tensor.floor(self)->Tensor
torch.__init__.Tensor.floor_(self)->Tensor
torch.__init__.Tensor.fmod(self,other:Number)->Tensor
torch.__init__.Tensor.fmod(self,other:Tensor)->Tensor
torch.__init__.Tensor.fmod_(self,other:Number)->Tensor
torch.__init__.Tensor.fmod_(self,other:Tensor)->Tensor
torch.__init__.Tensor.frac(self)->Tensor
torch.__init__.Tensor.frac_(self)->Tensor
torch.__init__.Tensor.gather(self,dim:Union[str,None],index:Tensor,*,sparse_grad:_bool=False)->Tensor
torch.__init__.Tensor.gather(self,dim:_int,index:Tensor,*,sparse_grad:_bool=False)->Tensor
torch.__init__.Tensor.ge(self,other:Number)->Tensor
torch.__init__.Tensor.ge(self,other:Tensor)->Tensor
torch.__init__.Tensor.ge_(self,other:Number)->Tensor
torch.__init__.Tensor.ge_(self,other:Tensor)->Tensor
torch.__init__.Tensor.geometric_(self,p:_float,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.geqrf(self)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.ger(self,vec2:Tensor)->Tensor
torch.__init__.Tensor.get_device(self)->_int
torch.__init__.Tensor.gt(self,other:Number)->Tensor
torch.__init__.Tensor.gt(self,other:Tensor)->Tensor
torch.__init__.Tensor.gt_(self,other:Number)->Tensor
torch.__init__.Tensor.gt_(self,other:Tensor)->Tensor
torch.__init__.Tensor.half(self)->Tensor
torch.__init__.Tensor.hardshrink(self,lambd:Number=0.5)->Tensor
torch.__init__.Tensor.histc(self,bins:_int=100,min:Number=0,max:Number=0)->Tensor
torch.__init__.Tensor.ifft(self,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.Tensor.index_add(self,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_add(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_add_(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy(self,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy_(self,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_copy_(self,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.index_fill(self,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill(self,dim:Union[str,None],index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_fill(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill(self,dim:_int,index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:Union[str,None],index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.index_fill_(self,dim:_int,index:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.index_put(self,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.Tensor.index_put_(self,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.Tensor.index_select(self,dim:Union[str,None],index:Tensor)->Tensor
torch.__init__.Tensor.index_select(self,dim:_int,index:Tensor)->Tensor
torch.__init__.Tensor.indices(self)->Tensor
torch.__init__.Tensor.int(self)->Tensor
torch.__init__.Tensor.int_repr(self)->Tensor
torch.__init__.Tensor.inverse(self)->Tensor
torch.__init__.Tensor.irfft(self,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True,signal_sizes:_size=())->Tensor
torch.__init__.Tensor.is_coalesced(self)->_bool
torch.__init__.Tensor.is_complex(self)->_bool
torch.__init__.Tensor.is_contiguous(self)->_bool
torch.__init__.Tensor.is_distributed(self)->_bool
torch.__init__.Tensor.is_floating_point(self)->_bool
torch.__init__.Tensor.is_nonzero(self)->_bool
torch.__init__.Tensor.is_pinned(self)->_bool
torch.__init__.Tensor.is_same_size(self,other:Tensor)->_bool
torch.__init__.Tensor.is_set_to(self,tensor:Tensor)->_bool
torch.__init__.Tensor.is_shared(self)->_bool
torch.__init__.Tensor.is_signed(self)->_bool
torch.__init__.Tensor.isclose(self,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->Tensor
torch.__init__.Tensor.item(self)->Number
torch.__init__.Tensor.kthvalue(self,k:_int,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.kthvalue(self,k:_int,dim:_int=-1,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.le(self,other:Number)->Tensor
torch.__init__.Tensor.le(self,other:Tensor)->Tensor
torch.__init__.Tensor.le_(self,other:Number)->Tensor
torch.__init__.Tensor.le_(self,other:Tensor)->Tensor
torch.__init__.Tensor.lerp(self,end:Tensor,weight:Number)->Tensor
torch.__init__.Tensor.lerp(self,end:Tensor,weight:Tensor)->Tensor
torch.__init__.Tensor.lerp_(self,end:Tensor,weight:Number)->Tensor
torch.__init__.Tensor.lerp_(self,end:Tensor,weight:Tensor)->Tensor
torch.__init__.Tensor.lgamma(self)->Tensor
torch.__init__.Tensor.lgamma_(self)->Tensor
torch.__init__.Tensor.log(self)->Tensor
torch.__init__.Tensor.log10(self)->Tensor
torch.__init__.Tensor.log10_(self)->Tensor
torch.__init__.Tensor.log1p(self)->Tensor
torch.__init__.Tensor.log1p_(self)->Tensor
torch.__init__.Tensor.log2(self)->Tensor
torch.__init__.Tensor.log2_(self)->Tensor
torch.__init__.Tensor.log_(self)->Tensor
torch.__init__.Tensor.log_normal_(self,mean:_float=1,std:_float=2,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.log_softmax(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.log_softmax(self,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.logdet(self)->Tensor
torch.__init__.Tensor.logical_not(self)->Tensor
torch.__init__.Tensor.logical_not_(self)->Tensor
torch.__init__.Tensor.logical_xor(self,other:Tensor)->Tensor
torch.__init__.Tensor.logical_xor_(self,other:Tensor)->Tensor
torch.__init__.Tensor.logsumexp(self,dim:List[Union[str,None]],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.logsumexp(self,dim:Union[_int,_size],keepdim:_bool=False)->Tensor
torch.__init__.Tensor.long(self)->Tensor
torch.__init__.Tensor.lstsq(self,A:Tensor)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.lt(self,other:Number)->Tensor
torch.__init__.Tensor.lt(self,other:Tensor)->Tensor
torch.__init__.Tensor.lt_(self,other:Number)->Tensor
torch.__init__.Tensor.lt_(self,other:Tensor)->Tensor
torch.__init__.Tensor.lu(self,pivot=True,get_infos=False)
torch.__init__.Tensor.lu_solve(self,LU_data:Tensor,LU_pivots:Tensor)->Tensor
torch.__init__.Tensor.map_(tensor:Tensor,callable:Callable)->Tensor
torch.__init__.Tensor.masked_fill(self,mask:Tensor,value:Number)->Tensor
torch.__init__.Tensor.masked_fill(self,mask:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.masked_fill_(self,mask:Tensor,value:Number)->Tensor
torch.__init__.Tensor.masked_fill_(self,mask:Tensor,value:Tensor)->Tensor
torch.__init__.Tensor.masked_scatter(self,mask:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.masked_scatter_(self,mask:Tensor,source:Tensor)->Tensor
torch.__init__.Tensor.masked_select(self,mask:Tensor)->Tensor
torch.__init__.Tensor.matmul(self,other:Tensor)->Tensor
torch.__init__.Tensor.matrix_power(self,n:_int)->Tensor
torch.__init__.Tensor.max(self)->Tensor
torch.__init__.Tensor.max(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.max(self,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.max(self,other:Tensor)->Tensor
torch.__init__.Tensor.mean(self,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.mean(self,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.mean(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.median(self)->Tensor
torch.__init__.Tensor.median(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.median(self,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.min(self)->Tensor
torch.__init__.Tensor.min(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.min(self,dim:_int,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.min(self,other:Tensor)->Tensor
torch.__init__.Tensor.mm(self,mat2:Tensor)->Tensor
torch.__init__.Tensor.mode(self,dim:Union[str,None],keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.mode(self,dim:_int=-1,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.multinomial(self,num_samples:_int,replacement:_bool=False,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.mv(self,vec:Tensor)->Tensor
torch.__init__.Tensor.mvlgamma(self,p:_int)->Tensor
torch.__init__.Tensor.mvlgamma_(self,p:_int)->Tensor
torch.__init__.Tensor.narrow(self,dim:_int,start:_int,length:_int)->Tensor
torch.__init__.Tensor.narrow_copy(self,dim:_int,start:_int,length:_int)->Tensor
torch.__init__.Tensor.ndimension(self)->_int
torch.__init__.Tensor.ne(self,other:Number)->Tensor
torch.__init__.Tensor.ne(self,other:Tensor)->Tensor
torch.__init__.Tensor.ne_(self,other:Number)->Tensor
torch.__init__.Tensor.ne_(self,other:Tensor)->Tensor
torch.__init__.Tensor.neg(self)->Tensor
torch.__init__.Tensor.neg_(self)->Tensor
torch.__init__.Tensor.nelement(self)->_int
torch.__init__.Tensor.new_empty(self,*size:_int,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_empty(self,size:_size,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_full(self,size:_size,fill_value:Number,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_ones(self,size:_size,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_tensor(self,data:Any,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.new_zeros(self,size:_size,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.Tensor.nonzero(self,*,as_tuple=True)
torch.__init__.Tensor.norm(self,p='fro',dim=None,keepdim=False)
torch.__init__.Tensor.normal_(self,mean:_float=0,std:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.numel(self)->_int
torch.__init__.Tensor.numpy(self)->Any
torch.__init__.Tensor.orgqr(self,input2:Tensor)->Tensor
torch.__init__.Tensor.ormqr(self,input2:Tensor,input3:Tensor,left:_bool=True,transpose:_bool=False)->Tensor
torch.__init__.Tensor.permute(self,*dims:_int)->Tensor
torch.__init__.Tensor.permute(self,dims:_size)->Tensor
torch.__init__.Tensor.pin_memory(self)->Tensor
torch.__init__.Tensor.pinverse(self,rcond:_float=1e-15)->Tensor
torch.__init__.Tensor.polygamma(self,n:_int)->Tensor
torch.__init__.Tensor.polygamma_(self,n:_int)->Tensor
torch.__init__.Tensor.pow(self,exponent:Number)->Tensor
torch.__init__.Tensor.pow(self,exponent:Tensor)->Tensor
torch.__init__.Tensor.pow_(self,exponent:Number)->Tensor
torch.__init__.Tensor.pow_(self,exponent:Tensor)->Tensor
torch.__init__.Tensor.prelu(self,weight:Tensor)->Tensor
torch.__init__.Tensor.prod(self,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.prod(self,dim:Union[str,None],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.prod(self,dim:_int,keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.put_(self,index:Tensor,source:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.Tensor.q_per_channel_axis(self)->_int
torch.__init__.Tensor.q_per_channel_scales(self)->Tensor
torch.__init__.Tensor.q_per_channel_zero_points(self)->Tensor
torch.__init__.Tensor.q_scale(self)->_float
torch.__init__.Tensor.q_zero_point(self)->_int
torch.__init__.Tensor.qr(self,some:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.qscheme(self)->_qscheme
torch.__init__.Tensor.random_(self,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.random_(self,from_:_int,to:_int,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.random_(self,to:_int,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.reciprocal(self)->Tensor
torch.__init__.Tensor.reciprocal_(self)->Tensor
torch.__init__.Tensor.refine_names(self,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.register_hook(self,hook:Callable)->Any
torch.__init__.Tensor.relu(self)->Tensor
torch.__init__.Tensor.relu_(self)->Tensor
torch.__init__.Tensor.remainder(self,other:Number)->Tensor
torch.__init__.Tensor.remainder(self,other:Tensor)->Tensor
torch.__init__.Tensor.remainder_(self,other:Number)->Tensor
torch.__init__.Tensor.remainder_(self,other:Tensor)->Tensor
torch.__init__.Tensor.rename(self,names:Optional[List[Union[str,None]]])->Tensor
torch.__init__.Tensor.rename_(self,names:Optional[List[Union[str,None]]])->Tensor
torch.__init__.Tensor.renorm(self,p:Number,dim:_int,maxnorm:Number)->Tensor
torch.__init__.Tensor.renorm_(self,p:Number,dim:_int,maxnorm:Number)->Tensor
torch.__init__.Tensor.repeat(self,*repeats:_int)->Tensor
torch.__init__.Tensor.repeat(self,repeats:_size)->Tensor
torch.__init__.Tensor.repeat_interleave(self,repeats:Tensor,dim:Optional[_int]=None)->Tensor
torch.__init__.Tensor.repeat_interleave(self,repeats:_int,dim:Optional[_int]=None)->Tensor
torch.__init__.Tensor.requires_grad_(self,mode:_bool=True)->Tensor
torch.__init__.Tensor.reshape(self,*shape:_int)->Tensor
torch.__init__.Tensor.reshape(self,shape:_size)->Tensor
torch.__init__.Tensor.reshape_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.resize_(self,*size:_int)->Tensor
torch.__init__.Tensor.resize_(self,size:_size)->Tensor
torch.__init__.Tensor.resize_as_(self,the_template:Tensor)->Tensor
torch.__init__.Tensor.retain_grad(self)->None
torch.__init__.Tensor.rfft(self,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True)->Tensor
torch.__init__.Tensor.roll(self,shifts:Union[_int,_size],dims:Union[_int,_size]=())->Tensor
torch.__init__.Tensor.rot90(self,k:_int=1,dims:_size=(0,1))->Tensor
torch.__init__.Tensor.round(self)->Tensor
torch.__init__.Tensor.round_(self)->Tensor
torch.__init__.Tensor.rsqrt(self)->Tensor
torch.__init__.Tensor.rsqrt_(self)->Tensor
torch.__init__.Tensor.scatter(self,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter(self,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.scatter(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.scatter_(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter_(self,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.Tensor.scatter_add(self,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter_add(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.scatter_add_(self,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.Tensor.select(self,dim:Union[str,None],index:_int)->Tensor
torch.__init__.Tensor.select(self,dim:_int,index:_int)->Tensor
torch.__init__.Tensor.set_(self)->Tensor
torch.__init__.Tensor.set_(self,source:Storage)->Tensor
torch.__init__.Tensor.set_(self,source:Storage,storage_offset:_int,size:_size,stride:_size=())->Tensor
torch.__init__.Tensor.set_(self,source:Tensor)->Tensor
torch.__init__.Tensor.share_memory_(self)->None
torch.__init__.Tensor.short(self)->Tensor
torch.__init__.Tensor.sigmoid(self)->Tensor
torch.__init__.Tensor.sigmoid_(self)->Tensor
torch.__init__.Tensor.sign(self)->Tensor
torch.__init__.Tensor.sign_(self)->Tensor
torch.__init__.Tensor.sin(self)->Tensor
torch.__init__.Tensor.sin_(self)->Tensor
torch.__init__.Tensor.sinh(self)->Tensor
torch.__init__.Tensor.sinh_(self)->Tensor
torch.__init__.Tensor.size(self)->Size
torch.__init__.Tensor.size(self,_int)->_int
torch.__init__.Tensor.slogdet(self)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.smm(self,mat2:Tensor)->Tensor
torch.__init__.Tensor.softmax(self,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.softmax(self,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.solve(self,A:Tensor)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.sort(self,dim:Union[str,None],descending:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.sort(self,dim:_int=-1,descending:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.sparse_dim(self)->_int
torch.__init__.Tensor.sparse_mask(self,mask:Tensor)->Tensor
torch.__init__.Tensor.sparse_resize_(self,size:_size,sparse_dim:_int,dense_dim:_int)->Tensor
torch.__init__.Tensor.sparse_resize_and_clear_(self,size:_size,sparse_dim:_int,dense_dim:_int)->Tensor
torch.__init__.Tensor.split(self,split_size,dim=0)
torch.__init__.Tensor.split_with_sizes(self,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.sqrt(self)->Tensor
torch.__init__.Tensor.sqrt_(self)->Tensor
torch.__init__.Tensor.squeeze(self)->Tensor
torch.__init__.Tensor.squeeze(self,dim:Union[str,None])->Tensor
torch.__init__.Tensor.squeeze(self,dim:_int)->Tensor
torch.__init__.Tensor.squeeze_(self)->Tensor
torch.__init__.Tensor.squeeze_(self,dim:Union[str,None])->Tensor
torch.__init__.Tensor.squeeze_(self,dim:_int)->Tensor
torch.__init__.Tensor.sspaddmm(self,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.Tensor.std(self,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.std(self,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.std(self,unbiased:_bool=True)->Tensor
torch.__init__.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.__init__.Tensor.storage(self)->Storage
torch.__init__.Tensor.storage_offset(self)->_int
torch.__init__.Tensor.stride(self)->Tuple[_int]
torch.__init__.Tensor.stride(self,_int)->_int
torch.__init__.Tensor.sum(self,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.sum(self,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.sum(self,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.Tensor.sum_to_size(self,*size:_int)->Tensor
torch.__init__.Tensor.sum_to_size(self,size:_size)->Tensor
torch.__init__.Tensor.svd(self,some:_bool=True,compute_uv:_bool=True)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.Tensor.symeig(self,eigenvectors:_bool=False,upper:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.t(self)->Tensor
torch.__init__.Tensor.t_(self)->Tensor
torch.__init__.Tensor.take(self,index:Tensor)->Tensor
torch.__init__.Tensor.tan(self)->Tensor
torch.__init__.Tensor.tan_(self)->Tensor
torch.__init__.Tensor.tanh(self)->Tensor
torch.__init__.Tensor.tanh_(self)->Tensor
torch.__init__.Tensor.to(self,device:Optional[Union[_device,str]]=None,dtype:Optional[_dtype]=None,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch.__init__.Tensor.to(self,dtype:_dtype,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch.__init__.Tensor.to(self,other:Tensor,non_blocking:_bool=False,copy:_bool=False)->Tensor
torch.__init__.Tensor.to_dense(self)->Tensor
torch.__init__.Tensor.to_mkldnn(self)->Tensor
torch.__init__.Tensor.to_sparse(self)->Tensor
torch.__init__.Tensor.to_sparse(self,sparse_dim:_int)->Tensor
torch.__init__.Tensor.tolist(self)->List
torch.__init__.Tensor.topk(self,k:_int,dim:_int=-1,largest:_bool=True,sorted:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.trace(self)->Tensor
torch.__init__.Tensor.transpose(self,dim0:Union[str,None],dim1:Union[str,None])->Tensor
torch.__init__.Tensor.transpose(self,dim0:_int,dim1:_int)->Tensor
torch.__init__.Tensor.transpose_(self,dim0:_int,dim1:_int)->Tensor
torch.__init__.Tensor.triangular_solve(self,A:Tensor,upper:_bool=True,transpose:_bool=False,unitriangular:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.Tensor.tril(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.tril_(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.triu(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.triu_(self,diagonal:_int=0)->Tensor
torch.__init__.Tensor.trunc(self)->Tensor
torch.__init__.Tensor.trunc_(self)->Tensor
torch.__init__.Tensor.type(self,dtype:Union[None,str,_dtype]=None,non_blocking:_bool=False)->Union[str, Tensor]
torch.__init__.Tensor.type_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.unbind(self,dim:Union[str,None])->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.unbind(self,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.Tensor.unflatten(self,dim:Union[str,None],sizes:_size,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.unflatten(self,dim:_int,sizes:_size,names:List[Union[str,None]])->Tensor
torch.__init__.Tensor.unfold(self,dimension:_int,size:_int,step:_int)->Tensor
torch.__init__.Tensor.uniform_(self,from_:_float=0,to:_float=1,*,generator:Generator=None)->Tensor
torch.__init__.Tensor.unique(self,sorted=True,return_inverse=False,dim=None)
torch.__init__.Tensor.unique_consecutive(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.__init__.Tensor.unsqueeze(self,dim:_int)->Tensor
torch.__init__.Tensor.unsqueeze_(self,dim:_int)->Tensor
torch.__init__.Tensor.values(self)->Tensor
torch.__init__.Tensor.var(self,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.var(self,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tensor
torch.__init__.Tensor.var(self,unbiased:_bool=True)->Tensor
torch.__init__.Tensor.view(self,*size:_int)->Tensor
torch.__init__.Tensor.view(self,size:_size)->Tensor
torch.__init__.Tensor.view_as(self,other:Tensor)->Tensor
torch.__init__.Tensor.where(self,condition:Tensor,other:Tensor)->Tensor
torch.__init__.Tensor.zero_(self)->Tensor
torch.__init__.Tensor.zeros_like_(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like_(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like_(self,value:Number,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like_(self,value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like__(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like__(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like__(self,value:Number,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like__(self,value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like___(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like___(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like___(self,value:Number,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like___(self,value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like____(self,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like____(self,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.Tensor.zeros_like____(self,value:Number,other:Union[Tensor,Number])->Tensor
torch.__init__.Tensor.zeros_like____(self,value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.__and__(self:Tensor,other:Number)->Tensor
torch.__init__.__and__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__lshift__(self:Tensor,other:Number)->Tensor
torch.__init__.__lshift__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__or__(self:Tensor,other:Number)->Tensor
torch.__init__.__or__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__rshift__(self:Tensor,other:Number)->Tensor
torch.__init__.__rshift__(self:Tensor,other:Tensor)->Tensor
torch.__init__.__xor__(self:Tensor,other:Number)->Tensor
torch.__init__.__xor__(self:Tensor,other:Tensor)->Tensor
torch.__init__._adaptive_avg_pool2d(self:Tensor,output_size:Union[_int,_size])->Tensor
torch.__init__._addr(self:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__._addr_(self:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__._baddbmm_mkl_(self:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__._batch_norm_impl_index(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tuple[Tensor, Tensor, Tensor, _int]
torch.__init__._cast_Byte(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Char(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Double(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Float(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Half(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Int(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Long(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cast_Short(self:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__._convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int,benchmark:_bool,deterministic:_bool,cudnn_enabled:_bool)->Tensor
torch.__init__._convolution_nogroup(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size)->Tensor
torch.__init__._copy_from(self:Tensor,dst:Tensor,non_blocking:_bool=False)->Tensor
torch.__init__._ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int=0,zero_infinity:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__._cudnn_ctc_loss(log_probs:Tensor,targets:Tensor,input_lengths:_size,target_lengths:_size,blank:_int,deterministic:_bool,zero_infinity:_bool)->Tuple[Tensor, Tensor]
torch.__init__._cudnn_init_dropout_state(dropout:_float,train:_bool,dropout_seed:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._cudnn_rnn(input:Tensor,weight:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,weight_buf:Optional[Tensor],hx:Tensor,cx:Optional[Tensor],mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,dropout:_float,train:_bool,bidirectional:_bool,batch_sizes:_size,dropout_state:Optional[Tensor])->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]
torch.__init__._cudnn_rnn_flatten_weight(weight_arr:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,input_size:_int,mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,bidirectional:_bool)->Tensor
torch.__init__._cufft_clear_plan_cache(device_index:_int)->None
torch.__init__._cufft_get_plan_cache_max_size(device_index:_int)->_int
torch.__init__._cufft_get_plan_cache_size(device_index:_int)->_int
torch.__init__._cufft_set_plan_cache_max_size(device_index:_int,max_size:_int)->None
torch.__init__._debug_has_internal_overlap(self:Tensor)->_int
torch.__init__._dim_arange(like:Tensor,dim:_int)->Tensor
torch.__init__._dirichlet_grad(x:Tensor,alpha:Tensor,total:Tensor)->Tensor
torch.__init__._embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__._empty_affine_quantized(*size:_int,scale:_float=1,zero_point:_int=0,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._empty_affine_quantized(size:_size,*,scale:_float=1,zero_point:_int=0,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._empty_per_channel_affine_quantized(*size:_int,scales:Tensor,zero_points:Tensor,axis:_int,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._empty_per_channel_affine_quantized(size:_size,*,scales:Tensor,zero_points:Tensor,axis:_int,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__._fft_with_size(self:Tensor,signal_ndim:_int,complex_input:_bool,complex_output:_bool,inverse:_bool,checked_signal_sizes:_size,normalized:_bool,onesided:_bool,output_sizes:_size)->Tensor
torch.__init__._fused_dropout(self:Tensor,p:_float,generator:Generator=None)->Tuple[Tensor, Tensor]
torch.__init__._has_compatible_shallow_copy_type(self:Tensor,from_:Tensor)->_bool
torch.__init__._index_copy_(self:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__._index_put_impl_(self:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False,unsafe:_bool=False)->Tensor
torch.__init__._log_softmax(self:Tensor,dim:_int,half_to_float:_bool)->Tensor
torch.__init__._log_softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,self:Tensor)->Tensor
torch.__init__._lu_solve_helper(self:Tensor,LU_data:Tensor,LU_pivots:Tensor)->Tensor
torch.__init__._lu_with_info(self:Tensor,pivot:_bool=True,check_errors:_bool=True)->Tuple[Tensor, Tensor, Tensor]
torch.__init__._make_per_channel_quantized_tensor(self:Tensor,scale:Tensor,zero_point:Tensor,axis:_int)->Tensor
torch.__init__._make_per_tensor_quantized_tensor(self:Tensor,scale:_float,zero_point:_int)->Tensor
torch.__init__._masked_scale(self:Tensor,mask:Tensor,scale:_float)->Tensor
torch.__init__._max(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__._min(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__._mkldnn_reshape(self:Tensor,shape:_size)->Tensor
torch.__init__._mkldnn_transpose(self:Tensor,dim0:_int,dim1:_int)->Tensor
torch.__init__._mkldnn_transpose_(self:Tensor,dim0:_int,dim1:_int)->Tensor
torch.__init__._mode(self:Tensor,dim:_int=-1,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__._multinomial_alias_draw(J:Tensor,q:Tensor,num_samples:_int,*,generator:Generator=None)->Tensor
torch.__init__._multinomial_alias_setup(probs:Tensor)->Tuple[Tensor, Tensor]
torch.__init__._nnpack_available()->_bool
torch.__init__._nnpack_spatial_convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],padding:Union[_int,_size])->Tensor
torch.__init__._pack_padded_sequence(input:Tensor,lengths:Tensor,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__._pad_packed_sequence(data:Tensor,batch_sizes:Tensor,batch_first:_bool,padding_value:Number,total_length:_int)->Tuple[Tensor, Tensor]
torch.__init__._reshape_from_tensor(self:Tensor,shape:Tensor)->Tensor
torch.__init__._s_where(condition:Tensor,self:Tensor,other:Tensor)->Tensor
torch.__init__._sample_dirichlet(self:Tensor,generator:Generator=None)->Tensor
torch.__init__._shape_as_tensor(self:Tensor)->Tensor
torch.__init__._sobol_engine_draw(quasi:Tensor,n:_int,sobolstate:Tensor,dimension:_int,num_generated:_int,dtype:Optional[_dtype])->Tuple[Tensor, Tensor]
torch.__init__._sobol_engine_ff_(self:Tensor,n:_int,sobolstate:Tensor,dimension:_int,num_generated:_int)->Tensor
torch.__init__._sobol_engine_initialize_state_(self:Tensor,dimension:_int)->Tensor
torch.__init__._sobol_engine_scramble_(self:Tensor,ltm:Tensor,dimension:_int)->Tensor
torch.__init__._softmax(self:Tensor,dim:_int,half_to_float:_bool)->Tensor
torch.__init__._softmax_backward_data(grad_output:Tensor,output:Tensor,dim:_int,self:Tensor)->Tensor
torch.__init__._sparse_addmm(self:Tensor,sparse:Tensor,dense:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__._sparse_mm(sparse:Tensor,dense:Tensor)->Tensor
torch.__init__._sparse_sum(self:Tensor)->Tensor
torch.__init__._sparse_sum(self:Tensor,*,dtype:_dtype)->Tensor
torch.__init__._sparse_sum(self:Tensor,dim:Union[_int,_size])->Tensor
torch.__init__._sparse_sum(self:Tensor,dim:Union[_int,_size],*,dtype:_dtype)->Tensor
torch.__init__._standard_gamma(self:Tensor,generator:Generator=None)->Tensor
torch.__init__._standard_gamma_grad(self:Tensor,output:Tensor)->Tensor
torch.__init__._std(self:Tensor,unbiased:_bool=True)->Tensor
torch.__init__._trilinear(i1:Tensor,i2:Tensor,i3:Tensor,expand1:_size,expand2:_size,expand3:_size,sumdim:_size,unroll_dim:_int=1)->Tensor
torch.__init__._unique(self:Tensor,sorted:_bool=True,return_inverse:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__._unique2(self:Tensor,sorted:_bool=True,return_inverse:_bool=False,return_counts:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__._var(self:Tensor,unbiased:_bool=True)->Tensor
torch.__init__._weight_norm(v:Tensor,g:Tensor,dim:_int=0)->Tensor
torch.__init__._weight_norm_cuda_interface(v:Tensor,g:Tensor,dim:_int=0)->Tuple[Tensor, Tensor]
torch.__init__.abs(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.abs_(self:Tensor)->Tensor
torch.__init__.acos(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.acos_(self:Tensor)->Tensor
torch.__init__.adaptive_avg_pool1d(self:Tensor,output_size:Union[_int,_size])->Tensor
torch.__init__.adaptive_max_pool1d(self:Tensor,output_size:Union[_int,_size])->Tuple[Tensor, Tensor]
torch.__init__.add(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.add(input:Union[Tensor,Number],value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.add(self:Tensor,alpha:Number,other:Tensor)->Tensor
torch.__init__.add(self:Tensor,alpha:Number,other:Tensor,*,out:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.addbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addbmm(self:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addcdiv(self:Tensor,tensor1:Tensor,tensor2:Tensor,*,value:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addcdiv(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor)->Tensor
torch.__init__.addcdiv(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addcmul(self:Tensor,tensor1:Tensor,tensor2:Tensor,*,value:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addcmul(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor)->Tensor
torch.__init__.addcmul(self:Tensor,value:Number,tensor1:Tensor,tensor2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.addmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmm(self:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,alpha:Number,mat:Tensor,vec:Tensor)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,alpha:Number,mat:Tensor,vec:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,mat:Tensor,vec:Tensor)->Tensor
torch.__init__.addmv(beta:Number,self:Tensor,mat:Tensor,vec:Tensor,*,out:Tensor)->Tensor
torch.__init__.addmv(self:Tensor,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.addmv_(self:Tensor,mat:Tensor,vec:Tensor,*,beta:Number=1,alpha:Number=1)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,alpha:Number,vec1:Tensor,vec2:Tensor)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,alpha:Number,vec1:Tensor,vec2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,vec1:Tensor,vec2:Tensor)->Tensor
torch.__init__.addr(beta:Number,self:Tensor,vec1:Tensor,vec2:Tensor,*,out:Tensor)->Tensor
torch.__init__.addr(self:Tensor,vec1:Tensor,vec2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.affine_grid_generator(theta:Tensor,size:_size,align_corners:_bool)->Tensor
torch.__init__.all(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.all(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.all(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.allclose(self:Tensor,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->_bool
torch.__init__.alpha_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.alpha_dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.any(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.any(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.any(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.arange(end:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.arange(start:Number,end:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.arange(start:Number,end:Number,step:Number,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.argmax(self:Tensor,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.argmin(self:Tensor,dim:Optional[_int]=None,keepdim:_bool=False)->Tensor
torch.__init__.argsort(self:Tensor,dim:Union[str,None],descending:_bool=False)->Tensor
torch.__init__.argsort(self:Tensor,dim:_int=-1,descending:_bool=False)->Tensor
torch.__init__.as_strided(self:Tensor,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.as_strided_(self:Tensor,size:_size,stride:_size,storage_offset:Optional[_int]=None)->Tensor
torch.__init__.as_tensor(data:Any,dtype:_dtype=None,device:Optional[_device]=None)->Tensor
torch.__init__.asin(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.asin_(self:Tensor)->Tensor
torch.__init__.atan(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.atan2(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.atan_(self:Tensor)->Tensor
torch.__init__.avg_pool1d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,ceil_mode:_bool=False,count_include_pad:_bool=True)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,alpha:Number,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor)->Tensor
torch.__init__.baddbmm(beta:Number,self:Tensor,batch1:Tensor,batch2:Tensor,*,out:Tensor)->Tensor
torch.__init__.baddbmm(self:Tensor,batch1:Tensor,batch2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.bartlett_window(window_length:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.bartlett_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tensor
torch.__init__.batch_norm_backward_elemt(grad_out:Tensor,input:Tensor,mean:Tensor,invstd:Tensor,weight:Optional[Tensor],mean_dy:Tensor,mean_dy_xmu:Tensor)->Tensor
torch.__init__.batch_norm_backward_reduce(grad_out:Tensor,input:Tensor,mean:Tensor,invstd:Tensor,weight:Optional[Tensor],input_g:_bool,weight_g:_bool,bias_g:_bool)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__.batch_norm_elemt(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],mean:Tensor,invstd:Tensor,eps:_float)->Tensor
torch.__init__.batch_norm_gather_stats(input:Tensor,mean:Tensor,invstd:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float,eps:_float,count:_int)->Tuple[Tensor, Tensor]
torch.__init__.batch_norm_gather_stats_with_counts(input:Tensor,mean:Tensor,invstd:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float,eps:_float,counts:_size)->Tuple[Tensor, Tensor]
torch.__init__.batch_norm_stats(input:Tensor,eps:_float)->Tuple[Tensor, Tensor]
torch.__init__.batch_norm_update_stats(input:Tensor,running_mean:Optional[Tensor],running_var:Optional[Tensor],momentum:_float)->Tuple[Tensor, Tensor]
torch.__init__.bernoulli(self:Tensor,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.bernoulli(self:Tensor,p:_float,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.bilinear(input1:Tensor,input2:Tensor,weight:Tensor,bias:Optional[Tensor])->Tensor
torch.__init__.bincount(self:Tensor,weights:Optional[Tensor]=None,minlength:_int=0)->Tensor
torch.__init__.bitwise_not(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.blackman_window(window_length:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.blackman_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.bmm(self:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.can_cast(from_:_dtype,to:_dtype)->_bool
torch.__init__.cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:Union[str,None],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cat(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cdist(x1:Tensor,x2:Tensor,p:_float=2)->Tensor
torch.__init__.ceil(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ceil_(self:Tensor)->Tensor
torch.__init__.celu(self:Tensor,alpha:Number=1.0)->Tensor
torch.__init__.celu_(self:Tensor,alpha:Number=1.0)->Tensor
torch.__init__.cholesky(self:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cholesky_inverse(self:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cholesky_solve(self:Tensor,input2:Tensor,upper:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.chunk(self:Tensor,chunks:_int,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.clamp(self,min:_float=-inf,max:_float=inf,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.clamp_max(self:Tensor,max:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.clamp_max_(self:Tensor,max:Number)->Tensor
torch.__init__.clamp_min(self:Tensor,min:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.clamp_min_(self:Tensor,min:Number)->Tensor
torch.__init__.clone(self:Tensor)->Tensor
torch.__init__.combinations(self:Tensor,r:_int=2,with_replacement:_bool=False)->Tensor
torch.__init__.constant_pad_nd(self:Tensor,pad:_size,value:Number=0)->Tensor
torch.__init__.conv1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch.__init__.conv2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch.__init__.conv3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,groups:_int=1)->Tensor
torch.__init__.conv_tbc(self:Tensor,weight:Tensor,bias:Tensor,pad:_int=0)->Tensor
torch.__init__.conv_transpose1d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch.__init__.conv_transpose2d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch.__init__.conv_transpose3d(input:Tensor,weight:Tensor,bias:Optional[Tensor]=None,stride:Union[_int,_size]=1,padding:Union[_int,_size]=0,output_padding:Union[_int,_size]=0,groups:_int=1,dilation:Union[_int,_size]=1)->Tensor
torch.__init__.convolution(input:Tensor,weight:Tensor,bias:Optional[Tensor],stride:_size,padding:_size,dilation:_size,transposed:_bool,output_padding:_size,groups:_int)->Tensor
torch.__init__.cos(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cos_(self:Tensor)->Tensor
torch.__init__.cosh(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cosh_(self:Tensor)->Tensor
torch.__init__.cosine_similarity(x1:Tensor,x2:Tensor,dim:_int=1,eps:_float=1e-08)->Tensor
torch.__init__.cross(self:Tensor,other:Tensor,dim:Optional[_int]=None,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.cudnn_affine_grid_generator(theta:Tensor,N:_int,C:_int,H:_int,W:_int)->Tensor
torch.__init__.cudnn_batch_norm(input:Tensor,weight:Tensor,bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,exponential_average_factor:_float,epsilon:_float)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.cudnn_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.cudnn_convolution_transpose(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.cudnn_grid_sampler(self:Tensor,grid:Tensor)->Tensor
torch.__init__.cudnn_is_acceptable(self:Tensor)->_bool
torch.__init__.cumprod(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.cumprod(self:Tensor,dim:_int,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.cumsum(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.cumsum(self:Tensor,dim:_int,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.dequantize(self:Tensor)->Tensor
torch.__init__.det(self:Tensor)->Tensor
torch.__init__.detach(self:Tensor)->Tensor
torch.__init__.detach_(self:Tensor)->Tensor
torch.__init__.device(self,type:str,index:_int)
torch.__init__.device.__init__(self,type:str,index:_int)
torch.__init__.diag(self:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.diag_embed(self:Tensor,offset:_int=0,dim1:_int=-2,dim2:_int=-1)->Tensor
torch.__init__.diagflat(self:Tensor,offset:_int=0)->Tensor
torch.__init__.diagonal(self:Tensor,offset:_int=0,dim1:_int=0,dim2:_int=1)->Tensor
torch.__init__.digamma(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.dist(self:Tensor,other:Tensor,p:Number=2)->Tensor
torch.__init__.div(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.div(input:Union[Tensor,Number],value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.dot(self:Tensor,tensor:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.dtype
torch.__init__.eig(self:Tensor,eigenvectors:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.embedding(weight:Tensor,indices:Tensor,padding_idx:_int=-1,scale_grad_by_freq:_bool=False,sparse:_bool=False)->Tensor
torch.__init__.embedding_bag(weight:Tensor,indices:Tensor,offsets:Tensor,scale_grad_by_freq:_bool=False,mode:_int=0,sparse:_bool=False,per_sample_weights:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor, Tensor]
torch.__init__.embedding_renorm_(self:Tensor,indices:Tensor,max_norm:_float,norm_type:_float)->Tensor
torch.__init__.empty(*size:_int,memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty(*size:_int,names:Optional[List[Union[str,None]]],memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty(size:_size,*,memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty(size:_size,*,names:Optional[List[Union[str,None]]],memory_format:Optional[memory_format]=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty_like(self:Tensor)->Tensor
torch.__init__.empty_like(self:Tensor,*,memory_format:Optional[memory_format]=contiguous_format,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.empty_strided(size:_size,stride:_size,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.eq(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.eq(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.equal(self:Tensor,other:Tensor)->_bool
torch.__init__.erf(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.erf_(self:Tensor)->Tensor
torch.__init__.erfc(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.erfc_(self:Tensor)->Tensor
torch.__init__.erfinv(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.exp(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.exp_(self:Tensor)->Tensor
torch.__init__.expm1(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.expm1_(self:Tensor)->Tensor
torch.__init__.eye(n:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.eye(n:_int,m:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.fake_quantize_per_channel_affine(self:Tensor,scale:Tensor,zero_point:Tensor,axis:_int,quant_min:_int,quant_max:_int)->Tensor
torch.__init__.fake_quantize_per_tensor_affine(self:Tensor,scale:_float,zero_point:_int,quant_min:_int,quant_max:_int)->Tensor
torch.__init__.fbgemm_linear_fp16_weight(input:Tensor,packed_weight:Tensor,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_fp16_weight_fp32_activation(input:Tensor,packed_weight:Tensor,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_int8_weight(input:Tensor,weight:Tensor,packed:Tensor,col_offsets:Tensor,weight_scale:Number,weight_zero_point:Number,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_int8_weight_fp32_activation(input:Tensor,weight:Tensor,packed:Tensor,col_offsets:Tensor,weight_scale:Number,weight_zero_point:Number,bias:Tensor)->Tensor
torch.__init__.fbgemm_linear_quantize_weight(input:Tensor)->Tuple[Tensor, Tensor, _float, _int]
torch.__init__.fbgemm_pack_gemm_matrix_fp16(input:Tensor)->Tensor
torch.__init__.fbgemm_pack_quantized_matrix(input:Tensor)->Tensor
torch.__init__.fbgemm_pack_quantized_matrix(input:Tensor,K:_int,N:_int)->Tensor
torch.__init__.feature_alpha_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.feature_alpha_dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.feature_dropout(input:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.feature_dropout_(self:Tensor,p:_float,train:_bool)->Tensor
torch.__init__.fft(self:Tensor,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.fill_(self:Tensor,value:Number)->Tensor
torch.__init__.fill_(self:Tensor,value:Tensor)->Tensor
torch.__init__.flatten(self:Tensor,dims:List[Union[str,None]],out_dim:Union[str,None])->Tensor
torch.__init__.flatten(self:Tensor,start_dim:Union[str,None],end_dim:Union[str,None],out_dim:Union[str,None])->Tensor
torch.__init__.flatten(self:Tensor,start_dim:_int,end_dim:_int,out_dim:Union[str,None])->Tensor
torch.__init__.flatten(self:Tensor,start_dim:_int=0,end_dim:_int=-1)->Tensor
torch.__init__.flip(self:Tensor,dims:_size)->Tensor
torch.__init__.floor(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.floor_(self:Tensor)->Tensor
torch.__init__.fmod(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.fmod(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.frac(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.frac_(self:Tensor)->Tensor
torch.__init__.frobenius_norm(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.frobenius_norm(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.from_file(filename:str,shared:Optional[_bool]=None,size:Optional[_int]=0,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.from_numpy(ndarray)->Tensor
torch.__init__.full(size:_size,fill_value:Number,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.full(size:_size,fill_value:Number,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.full_like(self:Tensor,fill_value:Number)->Tensor
torch.__init__.full_like(self:Tensor,fill_value:Number,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.gather(self:Tensor,dim:Union[str,None],index:Tensor,*,sparse_grad:_bool=False,out:Optional[Tensor]=None)->Tensor
torch.__init__.gather(self:Tensor,dim:_int,index:Tensor,*,sparse_grad:_bool=False,out:Optional[Tensor]=None)->Tensor
torch.__init__.ge(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ge(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.geqrf(self:Tensor,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.ger(self:Tensor,vec2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.get_default_dtype()->_dtype
torch.__init__.get_num_interop_threads()->_int
torch.__init__.get_num_threads()->_int
torch.__init__.grid_sampler(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch.__init__.grid_sampler_2d(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch.__init__.grid_sampler_3d(input:Tensor,grid:Tensor,interpolation_mode:_int,padding_mode:_int,align_corners:_bool)->Tensor
torch.__init__.group_norm(input:Tensor,num_groups:_int,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:_float=1e-05,cudnn_enabled:_bool=True)->Tensor
torch.__init__.gru(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.gru(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.gru_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch.__init__.gt(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.gt(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.hamming_window(window_length:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hamming_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hamming_window(window_length:_int,periodic:_bool,alpha:_float,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hamming_window(window_length:_int,periodic:_bool,alpha:_float,beta:_float,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hann_window(window_length:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hann_window(window_length:_int,periodic:_bool,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.hardshrink(self:Tensor,lambd:Number=0.5)->Tensor
torch.__init__.histc(self:Tensor,bins:_int=100,min:Number=0,max:Number=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.hspmm(mat1:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ifft(self:Tensor,signal_ndim:_int,normalized:_bool=False)->Tensor
torch.__init__.index_add(self:Tensor,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.index_add(self:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.index_copy(self:Tensor,dim:Union[str,None],index:Tensor,source:Tensor)->Tensor
torch.__init__.index_copy(self:Tensor,dim:_int,index:Tensor,source:Tensor)->Tensor
torch.__init__.index_fill(self:Tensor,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.index_fill(self:Tensor,dim:Union[str,None],index:Tensor,value:Tensor)->Tensor
torch.__init__.index_fill(self:Tensor,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.index_fill(self:Tensor,dim:_int,index:Tensor,value:Tensor)->Tensor
torch.__init__.index_put(self:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.index_put_(self:Tensor,indices:Optional[Union[Tuple[Tensor,...],List[Tensor]]],values:Tensor,accumulate:_bool=False)->Tensor
torch.__init__.index_select(self:Tensor,dim:Union[str,None],index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.index_select(self:Tensor,dim:_int,index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.instance_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],use_input_stats:_bool,momentum:_float,eps:_float,cudnn_enabled:_bool)->Tensor
torch.__init__.int_repr(self:Tensor)->Tensor
torch.__init__.inverse(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.irfft(self:Tensor,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True,signal_sizes:_size=())->Tensor
torch.__init__.is_complex(self:Tensor)->_bool
torch.__init__.is_distributed(self:Tensor)->_bool
torch.__init__.is_floating_point(self:Tensor)->_bool
torch.__init__.is_nonzero(self:Tensor)->_bool
torch.__init__.is_same_size(self:Tensor,other:Tensor)->_bool
torch.__init__.is_signed(self:Tensor)->_bool
torch.__init__.isclose(self:Tensor,other:Tensor,rtol:_float=1e-05,atol:_float=1e-08,equal_nan:_bool=False)->Tensor
torch.__init__.isnan(self:Tensor)->Tensor
torch.__init__.kthvalue(self:Tensor,k:_int,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.kthvalue(self:Tensor,k:_int,dim:_int=-1,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.layer_norm(input:Tensor,normalized_shape:_size,weight:Optional[Tensor]=None,bias:Optional[Tensor]=None,eps:_float=1e-05,cudnn_enable:_bool=True)->Tensor
torch.__init__.layout
torch.__init__.le(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.le(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lerp(self:Tensor,end:Tensor,weight:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lerp(self:Tensor,end:Tensor,weight:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lgamma(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.linspace(start:Number,end:Number,steps:_int=100,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.log(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log10(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log10_(self:Tensor)->Tensor
torch.__init__.log1p(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log1p_(self:Tensor)->Tensor
torch.__init__.log2(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.log2_(self:Tensor)->Tensor
torch.__init__.log_(self:Tensor)->Tensor
torch.__init__.log_softmax(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.log_softmax(self:Tensor,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.logdet(self:Tensor)->Tensor
torch.__init__.logical_not(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logical_xor(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logspace(start:Number,end:Number,steps:_int=100,base:_float=10.0,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.logsumexp(self:Tensor,dim:List[Union[str,None]],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.logsumexp(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lstm(data:Tensor,batch_sizes:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.lstm(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.lstm_cell(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.lstsq(self:Tensor,A:Tensor,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.lt(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lt(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.lu_solve(self:Tensor,LU_data:Tensor,LU_pivots:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.masked_fill(self:Tensor,mask:Tensor,value:Number)->Tensor
torch.__init__.masked_fill(self:Tensor,mask:Tensor,value:Tensor)->Tensor
torch.__init__.masked_scatter(self:Tensor,mask:Tensor,source:Tensor)->Tensor
torch.__init__.masked_select(self:Tensor,mask:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.matmul(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.matrix_power(self:Tensor,n:_int)->Tensor
torch.__init__.matrix_rank(self:Tensor,symmetric:_bool=False)->Tensor
torch.__init__.matrix_rank(self:Tensor,tol:_float,symmetric:_bool=False)->Tensor
torch.__init__.max(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.max(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.max(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.max(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.max_pool1d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.max_pool1d_with_indices(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.max_pool2d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.max_pool3d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.mean(self:Tensor,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.mean(self:Tensor,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.mean(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.median(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.median(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.median(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.memory_format
torch.__init__.min(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.min(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.min(self:Tensor,dim:_int,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.min(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.miopen_batch_norm(input:Tensor,weight:Tensor,bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,exponential_average_factor:_float,epsilon:_float)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.miopen_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.miopen_convolution_transpose(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,output_padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.miopen_depthwise_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int,benchmark:_bool,deterministic:_bool)->Tensor
torch.__init__.miopen_rnn(input:Tensor,weight:Union[Tuple[Tensor,...],List[Tensor]],weight_stride0:_int,hx:Tensor,cx:Optional[Tensor],mode:_int,hidden_size:_int,num_layers:_int,batch_first:_bool,dropout:_float,train:_bool,bidirectional:_bool,batch_sizes:_size,dropout_state:Optional[Tensor])->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]
torch.__init__.mkldnn_adaptive_avg_pool2d(self:Tensor,output_size:Union[_int,_size])->Tensor
torch.__init__.mkldnn_convolution(self:Tensor,weight:Tensor,bias:Optional[Tensor],padding:_size,stride:_size,dilation:_size,groups:_int)->Tensor
torch.__init__.mkldnn_convolution_backward_weights(weight_size:_size,grad_output:Tensor,self:Tensor,padding:_size,stride:_size,dilation:_size,groups:_int,bias_defined:_bool)->Tuple[Tensor, Tensor]
torch.__init__.mkldnn_max_pool2d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.mm(self:Tensor,mat2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.mode(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.mode(self:Tensor,dim:_int=-1,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.mul(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.mul(input:Union[Tensor,Number],value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.multinomial(self:Tensor,num_samples:_int,replacement:_bool=False,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.mv(self:Tensor,vec:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.mvlgamma(self:Tensor,p:_int)->Tensor
torch.__init__.narrow(self:Tensor,dim:_int,start:_int,length:_int)->Tensor
torch.__init__.native_batch_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],running_mean:Optional[Tensor],running_var:Optional[Tensor],training:_bool,momentum:_float,eps:_float)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.native_layer_norm(input:Tensor,weight:Optional[Tensor],bias:Optional[Tensor],M:_int,N:_int,eps:_float)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.native_norm(self:Tensor,p:Number=2)->Tensor
torch.__init__.ne(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ne(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.neg(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.neg_(self:Tensor)->Tensor
torch.__init__.nonzero(input:Tensor,*,out:Optional[Tensor]=None,as_tuple:Optional[_bool]=None)
torch.__init__.norm_except_dim(v:Tensor,pow:_int=2,dim:_int=0)->Tensor
torch.__init__.normal(mean:Tensor,std:Tensor,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.normal(mean:Tensor,std:_float=1,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.normal(mean:_float,std:Tensor,*,generator:Generator=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.normal(mean:_float,std:_float,size:_size,*,generator:Generator=None,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.nuclear_norm(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.nuclear_norm(self:Tensor,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.numel(self:Tensor)->_int
torch.__init__.ones(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.ones_like(self:Tensor)->Tensor
torch.__init__.ones_like(self:Tensor,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.orgqr(self:Tensor,input2:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.ormqr(self:Tensor,input2:Tensor,input3:Tensor,left:_bool=True,transpose:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pairwise_distance(x1:Tensor,x2:Tensor,p:_float=2,eps:_float=1e-06,keepdim:_bool=False)->Tensor
torch.__init__.pdist(self:Tensor,p:_float=2)->Tensor
torch.__init__.pinverse(self:Tensor,rcond:_float=1e-15)->Tensor
torch.__init__.pixel_shuffle(self:Tensor,upscale_factor:_int)->Tensor
torch.__init__.poisson(self:Tensor,generator:Generator=None)->Tensor
torch.__init__.poisson_nll_loss(input:Tensor,target:Tensor,log_input:_bool,full:_bool,eps:_float,reduction:_int)->Tensor
torch.__init__.polygamma(n:_int,self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pow(self:Number,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pow(self:Tensor,exponent:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.pow(self:Tensor,exponent:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.prelu(self:Tensor,weight:Tensor)->Tensor
torch.__init__.prod(self:Tensor,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.prod(self:Tensor,dim:Union[str,None],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.prod(self:Tensor,dim:_int,keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.promote_types(type1:_dtype,type2:_dtype)->_dtype
torch.__init__.q_per_channel_axis(self:Tensor)->_int
torch.__init__.q_per_channel_scales(self:Tensor)->Tensor
torch.__init__.q_per_channel_zero_points(self:Tensor)->Tensor
torch.__init__.q_scale(self:Tensor)->_float
torch.__init__.q_zero_point(self:Tensor)->_int
torch.__init__.qr(self:Tensor,some:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.qscheme
torch.__init__.quantize_per_channel(self:Tensor,scales:Tensor,zero_points:Tensor,axis:_int,dtype:_dtype)->Tensor
torch.__init__.quantize_per_tensor(self:Tensor,scale:_float,zero_point:_int,dtype:_dtype)->Tensor
torch.__init__.quantized_gru(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.quantized_gru(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.quantized_gru_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch.__init__.quantized_lstm(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool,*,dtype:Optional[_dtype]=None,use_dynamic:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.quantized_lstm_cell(input:Tensor,hx:Union[Tuple[Tensor,...],List[Tensor]],w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tuple[Tensor, Tensor]
torch.__init__.quantized_max_pool2d(self:Tensor,kernel_size:Union[_int,_size],stride:Union[_int,_size]=(),padding:Union[_int,_size]=0,dilation:Union[_int,_size]=1,ceil_mode:_bool=False)->Tensor
torch.__init__.quantized_rnn_relu_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch.__init__.quantized_rnn_tanh_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Tensor,b_hh:Tensor,packed_ih:Tensor,packed_hh:Tensor,col_offsets_ih:Tensor,col_offsets_hh:Tensor,scale_ih:Number,scale_hh:Number,zero_point_ih:Number,zero_point_hh:Number)->Tensor
torch.__init__.rand(*size:_int,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(*size:_int,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.rand_like(self:Tensor)->Tensor
torch.__init__.rand_like(self:Tensor,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint(high:_int,size:_size,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint(low:_int,high:_int,size:_size,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint_like(self:Tensor,high:_int)->Tensor
torch.__init__.randint_like(self:Tensor,high:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randint_like(self:Tensor,low:_int,high:_int)->Tensor
torch.__init__.randint_like(self:Tensor,low:_int,high:_int,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,generator:Generator,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randn_like(self:Tensor)->Tensor
torch.__init__.randn_like(self:Tensor,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randperm(n:_int,*,generator:Generator,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.randperm(n:_int,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.range(start:Number,end:Number,step:Number=1,*,out:Optional[Tensor]=None,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.reciprocal(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.reciprocal_(self:Tensor)->Tensor
torch.__init__.relu(self:Tensor)->Tensor
torch.__init__.relu_(self:Tensor)->Tensor
torch.__init__.remainder(self:Tensor,other:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.remainder(self:Tensor,other:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.renorm(self:Tensor,p:Number,dim:_int,maxnorm:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.repeat_interleave(repeats:Tensor)->Tensor
torch.__init__.repeat_interleave(self:Tensor,repeats:Tensor,dim:Optional[_int]=None)->Tensor
torch.__init__.repeat_interleave(self:Tensor,repeats:_int,dim:Optional[_int]=None)->Tensor
torch.__init__.reshape(self:Tensor,shape:_size)->Tensor
torch.__init__.resize_as_(self:Tensor,the_template:Tensor)->Tensor
torch.__init__.result_type(scalar1:Number,scalar2:Number)->_dtype
torch.__init__.result_type(scalar:Number,tensor:Tensor)->_dtype
torch.__init__.result_type(tensor:Tensor,other:Number)->_dtype
torch.__init__.result_type(tensor:Tensor,other:Tensor)->_dtype
torch.__init__.rfft(self:Tensor,signal_ndim:_int,normalized:_bool=False,onesided:_bool=True)->Tensor
torch.__init__.rnn_relu(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_relu(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_relu_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch.__init__.rnn_tanh(data:Tensor,batch_sizes:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_tanh(input:Tensor,hx:Tensor,params:Union[Tuple[Tensor,...],List[Tensor]],has_biases:_bool,num_layers:_int,dropout:_float,train:_bool,bidirectional:_bool,batch_first:_bool)->Tuple[Tensor, Tensor]
torch.__init__.rnn_tanh_cell(input:Tensor,hx:Tensor,w_ih:Tensor,w_hh:Tensor,b_ih:Optional[Tensor]=None,b_hh:Optional[Tensor]=None)->Tensor
torch.__init__.roll(self:Tensor,shifts:Union[_int,_size],dims:Union[_int,_size]=())->Tensor
torch.__init__.rot90(self:Tensor,k:_int=1,dims:_size=(0,1))->Tensor
torch.__init__.round(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.round_(self:Tensor)->Tensor
torch.__init__.rrelu(self:Tensor,lower:Number=0.125,upper:Number=0.3333333333333333,training:_bool=False,generator:Generator=None)->Tensor
torch.__init__.rrelu_(self:Tensor,lower:Number=0.125,upper:Number=0.3333333333333333,training:_bool=False,generator:Generator=None)->Tensor
torch.__init__.rsqrt(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.rsqrt_(self:Tensor)->Tensor
torch.__init__.rsub(self:Tensor,other:Number,alpha:Number=1)->Tensor
torch.__init__.rsub(self:Tensor,other:Tensor,*,alpha:Number=1)->Tensor
torch.__init__.scalar_tensor(s:Number,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.scatter(self:Tensor,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.scatter(self:Tensor,dim:Union[str,None],index:Tensor,value:Number)->Tensor
torch.__init__.scatter(self:Tensor,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.scatter(self:Tensor,dim:_int,index:Tensor,value:Number)->Tensor
torch.__init__.scatter_add(self:Tensor,dim:Union[str,None],index:Tensor,src:Tensor)->Tensor
torch.__init__.scatter_add(self:Tensor,dim:_int,index:Tensor,src:Tensor)->Tensor
torch.__init__.select(self:Tensor,dim:Union[str,None],index:_int)->Tensor
torch.__init__.select(self:Tensor,dim:_int,index:_int)->Tensor
torch.__init__.selu(self:Tensor)->Tensor
torch.__init__.selu_(self:Tensor)->Tensor
torch.__init__.set_flush_denormal(mode:_bool)->_bool
torch.__init__.set_num_interop_threads(num:_int)->None
torch.__init__.set_num_threads(num:_int)->None
torch.__init__.sigmoid(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sigmoid_(self:Tensor)->Tensor
torch.__init__.sign(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sin(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sin_(self:Tensor)->Tensor
torch.__init__.sinh(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sinh_(self:Tensor)->Tensor
torch.__init__.slogdet(self:Tensor)->Tuple[Tensor, Tensor]
torch.__init__.smm(self:Tensor,mat2:Tensor)->Tensor
torch.__init__.softmax(self:Tensor,dim:Union[str,None],*,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.softmax(self:Tensor,dim:_int,dtype:Optional[_dtype]=None)->Tensor
torch.__init__.solve(self:Tensor,A:Tensor,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.sort(self:Tensor,dim:Union[str,None],descending:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.sort(self:Tensor,dim:_int=-1,descending:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.sparse_coo_tensor(indices:Tensor,values:Union[Tensor,List],size:Optional[_size]=None,*,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.split_with_sizes(self:Tensor,split_sizes:_size,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.sqrt(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sqrt_(self:Tensor)->Tensor
torch.__init__.squeeze(self:Tensor)->Tensor
torch.__init__.squeeze(self:Tensor,dim:Union[str,None])->Tensor
torch.__init__.squeeze(self:Tensor,dim:_int)->Tensor
torch.__init__.sspaddmm(beta:Number,self:Tensor,alpha:Number,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.sspaddmm(beta:Number,self:Tensor,mat1:Tensor,mat2:Tensor)->Tensor
torch.__init__.sspaddmm(self:Tensor,mat1:Tensor,mat2:Tensor,*,beta:Number=1,alpha:Number=1,out:Optional[Tensor]=None)->Tensor
torch.__init__.stack(tensors:Union[Tuple[Tensor,...],List[Tensor]],dim:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std(self:Tensor,unbiased:_bool=True,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.std_mean(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.std_mean(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.std_mean(self:Tensor,unbiased:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.sub(input:Union[Tensor,Number],other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sub(input:Union[Tensor,Number],value:Number,other:Union[Tensor,Number],*,out:Optional[Tensor]=None)->Tensor
torch.__init__.sub(self:Tensor,alpha:Number,other:Tensor)->Tensor
torch.__init__.sub(self:Tensor,alpha:Number,other:Tensor,*,out:Tensor)->Tensor
torch.__init__.sum(self:Tensor,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.sum(self:Tensor,dim:List[Union[str,None]],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.sum(self:Tensor,dim:Union[_int,_size],keepdim:_bool=False,*,dtype:Optional[_dtype]=None,out:Optional[Tensor]=None)->Tensor
torch.__init__.svd(self:Tensor,some:_bool=True,compute_uv:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.symeig(self:Tensor,eigenvectors:_bool=False,upper:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.t(self:Tensor)->Tensor
torch.__init__.take(self:Tensor,index:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tan(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tan_(self:Tensor)->Tensor
torch.__init__.tanh(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tanh_(self:Tensor)->Tensor
torch.__init__.tensor(data:Any,dtype:Optional[_dtype]=None,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.threshold(self:Tensor,threshold:Number,value:Number,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.threshold_(self:Tensor,threshold:Number,value:Number)->Tensor
torch.__init__.topk(self:Tensor,k:_int,dim:_int=-1,largest:_bool=True,sorted:_bool=True,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.trace(self:Tensor)->Tensor
torch.__init__.transpose(self:Tensor,dim0:Union[str,None],dim1:Union[str,None])->Tensor
torch.__init__.transpose(self:Tensor,dim0:_int,dim1:_int)->Tensor
torch.__init__.trapz(y:Tensor,*,dx:_float=1,dim:_int=-1)->Tensor
torch.__init__.trapz(y:Tensor,x:Tensor,*,dim:_int=-1)->Tensor
torch.__init__.triangular_solve(self:Tensor,A:Tensor,upper:_bool=True,transpose:_bool=False,unitriangular:_bool=False,*,out:Optional[Tensor]=None)->Tuple[Tensor, Tensor]
torch.__init__.tril(self:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.tril_indices(row:_int,col:_int,offset:_int=0,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.triu(self:Tensor,diagonal:_int=0,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.triu_indices(row:_int,col:_int,offset:_int=0,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.trunc(self:Tensor,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.trunc_(self:Tensor)->Tensor
torch.__init__.unbind(self:Tensor,dim:Union[str,None])->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.unbind(self:Tensor,dim:_int=0)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.unique_dim(self:Tensor,dim:_int,sorted:_bool=True,return_inverse:_bool=False,return_counts:_bool=False)->Tuple[Tensor, Tensor, Tensor]
torch.__init__.unsqueeze(self:Tensor,dim:_int)->Tensor
torch.__init__.var(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.var(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.var(self:Tensor,unbiased:_bool=True,*,out:Optional[Tensor]=None)->Tensor
torch.__init__.var_mean(self:Tensor,dim:List[Union[str,None]],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.var_mean(self:Tensor,dim:Union[_int,_size],unbiased:_bool=True,keepdim:_bool=False)->Tuple[Tensor, Tensor]
torch.__init__.var_mean(self:Tensor,unbiased:_bool=True)->Tuple[Tensor, Tensor]
torch.__init__.where(condition:Tensor)->Union[Tuple[Tensor, ...], List[Tensor]]
torch.__init__.where(condition:Tensor,self:Tensor,other:Tensor)->Tensor
torch.__init__.zero_(self:Tensor)->Tensor
torch.__init__.zeros(*size:_int,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros(*size:_int,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros(size:_size,*,names:Optional[List[Union[str,None]]],out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros(size:_size,*,out:Optional[Tensor]=None,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor
torch.__init__.zeros_like(self:Tensor)->Tensor
torch.__init__.zeros_like(self:Tensor,*,dtype:_dtype=None,layout:layout=strided,device:Union[_device,str,None]=None,requires_grad:_bool=False)->Tensor


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/__config__.py----------------------------------------
torch.__config__.parallel_info()
torch.__config__.show()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/tensor.py----------------------------------------
A:torch.tensor.new_tensor->self.new()
A:torch.tensor.new_storage->self.storage().__deepcopy__(memo)
A:torch.tensor.self._backward_hooks->OrderedDict()
A:torch.tensor.handle->torch.utils.hooks.RemovableHandle(self._backward_hooks)
A:torch.tensor.detach->_add_docstr(_C._TensorBase.detach, '\n    Returns a new Tensor, detached from the current graph.\n\n    The result will never require gradient.\n\n    .. note::\n\n      Returned Tensor shares the same storage with the original one.\n      In-place modifications on either of them will be seen, and may trigger\n      errors in correctness checks.\n      IMPORTANT NOTE: Previously, in-place size / stride / storage changes\n      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor\n      also update the original tensor. Now, these in-place changes will not update the\n      original tensor anymore, and will instead trigger an error.\n      For sparse tensors:\n      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the\n      returned tensor will not update the original tensor anymore, and will instead\n      trigger an error.\n    ')
A:torch.tensor.detach_->_add_docstr(_C._TensorBase.detach_, '\n    Detaches the Tensor from the graph that created it, making it a leaf.\n    Views cannot be detached in-place.\n    ')
A:torch.tensor.weak_self->weakref.ref(self)
A:torch.tensor.var->weak_self()
A:torch.tensor.var._grad->grad.clone()
A:torch.tensor.(LU, pivots, infos)->torch._lu_with_info(self, pivot=pivot, check_errors=not get_infos)
A:torch.tensor.result->result.trunc().trunc()
A:torch.tensor.tensor_methods->dir(self.__class__)
A:torch.tensor.attrs->list(self.__dict__.keys())
A:torch.tensor.array->array.astype('uint8').astype('uint8')
A:torch.tensor.itemsize->self.storage().element_size()
A:torch.tensor.shape->tuple(self.shape)
A:torch.tensor.strides->tuple((s * itemsize for s in self.stride()))
A:torch.tensor.names->resolve_ellipsis(names, self.names, 'refine_names')
A:torch.tensor.(names, sizes)->unzip_namedshape(namedshape)
torch.Tensor(torch._C._TensorBase)
torch.Tensor.__array__(self,dtype=None)
torch.Tensor.__array_wrap__(self,array)
torch.Tensor.__contains__(self,element)
torch.Tensor.__cuda_array_interface__(self)
torch.Tensor.__deepcopy__(self,memo)
torch.Tensor.__dir__(self)
torch.Tensor.__floordiv__(self,other)
torch.Tensor.__format__(self,format_spec)
torch.Tensor.__hash__(self)
torch.Tensor.__ipow__(self,other)
torch.Tensor.__iter__(self)
torch.Tensor.__len__(self)
torch.Tensor.__rdiv__(self,other)
torch.Tensor.__reduce_ex__(self,proto)
torch.Tensor.__repr__(self)
torch.Tensor.__reversed__(self)
torch.Tensor.__rfloordiv__(self,other)
torch.Tensor.__rpow__(self,other)
torch.Tensor.__rsub__(self,other)
torch.Tensor.__setstate__(self,state)
torch.Tensor._update_names(self,names,inplace)
torch.Tensor.align_to(self,*names)
torch.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False)
torch.Tensor.is_shared(self)
torch.Tensor.lu(self,pivot=True,get_infos=False)
torch.Tensor.norm(self,p='fro',dim=None,keepdim=False,dtype=None)
torch.Tensor.refine_names(self,*names)
torch.Tensor.register_hook(self,hook)
torch.Tensor.reinforce(self,reward)
torch.Tensor.rename(self,*names,**rename_map)
torch.Tensor.rename_(self,*names,**rename_map)
torch.Tensor.resize(self,*sizes)
torch.Tensor.resize_as(self,tensor)
torch.Tensor.retain_grad(self)
torch.Tensor.share_memory_(self)
torch.Tensor.split(self,split_size,dim=0)
torch.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.Tensor.unflatten(self,dim,namedshape)
torch.Tensor.unique(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.Tensor.unique_consecutive(self,return_inverse=False,return_counts=False,dim=None)
torch.tensor.Tensor(torch._C._TensorBase)
torch.tensor.Tensor.__array__(self,dtype=None)
torch.tensor.Tensor.__array_wrap__(self,array)
torch.tensor.Tensor.__contains__(self,element)
torch.tensor.Tensor.__cuda_array_interface__(self)
torch.tensor.Tensor.__deepcopy__(self,memo)
torch.tensor.Tensor.__dir__(self)
torch.tensor.Tensor.__floordiv__(self,other)
torch.tensor.Tensor.__format__(self,format_spec)
torch.tensor.Tensor.__hash__(self)
torch.tensor.Tensor.__ipow__(self,other)
torch.tensor.Tensor.__iter__(self)
torch.tensor.Tensor.__len__(self)
torch.tensor.Tensor.__rdiv__(self,other)
torch.tensor.Tensor.__reduce_ex__(self,proto)
torch.tensor.Tensor.__repr__(self)
torch.tensor.Tensor.__reversed__(self)
torch.tensor.Tensor.__rfloordiv__(self,other)
torch.tensor.Tensor.__rpow__(self,other)
torch.tensor.Tensor.__rsub__(self,other)
torch.tensor.Tensor.__setstate__(self,state)
torch.tensor.Tensor._update_names(self,names,inplace)
torch.tensor.Tensor.align_to(self,*names)
torch.tensor.Tensor.backward(self,gradient=None,retain_graph=None,create_graph=False)
torch.tensor.Tensor.is_shared(self)
torch.tensor.Tensor.lu(self,pivot=True,get_infos=False)
torch.tensor.Tensor.norm(self,p='fro',dim=None,keepdim=False,dtype=None)
torch.tensor.Tensor.refine_names(self,*names)
torch.tensor.Tensor.register_hook(self,hook)
torch.tensor.Tensor.reinforce(self,reward)
torch.tensor.Tensor.rename(self,*names,**rename_map)
torch.tensor.Tensor.rename_(self,*names,**rename_map)
torch.tensor.Tensor.resize(self,*sizes)
torch.tensor.Tensor.resize_as(self,tensor)
torch.tensor.Tensor.retain_grad(self)
torch.tensor.Tensor.share_memory_(self)
torch.tensor.Tensor.split(self,split_size,dim=0)
torch.tensor.Tensor.stft(self,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)
torch.tensor.Tensor.unflatten(self,dim,namedshape)
torch.tensor.Tensor.unique(self,sorted=True,return_inverse=False,return_counts=False,dim=None)
torch.tensor.Tensor.unique_consecutive(self,return_inverse=False,return_counts=False,dim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_utils_internal.py----------------------------------------
A:torch._utils_internal.torch_parent->os.path.dirname(os.path.dirname(__file__))
A:torch._utils_internal.filename->inspect.getsourcefile(obj)
A:torch._utils_internal.(sourcelines, file_lineno)->inspect.getsourcelines(obj)
torch._utils_internal.get_file_path(*path_components)
torch._utils_internal.get_file_path_2(*path_components)
torch._utils_internal.get_source_lines_and_file(obj)
torch._utils_internal.get_writable_path(path)
torch._utils_internal.prepare_multiprocessing_environment(path)
torch._utils_internal.resolve_library_path(path)
torch.get_file_path(*path_components)
torch.get_file_path_2(*path_components)
torch.prepare_multiprocessing_environment(path)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_utils.py----------------------------------------
A:torch._utils.non_blocking->_get_async_or_non_blocking('cuda', non_blocking, kwargs)
A:torch._utils.dtype->_import_dotted_name(dtype)
A:torch._utils.new_module_name->_import_dotted_name(dtype).__module__.replace('.sparse', '')
A:torch._utils.new_values->torch._values(self).type(new_values_type_name, non_blocking)
A:torch._utils.new_indices->torch._indices(self).type(new_indices_type_name, non_blocking)
A:torch._utils.device->torch.cuda.current_device()
A:torch._utils.new_type->getattr(torch.cuda, self.__class__.__name__)
A:torch._utils.indices->torch._indices(tensor)
A:torch._utils.values->torch._values(tensor)
A:torch._utils.argument->list(kwargs.keys()).pop()
A:torch._utils.t->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype).type()
A:torch._utils.tensor->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype)
A:torch._utils.scales->torch.tensor(scales, dtype=torch.float64)
A:torch._utils.zero_points->torch.tensor(zero_points, dtype=torch.int64)
A:torch._utils.param->torch.nn.Parameter(data, requires_grad)
A:torch._utils.components->name.split('.')
A:torch._utils.obj->getattr(obj, component)
A:torch._utils.it->iter(iterable)
A:torch._utils.total->fn(total, element)
A:torch._utils.flat->torch.cat([t.contiguous().view(-1) for t in tensors], dim=0)
A:torch._utils.flat_indices->_flatten_dense_tensors([torch._indices(t) for t in tensors])
A:torch._utils.flat_values->_flatten_dense_tensors([torch._values(t) for t in tensors])
A:torch._utils.numel->torch._empty_per_channel_affine_quantized(size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype).numel()
A:torch._utils.type_dict->defaultdict(list)
A:torch._utils.buf_dict->defaultdict(lambda : [[], 0])
A:torch._utils.fun.__annotations__->dict(kwargs)
A:torch._utils.exc_info->sys.exc_info()
A:torch._utils.self.exc_msg->''.join(traceback.format_exception(*exc_info))
A:torch._utils.msg->KeyErrorMessage(msg)
torch._import_dotted_name(name)
torch._utils.ExceptionWrapper(self,exc_info=None,where='inbackground')
torch._utils.ExceptionWrapper.__init__(self,exc_info=None,where='inbackground')
torch._utils.ExceptionWrapper.reraise(self)
torch._utils.KeyErrorMessage(str)
torch._utils.KeyErrorMessage.__repr__(self)
torch._utils._accumulate(iterable,fn=lambdax,y:x+y)
torch._utils._cuda(self,device=None,non_blocking=False,**kwargs)
torch._utils._flatten_dense_tensors(tensors)
torch._utils._flatten_sparse_tensors(tensors)
torch._utils._get_async_or_non_blocking(function_name,non_blocking,kwargs)
torch._utils._import_dotted_name(name)
torch._utils._rebuild_parameter(data,requires_grad,backward_hooks)
torch._utils._rebuild_qtensor(storage,storage_offset,size,stride,quantizer_params,requires_grad,backward_hooks)
torch._utils._rebuild_tensor(storage,storage_offset,size,stride)
torch._utils._rebuild_tensor_v2(storage,storage_offset,size,stride,requires_grad,backward_hooks)
torch._utils._rebuild_xla_tensor(data,dtype,device,requires_grad)
torch._utils._reorder_tensors_as(tensors,ordered_tensors)
torch._utils._take_tensors(tensors,size_limit)
torch._utils._type(self,dtype=None,non_blocking=False,**kwargs)
torch._utils._unflatten_dense_tensors(flat,tensors)
torch._utils._unflatten_sparse_tensors(flat,tensors)
torch._utils.annotate(ret,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quasirandom.py----------------------------------------
A:torch.quasirandom.self.sobolstate->torch.zeros(dimension, self.MAXBIT, dtype=torch.long)
A:torch.quasirandom.g->torch.Generator()
A:torch.quasirandom.self.shift->torch.zeros(self.dimension, dtype=torch.long)
A:torch.quasirandom.ltm->torch.randint(2, (self.dimension, self.MAXBIT, self.MAXBIT), generator=g).tril()
A:torch.quasirandom.self.quasi->self.shift.clone()
A:torch.quasirandom.(result, self.quasi)->torch._sobol_engine_draw(self.quasi, n, self.sobolstate, self.dimension, self.num_generated, dtype=dtype)
torch.quasirandom.SobolEngine(self,dimension,scramble=False,seed=None)
torch.quasirandom.SobolEngine.__init__(self,dimension,scramble=False,seed=None)
torch.quasirandom.SobolEngine.__repr__(self)
torch.quasirandom.SobolEngine.draw(self,n=1,out=None,dtype=torch.float32)
torch.quasirandom.SobolEngine.fast_forward(self,n)
torch.quasirandom.SobolEngine.reset(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_six.py----------------------------------------
A:torch._six.inf->float('inf')
A:torch._six.nan->float('nan')
A:torch._six.exec_->getattr(builtins, 'exec')
A:torch._six.frame->sys._getframe(1)
A:torch._six.method->getattr(cls, name, None)
A:torch._six.t->type(obj)
torch._six.istuple(obj)
torch._six.with_metaclass(meta,*bases)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/version.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/hub.py----------------------------------------
A:torch.hub.HASH_REGEX->re.compile('-([a-f0-9]*)\\.')
A:torch.hub.spec->importlib.util.spec_from_file_location(name, path)
A:torch.hub.module->importlib.util.module_from_spec(spec)
A:torch.hub.torch_home->_get_torch_home()
A:torch.hub.hub_dir->os.path.join(torch_home, 'hub')
A:torch.hub.(repo_info, branch)->github.split(':')
A:torch.hub.(repo_owner, repo_name)->repo_info.split('/')
A:torch.hub.(repo_owner, repo_name, branch)->_parse_repo_info(github)
A:torch.hub.repo_dir->_get_cache_or_reload(github, force_reload, verbose)
A:torch.hub.cached_file->os.path.join(model_dir, extraced_name)
A:torch.hub.url->_git_archive_link(repo_owner, repo_name, branch)
A:torch.hub.extracted_repo->os.path.join(hub_dir, extraced_repo_name)
A:torch.hub.result->sys.path_importer_cache.get(item).find_module(name, [item])
A:torch.hub.importer->sys.path_importer_cache.get(item)
A:torch.hub.dependencies->_load_attr_from_module(m, VAR_DEPENDENCY)
A:torch.hub.func->_load_attr_from_module(m, model)
A:torch.hub.hub_module->import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)
A:torch.hub.entry->_load_entry_from_hubconf(hub_module, model)
A:torch.hub.force_reload->kwargs.get('force_reload', False)
A:torch.hub.verbose->kwargs.get('verbose', True)
A:torch.hub.model->entry(*args, **kwargs)
A:torch.hub.u->urlopen(url)
A:torch.hub.meta->urlopen(url).info()
A:torch.hub.content_length->urlopen(url).info().get_all('Content-Length')
A:torch.hub.file_size->int(content_length[0])
A:torch.hub.dst->os.path.expanduser(dst)
A:torch.hub.dst_dir->os.path.dirname(dst)
A:torch.hub.f->tempfile.NamedTemporaryFile(delete=False, dir=dst_dir)
A:torch.hub.sha256->hashlib.sha256()
A:torch.hub.buffer->urlopen(url).read(8192)
A:torch.hub.digest->hashlib.sha256().hexdigest()
A:torch.hub.model_dir->os.path.join(torch_home, 'checkpoints')
A:torch.hub.parts->urlparse(url)
A:torch.hub.filename->os.path.basename(parts.path)
A:torch.hub.members->cached_zipfile.infolist()
torch.hub._check_dependencies(m)
torch.hub._check_module_exists(name)
torch.hub._download_url_to_file(url,dst,hash_prefix=None,progress=True)
torch.hub._get_cache_or_reload(github,force_reload,verbose=True)
torch.hub._get_torch_home()
torch.hub._git_archive_link(repo_owner,repo_name,branch)
torch.hub._load_attr_from_module(module,func_name)
torch.hub._load_entry_from_hubconf(m,model)
torch.hub._parse_repo_info(github)
torch.hub._remove_if_exists(path)
torch.hub._setup_hubdir()
torch.hub.download_url_to_file(url,dst,hash_prefix=None,progress=True)
torch.hub.help(github,model,force_reload=False)
torch.hub.import_module(name,path)
torch.hub.list(github,force_reload=False)
torch.hub.load(github,model,*args,**kwargs)
torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)
torch.hub.set_dir(d)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_jit_internal.py----------------------------------------
A:torch._jit_internal.boolean_dispatched->weakref.WeakKeyDictionary()
A:torch._jit_internal.frame->inspect.currentframe()
A:torch._jit_internal.closure->get_closure(fn)
A:torch._jit_internal.drop_on_export->kwargs.pop('drop_on_export', None)
A:torch._jit_internal.item->getattr(mod, name)
A:torch._jit_internal.attr->get_torchscript_modifier(fn)
A:torch._jit_internal.mod->get_torchscript_modifier(fn)
A:torch._jit_internal.qual_name->_qualified_name(method)
A:torch._jit_internal.fn_overload_list->_overloaded_fns.get(qual_name)
A:torch._jit_internal.current_frame->inspect.currentframe()
A:torch._jit_internal.class_name_map->_overloaded_methods.get(qual_name, None)
A:torch._jit_internal.(class_name, line_no)->get_class_name_lineno(func)
A:torch._jit_internal.method_overloads->_overloaded_methods.get(qual_name, None).get(class_name, None)
A:torch._jit_internal.overloads->_overloaded_methods.get(qual_name, None).get(mod_class.__name__, None)
A:torch._jit_internal.args->getattr(ann, '__args__', ())
A:torch._jit_internal.Tuple->TupleCls()
A:torch._jit_internal.List->ListCls()
A:torch._jit_internal.Dict->DictCls()
A:torch._jit_internal.Optional->DictCls()
A:torch._jit_internal.Final->FinalCls()
A:torch._jit_internal.BroadcastingList1->BroadcastingListCls()
torch._jit_internal.BroadcastingListCls(object)
torch._jit_internal.BroadcastingListCls.__getitem__(self,types)
torch._jit_internal.FunctionModifiers(object)
torch._jit_internal._clear_fn_overloads(qual_name)
torch._jit_internal._get_fn_overloads(qual_name)
torch._jit_internal._get_overloaded_methods(method,mod_class)
torch._jit_internal._overload(func)
torch._jit_internal._overload_method(func)
torch._jit_internal._parameter_list(parameter_names_fn)
torch._jit_internal._qualified_name(obj)
torch._jit_internal.boolean_dispatch(arg_name,arg_index,default,if_true,if_false,module_name,func_name)
torch._jit_internal.can_compile_class(cls)
torch._jit_internal.createResolutionCallback(frames_up=0)
torch._jit_internal.createResolutionCallbackForClassMethods(cls)
torch._jit_internal.createResolutionCallbackFromClosure(fn)
torch._jit_internal.export(fn)
torch._jit_internal.get_class_name_lineno(method)
torch._jit_internal.get_closure(fn)
torch._jit_internal.get_torchscript_modifier(fn)
torch._jit_internal.ignore(drop=False,**kwargs)
torch._jit_internal.is_ignored_fn(fn)
torch._jit_internal.module_has_exports(mod)
torch._jit_internal.should_drop(fn)
torch._jit_internal.unused(fn)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_tensor_str.py----------------------------------------
A:torch._tensor_str.PRINT_OPTS->__PrinterOptions()
A:torch._tensor_str.tensor_view->tensor.reshape(-1)
A:torch._tensor_str.value_str->'{{:.{}f}}'.format(PRINT_OPTS.precision).format(value)
A:torch._tensor_str.self.max_width->max(self.max_width, len(value_str))
A:torch._tensor_str.nonzero_finite_vals->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
A:torch._tensor_str.nonzero_finite_abs->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double()
A:torch._tensor_str.nonzero_finite_min->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().min().double()
A:torch._tensor_str.nonzero_finite_max->torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)).abs().double().max().double()
A:torch._tensor_str.ret->'{}'.format(value)
A:torch._tensor_str.elements_per_line->max(1, int(math.floor((PRINT_OPTS.linewidth - indent) / element_length)))
A:torch._tensor_str.dim->self.float().dim()
A:torch._tensor_str.tensor_str->_tensor_str(self, indent)
A:torch._tensor_str.self->self.float().float()
A:torch._tensor_str.formatter->_Formatter(get_summarized_data(self) if summarize else self)
A:torch._tensor_str.suffix_len->len(suffix)
A:torch._tensor_str.indent->len(prefix)
A:torch._tensor_str.indices->self.float().float()._indices().detach()
A:torch._tensor_str.indices_str->_tensor_str(indices, indent + len(indices_prefix))
A:torch._tensor_str.values->self.float().float()._values().detach()
A:torch._tensor_str.values_str->_tensor_str(values, indent + len(values_prefix))
torch._tensor_str._Formatter(self,tensor)
torch._tensor_str._Formatter.__init__(self,tensor)
torch._tensor_str._Formatter.format(self,value)
torch._tensor_str._Formatter.width(self)
torch._tensor_str.__PrinterOptions(object)
torch._tensor_str._add_suffixes(tensor_str,suffixes,indent,force_newline)
torch._tensor_str._scalar_str(self,formatter)
torch._tensor_str._str(self)
torch._tensor_str._tensor_str(self,indent)
torch._tensor_str._tensor_str_with_formatter(self,indent,formatter,summarize)
torch._tensor_str._vector_str(self,indent,formatter,summarize)
torch._tensor_str.get_summarized_data(self)
torch._tensor_str.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)
torch.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/serialization.py----------------------------------------
A:torch.serialization.path->tempfile.mkdtemp()
A:torch.serialization.start->f.open('rb').tell()
A:torch.serialization.byte->f.open('rb').read(1)
A:torch.serialization.location->maybe_decode_ascii(location)
A:torch.serialization.device->validate_cuda_device(location)
A:torch.serialization.storage_type->normalize_storage_type(type(obj))
A:torch.serialization.result->pickle_module.Unpickler(f, **pickle_load_args).load()
A:torch.serialization.module->_import_dotted_name(storage_type.__module__)
A:torch.serialization.f->f.open('rb').open('rb')
A:torch.serialization.(source_lines, _, source_file)->get_source_lines_and_file(obj)
A:torch.serialization.source->''.join(obj)
A:torch.serialization.obj_key->str(obj._cdata)
A:torch.serialization.sys_info->dict(protocol_version=PROTOCOL_VERSION, little_endian=sys.byteorder == 'little', type_sizes=dict(short=SHORT_SIZE, int=INT_SIZE, long=LONG_SIZE))
A:torch.serialization.pickler->pickle_module.Pickler(f, protocol=pickle_protocol)
A:torch.serialization.serialized_storage_keys->sorted(serialized_storages.keys())
A:torch.serialization.current_source->''.join(get_source_lines_and_file(container_type)[0])
A:torch.serialization.diff->difflib.unified_diff(current_source.split('\n'), original_source.split('\n'), source_file, source_file, lineterm='')
A:torch.serialization.lines->'\n'.join(diff)
A:torch.serialization.file_size->f.open('rb').open('rb').seek(0, 2)
A:torch.serialization.msg->"source code of class '{}' has changed. {}".format(torch.typename(container_type), msg)
A:torch.serialization.num_storages->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.args->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.obj->data_type(size)
A:torch.serialization.storage_views->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.num_tensors->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.tensor_type->storage_to_tensor_type(storage)
A:torch.serialization.(ndim,)->struct.unpack('<i', f.read(4))
A:torch.serialization.size->struct.unpack('<{}q'.format(ndim), f.read(8 * ndim))
A:torch.serialization.stride->struct.unpack('<{}q'.format(ndim), f.read(8 * ndim))
A:torch.serialization.(storage_offset,)->struct.unpack('<q', f.read(8))
A:torch.serialization.tensor->tensor_type().set_(storage, storage_offset, size, stride)
A:torch.serialization.pickle_file->tar.extractfile('pickle')
A:torch.serialization.unpickler->pickle_module.Unpickler(f, **pickle_load_args)
A:torch.serialization.typename->maybe_decode_ascii(saved_id[0])
A:torch.serialization.deserialized_objects[root_key]->restore_location(obj, location)
A:torch.serialization.f_should_read_directly->_should_read_directly(f)
A:torch.serialization.magic_number->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.protocol_version->pickle_module.load(f, **pickle_load_args)
A:torch.serialization._sys_info->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.deserialized_storage_keys->pickle_module.load(f, **pickle_load_args)
A:torch.serialization.offset->f.open('rb').open('rb').tell()
torch.load(f,map_location=None,pickle_module=pickle,**pickle_load_args)
torch.save(obj,f,pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL)
torch.serialization.SourceChangeWarning(Warning)
torch.serialization._check_seekable(f)
torch.serialization._cpu_deserialize(obj,location)
torch.serialization._cpu_tag(obj)
torch.serialization._cuda_deserialize(obj,location)
torch.serialization._cuda_tag(obj)
torch.serialization._is_compressed_file(f)
torch.serialization._is_zipfile(f)
torch.serialization._load(f,map_location,pickle_module,**pickle_load_args)
torch.serialization._save(obj,f,pickle_module,pickle_protocol)
torch.serialization._should_read_directly(f)
torch.serialization._with_file_like(f,mode,body)
torch.serialization.default_restore_location(storage,location)
torch.serialization.load(f,map_location=None,pickle_module=pickle,**pickle_load_args)
torch.serialization.location_tag(storage)
torch.serialization.mkdtemp()
torch.serialization.normalize_storage_type(storage_type)
torch.serialization.register_package(priority,tagger,deserializer)
torch.serialization.save(obj,f,pickle_module=pickle,pickle_protocol=DEFAULT_PROTOCOL)
torch.serialization.storage_to_tensor_type(storage)
torch.serialization.validate_cuda_device(location)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/__future__.py----------------------------------------
torch.__future__.get_overwrite_module_params_on_conversion()
torch.__future__.set_overwrite_module_params_on_conversion(value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_torch_docs.py----------------------------------------
A:torch._torch_docs.regx->re.compile('\\n\\s{4}(?!\\s)')
A:torch._torch_docs.common_args->parse_kwargs('\n    input (Tensor): the input tensor.\n    out (Tensor, optional): the output tensor.\n')
A:torch._torch_docs.reduceops_common_args->merge_dicts(common_args, parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n'))
A:torch._torch_docs.multi_dim_common->merge_dicts(reduceops_common_args, parse_kwargs('\n    dim (int or tuple of ints): the dimension or dimensions to reduce.\n'), {'keepdim_details': '\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n'})
A:torch._torch_docs.single_dim_common->merge_dicts(reduceops_common_args, parse_kwargs('\n    dim (int): the dimension to reduce.\n'), {'keepdim_details': 'If :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\nthe output tensor having 1 fewer dimension than :attr:`input`.'})
A:torch._torch_docs.factory_common_args->merge_dicts(common_args, parse_kwargs('\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n'))
A:torch._torch_docs.factory_like_common_args->parse_kwargs('\n    input (Tensor): the size of :attr:`input` will determine size of the output tensor.\n    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n        Default: if ``None``, defaults to the layout of :attr:`input`.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n        Default: if ``None``, defaults to the dtype of :attr:`input`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, defaults to the device of :attr:`input`.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
A:torch._torch_docs.factory_data_common_args->parse_kwargs('\n    data (array_like): Initial data for the tensor. Can be a list, tuple,\n        NumPy ``ndarray``, scalar, and other types.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, infers data type from :attr:`data`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
torch._torch_docs.merge_dicts(*dicts)
torch._torch_docs.parse_kwargs(desc)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_storage_docs.py----------------------------------------
A:torch._storage_docs.cls->getattr(torch._C, cls_name)
torch._storage_docs.add_docstr_all(method,docstr)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/storage.py----------------------------------------
A:torch.storage.memo->memo.setdefault('torch', {}).setdefault('torch', {})
A:torch.storage.new_storage->self.clone()
A:torch.storage.b->io.BytesIO()
A:torch.storage.allocator->torch.cuda._host_allocator()
torch._StorageBase(object)
torch._StorageBase.__copy__(self)
torch._StorageBase.__deepcopy__(self,memo)
torch._StorageBase.__iter__(self)
torch._StorageBase.__reduce__(self)
torch._StorageBase.__repr__(self)
torch._StorageBase.__sizeof__(self)
torch._StorageBase.__str__(self)
torch._StorageBase._new_shared(cls,size)
torch._StorageBase.bfloat16(self)
torch._StorageBase.bool(self)
torch._StorageBase.byte(self)
torch._StorageBase.char(self)
torch._StorageBase.clone(self)
torch._StorageBase.cpu(self)
torch._StorageBase.double(self)
torch._StorageBase.float(self)
torch._StorageBase.half(self)
torch._StorageBase.int(self)
torch._StorageBase.long(self)
torch._StorageBase.pin_memory(self)
torch._StorageBase.share_memory_(self)
torch._StorageBase.short(self)
torch._StorageBase.tolist(self)
torch.storage._StorageBase(object)
torch.storage._StorageBase.__copy__(self)
torch.storage._StorageBase.__deepcopy__(self,memo)
torch.storage._StorageBase.__iter__(self)
torch.storage._StorageBase.__reduce__(self)
torch.storage._StorageBase.__repr__(self)
torch.storage._StorageBase.__sizeof__(self)
torch.storage._StorageBase.__str__(self)
torch.storage._StorageBase._new_shared(cls,size)
torch.storage._StorageBase.bfloat16(self)
torch.storage._StorageBase.bool(self)
torch.storage._StorageBase.byte(self)
torch.storage._StorageBase.char(self)
torch.storage._StorageBase.clone(self)
torch.storage._StorageBase.cpu(self)
torch.storage._StorageBase.double(self)
torch.storage._StorageBase.float(self)
torch.storage._StorageBase.half(self)
torch.storage._StorageBase.int(self)
torch.storage._StorageBase.long(self)
torch.storage._StorageBase.pin_memory(self)
torch.storage._StorageBase.share_memory_(self)
torch.storage._StorageBase.short(self)
torch.storage._StorageBase.tolist(self)
torch.storage._load_from_bytes(b)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_namedtensor_internals.py----------------------------------------
A:torch._namedtensor_internals.namedshape->namedshape.items().items()
A:torch._namedtensor_internals.globbed_names->expand_single_ellipsis(ellipsis_idx, len(names) - ellipsis_idx - 1, tensor_names)
A:torch._namedtensor_internals.desired_ordering_set->set(names)
A:torch._namedtensor_internals.missing_names->tuple([name for name in tensor_names if name not in desired_ordering_set])
A:torch._namedtensor_internals.dim_map->build_dim_map(tensor)
A:torch._namedtensor_internals.has_rename_pairs->bool(rename_map)
torch._namedtensor_internals.assert_namedtensor_build(api_name)
torch._namedtensor_internals.build_dim_map(tensor)
torch._namedtensor_internals.check_serializing_named_tensor(tensor)
torch._namedtensor_internals.expand_single_ellipsis(numel_pre_glob,numel_post_glob,names)
torch._namedtensor_internals.is_ellipsis(item)
torch._namedtensor_internals.namer_api_name(inplace)
torch._namedtensor_internals.replace_ellipsis_by_position(ellipsis_idx,names,tensor_names)
torch._namedtensor_internals.replace_ellipsis_with_missing_names(ellipsis_idx,names,tensor_names,fn_name)
torch._namedtensor_internals.resolve_ellipsis(names,tensor_names,fn_name,is_positional=True)
torch._namedtensor_internals.unzip_namedshape(namedshape)
torch._namedtensor_internals.update_names(tensor,names,rename_map,inplace)
torch._namedtensor_internals.update_names_with_list(tensor,names,inplace)
torch._namedtensor_internals.update_names_with_mapping(tensor,rename_map,inplace)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/random.py----------------------------------------
A:torch.random.seed->torch._C.default_generator.seed()
A:torch.random.num_devices->torch.cuda.device_count()
A:torch.random.devices->list(devices)
A:torch.random.cpu_rng_state->torch.get_rng_state()
torch.get_rng_state()
torch.initial_seed()
torch.manual_seed(seed)
torch.random.fork_rng(devices=None,enabled=True,_caller='fork_rng',_devices_kw='devices')
torch.random.get_rng_state()
torch.random.initial_seed()
torch.random.manual_seed(seed)
torch.random.seed()
torch.random.set_rng_state(new_state)
torch.seed()
torch.set_rng_state(new_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_tensor_docs.py----------------------------------------
A:torch._tensor_docs.new_common_args->parse_kwargs('\n    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n        shape of the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n        Default: if None, same :class:`torch.dtype` as this tensor.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if None, same :class:`torch.device` as this tensor.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n')
torch._tensor_docs.add_docstr_all(method,docstr)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_classes.py----------------------------------------
A:torch._classes.classes->_Classes()
torch._classes._Classes(self)
torch._classes._Classes.__init__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/_ops.py----------------------------------------
A:torch._ops.old_flags->sys.getdlopenflags()
A:torch._ops.qualified_op_name->'{}::{}'.format(self.name, op_name)
A:torch._ops.op->torch._C._jit_get_operation(qualified_op_name)
A:torch._ops.__file__->os.path.join(os.path.dirname(__file__), '_ops.py')
A:torch._ops.self.loaded_libraries->set()
A:torch._ops.namespace->_OpNamespace(name)
A:torch._ops.path->torch._utils_internal.resolve_library_path(path)
A:torch._ops.ops->_Ops()
torch._ops._OpNamespace(self,name)
torch._ops._OpNamespace.__getattr__(self,op_name)
torch._ops._OpNamespace.__init__(self,name)
torch._ops._Ops(self)
torch._ops._Ops.__getattr__(self,name)
torch._ops._Ops.__init__(self)
torch._ops._Ops.load_library(self,path)
torch._ops.dl_open_guard()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/__init__.py----------------------------------------
torch.backends.__init__.ContextProp(self,getter,setter)
torch.backends.__init__.ContextProp.__get__(self,obj,objtype)
torch.backends.__init__.ContextProp.__init__(self,getter,setter)
torch.backends.__init__.ContextProp.__set__(self,obj,val)
torch.backends.__init__.PropModule(self,m,name)
torch.backends.__init__.PropModule.__getattr__(self,attr)
torch.backends.__init__.PropModule.__init__(self,m,name)
torch.backends.__init__.__allow_nonbracketed_mutation()
torch.backends.__init__.disable_global_flags()
torch.backends.__init__.flags_frozen()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/quantized/__init__.py----------------------------------------
A:torch.backends.quantized.__init__.qengines->torch._C._supported_qengines()
A:torch.backends.quantized.__init__.engine->_QEngineProp()
A:torch.backends.quantized.__init__.supported_engines->_SupportedQEnginesProp()
A:torch.backends.quantized.__init__.sys.modules[__name__]->QuantizedEngine(sys.modules[__name__], __name__)
torch.backends.quantized.__init__.QuantizedEngine(self,m,name)
torch.backends.quantized.__init__.QuantizedEngine.__getattr__(self,attr)
torch.backends.quantized.__init__.QuantizedEngine.__init__(self,m,name)
torch.backends.quantized.__init__._QEngineProp(object)
torch.backends.quantized.__init__._QEngineProp.__get__(self,obj,objtype)
torch.backends.quantized.__init__._QEngineProp.__set__(self,obj,val)
torch.backends.quantized.__init__._SupportedQEnginesProp(object)
torch.backends.quantized.__init__._SupportedQEnginesProp.__get__(self,obj,objtype)
torch.backends.quantized.__init__._SupportedQEnginesProp.__set__(self,obj,val)
torch.backends.quantized.__init__._get_qengine_id(qengine)
torch.backends.quantized.__init__._get_qengine_str(qengine)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/openmp/__init__.py----------------------------------------
torch.backends.openmp.__init__.is_available()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/mkl/__init__.py----------------------------------------
torch.backends.mkl.__init__.is_available()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/cuda/__init__.py----------------------------------------
A:torch.backends.cuda.__init__.size->cuFFTPlanCacheAttrContextProp(torch._cufft_get_plan_cache_size, '.size is a read-only property showing the number of plans currently in the cache. To change the cache capacity, set cufft_plan_cache.max_size.')
A:torch.backends.cuda.__init__.max_size->cuFFTPlanCacheAttrContextProp(torch._cufft_get_plan_cache_max_size, torch._cufft_set_plan_cache_max_size)
A:torch.backends.cuda.__init__.index->torch.cuda._utils._get_device_index(device)
A:torch.backends.cuda.__init__.cufft_plan_cache->cuFFTPlanCacheManager()
A:torch.backends.cuda.__init__.sys.modules[__name__]->CUDAModule(sys.modules[__name__])
torch.backends.cuda.__init__.CUDAModule(self,m)
torch.backends.cuda.__init__.CUDAModule.__init__(self,m)
torch.backends.cuda.__init__.cuFFTPlanCache(self,device_index)
torch.backends.cuda.__init__.cuFFTPlanCache.__init__(self,device_index)
torch.backends.cuda.__init__.cuFFTPlanCache.clear(self)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp(self,getter,setter)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__get__(self,obj,objtype)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__init__(self,getter,setter)
torch.backends.cuda.__init__.cuFFTPlanCacheAttrContextProp.__set__(self,obj,val)
torch.backends.cuda.__init__.cuFFTPlanCacheManager(self)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__getattr__(self,name)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__getitem__(self,device)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__init__(self)
torch.backends.cuda.__init__.cuFFTPlanCacheManager.__setattr__(self,name,value)
torch.backends.cuda.__init__.is_built()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/mkldnn/__init__.py----------------------------------------
A:torch.backends.mkldnn.__init__.orig_flags->set_flags(enabled)
A:torch.backends.mkldnn.__init__.enabled->ContextProp(torch._C._get_mkldnn_enabled, torch._C._set_mkldnn_enabled)
A:torch.backends.mkldnn.__init__.sys.modules[__name__]->MkldnnModule(sys.modules[__name__], __name__)
torch.backends.mkldnn.__init__.MkldnnModule(self,m,name)
torch.backends.mkldnn.__init__.MkldnnModule.__init__(self,m,name)
torch.backends.mkldnn.__init__.flags(enabled=False)
torch.backends.mkldnn.__init__.is_available()
torch.backends.mkldnn.__init__.set_flags(_enabled)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py----------------------------------------
A:torch.backends.cudnn.rnn.dropout_state[dropout_desc_name]->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda')))
A:torch.backends.cudnn.rnn.dropout_ts->Unserializable(torch._cudnn_init_dropout_state(dropout_p, train, dropout_seed, self_ty=torch.uint8, device=torch.device('cuda'))).get()
torch.backends.cudnn.rnn.Unserializable(self,inner)
torch.backends.cudnn.rnn.Unserializable.__getstate__(self)
torch.backends.cudnn.rnn.Unserializable.__init__(self,inner)
torch.backends.cudnn.rnn.Unserializable.__setstate__(self,state)
torch.backends.cudnn.rnn.Unserializable.get(self)
torch.backends.cudnn.rnn.get_cudnn_mode(mode)
torch.backends.cudnn.rnn.init_dropout_state(dropout,train,dropout_seed,dropout_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py----------------------------------------
A:torch.backends.cudnn.__init__.test_env->os.environ.copy()
A:torch.backends.cudnn.__init__.th_dll_path->os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'lib')
A:torch.backends.cudnn.__init__.test_env['PATH']->';'.join([th_dll_path, old_path])
A:torch.backends.cudnn.__init__.proc->Popen(['where', 'cudnn64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE, env=test_env)
A:torch.backends.cudnn.__init__.(out, err)->Popen(['where', 'cudnn64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE, env=test_env).communicate()
A:torch.backends.cudnn.__init__.out->out.decode().strip().decode().strip()
A:torch.backends.cudnn.__init__.cudnn_lib_name->os.path.basename(out)
A:torch.backends.cudnn.__init__.cudnn_lib->str(cudnn_lib)
A:torch.backends.cudnn.__init__.lib->ctypes.cdll.LoadLibrary(None)
A:torch.backends.cudnn.__init__.__cudnn_version->ctypes.cdll.LoadLibrary(None).cudnnGetVersion()
A:torch.backends.cudnn.__init__.compile_version->torch._C._cudnn_version()
A:torch.backends.cudnn.__init__.orig_flags->set_flags(enabled, benchmark, deterministic, verbose)
A:torch.backends.cudnn.__init__.ptr->ctypes.c_void_p()
A:torch.backends.cudnn.__init__.msg->'{}: {}'.format(status, get_error_string(status))
A:torch.backends.cudnn.__init__.self._type->tensor.view(padded_size).type()
A:torch.backends.cudnn.__init__.self._size->weight.size()
A:torch.backends.cudnn.__init__.self._stride->tensor.view(padded_size).stride()
A:torch.backends.cudnn.__init__.self.ptrs->(ctypes.c_void_p * N)()
A:torch.backends.cudnn.__init__._ndim->tensor.view(padded_size).dim()
A:torch.backends.cudnn.__init__._size->int_array(tensor.size() + dim_pad)
A:torch.backends.cudnn.__init__._stride->int_array(tensor.stride() + dim_pad)
A:torch.backends.cudnn.__init__.dropout_states_size->ctypes.c_long()
A:torch.backends.cudnn.__init__.self.state->torch.cuda.ByteTensor(dropout_states_size.value)
A:torch.backends.cudnn.__init__.state_ptr->self.state.data_ptr()
A:torch.backends.cudnn.__init__.state_size->self.state.size(0)
A:torch.backends.cudnn.__init__.current_device->torch.cuda.current_device()
A:torch.backends.cudnn.__init__.handle->CuDNNHandle()
A:torch.backends.cudnn.__init__.tensor->tensor.view(padded_size).view(padded_size)
A:torch.backends.cudnn.__init__.descriptor->TensorDescriptor()
A:torch.backends.cudnn.__init__.descriptors->TensorDescriptorArray(len(batch_sizes))
A:torch.backends.cudnn.__init__.enabled->ContextProp(torch._C._get_cudnn_enabled, torch._C._set_cudnn_enabled)
A:torch.backends.cudnn.__init__.deterministic->ContextProp(torch._C._get_cudnn_deterministic, torch._C._set_cudnn_deterministic)
A:torch.backends.cudnn.__init__.benchmark->ContextProp(torch._C._get_cudnn_benchmark, torch._C._set_cudnn_benchmark)
A:torch.backends.cudnn.__init__.sys.modules[__name__]->CudnnModule(sys.modules[__name__], __name__)
torch.backends.cudnn.__init__.CuDNNError(self,status)
torch.backends.cudnn.__init__.CuDNNError.__init__(self,status)
torch.backends.cudnn.__init__.CuDNNHandle(self)
torch.backends.cudnn.__init__.CuDNNHandle.__del__(self)
torch.backends.cudnn.__init__.CuDNNHandle.__init__(self)
torch.backends.cudnn.__init__.CudnnModule(self,m,name)
torch.backends.cudnn.__init__.CudnnModule.__init__(self,m,name)
torch.backends.cudnn.__init__.DropoutDescriptor(self,handle,dropout,seed)
torch.backends.cudnn.__init__.DropoutDescriptor.__del__(self)
torch.backends.cudnn.__init__.DropoutDescriptor.__init__(self,handle,dropout,seed)
torch.backends.cudnn.__init__.DropoutDescriptor._set(self,dropout,seed)
torch.backends.cudnn.__init__.DropoutDescriptor.set_dropout(self,dropout,seed)
torch.backends.cudnn.__init__.FilterDescriptor(self)
torch.backends.cudnn.__init__.FilterDescriptor.__del__(self)
torch.backends.cudnn.__init__.FilterDescriptor.__init__(self)
torch.backends.cudnn.__init__.FilterDescriptor.as_tuple(self)
torch.backends.cudnn.__init__.FilterDescriptor.set(self,weight)
torch.backends.cudnn.__init__.RNNDescriptor(self,handle,hidden_size,num_layers,dropout_desc,input_mode,bidirectional,mode,datatype)
torch.backends.cudnn.__init__.RNNDescriptor.__del__(self)
torch.backends.cudnn.__init__.RNNDescriptor.__init__(self,handle,hidden_size,num_layers,dropout_desc,input_mode,bidirectional,mode,datatype)
torch.backends.cudnn.__init__.TensorDescriptor(self)
torch.backends.cudnn.__init__.TensorDescriptor.__del__(self)
torch.backends.cudnn.__init__.TensorDescriptor.__init__(self)
torch.backends.cudnn.__init__.TensorDescriptor.as_tuple(self)
torch.backends.cudnn.__init__.TensorDescriptor.set(self,tensor)
torch.backends.cudnn.__init__.TensorDescriptorArray(self,N)
torch.backends.cudnn.__init__.TensorDescriptorArray.__del__(self)
torch.backends.cudnn.__init__.TensorDescriptorArray.__getitem__(self,key)
torch.backends.cudnn.__init__.TensorDescriptorArray.__init__(self,N)
torch.backends.cudnn.__init__.TensorDescriptorArray.set_all(self,tensor)
torch.backends.cudnn.__init__.TensorDescriptorArray.set_raw(self,i,_type,_ndim,_size,_stride)
torch.backends.cudnn.__init__._libcudnn()
torch.backends.cudnn.__init__.add_tensor(*args)
torch.backends.cudnn.__init__.c_type(tensor)
torch.backends.cudnn.__init__.check_error(status)
torch.backends.cudnn.__init__.descriptor(tensor,N=None)
torch.backends.cudnn.__init__.descriptor_sequence(tensor,batch_sizes)
torch.backends.cudnn.__init__.find_cudnn_windows_lib()
torch.backends.cudnn.__init__.flags(enabled=False,benchmark=False,deterministic=False,verbose=False)
torch.backends.cudnn.__init__.get_error_string(status)
torch.backends.cudnn.__init__.get_handle()
torch.backends.cudnn.__init__.int_array(itr)
torch.backends.cudnn.__init__.is_acceptable(tensor)
torch.backends.cudnn.__init__.is_available()
torch.backends.cudnn.__init__.set_flags(_enabled,_benchmark,_deterministic,_verbose)
torch.backends.cudnn.__init__.version()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/__init__.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/optimizer.py----------------------------------------
A:torch.optim.optimizer.required->_RequiredParameter()
A:torch.optim.optimizer.self.state->defaultdict(dict)
A:torch.optim.optimizer.param_groups->list(params)
A:torch.optim.optimizer.state_dict->deepcopy(state_dict)
A:torch.optim.optimizer.value->value.to(param.device).to(param.device)
A:torch.optim.optimizer.state->defaultdict(dict)
A:torch.optim.optimizer.state[param]->cast(param, v)
A:torch.optim.optimizer.param_group['params']->list(params)
A:torch.optim.optimizer.param_set->set()
torch.optim.Optimizer(self,params,defaults)
torch.optim.Optimizer.__getstate__(self)
torch.optim.Optimizer.__repr__(self)
torch.optim.Optimizer.__setstate__(self,state)
torch.optim.Optimizer.add_param_group(self,param_group)
torch.optim.Optimizer.load_state_dict(self,state_dict)
torch.optim.Optimizer.state_dict(self)
torch.optim.Optimizer.step(self,closure)
torch.optim.Optimizer.zero_grad(self)
torch.optim.optimizer.Optimizer(self,params,defaults)
torch.optim.optimizer.Optimizer.__getstate__(self)
torch.optim.optimizer.Optimizer.__init__(self,params,defaults)
torch.optim.optimizer.Optimizer.__repr__(self)
torch.optim.optimizer.Optimizer.__setstate__(self,state)
torch.optim.optimizer.Optimizer.add_param_group(self,param_group)
torch.optim.optimizer.Optimizer.load_state_dict(self,state_dict)
torch.optim.optimizer.Optimizer.state_dict(self)
torch.optim.optimizer.Optimizer.step(self,closure)
torch.optim.optimizer.Optimizer.zero_grad(self)
torch.optim.optimizer._RequiredParameter(object)
torch.optim.optimizer._RequiredParameter.__repr__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/optimizer.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/lr_scheduler.py----------------------------------------
A:torch.optim.lr_scheduler.self.base_lrs->list(map(lambda group: group['initial_lr'], optimizer.param_groups))
A:torch.optim.lr_scheduler.instance_ref->weakref.ref(method.__self__)
A:torch.optim.lr_scheduler.instance->instance_ref()
A:torch.optim.lr_scheduler.wrapped->func.__get__(instance, cls)
A:torch.optim.lr_scheduler.self.optimizer.step->with_counter(self.optimizer.step)
A:torch.optim.lr_scheduler.self.lr_lambdas->list(lr_lambda)
A:torch.optim.lr_scheduler.state_dict['lr_lambdas'][idx]->fn.__dict__.copy()
A:torch.optim.lr_scheduler.lr_lambdas->state_dict.pop('lr_lambdas')
A:torch.optim.lr_scheduler.self.min_lrs->list(min_lr)
A:torch.optim.lr_scheduler.current->float(metrics)
A:torch.optim.lr_scheduler.old_lr->float(param_group['lr'])
A:torch.optim.lr_scheduler.new_lr->max(old_lr * self.factor, self.min_lrs[i])
A:torch.optim.lr_scheduler.base_lrs->self._format_param('base_lr', optimizer, base_lr)
A:torch.optim.lr_scheduler.self.max_lrs->self._format_param('max_lr', optimizer, max_lr)
A:torch.optim.lr_scheduler.step_size_up->float(step_size_up)
A:torch.optim.lr_scheduler.base_momentums->self._format_param('base_momentum', optimizer, base_momentum)
A:torch.optim.lr_scheduler.self.base_momentums->list(map(lambda group: group['momentum'], optimizer.param_groups))
A:torch.optim.lr_scheduler.self.max_momentums->self._format_param('max_momentum', optimizer, max_momentum)
A:torch.optim.lr_scheduler.cycle->math.floor(1 + self.last_epoch / self.total_size)
A:torch.optim.lr_scheduler.n->int(math.log(epoch / self.T_0 * (self.T_mult - 1) + 1, self.T_mult))
A:torch.optim.lr_scheduler.self.last_epoch->math.floor(epoch)
A:torch.optim.lr_scheduler.max_lrs->self._format_param('max_lr', self.optimizer, max_lr)
A:torch.optim.lr_scheduler.max_momentums->self._format_param('max_momentum', optimizer, max_momentum)
A:torch.optim.lr_scheduler.computed_lr->self.anneal_func(group['max_lr'], group['min_lr'], down_step_num / self.step_size_down)
A:torch.optim.lr_scheduler.computed_momentum->self.anneal_func(group['base_momentum'], group['max_momentum'], down_step_num / self.step_size_down)
torch.optim.lr_scheduler.CosineAnnealingLR(self,optimizer,T_max,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingLR.__init__(self,optimizer,T_max,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingLR.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self,optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.__init__(self,optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_lr(self)
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step(self,epoch=None)
torch.optim.lr_scheduler.CyclicLR(self,optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1)
torch.optim.lr_scheduler.CyclicLR.__init__(self,optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1)
torch.optim.lr_scheduler.CyclicLR._exp_range_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR._format_param(self,name,optimizer,param)
torch.optim.lr_scheduler.CyclicLR._triangular2_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR._triangular_scale_fn(self,x)
torch.optim.lr_scheduler.CyclicLR.get_lr(self)
torch.optim.lr_scheduler.ExponentialLR(self,optimizer,gamma,last_epoch=-1)
torch.optim.lr_scheduler.ExponentialLR.__init__(self,optimizer,gamma,last_epoch=-1)
torch.optim.lr_scheduler.ExponentialLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.LambdaLR.__init__(self,optimizer,lr_lambda,last_epoch=-1)
torch.optim.lr_scheduler.LambdaLR.get_lr(self)
torch.optim.lr_scheduler.LambdaLR.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.LambdaLR.state_dict(self)
torch.optim.lr_scheduler.MultiStepLR(self,optimizer,milestones,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.MultiStepLR.__init__(self,optimizer,milestones,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.MultiStepLR.get_lr(self)
torch.optim.lr_scheduler.OneCycleLR(self,optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,last_epoch=-1)
torch.optim.lr_scheduler.OneCycleLR.__init__(self,optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,last_epoch=-1)
torch.optim.lr_scheduler.OneCycleLR._annealing_cos(self,start,end,pct)
torch.optim.lr_scheduler.OneCycleLR._annealing_linear(self,start,end,pct)
torch.optim.lr_scheduler.OneCycleLR._format_param(self,name,optimizer,param)
torch.optim.lr_scheduler.OneCycleLR.get_lr(self)
torch.optim.lr_scheduler.ReduceLROnPlateau(self,optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)
torch.optim.lr_scheduler.ReduceLROnPlateau.__init__(self,optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)
torch.optim.lr_scheduler.ReduceLROnPlateau._init_is_better(self,mode,threshold,threshold_mode)
torch.optim.lr_scheduler.ReduceLROnPlateau._reduce_lr(self,epoch)
torch.optim.lr_scheduler.ReduceLROnPlateau._reset(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.in_cooldown(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.is_better(self,a,best)
torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict(self,state_dict)
torch.optim.lr_scheduler.ReduceLROnPlateau.state_dict(self)
torch.optim.lr_scheduler.ReduceLROnPlateau.step(self,metrics,epoch=None)
torch.optim.lr_scheduler.StepLR(self,optimizer,step_size,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.StepLR.__init__(self,optimizer,step_size,gamma=0.1,last_epoch=-1)
torch.optim.lr_scheduler.StepLR.get_lr(self)
torch.optim.lr_scheduler._LRScheduler(self,optimizer,last_epoch=-1)
torch.optim.lr_scheduler._LRScheduler.__init__(self,optimizer,last_epoch=-1)
torch.optim.lr_scheduler._LRScheduler.get_lr(self)
torch.optim.lr_scheduler._LRScheduler.load_state_dict(self,state_dict)
torch.optim.lr_scheduler._LRScheduler.state_dict(self)
torch.optim.lr_scheduler._LRScheduler.step(self,epoch=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/lr_scheduler.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/rprop.py----------------------------------------
A:torch.optim.rprop.defaults->dict(lr=lr, etas=etas, step_sizes=step_sizes)
A:torch.optim.rprop.loss->closure()
A:torch.optim.rprop.state['prev']->torch.zeros_like(p.data)
A:torch.optim.rprop.state['step_size']->grad.clone().new().resize_as_(grad).fill_(group['lr'])
A:torch.optim.rprop.sign->grad.clone().mul(state['prev']).sign()
A:torch.optim.rprop.grad->grad.clone().clone()
torch.optim.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.Rprop.step(self,closure=None)
torch.optim.rprop.Rprop(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.__init__(self,params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))
torch.optim.rprop.Rprop.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/adagrad.py----------------------------------------
A:torch.optim.adagrad.defaults->dict(lr=lr, lr_decay=lr_decay, eps=eps, weight_decay=weight_decay, initial_accumulator_value=initial_accumulator_value)
A:torch.optim.adagrad.state['sum']->torch.full_like(p.data, initial_accumulator_value)
A:torch.optim.adagrad.loss->closure()
A:torch.optim.adagrad.grad->grad.coalesce().coalesce()
A:torch.optim.adagrad.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim.adagrad.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim.adagrad.size->grad.coalesce().coalesce().size()
A:torch.optim.adagrad.std->torch.full_like(p.data, initial_accumulator_value).sqrt().add_(group['eps'])
A:torch.optim.adagrad.std_values->torch.full_like(p.data, initial_accumulator_value).sqrt().add_(group['eps'])._values().sqrt_().add_(group['eps'])
torch.optim.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.Adagrad.share_memory(self)
torch.optim.Adagrad.step(self,closure=None)
torch.optim.adagrad.Adagrad(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.adagrad.Adagrad.__init__(self,params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)
torch.optim.adagrad.Adagrad.share_memory(self)
torch.optim.adagrad.Adagrad.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/asgd.py----------------------------------------
A:torch.optim.asgd.defaults->dict(lr=lr, lambd=lambd, alpha=alpha, t0=t0, weight_decay=weight_decay)
A:torch.optim.asgd.loss->closure()
A:torch.optim.asgd.state['ax']->torch.zeros_like(p.data)
A:torch.optim.asgd.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
torch.optim.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.ASGD.step(self,closure=None)
torch.optim.asgd.ASGD(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.__init__(self,params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)
torch.optim.asgd.ASGD.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/sgd.py----------------------------------------
A:torch.optim.sgd.defaults->dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov)
A:torch.optim.sgd.loss->closure()
A:torch.optim.sgd.bufparam_state['momentum_buffer']->torch.clone(d_p).detach()
A:torch.optim.sgd.d_p->d_p.add(momentum, buf).add(momentum, buf)
torch.optim.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.SGD.__setstate__(self,state)
torch.optim.SGD.step(self,closure=None)
torch.optim.sgd.SGD(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.sgd.SGD.__init__(self,params,lr=required,momentum=0,dampening=0,weight_decay=0,nesterov=False)
torch.optim.sgd.SGD.__setstate__(self,state)
torch.optim.sgd.SGD.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/sgd.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/lbfgs.py----------------------------------------
A:torch.optim.lbfgs.d2->d2_square.sqrt()
A:torch.optim.lbfgs.d_norm->flat_grad.neg().abs().max()
A:torch.optim.lbfgs.g->g.clone().clone()
A:torch.optim.lbfgs.(f_new, g_new)->obj_func(x, t, d)
A:torch.optim.lbfgs.gtd_new->g_new.dot(d)
A:torch.optim.lbfgs.t->state.get('t')
A:torch.optim.lbfgs.g_prev->g_new.clone()
A:torch.optim.lbfgs.bracket_g[high_pos]->g_new.clone()
A:torch.optim.lbfgs.bracket_g[low_pos]->g_new.clone()
A:torch.optim.lbfgs.defaults->dict(lr=lr, max_iter=max_iter, max_eval=max_eval, tolerance_grad=tolerance_grad, tolerance_change=tolerance_change, history_size=history_size, line_search_fn=line_search_fn)
A:torch.optim.lbfgs.self._numel_cache->reduce(lambda total, p: total + p.numel(), self._params, 0)
A:torch.optim.lbfgs.view->p.grad.view(-1)
A:torch.optim.lbfgs.numel->p.numel()
A:torch.optim.lbfgs.loss->float(closure())
A:torch.optim.lbfgs.flat_grad->self._gather_flat_grad()
A:torch.optim.lbfgs.orig_loss->closure()
A:torch.optim.lbfgs.d->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.old_dirs->state.get('old_dirs')
A:torch.optim.lbfgs.old_stps->state.get('old_stps')
A:torch.optim.lbfgs.ro->state.get('ro')
A:torch.optim.lbfgs.H_diag->state.get('H_diag')
A:torch.optim.lbfgs.prev_flat_grad->self._gather_flat_grad().clone()
A:torch.optim.lbfgs.prev_loss->state.get('prev_loss')
A:torch.optim.lbfgs.y->self._gather_flat_grad().sub(prev_flat_grad)
A:torch.optim.lbfgs.s->self._gather_flat_grad().neg().mul(t)
A:torch.optim.lbfgs.ys->self._gather_flat_grad().sub(prev_flat_grad).dot(s)
A:torch.optim.lbfgs.num_old->len(old_dirs)
A:torch.optim.lbfgs.q->self._gather_flat_grad().neg()
A:torch.optim.lbfgs.dr->torch.mul(q, H_diag)
A:torch.optim.lbfgs.gtd->self._gather_flat_grad().dot(d)
A:torch.optim.lbfgs.x_init->self._clone_param()
A:torch.optim.lbfgs.(loss, flat_grad, t, ls_func_evals)->_strong_wolfe(obj_func, x_init, t, d, loss, flat_grad, gtd)
torch.optim.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.LBFGS._add_grad(self,step_size,update)
torch.optim.LBFGS._clone_param(self)
torch.optim.LBFGS._directional_evaluate(self,closure,x,t,d)
torch.optim.LBFGS._gather_flat_grad(self)
torch.optim.LBFGS._numel(self)
torch.optim.LBFGS._set_param(self,params_data)
torch.optim.LBFGS.step(self,closure)
torch.optim.lbfgs.LBFGS(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS.__init__(self,params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)
torch.optim.lbfgs.LBFGS._add_grad(self,step_size,update)
torch.optim.lbfgs.LBFGS._clone_param(self)
torch.optim.lbfgs.LBFGS._directional_evaluate(self,closure,x,t,d)
torch.optim.lbfgs.LBFGS._gather_flat_grad(self)
torch.optim.lbfgs.LBFGS._numel(self)
torch.optim.lbfgs.LBFGS._set_param(self,params_data)
torch.optim.lbfgs.LBFGS.step(self,closure)
torch.optim.lbfgs._cubic_interpolate(x1,f1,g1,x2,f2,g2,bounds=None)
torch.optim.lbfgs._strong_wolfe(obj_func,x,t,d,f,g,gtd,c1=0.0001,c2=0.9,tolerance_change=1e-09,max_ls=25)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/sparse_adam.py----------------------------------------
A:torch.optim.sparse_adam.defaults->dict(lr=lr, betas=betas, eps=eps)
A:torch.optim.sparse_adam.loss->closure()
A:torch.optim.sparse_adam.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.sparse_adam.state['exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.sparse_adam.grad->grad.coalesce().coalesce()
A:torch.optim.sparse_adam.grad_indices->grad.coalesce().coalesce()._indices()
A:torch.optim.sparse_adam.grad_values->grad.coalesce().coalesce()._values()
A:torch.optim.sparse_adam.size->grad.coalesce().coalesce().size()
A:torch.optim.sparse_adam.old_exp_avg_values->exp_avg.sparse_mask(grad)._values()
A:torch.optim.sparse_adam.exp_avg_update_values->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1)
A:torch.optim.sparse_adam.old_exp_avg_sq_values->exp_avg_sq.sparse_mask(grad)._values()
A:torch.optim.sparse_adam.exp_avg_sq_update_values->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2)
A:torch.optim.sparse_adam.numer->grad.coalesce().coalesce()._values().sub(old_exp_avg_values).mul_(1 - beta1).add_(old_exp_avg_values)
A:torch.optim.sparse_adam.denom->grad.coalesce().coalesce()._values().pow(2).sub_(old_exp_avg_sq_values).mul_(1 - beta2).sqrt_().add_(group['eps'])
torch.optim.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.SparseAdam.step(self,closure=None)
torch.optim.sparse_adam.SparseAdam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08)
torch.optim.sparse_adam.SparseAdam.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/adamax.py----------------------------------------
A:torch.optim.adamax.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
A:torch.optim.adamax.loss->closure()
A:torch.optim.adamax.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.adamax.state['exp_inf']->torch.zeros_like(p.data)
A:torch.optim.adamax.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
A:torch.optim.adamax.norm_buf->torch.cat([exp_inf.mul_(beta2).unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)
torch.optim.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.Adamax.step(self,closure=None)
torch.optim.adamax.Adamax(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.__init__(self,params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)
torch.optim.adamax.Adamax.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/adamw.py----------------------------------------
A:torch.optim.adamw.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)
A:torch.optim.adamw.loss->closure()
A:torch.optim.adamw.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.adamw.state['exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.adamw.state['max_exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.adamw.denom->(exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])
torch.optim.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)
torch.optim.AdamW.__setstate__(self,state)
torch.optim.AdamW.step(self,closure=None)
torch.optim.adamw.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)
torch.optim.adamw.AdamW.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)
torch.optim.adamw.AdamW.__setstate__(self,state)
torch.optim.adamw.AdamW.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/adam.py----------------------------------------
A:torch.optim.adam.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)
A:torch.optim.adam.loss->closure()
A:torch.optim.adam.state['exp_avg']->torch.zeros_like(p.data)
A:torch.optim.adam.state['exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.adam.state['max_exp_avg_sq']->torch.zeros_like(p.data)
A:torch.optim.adam.denom->(exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])
torch.optim.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.Adam.__setstate__(self,state)
torch.optim.Adam.step(self,closure=None)
torch.optim.adam.Adam(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.adam.Adam.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)
torch.optim.adam.Adam.__setstate__(self,state)
torch.optim.adam.Adam.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/adam.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/rmsprop.py----------------------------------------
A:torch.optim.rmsprop.defaults->dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)
A:torch.optim.rmsprop.loss->closure()
A:torch.optim.rmsprop.state['square_avg']->torch.zeros_like(p.data)
A:torch.optim.rmsprop.state['momentum_buffer']->torch.zeros_like(p.data)
A:torch.optim.rmsprop.state['grad_avg']->torch.zeros_like(p.data)
A:torch.optim.rmsprop.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
A:torch.optim.rmsprop.avg->square_avg.sqrt().add_(group['eps'])
torch.optim.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.RMSprop.__setstate__(self,state)
torch.optim.RMSprop.step(self,closure=None)
torch.optim.rmsprop.RMSprop(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__init__(self,params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)
torch.optim.rmsprop.RMSprop.__setstate__(self,state)
torch.optim.rmsprop.RMSprop.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/optim/adadelta.py----------------------------------------
A:torch.optim.adadelta.defaults->dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)
A:torch.optim.adadelta.loss->closure()
A:torch.optim.adadelta.state['square_avg']->torch.zeros_like(p.data)
A:torch.optim.adadelta.state['acc_delta']->torch.zeros_like(p.data)
A:torch.optim.adadelta.grad->grad.add(group['weight_decay'], p.data).add(group['weight_decay'], p.data)
A:torch.optim.adadelta.std->square_avg.add(eps).sqrt_()
A:torch.optim.adadelta.delta->acc_delta.add(eps).sqrt_().div_(std).mul_(grad)
torch.optim.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.Adadelta.step(self,closure=None)
torch.optim.adadelta.Adadelta(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.__init__(self,params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)
torch.optim.adadelta.Adadelta.step(self,closure=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/sparse/__init__.py----------------------------------------
torch.sparse.__init__.addmm(mat,mat1,mat2,beta=1,alpha=1)
torch.sparse.__init__.mm(mat1,mat2)
torch.sparse.__init__.sum(input,dim=None,dtype=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/streams.py----------------------------------------
A:torch.cuda.streams.event->Event()
A:torch.cuda.streams.stream->torch.cuda.current_stream()
torch.cuda.Event(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.Event.__repr__(self)
torch.cuda.Event._as_parameter_(self)
torch.cuda.Event.elapsed_time(self,end_event)
torch.cuda.Event.from_ipc_handle(cls,device,handle)
torch.cuda.Event.ipc_handle(self)
torch.cuda.Event.query(self)
torch.cuda.Event.record(self,stream=None)
torch.cuda.Event.synchronize(self)
torch.cuda.Event.wait(self,stream=None)
torch.cuda.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.Stream.__eq__(self,o)
torch.cuda.Stream.__hash__(self)
torch.cuda.Stream.__repr__(self)
torch.cuda.Stream._as_parameter_(self)
torch.cuda.Stream.query(self)
torch.cuda.Stream.record_event(self,event=None)
torch.cuda.Stream.synchronize(self)
torch.cuda.Stream.wait_event(self,event)
torch.cuda.Stream.wait_stream(self,stream)
torch.cuda.streams.Event(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.streams.Event.__new__(cls,enable_timing=False,blocking=False,interprocess=False)
torch.cuda.streams.Event.__repr__(self)
torch.cuda.streams.Event._as_parameter_(self)
torch.cuda.streams.Event.elapsed_time(self,end_event)
torch.cuda.streams.Event.from_ipc_handle(cls,device,handle)
torch.cuda.streams.Event.ipc_handle(self)
torch.cuda.streams.Event.query(self)
torch.cuda.streams.Event.record(self,stream=None)
torch.cuda.streams.Event.synchronize(self)
torch.cuda.streams.Event.wait(self,stream=None)
torch.cuda.streams.Stream(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__eq__(self,o)
torch.cuda.streams.Stream.__hash__(self)
torch.cuda.streams.Stream.__new__(cls,device=None,priority=0,**kwargs)
torch.cuda.streams.Stream.__repr__(self)
torch.cuda.streams.Stream._as_parameter_(self)
torch.cuda.streams.Stream.query(self)
torch.cuda.streams.Stream.record_event(self,event=None)
torch.cuda.streams.Stream.synchronize(self)
torch.cuda.streams.Stream.wait_event(self,event)
torch.cuda.streams.Stream.wait_stream(self,stream)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/__init__.py----------------------------------------
A:torch.cuda.__init__._tls->threading.local()
A:torch.cuda.__init__._initialization_lock->threading.Lock()
A:torch.cuda.__init__.test_env->os.environ.copy()
A:torch.cuda.__init__.py_dll_path->os.path.join(sys.exec_prefix, 'Library', 'bin')
A:torch.cuda.__init__.th_dll_path->os.path.join(os.path.dirname(os.path.dirname(__file__)), 'lib')
A:torch.cuda.__init__.test_env['PATH']->';'.join([th_dll_path, py_dll_path, old_path])
A:torch.cuda.__init__.proc->Popen(['where', 'cudart64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE, env=test_env)
A:torch.cuda.__init__.(out, err)->Popen(['where', 'cudart64*.dll'], stdout=PIPE, stderr=PIPE, stdin=PIPE, env=test_env).communicate()
A:torch.cuda.__init__.out->out.decode().strip().decode().strip()
A:torch.cuda.__init__.cuda_lib_name->os.path.basename(out)
A:torch.cuda.__init__.cuda_lib->str(cuda_lib)
A:torch.cuda.__init__.lib->ctypes.cdll.LoadLibrary(None)
A:torch.cuda.__init__.CUDA_VERSION->torch._C._cuda_getCompiledVersion()
A:torch.cuda.__init__.capability->get_device_capability(d)
A:torch.cuda.__init__.name->get_device_name(d)
A:torch.cuda.__init__._cudart->_load_cudart()
A:torch.cuda.__init__._original_pid->os.getpid()
A:torch.cuda.__init__.msg->cudart().cudaGetErrorString(code).decode('utf-8')
A:torch.cuda.__init__.self.idx->_get_device_index(device, optional=True)
A:torch.cuda.__init__.self.prev_idx->torch._C._cuda_getDevice()
A:torch.cuda.__init__.device->_get_device_index(device, optional=True)
A:torch.cuda.__init__.prop->get_device_properties(device)
A:torch.cuda.__init__.src_prev_stream->current_stream()
A:torch.cuda.__init__.dst_prev_stream->current_stream()
A:torch.cuda.__init__.storage_name->'Cuda{0}StorageBase'.format(t)
A:torch.cuda.__init__.tensor_name->'Cuda{0}TensorBase'.format(t)
A:torch.cuda.__init__.torch._C.__dict__[storage_name]->_dummy_type(storage_name)
A:torch.cuda.__init__.torch._C.__dict__[tensor_name]->_dummy_type(tensor_name)
A:torch.cuda.__init__.torch._C.__dict__['_CudaStreamBase']->_dummy_type('CudaStreamBase')
A:torch.cuda.__init__.torch._C.__dict__['_CudaEventBase']->_dummy_type('CudaEventBase')
torch.cuda.__init__.BFloat16Storage(_CudaBase,torch._C.CudaBFloat16StorageBase,_StorageBase)
torch.cuda.__init__.BoolStorage(_CudaBase,torch._C.CudaBoolStorageBase,_StorageBase)
torch.cuda.__init__.ByteStorage(_CudaBase,torch._C.CudaByteStorageBase,_StorageBase)
torch.cuda.__init__.CharStorage(_CudaBase,torch._C.CudaCharStorageBase,_StorageBase)
torch.cuda.__init__.CudaError(self,code)
torch.cuda.__init__.CudaError.__init__(self,code)
torch.cuda.__init__.DeferredCudaCallError(Exception)
torch.cuda.__init__.DoubleStorage(_CudaBase,torch._C.CudaDoubleStorageBase,_StorageBase)
torch.cuda.__init__.FloatStorage(_CudaBase,torch._C.CudaFloatStorageBase,_StorageBase)
torch.cuda.__init__.HalfStorage(_CudaBase,torch._C.CudaHalfStorageBase,_StorageBase)
torch.cuda.__init__.IntStorage(_CudaBase,torch._C.CudaIntStorageBase,_StorageBase)
torch.cuda.__init__.LongStorage(_CudaBase,torch._C.CudaLongStorageBase,_StorageBase)
torch.cuda.__init__.ShortStorage(_CudaBase,torch._C.CudaShortStorageBase,_StorageBase)
torch.cuda.__init__._CudaBase(object)
torch.cuda.__init__._CudaBase.type(self,*args,**kwargs)
torch.cuda.__init__._after_fork(arg)
torch.cuda.__init__._check_capability()
torch.cuda.__init__._check_driver()
torch.cuda.__init__._dummy_type(name)
torch.cuda.__init__._free_mutex()
torch.cuda.__init__._host_allocator()
torch.cuda.__init__._lazy_call(callable)
torch.cuda.__init__._lazy_init()
torch.cuda.__init__._lazy_new(cls,*args,**kwargs)
torch.cuda.__init__._load_cudart()
torch.cuda.__init__._sleep(cycles)
torch.cuda.__init__.check_error(res)
torch.cuda.__init__.cudaStatus(object)
torch.cuda.__init__.cudart()
torch.cuda.__init__.current_blas_handle()
torch.cuda.__init__.current_device()
torch.cuda.__init__.current_stream(device=None)
torch.cuda.__init__.default_stream(device=None)
torch.cuda.__init__.device(self,device)
torch.cuda.__init__.device.__enter__(self)
torch.cuda.__init__.device.__exit__(self,*args)
torch.cuda.__init__.device.__init__(self,device)
torch.cuda.__init__.device_count()
torch.cuda.__init__.device_of(self,obj)
torch.cuda.__init__.device_of.__init__(self,obj)
torch.cuda.__init__.empty_cache()
torch.cuda.__init__.find_cuda_windows_lib()
torch.cuda.__init__.get_device_capability(device=None)
torch.cuda.__init__.get_device_name(device=None)
torch.cuda.__init__.get_device_properties(device)
torch.cuda.__init__.init()
torch.cuda.__init__.ipc_collect()
torch.cuda.__init__.is_available()
torch.cuda.__init__.max_memory_allocated(device=None)
torch.cuda.__init__.max_memory_cached(device=None)
torch.cuda.__init__.memory_allocated(device=None)
torch.cuda.__init__.memory_cached(device=None)
torch.cuda.__init__.reset_max_memory_allocated(device=None)
torch.cuda.__init__.reset_max_memory_cached(device=None)
torch.cuda.__init__.set_device(device)
torch.cuda.__init__.stream(stream)
torch.cuda.__init__.synchronize(device=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/__init__.pyi----------------------------------------
torch.cuda.__init__._CudaDeviceProperties
torch.cuda.__init__.get_rng_state()
torch.cuda.__init__.set_rng_state(new_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/sparse.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/nvtx.py----------------------------------------
A:torch.cuda.nvtx.lib_path->windows_nvToolsExt_path()
A:torch.cuda.nvtx.lib_name->os.path.basename(lib_path)
A:torch.cuda.nvtx.NVTOOLEXT_HOME->os.getenv('NVTOOLSEXT_PATH', WINDOWS_HOME)
A:torch.cuda.nvtx.lib_paths->glob.glob(NVTOOLEXT_HOME + '/bin/x64/nvToolsExt*.dll')
A:torch.cuda.nvtx.lib->windows_nvToolsExt_lib()
torch.cuda.nvtx._libnvToolsExt()
torch.cuda.nvtx.mark(msg)
torch.cuda.nvtx.range_pop()
torch.cuda.nvtx.range_push(msg)
torch.cuda.nvtx.windows_nvToolsExt_lib()
torch.cuda.nvtx.windows_nvToolsExt_path()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/_utils.py----------------------------------------
A:torch.cuda._utils.device->torch.device(device)
torch.cuda._get_device_index(device,optional=False)
torch.cuda._utils._get_device_index(device,optional=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/comm.py----------------------------------------
A:torch.cuda.comm.destination->torch.cuda.current_device()
A:torch.cuda.comm.input_size->inputs[0].size()
A:torch.cuda.comm.got->'x'.join((str(x) for x in inp.size()))
A:torch.cuda.comm.expected->'x'.join((str(x) for x in input_size))
A:torch.cuda.comm.result->reduce_add(tensor_at_gpus, destination)
A:torch.cuda.comm.input_correct_gpu->inp.cuda(result.get_device())
A:torch.cuda.comm.flat_result->reduce_add(flat_tensors, destination)
torch.cuda.comm.broadcast(tensor,devices)
torch.cuda.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)
torch.cuda.comm.gather(tensors,dim=0,destination=None)
torch.cuda.comm.reduce_add(inputs,destination=None)
torch.cuda.comm.reduce_add_coalesced(inputs,destination=None,buffer_size=10485760)
torch.cuda.comm.scatter(tensor,devices,chunk_sizes=None,dim=0,streams=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/nccl.py----------------------------------------
A:torch.cuda.nccl.devices->set()
A:torch.cuda.nccl.device->tensor.get_device()
torch.cuda.nccl.all_gather(inputs,outputs,streams=None,comms=None)
torch.cuda.nccl.all_reduce(inputs,outputs=None,op=SUM,streams=None,comms=None)
torch.cuda.nccl.broadcast(inputs,root=0,streams=None,comms=None)
torch.cuda.nccl.init_rank(num_ranks,uid,rank)
torch.cuda.nccl.is_available(tensors)
torch.cuda.nccl.reduce(inputs,outputs=None,root=0,op=SUM,streams=None,comms=None)
torch.cuda.nccl.reduce_scatter(inputs,outputs,op=SUM,streams=None,comms=None)
torch.cuda.nccl.unique_id()
torch.cuda.nccl.version()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/profiler.py----------------------------------------
A:torch.cuda.profiler.cudaKeyValuePair->ctypes.c_int(0)
A:torch.cuda.profiler.cudaCSV->ctypes.c_int(1)
A:torch.cuda.profiler.output_mode->cudaOutputMode.for_key(output_mode)
torch.cuda.profiler.cudaOutputMode(object)
torch.cuda.profiler.cudaOutputMode.for_key(key)
torch.cuda.profiler.init(output_file,flags=None,output_mode='key_value')
torch.cuda.profiler.profile()
torch.cuda.profiler.start()
torch.cuda.profiler.stop()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/random.py----------------------------------------
A:torch.cuda.random.device->torch.device('cuda', device)
A:torch.cuda.random.idx->current_device()
A:torch.cuda.random.new_state_copy->new_state.clone()
A:torch.cuda.random.seed->int(seed)
A:torch.cuda.random.random_seed->default_generator.initial_seed()
torch.cuda.get_rng_state(device='cuda')
torch.cuda.get_rng_state_all()
torch.cuda.initial_seed()
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.cuda.random.get_rng_state(device='cuda')
torch.cuda.random.get_rng_state_all()
torch.cuda.random.initial_seed()
torch.cuda.random.manual_seed(seed)
torch.cuda.random.manual_seed_all(seed)
torch.cuda.random.seed()
torch.cuda.random.seed_all()
torch.cuda.random.set_rng_state(new_state,device='cuda')
torch.cuda.random.set_rng_state_all(new_states)
torch.cuda.seed()
torch.cuda.seed_all()
torch.cuda.set_rng_state(new_state,device='cuda')
torch.cuda.set_rng_state_all(new_states)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/cuda/error.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/testing/__init__.py----------------------------------------
A:torch.testing.__init__.actual->torch.tensor(actual)
A:torch.testing.__init__.expected->expected.expand_as(actual).expand_as(actual)
A:torch.testing.__init__.(rtol, atol)->_get_default_tolerance(actual, expected)
A:torch.testing.__init__.close->torch.isclose(actual, expected, rtol, atol, equal_nan)
A:torch.testing.__init__.error->(expected - actual).abs()
A:torch.testing.__init__.(_, index)->delta.reshape(-1).max(0)
A:torch.testing.__init__.index->_unravel_index(index.item(), actual.shape)
A:torch.testing.__init__.count->(~close).long().sum()
A:torch.testing.__init__.osize->list(tensor.size())
A:torch.testing.__init__.dim->random.randint(0, len(osize) - 1)
A:torch.testing.__init__.add->random.randint(4, 15)
A:torch.testing.__init__.input->input.narrow(i, bounds, tensor.size(i)).narrow(i, bounds, tensor.size(i))
A:torch.testing.__init__.bounds->random.randint(1, input.size(i) - tensor.size(i))
A:torch.testing.__init__.a_tol->_get_default_tolerance(a)
A:torch.testing.__init__.b_tol->_get_default_tolerance(b)
torch.testing.__init__._get_default_tolerance(a,b=None)
torch.testing.__init__.assert_allclose(actual,expected,rtol=None,atol=None,equal_nan=True)
torch.testing.__init__.get_all_device_types()
torch.testing.__init__.get_all_dtypes()
torch.testing.__init__.get_all_math_dtypes(device)
torch.testing.__init__.make_non_contiguous(tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/__init__.py----------------------------------------
torch.quantization.__init__.default_eval_fn(model,calib_data)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/QConfig.py----------------------------------------
A:torch.quantization.QConfig.default_qconfig->QConfig(activation=default_observer, weight=default_weight_observer)
A:torch.quantization.QConfig.default_debug_qconfig->QConfig(weight=default_weight_observer, activation=default_debug_observer)
A:torch.quantization.QConfig.default_per_channel_qconfig->QConfig(activation=default_observer, weight=default_per_channel_weight_observer)
A:torch.quantization.QConfig.default_dynamic_qconfig->QConfigDynamic(weight=default_weight_observer)
A:torch.quantization.QConfig.float16_dynamic_qconfig->QConfigDynamic(weight=NoopObserver.with_args(dtype=torch.float16))
A:torch.quantization.QConfig.default_qat_qconfig->QConfig(activation=default_fake_quant, weight=default_weight_fake_quant)
A:torch.quantization.QConfig.default_weight_only_quant_qconfig->QConfig(activation=torch.nn.Identity, weight=default_weight_fake_quant)
A:torch.quantization.QConfig.default_activation_only_quant_qconfig->QConfig(activation=default_fake_quant, weight=torch.nn.Identity)
A:torch.quantization.QConfig.qconfig->QConfig(activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, reduce_range=False), weight=default_weight_fake_quant)
torch.quantization.QConfig(cls,activation,weight)
torch.quantization.QConfig.QConfig(cls,activation,weight)
torch.quantization.QConfig.QConfig.__new__(cls,activation,weight)
torch.quantization.QConfig.QConfigDynamic(cls,weight)
torch.quantization.QConfig.QConfigDynamic.__new__(cls,weight)
torch.quantization.QConfig.get_default_qat_qconfig(backend='fbgemm')
torch.quantization.QConfig.get_default_qconfig(backend='fbgemm')
torch.quantization.QConfigDynamic(cls,weight)
torch.quantization.get_default_qat_qconfig(backend='fbgemm')
torch.quantization.get_default_qconfig(backend='fbgemm')


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/fuse_modules.py----------------------------------------
A:torch.quantization.fuse_modules.tokens->submodule_key.split('.')
A:torch.quantization.fuse_modules.cur_mod->getattr(cur_mod, s)
A:torch.quantization.fuse_modules.types->tuple((type(m) for m in mod_list))
A:torch.quantization.fuse_modules.fuser_method->OP_LIST_TO_FUSER_METHOD.get(types, None)
A:torch.quantization.fuse_modules.new_mod[0]->fuser_method(*mod_list)
A:torch.quantization.fuse_modules.new_mod[i]->torch.nn.Identity()
A:torch.quantization.fuse_modules.new_mod_list->fuser_func(mod_list)
A:torch.quantization.fuse_modules.model->copy.deepcopy(model)
torch.quantization.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules)
torch.quantization.fuse_modules._fuse_modules(model,modules_to_fuse,fuser_func=fuse_known_modules)
torch.quantization.fuse_modules._get_module(model,submodule_key)
torch.quantization.fuse_modules._set_module(model,submodule_key,module)
torch.quantization.fuse_modules.fuse_conv_bn(conv,bn)
torch.quantization.fuse_modules.fuse_conv_bn_relu(conv,bn,relu)
torch.quantization.fuse_modules.fuse_known_modules(mod_list)
torch.quantization.fuse_modules.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=fuse_known_modules)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/default_mappings.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/_quantize_script.py----------------------------------------
A:torch.quantization._quantize_script.model->model.copy().copy()
torch.quantization._quantize_script._check_is_script_module(model)
torch.quantization._quantize_script.convert_script(model,inplace=False)
torch.quantization._quantize_script.prepare_script(model,qconfig_dict,inplace=False)
torch.quantization._quantize_script.quantize_script(model,qconfig_dict,run_fn,run_args,inplace=False)
torch.quantization._quantize_script.script_qconfig(qconfig)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/stubs.py----------------------------------------
A:torch.quantization.stubs.X->self.module(X)
torch.quantization.DeQuantStub(self)
torch.quantization.DeQuantStub.forward(self,x)
torch.quantization.QuantStub(self,qconfig=None)
torch.quantization.QuantStub.forward(self,x)
torch.quantization.QuantWrapper(self,module)
torch.quantization.QuantWrapper.forward(self,X)
torch.quantization.stubs.DeQuantStub(self)
torch.quantization.stubs.DeQuantStub.__init__(self)
torch.quantization.stubs.DeQuantStub.forward(self,x)
torch.quantization.stubs.QuantStub(self,qconfig=None)
torch.quantization.stubs.QuantStub.__init__(self,qconfig=None)
torch.quantization.stubs.QuantStub.forward(self,x)
torch.quantization.stubs.QuantWrapper(self,module)
torch.quantization.stubs.QuantWrapper.__init__(self,module)
torch.quantization.stubs.QuantWrapper.forward(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/quantize.py----------------------------------------
A:torch.quantization.quantize.module_qconfig->getattr(module, 'qconfig', module_qconfig)
A:torch.quantization.quantize.child.observer->child.qconfig.activation()
A:torch.quantization.quantize.module._modules[name]->add_quant_dequant(child)
A:torch.quantization.quantize.model->copy.deepcopy(model)
A:torch.quantization.quantize.qconfig_spec->dict(zip(qconfig_spec, itertools.repeat(default_qconfig)))
A:torch.quantization.quantize.module->copy.deepcopy(module)
A:torch.quantization.quantize.reassign[name]->swap_module(mod, mapping)
A:torch.quantization.quantize.new_mod->mapping[type(mod)].from_float(mod)
torch.quantization._observer_forward_hook(self,input,output)
torch.quantization._propagate_qconfig_helper(module,qconfig_dict,white_list=None,qconfig_parent=None,prefix='')
torch.quantization.add_observer_(module)
torch.quantization.add_quant_dequant(module)
torch.quantization.convert(module,mapping=None,inplace=False)
torch.quantization.get_observer_dict(mod,target_dict,prefix='')
torch.quantization.prepare(model,qconfig_dict=None,inplace=False)
torch.quantization.prepare_qat(model,mapping=DEFAULT_QAT_MODULE_MAPPING,inplace=False)
torch.quantization.propagate_qconfig_(module,qconfig_dict=None)
torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)
torch.quantization.quantize._observer_forward_hook(self,input,output)
torch.quantization.quantize._propagate_qconfig_helper(module,qconfig_dict,white_list=None,qconfig_parent=None,prefix='')
torch.quantization.quantize.add_observer_(module)
torch.quantization.quantize.add_quant_dequant(module)
torch.quantization.quantize.convert(module,mapping=None,inplace=False)
torch.quantization.quantize.get_observer_dict(mod,target_dict,prefix='')
torch.quantization.quantize.prepare(model,qconfig_dict=None,inplace=False)
torch.quantization.quantize.prepare_qat(model,mapping=DEFAULT_QAT_MODULE_MAPPING,inplace=False)
torch.quantization.quantize.propagate_qconfig_(module,qconfig_dict=None)
torch.quantization.quantize.quantize(model,run_fn,run_args,mapping=None,inplace=False)
torch.quantization.quantize.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)
torch.quantization.quantize.quantize_qat(model,run_fn,run_args,inplace=False)
torch.quantization.quantize.swap_module(mod,mapping)
torch.quantization.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)
torch.quantization.quantize_qat(model,run_fn,run_args,inplace=False)
torch.quantization.swap_module(mod,mapping)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/observer.py----------------------------------------
A:torch.quantization.observer.r->_PartialWrapper(partial(cls_or_self, **kwargs))
A:torch.quantization.observer.ABC->ABCMeta(str('ABC'), (object,), {})
A:torch.quantization.observer.with_args->classmethod(_with_args)
A:torch.quantization.observer.scales->torch.empty(min_vals.size(), dtype=torch.float32)
A:torch.quantization.observer.zero_points->torch.empty(min_vals.size(), dtype=torch.int64)
A:torch.quantization.observer.qparam->self._calculate_qparams(min_vals[i], max_vals[i])
A:torch.quantization.observer.scales[i]->float(qparam[0])
A:torch.quantization.observer.zero_points[i]->int(qparam[1])
A:torch.quantization.observer.min_val->torch.min(x)
A:torch.quantization.observer.max_val->torch.max(x)
A:torch.quantization.observer.scale->max(scale, self.eps)
A:torch.quantization.observer.zero_point->int(zero_point)
A:torch.quantization.observer.x->x_orig.detach()
A:torch.quantization.observer.self.min_val->state_dict.pop(prefix + 'min_val')
A:torch.quantization.observer.self.max_val->state_dict.pop(prefix + 'max_val')
A:torch.quantization.observer.x_dim->x_orig.detach().size()
A:torch.quantization.observer.new_axis_list->list(range(len(x_dim)))
A:torch.quantization.observer.y->torch.flatten(y, start_dim=1)
A:torch.quantization.observer.min_vals->torch.min(torch.min(y, 1)[0], min_vals)
A:torch.quantization.observer.max_vals->torch.max(torch.max(y, 1)[0], max_vals)
A:torch.quantization.observer.self.min_vals->state_dict.pop(prefix + 'min_vals')
A:torch.quantization.observer.self.max_vals->state_dict.pop(prefix + 'max_vals')
A:torch.quantization.observer.dst_bin_of_begin->min(dst_nbins - 1, max(0.0, math.floor(src_bin_begin / dst_bin_width)))
A:torch.quantization.observer.dst_bin_of_end->min(dst_nbins - 1, max(0.0, math.floor(src_bin_end / dst_bin_width)))
A:torch.quantization.observer.total->sum(self.histogram)
A:torch.quantization.observer.cSum->torch.cumsum(self.histogram, dim=0)
A:torch.quantization.observer.norm_min->float('inf')
A:torch.quantization.observer.norm->self._compute_quantization_error(next_start_bin, next_end_bin, 'L2')
A:torch.quantization.observer.src_bin_count->src_histogram[i].item()
A:torch.quantization.observer.dst_bin->int((src_bin_begin - dst_min) / dst_bin_width)
A:torch.quantization.observer.dst_bin2->min(int((src_bin_end - dst_min) / dst_bin_width), bins_dst - 1)
A:torch.quantization.observer.dst_bin_cnt->min(round((dst_bin_end - src_bin_begin) / src_bin_width * src_bin_count), src_bin_count)
A:torch.quantization.observer.self.histogram->torch.histc(x, self.bins, min=min_val, max=max_val)
A:torch.quantization.observer.new_min->torch.min(x)
A:torch.quantization.observer.new_max->torch.max(x)
A:torch.quantization.observer.new_histogram->torch.histc(x, self.bins, min=new_min, max=new_max)
A:torch.quantization.observer.combined_histogram->torch.zeros_like(self.histogram)
A:torch.quantization.observer.combined_min->torch.min(new_min, self.min_val)
A:torch.quantization.observer.combined_max->torch.max(new_max, self.max_val)
A:torch.quantization.observer.(new_min, new_max)->self._non_linear_param_search()
A:torch.quantization.observer.default_observer->MinMaxObserver.with_args(reduce_range=True)
A:torch.quantization.observer.default_weight_observer->MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)
A:torch.quantization.observer.default_histogram_observer->HistogramObserver.with_args(reduce_range=True)
A:torch.quantization.observer.default_per_channel_weight_observer->PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
torch.quantization.HistogramObserver(self,bins=2048,**kwargs)
torch.quantization.HistogramObserver._combine_histograms(self,dst_histogram,dst_min,dst_max,src_histogram,src_min,src_max)
torch.quantization.HistogramObserver._compute_quantization_error(self,next_start_bin,next_end_bin,norm_type)
torch.quantization.HistogramObserver._get_norm(delta_begin,delta_end,density,norm_type)
torch.quantization.HistogramObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.HistogramObserver._non_linear_param_search(self)
torch.quantization.HistogramObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.HistogramObserver.calculate_qparams(self)
torch.quantization.HistogramObserver.forward(self,x)
torch.quantization.MinMaxObserver(self,**kwargs)
torch.quantization.MinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.MinMaxObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.MinMaxObserver.calculate_qparams(self)
torch.quantization.MinMaxObserver.extra_repr(self)
torch.quantization.MinMaxObserver.forward(self,x_orig)
torch.quantization.MovingAverageMinMaxObserver(self,averaging_constant=0.01,**kwargs)
torch.quantization.MovingAverageMinMaxObserver.forward(self,x_orig)
torch.quantization.MovingAveragePerChannelMinMaxObserver(self,averaging_constant=0.01,**kwargs)
torch.quantization.MovingAveragePerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.NoopObserver(self,dtype=torch.float16)
torch.quantization.NoopObserver.calculate_qparams(self)
torch.quantization.NoopObserver.forward(self,x)
torch.quantization.Observer(self,dtype)
torch.quantization.Observer.calculate_qparams(self,**kwargs)
torch.quantization.Observer.forward(self,x)
torch.quantization.PerChannelMinMaxObserver(self,ch_axis=0,**kwargs)
torch.quantization.PerChannelMinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.PerChannelMinMaxObserver.calculate_qparams(self)
torch.quantization.PerChannelMinMaxObserver.extra_repr(self)
torch.quantization.PerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.RecordingObserver(self,**kwargs)
torch.quantization.RecordingObserver.calculate_qparams(self)
torch.quantization.RecordingObserver.forward(self,x)
torch.quantization.RecordingObserver.get_tensor_value(self)
torch.quantization._ObserverBase(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization._ObserverBase._calculate_per_channel_qparams(self,min_vals,max_vals)
torch.quantization._ObserverBase._calculate_qparams(self,min_val,max_val)
torch.quantization._PartialWrapper(self,p)
torch.quantization._PartialWrapper.__repr__(self)
torch.quantization._with_args(cls_or_self,**kwargs)
torch.quantization.observer.HistogramObserver(self,bins=2048,**kwargs)
torch.quantization.observer.HistogramObserver.__init__(self,bins=2048,**kwargs)
torch.quantization.observer.HistogramObserver._combine_histograms(self,dst_histogram,dst_min,dst_max,src_histogram,src_min,src_max)
torch.quantization.observer.HistogramObserver._compute_quantization_error(self,next_start_bin,next_end_bin,norm_type)
torch.quantization.observer.HistogramObserver._get_norm(delta_begin,delta_end,density,norm_type)
torch.quantization.observer.HistogramObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.observer.HistogramObserver._non_linear_param_search(self)
torch.quantization.observer.HistogramObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.observer.HistogramObserver.calculate_qparams(self)
torch.quantization.observer.HistogramObserver.forward(self,x)
torch.quantization.observer.MinMaxObserver(self,**kwargs)
torch.quantization.observer.MinMaxObserver.__init__(self,**kwargs)
torch.quantization.observer.MinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.observer.MinMaxObserver._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.observer.MinMaxObserver.calculate_qparams(self)
torch.quantization.observer.MinMaxObserver.extra_repr(self)
torch.quantization.observer.MinMaxObserver.forward(self,x_orig)
torch.quantization.observer.MovingAverageMinMaxObserver(self,averaging_constant=0.01,**kwargs)
torch.quantization.observer.MovingAverageMinMaxObserver.__init__(self,averaging_constant=0.01,**kwargs)
torch.quantization.observer.MovingAverageMinMaxObserver.forward(self,x_orig)
torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(self,averaging_constant=0.01,**kwargs)
torch.quantization.observer.MovingAveragePerChannelMinMaxObserver.__init__(self,averaging_constant=0.01,**kwargs)
torch.quantization.observer.MovingAveragePerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.observer.NoopObserver(self,dtype=torch.float16)
torch.quantization.observer.NoopObserver.__init__(self,dtype=torch.float16)
torch.quantization.observer.NoopObserver.calculate_qparams(self)
torch.quantization.observer.NoopObserver.forward(self,x)
torch.quantization.observer.Observer(self,dtype)
torch.quantization.observer.Observer.__init__(self,dtype)
torch.quantization.observer.Observer.calculate_qparams(self,**kwargs)
torch.quantization.observer.Observer.forward(self,x)
torch.quantization.observer.PerChannelMinMaxObserver(self,ch_axis=0,**kwargs)
torch.quantization.observer.PerChannelMinMaxObserver.__init__(self,ch_axis=0,**kwargs)
torch.quantization.observer.PerChannelMinMaxObserver._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.observer.PerChannelMinMaxObserver.calculate_qparams(self)
torch.quantization.observer.PerChannelMinMaxObserver.extra_repr(self)
torch.quantization.observer.PerChannelMinMaxObserver.forward(self,x_orig)
torch.quantization.observer.RecordingObserver(self,**kwargs)
torch.quantization.observer.RecordingObserver.__init__(self,**kwargs)
torch.quantization.observer.RecordingObserver.calculate_qparams(self)
torch.quantization.observer.RecordingObserver.forward(self,x)
torch.quantization.observer.RecordingObserver.get_tensor_value(self)
torch.quantization.observer._ObserverBase(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer._ObserverBase.__init__(self,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)
torch.quantization.observer._ObserverBase._calculate_per_channel_qparams(self,min_vals,max_vals)
torch.quantization.observer._ObserverBase._calculate_qparams(self,min_val,max_val)
torch.quantization.observer._PartialWrapper(self,p)
torch.quantization.observer._PartialWrapper.__init__(self,p)
torch.quantization.observer._PartialWrapper.__repr__(self)
torch.quantization.observer._with_args(cls_or_self,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/quantization/fake_quantize.py----------------------------------------
A:torch.quantization.fake_quantize.self.observer->observer(**observer_kwargs)
A:torch.quantization.fake_quantize.(self.scale, self.zero_point)->self.calculate_qparams()
A:torch.quantization.fake_quantize.X->torch.fake_quantize_per_tensor_affine(X, float(self.scale), int(self.zero_point), self.quant_min, self.quant_max)
A:torch.quantization.fake_quantize.with_args->classmethod(_with_args)
A:torch.quantization.fake_quantize.self.scale->state_dict.pop(prefix + 'scale')
A:torch.quantization.fake_quantize.self.zero_point->state_dict.pop(prefix + 'zero_point')
A:torch.quantization.fake_quantize.default_fake_quant->FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
A:torch.quantization.fake_quantize.default_weight_fake_quant->FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False)
A:torch.quantization.fake_quantize.default_per_channel_weight_fake_quant->FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0)
A:torch.quantization.fake_quantize.default_histogram_fake_quant->FakeQuantize.with_args(observer=HistogramObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
torch.quantization.FakeQuantize(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.quantization.FakeQuantize._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.FakeQuantize._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.FakeQuantize.calculate_qparams(self)
torch.quantization.FakeQuantize.disable_fake_quant(self)
torch.quantization.FakeQuantize.disable_observer(self)
torch.quantization.FakeQuantize.enable_fake_quant(self,enabled=True)
torch.quantization.FakeQuantize.enable_observer(self,enabled=True)
torch.quantization.FakeQuantize.extra_repr(self)
torch.quantization.FakeQuantize.forward(self,X)
torch.quantization.disable_fake_quant(mod)
torch.quantization.disable_observer(mod)
torch.quantization.enable_fake_quant(mod)
torch.quantization.enable_observer(mod)
torch.quantization.fake_quantize.FakeQuantize(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.quantization.fake_quantize.FakeQuantize.__init__(self,observer=MovingAverageMinMaxObserver,quant_min=0,quant_max=255,**observer_kwargs)
torch.quantization.fake_quantize.FakeQuantize._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.quantization.fake_quantize.FakeQuantize._save_to_state_dict(self,destination,prefix,keep_vars)
torch.quantization.fake_quantize.FakeQuantize.calculate_qparams(self)
torch.quantization.fake_quantize.FakeQuantize.disable_fake_quant(self)
torch.quantization.fake_quantize.FakeQuantize.disable_observer(self)
torch.quantization.fake_quantize.FakeQuantize.enable_fake_quant(self,enabled=True)
torch.quantization.fake_quantize.FakeQuantize.enable_observer(self,enabled=True)
torch.quantization.fake_quantize.FakeQuantize.extra_repr(self)
torch.quantization.fake_quantize.FakeQuantize.forward(self,X)
torch.quantization.fake_quantize.disable_fake_quant(mod)
torch.quantization.fake_quantize.disable_observer(mod)
torch.quantization.fake_quantize.enable_fake_quant(mod)
torch.quantization.fake_quantize.enable_observer(mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/__init__.py----------------------------------------
A:torch.autograd.__init__.grad_tensors->_make_grads(tensors, grad_tensors)
A:torch.autograd.__init__.grad_outputs->_make_grads(outputs, grad_outputs)
torch.autograd.__init__._is_checkpoint_valid()
torch.autograd.__init__._make_grads(outputs,grads)
torch.autograd.__init__.backward(tensors,grad_tensors=None,retain_graph=None,create_graph=False,grad_variables=None)
torch.autograd.__init__.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)
torch.autograd.__init__.variable(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/__init__.pyi----------------------------------------
torch.autograd.__init__.Function
torch.autograd.__init__.Function.backward(ctx:Any,*grad_outputs:Any)->Any
torch.autograd.__init__.Function.forward(ctx:Any,*args:Any,**kwargs:Any)->Any
torch.autograd.__init__.NestedIOFunction(Function)
torch.autograd.__init__.NestedIOFunction.backward(self,*gradients:Any)->Any
torch.autograd.__init__.NestedIOFunction.backward_extended(self,*grad_output:Any)->None
torch.autograd.__init__.NestedIOFunction.forward(self,*args:Any)->tuple
torch.autograd.__init__.NestedIOFunction.forward_extended(self,*input:Any)->None
torch.autograd.__init__.NestedIOFunction.mark_dirty(self,*args:Any,**kwargs:Any)->None
torch.autograd.__init__.NestedIOFunction.mark_non_differentiable(self,*args:Any,**kwargs:Any)->None
torch.autograd.__init__.NestedIOFunction.save_for_backward(self,*args:Any)->None
torch.autograd.__init__.Variable
torch.autograd.__init__.detect_anomaly
torch.autograd.__init__.detect_anomaly.__enter__(self)->None
torch.autograd.__init__.detect_anomaly.__exit__(self,*args:Any)->bool
torch.autograd.__init__.gradcheck(func:Callable[...,Union[Tensor,Tuple[Tensor,...]]],inputs:Union[Tensor,Tuple[Tensor,...]],eps:float=...,atol:float=...,rtol:float=...,raise_exception:bool=...,check_sparse_nnz:bool=...)->bool
torch.autograd.__init__.gradgradcheck(func:Callable[...,Union[Tensor,Tuple[Tensor,...]]],inputs:Union[Tensor,Tuple[Tensor,...]],eps:float=...,atol:float=...,rtol:float=...,gen_non_contig_grad_outputs:bool=...,raise_exception:bool=...)->bool
torch.autograd.__init__.set_detect_anomaly(self,mode:bool)
torch.autograd.__init__.set_detect_anomaly.__enter__(self)->None
torch.autograd.__init__.set_detect_anomaly.__exit__(self,*args:Any)->bool
torch.autograd.__init__.set_detect_anomaly.__init__(self,mode:bool)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/variable.py----------------------------------------
A:torch.autograd.variable.Variable._execution_engine->ImperativeEngine()
torch.autograd.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.VariableMeta(type)
torch.autograd.VariableMeta.__instancecheck__(cls,other)
torch.autograd.variable.Variable(with_metaclass(VariableMeta,torch._C._LegacyVariableBase))
torch.autograd.variable.VariableMeta(type)
torch.autograd.variable.VariableMeta.__instancecheck__(cls,other)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/anomaly_mode.py----------------------------------------
A:torch.autograd.anomaly_mode.self.prev->torch.is_anomaly_enabled()
torch.autograd.anomaly_mode.detect_anomaly(self)
torch.autograd.anomaly_mode.detect_anomaly.__enter__(self)
torch.autograd.anomaly_mode.detect_anomaly.__exit__(self,*args)
torch.autograd.anomaly_mode.detect_anomaly.__init__(self)
torch.autograd.anomaly_mode.set_detect_anomaly(self,mode)
torch.autograd.anomaly_mode.set_detect_anomaly.__enter__(self)
torch.autograd.anomaly_mode.set_detect_anomaly.__exit__(self,*args)
torch.autograd.anomaly_mode.set_detect_anomaly.__init__(self,mode)
torch.autograd.detect_anomaly(self)
torch.autograd.detect_anomaly.__enter__(self)
torch.autograd.detect_anomaly.__exit__(self,*args)
torch.autograd.set_detect_anomaly(self,mode)
torch.autograd.set_detect_anomaly.__enter__(self)
torch.autograd.set_detect_anomaly.__exit__(self,*args)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/grad_mode.py----------------------------------------
A:torch.autograd.grad_mode.self.prev->torch.is_grad_enabled()
torch.autograd.grad_mode.enable_grad(self,func)
torch.autograd.grad_mode.enable_grad.__call__(self,func)
torch.autograd.grad_mode.enable_grad.__enter__(self)
torch.autograd.grad_mode.enable_grad.__exit__(self,*args)
torch.autograd.grad_mode.no_grad(self,func)
torch.autograd.grad_mode.no_grad.__call__(self,func)
torch.autograd.grad_mode.no_grad.__enter__(self)
torch.autograd.grad_mode.no_grad.__exit__(self,*args)
torch.autograd.grad_mode.set_grad_enabled(self,mode)
torch.autograd.grad_mode.set_grad_enabled.__enter__(self)
torch.autograd.grad_mode.set_grad_enabled.__exit__(self,*args)
torch.autograd.grad_mode.set_grad_enabled.__init__(self,mode)
torch.enable_grad(self,func)
torch.enable_grad.__enter__(self)
torch.enable_grad.__exit__(self,*args)
torch.no_grad(self,func)
torch.no_grad.__enter__(self)
torch.no_grad.__exit__(self,*args)
torch.set_grad_enabled(self,mode)
torch.set_grad_enabled.__enter__(self)
torch.set_grad_enabled.__exit__(self,*args)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/grad_mode.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/profiler.py----------------------------------------
A:torch.autograd.profiler.events->EventList(sorted(events, key=lambda evt: getattr(evt, sort_by), reverse=True))
A:torch.autograd.profiler.threads->itertools.groupby(events, key=attrgetter('thread'))
A:torch.autograd.profiler.thread_events->sorted(thread_events, key=lambda event: [event.cpu_interval.start, -event.cpu_interval.end])
A:torch.autograd.profiler.stats->defaultdict(FunctionEventAvg)
A:torch.autograd.profiler.total_stat->FunctionEventAvg()
A:torch.autograd.profiler.records->torch.autograd._disable_profiler()
A:torch.autograd.profiler.self.function_events->EventList(parse_cpu_trace(records))
A:torch.autograd.profiler.cpu_time_str->attr_formatter('cpu_time')
A:torch.autograd.profiler.cuda_time_str->attr_formatter('cuda_time')
A:torch.autograd.profiler.cpu_time_total_str->attr_formatter('cpu_time_total')
A:torch.autograd.profiler.cuda_time_total_str->attr_formatter('cuda_time_total')
A:torch.autograd.profiler.self_cpu_time_total_str->attr_formatter('self_cpu_time_total')
A:torch.autograd.profiler.Kernel->namedtuple('Kernel', ['name', 'device', 'interval'])
A:torch.autograd.profiler.self.cpu_interval->Interval(cpu_start, cpu_end)
A:torch.autograd.profiler.self[key]->torch._C._demangle(key)
A:torch.autograd.profiler.string_table->StringTable()
A:torch.autograd.profiler.(function_id, start)->record_stack.pop()
A:torch.autograd.profiler.fe->FunctionEvent(id=function_id, name=string_table[start.name()], thread=start.thread_id(), cpu_start=start_record.cpu_elapsed_us(start), cpu_end=start_record.cpu_elapsed_us(record), input_shapes=start.shapes())
A:torch.autograd.profiler.cuda_start->adjusted_time(start)
A:torch.autograd.profiler.cuda_end->adjusted_time(record)
A:torch.autograd.profiler.self.seen->set()
A:torch.autograd.profiler.conn->sqlite3.connect(path)
A:torch.autograd.profiler.strings[r['id']]->torch._C._demangle(r['value'])
A:torch.autograd.profiler.unique->EnforceUnique()
A:torch.autograd.profiler.evt->FunctionEvent(id=row['marker_id'], name=strings[row['name']], cpu_start=row['start_time'], cpu_end=row['end_time'], thread=0)
A:torch.autograd.profiler.has_input_shapes->any([event.input_shapes is not None for event in events])
A:torch.autograd.profiler.self_cpu_time_total->sum([event.self_cpu_time_total for event in events])
A:torch.autograd.profiler.cuda_time_total->sum([evt.cuda_time_total for evt in events])
torch.autograd.profiler.EnforceUnique(self)
torch.autograd.profiler.EnforceUnique.__init__(self)
torch.autograd.profiler.EnforceUnique.see(self,*key)
torch.autograd.profiler.EventList(self,*args,**kwargs)
torch.autograd.profiler.EventList.__init__(self,*args,**kwargs)
torch.autograd.profiler.EventList.__str__(self)
torch.autograd.profiler.EventList.cpu_children_populated(self)
torch.autograd.profiler.EventList.export_chrome_trace(self,path)
torch.autograd.profiler.EventList.key_averages(self,group_by_input_shapes=False)
torch.autograd.profiler.EventList.populate_cpu_children(self)
torch.autograd.profiler.EventList.self_cpu_time_total(self)
torch.autograd.profiler.EventList.table(self,sort_by=None,row_limit=100,header=None)
torch.autograd.profiler.EventList.total_average(self)
torch.autograd.profiler.FormattedTimesMixin(object)
torch.autograd.profiler.FormattedTimesMixin.cpu_time(self)
torch.autograd.profiler.FormattedTimesMixin.cuda_time(self)
torch.autograd.profiler.FunctionEvent(self,id,name,thread,cpu_start,cpu_end,input_shapes=None)
torch.autograd.profiler.FunctionEvent.__init__(self,id,name,thread,cpu_start,cpu_end,input_shapes=None)
torch.autograd.profiler.FunctionEvent.__repr__(self)
torch.autograd.profiler.FunctionEvent.append_cpu_child(self,child)
torch.autograd.profiler.FunctionEvent.append_kernel(self,name,device,start,end)
torch.autograd.profiler.FunctionEvent.cpu_time_total(self)
torch.autograd.profiler.FunctionEvent.cuda_time_total(self)
torch.autograd.profiler.FunctionEvent.key(self)
torch.autograd.profiler.FunctionEvent.self_cpu_time_total(self)
torch.autograd.profiler.FunctionEventAvg(self)
torch.autograd.profiler.FunctionEventAvg.__init__(self)
torch.autograd.profiler.FunctionEventAvg.__repr__(self)
torch.autograd.profiler.FunctionEventAvg.add(self,other,group_by_input_shapes=False)
torch.autograd.profiler.Interval(self,start,end)
torch.autograd.profiler.Interval.__init__(self,start,end)
torch.autograd.profiler.Interval.elapsed_us(self)
torch.autograd.profiler.StringTable(defaultdict)
torch.autograd.profiler.StringTable.__missing__(self,key)
torch.autograd.profiler.attr_formatter(name)
torch.autograd.profiler.build_table(events,sort_by=None,header=None,row_limit=100)
torch.autograd.profiler.emit_nvtx(self,enabled=True,record_shapes=False)
torch.autograd.profiler.emit_nvtx.__enter__(self)
torch.autograd.profiler.emit_nvtx.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.emit_nvtx.__init__(self,enabled=True,record_shapes=False)
torch.autograd.profiler.format_time(time_us)
torch.autograd.profiler.format_time_share(time_us,total_time_us)
torch.autograd.profiler.load_nvprof(path)
torch.autograd.profiler.parse_cpu_trace(thread_records)
torch.autograd.profiler.parse_nvprof_trace(path)
torch.autograd.profiler.profile(self,enabled=True,use_cuda=False,record_shapes=False)
torch.autograd.profiler.profile.__enter__(self)
torch.autograd.profiler.profile.__exit__(self,exc_type,exc_val,exc_tb)
torch.autograd.profiler.profile.__init__(self,enabled=True,use_cuda=False,record_shapes=False)
torch.autograd.profiler.profile.__repr__(self)
torch.autograd.profiler.profile.__str__(self)
torch.autograd.profiler.profile._check_finish(self)
torch.autograd.profiler.profile.export_chrome_trace(self,path)
torch.autograd.profiler.profile.key_averages(self,group_by_input_shape=False)
torch.autograd.profiler.profile.self_cpu_time_total(self)
torch.autograd.profiler.profile.table(self,sort_by=None,row_limit=100,header=None)
torch.autograd.profiler.profile.total_average(self)
torch.autograd.profiler.record_function(self,name)
torch.autograd.profiler.record_function.__enter__(self)
torch.autograd.profiler.record_function.__exit__(self,*args)
torch.autograd.profiler.record_function.__init__(self,name)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/gradcheck.py----------------------------------------
A:torch.autograd.gradcheck.jacobians->list(filter(lambda x: x is not None, (make_jacobian(elem, num_out) for elem in input)))
A:torch.autograd.gradcheck.output_size->fn(input).numel()
A:torch.autograd.gradcheck.jacobian->make_jacobian(input, output.numel())
A:torch.autograd.gradcheck.dim->len(size)
A:torch.autograd.gradcheck.x_nnz->x_tensor._nnz()
A:torch.autograd.gradcheck.x_size->list(x_tensor.size())
A:torch.autograd.gradcheck.x_indices->x_tensor._indices().t()
A:torch.autograd.gradcheck.x_stride->get_stride(x_size)
A:torch.autograd.gradcheck.d_idx->sum((indices[k] * x_stride[k] for k in range(len(x_size))))
A:torch.autograd.gradcheck.orig->x_tensor[x_idx].item()
A:torch.autograd.gradcheck.outa->fn(input).clone()
A:torch.autograd.gradcheck.outb->fn(input).clone()
A:torch.autograd.gradcheck.d_tensor[d_idx]->r.detach().reshape(-1)
A:torch.autograd.gradcheck.x_tensor_dense->x_tensor.to_dense()
A:torch.autograd.gradcheck.x_tensor_mkl->x_tensor.to_dense().to_mkldnn()
A:torch.autograd.gradcheck.diff_input_list->list(iter_tensors(tupled_inputs, True))
A:torch.autograd.gradcheck.jacobian_reentrant->make_jacobian(input, output.numel())
A:torch.autograd.gradcheck.grad_output->torch.zeros_like(output)
A:torch.autograd.gradcheck.flat_grad_output->torch.zeros_like(output).view(-1)
A:torch.autograd.gradcheck.grads_input->torch.autograd.grad(output, diff_input_list, [torch.zeros_like(o) for o in output], allow_unused=True)
A:torch.autograd.gradcheck.jacobian_x[:, i]->d_x_dense.contiguous().view(-1)
A:torch.autograd.gradcheck.tupled_inputs->_as_tuple(inputs)
A:torch.autograd.gradcheck.func_out->func(*tupled_inputs)
A:torch.autograd.gradcheck.output->_differentiable_outputs(func(*tupled_inputs))
A:torch.autograd.gradcheck.numerical->get_numerical_jacobian(fn, tupled_inputs, eps=eps)
A:torch.autograd.gradcheck.(analytical, reentrant, correct_grad_sizes)->get_analytical_jacobian(tupled_inputs, o, nondet_tol=nondet_tol)
A:torch.autograd.gradcheck.gi->gi.to_dense().to_dense()
A:torch.autograd.gradcheck.i->i.to_dense().to_dense()
A:torch.autograd.gradcheck.y->torch.testing.make_non_contiguous(y)
A:torch.autograd.gradcheck.outputs->_differentiable_outputs(func(*input_args))
A:torch.autograd.gradcheck.tupled_grad_outputs->_as_tuple(grad_outputs)
A:torch.autograd.gradcheck.num_outputs->len(tupled_grad_outputs)
A:torch.autograd.gradcheck.input_args->tuple((x for x in input_args if isinstance(x, torch.Tensor) and x.requires_grad))
A:torch.autograd.gradcheck.grad_inputs->torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)
torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)
torch.autograd.gradcheck._as_tuple(x)
torch.autograd.gradcheck._differentiable_outputs(x)
torch.autograd.gradcheck.get_analytical_jacobian(input,output,nondet_tol=0.0)
torch.autograd.gradcheck.get_numerical_jacobian(fn,input,target=None,eps=0.001)
torch.autograd.gradcheck.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)
torch.autograd.gradcheck.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)
torch.autograd.gradcheck.iter_tensors(x,only_requiring_grad=False)
torch.autograd.gradcheck.make_jacobian(input,num_out)
torch.autograd.gradcheck.zero_gradients(x)
torch.autograd.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/function.py----------------------------------------
A:torch.autograd.function.backward_hooks->OrderedDict()
A:torch.autograd.function.handle->torch.utils.hooks.RemovableHandle(backward_hooks)
A:torch.autograd.function.forward->super_cls.__dict__.get('forward')
A:torch.autograd.function.backward_fn->type(name + 'Backward', (BackwardCFunction,), {'_forward_cls': cls})
A:torch.autograd.function.outputs->fn(ctx, *args)
A:torch.autograd.function.requires_grad->any((isinstance(arg, torch.Tensor) and arg.requires_grad for arg in args))
A:torch.autograd.function.err_fn->torch._C._functions.DelayedError(b'trying to differentiate twice a function that was markedwith @once_differentiable', len(outputs))
A:torch.autograd.function.var->var.detach().detach()
A:torch.autograd.function.obj->conversion(obj)
A:torch.autograd.function.(res_e, input)->unflatten_helper(input, e)
A:torch.autograd.function._iter_jit_values->_iter_filter(lambda o: o is None or isinstance(o, torch._C.Value), condition_msg="jit's Values or None")
A:torch.autograd.function._iter_tensors->_iter_filter(lambda x: isinstance(x, torch.Tensor), condition_msg='Tensors', conversion=_jit_unwrap_structured)
A:torch.autograd.function._iter_tensors_permissive->_iter_filter(lambda x: isinstance(x, torch.Tensor), allow_unknown=True, condition_msg='Tensors (permissive)')
A:torch.autograd.function._iter_None_tensors->_iter_filter(lambda o: o is None or isinstance(o, torch.Tensor), condition_msg='Tensors or None')
A:torch.autograd.function._map_tensor_data->_nested_map(lambda x: isinstance(x, torch.Tensor), lambda o: o.data, condition_msg='Tensors')
A:torch.autograd.function.flat_input->tuple(_iter_tensors(input))
A:torch.autograd.function.flat_output->super(NestedIOFunction, self)._do_forward(*flat_input)
A:torch.autograd.function.nested_tensors->_map_tensor_data(self._nested_input)
A:torch.autograd.function.result->self.forward_extended(*nested_tensors)
A:torch.autograd.function.nested_gradients->_unflatten(gradients, self._nested_output)
A:torch.autograd.function.self.to_save->tuple(_iter_tensors(args))
A:torch.autograd.function.self.dirty_tensors->tuple(_iter_tensors((args, kwargs)))
A:torch.autograd.function.self.non_differentiable->tuple(_iter_tensors((args, kwargs)))
torch.autograd.Function(with_metaclass(FunctionMeta,_C._FunctionBase,_ContextMethodMixin,_HookMixin))
torch.autograd.Function.backward(ctx,*grad_outputs)
torch.autograd.Function.forward(ctx,*args,**kwargs)
torch.autograd.FunctionMeta(cls,name,bases,attrs)
torch.autograd.NestedIOFunction(Function)
torch.autograd.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.NestedIOFunction._do_forward(self,*input)
torch.autograd.NestedIOFunction.backward(self,*gradients)
torch.autograd.NestedIOFunction.backward_extended(self,*grad_output)
torch.autograd.NestedIOFunction.forward(self,*args)
torch.autograd.NestedIOFunction.forward_extended(self,*input)
torch.autograd.NestedIOFunction.mark_dirty(self,*args,**kwargs)
torch.autograd.NestedIOFunction.mark_non_differentiable(self,*args,**kwargs)
torch.autograd.NestedIOFunction.save_for_backward(self,*args)
torch.autograd.NestedIOFunction.saved_tensors(self)
torch.autograd.function.BackwardCFunction(_C._FunctionBase,_ContextMethodMixin,_HookMixin)
torch.autograd.function.BackwardCFunction.apply(self,*args)
torch.autograd.function.Function(with_metaclass(FunctionMeta,_C._FunctionBase,_ContextMethodMixin,_HookMixin))
torch.autograd.function.Function.backward(ctx,*grad_outputs)
torch.autograd.function.Function.forward(ctx,*args,**kwargs)
torch.autograd.function.FunctionMeta(cls,name,bases,attrs)
torch.autograd.function.FunctionMeta.__init__(cls,name,bases,attrs)
torch.autograd.function.InplaceFunction(self,inplace=False)
torch.autograd.function.InplaceFunction.__init__(self,inplace=False)
torch.autograd.function.NestedIOFunction(Function)
torch.autograd.function.NestedIOFunction._do_backward(self,gradients,retain_variables)
torch.autograd.function.NestedIOFunction._do_forward(self,*input)
torch.autograd.function.NestedIOFunction.backward(self,*gradients)
torch.autograd.function.NestedIOFunction.backward_extended(self,*grad_output)
torch.autograd.function.NestedIOFunction.forward(self,*args)
torch.autograd.function.NestedIOFunction.forward_extended(self,*input)
torch.autograd.function.NestedIOFunction.mark_dirty(self,*args,**kwargs)
torch.autograd.function.NestedIOFunction.mark_non_differentiable(self,*args,**kwargs)
torch.autograd.function.NestedIOFunction.save_for_backward(self,*args)
torch.autograd.function.NestedIOFunction.saved_tensors(self)
torch.autograd.function._ContextMethodMixin(object)
torch.autograd.function._ContextMethodMixin.mark_dirty(self,*args)
torch.autograd.function._ContextMethodMixin.mark_non_differentiable(self,*args)
torch.autograd.function._ContextMethodMixin.mark_shared_storage(self,*pairs)
torch.autograd.function._ContextMethodMixin.save_for_backward(self,*tensors)
torch.autograd.function._HookMixin(object)
torch.autograd.function._HookMixin._register_hook(backward_hooks,hook)
torch.autograd.function._iter_filter(condition,allow_unknown=False,condition_msg=None,conversion=None)
torch.autograd.function._jit_unwrap_structured(obj)
torch.autograd.function._nested_map(condition,fn,condition_msg=None)
torch.autograd.function._unflatten(input,proto)
torch.autograd.function.once_differentiable(fn)
torch.autograd.function.traceable(fn_cls)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/_functions/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py----------------------------------------
A:torch.autograd._functions.tensor.ctx.input_type->type(i)
A:torch.autograd._functions.tensor.ctx.numel->reduce(lambda x, y: x * y, sizes, 1)
A:torch.autograd._functions.tensor.ctx.input_sizes->tensor.size()
A:torch.autograd._functions.tensor.result->tensor.new(tensor).contiguous().view(*sizes)
torch.autograd._functions.Resize(Function)
torch.autograd._functions.Resize.backward(ctx,grad_output)
torch.autograd._functions.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.Type(Function)
torch.autograd._functions.Type.backward(ctx,grad_output)
torch.autograd._functions.Type.forward(ctx,i,dest_type)
torch.autograd._functions.tensor.Resize(Function)
torch.autograd._functions.tensor.Resize.backward(ctx,grad_output)
torch.autograd._functions.tensor.Resize.forward(ctx,tensor,sizes)
torch.autograd._functions.tensor.Type(Function)
torch.autograd._functions.tensor.Type.backward(ctx,grad_output)
torch.autograd._functions.tensor.Type.forward(ctx,i,dest_type)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/autograd/_functions/utils.py----------------------------------------
A:torch.autograd._functions.utils.tensor->tensor.sum(dim, keepdim=True).sum(dim, keepdim=True)
A:torch.autograd._functions.utils.len1->len(dims1)
A:torch.autograd._functions.utils.len2->len(dims2)
A:torch.autograd._functions.utils.numel1->reduce(lambda x, y: x * y, dims1)
A:torch.autograd._functions.utils.numel2->reduce(lambda x, y: x * y, dims2)
torch.autograd._functions.utils.check_onnx_broadcast(dims1,dims2)
torch.autograd._functions.utils.maybe_unexpand(tensor,old_size,check_same_size=True)
torch.autograd._functions.utils.maybe_view(tensor,size,check_same_size=True)
torch.autograd._functions.utils.prepare_onnx_paddings(dim,pad)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/beta.py----------------------------------------
A:torch.distributions.beta.concentration1_concentration0->torch.stack([concentration1, concentration0], -1)
A:torch.distributions.beta.(concentration1, concentration0)->broadcast_all(concentration1, concentration0)
A:torch.distributions.beta.self._dirichlet->Dirichlet(concentration1_concentration0)
A:torch.distributions.beta.new->self._get_checked_instance(Beta, _instance)
A:torch.distributions.beta.batch_shape->torch.Size(batch_shape)
A:torch.distributions.beta.new._dirichlet->self._dirichlet.expand(batch_shape)
A:torch.distributions.beta.heads_tails->torch.stack([value, 1.0 - value], -1)
torch.distributions.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.Beta._log_normalizer(self,x,y)
torch.distributions.Beta._natural_params(self)
torch.distributions.Beta.concentration0(self)
torch.distributions.Beta.concentration1(self)
torch.distributions.Beta.entropy(self)
torch.distributions.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.Beta.log_prob(self,value)
torch.distributions.Beta.mean(self)
torch.distributions.Beta.rsample(self,sample_shape=())
torch.distributions.Beta.variance(self)
torch.distributions.beta.Beta(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta.__init__(self,concentration1,concentration0,validate_args=None)
torch.distributions.beta.Beta._log_normalizer(self,x,y)
torch.distributions.beta.Beta._natural_params(self)
torch.distributions.beta.Beta.concentration0(self)
torch.distributions.beta.Beta.concentration1(self)
torch.distributions.beta.Beta.entropy(self)
torch.distributions.beta.Beta.expand(self,batch_shape,_instance=None)
torch.distributions.beta.Beta.log_prob(self,value)
torch.distributions.beta.Beta.mean(self)
torch.distributions.beta.Beta.rsample(self,sample_shape=())
torch.distributions.beta.Beta.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py----------------------------------------
A:torch.distributions.multivariate_normal.n->bx.permute(permute_dims).size(-1)
A:torch.distributions.multivariate_normal.bx_batch_dims->len(bx_batch_shape)
A:torch.distributions.multivariate_normal.bx->bx.permute(permute_dims).permute(permute_dims)
A:torch.distributions.multivariate_normal.flat_L->bL.reshape(-1, n, n)
A:torch.distributions.multivariate_normal.flat_x->bx.permute(permute_dims).permute(permute_dims).reshape(-1, flat_L.size(0), n)
A:torch.distributions.multivariate_normal.flat_x_swap->bx.permute(permute_dims).permute(permute_dims).reshape(-1, flat_L.size(0), n).permute(1, 2, 0)
A:torch.distributions.multivariate_normal.M_swap->torch.triangular_solve(flat_x_swap, flat_L, upper=False)[0].pow(2).sum(-2)
A:torch.distributions.multivariate_normal.M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff)
A:torch.distributions.multivariate_normal.permuted_M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff).reshape(bx.shape[:-1])
A:torch.distributions.multivariate_normal.permute_inv_dims->list(range(outer_batch_dims))
A:torch.distributions.multivariate_normal.reshaped_M->_batch_mahalanobis(self._unbroadcasted_scale_tril, diff).reshape(bx.shape[:-1]).permute(permute_inv_dims)
A:torch.distributions.multivariate_normal.Lf->torch.cholesky(torch.flip(P, (-2, -1)))
A:torch.distributions.multivariate_normal.L_inv->torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)
A:torch.distributions.multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.multivariate_normal.(self.scale_tril, loc_)->torch.broadcast_tensors(scale_tril, loc_)
A:torch.distributions.multivariate_normal.(self.covariance_matrix, loc_)->torch.broadcast_tensors(covariance_matrix, loc_)
A:torch.distributions.multivariate_normal.(self.precision_matrix, loc_)->torch.broadcast_tensors(precision_matrix, loc_)
A:torch.distributions.multivariate_normal.self._unbroadcasted_scale_tril->_precision_to_scale_tril(precision_matrix)
A:torch.distributions.multivariate_normal.new->self._get_checked_instance(MultivariateNormal, _instance)
A:torch.distributions.multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.multivariate_normal.new.covariance_matrix->self.covariance_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.scale_tril->self.scale_tril.expand(cov_shape)
A:torch.distributions.multivariate_normal.new.precision_matrix->self.precision_matrix.expand(cov_shape)
A:torch.distributions.multivariate_normal.scale_tril_inv->torch.inverse(self._unbroadcasted_scale_tril)
A:torch.distributions.multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.multivariate_normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.multivariate_normal.half_log_det->self._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)
torch.distributions.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.MultivariateNormal.covariance_matrix(self)
torch.distributions.MultivariateNormal.entropy(self)
torch.distributions.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.MultivariateNormal.log_prob(self,value)
torch.distributions.MultivariateNormal.mean(self)
torch.distributions.MultivariateNormal.precision_matrix(self)
torch.distributions.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.MultivariateNormal.scale_tril(self)
torch.distributions.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal.MultivariateNormal(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.__init__(self,loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)
torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.entropy(self)
torch.distributions.multivariate_normal.MultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.multivariate_normal.MultivariateNormal.log_prob(self,value)
torch.distributions.multivariate_normal.MultivariateNormal.mean(self)
torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix(self)
torch.distributions.multivariate_normal.MultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.multivariate_normal.MultivariateNormal.scale_tril(self)
torch.distributions.multivariate_normal.MultivariateNormal.variance(self)
torch.distributions.multivariate_normal._batch_mahalanobis(bL,bx)
torch.distributions.multivariate_normal._batch_mv(bmat,bvec)
torch.distributions.multivariate_normal._precision_to_scale_tril(P)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/chi2.py----------------------------------------
A:torch.distributions.chi2.new->self._get_checked_instance(Chi2, _instance)
torch.distributions.Chi2(self,df,validate_args=None)
torch.distributions.Chi2.df(self)
torch.distributions.Chi2.expand(self,batch_shape,_instance=None)
torch.distributions.chi2.Chi2(self,df,validate_args=None)
torch.distributions.chi2.Chi2.__init__(self,df,validate_args=None)
torch.distributions.chi2.Chi2.df(self)
torch.distributions.chi2.Chi2.expand(self,batch_shape,_instance=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/cauchy.py----------------------------------------
A:torch.distributions.cauchy.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.cauchy.batch_shape->torch.Size(batch_shape)
A:torch.distributions.cauchy.new->self._get_checked_instance(Cauchy, _instance)
A:torch.distributions.cauchy.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.cauchy.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.cauchy.shape->self._extended_shape(sample_shape)
A:torch.distributions.cauchy.eps->self.loc.new(shape).cauchy_()
torch.distributions.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.Cauchy.cdf(self,value)
torch.distributions.Cauchy.entropy(self)
torch.distributions.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.Cauchy.icdf(self,value)
torch.distributions.Cauchy.log_prob(self,value)
torch.distributions.Cauchy.mean(self)
torch.distributions.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.Cauchy.variance(self)
torch.distributions.cauchy.Cauchy(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.__init__(self,loc,scale,validate_args=None)
torch.distributions.cauchy.Cauchy.cdf(self,value)
torch.distributions.cauchy.Cauchy.entropy(self)
torch.distributions.cauchy.Cauchy.expand(self,batch_shape,_instance=None)
torch.distributions.cauchy.Cauchy.icdf(self,value)
torch.distributions.cauchy.Cauchy.log_prob(self,value)
torch.distributions.cauchy.Cauchy.mean(self)
torch.distributions.cauchy.Cauchy.rsample(self,sample_shape=torch.Size())
torch.distributions.cauchy.Cauchy.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/bernoulli.py----------------------------------------
A:torch.distributions.bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.bernoulli.new->self._get_checked_instance(Bernoulli, _instance)
A:torch.distributions.bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.bernoulli.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.Bernoulli._log_normalizer(self,x)
torch.distributions.Bernoulli._natural_params(self)
torch.distributions.Bernoulli._new(self,*args,**kwargs)
torch.distributions.Bernoulli.entropy(self)
torch.distributions.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.Bernoulli.log_prob(self,value)
torch.distributions.Bernoulli.logits(self)
torch.distributions.Bernoulli.mean(self)
torch.distributions.Bernoulli.param_shape(self)
torch.distributions.Bernoulli.probs(self)
torch.distributions.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.Bernoulli.variance(self)
torch.distributions.bernoulli.Bernoulli(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.bernoulli.Bernoulli._log_normalizer(self,x)
torch.distributions.bernoulli.Bernoulli._natural_params(self)
torch.distributions.bernoulli.Bernoulli._new(self,*args,**kwargs)
torch.distributions.bernoulli.Bernoulli.entropy(self)
torch.distributions.bernoulli.Bernoulli.enumerate_support(self,expand=True)
torch.distributions.bernoulli.Bernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.bernoulli.Bernoulli.log_prob(self,value)
torch.distributions.bernoulli.Bernoulli.logits(self)
torch.distributions.bernoulli.Bernoulli.mean(self)
torch.distributions.bernoulli.Bernoulli.param_shape(self)
torch.distributions.bernoulli.Bernoulli.probs(self)
torch.distributions.bernoulli.Bernoulli.sample(self,sample_shape=torch.Size())
torch.distributions.bernoulli.Bernoulli.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/categorical.py----------------------------------------
A:torch.distributions.categorical.new->self._get_checked_instance(Categorical, _instance)
A:torch.distributions.categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.categorical.new.probs->self.probs.expand(param_shape)
A:torch.distributions.categorical.new.logits->self.logits.expand(param_shape)
A:torch.distributions.categorical.sample_shape->self._extended_shape(sample_shape)
A:torch.distributions.categorical.probs->self.probs.expand(param_shape)
A:torch.distributions.categorical.probs_2d->self.probs.expand(param_shape).reshape(-1, self._num_events)
A:torch.distributions.categorical.sample_2d->torch.multinomial(probs_2d, 1, True)
A:torch.distributions.categorical.value->value.long().unsqueeze(-1).long().unsqueeze(-1)
A:torch.distributions.categorical.(value, log_pmf)->torch.broadcast_tensors(value, self.logits)
A:torch.distributions.categorical.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.Categorical._new(self,*args,**kwargs)
torch.distributions.Categorical.entropy(self)
torch.distributions.Categorical.enumerate_support(self,expand=True)
torch.distributions.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.Categorical.log_prob(self,value)
torch.distributions.Categorical.logits(self)
torch.distributions.Categorical.mean(self)
torch.distributions.Categorical.param_shape(self)
torch.distributions.Categorical.probs(self)
torch.distributions.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.Categorical.support(self)
torch.distributions.Categorical.variance(self)
torch.distributions.categorical.Categorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.categorical.Categorical._new(self,*args,**kwargs)
torch.distributions.categorical.Categorical.entropy(self)
torch.distributions.categorical.Categorical.enumerate_support(self,expand=True)
torch.distributions.categorical.Categorical.expand(self,batch_shape,_instance=None)
torch.distributions.categorical.Categorical.log_prob(self,value)
torch.distributions.categorical.Categorical.logits(self)
torch.distributions.categorical.Categorical.mean(self)
torch.distributions.categorical.Categorical.param_shape(self)
torch.distributions.categorical.Categorical.probs(self)
torch.distributions.categorical.Categorical.sample(self,sample_shape=torch.Size())
torch.distributions.categorical.Categorical.support(self)
torch.distributions.categorical.Categorical.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/weibull.py----------------------------------------
A:torch.distributions.weibull.(self.scale, self.concentration)->broadcast_all(scale, concentration)
A:torch.distributions.weibull.self.concentration_reciprocal->self.concentration.reciprocal()
A:torch.distributions.weibull.base_dist->self.base_dist.expand(batch_shape)
A:torch.distributions.weibull.new->self._get_checked_instance(Weibull, _instance)
A:torch.distributions.weibull.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.weibull.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.weibull.new.concentration_reciprocal->self._get_checked_instance(Weibull, _instance).concentration.reciprocal()
torch.distributions.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.Weibull.entropy(self)
torch.distributions.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.Weibull.mean(self)
torch.distributions.Weibull.variance(self)
torch.distributions.weibull.Weibull(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.__init__(self,scale,concentration,validate_args=None)
torch.distributions.weibull.Weibull.entropy(self)
torch.distributions.weibull.Weibull.expand(self,batch_shape,_instance=None)
torch.distributions.weibull.Weibull.mean(self)
torch.distributions.weibull.Weibull.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/geometric.py----------------------------------------
A:torch.distributions.geometric.(self.probs,)->broadcast_all(probs)
A:torch.distributions.geometric.(self.logits,)->broadcast_all(logits)
A:torch.distributions.geometric.batch_shape->torch.Size(batch_shape)
A:torch.distributions.geometric.new->self._get_checked_instance(Geometric, _instance)
A:torch.distributions.geometric.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.geometric.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.geometric.shape->self._extended_shape(sample_shape)
A:torch.distributions.geometric.u->self.probs.new(shape).uniform_(tiny, 1)
A:torch.distributions.geometric.(value, probs)->broadcast_all(value, self.probs.clone())
torch.distributions.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.Geometric.entropy(self)
torch.distributions.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.Geometric.log_prob(self,value)
torch.distributions.Geometric.logits(self)
torch.distributions.Geometric.mean(self)
torch.distributions.Geometric.probs(self)
torch.distributions.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.Geometric.variance(self)
torch.distributions.geometric.Geometric(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.geometric.Geometric.entropy(self)
torch.distributions.geometric.Geometric.expand(self,batch_shape,_instance=None)
torch.distributions.geometric.Geometric.log_prob(self,value)
torch.distributions.geometric.Geometric.logits(self)
torch.distributions.geometric.Geometric.mean(self)
torch.distributions.geometric.Geometric.probs(self)
torch.distributions.geometric.Geometric.sample(self,sample_shape=torch.Size())
torch.distributions.geometric.Geometric.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/one_hot_categorical.py----------------------------------------
A:torch.distributions.one_hot_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.one_hot_categorical.new->self._get_checked_instance(OneHotCategorical, _instance)
A:torch.distributions.one_hot_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.one_hot_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.one_hot_categorical.sample_shape->torch.Size(sample_shape)
A:torch.distributions.one_hot_categorical.indices->self._categorical.sample(sample_shape)
A:torch.distributions.one_hot_categorical.values->values.expand((n,) + self.batch_shape + (n,)).expand((n,) + self.batch_shape + (n,))
torch.distributions.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.OneHotCategorical._param(self)
torch.distributions.OneHotCategorical.entropy(self)
torch.distributions.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.OneHotCategorical.log_prob(self,value)
torch.distributions.OneHotCategorical.logits(self)
torch.distributions.OneHotCategorical.mean(self)
torch.distributions.OneHotCategorical.param_shape(self)
torch.distributions.OneHotCategorical.probs(self)
torch.distributions.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.OneHotCategorical.variance(self)
torch.distributions.one_hot_categorical.OneHotCategorical(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical.__init__(self,probs=None,logits=None,validate_args=None)
torch.distributions.one_hot_categorical.OneHotCategorical._new(self,*args,**kwargs)
torch.distributions.one_hot_categorical.OneHotCategorical._param(self)
torch.distributions.one_hot_categorical.OneHotCategorical.entropy(self)
torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support(self,expand=True)
torch.distributions.one_hot_categorical.OneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.one_hot_categorical.OneHotCategorical.log_prob(self,value)
torch.distributions.one_hot_categorical.OneHotCategorical.logits(self)
torch.distributions.one_hot_categorical.OneHotCategorical.mean(self)
torch.distributions.one_hot_categorical.OneHotCategorical.param_shape(self)
torch.distributions.one_hot_categorical.OneHotCategorical.probs(self)
torch.distributions.one_hot_categorical.OneHotCategorical.sample(self,sample_shape=torch.Size())
torch.distributions.one_hot_categorical.OneHotCategorical.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/normal.py----------------------------------------
A:torch.distributions.normal.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.normal.new->self._get_checked_instance(Normal, _instance)
A:torch.distributions.normal.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.normal.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.normal.eps->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
torch.distributions.Normal(self,loc,scale,validate_args=None)
torch.distributions.Normal._log_normalizer(self,x,y)
torch.distributions.Normal._natural_params(self)
torch.distributions.Normal.cdf(self,value)
torch.distributions.Normal.entropy(self)
torch.distributions.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.Normal.icdf(self,value)
torch.distributions.Normal.log_prob(self,value)
torch.distributions.Normal.mean(self)
torch.distributions.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.Normal.stddev(self)
torch.distributions.Normal.variance(self)
torch.distributions.normal.Normal(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal.__init__(self,loc,scale,validate_args=None)
torch.distributions.normal.Normal._log_normalizer(self,x,y)
torch.distributions.normal.Normal._natural_params(self)
torch.distributions.normal.Normal.cdf(self,value)
torch.distributions.normal.Normal.entropy(self)
torch.distributions.normal.Normal.expand(self,batch_shape,_instance=None)
torch.distributions.normal.Normal.icdf(self,value)
torch.distributions.normal.Normal.log_prob(self,value)
torch.distributions.normal.Normal.mean(self)
torch.distributions.normal.Normal.rsample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.sample(self,sample_shape=torch.Size())
torch.distributions.normal.Normal.stddev(self)
torch.distributions.normal.Normal.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/lowrank_multivariate_normal.py----------------------------------------
A:torch.distributions.lowrank_multivariate_normal.m->W.size(-1)
A:torch.distributions.lowrank_multivariate_normal.K->torch.matmul(Dinvsqrt_W, Dinvsqrt_W.transpose(-1, -2)).contiguous()
A:torch.distributions.lowrank_multivariate_normal.Wt_Dinv_x->_batch_mv(Wt_Dinv, x)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term1->(x.pow(2) / D).sum(-1)
A:torch.distributions.lowrank_multivariate_normal.mahalanobis_term2->_batch_mahalanobis(capacitance_tril, Wt_Dinv_x)
A:torch.distributions.lowrank_multivariate_normal.loc_->loc.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.cov_diag_->cov_diag.unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.(loc_, self.cov_factor, cov_diag_)->torch.broadcast_tensors(loc_, cov_factor, cov_diag_)
A:torch.distributions.lowrank_multivariate_normal.self._capacitance_tril->_batch_capacitance_tril(cov_factor, cov_diag)
A:torch.distributions.lowrank_multivariate_normal.new->self._get_checked_instance(LowRankMultivariateNormal, _instance)
A:torch.distributions.lowrank_multivariate_normal.batch_shape->torch.Size(batch_shape)
A:torch.distributions.lowrank_multivariate_normal.new.loc->self.loc.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_diag->self.cov_diag.expand(loc_shape)
A:torch.distributions.lowrank_multivariate_normal.new.cov_factor->self.cov_factor.expand(loc_shape + self.cov_factor.shape[-1:])
A:torch.distributions.lowrank_multivariate_normal.cov_diag_sqrt_unsqueeze->self._unbroadcasted_cov_diag.sqrt().unsqueeze(-1)
A:torch.distributions.lowrank_multivariate_normal.shape->self._extended_shape(sample_shape)
A:torch.distributions.lowrank_multivariate_normal.eps_W->_standard_normal(W_shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.eps_D->_standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)
A:torch.distributions.lowrank_multivariate_normal.M->_batch_lowrank_mahalanobis(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, diff, self._capacitance_tril)
A:torch.distributions.lowrank_multivariate_normal.log_det->_batch_lowrank_logdet(self._unbroadcasted_cov_factor, self._unbroadcasted_cov_diag, self._capacitance_tril)
torch.distributions.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.LowRankMultivariateNormal.entropy(self)
torch.distributions.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.LowRankMultivariateNormal.mean(self)
torch.distributions.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.__init__(self,loc,cov_factor,cov_diag,validate_args=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand(self,batch_shape,_instance=None)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob(self,value)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample(self,sample_shape=torch.Size())
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril(self)
torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance(self)
torch.distributions.lowrank_multivariate_normal._batch_capacitance_tril(W,D)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_logdet(W,D,capacitance_tril)
torch.distributions.lowrank_multivariate_normal._batch_lowrank_mahalanobis(W,D,x,capacitance_tril)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/utils.py----------------------------------------
A:torch.distributions.utils.options->dict(dtype=value.dtype, device=value.device)
A:torch.distributions.utils.ps_clamped->clamp_probs(probs)
A:torch.distributions.utils.value->self.wrapped(instance)
torch.distributions.utils._standard_normal(shape,dtype,device)
torch.distributions.utils._sum_rightmost(value,dim)
torch.distributions.utils.broadcast_all(*values)
torch.distributions.utils.clamp_probs(probs)
torch.distributions.utils.lazy_property(self,wrapped)
torch.distributions.utils.lazy_property.__get__(self,instance,obj_type=None)
torch.distributions.utils.lazy_property.__init__(self,wrapped)
torch.distributions.utils.logits_to_probs(logits,is_binary=False)
torch.distributions.utils.probs_to_logits(probs,is_binary=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/relaxed_bernoulli.py----------------------------------------
A:torch.distributions.relaxed_bernoulli.is_scalar->isinstance(logits, Number)
A:torch.distributions.relaxed_bernoulli.(self.probs,)->broadcast_all(probs)
A:torch.distributions.relaxed_bernoulli.(self.logits,)->broadcast_all(logits)
A:torch.distributions.relaxed_bernoulli.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_bernoulli.new->self._get_checked_instance(RelaxedBernoulli, _instance)
A:torch.distributions.relaxed_bernoulli.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.relaxed_bernoulli.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_bernoulli.probs->clamp_probs(self.probs.expand(shape))
A:torch.distributions.relaxed_bernoulli.uniforms->clamp_probs(torch.rand(shape, dtype=probs.dtype, device=probs.device))
A:torch.distributions.relaxed_bernoulli.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_bernoulli.base_dist->LogitRelaxedBernoulli(temperature, probs, logits)
torch.distributions.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedBernoulli.logits(self)
torch.distributions.RelaxedBernoulli.probs(self)
torch.distributions.RelaxedBernoulli.temperature(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli._new(self,*args,**kwargs)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob(self,value)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_bernoulli.RelaxedBernoulli(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs(self)
torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/kl.py----------------------------------------
A:torch.distributions.kl.n->bmat.size(-1)
A:torch.distributions.kl.m->bmat.size(-2)
A:torch.distributions.kl.flat_trace->bmat.reshape(-1, m * n).pow(2).sum(-1)
A:torch.distributions.kl.fun->_dispatch_kl(type(p), type(q))
A:torch.distributions.kl.kl[inf_idxs]->_infinite_like(kl[inf_idxs])
A:torch.distributions.kl.sum_p_concentration->p.concentration.sum(-1)
A:torch.distributions.kl.sum_q_concentration->q.concentration.sum(-1)
A:torch.distributions.kl.t2->p.alpha.reciprocal()
A:torch.distributions.kl.lg_normal->p._log_normalizer(*p_nparams)
A:torch.distributions.kl.gradients->torch.autograd.grad(lg_normal.sum(), p_nparams, create_graph=True)
A:torch.distributions.kl.t3->((p.high + p.low - 2 * q.loc) / 2).pow(2)
A:torch.distributions.kl.loc_abs_diff->(p.loc - q.loc).abs()
A:torch.distributions.kl.term3->_batch_mahalanobis(q._unbroadcasted_scale_tril, q.loc - p.loc)
A:torch.distributions.kl.term21->_batch_trace_XXT(torch.triangular_solve(p_cov_factor, q_scale_tril, upper=False)[0])
A:torch.distributions.kl.term22->_batch_trace_XXT(torch.triangular_solve(p_cov_diag, q_scale_tril, upper=False)[0])
A:torch.distributions.kl.term23->_batch_trace_XXT(A * p._unbroadcasted_cov_diag.sqrt().unsqueeze(-2))
A:torch.distributions.kl.term24->_batch_trace_XXT(A.matmul(p._unbroadcasted_cov_factor))
A:torch.distributions.kl.combined_batch_shape->torch._C._infer_size(q._unbroadcasted_scale_tril.shape[:-2], p._unbroadcasted_scale_tril.shape[:-2])
A:torch.distributions.kl.q_scale_tril->q._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_cov_factor->p._unbroadcasted_cov_factor.expand(combined_batch_shape + (n, p.cov_factor.size(-1)))
A:torch.distributions.kl.p_cov_diag->torch.diag_embed(p._unbroadcasted_cov_diag.sqrt()).expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.p_scale_tril->p._unbroadcasted_scale_tril.expand(combined_batch_shape + (n, n))
A:torch.distributions.kl.term2->_batch_trace_XXT(torch.triangular_solve(p_scale_tril, q_scale_tril, upper=False)[0])
A:torch.distributions.kl.var_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t1->(q.alpha * q.scale.pow(q.alpha) * support_uniform).log()
A:torch.distributions.kl.extra_event_dim->len(p.event_shape)
A:torch.distributions.kl.base_kl_divergence->kl_divergence(p.base_dist, q.base_dist)
A:torch.distributions.kl.result->kl_divergence(p.base_dist, q.base_dist)
A:torch.distributions.kl.var_normal->q.scale.pow(2)
A:torch.distributions.kl.rate_sqr->p.rate.pow(2)
A:torch.distributions.kl.beta_sqr->p.rate.pow(2)
A:torch.distributions.kl.var_scale_sqr_ratio->(p.scale / q.scale).pow(2)
A:torch.distributions.kl.t4->(p.alpha * common_term - q.loc).pow(2)
torch.distributions.kl._Match(self,*types)
torch.distributions.kl._Match.__eq__(self,other)
torch.distributions.kl._Match.__init__(self,*types)
torch.distributions.kl._Match.__le__(self,other)
torch.distributions.kl._batch_trace_XXT(bmat)
torch.distributions.kl._dispatch_kl(type_p,type_q)
torch.distributions.kl._infinite_like(tensor)
torch.distributions.kl._kl_bernoulli_bernoulli(p,q)
torch.distributions.kl._kl_bernoulli_poisson(p,q)
torch.distributions.kl._kl_beta_beta(p,q)
torch.distributions.kl._kl_beta_exponential(p,q)
torch.distributions.kl._kl_beta_gamma(p,q)
torch.distributions.kl._kl_beta_infinity(p,q)
torch.distributions.kl._kl_beta_normal(p,q)
torch.distributions.kl._kl_beta_uniform(p,q)
torch.distributions.kl._kl_binomial_binomial(p,q)
torch.distributions.kl._kl_categorical_categorical(p,q)
torch.distributions.kl._kl_dirichlet_dirichlet(p,q)
torch.distributions.kl._kl_expfamily_expfamily(p,q)
torch.distributions.kl._kl_exponential_exponential(p,q)
torch.distributions.kl._kl_exponential_gamma(p,q)
torch.distributions.kl._kl_exponential_gumbel(p,q)
torch.distributions.kl._kl_exponential_infinity(p,q)
torch.distributions.kl._kl_exponential_normal(p,q)
torch.distributions.kl._kl_gamma_exponential(p,q)
torch.distributions.kl._kl_gamma_gamma(p,q)
torch.distributions.kl._kl_gamma_gumbel(p,q)
torch.distributions.kl._kl_gamma_infinity(p,q)
torch.distributions.kl._kl_gamma_normal(p,q)
torch.distributions.kl._kl_geometric_geometric(p,q)
torch.distributions.kl._kl_gumbel_gumbel(p,q)
torch.distributions.kl._kl_gumbel_infinity(p,q)
torch.distributions.kl._kl_gumbel_normal(p,q)
torch.distributions.kl._kl_halfnormal_halfnormal(p,q)
torch.distributions.kl._kl_independent_independent(p,q)
torch.distributions.kl._kl_laplace_infinity(p,q)
torch.distributions.kl._kl_laplace_laplace(p,q)
torch.distributions.kl._kl_laplace_normal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_lowrankmultivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_lowrankmultivariatenormal(p,q)
torch.distributions.kl._kl_multivariatenormal_multivariatenormal(p,q)
torch.distributions.kl._kl_normal_gumbel(p,q)
torch.distributions.kl._kl_normal_infinity(p,q)
torch.distributions.kl._kl_normal_normal(p,q)
torch.distributions.kl._kl_onehotcategorical_onehotcategorical(p,q)
torch.distributions.kl._kl_pareto_exponential(p,q)
torch.distributions.kl._kl_pareto_gamma(p,q)
torch.distributions.kl._kl_pareto_infinity(p,q)
torch.distributions.kl._kl_pareto_normal(p,q)
torch.distributions.kl._kl_pareto_pareto(p,q)
torch.distributions.kl._kl_poisson_infinity(p,q)
torch.distributions.kl._kl_poisson_poisson(p,q)
torch.distributions.kl._kl_transformed_transformed(p,q)
torch.distributions.kl._kl_uniform_beta(p,q)
torch.distributions.kl._kl_uniform_exponetial(p,q)
torch.distributions.kl._kl_uniform_gamma(p,q)
torch.distributions.kl._kl_uniform_gumbel(p,q)
torch.distributions.kl._kl_uniform_normal(p,q)
torch.distributions.kl._kl_uniform_pareto(p,q)
torch.distributions.kl._kl_uniform_uniform(p,q)
torch.distributions.kl._x_log_x(tensor)
torch.distributions.kl.kl_divergence(p,q)
torch.distributions.kl.register_kl(type_p,type_q)
torch.distributions.kl_divergence(p,q)
torch.distributions.register_kl(type_p,type_q)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/gumbel.py----------------------------------------
A:torch.distributions.gumbel.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.gumbel.finfo->torch.finfo(self.loc.dtype)
A:torch.distributions.gumbel.base_dist->Uniform(torch.full_like(self.loc, finfo.tiny), torch.full_like(self.loc, 1 - finfo.eps))
A:torch.distributions.gumbel.new->self._get_checked_instance(Gumbel, _instance)
A:torch.distributions.gumbel.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.gumbel.new.scale->self.scale.expand(batch_shape)
torch.distributions.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.Gumbel.entropy(self)
torch.distributions.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.Gumbel.log_prob(self,value)
torch.distributions.Gumbel.mean(self)
torch.distributions.Gumbel.stddev(self)
torch.distributions.Gumbel.variance(self)
torch.distributions.gumbel.Gumbel(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.__init__(self,loc,scale,validate_args=None)
torch.distributions.gumbel.Gumbel.entropy(self)
torch.distributions.gumbel.Gumbel.expand(self,batch_shape,_instance=None)
torch.distributions.gumbel.Gumbel.log_prob(self,value)
torch.distributions.gumbel.Gumbel.mean(self)
torch.distributions.gumbel.Gumbel.stddev(self)
torch.distributions.gumbel.Gumbel.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/transformed_distribution.py----------------------------------------
A:torch.distributions.transformed_distribution.event_dim->len(self.event_shape)
A:torch.distributions.transformed_distribution.new->self._get_checked_instance(TransformedDistribution, _instance)
A:torch.distributions.transformed_distribution.batch_shape->torch.Size(batch_shape)
A:torch.distributions.transformed_distribution.new.base_dist->self.base_dist.expand(base_dist_batch_shape)
A:torch.distributions.transformed_distribution.x->transform.inv(y)
A:torch.distributions.transformed_distribution.value->transform(value)
torch.distributions.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.TransformedDistribution.cdf(self,value)
torch.distributions.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.TransformedDistribution.has_rsample(self)
torch.distributions.TransformedDistribution.icdf(self,value)
torch.distributions.TransformedDistribution.log_prob(self,value)
torch.distributions.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.TransformedDistribution.support(self)
torch.distributions.transformed_distribution.TransformedDistribution(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution.__init__(self,base_distribution,transforms,validate_args=None)
torch.distributions.transformed_distribution.TransformedDistribution._monotonize_cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.cdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.expand(self,batch_shape,_instance=None)
torch.distributions.transformed_distribution.TransformedDistribution.has_rsample(self)
torch.distributions.transformed_distribution.TransformedDistribution.icdf(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.log_prob(self,value)
torch.distributions.transformed_distribution.TransformedDistribution.rsample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.sample(self,sample_shape=torch.Size())
torch.distributions.transformed_distribution.TransformedDistribution.support(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/exp_family.py----------------------------------------
A:torch.distributions.exp_family.lg_normal->self._log_normalizer(*nparams)
A:torch.distributions.exp_family.gradients->torch.autograd.grad(lg_normal.sum(), nparams, create_graph=True)
torch.distributions.ExponentialFamily(Distribution)
torch.distributions.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.ExponentialFamily._natural_params(self)
torch.distributions.ExponentialFamily.entropy(self)
torch.distributions.exp_family.ExponentialFamily(Distribution)
torch.distributions.exp_family.ExponentialFamily._log_normalizer(self,*natural_params)
torch.distributions.exp_family.ExponentialFamily._mean_carrier_measure(self)
torch.distributions.exp_family.ExponentialFamily._natural_params(self)
torch.distributions.exp_family.ExponentialFamily.entropy(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/logistic_normal.py----------------------------------------
A:torch.distributions.logistic_normal.base_dist->Normal(loc, scale)
A:torch.distributions.logistic_normal.self._event_shape->torch.Size([s + 1 for s in self._event_shape])
A:torch.distributions.logistic_normal.new->self._get_checked_instance(LogisticNormal, _instance)
torch.distributions.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogisticNormal.loc(self)
torch.distributions.LogisticNormal.scale(self)
torch.distributions.logistic_normal.LogisticNormal(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.logistic_normal.LogisticNormal.expand(self,batch_shape,_instance=None)
torch.distributions.logistic_normal.LogisticNormal.loc(self)
torch.distributions.logistic_normal.LogisticNormal.scale(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/constraint_registry.py----------------------------------------
A:torch.distributions.constraint_registry.constraint->type(constraint)
A:torch.distributions.constraint_registry.biject_to->ConstraintRegistry()
A:torch.distributions.constraint_registry.transform_to->ConstraintRegistry()
torch.distributions.constraint_registry.ConstraintRegistry(self)
torch.distributions.constraint_registry.ConstraintRegistry.__init__(self)
torch.distributions.constraint_registry.ConstraintRegistry.register(self,constraint,factory=None)
torch.distributions.constraint_registry._biject_to_cat(constraint)
torch.distributions.constraint_registry._biject_to_simplex(constraint)
torch.distributions.constraint_registry._biject_to_stack(constraint)
torch.distributions.constraint_registry._transform_to_cat(constraint)
torch.distributions.constraint_registry._transform_to_greater_than(constraint)
torch.distributions.constraint_registry._transform_to_interval(constraint)
torch.distributions.constraint_registry._transform_to_less_than(constraint)
torch.distributions.constraint_registry._transform_to_lower_cholesky(constraint)
torch.distributions.constraint_registry._transform_to_positive(constraint)
torch.distributions.constraint_registry._transform_to_real(constraint)
torch.distributions.constraint_registry._transform_to_simplex(constraint)
torch.distributions.constraint_registry._transform_to_stack(constraint)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/poisson.py----------------------------------------
A:torch.distributions.poisson.(self.rate,)->broadcast_all(rate)
A:torch.distributions.poisson.batch_shape->torch.Size(batch_shape)
A:torch.distributions.poisson.new->self._get_checked_instance(Poisson, _instance)
A:torch.distributions.poisson.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.poisson.shape->self._extended_shape(sample_shape)
A:torch.distributions.poisson.(rate, value)->broadcast_all(self.rate, value)
torch.distributions.Poisson(self,rate,validate_args=None)
torch.distributions.Poisson._log_normalizer(self,x)
torch.distributions.Poisson._natural_params(self)
torch.distributions.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.Poisson.log_prob(self,value)
torch.distributions.Poisson.mean(self)
torch.distributions.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.Poisson.variance(self)
torch.distributions.poisson.Poisson(self,rate,validate_args=None)
torch.distributions.poisson.Poisson.__init__(self,rate,validate_args=None)
torch.distributions.poisson.Poisson._log_normalizer(self,x)
torch.distributions.poisson.Poisson._natural_params(self)
torch.distributions.poisson.Poisson.expand(self,batch_shape,_instance=None)
torch.distributions.poisson.Poisson.log_prob(self,value)
torch.distributions.poisson.Poisson.mean(self)
torch.distributions.poisson.Poisson.sample(self,sample_shape=torch.Size())
torch.distributions.poisson.Poisson.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/multinomial.py----------------------------------------
A:torch.distributions.multinomial.self._categorical->Categorical(probs=probs, logits=logits)
A:torch.distributions.multinomial.new->self._get_checked_instance(Multinomial, _instance)
A:torch.distributions.multinomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.multinomial.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.multinomial.sample_shape->torch.Size(sample_shape)
A:torch.distributions.multinomial.samples->samples.permute(*shifted_idx).permute(*shifted_idx)
A:torch.distributions.multinomial.shifted_idx->list(range(samples.dim()))
A:torch.distributions.multinomial.counts->samples.permute(*shifted_idx).permute(*shifted_idx).new(self._extended_shape(sample_shape)).zero_()
A:torch.distributions.multinomial.(logits, value)->broadcast_all(self.logits.clone(), value)
A:torch.distributions.multinomial.log_factorial_n->torch.lgamma(value.sum(-1) + 1)
A:torch.distributions.multinomial.log_factorial_xs->torch.lgamma(value + 1).sum(-1)
A:torch.distributions.multinomial.log_powers->(logits * value).sum(-1)
torch.distributions.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Multinomial._new(self,*args,**kwargs)
torch.distributions.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.Multinomial.log_prob(self,value)
torch.distributions.Multinomial.logits(self)
torch.distributions.Multinomial.mean(self)
torch.distributions.Multinomial.param_shape(self)
torch.distributions.Multinomial.probs(self)
torch.distributions.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.Multinomial.support(self)
torch.distributions.Multinomial.variance(self)
torch.distributions.multinomial.Multinomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.multinomial.Multinomial._new(self,*args,**kwargs)
torch.distributions.multinomial.Multinomial.expand(self,batch_shape,_instance=None)
torch.distributions.multinomial.Multinomial.log_prob(self,value)
torch.distributions.multinomial.Multinomial.logits(self)
torch.distributions.multinomial.Multinomial.mean(self)
torch.distributions.multinomial.Multinomial.param_shape(self)
torch.distributions.multinomial.Multinomial.probs(self)
torch.distributions.multinomial.Multinomial.sample(self,sample_shape=torch.Size())
torch.distributions.multinomial.Multinomial.support(self)
torch.distributions.multinomial.Multinomial.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/transforms.py----------------------------------------
A:torch.distributions.transforms.inv->ComposeTransform([p.inv for p in reversed(self.parts)])
A:torch.distributions.transforms.self._inv->weakref.ref(inv)
A:torch.distributions.transforms.y->y.clamp(min=finfo.tiny, max=1.0 - finfo.eps).clamp(min=finfo.tiny, max=1.0 - finfo.eps)
A:torch.distributions.transforms.x->part(x)
A:torch.distributions.transforms.inv._inv->weakref.ref(self)
A:torch.distributions.transforms.y_tmp->part(x)
A:torch.distributions.transforms.identity_transform->ComposeTransform([])
A:torch.distributions.transforms.(self.exponent,)->broadcast_all(exponent)
A:torch.distributions.transforms.finfo->torch.finfo(y.dtype)
A:torch.distributions.transforms.result->result.view(result_size).sum(-1).view(result_size).sum(-1)
A:torch.distributions.transforms.probs->(logprobs - logprobs.max(-1, True)[0]).exp()
A:torch.distributions.transforms.z->_clipped_sigmoid(x - offset.log())
A:torch.distributions.transforms.z_cumprod->(1 - z).cumprod(-1)
A:torch.distributions.transforms.sf->torch.clamp(sf, min=torch.finfo(y.dtype).tiny)
A:torch.distributions.transforms.detJ->(-x + F.logsigmoid(x) + y[..., :-1].log()).sum(-1)
A:torch.distributions.transforms.self.transforms->list(tseq)
A:torch.distributions.transforms.self.lengths->list(lengths)
A:torch.distributions.transforms.xslice->part(x).narrow(self.dim, start, length)
A:torch.distributions.transforms.yslice->y.clamp(min=finfo.tiny, max=1.0 - finfo.eps).clamp(min=finfo.tiny, max=1.0 - finfo.eps).narrow(self.dim, start, length)
A:torch.distributions.transforms.yslices->self._slice(y)
A:torch.distributions.transforms.xslices->self._slice(x)
torch.distributions.AbsTransform(Transform)
torch.distributions.AbsTransform.__eq__(self,other)
torch.distributions.AbsTransform._call(self,x)
torch.distributions.AbsTransform._inverse(self,y)
torch.distributions.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.AffineTransform.__eq__(self,other)
torch.distributions.AffineTransform._call(self,x)
torch.distributions.AffineTransform._inverse(self,y)
torch.distributions.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.AffineTransform.sign(self)
torch.distributions.CatTransform(self,tseq,dim=0,lengths=None)
torch.distributions.CatTransform._call(self,x)
torch.distributions.CatTransform._inverse(self,y)
torch.distributions.CatTransform.bijective(self)
torch.distributions.CatTransform.codomain(self)
torch.distributions.CatTransform.domain(self)
torch.distributions.CatTransform.length(self)
torch.distributions.CatTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ComposeTransform(self,parts)
torch.distributions.ComposeTransform.__eq__(self,other)
torch.distributions.ComposeTransform.__repr__(self)
torch.distributions.ComposeTransform.bijective(self)
torch.distributions.ComposeTransform.codomain(self)
torch.distributions.ComposeTransform.domain(self)
torch.distributions.ComposeTransform.event_dim(self)
torch.distributions.ComposeTransform.inv(self)
torch.distributions.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.ComposeTransform.sign(self)
torch.distributions.ExpTransform(Transform)
torch.distributions.ExpTransform.__eq__(self,other)
torch.distributions.ExpTransform._call(self,x)
torch.distributions.ExpTransform._inverse(self,y)
torch.distributions.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.LowerCholeskyTransform(Transform)
torch.distributions.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.LowerCholeskyTransform._call(self,x)
torch.distributions.LowerCholeskyTransform._inverse(self,y)
torch.distributions.PowerTransform(self,exponent,cache_size=0)
torch.distributions.PowerTransform.__eq__(self,other)
torch.distributions.PowerTransform._call(self,x)
torch.distributions.PowerTransform._inverse(self,y)
torch.distributions.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SigmoidTransform(Transform)
torch.distributions.SigmoidTransform.__eq__(self,other)
torch.distributions.SigmoidTransform._call(self,x)
torch.distributions.SigmoidTransform._inverse(self,y)
torch.distributions.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.SoftmaxTransform(Transform)
torch.distributions.SoftmaxTransform.__eq__(self,other)
torch.distributions.SoftmaxTransform._call(self,x)
torch.distributions.SoftmaxTransform._inverse(self,y)
torch.distributions.StackTransform(self,tseq,dim=0)
torch.distributions.StackTransform._call(self,x)
torch.distributions.StackTransform._inverse(self,y)
torch.distributions.StackTransform._slice(self,z)
torch.distributions.StackTransform.bijective(self)
torch.distributions.StackTransform.codomain(self)
torch.distributions.StackTransform.domain(self)
torch.distributions.StackTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.StickBreakingTransform(Transform)
torch.distributions.StickBreakingTransform.__eq__(self,other)
torch.distributions.StickBreakingTransform._call(self,x)
torch.distributions.StickBreakingTransform._inverse(self,y)
torch.distributions.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform(self,cache_size=0)
torch.distributions.Transform.__eq__(self,other)
torch.distributions.Transform.__ne__(self,other)
torch.distributions.Transform.__repr__(self)
torch.distributions.Transform._call(self,x)
torch.distributions.Transform._inv_call(self,y)
torch.distributions.Transform._inverse(self,y)
torch.distributions.Transform.inv(self)
torch.distributions.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.Transform.sign(self)
torch.distributions._InverseTransform(self,transform)
torch.distributions._InverseTransform.__eq__(self,other)
torch.distributions._InverseTransform.bijective(self)
torch.distributions._InverseTransform.codomain(self)
torch.distributions._InverseTransform.domain(self)
torch.distributions._InverseTransform.event_dim(self)
torch.distributions._InverseTransform.inv(self)
torch.distributions._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions._InverseTransform.sign(self)
torch.distributions._clipped_sigmoid(x)
torch.distributions.transforms.AbsTransform(Transform)
torch.distributions.transforms.AbsTransform.__eq__(self,other)
torch.distributions.transforms.AbsTransform._call(self,x)
torch.distributions.transforms.AbsTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform.__eq__(self,other)
torch.distributions.transforms.AffineTransform.__init__(self,loc,scale,event_dim=0,cache_size=0)
torch.distributions.transforms.AffineTransform._call(self,x)
torch.distributions.transforms.AffineTransform._inverse(self,y)
torch.distributions.transforms.AffineTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.AffineTransform.sign(self)
torch.distributions.transforms.CatTransform(self,tseq,dim=0,lengths=None)
torch.distributions.transforms.CatTransform.__init__(self,tseq,dim=0,lengths=None)
torch.distributions.transforms.CatTransform._call(self,x)
torch.distributions.transforms.CatTransform._inverse(self,y)
torch.distributions.transforms.CatTransform.bijective(self)
torch.distributions.transforms.CatTransform.codomain(self)
torch.distributions.transforms.CatTransform.domain(self)
torch.distributions.transforms.CatTransform.length(self)
torch.distributions.transforms.CatTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ComposeTransform(self,parts)
torch.distributions.transforms.ComposeTransform.__eq__(self,other)
torch.distributions.transforms.ComposeTransform.__init__(self,parts)
torch.distributions.transforms.ComposeTransform.__repr__(self)
torch.distributions.transforms.ComposeTransform.bijective(self)
torch.distributions.transforms.ComposeTransform.codomain(self)
torch.distributions.transforms.ComposeTransform.domain(self)
torch.distributions.transforms.ComposeTransform.event_dim(self)
torch.distributions.transforms.ComposeTransform.inv(self)
torch.distributions.transforms.ComposeTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.ComposeTransform.sign(self)
torch.distributions.transforms.ExpTransform(Transform)
torch.distributions.transforms.ExpTransform.__eq__(self,other)
torch.distributions.transforms.ExpTransform._call(self,x)
torch.distributions.transforms.ExpTransform._inverse(self,y)
torch.distributions.transforms.ExpTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.LowerCholeskyTransform(Transform)
torch.distributions.transforms.LowerCholeskyTransform.__eq__(self,other)
torch.distributions.transforms.LowerCholeskyTransform._call(self,x)
torch.distributions.transforms.LowerCholeskyTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform.__eq__(self,other)
torch.distributions.transforms.PowerTransform.__init__(self,exponent,cache_size=0)
torch.distributions.transforms.PowerTransform._call(self,x)
torch.distributions.transforms.PowerTransform._inverse(self,y)
torch.distributions.transforms.PowerTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SigmoidTransform(Transform)
torch.distributions.transforms.SigmoidTransform.__eq__(self,other)
torch.distributions.transforms.SigmoidTransform._call(self,x)
torch.distributions.transforms.SigmoidTransform._inverse(self,y)
torch.distributions.transforms.SigmoidTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.SoftmaxTransform(Transform)
torch.distributions.transforms.SoftmaxTransform.__eq__(self,other)
torch.distributions.transforms.SoftmaxTransform._call(self,x)
torch.distributions.transforms.SoftmaxTransform._inverse(self,y)
torch.distributions.transforms.StackTransform(self,tseq,dim=0)
torch.distributions.transforms.StackTransform.__init__(self,tseq,dim=0)
torch.distributions.transforms.StackTransform._call(self,x)
torch.distributions.transforms.StackTransform._inverse(self,y)
torch.distributions.transforms.StackTransform._slice(self,z)
torch.distributions.transforms.StackTransform.bijective(self)
torch.distributions.transforms.StackTransform.codomain(self)
torch.distributions.transforms.StackTransform.domain(self)
torch.distributions.transforms.StackTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.StickBreakingTransform(Transform)
torch.distributions.transforms.StickBreakingTransform.__eq__(self,other)
torch.distributions.transforms.StickBreakingTransform._call(self,x)
torch.distributions.transforms.StickBreakingTransform._inverse(self,y)
torch.distributions.transforms.StickBreakingTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform(self,cache_size=0)
torch.distributions.transforms.Transform.__eq__(self,other)
torch.distributions.transforms.Transform.__init__(self,cache_size=0)
torch.distributions.transforms.Transform.__ne__(self,other)
torch.distributions.transforms.Transform.__repr__(self)
torch.distributions.transforms.Transform._call(self,x)
torch.distributions.transforms.Transform._inv_call(self,y)
torch.distributions.transforms.Transform._inverse(self,y)
torch.distributions.transforms.Transform.inv(self)
torch.distributions.transforms.Transform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms.Transform.sign(self)
torch.distributions.transforms._InverseTransform(self,transform)
torch.distributions.transforms._InverseTransform.__eq__(self,other)
torch.distributions.transforms._InverseTransform.__init__(self,transform)
torch.distributions.transforms._InverseTransform.bijective(self)
torch.distributions.transforms._InverseTransform.codomain(self)
torch.distributions.transforms._InverseTransform.domain(self)
torch.distributions.transforms._InverseTransform.event_dim(self)
torch.distributions.transforms._InverseTransform.inv(self)
torch.distributions.transforms._InverseTransform.log_abs_det_jacobian(self,x,y)
torch.distributions.transforms._InverseTransform.sign(self)
torch.distributions.transforms._clipped_sigmoid(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/laplace.py----------------------------------------
A:torch.distributions.laplace.(self.loc, self.scale)->broadcast_all(loc, scale)
A:torch.distributions.laplace.batch_shape->torch.Size(batch_shape)
A:torch.distributions.laplace.new->self._get_checked_instance(Laplace, _instance)
A:torch.distributions.laplace.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.laplace.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.laplace.shape->self._extended_shape(sample_shape)
A:torch.distributions.laplace.finfo->torch.finfo(self.loc.dtype)
A:torch.distributions.laplace.u->self.loc.new(shape).uniform_(finfo.eps - 1, 1)
torch.distributions.Laplace(self,loc,scale,validate_args=None)
torch.distributions.Laplace.cdf(self,value)
torch.distributions.Laplace.entropy(self)
torch.distributions.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.Laplace.icdf(self,value)
torch.distributions.Laplace.log_prob(self,value)
torch.distributions.Laplace.mean(self)
torch.distributions.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.Laplace.stddev(self)
torch.distributions.Laplace.variance(self)
torch.distributions.laplace.Laplace(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.__init__(self,loc,scale,validate_args=None)
torch.distributions.laplace.Laplace.cdf(self,value)
torch.distributions.laplace.Laplace.entropy(self)
torch.distributions.laplace.Laplace.expand(self,batch_shape,_instance=None)
torch.distributions.laplace.Laplace.icdf(self,value)
torch.distributions.laplace.Laplace.log_prob(self,value)
torch.distributions.laplace.Laplace.mean(self)
torch.distributions.laplace.Laplace.rsample(self,sample_shape=torch.Size())
torch.distributions.laplace.Laplace.stddev(self)
torch.distributions.laplace.Laplace.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/binomial.py----------------------------------------
A:torch.distributions.binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.binomial.is_scalar->isinstance(self.logits, Number)
A:torch.distributions.binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.binomial.new->self._get_checked_instance(Binomial, _instance)
A:torch.distributions.binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.binomial.max_count->max(int(self.total_count.max()), 1)
A:torch.distributions.binomial.bernoullis->torch.bernoulli(self.probs.unsqueeze(-1).expand(shape))
A:torch.distributions.binomial.arange->torch.arange(max_count, dtype=self._param.dtype, device=self._param.device)
A:torch.distributions.binomial.log_factorial_n->torch.lgamma(self.total_count + 1)
A:torch.distributions.binomial.log_factorial_k->torch.lgamma(value + 1)
A:torch.distributions.binomial.log_factorial_nmk->torch.lgamma(self.total_count - value + 1)
A:torch.distributions.binomial.total_count->int(self.total_count.max())
A:torch.distributions.binomial.values->values.expand((-1,) + self._batch_shape).expand((-1,) + self._batch_shape)
torch.distributions.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.Binomial._new(self,*args,**kwargs)
torch.distributions.Binomial.enumerate_support(self,expand=True)
torch.distributions.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.Binomial.log_prob(self,value)
torch.distributions.Binomial.logits(self)
torch.distributions.Binomial.mean(self)
torch.distributions.Binomial.param_shape(self)
torch.distributions.Binomial.probs(self)
torch.distributions.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.Binomial.support(self)
torch.distributions.Binomial.variance(self)
torch.distributions.binomial.Binomial(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial.__init__(self,total_count=1,probs=None,logits=None,validate_args=None)
torch.distributions.binomial.Binomial._new(self,*args,**kwargs)
torch.distributions.binomial.Binomial.enumerate_support(self,expand=True)
torch.distributions.binomial.Binomial.expand(self,batch_shape,_instance=None)
torch.distributions.binomial.Binomial.log_prob(self,value)
torch.distributions.binomial.Binomial.logits(self)
torch.distributions.binomial.Binomial.mean(self)
torch.distributions.binomial.Binomial.param_shape(self)
torch.distributions.binomial.Binomial.probs(self)
torch.distributions.binomial.Binomial.sample(self,sample_shape=torch.Size())
torch.distributions.binomial.Binomial.support(self)
torch.distributions.binomial.Binomial.variance(self)
torch.distributions.binomial._clamp_by_zero(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/studentT.py----------------------------------------
A:torch.distributions.studentT.m->self.df.clone()
A:torch.distributions.studentT.(self.df, self.loc, self.scale)->broadcast_all(df, loc, scale)
A:torch.distributions.studentT.self._chi2->Chi2(self.df)
A:torch.distributions.studentT.batch_shape->torch.Size(batch_shape)
A:torch.distributions.studentT.new->self._get_checked_instance(StudentT, _instance)
A:torch.distributions.studentT.new.df->self.df.expand(batch_shape)
A:torch.distributions.studentT.new.loc->self.loc.expand(batch_shape)
A:torch.distributions.studentT.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.studentT.new._chi2->self._chi2.expand(batch_shape)
A:torch.distributions.studentT.shape->self._extended_shape(sample_shape)
A:torch.distributions.studentT.X->_standard_normal(shape, dtype=self.df.dtype, device=self.df.device)
A:torch.distributions.studentT.Z->self._chi2.rsample(sample_shape)
torch.distributions.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.StudentT.entropy(self)
torch.distributions.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.StudentT.log_prob(self,value)
torch.distributions.StudentT.mean(self)
torch.distributions.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.StudentT.variance(self)
torch.distributions.studentT.StudentT(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.__init__(self,df,loc=0.0,scale=1.0,validate_args=None)
torch.distributions.studentT.StudentT.entropy(self)
torch.distributions.studentT.StudentT.expand(self,batch_shape,_instance=None)
torch.distributions.studentT.StudentT.log_prob(self,value)
torch.distributions.studentT.StudentT.mean(self)
torch.distributions.studentT.StudentT.rsample(self,sample_shape=torch.Size())
torch.distributions.studentT.StudentT.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/distribution.py----------------------------------------
A:torch.distributions.distribution.sample_shape->torch.Size(sample_shape)
A:torch.distributions.distribution.actual_shape->value.size()
A:torch.distributions.distribution.args_string->', '.join(['{}: {}'.format(p, self.__dict__[p] if self.__dict__[p].numel() == 1 else self.__dict__[p].size()) for p in param_names])
torch.distributions.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.Distribution.__repr__(self)
torch.distributions.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.Distribution._validate_sample(self,value)
torch.distributions.Distribution.arg_constraints(self)
torch.distributions.Distribution.batch_shape(self)
torch.distributions.Distribution.cdf(self,value)
torch.distributions.Distribution.entropy(self)
torch.distributions.Distribution.enumerate_support(self,expand=True)
torch.distributions.Distribution.event_shape(self)
torch.distributions.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.Distribution.icdf(self,value)
torch.distributions.Distribution.log_prob(self,value)
torch.distributions.Distribution.mean(self)
torch.distributions.Distribution.perplexity(self)
torch.distributions.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.Distribution.sample_n(self,n)
torch.distributions.Distribution.set_default_validate_args(value)
torch.distributions.Distribution.stddev(self)
torch.distributions.Distribution.support(self)
torch.distributions.Distribution.variance(self)
torch.distributions.distribution.Distribution(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__init__(self,batch_shape=torch.Size(),event_shape=torch.Size(),validate_args=None)
torch.distributions.distribution.Distribution.__repr__(self)
torch.distributions.distribution.Distribution._extended_shape(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution._get_checked_instance(self,cls,_instance=None)
torch.distributions.distribution.Distribution._validate_sample(self,value)
torch.distributions.distribution.Distribution.arg_constraints(self)
torch.distributions.distribution.Distribution.batch_shape(self)
torch.distributions.distribution.Distribution.cdf(self,value)
torch.distributions.distribution.Distribution.entropy(self)
torch.distributions.distribution.Distribution.enumerate_support(self,expand=True)
torch.distributions.distribution.Distribution.event_shape(self)
torch.distributions.distribution.Distribution.expand(self,batch_shape,_instance=None)
torch.distributions.distribution.Distribution.icdf(self,value)
torch.distributions.distribution.Distribution.log_prob(self,value)
torch.distributions.distribution.Distribution.mean(self)
torch.distributions.distribution.Distribution.perplexity(self)
torch.distributions.distribution.Distribution.rsample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample(self,sample_shape=torch.Size())
torch.distributions.distribution.Distribution.sample_n(self,n)
torch.distributions.distribution.Distribution.set_default_validate_args(value)
torch.distributions.distribution.Distribution.stddev(self)
torch.distributions.distribution.Distribution.support(self)
torch.distributions.distribution.Distribution.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/negative_binomial.py----------------------------------------
A:torch.distributions.negative_binomial.(self.total_count, self.probs)->broadcast_all(total_count, probs)
A:torch.distributions.negative_binomial.self.total_count->self.total_count.type_as(self.logits)
A:torch.distributions.negative_binomial.(self.total_count, self.logits)->broadcast_all(total_count, logits)
A:torch.distributions.negative_binomial.batch_shape->torch.Size(batch_shape)
A:torch.distributions.negative_binomial.new->self._get_checked_instance(NegativeBinomial, _instance)
A:torch.distributions.negative_binomial.new.total_count->self.total_count.expand(batch_shape)
A:torch.distributions.negative_binomial.new.probs->self.probs.expand(batch_shape)
A:torch.distributions.negative_binomial.new.logits->self.logits.expand(batch_shape)
A:torch.distributions.negative_binomial.rate->self._gamma.sample(sample_shape=sample_shape)
torch.distributions.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.NegativeBinomial._gamma(self)
torch.distributions.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.NegativeBinomial.log_prob(self,value)
torch.distributions.NegativeBinomial.logits(self)
torch.distributions.NegativeBinomial.mean(self)
torch.distributions.NegativeBinomial.param_shape(self)
torch.distributions.NegativeBinomial.probs(self)
torch.distributions.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.NegativeBinomial.variance(self)
torch.distributions.negative_binomial.NegativeBinomial(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial.__init__(self,total_count,probs=None,logits=None,validate_args=None)
torch.distributions.negative_binomial.NegativeBinomial._gamma(self)
torch.distributions.negative_binomial.NegativeBinomial._new(self,*args,**kwargs)
torch.distributions.negative_binomial.NegativeBinomial.expand(self,batch_shape,_instance=None)
torch.distributions.negative_binomial.NegativeBinomial.log_prob(self,value)
torch.distributions.negative_binomial.NegativeBinomial.logits(self)
torch.distributions.negative_binomial.NegativeBinomial.mean(self)
torch.distributions.negative_binomial.NegativeBinomial.param_shape(self)
torch.distributions.negative_binomial.NegativeBinomial.probs(self)
torch.distributions.negative_binomial.NegativeBinomial.sample(self,sample_shape=torch.Size())
torch.distributions.negative_binomial.NegativeBinomial.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/fishersnedecor.py----------------------------------------
A:torch.distributions.fishersnedecor.(self.df1, self.df2)->broadcast_all(df1, df2)
A:torch.distributions.fishersnedecor.self._gamma1->Gamma(self.df1 * 0.5, self.df1)
A:torch.distributions.fishersnedecor.self._gamma2->Gamma(self.df2 * 0.5, self.df2)
A:torch.distributions.fishersnedecor.batch_shape->torch.Size(batch_shape)
A:torch.distributions.fishersnedecor.new->self._get_checked_instance(FisherSnedecor, _instance)
A:torch.distributions.fishersnedecor.new.df1->self.df1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new.df2->self.df2.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma1->self._gamma1.expand(batch_shape)
A:torch.distributions.fishersnedecor.new._gamma2->self._gamma2.expand(batch_shape)
A:torch.distributions.fishersnedecor.df2->self.df2.clone()
A:torch.distributions.fishersnedecor.shape->self._extended_shape(sample_shape)
A:torch.distributions.fishersnedecor.X1->self._gamma1.rsample(sample_shape).view(shape)
A:torch.distributions.fishersnedecor.X2->self._gamma2.rsample(sample_shape).view(shape)
torch.distributions.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.FisherSnedecor.log_prob(self,value)
torch.distributions.FisherSnedecor.mean(self)
torch.distributions.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.FisherSnedecor.variance(self)
torch.distributions.fishersnedecor.FisherSnedecor(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.__init__(self,df1,df2,validate_args=None)
torch.distributions.fishersnedecor.FisherSnedecor.expand(self,batch_shape,_instance=None)
torch.distributions.fishersnedecor.FisherSnedecor.log_prob(self,value)
torch.distributions.fishersnedecor.FisherSnedecor.mean(self)
torch.distributions.fishersnedecor.FisherSnedecor.rsample(self,sample_shape=torch.Size(()))
torch.distributions.fishersnedecor.FisherSnedecor.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/pareto.py----------------------------------------
A:torch.distributions.pareto.(self.scale, self.alpha)->broadcast_all(scale, alpha)
A:torch.distributions.pareto.base_dist->Exponential(self.alpha)
A:torch.distributions.pareto.new->self._get_checked_instance(Pareto, _instance)
A:torch.distributions.pareto.new.scale->self.scale.expand(batch_shape)
A:torch.distributions.pareto.new.alpha->self.alpha.expand(batch_shape)
A:torch.distributions.pareto.a->self.alpha.clone().clamp(min=2)
torch.distributions.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.Pareto.entropy(self)
torch.distributions.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.Pareto.mean(self)
torch.distributions.Pareto.support(self)
torch.distributions.Pareto.variance(self)
torch.distributions.pareto.Pareto(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.__init__(self,scale,alpha,validate_args=None)
torch.distributions.pareto.Pareto.entropy(self)
torch.distributions.pareto.Pareto.expand(self,batch_shape,_instance=None)
torch.distributions.pareto.Pareto.mean(self)
torch.distributions.pareto.Pareto.support(self)
torch.distributions.pareto.Pareto.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/half_cauchy.py----------------------------------------
A:torch.distributions.half_cauchy.base_dist->Cauchy(0, scale)
A:torch.distributions.half_cauchy.new->self._get_checked_instance(HalfCauchy, _instance)
A:torch.distributions.half_cauchy.value->torch.as_tensor(value, dtype=self.base_dist.scale.dtype, device=self.base_dist.scale.device)
torch.distributions.HalfCauchy(self,scale,validate_args=None)
torch.distributions.HalfCauchy.cdf(self,value)
torch.distributions.HalfCauchy.entropy(self)
torch.distributions.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.HalfCauchy.icdf(self,prob)
torch.distributions.HalfCauchy.log_prob(self,value)
torch.distributions.HalfCauchy.mean(self)
torch.distributions.HalfCauchy.scale(self)
torch.distributions.HalfCauchy.variance(self)
torch.distributions.half_cauchy.HalfCauchy(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.__init__(self,scale,validate_args=None)
torch.distributions.half_cauchy.HalfCauchy.cdf(self,value)
torch.distributions.half_cauchy.HalfCauchy.entropy(self)
torch.distributions.half_cauchy.HalfCauchy.expand(self,batch_shape,_instance=None)
torch.distributions.half_cauchy.HalfCauchy.icdf(self,prob)
torch.distributions.half_cauchy.HalfCauchy.log_prob(self,value)
torch.distributions.half_cauchy.HalfCauchy.mean(self)
torch.distributions.half_cauchy.HalfCauchy.scale(self)
torch.distributions.half_cauchy.HalfCauchy.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/independent.py----------------------------------------
A:torch.distributions.independent.new->self._get_checked_instance(Independent, _instance)
A:torch.distributions.independent.batch_shape->torch.Size(batch_shape)
A:torch.distributions.independent.new.base_dist->self.base_dist.expand(batch_shape + self.event_shape[:self.reinterpreted_batch_ndims])
A:torch.distributions.independent.log_prob->self.base_dist.log_prob(value)
A:torch.distributions.independent.entropy->self.base_dist.entropy()
torch.distributions.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.Independent.entropy(self)
torch.distributions.Independent.enumerate_support(self,expand=True)
torch.distributions.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.Independent.has_enumerate_support(self)
torch.distributions.Independent.has_rsample(self)
torch.distributions.Independent.log_prob(self,value)
torch.distributions.Independent.mean(self)
torch.distributions.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.Independent.support(self)
torch.distributions.Independent.variance(self)
torch.distributions.independent.Independent(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.__init__(self,base_distribution,reinterpreted_batch_ndims,validate_args=None)
torch.distributions.independent.Independent.entropy(self)
torch.distributions.independent.Independent.enumerate_support(self,expand=True)
torch.distributions.independent.Independent.expand(self,batch_shape,_instance=None)
torch.distributions.independent.Independent.has_enumerate_support(self)
torch.distributions.independent.Independent.has_rsample(self)
torch.distributions.independent.Independent.log_prob(self,value)
torch.distributions.independent.Independent.mean(self)
torch.distributions.independent.Independent.rsample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.sample(self,sample_shape=torch.Size())
torch.distributions.independent.Independent.support(self)
torch.distributions.independent.Independent.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/exponential.py----------------------------------------
A:torch.distributions.exponential.(self.rate,)->broadcast_all(rate)
A:torch.distributions.exponential.new->self._get_checked_instance(Exponential, _instance)
A:torch.distributions.exponential.batch_shape->torch.Size(batch_shape)
A:torch.distributions.exponential.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.exponential.shape->self._extended_shape(sample_shape)
A:torch.distributions.exponential.u->torch.rand(shape, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Exponential(self,rate,validate_args=None)
torch.distributions.Exponential._log_normalizer(self,x)
torch.distributions.Exponential._natural_params(self)
torch.distributions.Exponential.cdf(self,value)
torch.distributions.Exponential.entropy(self)
torch.distributions.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.Exponential.icdf(self,value)
torch.distributions.Exponential.log_prob(self,value)
torch.distributions.Exponential.mean(self)
torch.distributions.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.Exponential.stddev(self)
torch.distributions.Exponential.variance(self)
torch.distributions.exponential.Exponential(self,rate,validate_args=None)
torch.distributions.exponential.Exponential.__init__(self,rate,validate_args=None)
torch.distributions.exponential.Exponential._log_normalizer(self,x)
torch.distributions.exponential.Exponential._natural_params(self)
torch.distributions.exponential.Exponential.cdf(self,value)
torch.distributions.exponential.Exponential.entropy(self)
torch.distributions.exponential.Exponential.expand(self,batch_shape,_instance=None)
torch.distributions.exponential.Exponential.icdf(self,value)
torch.distributions.exponential.Exponential.log_prob(self,value)
torch.distributions.exponential.Exponential.mean(self)
torch.distributions.exponential.Exponential.rsample(self,sample_shape=torch.Size())
torch.distributions.exponential.Exponential.stddev(self)
torch.distributions.exponential.Exponential.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/dirichlet.py----------------------------------------
A:torch.distributions.dirichlet.total->self.concentration.expand(shape).sum(-1, True).expand_as(concentration)
A:torch.distributions.dirichlet.grad->torch._dirichlet_grad(x, concentration, total)
A:torch.distributions.dirichlet.x->torch._sample_dirichlet(concentration)
A:torch.distributions.dirichlet.new->self._get_checked_instance(Dirichlet, _instance)
A:torch.distributions.dirichlet.batch_shape->torch.Size(batch_shape)
A:torch.distributions.dirichlet.new.concentration->self.concentration.expand(batch_shape + self.event_shape)
A:torch.distributions.dirichlet.shape->self._extended_shape(sample_shape)
A:torch.distributions.dirichlet.concentration->self.concentration.expand(shape)
A:torch.distributions.dirichlet.con0->self.concentration.sum(-1, True)
A:torch.distributions.dirichlet.k->self.concentration.size(-1)
A:torch.distributions.dirichlet.a0->self.concentration.sum(-1)
torch.distributions.Dirichlet(self,concentration,validate_args=None)
torch.distributions.Dirichlet._log_normalizer(self,x)
torch.distributions.Dirichlet._natural_params(self)
torch.distributions.Dirichlet.entropy(self)
torch.distributions.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.Dirichlet.log_prob(self,value)
torch.distributions.Dirichlet.mean(self)
torch.distributions.Dirichlet.rsample(self,sample_shape=())
torch.distributions.Dirichlet.variance(self)
torch.distributions.dirichlet.Dirichlet(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet.__init__(self,concentration,validate_args=None)
torch.distributions.dirichlet.Dirichlet._log_normalizer(self,x)
torch.distributions.dirichlet.Dirichlet._natural_params(self)
torch.distributions.dirichlet.Dirichlet.entropy(self)
torch.distributions.dirichlet.Dirichlet.expand(self,batch_shape,_instance=None)
torch.distributions.dirichlet.Dirichlet.log_prob(self,value)
torch.distributions.dirichlet.Dirichlet.mean(self)
torch.distributions.dirichlet.Dirichlet.rsample(self,sample_shape=())
torch.distributions.dirichlet.Dirichlet.variance(self)
torch.distributions.dirichlet._Dirichlet(Function)
torch.distributions.dirichlet._Dirichlet.backward(ctx,grad_output)
torch.distributions.dirichlet._Dirichlet.forward(ctx,concentration)
torch.distributions.dirichlet._Dirichlet_backward(x,concentration,grad_output)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/log_normal.py----------------------------------------
A:torch.distributions.log_normal.base_dist->Normal(loc, scale)
A:torch.distributions.log_normal.new->self._get_checked_instance(LogNormal, _instance)
torch.distributions.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.LogNormal.entropy(self)
torch.distributions.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.LogNormal.loc(self)
torch.distributions.LogNormal.mean(self)
torch.distributions.LogNormal.scale(self)
torch.distributions.LogNormal.variance(self)
torch.distributions.log_normal.LogNormal(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.__init__(self,loc,scale,validate_args=None)
torch.distributions.log_normal.LogNormal.entropy(self)
torch.distributions.log_normal.LogNormal.expand(self,batch_shape,_instance=None)
torch.distributions.log_normal.LogNormal.loc(self)
torch.distributions.log_normal.LogNormal.mean(self)
torch.distributions.log_normal.LogNormal.scale(self)
torch.distributions.log_normal.LogNormal.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/constraints.py----------------------------------------
A:torch.distributions.constraints.value_tril->value.tril()
A:torch.distributions.constraints.flattened_value->value.reshape((-1,) + matrix_shape)
A:torch.distributions.constraints.self.cseq->list(cseq)
A:torch.distributions.constraints.self.lengths->list(lengths)
A:torch.distributions.constraints.v->value.narrow(self.dim, start, length)
A:torch.distributions.constraints.dependent->_Dependent()
A:torch.distributions.constraints.boolean->_Boolean()
A:torch.distributions.constraints.nonnegative_integer->_IntegerGreaterThan(0)
A:torch.distributions.constraints.positive_integer->_IntegerGreaterThan(1)
A:torch.distributions.constraints.real->_Real()
A:torch.distributions.constraints.real_vector->_RealVector()
A:torch.distributions.constraints.positive->_GreaterThan(0.0)
A:torch.distributions.constraints.unit_interval->_Interval(0.0, 1.0)
A:torch.distributions.constraints.simplex->_Simplex()
A:torch.distributions.constraints.lower_triangular->_LowerTriangular()
A:torch.distributions.constraints.lower_cholesky->_LowerCholesky()
A:torch.distributions.constraints.positive_definite->_PositiveDefinite()
torch.distributions.constraints.Constraint(object)
torch.distributions.constraints.Constraint.__repr__(self)
torch.distributions.constraints.Constraint.check(self,value)
torch.distributions.constraints._Boolean(Constraint)
torch.distributions.constraints._Boolean.check(self,value)
torch.distributions.constraints._Cat(self,cseq,dim=0,lengths=None)
torch.distributions.constraints._Cat.__init__(self,cseq,dim=0,lengths=None)
torch.distributions.constraints._Cat.check(self,value)
torch.distributions.constraints._Dependent(Constraint)
torch.distributions.constraints._Dependent.check(self,x)
torch.distributions.constraints._DependentProperty(property,_Dependent)
torch.distributions.constraints._GreaterThan(self,lower_bound)
torch.distributions.constraints._GreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThan.__repr__(self)
torch.distributions.constraints._GreaterThan.check(self,value)
torch.distributions.constraints._GreaterThanEq(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__init__(self,lower_bound)
torch.distributions.constraints._GreaterThanEq.__repr__(self)
torch.distributions.constraints._GreaterThanEq.check(self,value)
torch.distributions.constraints._HalfOpenInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._HalfOpenInterval.__repr__(self)
torch.distributions.constraints._HalfOpenInterval.check(self,value)
torch.distributions.constraints._IntegerGreaterThan(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__init__(self,lower_bound)
torch.distributions.constraints._IntegerGreaterThan.__repr__(self)
torch.distributions.constraints._IntegerGreaterThan.check(self,value)
torch.distributions.constraints._IntegerInterval(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._IntegerInterval.__repr__(self)
torch.distributions.constraints._IntegerInterval.check(self,value)
torch.distributions.constraints._IntegerLessThan(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__init__(self,upper_bound)
torch.distributions.constraints._IntegerLessThan.__repr__(self)
torch.distributions.constraints._IntegerLessThan.check(self,value)
torch.distributions.constraints._Interval(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__init__(self,lower_bound,upper_bound)
torch.distributions.constraints._Interval.__repr__(self)
torch.distributions.constraints._Interval.check(self,value)
torch.distributions.constraints._LessThan(self,upper_bound)
torch.distributions.constraints._LessThan.__init__(self,upper_bound)
torch.distributions.constraints._LessThan.__repr__(self)
torch.distributions.constraints._LessThan.check(self,value)
torch.distributions.constraints._LowerCholesky(Constraint)
torch.distributions.constraints._LowerCholesky.check(self,value)
torch.distributions.constraints._LowerTriangular(Constraint)
torch.distributions.constraints._LowerTriangular.check(self,value)
torch.distributions.constraints._PositiveDefinite(Constraint)
torch.distributions.constraints._PositiveDefinite.check(self,value)
torch.distributions.constraints._Real(Constraint)
torch.distributions.constraints._Real.check(self,value)
torch.distributions.constraints._RealVector(Constraint)
torch.distributions.constraints._RealVector.check(self,value)
torch.distributions.constraints._Simplex(Constraint)
torch.distributions.constraints._Simplex.check(self,value)
torch.distributions.constraints._Stack(self,cseq,dim=0)
torch.distributions.constraints._Stack.__init__(self,cseq,dim=0)
torch.distributions.constraints._Stack.check(self,value)
torch.distributions.constraints.is_dependent(constraint)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/half_normal.py----------------------------------------
A:torch.distributions.half_normal.base_dist->Normal(0, scale)
A:torch.distributions.half_normal.new->self._get_checked_instance(HalfNormal, _instance)
torch.distributions.HalfNormal(self,scale,validate_args=None)
torch.distributions.HalfNormal.cdf(self,value)
torch.distributions.HalfNormal.entropy(self)
torch.distributions.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.HalfNormal.icdf(self,prob)
torch.distributions.HalfNormal.log_prob(self,value)
torch.distributions.HalfNormal.mean(self)
torch.distributions.HalfNormal.scale(self)
torch.distributions.HalfNormal.variance(self)
torch.distributions.half_normal.HalfNormal(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.__init__(self,scale,validate_args=None)
torch.distributions.half_normal.HalfNormal.cdf(self,value)
torch.distributions.half_normal.HalfNormal.entropy(self)
torch.distributions.half_normal.HalfNormal.expand(self,batch_shape,_instance=None)
torch.distributions.half_normal.HalfNormal.icdf(self,prob)
torch.distributions.half_normal.HalfNormal.log_prob(self,value)
torch.distributions.half_normal.HalfNormal.mean(self)
torch.distributions.half_normal.HalfNormal.scale(self)
torch.distributions.half_normal.HalfNormal.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/gamma.py----------------------------------------
A:torch.distributions.gamma.(self.concentration, self.rate)->broadcast_all(concentration, rate)
A:torch.distributions.gamma.batch_shape->torch.Size(batch_shape)
A:torch.distributions.gamma.new->self._get_checked_instance(Gamma, _instance)
A:torch.distributions.gamma.new.concentration->self.concentration.expand(batch_shape)
A:torch.distributions.gamma.new.rate->self.rate.expand(batch_shape)
A:torch.distributions.gamma.shape->self._extended_shape(sample_shape)
A:torch.distributions.gamma.value->torch.as_tensor(value, dtype=self.rate.dtype, device=self.rate.device)
torch.distributions.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.Gamma._log_normalizer(self,x,y)
torch.distributions.Gamma._natural_params(self)
torch.distributions.Gamma.entropy(self)
torch.distributions.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.Gamma.log_prob(self,value)
torch.distributions.Gamma.mean(self)
torch.distributions.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.Gamma.variance(self)
torch.distributions.gamma.Gamma(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma.__init__(self,concentration,rate,validate_args=None)
torch.distributions.gamma.Gamma._log_normalizer(self,x,y)
torch.distributions.gamma.Gamma._natural_params(self)
torch.distributions.gamma.Gamma.entropy(self)
torch.distributions.gamma.Gamma.expand(self,batch_shape,_instance=None)
torch.distributions.gamma.Gamma.log_prob(self,value)
torch.distributions.gamma.Gamma.mean(self)
torch.distributions.gamma.Gamma.rsample(self,sample_shape=torch.Size())
torch.distributions.gamma.Gamma.variance(self)
torch.distributions.gamma._standard_gamma(concentration)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/uniform.py----------------------------------------
A:torch.distributions.uniform.(self.low, self.high)->broadcast_all(low, high)
A:torch.distributions.uniform.batch_shape->torch.Size(batch_shape)
A:torch.distributions.uniform.new->self._get_checked_instance(Uniform, _instance)
A:torch.distributions.uniform.new.low->self.low.expand(batch_shape)
A:torch.distributions.uniform.new.high->self.high.expand(batch_shape)
A:torch.distributions.uniform.shape->self._extended_shape(sample_shape)
A:torch.distributions.uniform.rand->torch.rand(shape, dtype=self.low.dtype, device=self.low.device)
A:torch.distributions.uniform.lb->self.low.le(value).type_as(self.low)
A:torch.distributions.uniform.ub->self.high.gt(value).type_as(self.low)
torch.distributions.Uniform(self,low,high,validate_args=None)
torch.distributions.Uniform.cdf(self,value)
torch.distributions.Uniform.entropy(self)
torch.distributions.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.Uniform.icdf(self,value)
torch.distributions.Uniform.log_prob(self,value)
torch.distributions.Uniform.mean(self)
torch.distributions.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.Uniform.stddev(self)
torch.distributions.Uniform.support(self)
torch.distributions.Uniform.variance(self)
torch.distributions.uniform.Uniform(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.__init__(self,low,high,validate_args=None)
torch.distributions.uniform.Uniform.cdf(self,value)
torch.distributions.uniform.Uniform.entropy(self)
torch.distributions.uniform.Uniform.expand(self,batch_shape,_instance=None)
torch.distributions.uniform.Uniform.icdf(self,value)
torch.distributions.uniform.Uniform.log_prob(self,value)
torch.distributions.uniform.Uniform.mean(self)
torch.distributions.uniform.Uniform.rsample(self,sample_shape=torch.Size())
torch.distributions.uniform.Uniform.stddev(self)
torch.distributions.uniform.Uniform.support(self)
torch.distributions.uniform.Uniform.variance(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributions/relaxed_categorical.py----------------------------------------
A:torch.distributions.relaxed_categorical.self._categorical->Categorical(probs, logits)
A:torch.distributions.relaxed_categorical.new->self._get_checked_instance(RelaxedOneHotCategorical, _instance)
A:torch.distributions.relaxed_categorical.batch_shape->torch.Size(batch_shape)
A:torch.distributions.relaxed_categorical.new._categorical->self._categorical.expand(batch_shape)
A:torch.distributions.relaxed_categorical.shape->self._extended_shape(sample_shape)
A:torch.distributions.relaxed_categorical.uniforms->clamp_probs(torch.rand(shape, dtype=self.logits.dtype, device=self.logits.device))
A:torch.distributions.relaxed_categorical.(logits, value)->broadcast_all(self.logits, value)
A:torch.distributions.relaxed_categorical.score->(score - score.logsumexp(dim=-1, keepdim=True)).sum(-1)
A:torch.distributions.relaxed_categorical.base_dist->ExpRelaxedCategorical(temperature, probs, logits)
torch.distributions.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.RelaxedOneHotCategorical.logits(self)
torch.distributions.RelaxedOneHotCategorical.probs(self)
torch.distributions.RelaxedOneHotCategorical.temperature(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical._new(self,*args,**kwargs)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.log_prob(self,value)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.logits(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.param_shape(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.probs(self)
torch.distributions.relaxed_categorical.ExpRelaxedCategorical.rsample(self,sample_shape=torch.Size())
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.__init__(self,temperature,probs=None,logits=None,validate_args=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand(self,batch_shape,_instance=None)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs(self)
torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/_recursive.py----------------------------------------
A:torch.jit._recursive.qualified_name->torch.jit._qualified_name(type(original))
A:torch.jit._recursive.script_module->torch.jit.ScriptModule(_qualified_name=qualified_name)
A:torch.jit._recursive.constants_set->set(getattr(original, '__constants__', []))
A:torch.jit._recursive.item->getattr(mod, name)
A:torch.jit._recursive.class_annotations->getattr(original, '__annotations__', {})
A:torch.jit._recursive.the_type->torch._C._jit_try_infer_type(item)
A:torch.jit._recursive.msg->'When compiling {}, could not register attribute {} of type {} with value {}\nOriginal error: {}'.format(type(original), name, the_type, item, str(e))
A:torch.jit._recursive.script_module.__dict__['_overloads']->dict(getattr(original, '__overloads__', {}))
A:torch.jit._recursive.method_overloads->torch._jit_internal._get_overloaded_methods(item, mod.__class__)
A:torch.jit._recursive.methods->tuple((name for name in methods if name not in exclude_methods))
A:torch.jit._recursive.overload_name_mappings->dict(getattr(mod, '__overloads__', {}))
A:torch.jit._recursive.orig_ast->torch.jit.get_jit_def(orig_fn, self_name='ScriptModule')
A:torch.jit._recursive.names->list(map(lambda i: orig_ast.name().name + '__' + str(i), range(len(overload_fns))))
A:torch.jit._recursive.over_ast->torch.jit.get_jit_def(overload_fn, self_name='ScriptModule')
A:torch.jit._recursive.new_ast->torch._C._replace_overloaded_method_decl(over_ast.decl(), orig_ast, name)
A:torch.jit._recursive._rcb->torch._jit_internal.createResolutionCallbackFromClosure(orig_fn)
A:torch.jit._recursive.func->get_function_from_type(type(mod), method)
A:torch.jit._recursive.filtered_methods->filter(ignore_overloaded, methods)
A:torch.jit._recursive.stubs->list(map(make_stub, filtered_methods))
A:torch.jit._recursive.stub->torch.jit.script_method(fn, _jit_internal.createResolutionCallbackFromClosure(fn))
A:torch.jit._recursive.new_strong_submodule->recursive_script(module)
A:torch.jit._recursive.rcb->torch._jit_internal.createResolutionCallbackFromClosure(fn)
A:torch.jit._recursive.modules->collections.OrderedDict()
A:torch.jit._recursive.modules[key]->recursive_script(submodule)
torch.jit._recursive.copy_to_script_module(original,stubs)
torch.jit._recursive.create_constant_iterable_module(module)
torch.jit._recursive.create_method_from_fn(module,fn)
torch.jit._recursive.make_strong_submodule(field,module,parent)
torch.jit._recursive.recursive_script(mod,exclude_methods=())
torch.jit._recursive.try_compile_fn(fn,loc)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/__init__.py----------------------------------------
A:torch.jit.__init__.value->os.environ.get(name)
A:torch.jit.__init__._enabled->_parse_env('PYTORCH_JIT', True, '> Using PyTorch JIT', '> PyTorch JIT DISABLED')
A:torch.jit.__init__._python_cu->torch._C.CompilationUnit()
A:torch.jit.__init__.tracing_state->torch._C._get_tracing_state()
A:torch.jit.__init__.stored_flag->torch._C._get_graph_executor_optimize()
A:torch.jit.__init__.DEFAULT_EXTRA_FILES_MAP->torch._C.ExtraFilesMap()
A:torch.jit.__init__.ret->m.save_to_buffer(_extra_files=_extra_files)
A:torch.jit.__init__.map_location->torch.device(map_location)
A:torch.jit.__init__.cu->torch._C.CompilationUnit()
A:torch.jit.__init__.cpp_module->torch._C.import_ir_module_from_buffer(cu, f.read(), map_location, _extra_files)
A:torch.jit.__init__.state_dict->torch.jit._recursive.recursive_script(module).state_dict(keep_vars=True)
A:torch.jit.__init__.filtered_dict->type(state_dict)()
A:torch.jit.__init__.seen_ids->set()
A:torch.jit.__init__.frame->inspect.currentframe()
A:torch.jit.__init__.(in_vars, in_desc)->_flatten(args)
A:torch.jit.__init__.module_state->list(_unique_state_dict(self, keep_vars=True).values())
A:torch.jit.__init__.(trace, all_trace_inputs)->torch._C._tracer_enter(*in_vars + module_state)
A:torch.jit.__init__.ret_inputs->tuple((x.clone() for x in all_trace_inputs))
A:torch.jit.__init__.trace_inputs->_unflatten(all_trace_inputs[:len(in_vars)], in_desc)
A:torch.jit.__init__.inputs_states->_unflatten(all_trace_inputs[:len(in_vars)], in_desc)
A:torch.jit.__init__.out->model(*args)
A:torch.jit.__init__.(out_vars, _)->_flatten(out)
A:torch.jit.__init__.v->getattr(mod, name)
A:torch.jit.__init__.v.grad->clone_input(v.grad)
A:torch.jit.__init__._JIT_DUMP->os.environ.get('PYTORCH_JIT_DUMP', False)
A:torch.jit.__init__._JIT_TIME->os.environ.get('PYTORCH_JIT_TIME', False)
A:torch.jit.__init__._JIT_DISABLE->os.environ.get('PYTORCH_JIT_DISABLE', False)
A:torch.jit.__init__._JIT_STATS->os.environ.get('PYTORCH_JIT_STATS', False)
A:torch.jit.__init__.filename->'{}_{}'.format(trace_name, pass_name)
A:torch.jit.__init__.stream->torch.cuda.current_stream()
A:torch.jit.__init__.start->torch.cuda.Event(enable_timing=True)
A:torch.jit.__init__.end->torch.cuda.Event(enable_timing=True)
A:torch.jit.__init__.is_module->isinstance(model, Module)
A:torch.jit.__init__.saved_args->_clone_inputs(args)
A:torch.jit.__init__.saved_state->copy.deepcopy(model.state_dict())
A:torch.jit.__init__.(in_vars, _)->_flatten((args, params))
A:torch.jit.__init__.loss->loss_fn(*out)
A:torch.jit.__init__.grads->torch.autograd.grad([loss], in_vars)
A:torch.jit.__init__.(uncompiled_outs, uncompiled_grads)->run_fwd_bwd(args, force_trace=True)
A:torch.jit.__init__.(compiled_outs, compiled_grads)->run_fwd_bwd(args, assert_compiled=True)
A:torch.jit.__init__.copied_dict[name]->_clone_inputs(data)
A:torch.jit.__init__.check_mod->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, _force_outplace=force_outplace, _module_class=_module_class)
A:torch.jit.__init__.check_mod_func->torch.jit.trace(func, _clone_inputs(inputs), check_trace=False, _force_outplace=force_outplace, _module_class=_module_class)._c._get_method(traced_func.name)
A:torch.jit.__init__.mod_canonicalized->torch._C._jit_pass_canonicalize(traced_func.graph)
A:torch.jit.__init__.check_canonicalized->torch._C._jit_pass_canonicalize(check_mod_func.graph)
A:torch.jit.__init__.graph_diff->difflib.ndiff(str(mod_canonicalized).splitlines(True), str(check_canonicalized).splitlines(True))
A:torch.jit.__init__.node_diff->difflib.ndiff(str(n_mod).splitlines(True), str(n_check).splitlines(True))
A:torch.jit.__init__.mod_stack->n_mod.sourceRange()
A:torch.jit.__init__.check_stack->n_check.sourceRange()
A:torch.jit.__init__.mod_tensor_val->n_mod.t('value')
A:torch.jit.__init__.check_tensor_val->n_check.t('value')
A:torch.jit.__init__.compare_stack->n_mod.sourceRange()
A:torch.jit.__init__.outs->wrap_retval(mod(*_clone_inputs(inputs)))
A:torch.jit.__init__.traced_outs->run_mod_and_filter_tensor_outputs(traced_func, inputs, 'trace')
A:torch.jit.__init__.fn_outs->run_mod_and_filter_tensor_outputs(func, inputs, 'Python function')
A:torch.jit.__init__.check_outs->run_mod_and_filter_tensor_outputs(check_mod_func, inputs, 'repeated trace')
A:torch.jit.__init__.diag_info->graph_diagnostic_info()
A:torch.jit.__init__.example_inputs->make_tuple(example_inputs)
A:torch.jit.__init__.var_lookup_fn->_create_interpreter_name_lookup_fn(0)
A:torch.jit.__init__.name->_qualified_name(func)
A:torch.jit.__init__.traced->torch._C._create_function_from_trace(name, func, example_inputs, var_lookup_fn, _force_outplace)
A:torch.jit.__init__.module->torch.jit._recursive.recursive_script(module)
A:torch.jit.__init__.check_trace_method->torch.jit._recursive.recursive_script(module)._c._get_method(method_name)
A:torch.jit.__init__.self._c->torch._C.CompilationUnit()
A:torch.jit.__init__.rcb->torch._jit_internal.createResolutionCallback(frames_up=1)
A:torch.jit.__init__.r->self._python_modules.items()
A:torch.jit.__init__.hooks->torch._C._jit_get_emit_hooks()
A:torch.jit.__init__.module_name->getattr(obj, '__module__', None)
A:torch.jit.__init__.ast->get_jit_def(fn, self_name='ScriptModule')
A:torch.jit.__init__.qualified_name->_qualified_name(obj)
A:torch.jit.__init__._rcb->_gen_rcb(impl_fn, _frames_up)
A:torch.jit.__init__.fn->torch._C._jit_script_compile_overload(qual_name, overload_decl, impl_ast, _rcb, overload_defaults)
A:torch.jit.__init__.closure_rcb->torch._jit_internal.createResolutionCallbackFromClosure(obj)
A:torch.jit.__init__.stack_rcb->torch._jit_internal.createResolutionCallback(_frames_up + 1)
A:torch.jit.__init__.result->closure_rcb(name)
A:torch.jit.__init__.ScriptMethodStub->namedtuple('ScriptMethodStub', ('resolution_callback', 'def_', 'original_method'))
A:torch.jit.__init__.self._python_modules->OrderedDict()
A:torch.jit.__init__.constants->', '.join((typ.__name__ for typ in _constant_types))
A:torch.jit.__init__.cls._constants_set->cls._constants_set.union(base_constants)
A:torch.jit.__init__.base_constants->getattr(base, '_constants_set', set())
A:torch.jit.__init__.original_init->getattr(cls, '__init__', lambda self: None)
A:torch.jit.__init__.cls._overloads->dict(getattr(cls, '__overloads__', {}))
A:torch.jit.__init__._qualified_name->torch._jit_internal._qualified_name(self.__class__)
A:torch.jit.__init__.self.__dict__['_c']->torch._C.ScriptModule(_qualified_name, _compilation_unit, True)
A:torch.jit.__init__.training->self._c._get_attribute('training')
A:torch.jit.__init__.self._parameters->OrderedParameterDict(self._c)
A:torch.jit.__init__.self._buffers->OrderedBufferDict(self._c)
A:torch.jit.__init__.self._modules->OrderedModuleDict(self._c)
A:torch.jit.__init__.forward->_CachedForward()
A:torch.jit.__init__.script_method->self._c._get_method(attr)
A:torch.jit.__init__.the_type->torch.jit.annotations.ann_to_type(value.type)
A:torch.jit.__init__.module_value[key]->conv_module_to_const(val)
A:torch.jit.__init__.module_value[i]->conv_module_to_const(module_value[i])
A:torch.jit.__init__.id_set->set()
A:torch.jit.__init__.self._modules[name]->make_module(submodule, TracedModule, _compilation_unit)
A:torch.jit.__init__.keys->super(_ConstModuleList, self).__dir__()
A:torch.jit.__init__.impl_ast->torch.jit.get_jit_def(impl_fn)
A:torch.jit.__init__.signature->torch.jit.annotations.get_signature(func, None, None)
A:torch.jit.__init__.qual_name->_qualified_name(obj)
A:torch.jit.__init__.compiled_overloads->_compiled_overloaded_fns.get(qual_name, None)
A:torch.jit.__init__.overloads->torch._jit_internal._get_fn_overloads(qual_name)
A:torch.jit.__init__.(overload_decl, overload_defaults)->_get_overload_decl_and_defaults(overload_fn)
A:torch.jit.__init__.compiled_fn->_compile_function_with_overload(qual_name, obj, overload_decl, overload_defaults)
A:torch.jit.__init__.fields->list(obj._fields)
A:torch.jit.__init__.has_annotations->hasattr(obj, '__annotations__')
A:torch.jit.__init__.TupleType->collections.namedtuple(unqual_name, field_names)
A:torch.jit.__init__.self.state->torch._C._get_tracing_state()
A:torch.jit.__init__.Attribute->collections.namedtuple('Attribute', ['value', 'type'])
torch.jit.__init__.CompilationUnit(self,lang=None,_frames_up=0)
torch.jit.__init__.CompilationUnit.__getattr__(self,attr)
torch.jit.__init__.CompilationUnit.__init__(self,lang=None,_frames_up=0)
torch.jit.__init__.CompilationUnit.define(self,lang,rcb=None,_frames_up=0)
torch.jit.__init__.LegacyTracedModule(self,inner,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit.__init__.LegacyTracedModule.__init__(self,inner,force_outplace=False,return_inputs=False,return_inputs_states=False)
torch.jit.__init__.LegacyTracedModule.forward(self,*args)
torch.jit.__init__.OrderedBufferDict(self,module)
torch.jit.__init__.OrderedBufferDict.__contains__(self,k)
torch.jit.__init__.OrderedBufferDict.__getitem__(self,k)
torch.jit.__init__.OrderedBufferDict.__init__(self,module)
torch.jit.__init__.OrderedBufferDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedBufferDict.items(self)
torch.jit.__init__.OrderedDictWrapper(self,module)
torch.jit.__init__.OrderedDictWrapper.__contains__(self,k)
torch.jit.__init__.OrderedDictWrapper.__delitem__(self,k)
torch.jit.__init__.OrderedDictWrapper.__getitem__(self,k)
torch.jit.__init__.OrderedDictWrapper.__init__(self,module)
torch.jit.__init__.OrderedDictWrapper.__len__(self)
torch.jit.__init__.OrderedDictWrapper.__setitem__(self,k,v)
torch.jit.__init__.OrderedDictWrapper.items(self)
torch.jit.__init__.OrderedDictWrapper.keys(self)
torch.jit.__init__.OrderedDictWrapper.values(self)
torch.jit.__init__.OrderedModuleDict(self,module)
torch.jit.__init__.OrderedModuleDict.__contains__(self,k)
torch.jit.__init__.OrderedModuleDict.__getitem__(self,k)
torch.jit.__init__.OrderedModuleDict.__init__(self,module)
torch.jit.__init__.OrderedModuleDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedModuleDict.items(self)
torch.jit.__init__.OrderedParameterDict(self,module)
torch.jit.__init__.OrderedParameterDict.__contains__(self,k)
torch.jit.__init__.OrderedParameterDict.__getitem__(self,k)
torch.jit.__init__.OrderedParameterDict.__init__(self,module)
torch.jit.__init__.OrderedParameterDict.__setitem__(self,k,v)
torch.jit.__init__.OrderedParameterDict.items(self)
torch.jit.__init__.ScriptMeta(cls,name,bases,attrs)
torch.jit.__init__.ScriptMeta.__init__(cls,name,bases,attrs)
torch.jit.__init__.ScriptWarning(Warning)
torch.jit.__init__.TracedModule(self,orig,id_set=None,_compilation_unit=None)
torch.jit.__init__.TracedModule.__init__(self,orig,id_set=None,_compilation_unit=None)
torch.jit.__init__.TracedModule.__setattr__(self,attr,value)
torch.jit.__init__.TracedModule._freeze(self)
torch.jit.__init__.TracedModule._get_name(self)
torch.jit.__init__.TracedModule.forward(self,*args,**kwargs)
torch.jit.__init__.TracerWarning(Warning)
torch.jit.__init__.TracerWarning.ignore_lib_warnings()
torch.jit.__init__.TracingCheckError(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit.__init__.TracingCheckError.__init__(self,graph_diff_error,tensor_compare_error,extra_msg=None)
torch.jit.__init__._ConstModuleDict(self,modules)
torch.jit.__init__._ConstModuleDict.__contains__(self,key)
torch.jit.__init__._ConstModuleDict.__getitem__(self,key)
torch.jit.__init__._ConstModuleDict.__init__(self,modules)
torch.jit.__init__._ConstModuleDict.__iter__(self)
torch.jit.__init__._ConstModuleDict.__len__(self)
torch.jit.__init__._ConstModuleDict.forward(self)
torch.jit.__init__._ConstModuleDict.items(self)
torch.jit.__init__._ConstModuleDict.keys(self)
torch.jit.__init__._ConstModuleDict.values(self)
torch.jit.__init__._ConstModuleList(self,modules)
torch.jit.__init__._ConstModuleList.__dir__(self)
torch.jit.__init__._ConstModuleList.__getitem__(self,idx)
torch.jit.__init__._ConstModuleList.__init__(self,modules)
torch.jit.__init__._ConstModuleList.__iter__(self)
torch.jit.__init__._ConstModuleList.__len__(self)
torch.jit.__init__._ConstSequential(self,mods)
torch.jit.__init__._ConstSequential.__init__(self,mods)
torch.jit.__init__._add_script_class(cls,name)
torch.jit.__init__._check_directly_compile_overloaded(obj)
torch.jit.__init__._check_no_signature(func)
torch.jit.__init__._check_trace(check_inputs,func,traced_func,check_tolerance,force_outplace,is_trace_module,_module_class)
torch.jit.__init__._clone_inputs(args)
torch.jit.__init__._compile_and_register_class(obj,rcb,qualified_name)
torch.jit.__init__._compile_function_with_overload(qual_name,impl_fn,overload_decl,overload_defaults)
torch.jit.__init__._create_interpreter_name_lookup_fn(frames_up=1)
torch.jit.__init__._create_methods_from_stubs(self,stubs)
torch.jit.__init__._create_named_tuple(t,unqual_name,field_names)
torch.jit.__init__._disable_emit_hooks()
torch.jit.__init__._disable_tracing(object)
torch.jit.__init__._disable_tracing.__enter__(self)
torch.jit.__init__._disable_tracing.__exit__(self,*args)
torch.jit.__init__._dump_trace(trace_name,pass_name,input_key,trace)
torch.jit.__init__._find_builtin(fn)
torch.jit.__init__._gen_rcb(obj,_frames_up)
torch.jit.__init__._get_builtin_table()
torch.jit.__init__._get_methods(cls)
torch.jit.__init__._get_named_tuple_properties(obj)
torch.jit.__init__._get_overload_decl_and_defaults(func)
torch.jit.__init__._get_overloads(obj)
torch.jit.__init__._get_script_class(name)
torch.jit.__init__._get_valid_constant(attr,v)
torch.jit.__init__._graph_for(self,*args,**kwargs)
torch.jit.__init__._is_new_style_class(cls)
torch.jit.__init__._make_fail(name)
torch.jit.__init__._parse_env(name,default,true_message,false_message)
torch.jit.__init__._register_builtin(fn,op)
torch.jit.__init__._time(trace_name,name,time=True)
torch.jit.__init__._try_get_dispatched_fn(fn)
torch.jit.__init__._try_get_overloaded_fn(mod,field)
torch.jit.__init__._unique_state_dict(module,keep_vars=False)
torch.jit.__init__._unwrap_optional(x)
torch.jit.__init__._verify_equal(xs,ys)
torch.jit.__init__.annotate(the_type,the_value)
torch.jit.__init__.get_trace_graph(f,args=(),kwargs=None,_force_outplace=False,return_inputs=False,_return_inputs_states=False)
torch.jit.__init__.indent(s)
torch.jit.__init__.interface(obj)
torch.jit.__init__.is_scripting()
torch.jit.__init__.load(f,map_location=None,_extra_files=DEFAULT_EXTRA_FILES_MAP)
torch.jit.__init__.make_module(mod,_module_class,_compilation_unit,exclude_methods=())
torch.jit.__init__.make_tuple(example_inputs)
torch.jit.__init__.optimized_execution(should_optimize)
torch.jit.__init__.save(m,f,_extra_files=DEFAULT_EXTRA_FILES_MAP)
torch.jit.__init__.scope(scope_name)
torch.jit.__init__.script(obj,optimize=None,_frames_up=0,_rcb=None)
torch.jit.__init__.script_method(fn,_rcb=None)
torch.jit.__init__.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit.__init__.trace_module(mod,inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-05,_force_outplace=False,_module_class=None,_compilation_unit=_python_cu)
torch.jit.__init__.verify(model,args,loss_fn=torch.sum,devices=None)
torch.jit.__init__.whichmodule(obj)
torch.jit.__init__.wrap_check_inputs(check_inputs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/supported_ops.py----------------------------------------
A:torch.jit.supported_ops.v->'\n{}{}'.format(' ' * indent, v)
A:torch.jit.supported_ops.qualified_name->'{}.{}'.format(mod, name)
A:torch.jit.supported_ops.schema->'{}({}) -> {}'.format(qualified_name, emit_args(len(qualified_name) + 1 + 4, schema.arguments[arg_start:]), emit_rets(schema.returns))
A:torch.jit.supported_ops.attr->getattr(mod, elem)
A:torch.jit.supported_ops.scripted->torch.jit.script(attr)
A:torch.jit.supported_ops.mod->inspect.getmodule(fn)
A:torch.jit.supported_ops.builtin->torch.jit._find_builtin(getattr(mod, elem))
A:torch.jit.supported_ops.schemas->torch._C._jit_get_schemas_for_operator('aten::' + elem)
A:torch.jit.supported_ops.__doc__->_list_supported_ops()
torch.jit.supported_ops._list_supported_ops()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/_pickle.py----------------------------------------
torch.jit._pickle.build_boollist(data)
torch.jit._pickle.build_doublelist(data)
torch.jit._pickle.build_intlist(data)
torch.jit._pickle.build_tensor_from_id(data)
torch.jit._pickle.build_tensorlist(data)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/quantized.py----------------------------------------
A:torch.jit.quantized.(self.weight, self.col_offsets, self.scale, self.zero_point)->torch.fbgemm_linear_quantize_weight(other.weight.clone().float())
A:torch.jit.quantized.self.weight->torch.fbgemm_pack_gemm_matrix_fp16(other.weight.clone().float())
A:torch.jit.quantized.self.col_offsets->torch.nn.Parameter(self.col_offsets, requires_grad=False)
A:torch.jit.quantized.self.bias->torch.nn.Parameter(other.bias.clone().float(), requires_grad=False)
A:torch.jit.quantized.out->torch.fbgemm_linear_fp16_weight_fp32_activation(input.float(), self.packed_weight, self.bias)
A:torch.jit.quantized.repr->'in_features={in_features}, out_features={out_features}, '.format(**self.__dict__)
A:torch.jit.quantized.(weight_ih, col_offsets_ih, self.scale_ih, self.zero_point_ih)->torch.fbgemm_linear_quantize_weight(other.weight_ih.clone().float())
A:torch.jit.quantized.(weight_hh, col_offsets_hh, self.scale_hh, self.zero_point_hh)->torch.fbgemm_linear_quantize_weight(other.weight_hh.clone().float())
A:torch.jit.quantized.packed_ih->torch.fbgemm_pack_quantized_matrix(self.weight_ih)
A:torch.jit.quantized.packed_hh->torch.fbgemm_pack_quantized_matrix(self.weight_hh)
A:torch.jit.quantized.self.bias_ih->torch.nn.Parameter(other.bias_ih.clone().float(), requires_grad=False)
A:torch.jit.quantized.self.bias_hh->torch.nn.Parameter(other.bias_hh.clone().float(), requires_grad=False)
A:torch.jit.quantized.hx->self.permute_hidden(hx, sorted_indices)
A:torch.jit.quantized.ret->torch.nn._VF.quantized_rnn_relu_cell(input, hx, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh, self.packed_ih, self.packed_hh, self.col_offsets_ih, self.col_offsets_hh, self.scale_ih, self.scale_hh, self.zero_point_ih, self.zero_point_hh)
A:torch.jit.quantized.zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.jit.quantized.weight_name->'weight_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.jit.quantized.bias_name->'bias_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.jit.quantized.weight->getattr(other, weight_name)
A:torch.jit.quantized.bias->getattr(other, bias_name)
A:torch.jit.quantized.(qweight, col_offsets, scale, zero_point)->torch.fbgemm_linear_quantize_weight(weight.clone().float())
A:torch.jit.quantized.packed_weight->torch.fbgemm_pack_gemm_matrix_fp16(weight.clone().float())
A:torch.jit.quantized.(ih_params, ih_param_names)->process_weights('ih', layer, suffix, dtype)
A:torch.jit.quantized.(hh_params, hh_param_names)->process_weights('hh', layer, suffix, dtype)
A:torch.jit.quantized.mini_batch->int(mini_batch)
A:torch.jit.quantized.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.jit.quantized.packed_weights->self._get_packed_weights()
A:torch.jit.quantized.quantized_weights->self._get_quantized_weights()
A:torch.jit.quantized.orig_weights->self._get_orig_weights()
A:torch.jit.quantized.result->torch.nn._VF.quantized_gru(input, batch_sizes, hx, self._get_all_weights(), self.bias, self.num_layers, float(self.dropout), self.training, self.bidirectional)
A:torch.jit.quantized.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.jit.quantized.max_batch_size->int(max_batch_size)
A:torch.jit.quantized.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.jit.quantized.new_mod->quantize_rnn_modules(mod, dtype)
torch.jit.quantized.QuantizedGRU(QuantizedRNNBase)
torch.jit.quantized.QuantizedGRU.forward(self,input,hx=None)
torch.jit.quantized.QuantizedGRU.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.jit.quantized.QuantizedGRU.forward_packed(self,input,hx=None)
torch.jit.quantized.QuantizedGRU.forward_tensor(self,input,hx=None)
torch.jit.quantized.QuantizedGRUCell(self,other)
torch.jit.quantized.QuantizedGRUCell.__init__(self,other)
torch.jit.quantized.QuantizedGRUCell.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM(self,other,dtype)
torch.jit.quantized.QuantizedLSTM.__init__(self,other,dtype)
torch.jit.quantized.QuantizedLSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.jit.quantized.QuantizedLSTM.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.jit.quantized.QuantizedLSTM.forward_packed(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.forward_tensor(self,input,hx=None)
torch.jit.quantized.QuantizedLSTM.permute_hidden(self,hx,permutation)
torch.jit.quantized.QuantizedLSTMCell(self,other)
torch.jit.quantized.QuantizedLSTMCell.__init__(self,other)
torch.jit.quantized.QuantizedLSTMCell.forward(self,input,hx=None)
torch.jit.quantized.QuantizedLinear(self,other)
torch.jit.quantized.QuantizedLinear.__init__(self,other)
torch.jit.quantized.QuantizedLinear._pack(self)
torch.jit.quantized.QuantizedLinear._unpack(self)
torch.jit.quantized.QuantizedLinear.extra_repr(self)
torch.jit.quantized.QuantizedLinear.forward(self,input)
torch.jit.quantized.QuantizedLinearFP16(self,other)
torch.jit.quantized.QuantizedLinearFP16.__init__(self,other)
torch.jit.quantized.QuantizedLinearFP16._pack(self)
torch.jit.quantized.QuantizedLinearFP16._unpack(self)
torch.jit.quantized.QuantizedLinearFP16.extra_repr(self)
torch.jit.quantized.QuantizedLinearFP16.forward(self,input)
torch.jit.quantized.QuantizedRNNBase(self,other,dtype=torch.int8)
torch.jit.quantized.QuantizedRNNBase.__init__(self,other,dtype=torch.int8)
torch.jit.quantized.QuantizedRNNBase._get_all_weights(self)
torch.jit.quantized.QuantizedRNNBase._get_all_weights_names(self)
torch.jit.quantized.QuantizedRNNBase._get_orig_weights(self)
torch.jit.quantized.QuantizedRNNBase._get_orig_weights_names(self)
torch.jit.quantized.QuantizedRNNBase._get_packed_weights(self)
torch.jit.quantized.QuantizedRNNBase._get_packed_weights_names(self)
torch.jit.quantized.QuantizedRNNBase._get_quantized_weights(self)
torch.jit.quantized.QuantizedRNNBase._get_quantized_weights_names(self)
torch.jit.quantized.QuantizedRNNBase._pack(self)
torch.jit.quantized.QuantizedRNNBase._unpack(self)
torch.jit.quantized.QuantizedRNNBase.all_weights(self)
torch.jit.quantized.QuantizedRNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.jit.quantized.QuantizedRNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.jit.quantized.QuantizedRNNBase.check_input(self,input,batch_sizes)
torch.jit.quantized.QuantizedRNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.jit.quantized.QuantizedRNNBase.permute_hidden(self,hx,permutation)
torch.jit.quantized.QuantizedRNNCell(self,other)
torch.jit.quantized.QuantizedRNNCell.__init__(self,other)
torch.jit.quantized.QuantizedRNNCell.forward(self,input,hx=None)
torch.jit.quantized.QuantizedRNNCellBase(self,other)
torch.jit.quantized.QuantizedRNNCellBase.__init__(self,other)
torch.jit.quantized.QuantizedRNNCellBase._pack(self)
torch.jit.quantized.QuantizedRNNCellBase._unpack(self)
torch.jit.quantized.QuantizedRNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.jit.quantized.QuantizedRNNCellBase.check_forward_input(self,input)
torch.jit.quantized.QuantizedRNNCellBase.extra_repr(self)
torch.jit.quantized.apply_permutation(tensor,permutation,dim=1)
torch.jit.quantized.quantize_linear_modules(module,dtype=torch.int8)
torch.jit.quantized.quantize_rnn_cell_modules(module)
torch.jit.quantized.quantize_rnn_modules(module,dtype=torch.int8)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/_logging.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/frontend.py----------------------------------------
A:torch.jit.frontend._identifier_chars->set(string.ascii_lowercase + string.ascii_uppercase + string.digits)
A:torch.jit.frontend.self.error_report->torch._C.ErrorReport(self.source_range)
A:torch.jit.frontend.node_type->type(offending_node)
A:torch.jit.frontend.range_len->len(node_start_tokens.get(node_type, ' '))
A:torch.jit.frontend.source_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(offending_node.lineno, offending_node.col_offset, offending_node.col_offset + range_len)
A:torch.jit.frontend.feature_name->pretty_node_names.get(node_type, node_type.__name__)
A:torch.jit.frontend.msg->"{} aren't supported".format(feature_name)
A:torch.jit.frontend.methods->inspect.getmembers(cls, predicate=lambda m: inspect.ismethod(m) or inspect.isfunction(m))
A:torch.jit.frontend.(sourcelines, file_lineno, filename)->get_source_lines_and_file(fn)
A:torch.jit.frontend.source->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).source.encode('utf-8')
A:torch.jit.frontend.dedent_src->dedent(source)
A:torch.jit.frontend.py_ast->ast.parse(dedent_src)
A:torch.jit.frontend.ctx->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn))
A:torch.jit.frontend.type_line->torch.jit.annotations.get_type_line(source)
A:torch.jit.frontend.method->getattr(self, 'build_' + node.__class__.__name__, None)
A:torch.jit.frontend.r->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(expr.lineno, expr.col_offset, expr.col_offset + 1)
A:torch.jit.frontend.param_list->build_param_list(ctx, py_def.args, self_name)
A:torch.jit.frontend.return_type->build_expr(ctx, py_def.returns)
A:torch.jit.frontend.decl->torch._C.merge_type_from_type_comment(decl, type_comment_decl, is_method)
A:torch.jit.frontend.type_comment_decl->torch._C.parse_type_comment(type_line)
A:torch.jit.frontend.ctx_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(expr.lineno, expr.col_offset - 1, expr.col_offset + len(expr.arg))
A:torch.jit.frontend.annotation_expr->EmptyTypeAnnotation(r)
A:torch.jit.frontend.argspec->inspect.getargspec(fn)
A:torch.jit.frontend.signature->inspect.signature(fn)
A:torch.jit.frontend.rhs->build_expr(ctx, expr.right)
A:torch.jit.frontend.lhs->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.the_type->build_expr(ctx, stmt.annotation)
A:torch.jit.frontend.expr->build_expr(ctx, stmt.exc)
A:torch.jit.frontend.test->build_expr(ctx, stmt.test)
A:torch.jit.frontend.op->type(op_)
A:torch.jit.frontend.base->build_expr(ctx, expr.value)
A:torch.jit.frontend.name_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_raw_range(start_pos, end_pos)
A:torch.jit.frontend.func->build_expr(ctx, expr.func)
A:torch.jit.frontend.stararg_expr->build_expr(ctx, expr.starargs)
A:torch.jit.frontend.kw_expr->build_expr(ctx, kw.value)
A:torch.jit.frontend.err_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_raw_range(sub_exprs[0].range().end, sub_exprs[1].range().start)
A:torch.jit.frontend.op_token->ExprBuilder.cmpop_map.get(op)
A:torch.jit.frontend.sub_expr->build_expr(ctx, expr.operand)
A:torch.jit.frontend.in_expr->BinOp('in', lhs, rhs)
A:torch.jit.frontend.cmp_expr->BinOp(op_token, lhs, rhs)
A:torch.jit.frontend.result->BinOp('and', result, cmp_expr)
A:torch.jit.frontend.sub_type->type(expr.slice)
A:torch.jit.frontend.value->str(expr.s)
A:torch.jit.frontend.error_range->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).make_range(expr.lineno, expr.col_offset, expr.col_offset + len(str(value)))
A:torch.jit.frontend.elt_expr->build_expr(ctx, stmt.elt)
A:torch.jit.frontend.target_expr->build_expr(ctx, stmt.generators[0].target)
A:torch.jit.frontend.iter_expr->build_expr(ctx, stmt.generators[0].iter)
A:torch.jit.frontend.build_expr->ExprBuilder()
A:torch.jit.frontend.build_stmt->StmtBuilder()
A:torch.jit.frontend.new_pos->SourceContext(source, filename, file_lineno, leading_whitespace_len, _uses_true_division(fn)).source[:pos].rindex(substr)
torch.jit.frontend.Builder(self,ctx,node)
torch.jit.frontend.Builder.__call__(self,ctx,node)
torch.jit.frontend.ExprBuilder(Builder)
torch.jit.frontend.ExprBuilder.build_Attribute(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BinOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_BoolOp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Call(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Compare(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Constant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Dict(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Ellipsis(ctx,expr)
torch.jit.frontend.ExprBuilder.build_IfExp(ctx,expr)
torch.jit.frontend.ExprBuilder.build_JoinedStr(ctx,expr)
torch.jit.frontend.ExprBuilder.build_List(ctx,expr)
torch.jit.frontend.ExprBuilder.build_ListComp(ctx,stmt)
torch.jit.frontend.ExprBuilder.build_Name(ctx,expr)
torch.jit.frontend.ExprBuilder.build_NameConstant(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Num(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Starred(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Str(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Subscript(ctx,expr)
torch.jit.frontend.ExprBuilder.build_Tuple(ctx,expr)
torch.jit.frontend.ExprBuilder.build_UnaryOp(ctx,expr)
torch.jit.frontend.FrontendError(self,source_range,msg)
torch.jit.frontend.FrontendError.__init__(self,source_range,msg)
torch.jit.frontend.FrontendError.__str__(self)
torch.jit.frontend.FrontendTypeError(FrontendError)
torch.jit.frontend.NotSupportedError(FrontendError)
torch.jit.frontend.SourceContext(self,source,filename,file_lineno,leading_whitespace_len,uses_true_division=True)
torch.jit.frontend.SourceContext.__init__(self,source,filename,file_lineno,leading_whitespace_len,uses_true_division=True)
torch.jit.frontend.StmtBuilder(Builder)
torch.jit.frontend.StmtBuilder.build_AnnAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assert(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Assign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_AugAssign(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Break(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Continue(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Expr(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_For(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_If(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Pass(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Print(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Raise(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_Return(ctx,stmt)
torch.jit.frontend.StmtBuilder.build_While(ctx,stmt)
torch.jit.frontend.UnsupportedNodeError(self,ctx,offending_node)
torch.jit.frontend.UnsupportedNodeError.__init__(self,ctx,offending_node)
torch.jit.frontend._uses_true_division(fn)
torch.jit.frontend.build_class_def(ctx,py_def,methods,self_name)
torch.jit.frontend.build_def(ctx,py_def,type_line,self_name=None)
torch.jit.frontend.build_param(ctx,py_arg,self_name,kwarg_only)
torch.jit.frontend.build_param_list(ctx,py_args,self_name)
torch.jit.frontend.build_stmts(ctx,stmts)
torch.jit.frontend.find_before(ctx,pos,substr,offsets=(0,0))
torch.jit.frontend.get_default_args(fn)
torch.jit.frontend.get_jit_class_def(cls,self_name)
torch.jit.frontend.get_jit_def(fn,self_name=None)
torch.jit.frontend.is_reserved_name(name)
torch.jit.get_default_args(fn)
torch.jit.get_jit_class_def(cls,self_name)
torch.jit.get_jit_def(fn,self_name=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/jit/annotations.py----------------------------------------
A:torch.jit.annotations.sig->inspect.signature(fn)
A:torch.jit.annotations.source->dedent(''.join(get_source_lines_and_file(fn)[0]))
A:torch.jit.annotations.type_line->get_type_line(source)
A:torch.jit.annotations.py_ast->ast.parse(source)
A:torch.jit.annotations.num_params->len(py_def.args.args)
A:torch.jit.annotations.(arg_ann_str, ret_ann_str)->split_type_line(type_line)
A:torch.jit.annotations.arg_ann->eval(arg_ann_str, {}, EvalEnv(rcb))
A:torch.jit.annotations.ret_ann->eval(ret_ann_str, {}, EvalEnv(rcb))
A:torch.jit.annotations.lines->dedent(''.join(get_source_lines_and_file(fn)[0])).split('\n')
A:torch.jit.annotations.type_lines->list(filter(lambda line: type_comment in line[1], lines))
A:torch.jit.annotations.lines_with_type->list(filter(lambda line: 'type' in line[1], lines))
A:torch.jit.annotations.type_pattern->re.compile('#[\t ]*type[\t ]*:')
A:torch.jit.annotations.wrong_type_lines->list(filter(lambda line: type_pattern.search(line[1]), lines))
A:torch.jit.annotations.types->map(get_parameter_type, parameter_type_lines)
A:torch.jit.annotations.parameter_types->', '.join(types)
A:torch.jit.annotations.start_offset->len('# type:')
A:torch.jit.annotations.arrow_pos->get_type_line(source).index('->')
A:torch.jit.annotations.return_type->ann_to_type(as_ann(sig.return_annotation))
A:torch.jit.annotations.key->ann_to_type(ann.__args__[0])
A:torch.jit.annotations.value->ann_to_type(ann.__args__[1])
A:torch.jit.annotations.the_type->torch._C._resolve_type(ann.__name__, loc, rcb)
torch.jit.annotations.EvalEnv(self,rcb)
torch.jit.annotations.EvalEnv.__getitem__(self,name)
torch.jit.annotations.EvalEnv.__init__(self,rcb)
torch.jit.annotations.Module(self,name,members)
torch.jit.annotations.Module.__getattr__(self,name)
torch.jit.annotations.Module.__init__(self,name,members)
torch.jit.annotations.ann_to_type(ann,resolver=None)
torch.jit.annotations.get_num_params(fn,loc)
torch.jit.annotations.get_signature(fn,rcb,loc)
torch.jit.annotations.get_type_line(source)
torch.jit.annotations.parse_type_line(type_line,rcb,loc)
torch.jit.annotations.split_type_line(type_line)
torch.jit.annotations.try_real_annotations(fn)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/functional.py----------------------------------------
A:torch.nn.functional.conv1d->_add_docstr(torch.conv1d, '\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or\n      a one-element tuple `(sW,)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a\n      single number or a one-element tuple `(padW,)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a one-element tuple `(dW,)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3)\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> F.conv1d(inputs, filters)\n')
A:torch.nn.functional.conv2d->_add_docstr(torch.conv2d, '\nconv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8,4,3,3)\n    >>> inputs = torch.randn(1,4,5,5)\n    >>> F.conv2d(inputs, filters, padding=1)\n')
A:torch.nn.functional.conv3d->_add_docstr(torch.conv3d, '\nconv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 3D convolution over an input image composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv3d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: 1\n    padding: implicit paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`. Default: 0\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by\n      the number of groups. Default: 1\n\nExamples::\n\n    >>> filters = torch.randn(33, 16, 3, 3, 3)\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> F.conv3d(inputs, filters)\n')
A:torch.nn.functional.conv_transpose1d->_add_docstr(torch.conv_transpose1d, '\nconv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 1D transposed convolution operator over an input signal\ncomposed of several input planes, sometimes also called "deconvolution".\n\nSee :class:`~torch.nn.ConvTranspose1d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sW,)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padW,)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dW,)``. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50)\n    >>> weights = torch.randn(16, 33, 5)\n    >>> F.conv_transpose1d(inputs, weights)\n')
A:torch.nn.functional.conv_transpose2d->_add_docstr(torch.conv_transpose2d, '\nconv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 2D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution".\n\nSee :class:`~torch.nn.ConvTranspose2d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.\n      Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple ``(dH, dW)``. Default: 1\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> weights = torch.randn(4, 8, 3, 3)\n    >>> F.conv_transpose2d(inputs, weights, padding=1)\n')
A:torch.nn.functional.conv_transpose3d->_add_docstr(torch.conv_transpose3d, '\nconv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -> Tensor\n\nApplies a 3D transposed convolution operator over an input image\ncomposed of several input planes, sometimes also called "deconvolution"\n\nSee :class:`~torch.nn.ConvTranspose3d` for details and output shape.\n\n.. include:: cudnn_deterministic.rst\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)`\n    weight: filters of shape :math:`(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)`\n    bias: optional bias of shape :math:`(\\text{out\\_channels})`. Default: None\n    stride: the stride of the convolving kernel. Can be a single number or a\n      tuple ``(sT, sH, sW)``. Default: 1\n    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both\n      sides of each dimension in the input. Can be a single number or a tuple\n      ``(padT, padH, padW)``. Default: 0\n    output_padding: additional size added to one side of each dimension in the\n      output shape. Can be a single number or a tuple\n      ``(out_padT, out_padH, out_padW)``. Default: 0\n    groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the\n      number of groups. Default: 1\n    dilation: the spacing between kernel elements. Can be a single number or\n      a tuple `(dT, dH, dW)`. Default: 1\n\nExamples::\n\n    >>> inputs = torch.randn(20, 16, 50, 10, 20)\n    >>> weights = torch.randn(16, 33, 3, 3, 3)\n    >>> F.conv_transpose3d(inputs, weights)\n')
A:torch.nn.functional.conv_tbc->_add_docstr(torch.conv_tbc, '\nApplies a 1-dimensional sequence convolution over an input sequence.\nInput and output dimensions are (Time, Batch, Channels) - hence TBC.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{sequence length} \\times batch \\times \\text{in\\_channels})`\n    weight: filter of shape (:math:`\\text{kernel width} \\times \\text{in\\_channels} \\times \\text{out\\_channels}`)\n    bias: bias of shape (:math:`\\text{out\\_channels}`)\n    pad: number of timesteps to pad. Default: 0\n')
A:torch.nn.functional.avg_pool1d->_add_docstr(torch.avg_pool1d, '\navg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple `(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])\n\n')
A:torch.nn.functional.avg_pool2d->_add_docstr(torch._C._nn.avg_pool2d, '\navg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n:math:`sH \\times sW` steps. The number of output features is equal to the number of\ninput planes.\n\nSee :class:`~torch.nn.AvgPool2d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padH, padW)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n    divisor_override: if specified, it will be used as divisor, otherwise\n         size of the pooling region will be used. Default: None\n')
A:torch.nn.functional.avg_pool3d->_add_docstr(torch._C._nn.avg_pool3d, '\navg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n\nApplies 3D average-pooling operation in :math:`kT \\times kH \\times kW` regions by step\nsize :math:`sT \\times sH \\times sW` steps. The number of output features is equal to\n:math:`\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor`.\n\nSee :class:`~torch.nn.AvgPool3d` for details and output shape.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)`\n    kernel_size: size of the pooling region. Can be a single number or a\n      tuple `(kT, kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padT, padH, padW)`, Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n        to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation\n    divisor_override: if specified, it will be used as divisor, otherwise\n        size of the pooling region will be used. Default: None\n')
A:torch.nn.functional._output_ratio->_triple(output_ratio)
A:torch.nn.functional._random_samples->torch.rand(input.size(0), input.size(1), 3, dtype=input.dtype, device=input.device)
A:torch.nn.functional.fractional_max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool2d_with_indices, if_false=_fractional_max_pool2d, module_name=__name__, func_name='fractional_max_pool2d')
A:torch.nn.functional.fractional_max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=4, default=False, if_true=fractional_max_pool3d_with_indices, if_false=_fractional_max_pool3d, module_name=__name__, func_name='fractional_max_pool3d')
A:torch.nn.functional.stride->torch.jit.annotate(List[int], [])
A:torch.nn.functional.max_pool1d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool1d_with_indices, if_false=_max_pool1d, module_name=__name__, func_name='max_pool1d')
A:torch.nn.functional.max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool2d_with_indices, if_false=_max_pool2d, module_name=__name__, func_name='max_pool2d')
A:torch.nn.functional.max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=6, default=False, if_true=max_pool3d_with_indices, if_false=_max_pool3d, module_name=__name__, func_name='max_pool3d')
A:torch.nn.functional.input_size->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size()
A:torch.nn.functional.default_size->torch.jit.annotate(List[int], [])
A:torch.nn.functional.kernel_size->_triple(kernel_size)
A:torch.nn.functional._stride->_triple(stride)
A:torch.nn.functional.padding->_triple(padding)
A:torch.nn.functional.output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.(kw, kh)->modules.utils._pair(kernel_size)
A:torch.nn.functional.out->torch._C._nn.nll_loss2d(input, target, weight, reduction_enum, ignore_index)
A:torch.nn.functional.adaptive_max_pool1d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool1d_with_indices, if_false=_adaptive_max_pool1d, module_name=__name__, func_name='adaptive_max_pool1d')
A:torch.nn.functional.adaptive_max_pool2d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool2d_with_indices, if_false=_adaptive_max_pool2d, module_name=__name__, func_name='adaptive_max_pool2d')
A:torch.nn.functional.adaptive_max_pool3d->boolean_dispatch(arg_name='return_indices', arg_index=2, default=False, if_true=adaptive_max_pool3d_with_indices, if_false=_adaptive_max_pool3d, module_name=__name__, func_name='adaptive_max_pool3d')
A:torch.nn.functional.adaptive_avg_pool1d->_add_docstr(torch.adaptive_avg_pool1d, '\nadaptive_avg_pool1d(input, output_size) -> Tensor\n\nApplies a 1D adaptive average pooling over an input signal composed of\nseveral input planes.\n\nSee :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.\n\nArgs:\n    output_size: the target output size (single integer)\n')
A:torch.nn.functional._output_size->_list_with_default(output_size, input.size())
A:torch.nn.functional.result->torch.rrelu(input, lower, upper, training)
A:torch.nn.functional.threshold_->_add_docstr(_VF.threshold_, '\nthreshold_(input, threshold, value) -> Tensor\n\nIn-place version of :func:`~threshold`.\n')
A:torch.nn.functional.relu_->_add_docstr(torch.relu_, '\nrelu_(input) -> Tensor\n\nIn-place version of :func:`~relu`.\n')
A:torch.nn.functional.hardtanh_->_add_docstr(torch._C._nn.hardtanh_, '\nhardtanh_(input, min_val=-1., max_val=1.) -> Tensor\n\nIn-place version of :func:`~hardtanh`.\n')
A:torch.nn.functional.elu_->_add_docstr(torch._C._nn.elu_, '\nelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~elu`.\n')
A:torch.nn.functional.selu_->_add_docstr(torch.selu_, '\nselu_(input) -> Tensor\n\nIn-place version of :func:`~selu`.\n')
A:torch.nn.functional.celu_->_add_docstr(torch.celu_, '\ncelu_(input, alpha=1.) -> Tensor\n\nIn-place version of :func:`~celu`.\n')
A:torch.nn.functional.leaky_relu_->_add_docstr(torch._C._nn.leaky_relu_, '\nleaky_relu_(input, negative_slope=0.01) -> Tensor\n\nIn-place version of :func:`~leaky_relu`.\n')
A:torch.nn.functional.rrelu_->_add_docstr(torch.rrelu_, '\nrrelu_(input, lower=1./8, upper=1./3, training=False) -> Tensor\n\nIn-place version of :func:`~rrelu`.\n')
A:torch.nn.functional.logsigmoid->_add_docstr(torch._C._nn.log_sigmoid, '\nlogsigmoid(input) -> Tensor\n\nApplies element-wise :math:`\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)`\n\nSee :class:`~torch.nn.LogSigmoid` for more details.\n')
A:torch.nn.functional.softplus->_add_docstr(torch._C._nn.softplus, '\nsoftplus(input, beta=1, threshold=20) -> Tensor\n')
A:torch.nn.functional.dim->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).dim()
A:torch.nn.functional.ret->loss.sum()
A:torch.nn.functional.y_soft->gumbels.softmax(dim)
A:torch.nn.functional.y_hard->torch.zeros_like(logits).scatter_(dim, index, 1.0)
A:torch.nn.functional.softshrink->_add_docstr(torch._C._nn.softshrink, '\nsoftshrink(input, lambd=0.5) -> Tensor\n\nApplies the soft shrinkage function elementwise\n\nSee :class:`~torch.nn.Softshrink` for more details.\n')
A:torch.nn.functional.output->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).matmul(weight.t())
A:torch.nn.functional.input->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4)
A:torch.nn.functional.offsets->torch.arange(0, input.numel(), input.size(1), dtype=torch.long, device=input.device)
A:torch.nn.functional.per_sample_weights->per_sample_weights.reshape(-1).reshape(-1)
A:torch.nn.functional.(ret, _, _, _)->torch.embedding_bag(weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights)
A:torch.nn.functional.size->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size()
A:torch.nn.functional.div->div.mul(alpha).add(k).pow(beta).mul(alpha).add(k).pow(beta)
A:torch.nn.functional.sizes->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size()
A:torch.nn.functional.reduction->_Reduction.legacy_get_string(size_average, reduce)
A:torch.nn.functional.n->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size(0)
A:torch.nn.functional.c->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).size(1)
A:torch.nn.functional.target->target.contiguous().view(n, 1, -1).contiguous().view(n, 1, -1)
A:torch.nn.functional.reduction_enum->_Reduction.get_enum(reduction)
A:torch.nn.functional.reduced->torch.kl_div(input, target, reduction_enum)
A:torch.nn.functional.new_size->_infer_size(target.size(), weight.size())
A:torch.nn.functional.weight->weight.expand(new_size).expand(new_size)
A:torch.nn.functional.d->lambd(input, target)
A:torch.nn.functional.(expanded_input, expanded_target)->torch.broadcast_tensors(input, target)
A:torch.nn.functional.t->torch.abs(input - target)
A:torch.nn.functional.pixel_shuffle->_add_docstr(torch.pixel_shuffle, '\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)` to a\ntensor of shape :math:`(*, C, H \\times r, W \\times r)`.\n\nSee :class:`~torch.nn.PixelShuffle` for details.\n\nArgs:\n    input (Tensor): the input tensor\n    upscale_factor (int): factor to increase spatial resolution by\n\nExamples::\n\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = torch.nn.functional.pixel_shuffle(input, 3)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n')
A:torch.nn.functional.scale_factors->_ntuple(dim)(scale_factor)
A:torch.nn.functional.pdist->_add_docstr(torch.pdist, "\npdist(input, p=2) -> Tensor\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape :math:`N \\times M` then the output will have shape\n:math:`\\frac{1}{2} N (N - 1)`.\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n'minkowski', p=p)` if :math:`p \\in (0, \\infty)`. When :math:`p = 0` it is\nequivalent to `scipy.spatial.distance.pdist(input, 'hamming') * M`.\nWhen :math:`p = \\infty`, the closest scipy function is\n`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.\n\nArgs:\n    input: input tensor of shape :math:`N \\times M`.\n    p: p value for the p-norm distance to calculate between each vector pair\n        :math:`\\in [0, \\infty]`.\n")
A:torch.nn.functional.cosine_similarity->_add_docstr(torch.cosine_similarity, '\ncosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between x1 and x2, computed along dim.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input (of size matching x1).\n    dim (int, optional): Dimension of vectors. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nShape:\n    - Input: :math:`(\\ast_1, D, \\ast_2)` where D is at position `dim`.\n    - Output: :math:`(\\ast_1, \\ast_2)` where 1 is at position `dim`.\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)\n')
A:torch.nn.functional.one_hot->_add_docstr(torch._C._nn.one_hot, '\none_hot(tensor, num_classes=-1) -> LongTensor\n\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.\n\nSee also `One-hot on Wikipedia`_ .\n\n.. _One-hot on Wikipedia:\n    https://en.wikipedia.org/wiki/One-hot\n\nArguments:\n    tensor (LongTensor): class values of any shape.\n    num_classes (int):  Total number of classes. If set to -1, the number\n        of classes will be inferred as one greater than the largest class\n        value in the input tensor.\n\nReturns:\n    LongTensor that has one more dimension with 1 values at the\n    index of last dimension indicated by the input, and 0 everywhere\n    else.\n\nExamples:\n    >>> F.one_hot(torch.arange(0, 5) % 3)\n    tensor([[1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 1, 0]])\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n    tensor([[1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0]])\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n    tensor([[[1, 0, 0],\n             [0, 1, 0]],\n            [[0, 0, 1],\n             [1, 0, 0]],\n            [[0, 1, 0],\n             [0, 0, 1]]])\n')
A:torch.nn.functional.denom->torch.cat([input[:, :, :, :, -(padding[-5] + padding[-6]):-padding[-5]], input], dim=4).norm(p, dim, True).clamp_min(eps).expand_as(input)
A:torch.nn.functional.kv_same->torch.equal(key, value)
A:torch.nn.functional.(tgt_len, bsz, embed_dim)->query.size()
A:torch.nn.functional.(q, k, v)->linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)
A:torch.nn.functional.q->q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1).contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
A:torch.nn.functional.(k, v)->linear(key, _w, _b).chunk(2, dim=-1)
A:torch.nn.functional.k->torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)
A:torch.nn.functional.v->torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)
A:torch.nn.functional.q_proj_weight_non_opt->torch.jit._unwrap_optional(q_proj_weight)
A:torch.nn.functional.(len1, len2)->torch.jit._unwrap_optional(v_proj_weight).size()
A:torch.nn.functional.k_proj_weight_non_opt->torch.jit._unwrap_optional(k_proj_weight)
A:torch.nn.functional.v_proj_weight_non_opt->torch.jit._unwrap_optional(v_proj_weight)
A:torch.nn.functional.attn_mask->attn_mask.unsqueeze(0).unsqueeze(0)
A:torch.nn.functional.key_padding_mask->torch.cat([key_padding_mask, torch.zeros((key_padding_mask.size(0), 1), dtype=key_padding_mask.dtype, device=key_padding_mask.device)], dim=1)
A:torch.nn.functional.src_len->torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1).size(1)
A:torch.nn.functional.attn_output_weights->attn_output_weights.view(bsz, num_heads, tgt_len, src_len).view(bsz, num_heads, tgt_len, src_len)
A:torch.nn.functional.attn_output->linear(attn_output, out_proj_weight, out_proj_bias)
torch.nn._adaptive_max_pool1d(input,output_size,return_indices=False)
torch.nn._adaptive_max_pool2d(input,output_size,return_indices=False)
torch.nn._adaptive_max_pool3d(input,output_size,return_indices=False)
torch.nn._fractional_max_pool2d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn._fractional_max_pool3d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn._get_softmax_dim(name,ndim,stacklevel)
torch.nn._max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._max_pool3d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn._no_grad_embedding_renorm_(weight,input,max_norm,norm_type)
torch.nn._pad_circular(input,padding)
torch.nn._pointwise_loss(lambd,lambd_optimized,input,target,reduction='mean')
torch.nn._smooth_l1_loss(input,target)
torch.nn._unpool_output_size(input,kernel_size,stride,padding,output_size)
torch.nn.adaptive_avg_pool2d(input,output_size)
torch.nn.adaptive_avg_pool3d(input,output_size)
torch.nn.adaptive_max_pool1d_with_indices(input,output_size,return_indices=False)
torch.nn.adaptive_max_pool2d_with_indices(input,output_size,return_indices=False)
torch.nn.adaptive_max_pool3d_with_indices(input,output_size,return_indices=False)
torch.nn.affine_grid(theta,size,align_corners=None)
torch.nn.alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.assert_int_or_pair(arg,arg_name,message)
torch.nn.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)
torch.nn.bilinear(input1,input2,weight,bias=None)
torch.nn.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.celu(input,alpha=1.0,inplace=False)
torch.nn.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean',zero_infinity=False)
torch.nn.dropout(input,p=0.5,training=True,inplace=False)
torch.nn.dropout2d(input,p=0.5,training=True,inplace=False)
torch.nn.dropout3d(input,p=0.5,training=True,inplace=False)
torch.nn.elu(input,alpha=1.0,inplace=False)
torch.nn.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False,per_sample_weights=None)
torch.nn.feature_alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.fractional_max_pool2d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.fractional_max_pool3d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._adaptive_max_pool1d(input,output_size,return_indices=False)
torch.nn.functional._adaptive_max_pool2d(input,output_size,return_indices=False)
torch.nn.functional._adaptive_max_pool3d(input,output_size,return_indices=False)
torch.nn.functional._fractional_max_pool2d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._fractional_max_pool3d(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional._get_softmax_dim(name,ndim,stacklevel)
torch.nn.functional._max_pool1d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._max_pool3d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional._no_grad_embedding_renorm_(weight,input,max_norm,norm_type)
torch.nn.functional._pad_circular(input,padding)
torch.nn.functional._pointwise_loss(lambd,lambd_optimized,input,target,reduction='mean')
torch.nn.functional._smooth_l1_loss(input,target)
torch.nn.functional._unpool_output_size(input,kernel_size,stride,padding,output_size)
torch.nn.functional.adaptive_avg_pool2d(input,output_size)
torch.nn.functional.adaptive_avg_pool3d(input,output_size)
torch.nn.functional.adaptive_max_pool1d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.adaptive_max_pool2d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.adaptive_max_pool3d_with_indices(input,output_size,return_indices=False)
torch.nn.functional.affine_grid(theta,size,align_corners=None)
torch.nn.functional.alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.functional.assert_int_or_pair(arg,arg_name,message)
torch.nn.functional.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)
torch.nn.functional.bilinear(input1,input2,weight,bias=None)
torch.nn.functional.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.functional.celu(input,alpha=1.0,inplace=False)
torch.nn.functional.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.functional.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean',zero_infinity=False)
torch.nn.functional.dropout(input,p=0.5,training=True,inplace=False)
torch.nn.functional.dropout2d(input,p=0.5,training=True,inplace=False)
torch.nn.functional.dropout3d(input,p=0.5,training=True,inplace=False)
torch.nn.functional.elu(input,alpha=1.0,inplace=False)
torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.functional.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False,per_sample_weights=None)
torch.nn.functional.feature_alpha_dropout(input,p=0.5,training=False,inplace=False)
torch.nn.functional.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.functional.fractional_max_pool2d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional.fractional_max_pool3d_with_indices(input,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.functional.gelu(input)
torch.nn.functional.glu(input,dim=-1)
torch.nn.functional.grid_sample(input,grid,mode='bilinear',padding_mode='zeros',align_corners=None)
torch.nn.functional.group_norm(input,num_groups,weight=None,bias=None,eps=1e-05)
torch.nn.functional.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)
torch.nn.functional.hardshrink(input,lambd=0.5)
torch.nn.functional.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.functional.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)
torch.nn.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)
torch.nn.functional.leaky_relu(input,negative_slope=0.01,inplace=False)
torch.nn.functional.linear(input,weight,bias=None)
torch.nn.functional.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.functional.log_softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.functional.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.functional.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.max_pool1d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_pool2d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_pool3d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.functional.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.functional.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multi_head_attention_forward(query,key,value,embed_dim_to_check,num_heads,in_proj_weight,in_proj_bias,bias_k,bias_v,add_zero_attn,dropout_p,out_proj_weight,out_proj_bias,training=True,key_padding_mask=None,need_weights=True,attn_mask=None,use_separate_proj_weight=False,q_proj_weight=None,k_proj_weight=None,v_proj_weight=None,static_k=None,static_v=None)
torch.nn.functional.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.multilabel_soft_margin_loss(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.functional.normalize(input,p=2,dim=1,eps=1e-12,out=None)
torch.nn.functional.pad(input,pad,mode='constant',value=0)
torch.nn.functional.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)
torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.functional.prelu(input,weight)
torch.nn.functional.relu(input,inplace=False)
torch.nn.functional.relu6(input,inplace=False)
torch.nn.functional.rrelu(input,lower=1.0/8,upper=1.0/3,training=False,inplace=False)
torch.nn.functional.selu(input,inplace=False)
torch.nn.functional.sigmoid(input)
torch.nn.functional.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.softmin(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.functional.softsign(input)
torch.nn.functional.tanh(input)
torch.nn.functional.tanhshrink(input)
torch.nn.functional.threshold(input,threshold,value,inplace=False)
torch.nn.functional.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.functional.unfold(input,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.functional.upsample_nearest(input,size=None,scale_factor=None)
torch.nn.gelu(input)
torch.nn.glu(input,dim=-1)
torch.nn.grid_sample(input,grid,mode='bilinear',padding_mode='zeros',align_corners=None)
torch.nn.group_norm(input,num_groups,weight=None,bias=None,eps=1e-05)
torch.nn.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)
torch.nn.hardshrink(input,lambd=0.5)
torch.nn.hardtanh(input,min_val=-1.0,max_val=1.0,inplace=False)
torch.nn.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)
torch.nn.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.kl_div(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)
torch.nn.leaky_relu(input,negative_slope=0.01,inplace=False)
torch.nn.linear(input,weight,bias=None)
torch.nn.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.log_softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')
torch.nn.max_pool1d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_pool2d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_pool3d_with_indices(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)
torch.nn.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.multi_head_attention_forward(query,key,value,embed_dim_to_check,num_heads,in_proj_weight,in_proj_bias,bias_k,bias_v,add_zero_attn,dropout_p,out_proj_weight,out_proj_bias,training=True,key_padding_mask=None,need_weights=True,attn_mask=None,use_separate_proj_weight=False,q_proj_weight=None,k_proj_weight=None,v_proj_weight=None,static_k=None,static_v=None)
torch.nn.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.multilabel_soft_margin_loss(input,target,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.normalize(input,p=2,dim=1,eps=1e-12,out=None)
torch.nn.pad(input,pad,mode='constant',value=0)
torch.nn.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)
torch.nn.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.prelu(input,weight)
torch.nn.relu(input,inplace=False)
torch.nn.relu6(input,inplace=False)
torch.nn.rrelu(input,lower=1.0/8,upper=1.0/3,training=False,inplace=False)
torch.nn.selu(input,inplace=False)
torch.nn.sigmoid(input)
torch.nn.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')
torch.nn.softmax(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.softmin(input,dim=None,_stacklevel=3,dtype=None)
torch.nn.softsign(input)
torch.nn.tanh(input)
torch.nn.tanhshrink(input)
torch.nn.threshold(input,threshold,value,inplace=False)
torch.nn.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.unfold(input,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/functional.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/__init__.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/_reduction.py----------------------------------------
torch.nn._reduction.get_enum(reduction)
torch.nn._reduction.legacy_get_enum(size_average,reduce,emit_warning=True)
torch.nn._reduction.legacy_get_string(size_average,reduce,emit_warning=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/init.py----------------------------------------
A:torch.nn.init.dimensions->tensor.dim()
A:torch.nn.init.sizes->tensor.size()
A:torch.nn.init.min_dim->min(sizes[0], sizes[1])
A:torch.nn.init.fan_in->tensor.size(1)
A:torch.nn.init.fan_out->tensor.size(0)
A:torch.nn.init.num_input_fmaps->tensor.size(1)
A:torch.nn.init.num_output_fmaps->tensor.size(0)
A:torch.nn.init.receptive_field_size->tensor[0][0].numel()
A:torch.nn.init.(fan_in, fan_out)->_calculate_fan_in_and_fan_out(tensor)
A:torch.nn.init.mode->mode.lower().lower()
A:torch.nn.init.fan->_calculate_correct_fan(tensor, mode)
A:torch.nn.init.gain->calculate_gain(nonlinearity, a)
A:torch.nn.init.rows->tensor.size(0)
A:torch.nn.init.flattened->tensor.new(rows, cols).normal_(0, 1)
A:torch.nn.init.(q, r)->torch.qr(flattened)
A:torch.nn.init.d->torch.diag(r, 0)
A:torch.nn.init.ph->torch.diag(r, 0).sign()
A:torch.nn.init.num_zeros->int(math.ceil(sparsity * rows))
A:torch.nn.init.row_indices->torch.randperm(rows)
A:torch.nn.init.deprecated_init.__doc__->'\n    {old_name}(...)\n\n    .. warning::\n        This method is now deprecated in favor of :func:`torch.nn.init.{new_name}`.\n\n    See :func:`~torch.nn.init.{new_name}` for details.'.format(old_name=old_name, new_name=new_name)
A:torch.nn.init.uniform->_make_deprecate(uniform_)
A:torch.nn.init.normal->_make_deprecate(normal_)
A:torch.nn.init.constant->_make_deprecate(constant_)
A:torch.nn.init.eye->_make_deprecate(eye_)
A:torch.nn.init.dirac->_make_deprecate(dirac_)
A:torch.nn.init.xavier_uniform->_make_deprecate(xavier_uniform_)
A:torch.nn.init.xavier_normal->_make_deprecate(xavier_normal_)
A:torch.nn.init.kaiming_uniform->_make_deprecate(kaiming_uniform_)
A:torch.nn.init.kaiming_normal->_make_deprecate(kaiming_normal_)
A:torch.nn.init.orthogonal->_make_deprecate(orthogonal_)
A:torch.nn.init.sparse->_make_deprecate(sparse_)
torch.nn.init._calculate_correct_fan(tensor,mode)
torch.nn.init._calculate_fan_in_and_fan_out(tensor)
torch.nn.init._make_deprecate(meth)
torch.nn.init._no_grad_fill_(tensor,val)
torch.nn.init._no_grad_normal_(tensor,mean,std)
torch.nn.init._no_grad_uniform_(tensor,a,b)
torch.nn.init._no_grad_zero_(tensor)
torch.nn.init.calculate_gain(nonlinearity,param=None)
torch.nn.init.constant_(tensor,val)
torch.nn.init.dirac_(tensor)
torch.nn.init.eye_(tensor)
torch.nn.init.kaiming_normal_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.kaiming_uniform_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')
torch.nn.init.normal_(tensor,mean=0.0,std=1.0)
torch.nn.init.ones_(tensor)
torch.nn.init.orthogonal_(tensor,gain=1)
torch.nn.init.sparse_(tensor,sparsity,std=0.01)
torch.nn.init.uniform_(tensor,a=0.0,b=1.0)
torch.nn.init.xavier_normal_(tensor,gain=1.0)
torch.nn.init.xavier_uniform_(tensor,gain=1.0)
torch.nn.init.zeros_(tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/_VF.py----------------------------------------
A:torch.nn._VF.sys.modules[__name__]->VFModule(__name__)
torch.nn._VF.VFModule(self,name)
torch.nn._VF.VFModule.__getattr__(self,attr)
torch.nn._VF.VFModule.__init__(self,name)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/cpp.py----------------------------------------
A:torch.nn.cpp.self._parameters->OrderedDictWrapper(cpp_module, '_parameters')
A:torch.nn.cpp.self._buffers->OrderedDictWrapper(cpp_module, '_buffers')
A:torch.nn.cpp.self._modules->OrderedDictWrapper(cpp_module, '_modules')
A:torch.nn.cpp.param.data->fn(param.data)
A:torch.nn.cpp.param._grad.data->fn(param._grad.data)
A:torch.nn.cpp.buf.data->fn(buf.data)
torch.nn.cpp.ModuleWrapper(self,cpp_module)
torch.nn.cpp.ModuleWrapper.__init__(self,cpp_module)
torch.nn.cpp.ModuleWrapper.__repr__(self)
torch.nn.cpp.ModuleWrapper._apply(self,fn)
torch.nn.cpp.ModuleWrapper.training(self)
torch.nn.cpp.ModuleWrapper.training(self,mode)
torch.nn.cpp.OrderedDictWrapper(self,cpp_module,attr)
torch.nn.cpp.OrderedDictWrapper.__contains__(self,key)
torch.nn.cpp.OrderedDictWrapper.__getitem__(self,key)
torch.nn.cpp.OrderedDictWrapper.__init__(self,cpp_module,attr)
torch.nn.cpp.OrderedDictWrapper.__iter__(self)
torch.nn.cpp.OrderedDictWrapper.__len__(self)
torch.nn.cpp.OrderedDictWrapper.cpp_dict(self)
torch.nn.cpp.OrderedDictWrapper.items(self)
torch.nn.cpp.OrderedDictWrapper.keys(self)
torch.nn.cpp.OrderedDictWrapper.values(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parameter.py----------------------------------------
A:torch.nn.parameter.data->torch.Tensor()
A:torch.nn.parameter.result->type(self)(self.data.clone(), self.requires_grad)
torch.nn.Parameter(cls,data=None,requires_grad=True)
torch.nn.Parameter.__deepcopy__(self,memo)
torch.nn.Parameter.__reduce_ex__(self,proto)
torch.nn.Parameter.__repr__(self)
torch.nn.parameter.Parameter(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__deepcopy__(self,memo)
torch.nn.parameter.Parameter.__new__(cls,data=None,requires_grad=True)
torch.nn.parameter.Parameter.__reduce_ex__(self,proto)
torch.nn.parameter.Parameter.__repr__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parameter.pyi----------------------------------------
torch.nn.parameter.Parameter.__init__(self,data:Tensor,requires_grad:builtins.bool)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/grad.py----------------------------------------
A:torch.nn.grad.input_size->list(input_size)
A:torch.nn.grad.stride->_triple(stride)
A:torch.nn.grad.padding->_triple(padding)
A:torch.nn.grad.dilation->_triple(dilation)
A:torch.nn.grad.grad_input_padding->_grad_input_padding(grad_output, input_size, stride, padding, kernel_size)
A:torch.nn.grad.grad_output->grad_output.contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4]).contiguous().view(grad_output.shape[0] * grad_output.shape[1], 1, grad_output.shape[2], grad_output.shape[3], grad_output.shape[4])
A:torch.nn.grad.input->input.contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4]).contiguous().view(1, input.shape[0] * input.shape[1], input.shape[2], input.shape[3], input.shape[4])
A:torch.nn.grad.grad_weight->grad_weight.contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4]).contiguous().view(min_batch, grad_weight.shape[1] // min_batch, grad_weight.shape[2], grad_weight.shape[3], grad_weight.shape[4])
torch.nn.grad._grad_input_padding(grad_output,input_size,stride,padding,kernel_size)
torch.nn.grad.conv1d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv1d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv2d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_input(input_size,weight,grad_output,stride=1,padding=0,dilation=1,groups=1)
torch.nn.grad.conv3d_weight(input,weight_size,grad_output,stride=1,padding=0,dilation=1,groups=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/common_types.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/backends/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/backends/thnn.py----------------------------------------
torch.nn.backends.thnn._get_thnn_function_backend()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/functional.py----------------------------------------
A:torch.nn.quantized.functional.stride->torch.jit.annotate(_List[int], [])
A:torch.nn.quantized.functional.padding->_pair(padding)
A:torch.nn.quantized.functional.dilation->_pair(dilation)
A:torch.nn.quantized.functional.prepacked_weight->torch.ops.quantized.conv_prepack(weight, bias, stride, padding, dilation, groups)
A:torch.nn.quantized.functional.scale->input.q_scale()
A:torch.nn.quantized.functional.zero_point->input.q_zero_point()
A:torch.nn.quantized.functional._packed_params->torch.ops.quantized.linear_prepack(weight, bias)
torch.nn.quantized.adaptive_avg_pool2d(input,output_size)
torch.nn.quantized.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.adaptive_avg_pool2d(input,output_size)
torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)
torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.functional.linear(input,weight,bias=None,scale=None,zero_point=None)
torch.nn.quantized.functional.max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.quantized.functional.relu(input,inplace=False)
torch.nn.quantized.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.functional.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.quantized.functional.upsample_nearest(input,size=None,scale_factor=None)
torch.nn.quantized.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.linear(input,weight,bias=None,scale=None,zero_point=None)
torch.nn.quantized.max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)
torch.nn.quantized.relu(input,inplace=False)
torch.nn.quantized.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.quantized.upsample_bilinear(input,size=None,scale_factor=None)
torch.nn.quantized.upsample_nearest(input,size=None,scale_factor=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/dynamic/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/dynamic/modules/rnn.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.rnn.self.dropout->float(dropout)
A:torch.nn.quantized.dynamic.modules.rnn.packed_weight->torch.fbgemm_pack_gemm_matrix_fp16(weight.float())
A:torch.nn.quantized.dynamic.modules.rnn.w_ih->torch.Tensor(gate_size, layer_input_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.w_hh->torch.Tensor(gate_size, hidden_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.b_ih->torch.Tensor(gate_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.b_hh->torch.Tensor(gate_size).float()
A:torch.nn.quantized.dynamic.modules.rnn.(ih_params, ih_param_names)->process_weights('ih', layer, suffix, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.(hh_params, hh_param_names)->process_weights('hh', layer, suffix, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.mini_batch->int(mini_batch)
A:torch.nn.quantized.dynamic.modules.rnn.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.nn.quantized.dynamic.modules.rnn.dynamic_vals->torch.jit.annotate(List[Tuple[torch.Tensor, Optional[torch.Tensor]]], [])
A:torch.nn.quantized.dynamic.modules.rnn.weight_observer->torch.quantization.QConfig.default_dynamic_qconfig.weight()
A:torch.nn.quantized.dynamic.modules.rnn.qRNNBase->LSTM(mod.input_size, mod.hidden_size, mod.num_layers, mod.bias, mod.batch_first, mod.dropout, mod.bidirectional, dtype)
A:torch.nn.quantized.dynamic.modules.rnn.weight_name->'weight_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.nn.quantized.dynamic.modules.rnn.bias_name->'bias_{}_l{}{}'.format(ihhh, layer, suffix)
A:torch.nn.quantized.dynamic.modules.rnn.weight->getattr(mod, weight_name)
A:torch.nn.quantized.dynamic.modules.rnn.bias->getattr(mod, bias_name)
A:torch.nn.quantized.dynamic.modules.rnn.(wt_scale, wt_zp)->torch.quantization.QConfig.default_dynamic_qconfig.weight().calculate_qparams()
A:torch.nn.quantized.dynamic.modules.rnn.qweight->torch.quantize_per_tensor(weight.float(), float(wt_scale), int(wt_zp), torch.qint8)
A:torch.nn.quantized.dynamic.modules.rnn.zeros->torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.quantized.dynamic.modules.rnn.hx->self.permute_hidden(hx, sorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.result->torch.nn._VF.quantized_lstm(input, hx, self._all_weight_values, self.bias, self.num_layers, float(self.dropout), self.training, self.bidirectional, self.batch_first, dtype=self.dtype, use_dynamic=True)
A:torch.nn.quantized.dynamic.modules.rnn.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.nn.quantized.dynamic.modules.rnn.max_batch_size->int(max_batch_size)
A:torch.nn.quantized.dynamic.modules.rnn.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
torch.nn.quantized.dynamic.LSTM(self,*args,**kwargs)
torch.nn.quantized.dynamic.LSTM._get_name(self)
torch.nn.quantized.dynamic.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.LSTM.forward(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.quantized.dynamic.LSTM.forward_packed(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.forward_tensor(self,input,hx=None)
torch.nn.quantized.dynamic.LSTM.from_float(cls,mod)
torch.nn.quantized.dynamic.LSTM.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.quantized.dynamic.modules.rnn.LSTM._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_packed(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.forward_tensor(self,input,hx=None)
torch.nn.quantized.dynamic.modules.rnn.LSTM.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.LSTM.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.modules.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__getstate__(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__init__(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.__setstate__(self,state)
torch.nn.quantized.dynamic.modules.rnn.RNNBase._get_name(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.quantized.dynamic.modules.rnn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.extra_repr(self)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.quantized.dynamic.modules.rnn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.modules.rnn.apply_permutation(tensor,permutation,dim=1)
torch.nn.quantized.dynamic.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,dtype=torch.qint8)
torch.nn.quantized.dynamic.rnn.RNNBase.__getstate__(self)
torch.nn.quantized.dynamic.rnn.RNNBase.__setstate__(self,state)
torch.nn.quantized.dynamic.rnn.RNNBase._get_name(self)
torch.nn.quantized.dynamic.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.quantized.dynamic.rnn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.quantized.dynamic.rnn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.quantized.dynamic.rnn.RNNBase.extra_repr(self)
torch.nn.quantized.dynamic.rnn.RNNBase.from_float(cls,mod)
torch.nn.quantized.dynamic.rnn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.quantized.dynamic.rnn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.quantized.dynamic.rnn.apply_permutation(tensor,permutation,dim=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/dynamic/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/dynamic/modules/linear.py----------------------------------------
A:torch.nn.quantized.dynamic.modules.linear.Y->torch.ops.quantized.linear_dynamic(x, self._packed_params)
A:torch.nn.quantized.dynamic.modules.linear.weight_observer->torch.quantization.QConfig.default_dynamic_qconfig.weight()
A:torch.nn.quantized.dynamic.modules.linear.(wt_scale, wt_zp)->torch.quantization.QConfig.default_dynamic_qconfig.weight().calculate_qparams()
A:torch.nn.quantized.dynamic.modules.linear.qweight->torch.quantize_per_tensor(mod.weight.float(), float(wt_scale), int(wt_zp), torch.qint8)
A:torch.nn.quantized.dynamic.modules.linear.qlinear->Linear(mod.in_features, mod.out_features)
torch.nn.quantized.dynamic.Linear(self,in_features,out_features,bias_=True)
torch.nn.quantized.dynamic.Linear._get_name(self)
torch.nn.quantized.dynamic.Linear.forward(self,x)
torch.nn.quantized.dynamic.Linear.from_float(cls,mod)
torch.nn.quantized.dynamic.modules.linear.Linear(self,in_features,out_features,bias_=True)
torch.nn.quantized.dynamic.modules.linear.Linear.__init__(self,in_features,out_features,bias_=True)
torch.nn.quantized.dynamic.modules.linear.Linear._get_name(self)
torch.nn.quantized.dynamic.modules.linear.Linear.forward(self,x)
torch.nn.quantized.dynamic.modules.linear.Linear.from_float(cls,mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/modules/utils.py----------------------------------------
A:torch.nn.quantized.modules.utils.(wt_scale, wt_zp)->observer.calculate_qparams()
A:torch.nn.quantized.modules.utils.qweight->torch.quantize_per_channel(float_wt, wt_scale.to(torch.double), wt_zp.to(torch.int64), 0, torch.qint8)
torch.nn.quantized.modules.utils._quantize_weight(float_wt,observer)
torch.nn.quantized.utils._quantize_weight(float_wt,observer)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/modules/functional_modules.py----------------------------------------
A:torch.nn.quantized.modules.functional_modules.self.observer->torch.nn.Identity()
A:torch.nn.quantized.modules.functional_modules.r->self.observer(r)
A:torch.nn.quantized.modules.functional_modules.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.functional_modules.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.functional_modules.self.scale->float(state_dict.pop(prefix + 'scale'))
A:torch.nn.quantized.modules.functional_modules.self.zero_point->int(state_dict.pop(prefix + 'zero_point'))
A:torch.nn.quantized.modules.functional_modules.(scale, zero_point)->mod.observer.calculate_qparams()
A:torch.nn.quantized.modules.functional_modules.new_mod->QFunctional()
A:torch.nn.quantized.modules.functional_modules.new_mod.scale->float(scale)
A:torch.nn.quantized.modules.functional_modules.new_mod.zero_point->int(zero_point)
torch.nn.quantized.FloatFunctional(self)
torch.nn.quantized.FloatFunctional.add(self,x,y)
torch.nn.quantized.FloatFunctional.add_relu(self,x,y)
torch.nn.quantized.FloatFunctional.add_scalar(self,x,y)
torch.nn.quantized.FloatFunctional.cat(self,x,dim=0)
torch.nn.quantized.FloatFunctional.forward(self,x)
torch.nn.quantized.FloatFunctional.mul(self,x,y)
torch.nn.quantized.FloatFunctional.mul_scalar(self,x,y)
torch.nn.quantized.QFunctional(self)
torch.nn.quantized.QFunctional._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.QFunctional._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.QFunctional.add(self,x,y)
torch.nn.quantized.QFunctional.add_relu(self,x,y)
torch.nn.quantized.QFunctional.add_scalar(self,x,y)
torch.nn.quantized.QFunctional.cat(self,x,dim=0)
torch.nn.quantized.QFunctional.forward(self,x)
torch.nn.quantized.QFunctional.from_float(cls,mod)
torch.nn.quantized.QFunctional.mul(self,x,y)
torch.nn.quantized.QFunctional.mul_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional(self)
torch.nn.quantized.modules.functional_modules.FloatFunctional.__init__(self)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add_relu(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.add_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.cat(self,x,dim=0)
torch.nn.quantized.modules.functional_modules.FloatFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.FloatFunctional.mul(self,x,y)
torch.nn.quantized.modules.functional_modules.FloatFunctional.mul_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional(self)
torch.nn.quantized.modules.functional_modules.QFunctional.__init__(self)
torch.nn.quantized.modules.functional_modules.QFunctional._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.functional_modules.QFunctional._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.functional_modules.QFunctional.add(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.add_relu(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.add_scalar(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.cat(self,x,dim=0)
torch.nn.quantized.modules.functional_modules.QFunctional.forward(self,x)
torch.nn.quantized.modules.functional_modules.QFunctional.from_float(cls,mod)
torch.nn.quantized.modules.functional_modules.QFunctional.mul(self,x,y)
torch.nn.quantized.modules.functional_modules.QFunctional.mul_scalar(self,x,y)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/modules/linear.py----------------------------------------
A:torch.nn.quantized.modules.linear.(scale, zero_point)->mod.observer.calculate_qparams()
A:torch.nn.quantized.modules.linear.bias->torch.zeros(out_features, dtype=torch.float)
A:torch.nn.quantized.modules.linear.qweight->_quantize_weight(mod.weight.float(), weight_observer)
A:torch.nn.quantized.modules.linear.(w, b)->torch.ops.quantized.linear_unpack(self._packed_params)
A:torch.nn.quantized.modules.linear.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.linear.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.linear.self.scale->float(state_dict[prefix + 'scale'])
A:torch.nn.quantized.modules.linear.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.nn.quantized.modules.linear.self._packed_params->torch.ops.quantized.linear_prepack(w, b)
A:torch.nn.quantized.modules.linear.weight_observer->mod.qconfig.weight()
A:torch.nn.quantized.modules.linear.(act_scale, act_zp)->activation_observer.calculate_qparams()
A:torch.nn.quantized.modules.linear.qlinear->cls(mod.in_features, mod.out_features)
A:torch.nn.quantized.modules.linear.qlinear.scale->float(act_scale)
A:torch.nn.quantized.modules.linear.qlinear.zero_point->int(act_zp)
torch.nn.quantized.DeQuantize(self)
torch.nn.quantized.DeQuantize.forward(self,Xq)
torch.nn.quantized.DeQuantize.from_float(mod)
torch.nn.quantized.Linear(self,in_features,out_features,bias_=True)
torch.nn.quantized.Linear.__getstate__(self)
torch.nn.quantized.Linear.__setstate__(self,state)
torch.nn.quantized.Linear._get_name(self)
torch.nn.quantized.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.Linear._weight_bias(self)
torch.nn.quantized.Linear.bias(self)
torch.nn.quantized.Linear.extra_repr(self)
torch.nn.quantized.Linear.forward(self,x)
torch.nn.quantized.Linear.from_float(cls,mod)
torch.nn.quantized.Linear.set_weight_bias(self,w,b)
torch.nn.quantized.Linear.weight(self)
torch.nn.quantized.Quantize(self,scale,zero_point,dtype)
torch.nn.quantized.Quantize.extra_repr(self)
torch.nn.quantized.Quantize.forward(self,X)
torch.nn.quantized.Quantize.from_float(mod)
torch.nn.quantized.modules.linear.DeQuantize(self)
torch.nn.quantized.modules.linear.DeQuantize.__init__(self)
torch.nn.quantized.modules.linear.DeQuantize.forward(self,Xq)
torch.nn.quantized.modules.linear.DeQuantize.from_float(mod)
torch.nn.quantized.modules.linear.Linear(self,in_features,out_features,bias_=True)
torch.nn.quantized.modules.linear.Linear.__getstate__(self)
torch.nn.quantized.modules.linear.Linear.__init__(self,in_features,out_features,bias_=True)
torch.nn.quantized.modules.linear.Linear.__setstate__(self,state)
torch.nn.quantized.modules.linear.Linear._get_name(self)
torch.nn.quantized.modules.linear.Linear._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.linear.Linear._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.linear.Linear._weight_bias(self)
torch.nn.quantized.modules.linear.Linear.bias(self)
torch.nn.quantized.modules.linear.Linear.extra_repr(self)
torch.nn.quantized.modules.linear.Linear.forward(self,x)
torch.nn.quantized.modules.linear.Linear.from_float(cls,mod)
torch.nn.quantized.modules.linear.Linear.set_weight_bias(self,w,b)
torch.nn.quantized.modules.linear.Linear.weight(self)
torch.nn.quantized.modules.linear.Quantize(self,scale,zero_point,dtype)
torch.nn.quantized.modules.linear.Quantize.__init__(self,scale,zero_point,dtype)
torch.nn.quantized.modules.linear.Quantize.extra_repr(self)
torch.nn.quantized.modules.linear.Quantize.forward(self,X)
torch.nn.quantized.modules.linear.Quantize.from_float(mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/modules/conv.py----------------------------------------
A:torch.nn.quantized.modules.conv.self.kernel_size->_pair(kernel_size)
A:torch.nn.quantized.modules.conv.self.stride->_pair(stride)
A:torch.nn.quantized.modules.conv.self.padding->_pair(padding)
A:torch.nn.quantized.modules.conv.self.dilation->_pair(dilation)
A:torch.nn.quantized.modules.conv.qweight->_quantize_weight(mod.weight.float(), weight_observer)
A:torch.nn.quantized.modules.conv.bias_float->torch.zeros(out_channels, dtype=torch.float)
A:torch.nn.quantized.modules.conv.self._packed_params->torch.ops.quantized.conv_prepack(w, b, self.stride, self.padding, self.dilation, self.groups)
A:torch.nn.quantized.modules.conv.(w, b)->self._weight_bias()
A:torch.nn.quantized.modules.conv.destination[prefix + 'scale']->torch.tensor(self.scale)
A:torch.nn.quantized.modules.conv.destination[prefix + 'zero_point']->torch.tensor(self.zero_point)
A:torch.nn.quantized.modules.conv.self.scale->float(state_dict[prefix + 'scale'])
A:torch.nn.quantized.modules.conv.self.zero_point->int(state_dict[prefix + 'zero_point'])
A:torch.nn.quantized.modules.conv.(mod.weight, mod.bias)->fuse_conv_bn_weights(mod.weight, mod.bias, mod.running_mean, mod.running_var, mod.eps, mod.gamma, mod.beta)
A:torch.nn.quantized.modules.conv.weight_observer->mod.qconfig.weight()
A:torch.nn.quantized.modules.conv.(act_scale, act_zp)->activation_observer.calculate_qparams()
A:torch.nn.quantized.modules.conv.qconv->cls(mod.in_channels, mod.out_channels, mod.kernel_size, mod.stride, mod.padding, mod.dilation, mod.groups, mod.bias is not None, mod.padding_mode)
A:torch.nn.quantized.modules.conv.qconv.scale->float(act_scale)
A:torch.nn.quantized.modules.conv.qconv.zero_point->int(act_zp)
torch.nn.quantized.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.Conv2d.__getstate__(self)
torch.nn.quantized.Conv2d.__setstate__(self,state)
torch.nn.quantized.Conv2d._get_name(self)
torch.nn.quantized.Conv2d._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.Conv2d._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.Conv2d._weight_bias(self)
torch.nn.quantized.Conv2d.bias(self)
torch.nn.quantized.Conv2d.extra_repr(self)
torch.nn.quantized.Conv2d.forward(self,input)
torch.nn.quantized.Conv2d.from_float(cls,mod)
torch.nn.quantized.Conv2d.set_weight_bias(self,w,b)
torch.nn.quantized.Conv2d.weight(self)
torch.nn.quantized.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv.Conv2d.__getstate__(self)
torch.nn.quantized.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.quantized.modules.conv.Conv2d.__setstate__(self,state)
torch.nn.quantized.modules.conv.Conv2d._get_name(self)
torch.nn.quantized.modules.conv.Conv2d._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.quantized.modules.conv.Conv2d._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.quantized.modules.conv.Conv2d._weight_bias(self)
torch.nn.quantized.modules.conv.Conv2d.bias(self)
torch.nn.quantized.modules.conv.Conv2d.extra_repr(self)
torch.nn.quantized.modules.conv.Conv2d.forward(self,input)
torch.nn.quantized.modules.conv.Conv2d.from_float(cls,mod)
torch.nn.quantized.modules.conv.Conv2d.set_weight_bias(self,w,b)
torch.nn.quantized.modules.conv.Conv2d.weight(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/quantized/modules/activation.py----------------------------------------
torch.nn.quantized.ReLU(self,inplace=False)
torch.nn.quantized.ReLU._get_name(self)
torch.nn.quantized.ReLU.forward(self,input)
torch.nn.quantized.ReLU.from_float(mod)
torch.nn.quantized.ReLU6(self,inplace=False)
torch.nn.quantized.ReLU6._get_name(self)
torch.nn.quantized.ReLU6.forward(self,input)
torch.nn.quantized.ReLU6.from_float(mod)
torch.nn.quantized.modules.activation.ReLU(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU.__init__(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU._get_name(self)
torch.nn.quantized.modules.activation.ReLU.forward(self,input)
torch.nn.quantized.modules.activation.ReLU.from_float(mod)
torch.nn.quantized.modules.activation.ReLU6(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU6.__init__(self,inplace=False)
torch.nn.quantized.modules.activation.ReLU6._get_name(self)
torch.nn.quantized.modules.activation.ReLU6.forward(self,input)
torch.nn.quantized.modules.activation.ReLU6.from_float(mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/quantized/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/quantized/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/quantized/modules/conv_relu.py----------------------------------------
A:torch.nn.intrinsic.quantized.modules.conv_relu.(mod.weight, mod.bias)->fuse_conv_bn_weights(mod.weight, mod.bias, mod.running_mean, mod.running_var, mod.eps, mod.gamma, mod.beta)
torch.nn.intrinsic.quantized.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.ConvReLU2d._get_name(self)
torch.nn.intrinsic.quantized.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.ConvReLU2d.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d._get_name(self)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.from_float(cls,mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/quantized/modules/linear_relu.py----------------------------------------
A:torch.nn.intrinsic.quantized.modules.linear_relu.Y_q->torch.ops.quantized.linear_relu(input, self._packed_params, float(self.scale), int(self.zero_point))
torch.nn.intrinsic.quantized.LinearReLU(self,in_features,out_features,bias=True)
torch.nn.intrinsic.quantized.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.LinearReLU.forward(self,input)
torch.nn.intrinsic.quantized.LinearReLU.from_float(cls,mod)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU._get_name(self)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.forward(self,input)
torch.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.from_float(cls,mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/qat/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/qat/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py----------------------------------------
A:torch.nn.intrinsic.qat.modules.conv_fused.self.gamma->torch.nn.Parameter(torch.Tensor(out_channels))
A:torch.nn.intrinsic.qat.modules.conv_fused.self.beta->torch.nn.Parameter(torch.Tensor(out_channels))
A:torch.nn.intrinsic.qat.modules.conv_fused.self.observer->self.qconfig.activation()
A:torch.nn.intrinsic.qat.modules.conv_fused.self.weight_fake_quant->self.qconfig.weight()
A:torch.nn.intrinsic.qat.modules.conv_fused.running_std->torch.sqrt(self.running_var + self.eps)
A:torch.nn.intrinsic.qat.modules.conv_fused.conv->self.conv2d_forward(input, self.weight_fake_quant(scaled_weight))
A:torch.nn.intrinsic.qat.modules.conv_fused.batch_mean->torch.mean(conv_orig, dim=[0, 2, 3])
A:torch.nn.intrinsic.qat.modules.conv_fused.batch_var->torch.var(conv_orig, dim=[0, 2, 3], unbiased=False)
A:torch.nn.intrinsic.qat.modules.conv_fused.n->float(conv_orig.numel() / conv_orig.size()[1])
A:torch.nn.intrinsic.qat.modules.conv_fused.qat_convbn->cls(conv.in_channels, conv.out_channels, conv.kernel_size, conv.stride, conv.padding, conv.dilation, conv.groups, conv.padding_mode, bn.eps, bn.momentum, False, qconfig)
torch.nn.intrinsic.qat.ConvBn2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBn2d._forward(self,input)
torch.nn.intrinsic.qat.ConvBn2d.extra_repr(self)
torch.nn.intrinsic.qat.ConvBn2d.forward(self,input)
torch.nn.intrinsic.qat.ConvBn2d.freeze_bn_stats(self)
torch.nn.intrinsic.qat.ConvBn2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.ConvBn2d.reset_bn_parameters(self)
torch.nn.intrinsic.qat.ConvBn2d.reset_parameters(self)
torch.nn.intrinsic.qat.ConvBn2d.reset_running_stats(self)
torch.nn.intrinsic.qat.ConvBn2d.update_bn_stats(self)
torch.nn.intrinsic.qat.ConvBnReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.ConvBnReLU2d.forward(self,input)
torch.nn.intrinsic.qat.ConvBnReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.qat.ConvReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.freeze_bn_stats(mod)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d._forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.extra_repr(self)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.freeze_bn_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.reset_bn_parameters(self)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.reset_parameters(self)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.reset_running_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBn2d.update_bn_stats(self)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.forward(self,input)
torch.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.conv_fused.freeze_bn_stats(mod)
torch.nn.intrinsic.qat.modules.conv_fused.update_bn_stats(mod)
torch.nn.intrinsic.qat.update_bn_stats(mod)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/qat/modules/linear_relu.py----------------------------------------
torch.nn.intrinsic.qat.LinearReLU(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.LinearReLU.forward(self,input)
torch.nn.intrinsic.qat.LinearReLU.from_float(cls,mod,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.__init__(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.forward(self,input)
torch.nn.intrinsic.qat.modules.linear_relu.LinearReLU.from_float(cls,mod,qconfig=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/intrinsic/modules/fused.py----------------------------------------
torch.nn.intrinsic.ConvBn2d(self,conv,bn)
torch.nn.intrinsic.ConvBnReLU2d(self,conv,bn,relu)
torch.nn.intrinsic.ConvReLU2d(self,conv,relu)
torch.nn.intrinsic.LinearReLU(self,linear,relu)
torch.nn.intrinsic.modules.fused.ConvBn2d(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBn2d.__init__(self,conv,bn)
torch.nn.intrinsic.modules.fused.ConvBnReLU2d(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvBnReLU2d.__init__(self,conv,bn,relu)
torch.nn.intrinsic.modules.fused.ConvReLU2d(self,conv,relu)
torch.nn.intrinsic.modules.fused.ConvReLU2d.__init__(self,conv,relu)
torch.nn.intrinsic.modules.fused.LinearReLU(self,linear,relu)
torch.nn.intrinsic.modules.fused.LinearReLU.__init__(self,linear,relu)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/__init__.py----------------------------------------
torch.nn.parallel.__init__.DistributedDataParallelCPU(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/__init__.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py----------------------------------------
A:torch.nn.parallel.data_parallel.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.data_parallel.(min_pos, min_val)->min(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.(max_pos, max_val)->max(enumerate(values), key=operator.itemgetter(1))
A:torch.nn.parallel.data_parallel.self.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.data_parallel.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.self.src_device_obj->torch.device('cuda:{}'.format(self.device_ids[0]))
A:torch.nn.parallel.data_parallel.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.data_parallel.replicas->replicate(module, used_device_ids)
A:torch.nn.parallel.data_parallel.outputs->parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
A:torch.nn.parallel.data_parallel.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.data_parallel.src_device_obj->torch.device('cuda:{}'.format(device_ids[0]))
A:torch.nn.parallel.data_parallel.(inputs, module_kwargs)->scatter_kwargs(inputs, module_kwargs, device_ids, dim)
torch.nn.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.DataParallel.gather(self,outputs,output_device)
torch.nn.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.DataParallel.replicate(self,module,device_ids)
torch.nn.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)
torch.nn.parallel.data_parallel.DataParallel(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0)
torch.nn.parallel.data_parallel.DataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.data_parallel.DataParallel.gather(self,outputs,output_device)
torch.nn.parallel.data_parallel.DataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.data_parallel.DataParallel.replicate(self,module,device_ids)
torch.nn.parallel.data_parallel.DataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.data_parallel._check_balance(device_ids)
torch.nn.parallel.data_parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/replicate.py----------------------------------------
A:torch.nn.parallel.replicate.gen->module.modules()
A:torch.nn.parallel.replicate.memo->set()
A:torch.nn.parallel.replicate.tensor_copies->_functions.Broadcast.apply(devices, *tensors)
A:torch.nn.parallel.replicate.devices->list(map(lambda x: _get_device_index(x, True), devices))
A:torch.nn.parallel.replicate.num_replicas->len(devices)
A:torch.nn.parallel.replicate.params->list(network.parameters())
A:torch.nn.parallel.replicate.param_copies->_broadcast_coalesced_reshape(params, devices, detach)
A:torch.nn.parallel.replicate.buffers->list(network.buffers())
A:torch.nn.parallel.replicate.buffer_copies_rg->_broadcast_coalesced_reshape(buffers_rg, devices, detach=detach)
A:torch.nn.parallel.replicate.buffer_copies_not_rg->_broadcast_coalesced_reshape(buffers_not_rg, devices, detach=True)
A:torch.nn.parallel.replicate.modules->list(network.modules())
A:torch.nn.parallel.replicate.replica->module.__new__(type(module))
A:torch.nn.parallel.replicate.attribute_names->set((entry[0] for entry in module._c._get_attributes()))
A:torch.nn.parallel.replicate.replica.__dict__->module.__dict__.copy()
A:torch.nn.parallel.replicate.replica._parameters->module.__new__(type(module))._parameters.copy()
A:torch.nn.parallel.replicate.replica._buffers->module.__new__(type(module))._buffers.copy()
A:torch.nn.parallel.replicate.replica._modules->module.__new__(type(module))._modules.copy()
torch.nn.parallel.replicate(network,devices,detach=False)
torch.nn.parallel.replicate._broadcast_coalesced_reshape(tensors,devices,detach=False)
torch.nn.parallel.replicate._copy_scriptmodule_methods(modules,module_copies,module_indices)
torch.nn.parallel.replicate._init_script_module()
torch.nn.parallel.replicate._is_jit_enabled()
torch.nn.parallel.replicate._is_script_method(module)
torch.nn.parallel.replicate._is_script_module(module)
torch.nn.parallel.replicate._replicatable_module(module,memo=None)
torch.nn.parallel.replicate.replicate(network,devices,detach=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/replicate.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/_functions.py----------------------------------------
A:torch.nn.parallel._functions.target_gpus->list(map(lambda x: _get_device_index(x, True), target_gpus))
A:torch.nn.parallel._functions.ctx.num_inputs->len(inputs)
A:torch.nn.parallel._functions.ctx.input_device->inputs[0].get_device()
A:torch.nn.parallel._functions.outputs->torch.cuda.comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
A:torch.nn.parallel._functions.target_device->_get_device_index(target_device, True)
A:torch.nn.parallel._functions.ctx.input_gpus->tuple(map(lambda i: i.get_device(), inputs))
A:torch.nn.parallel._functions.inputs->tuple((t.view(1) for t in inputs))
A:torch.nn.parallel._functions.ctx.input_sizes->tuple(map(lambda i: i.size(ctx.dim), inputs))
A:torch.nn.parallel._functions.scattered_grads->tuple((g[0] for g in scattered_grads))
A:torch.nn.parallel._functions.main_stream->torch.cuda.current_stream()
A:torch.nn.parallel._functions._streams[device]->torch.cuda.Stream(device)
torch.nn.parallel._functions.Broadcast(Function)
torch.nn.parallel._functions.Broadcast.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.Broadcast.forward(ctx,target_gpus,*inputs)
torch.nn.parallel._functions.Gather(Function)
torch.nn.parallel._functions.Gather.backward(ctx,grad_output)
torch.nn.parallel._functions.Gather.forward(ctx,target_device,dim,*inputs)
torch.nn.parallel._functions.ReduceAddCoalesced(Function)
torch.nn.parallel._functions.ReduceAddCoalesced.backward(ctx,*grad_outputs)
torch.nn.parallel._functions.ReduceAddCoalesced.forward(ctx,destination,num_inputs,*grads)
torch.nn.parallel._functions.Scatter(Function)
torch.nn.parallel._functions.Scatter.backward(ctx,*grad_output)
torch.nn.parallel._functions.Scatter.forward(ctx,target_gpus,chunk_sizes,dim,input)
torch.nn.parallel._functions._get_stream(device)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py----------------------------------------
A:torch.nn.parallel.scatter_gather.res->gather_map(outputs)
A:torch.nn.parallel.scatter_gather.inputs->tuple(inputs)
A:torch.nn.parallel.scatter_gather.kwargs->tuple(kwargs)
torch.nn.parallel.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.gather(outputs,target_device,dim=0)
torch.nn.parallel.scatter_gather.scatter(inputs,target_gpus,dim=0)
torch.nn.parallel.scatter_gather.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)
torch.nn.parallel.scatter_kwargs(inputs,kwargs,target_gpus,dim=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py----------------------------------------
A:torch.nn.parallel.parallel_apply.devices->list(map(lambda x: _get_device_index(x, True), devices))
A:torch.nn.parallel.parallel_apply.lock->threading.Lock()
A:torch.nn.parallel.parallel_apply.grad_enabled->torch.is_grad_enabled()
A:torch.nn.parallel.parallel_apply.device->get_a_var(input).get_device()
A:torch.nn.parallel.parallel_apply.output->module(*input, **kwargs)
A:torch.nn.parallel.parallel_apply.results[i]->ExceptionWrapper(where='in replica {} on device {}'.format(i, device))
torch.nn.parallel.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)
torch.nn.parallel.parallel_apply.get_a_var(obj)
torch.nn.parallel.parallel_apply.parallel_apply(modules,inputs,kwargs_tup=None,devices=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py----------------------------------------
A:torch.nn.parallel.distributed.self.is_cuda->all([p.device.type == 'cuda' for p in module.parameters()])
A:torch.nn.parallel.distributed.device_ids->list(range(torch.cuda.device_count()))
A:torch.nn.parallel.distributed.self.device_ids->list(map(lambda x: _get_device_index(x, True), device_ids))
A:torch.nn.parallel.distributed.self.output_device->_get_device_index(output_device, True)
A:torch.nn.parallel.distributed.self.process_group->_get_default_group()
A:torch.nn.parallel.distributed.self.broadcast_bucket_size->int(250 * MB)
A:torch.nn.parallel.distributed.self.bucket_bytes_cap->int(bucket_cap_mb * MB)
A:torch.nn.parallel.distributed.module_states->list(self.module.state_dict().values())
A:torch.nn.parallel.distributed.self._module_copies->replicate(self.module, self.device_ids, detach=True)
A:torch.nn.parallel.distributed.bucket_indices->torch.distributed._compute_bucket_assignment_by_size(parameters[0], [1024 * 1024, self.bucket_bytes_cap], expect_sparse_gradient[0])
A:torch.nn.parallel.distributed.self.reducer->torch.distributed.Reducer(parameters, list(reversed(bucket_indices)), self.process_group, expect_sparse_gradient)
A:torch.nn.parallel.distributed.attrs->copy.copy(self.__dict__)
A:torch.nn.parallel.distributed.(inputs, kwargs)->self.scatter(inputs, kwargs, self.device_ids)
A:torch.nn.parallel.distributed.output->self.module(*inputs, **kwargs)
A:torch.nn.parallel.distributed.outputs->self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
A:torch.nn.parallel.distributed.result->torch.cuda.comm.broadcast_coalesced(self.modules_buffers[0], self.device_ids, self.broadcast_bucket_size)
torch.nn.parallel.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)
torch.nn.parallel.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.DistributedDataParallel._ddp_init_helper(self)
torch.nn.parallel.DistributedDataParallel._distributed_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.DistributedDataParallel._passing_sync_batchnorm_handle(self,module_copies)
torch.nn.parallel.DistributedDataParallel._sync_params(self)
torch.nn.parallel.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.DistributedDataParallel.no_sync(self)
torch.nn.parallel.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.distributed.DistributedDataParallel(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)
torch.nn.parallel.distributed.DistributedDataParallel.__getstate__(self)
torch.nn.parallel.distributed.DistributedDataParallel.__init__(self,module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)
torch.nn.parallel.distributed.DistributedDataParallel.__setstate__(self,state)
torch.nn.parallel.distributed.DistributedDataParallel._check_default_group(self)
torch.nn.parallel.distributed.DistributedDataParallel._ddp_init_helper(self)
torch.nn.parallel.distributed.DistributedDataParallel._distributed_broadcast_coalesced(self,tensors,buffer_size)
torch.nn.parallel.distributed.DistributedDataParallel._passing_sync_batchnorm_handle(self,module_copies)
torch.nn.parallel.distributed.DistributedDataParallel._sync_params(self)
torch.nn.parallel.distributed.DistributedDataParallel.forward(self,*inputs,**kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.gather(self,outputs,output_device)
torch.nn.parallel.distributed.DistributedDataParallel.no_sync(self)
torch.nn.parallel.distributed.DistributedDataParallel.parallel_apply(self,replicas,inputs,kwargs)
torch.nn.parallel.distributed.DistributedDataParallel.scatter(self,inputs,kwargs,device_ids)
torch.nn.parallel.distributed.DistributedDataParallel.train(self,mode=True)
torch.nn.parallel.distributed._find_tensors(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/distributed.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/parallel/common_types.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/qat/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/qat/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/qat/modules/linear.py----------------------------------------
A:torch.nn.qat.modules.linear.self.observer->qconfig.activation()
A:torch.nn.qat.modules.linear.self.weight_fake_quant->qconfig.weight()
A:torch.nn.qat.modules.linear.qat_linear->cls(mod.in_features, mod.out_features, bias=mod.bias is not None, qconfig=qconfig)
torch.nn.qat.Linear(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.qat.Linear.forward(self,input)
torch.nn.qat.Linear.from_float(cls,mod,qconfig=None)
torch.nn.qat.modules.linear.Linear(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.qat.modules.linear.Linear.__init__(self,in_features,out_features,bias=True,qconfig=None)
torch.nn.qat.modules.linear.Linear.forward(self,input)
torch.nn.qat.modules.linear.Linear.from_float(cls,mod,qconfig=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/qat/modules/conv.py----------------------------------------
A:torch.nn.qat.modules.conv.self.observer->qconfig.activation()
A:torch.nn.qat.modules.conv.self.weight_fake_quant->qconfig.weight()
A:torch.nn.qat.modules.conv.qat_conv->cls(mod.in_channels, mod.out_channels, mod.kernel_size, stride=mod.stride, padding=mod.padding, dilation=mod.dilation, groups=mod.groups, bias=mod.bias is not None, padding_mode=mod.padding_mode, qconfig=qconfig)
torch.nn.qat.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.qat.Conv2d.forward(self,input)
torch.nn.qat.Conv2d.from_float(cls,mod,qconfig=None)
torch.nn.qat.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.qat.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)
torch.nn.qat.modules.conv.Conv2d.forward(self,input)
torch.nn.qat.modules.conv.Conv2d.from_float(cls,mod,qconfig=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/rnn.py----------------------------------------
A:torch.nn.utils.rnn.PackedSequence_->namedtuple('PackedSequence', ['data', 'batch_sizes', 'sorted_indices', 'unsorted_indices'])
A:torch.nn.utils.rnn.unsorted_indices->bind(unsorted_indices, lambda t: t.to(kwargs[device_kw]))
A:torch.nn.utils.rnn.data->self.data.to(*args, **kwargs)
A:torch.nn.utils.rnn.sorted_indices->sorted_indices.to(input.device).to(input.device)
A:torch.nn.utils.rnn.output->torch.empty_like(permutation)
A:torch.nn.utils.rnn.lengths->torch.as_tensor(lengths, dtype=torch.int64)
A:torch.nn.utils.rnn.(lengths, sorted_indices)->torch.sort(lengths, descending=True)
A:torch.nn.utils.rnn.input->input.index_select(batch_dim, sorted_indices).index_select(batch_dim, sorted_indices)
A:torch.nn.utils.rnn.(data, batch_sizes)->_VF._pack_padded_sequence(input, lengths, batch_first)
A:torch.nn.utils.rnn.max_seq_length->sequence.batch_sizes.size(0)
A:torch.nn.utils.rnn.(padded_output, lengths)->_VF._pad_packed_sequence(sequence.data, sequence.batch_sizes, batch_first, padding_value, max_seq_length)
A:torch.nn.utils.rnn.max_size->sequences[0].size()
A:torch.nn.utils.rnn.max_len->max([s.size(0) for s in sequences])
A:torch.nn.utils.rnn.out_tensor->sequences[0].data.new(*out_dims).fill_(padding_value)
A:torch.nn.utils.rnn.length->tensor.size(0)
torch.nn.utils.rnn.PackedSequence(cls,data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.PackedSequence.__new__(cls,data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)
torch.nn.utils.rnn.PackedSequence.byte(self)
torch.nn.utils.rnn.PackedSequence.char(self)
torch.nn.utils.rnn.PackedSequence.cpu(self)
torch.nn.utils.rnn.PackedSequence.cuda(self,*args,**kwargs)
torch.nn.utils.rnn.PackedSequence.double(self)
torch.nn.utils.rnn.PackedSequence.float(self)
torch.nn.utils.rnn.PackedSequence.half(self)
torch.nn.utils.rnn.PackedSequence.int(self)
torch.nn.utils.rnn.PackedSequence.is_cuda(self)
torch.nn.utils.rnn.PackedSequence.is_pinned(self)
torch.nn.utils.rnn.PackedSequence.long(self)
torch.nn.utils.rnn.PackedSequence.pin_memory(self)
torch.nn.utils.rnn.PackedSequence.short(self)
torch.nn.utils.rnn.PackedSequence.to(self,*args,**kwargs)
torch.nn.utils.rnn.bind(optional,fn)
torch.nn.utils.rnn.invert_permutation(permutation)
torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)
torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)
torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)
torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/fusion.py----------------------------------------
A:torch.nn.utils.fusion.fused_conv->copy.deepcopy(conv)
A:torch.nn.utils.fusion.(fused_conv.weight, fused_conv.bias)->fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias, bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)
A:torch.nn.utils.fusion.conv_b->bn_rm.new_zeros(bn_rm.shape)
A:torch.nn.utils.fusion.bn_var_rsqrt->torch.rsqrt(bn_rv + bn_eps)
torch.nn.utils.fuse_conv_bn_eval(conv,bn)
torch.nn.utils.fuse_conv_bn_weights(conv_w,conv_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b)
torch.nn.utils.fusion.fuse_conv_bn_eval(conv,bn)
torch.nn.utils.fusion.fuse_conv_bn_weights(conv_w,conv_b,bn_rm,bn_rv,bn_eps,bn_w,bn_b)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/weight_norm.py----------------------------------------
A:torch.nn.utils.weight_norm.g->getattr(module, self.name + '_g')
A:torch.nn.utils.weight_norm.v->getattr(module, self.name + '_v')
A:torch.nn.utils.weight_norm.fn->WeightNorm(name, dim)
A:torch.nn.utils.weight_norm.weight->self.compute_weight(module)
torch.nn.utils.remove_weight_norm(module,name='weight')
torch.nn.utils.weight_norm(module,name='weight',dim=0)
torch.nn.utils.weight_norm.WeightNorm(self,name,dim)
torch.nn.utils.weight_norm.WeightNorm.__init__(self,name,dim)
torch.nn.utils.weight_norm.WeightNorm.apply(module,name,dim)
torch.nn.utils.weight_norm.WeightNorm.compute_weight(self,module)
torch.nn.utils.weight_norm.WeightNorm.remove(self,module)
torch.nn.utils.weight_norm.remove_weight_norm(module,name='weight')
torch.nn.utils.weight_norm.weight_norm(module,name='weight',dim=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/spectral_norm.py----------------------------------------
A:torch.nn.utils.spectral_norm.weight_mat->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig)
A:torch.nn.utils.spectral_norm.height->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size(0)
A:torch.nn.utils.spectral_norm.weight->state_dict.pop(weight_key)
A:torch.nn.utils.spectral_norm.u->normalize(weight.new_empty(h).normal_(0, 1), dim=0, eps=fn.eps)
A:torch.nn.utils.spectral_norm.v->SpectralNorm(name, n_power_iterations, dim, eps)._solve_v_and_rescale(weight_mat, u, sigma)
A:torch.nn.utils.spectral_norm.sigma->(weight_orig / weight).mean()
A:torch.nn.utils.spectral_norm.fn->SpectralNorm(name, n_power_iterations, dim, eps)
A:torch.nn.utils.spectral_norm.(h, w)->SpectralNorm(name, n_power_iterations, dim, eps).reshape_weight_to_matrix(weight_orig).size()
A:torch.nn.utils.spectral_norm.version->local_metadata.get('spectral_norm', {}).get(fn.name + '.version', None)
torch.nn.utils.remove_spectral_norm(module,name='weight')
torch.nn.utils.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)
torch.nn.utils.spectral_norm.SpectralNorm(self,name='weight',n_power_iterations=1,dim=0,eps=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm.__init__(self,name='weight',n_power_iterations=1,dim=0,eps=1e-12)
torch.nn.utils.spectral_norm.SpectralNorm._solve_v_and_rescale(self,weight_mat,u,target_sigma)
torch.nn.utils.spectral_norm.SpectralNorm.apply(module,name,n_power_iterations,dim,eps)
torch.nn.utils.spectral_norm.SpectralNorm.compute_weight(self,module,do_power_iteration)
torch.nn.utils.spectral_norm.SpectralNorm.remove(self,module)
torch.nn.utils.spectral_norm.SpectralNorm.reshape_weight_to_matrix(self,weight)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormLoadStateDictPreHook.__init__(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook(self,fn)
torch.nn.utils.spectral_norm.SpectralNormStateDictHook.__init__(self,fn)
torch.nn.utils.spectral_norm.remove_spectral_norm(module,name='weight')
torch.nn.utils.spectral_norm.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/convert_parameters.py----------------------------------------
A:torch.nn.utils.convert_parameters.param_device->_check_param_device(param, param_device)
A:torch.nn.utils.convert_parameters.num_param->param.numel()
torch.nn.utils.convert_parameters._check_param_device(param,old_param_device)
torch.nn.utils.convert_parameters.parameters_to_vector(parameters)
torch.nn.utils.convert_parameters.vector_to_parameters(vec,parameters)
torch.nn.utils.parameters_to_vector(parameters)
torch.nn.utils.vector_to_parameters(vec,parameters)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py----------------------------------------
A:torch.nn.utils.clip_grad.parameters->list(filter(lambda p: p.grad is not None, parameters))
A:torch.nn.utils.clip_grad.max_norm->float(max_norm)
A:torch.nn.utils.clip_grad.norm_type->float(norm_type)
A:torch.nn.utils.clip_grad.total_norm->max((p.grad.data.abs().max() for p in parameters))
A:torch.nn.utils.clip_grad.param_norm->p.grad.data.norm(norm_type)
A:torch.nn.utils.clip_grad.clip_value->float(clip_value)
torch.nn.utils.clip_grad.clip_grad_norm(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad.clip_grad_norm_(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad.clip_grad_value_(parameters,clip_value)
torch.nn.utils.clip_grad_norm(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)
torch.nn.utils.clip_grad_value_(parameters,clip_value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py----------------------------------------
A:torch.nn.modules.batchnorm.self.weight->Parameter(torch.Tensor(num_features))
A:torch.nn.modules.batchnorm.self.bias->Parameter(torch.Tensor(num_features))
A:torch.nn.modules.batchnorm.version->local_metadata.get('version', None)
A:torch.nn.modules.batchnorm.state_dict[num_batches_tracked_key]->torch.tensor(0, dtype=torch.long)
A:torch.nn.modules.batchnorm.world_size->torch.distributed.get_world_size(process_group)
A:torch.nn.modules.batchnorm.module_output->torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group)
A:torch.nn.modules.batchnorm.module_output.weight.data->module.weight.data.clone().detach()
A:torch.nn.modules.batchnorm.module_output.bias.data->module.bias.data.clone().detach()
torch.nn.BatchNorm1d(_BatchNorm)
torch.nn.BatchNorm1d._check_input_dim(self,input)
torch.nn.BatchNorm2d(_BatchNorm)
torch.nn.BatchNorm2d._check_input_dim(self,input)
torch.nn.BatchNorm3d(_BatchNorm)
torch.nn.BatchNorm3d._check_input_dim(self,input)
torch.nn.SyncBatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)
torch.nn.SyncBatchNorm._check_input_dim(self,input)
torch.nn.SyncBatchNorm._specify_ddp_gpu_num(self,gpu_size)
torch.nn.SyncBatchNorm.convert_sync_batchnorm(cls,module,process_group=None)
torch.nn.SyncBatchNorm.forward(self,input)
torch.nn.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.batchnorm._BatchNorm._check_input_dim(self,input)
torch.nn.batchnorm._BatchNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.batchnorm._BatchNorm.extra_repr(self)
torch.nn.batchnorm._BatchNorm.forward(self,input)
torch.nn.batchnorm._BatchNorm.reset_parameters(self)
torch.nn.batchnorm._BatchNorm.reset_running_stats(self)
torch.nn.modules.batchnorm.BatchNorm1d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm1d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm2d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm2d._check_input_dim(self,input)
torch.nn.modules.batchnorm.BatchNorm3d(_BatchNorm)
torch.nn.modules.batchnorm.BatchNorm3d._check_input_dim(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm._check_input_dim(self,input)
torch.nn.modules.batchnorm.SyncBatchNorm._specify_ddp_gpu_num(self,gpu_size)
torch.nn.modules.batchnorm.SyncBatchNorm.convert_sync_batchnorm(cls,module,process_group=None)
torch.nn.modules.batchnorm.SyncBatchNorm.forward(self,input)
torch.nn.modules.batchnorm._BatchNorm(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._BatchNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)
torch.nn.modules.batchnorm._BatchNorm._check_input_dim(self,input)
torch.nn.modules.batchnorm._BatchNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.batchnorm._BatchNorm.extra_repr(self)
torch.nn.modules.batchnorm._BatchNorm.forward(self,input)
torch.nn.modules.batchnorm._BatchNorm.reset_parameters(self)
torch.nn.modules.batchnorm._BatchNorm.reset_running_stats(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/batchnorm.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/container.py----------------------------------------
A:torch.nn.modules.container.size->len(self)
A:torch.nn.modules.container.idx->self._get_abs_string_index(idx)
A:torch.nn.modules.container.key->self._get_item_by_idx(self._modules.keys(), idx)
A:torch.nn.modules.container.keys->super(ParameterList, self).__dir__()
A:torch.nn.modules.container.input->module(input)
A:torch.nn.modules.container.self._modules->OrderedDict(list(zip(str_indices, self._modules.values())))
A:torch.nn.modules.container.offset->len(self)
A:torch.nn.modules.container.size_str->'x'.join((str(size) for size in p.size()))
A:torch.nn.modules.container.parastr->'Parameter containing: [{} of size {}{}]'.format(torch.typename(p.data), size_str, device_str)
A:torch.nn.modules.container.tmpstr->'\n'.join(child_lines)
torch.nn.Container(self,**kwargs)
torch.nn.ModuleDict(self,modules=None)
torch.nn.ModuleDict.__contains__(self,key)
torch.nn.ModuleDict.__delitem__(self,key)
torch.nn.ModuleDict.__getitem__(self,key)
torch.nn.ModuleDict.__iter__(self)
torch.nn.ModuleDict.__len__(self)
torch.nn.ModuleDict.__setitem__(self,key,module)
torch.nn.ModuleDict.clear(self)
torch.nn.ModuleDict.forward(self)
torch.nn.ModuleDict.items(self)
torch.nn.ModuleDict.keys(self)
torch.nn.ModuleDict.pop(self,key)
torch.nn.ModuleDict.update(self,modules)
torch.nn.ModuleDict.values(self)
torch.nn.ModuleList(self,modules=None)
torch.nn.ModuleList.__delitem__(self,idx)
torch.nn.ModuleList.__dir__(self)
torch.nn.ModuleList.__getitem__(self,idx)
torch.nn.ModuleList.__iadd__(self,modules)
torch.nn.ModuleList.__iter__(self)
torch.nn.ModuleList.__len__(self)
torch.nn.ModuleList.__setitem__(self,idx,module)
torch.nn.ModuleList._get_abs_string_index(self,idx)
torch.nn.ModuleList.append(self,module)
torch.nn.ModuleList.extend(self,modules)
torch.nn.ModuleList.insert(self,index,module)
torch.nn.ParameterDict(self,parameters=None)
torch.nn.ParameterDict.__contains__(self,key)
torch.nn.ParameterDict.__delitem__(self,key)
torch.nn.ParameterDict.__getitem__(self,key)
torch.nn.ParameterDict.__iter__(self)
torch.nn.ParameterDict.__len__(self)
torch.nn.ParameterDict.__setitem__(self,key,parameter)
torch.nn.ParameterDict.clear(self)
torch.nn.ParameterDict.extra_repr(self)
torch.nn.ParameterDict.items(self)
torch.nn.ParameterDict.keys(self)
torch.nn.ParameterDict.pop(self,key)
torch.nn.ParameterDict.update(self,parameters)
torch.nn.ParameterDict.values(self)
torch.nn.ParameterList(self,parameters=None)
torch.nn.ParameterList.__dir__(self)
torch.nn.ParameterList.__getitem__(self,idx)
torch.nn.ParameterList.__iadd__(self,parameters)
torch.nn.ParameterList.__iter__(self)
torch.nn.ParameterList.__len__(self)
torch.nn.ParameterList.__setitem__(self,idx,param)
torch.nn.ParameterList._get_abs_string_index(self,idx)
torch.nn.ParameterList.append(self,parameter)
torch.nn.ParameterList.extend(self,parameters)
torch.nn.ParameterList.extra_repr(self)
torch.nn.Sequential(self,*args)
torch.nn.Sequential.__delitem__(self,idx)
torch.nn.Sequential.__dir__(self)
torch.nn.Sequential.__getitem__(self,idx)
torch.nn.Sequential.__len__(self)
torch.nn.Sequential.__setitem__(self,idx,module)
torch.nn.Sequential._get_item_by_idx(self,iterator,idx)
torch.nn.Sequential.forward(self,input)
torch.nn.modules.container.Container(self,**kwargs)
torch.nn.modules.container.Container.__init__(self,**kwargs)
torch.nn.modules.container.ModuleDict(self,modules=None)
torch.nn.modules.container.ModuleDict.__contains__(self,key)
torch.nn.modules.container.ModuleDict.__delitem__(self,key)
torch.nn.modules.container.ModuleDict.__getitem__(self,key)
torch.nn.modules.container.ModuleDict.__init__(self,modules=None)
torch.nn.modules.container.ModuleDict.__iter__(self)
torch.nn.modules.container.ModuleDict.__len__(self)
torch.nn.modules.container.ModuleDict.__setitem__(self,key,module)
torch.nn.modules.container.ModuleDict.clear(self)
torch.nn.modules.container.ModuleDict.forward(self)
torch.nn.modules.container.ModuleDict.items(self)
torch.nn.modules.container.ModuleDict.keys(self)
torch.nn.modules.container.ModuleDict.pop(self,key)
torch.nn.modules.container.ModuleDict.update(self,modules)
torch.nn.modules.container.ModuleDict.values(self)
torch.nn.modules.container.ModuleList(self,modules=None)
torch.nn.modules.container.ModuleList.__delitem__(self,idx)
torch.nn.modules.container.ModuleList.__dir__(self)
torch.nn.modules.container.ModuleList.__getitem__(self,idx)
torch.nn.modules.container.ModuleList.__iadd__(self,modules)
torch.nn.modules.container.ModuleList.__init__(self,modules=None)
torch.nn.modules.container.ModuleList.__iter__(self)
torch.nn.modules.container.ModuleList.__len__(self)
torch.nn.modules.container.ModuleList.__setitem__(self,idx,module)
torch.nn.modules.container.ModuleList._get_abs_string_index(self,idx)
torch.nn.modules.container.ModuleList.append(self,module)
torch.nn.modules.container.ModuleList.extend(self,modules)
torch.nn.modules.container.ModuleList.insert(self,index,module)
torch.nn.modules.container.ParameterDict(self,parameters=None)
torch.nn.modules.container.ParameterDict.__contains__(self,key)
torch.nn.modules.container.ParameterDict.__delitem__(self,key)
torch.nn.modules.container.ParameterDict.__getitem__(self,key)
torch.nn.modules.container.ParameterDict.__init__(self,parameters=None)
torch.nn.modules.container.ParameterDict.__iter__(self)
torch.nn.modules.container.ParameterDict.__len__(self)
torch.nn.modules.container.ParameterDict.__setitem__(self,key,parameter)
torch.nn.modules.container.ParameterDict.clear(self)
torch.nn.modules.container.ParameterDict.extra_repr(self)
torch.nn.modules.container.ParameterDict.items(self)
torch.nn.modules.container.ParameterDict.keys(self)
torch.nn.modules.container.ParameterDict.pop(self,key)
torch.nn.modules.container.ParameterDict.update(self,parameters)
torch.nn.modules.container.ParameterDict.values(self)
torch.nn.modules.container.ParameterList(self,parameters=None)
torch.nn.modules.container.ParameterList.__dir__(self)
torch.nn.modules.container.ParameterList.__getitem__(self,idx)
torch.nn.modules.container.ParameterList.__iadd__(self,parameters)
torch.nn.modules.container.ParameterList.__init__(self,parameters=None)
torch.nn.modules.container.ParameterList.__iter__(self)
torch.nn.modules.container.ParameterList.__len__(self)
torch.nn.modules.container.ParameterList.__setitem__(self,idx,param)
torch.nn.modules.container.ParameterList._get_abs_string_index(self,idx)
torch.nn.modules.container.ParameterList.append(self,parameter)
torch.nn.modules.container.ParameterList.extend(self,parameters)
torch.nn.modules.container.ParameterList.extra_repr(self)
torch.nn.modules.container.Sequential(self,*args)
torch.nn.modules.container.Sequential.__delitem__(self,idx)
torch.nn.modules.container.Sequential.__dir__(self)
torch.nn.modules.container.Sequential.__getitem__(self,idx)
torch.nn.modules.container.Sequential.__init__(self,*args)
torch.nn.modules.container.Sequential.__len__(self)
torch.nn.modules.container.Sequential.__setitem__(self,idx,module)
torch.nn.modules.container.Sequential._get_item_by_idx(self,iterator,idx)
torch.nn.modules.container.Sequential.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/container.pyi----------------------------------------
torch.nn.ParameterList.__delitem__(self,idx:Union[int,slice])->None
torch.nn.ParameterList.insert(self,index:int,parameter:Parameter)->None
torch.nn.modules.container.ParameterList.__delitem__(self,idx:Union[int,slice])->None
torch.nn.modules.container.ParameterList.insert(self,index:int,parameter:Parameter)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/rnn.py----------------------------------------
A:torch.nn.modules.rnn.self.dropout->float(dropout)
A:torch.nn.modules.rnn.w_ih->Parameter(torch.Tensor(gate_size, layer_input_size))
A:torch.nn.modules.rnn.w_hh->Parameter(torch.Tensor(gate_size, hidden_size))
A:torch.nn.modules.rnn.b_ih->Parameter(torch.Tensor(gate_size))
A:torch.nn.modules.rnn.b_hh->Parameter(torch.Tensor(gate_size))
A:torch.nn.modules.rnn.unique_data_ptrs->set((p.data_ptr() for p in all_weights))
A:torch.nn.modules.rnn.ret->_VF.rnn_relu_cell(input, hx, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh)
A:torch.nn.modules.rnn.mini_batch->int(mini_batch)
A:torch.nn.modules.rnn.expected_hidden_size->self.get_expected_hidden_size(input, batch_sizes)
A:torch.nn.modules.rnn.is_packed->isinstance(input, PackedSequence)
A:torch.nn.modules.rnn.max_batch_size->int(max_batch_size)
A:torch.nn.modules.rnn.hx->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.result->self.run_impl(input, hx, batch_sizes)
A:torch.nn.modules.rnn.output->PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
A:torch.nn.modules.rnn.zeros->torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)
A:torch.nn.modules.rnn.(output, hidden)->self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)
A:torch.nn.modules.rnn.self.weight_ih->Parameter(torch.Tensor(num_chunks * hidden_size, input_size))
A:torch.nn.modules.rnn.self.weight_hh->Parameter(torch.Tensor(num_chunks * hidden_size, hidden_size))
A:torch.nn.modules.rnn.self.bias_ih->Parameter(torch.Tensor(num_chunks * hidden_size))
A:torch.nn.modules.rnn.self.bias_hh->Parameter(torch.Tensor(num_chunks * hidden_size))
torch.nn.GRU(self,*args,**kwargs)
torch.nn.GRU.forward(self,input,hx=None)
torch.nn.GRU.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.GRU.forward_packed(self,input,hx=None)
torch.nn.GRU.forward_tensor(self,input,hx=None)
torch.nn.GRU.run_impl(self,input,hx,batch_sizes)
torch.nn.GRUCell(self,input_size,hidden_size,bias=True)
torch.nn.GRUCell.forward(self,input,hx=None)
torch.nn.LSTM(self,*args,**kwargs)
torch.nn.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.LSTM.forward(self,input,hx=None)
torch.nn.LSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.LSTM.forward_packed(self,input,hx=None)
torch.nn.LSTM.forward_tensor(self,input,hx=None)
torch.nn.LSTM.permute_hidden(self,hx,permutation)
torch.nn.LSTMCell(self,input_size,hidden_size,bias=True)
torch.nn.LSTMCell.forward(self,input,hx=None)
torch.nn.RNN(self,*args,**kwargs)
torch.nn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)
torch.nn.RNNBase.__setstate__(self,d)
torch.nn.RNNBase._apply(self,fn)
torch.nn.RNNBase._flat_weights(self)
torch.nn.RNNBase._get_flat_weights(self)
torch.nn.RNNBase._get_flat_weights_names(self)
torch.nn.RNNBase.all_weights(self)
torch.nn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.RNNBase.extra_repr(self)
torch.nn.RNNBase.flatten_parameters(self)
torch.nn.RNNBase.forward(self,input,hx=None)
torch.nn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.RNNBase.reset_parameters(self)
torch.nn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.RNNCell.forward(self,input,hx=None)
torch.nn.RNNCellBase(self,input_size,hidden_size,bias,num_chunks)
torch.nn.RNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.nn.RNNCellBase.check_forward_input(self,input)
torch.nn.RNNCellBase.extra_repr(self)
torch.nn.RNNCellBase.reset_parameters(self)
torch.nn.modules.rnn.GRU(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.GRU.forward(self,input,hx=None)
torch.nn.modules.rnn.GRU.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.modules.rnn.GRU.forward_packed(self,input,hx=None)
torch.nn.modules.rnn.GRU.forward_tensor(self,input,hx=None)
torch.nn.modules.rnn.GRU.run_impl(self,input,hx,batch_sizes)
torch.nn.modules.rnn.GRUCell(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.GRUCell.__init__(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.GRUCell.forward(self,input,hx=None)
torch.nn.modules.rnn.LSTM(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.LSTM.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.modules.rnn.LSTM.forward(self,input,hx=None)
torch.nn.modules.rnn.LSTM.forward_impl(self,input,hx,batch_sizes,max_batch_size,sorted_indices)
torch.nn.modules.rnn.LSTM.forward_packed(self,input,hx=None)
torch.nn.modules.rnn.LSTM.forward_tensor(self,input,hx=None)
torch.nn.modules.rnn.LSTM.permute_hidden(self,hx,permutation)
torch.nn.modules.rnn.LSTMCell(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.LSTMCell.__init__(self,input_size,hidden_size,bias=True)
torch.nn.modules.rnn.LSTMCell.forward(self,input,hx=None)
torch.nn.modules.rnn.RNN(self,*args,**kwargs)
torch.nn.modules.rnn.RNN.__init__(self,*args,**kwargs)
torch.nn.modules.rnn.RNNBase(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)
torch.nn.modules.rnn.RNNBase.__init__(self,mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)
torch.nn.modules.rnn.RNNBase.__setstate__(self,d)
torch.nn.modules.rnn.RNNBase._apply(self,fn)
torch.nn.modules.rnn.RNNBase._flat_weights(self)
torch.nn.modules.rnn.RNNBase._get_flat_weights(self)
torch.nn.modules.rnn.RNNBase._get_flat_weights_names(self)
torch.nn.modules.rnn.RNNBase.all_weights(self)
torch.nn.modules.rnn.RNNBase.check_forward_args(self,input,hidden,batch_sizes)
torch.nn.modules.rnn.RNNBase.check_hidden_size(self,hx,expected_hidden_size,msg='Expectedhiddensize{},got{}')
torch.nn.modules.rnn.RNNBase.check_input(self,input,batch_sizes)
torch.nn.modules.rnn.RNNBase.extra_repr(self)
torch.nn.modules.rnn.RNNBase.flatten_parameters(self)
torch.nn.modules.rnn.RNNBase.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNBase.get_expected_hidden_size(self,input,batch_sizes)
torch.nn.modules.rnn.RNNBase.permute_hidden(self,hx,permutation)
torch.nn.modules.rnn.RNNBase.reset_parameters(self)
torch.nn.modules.rnn.RNNCell(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.modules.rnn.RNNCell.__init__(self,input_size,hidden_size,bias=True,nonlinearity='tanh')
torch.nn.modules.rnn.RNNCell.forward(self,input,hx=None)
torch.nn.modules.rnn.RNNCellBase(self,input_size,hidden_size,bias,num_chunks)
torch.nn.modules.rnn.RNNCellBase.__init__(self,input_size,hidden_size,bias,num_chunks)
torch.nn.modules.rnn.RNNCellBase.check_forward_hidden(self,input,hx,hidden_label='')
torch.nn.modules.rnn.RNNCellBase.check_forward_input(self,input)
torch.nn.modules.rnn.RNNCellBase.extra_repr(self)
torch.nn.modules.rnn.RNNCellBase.reset_parameters(self)
torch.nn.modules.rnn.apply_permutation(tensor,permutation,dim=1)
torch.nn.rnn.apply_permutation(tensor,permutation,dim=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/rnn.pyi----------------------------------------
torch.nn.RNN.forward(self,input:Tensor,hx:Optional[Tensor]=...)->Tensor
torch.nn.RNNBase.get_flat_weights(self)
torch.nn.modules.rnn.RNN.forward(self,input:Tensor,hx:Optional[Tensor]=...)->Tensor
torch.nn.modules.rnn.RNNBase.get_flat_weights(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/flatten.py----------------------------------------
torch.nn.Flatten(self,start_dim=1,end_dim=-1)
torch.nn.Flatten.forward(self,input)
torch.nn.modules.flatten.Flatten(self,start_dim=1,end_dim=-1)
torch.nn.modules.flatten.Flatten.__init__(self,start_dim=1,end_dim=-1)
torch.nn.modules.flatten.Flatten.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/flatten.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/__init__.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/sparse.py----------------------------------------
A:torch.nn.modules.sparse.self.weight->Parameter(_weight)
A:torch.nn.modules.sparse.embedding->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, padding_idx=padding_idx, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, sparse=sparse)
A:torch.nn.modules.sparse.embeddingbag->cls(num_embeddings=rows, embedding_dim=cols, _weight=embeddings, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, mode=mode, sparse=sparse)
torch.nn.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.Embedding.extra_repr(self)
torch.nn.Embedding.forward(self,input)
torch.nn.Embedding.from_pretrained(cls,embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.Embedding.reset_parameters(self)
torch.nn.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None)
torch.nn.EmbeddingBag.extra_repr(self)
torch.nn.EmbeddingBag.forward(self,input,offsets=None,per_sample_weights=None)
torch.nn.EmbeddingBag.from_pretrained(cls,embeddings,freeze=True,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.EmbeddingBag.reset_parameters(self)
torch.nn.modules.sparse.Embedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.modules.sparse.Embedding.__init__(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)
torch.nn.modules.sparse.Embedding.extra_repr(self)
torch.nn.modules.sparse.Embedding.forward(self,input)
torch.nn.modules.sparse.Embedding.from_pretrained(cls,embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)
torch.nn.modules.sparse.Embedding.reset_parameters(self)
torch.nn.modules.sparse.EmbeddingBag(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None)
torch.nn.modules.sparse.EmbeddingBag.__init__(self,num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None)
torch.nn.modules.sparse.EmbeddingBag.extra_repr(self)
torch.nn.modules.sparse.EmbeddingBag.forward(self,input,offsets=None,per_sample_weights=None)
torch.nn.modules.sparse.EmbeddingBag.from_pretrained(cls,embeddings,freeze=True,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False)
torch.nn.modules.sparse.EmbeddingBag.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/sparse.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/transformer.py----------------------------------------
A:torch.nn.modules.transformer.encoder_layer->TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
A:torch.nn.modules.transformer.encoder_norm->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.encoder->TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
A:torch.nn.modules.transformer.decoder_layer->TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
A:torch.nn.modules.transformer.decoder_norm->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.decoder->TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)
A:torch.nn.modules.transformer.memory->self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)
A:torch.nn.modules.transformer.output->self.norm(output)
A:torch.nn.modules.transformer.mask->mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
A:torch.nn.modules.transformer.self.layers->_get_clones(decoder_layer, num_layers)
A:torch.nn.modules.transformer.self.self_attn->MultiheadAttention(d_model, nhead, dropout=dropout)
A:torch.nn.modules.transformer.self.linear1->Linear(d_model, dim_feedforward)
A:torch.nn.modules.transformer.self.dropout->Dropout(dropout)
A:torch.nn.modules.transformer.self.linear2->Linear(dim_feedforward, d_model)
A:torch.nn.modules.transformer.self.norm1->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.norm2->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.dropout1->Dropout(dropout)
A:torch.nn.modules.transformer.self.dropout2->Dropout(dropout)
A:torch.nn.modules.transformer.self.activation->_get_activation_fn(activation)
A:torch.nn.modules.transformer.src->self.norm2(src)
A:torch.nn.modules.transformer.src2->self.linear2(self.dropout(F.relu(self.linear1(src))))
A:torch.nn.modules.transformer.self.multihead_attn->MultiheadAttention(d_model, nhead, dropout=dropout)
A:torch.nn.modules.transformer.self.norm3->LayerNorm(d_model)
A:torch.nn.modules.transformer.self.dropout3->Dropout(dropout)
A:torch.nn.modules.transformer.tgt->self.norm3(tgt)
A:torch.nn.modules.transformer.tgt2->self.linear2(self.dropout(F.relu(self.linear1(tgt))))
torch.nn.Transformer(self,d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)
torch.nn.Transformer._reset_parameters(self)
torch.nn.Transformer.forward(self,src,tgt,src_mask=None,tgt_mask=None,memory_mask=None,src_key_padding_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.Transformer.generate_square_subsequent_mask(self,sz)
torch.nn.TransformerDecoder(self,decoder_layer,num_layers,norm=None)
torch.nn.TransformerDecoder.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.TransformerDecoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.TransformerDecoderLayer.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.TransformerEncoder(self,encoder_layer,num_layers,norm=None)
torch.nn.TransformerEncoder.forward(self,src,mask=None,src_key_padding_mask=None)
torch.nn.TransformerEncoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.TransformerEncoderLayer.forward(self,src,src_mask=None,src_key_padding_mask=None)
torch.nn.modules.transformer.Transformer(self,d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)
torch.nn.modules.transformer.Transformer.__init__(self,d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)
torch.nn.modules.transformer.Transformer._reset_parameters(self)
torch.nn.modules.transformer.Transformer.forward(self,src,tgt,src_mask=None,tgt_mask=None,memory_mask=None,src_key_padding_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.modules.transformer.Transformer.generate_square_subsequent_mask(self,sz)
torch.nn.modules.transformer.TransformerDecoder(self,decoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerDecoder.__init__(self,decoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerDecoder.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.modules.transformer.TransformerDecoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerDecoderLayer.__init__(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerDecoderLayer.forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)
torch.nn.modules.transformer.TransformerEncoder(self,encoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerEncoder.__init__(self,encoder_layer,num_layers,norm=None)
torch.nn.modules.transformer.TransformerEncoder.forward(self,src,mask=None,src_key_padding_mask=None)
torch.nn.modules.transformer.TransformerEncoderLayer(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerEncoderLayer.__init__(self,d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')
torch.nn.modules.transformer.TransformerEncoderLayer.forward(self,src,src_mask=None,src_key_padding_mask=None)
torch.nn.modules.transformer._get_activation_fn(activation)
torch.nn.modules.transformer._get_clones(module,N)
torch.nn.transformer._get_activation_fn(activation)
torch.nn.transformer._get_clones(module,N)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/loss.py----------------------------------------
A:torch.nn.modules.loss.self.reduction->_Reduction.legacy_get_string(size_average, reduce)
torch.nn.BCELoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.BCELoss.forward(self,input,target)
torch.nn.BCEWithLogitsLoss(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.BCEWithLogitsLoss.forward(self,input,target)
torch.nn.CTCLoss(self,blank=0,reduction='mean',zero_infinity=False)
torch.nn.CTCLoss.forward(self,log_probs,targets,input_lengths,target_lengths)
torch.nn.CosineEmbeddingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.CosineEmbeddingLoss.forward(self,input1,input2,target)
torch.nn.CrossEntropyLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.CrossEntropyLoss.forward(self,input,target)
torch.nn.HingeEmbeddingLoss(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.HingeEmbeddingLoss.forward(self,input,target)
torch.nn.KLDivLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.KLDivLoss.forward(self,input,target)
torch.nn.L1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.L1Loss.forward(self,input,target)
torch.nn.MSELoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.MSELoss.forward(self,input,target)
torch.nn.MarginRankingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.MarginRankingLoss.forward(self,input1,input2,target)
torch.nn.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiLabelMarginLoss.forward(self,input,target)
torch.nn.MultiLabelSoftMarginLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiLabelSoftMarginLoss.forward(self,input,target)
torch.nn.MultiMarginLoss(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.MultiMarginLoss.forward(self,input,target)
torch.nn.NLLLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.NLLLoss.forward(self,input,target)
torch.nn.NLLLoss2d(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.PoissonNLLLoss(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.PoissonNLLLoss.forward(self,log_input,target)
torch.nn.SmoothL1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.SmoothL1Loss.forward(self,input,target)
torch.nn.SoftMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.SoftMarginLoss.forward(self,input,target)
torch.nn.TripletMarginLoss(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.TripletMarginLoss.forward(self,anchor,positive,negative)
torch.nn.loss._Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.loss._WeightedLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.BCELoss.forward(self,input,target)
torch.nn.modules.loss.BCEWithLogitsLoss(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.modules.loss.BCEWithLogitsLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)
torch.nn.modules.loss.BCEWithLogitsLoss.forward(self,input,target)
torch.nn.modules.loss.CTCLoss(self,blank=0,reduction='mean',zero_infinity=False)
torch.nn.modules.loss.CTCLoss.__init__(self,blank=0,reduction='mean',zero_infinity=False)
torch.nn.modules.loss.CTCLoss.forward(self,log_probs,targets,input_lengths,target_lengths)
torch.nn.modules.loss.CosineEmbeddingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.__init__(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.CosineEmbeddingLoss.forward(self,input1,input2,target)
torch.nn.modules.loss.CrossEntropyLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.CrossEntropyLoss.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.CrossEntropyLoss.forward(self,input,target)
torch.nn.modules.loss.HingeEmbeddingLoss(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.__init__(self,margin=1.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.HingeEmbeddingLoss.forward(self,input,target)
torch.nn.modules.loss.KLDivLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.KLDivLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.KLDivLoss.forward(self,input,target)
torch.nn.modules.loss.L1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.L1Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.L1Loss.forward(self,input,target)
torch.nn.modules.loss.MSELoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MSELoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MSELoss.forward(self,input,target)
torch.nn.modules.loss.MarginRankingLoss(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MarginRankingLoss.__init__(self,margin=0.0,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MarginRankingLoss.forward(self,input1,input2,target)
torch.nn.modules.loss.MultiLabelMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelMarginLoss.forward(self,input,target)
torch.nn.modules.loss.MultiLabelSoftMarginLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiLabelSoftMarginLoss.forward(self,input,target)
torch.nn.modules.loss.MultiMarginLoss(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiMarginLoss.__init__(self,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.MultiMarginLoss.forward(self,input,target)
torch.nn.modules.loss.NLLLoss(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss.forward(self,input,target)
torch.nn.modules.loss.NLLLoss2d(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.NLLLoss2d.__init__(self,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss.__init__(self,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')
torch.nn.modules.loss.PoissonNLLLoss.forward(self,log_input,target)
torch.nn.modules.loss.SmoothL1Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SmoothL1Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SmoothL1Loss.forward(self,input,target)
torch.nn.modules.loss.SoftMarginLoss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SoftMarginLoss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.SoftMarginLoss.forward(self,input,target)
torch.nn.modules.loss.TripletMarginLoss(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.TripletMarginLoss.__init__(self,margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss.TripletMarginLoss.forward(self,anchor,positive,negative)
torch.nn.modules.loss._Loss(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._Loss.__init__(self,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._WeightedLoss(self,weight=None,size_average=None,reduce=None,reduction='mean')
torch.nn.modules.loss._WeightedLoss.__init__(self,weight=None,size_average=None,reduce=None,reduction='mean')


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/loss.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/instancenorm.py----------------------------------------
A:torch.nn.modules.instancenorm.version->local_metadata.get('version', None)
torch.nn.InstanceNorm1d(_InstanceNorm)
torch.nn.InstanceNorm1d._check_input_dim(self,input)
torch.nn.InstanceNorm2d(_InstanceNorm)
torch.nn.InstanceNorm2d._check_input_dim(self,input)
torch.nn.InstanceNorm3d(_InstanceNorm)
torch.nn.InstanceNorm3d._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.instancenorm._InstanceNorm.forward(self,input)
torch.nn.modules.instancenorm.InstanceNorm1d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm1d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm2d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm2d._check_input_dim(self,input)
torch.nn.modules.instancenorm.InstanceNorm3d(_InstanceNorm)
torch.nn.modules.instancenorm.InstanceNorm3d._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.modules.instancenorm._InstanceNorm.__init__(self,num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)
torch.nn.modules.instancenorm._InstanceNorm._check_input_dim(self,input)
torch.nn.modules.instancenorm._InstanceNorm._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.instancenorm._InstanceNorm.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/instancenorm.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/distance.py----------------------------------------
torch.nn.CosineSimilarity(self,dim=1,eps=1e-08)
torch.nn.CosineSimilarity.forward(self,x1,x2)
torch.nn.PairwiseDistance(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.PairwiseDistance.forward(self,x1,x2)
torch.nn.modules.distance.CosineSimilarity(self,dim=1,eps=1e-08)
torch.nn.modules.distance.CosineSimilarity.__init__(self,dim=1,eps=1e-08)
torch.nn.modules.distance.CosineSimilarity.forward(self,x1,x2)
torch.nn.modules.distance.PairwiseDistance(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.modules.distance.PairwiseDistance.__init__(self,p=2.0,eps=1e-06,keepdim=False)
torch.nn.modules.distance.PairwiseDistance.forward(self,x1,x2)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/distance.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/utils.py----------------------------------------
A:torch.nn.modules.utils._single->_ntuple(1)
A:torch.nn.modules.utils._pair->_ntuple(2)
A:torch.nn.modules.utils._triple->_ntuple(3)
A:torch.nn.modules.utils._quadruple->_ntuple(4)
torch.nn.modules.utils._list_with_default(out_size,defaults)
torch.nn.modules.utils._ntuple(n)
torch.nn.utils._list_with_default(out_size,defaults)
torch.nn.utils._ntuple(n)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/linear.py----------------------------------------
A:torch.nn.modules.linear.self.weight->Parameter(torch.Tensor(out_features, in1_features, in2_features))
A:torch.nn.modules.linear.self.bias->Parameter(torch.Tensor(out_features))
A:torch.nn.modules.linear.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
torch.nn.Bilinear(self,in1_features,in2_features,out_features,bias=True)
torch.nn.Bilinear.extra_repr(self)
torch.nn.Bilinear.forward(self,input1,input2)
torch.nn.Bilinear.reset_parameters(self)
torch.nn.Identity(self,*args,**kwargs)
torch.nn.Identity.forward(self,input)
torch.nn.Linear(self,in_features,out_features,bias=True)
torch.nn.Linear.extra_repr(self)
torch.nn.Linear.forward(self,input)
torch.nn.Linear.reset_parameters(self)
torch.nn.modules.linear.Bilinear(self,in1_features,in2_features,out_features,bias=True)
torch.nn.modules.linear.Bilinear.__init__(self,in1_features,in2_features,out_features,bias=True)
torch.nn.modules.linear.Bilinear.extra_repr(self)
torch.nn.modules.linear.Bilinear.forward(self,input1,input2)
torch.nn.modules.linear.Bilinear.reset_parameters(self)
torch.nn.modules.linear.Identity(self,*args,**kwargs)
torch.nn.modules.linear.Identity.__init__(self,*args,**kwargs)
torch.nn.modules.linear.Identity.forward(self,input)
torch.nn.modules.linear.Linear(self,in_features,out_features,bias=True)
torch.nn.modules.linear.Linear.__init__(self,in_features,out_features,bias=True)
torch.nn.modules.linear.Linear.extra_repr(self)
torch.nn.modules.linear.Linear.forward(self,input)
torch.nn.modules.linear.Linear.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/linear.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/pooling.py----------------------------------------
A:torch.nn.modules.pooling.self.kernel_size->_triple(kernel_size)
A:torch.nn.modules.pooling.self.stride->_single(stride if stride is not None else kernel_size)
A:torch.nn.modules.pooling.self.padding->_single(padding)
torch.nn.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool1d.forward(self,input)
torch.nn.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool2d.forward(self,input)
torch.nn.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.AdaptiveAvgPool3d.forward(self,input)
torch.nn.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool1d.forward(self,input)
torch.nn.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool2d.forward(self,input)
torch.nn.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.AdaptiveMaxPool3d.forward(self,input)
torch.nn.AvgPool1d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.AvgPool1d.forward(self,input)
torch.nn.AvgPool2d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.AvgPool2d.forward(self,input)
torch.nn.AvgPool3d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.AvgPool3d.__setstate__(self,d)
torch.nn.AvgPool3d.forward(self,input)
torch.nn.FractionalMaxPool2d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.FractionalMaxPool2d.forward(self,input)
torch.nn.FractionalMaxPool3d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.FractionalMaxPool3d.forward(self,input)
torch.nn.LPPool1d(_LPPoolNd)
torch.nn.LPPool1d.forward(self,input)
torch.nn.LPPool2d(_LPPoolNd)
torch.nn.LPPool2d.forward(self,input)
torch.nn.MaxPool1d(_MaxPoolNd)
torch.nn.MaxPool1d.forward(self,input)
torch.nn.MaxPool2d(_MaxPoolNd)
torch.nn.MaxPool2d.forward(self,input)
torch.nn.MaxPool3d(_MaxPoolNd)
torch.nn.MaxPool3d.forward(self,input)
torch.nn.MaxUnpool1d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool1d.forward(self,input,indices,output_size=None)
torch.nn.MaxUnpool2d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool2d.forward(self,input,indices,output_size=None)
torch.nn.MaxUnpool3d(self,kernel_size,stride=None,padding=0)
torch.nn.MaxUnpool3d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.AdaptiveAvgPool1d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool1d.forward(self,input)
torch.nn.modules.pooling.AdaptiveAvgPool2d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool2d.forward(self,input)
torch.nn.modules.pooling.AdaptiveAvgPool3d(_AdaptiveAvgPoolNd)
torch.nn.modules.pooling.AdaptiveAvgPool3d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool1d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool1d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool2d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool2d.forward(self,input)
torch.nn.modules.pooling.AdaptiveMaxPool3d(_AdaptiveMaxPoolNd)
torch.nn.modules.pooling.AdaptiveMaxPool3d.forward(self,input)
torch.nn.modules.pooling.AvgPool1d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool1d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)
torch.nn.modules.pooling.AvgPool1d.forward(self,input)
torch.nn.modules.pooling.AvgPool2d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool2d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool2d.forward(self,input)
torch.nn.modules.pooling.AvgPool3d(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool3d.__init__(self,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)
torch.nn.modules.pooling.AvgPool3d.__setstate__(self,d)
torch.nn.modules.pooling.AvgPool3d.forward(self,input)
torch.nn.modules.pooling.FractionalMaxPool2d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.__init__(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool2d.forward(self,input)
torch.nn.modules.pooling.FractionalMaxPool3d(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool3d.__init__(self,kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)
torch.nn.modules.pooling.FractionalMaxPool3d.forward(self,input)
torch.nn.modules.pooling.LPPool1d(_LPPoolNd)
torch.nn.modules.pooling.LPPool1d.forward(self,input)
torch.nn.modules.pooling.LPPool2d(_LPPoolNd)
torch.nn.modules.pooling.LPPool2d.forward(self,input)
torch.nn.modules.pooling.MaxPool1d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool1d.forward(self,input)
torch.nn.modules.pooling.MaxPool2d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool2d.forward(self,input)
torch.nn.modules.pooling.MaxPool3d(_MaxPoolNd)
torch.nn.modules.pooling.MaxPool3d.forward(self,input)
torch.nn.modules.pooling.MaxUnpool1d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool1d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool1d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.MaxUnpool2d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool2d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool2d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling.MaxUnpool3d(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool3d.__init__(self,kernel_size,stride=None,padding=0)
torch.nn.modules.pooling.MaxUnpool3d.forward(self,input,indices,output_size=None)
torch.nn.modules.pooling._AdaptiveAvgPoolNd(self,output_size)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.__init__(self,output_size)
torch.nn.modules.pooling._AdaptiveAvgPoolNd.extra_repr(self)
torch.nn.modules.pooling._AdaptiveMaxPoolNd(self,output_size,return_indices=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.__init__(self,output_size,return_indices=False)
torch.nn.modules.pooling._AdaptiveMaxPoolNd.extra_repr(self)
torch.nn.modules.pooling._AvgPoolNd(Module)
torch.nn.modules.pooling._AvgPoolNd.extra_repr(self)
torch.nn.modules.pooling._LPPoolNd(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.modules.pooling._LPPoolNd.__init__(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.modules.pooling._LPPoolNd.extra_repr(self)
torch.nn.modules.pooling._MaxPoolNd(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.modules.pooling._MaxPoolNd.__init__(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.modules.pooling._MaxPoolNd.extra_repr(self)
torch.nn.modules.pooling._MaxUnpoolNd(Module)
torch.nn.modules.pooling._MaxUnpoolNd.extra_repr(self)
torch.nn.pooling._AdaptiveAvgPoolNd(self,output_size)
torch.nn.pooling._AdaptiveAvgPoolNd.extra_repr(self)
torch.nn.pooling._AdaptiveMaxPoolNd(self,output_size,return_indices=False)
torch.nn.pooling._AdaptiveMaxPoolNd.extra_repr(self)
torch.nn.pooling._AvgPoolNd(Module)
torch.nn.pooling._AvgPoolNd.extra_repr(self)
torch.nn.pooling._LPPoolNd(self,norm_type,kernel_size,stride=None,ceil_mode=False)
torch.nn.pooling._LPPoolNd.extra_repr(self)
torch.nn.pooling._MaxPoolNd(self,kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)
torch.nn.pooling._MaxPoolNd.extra_repr(self)
torch.nn.pooling._MaxUnpoolNd(Module)
torch.nn.pooling._MaxUnpoolNd.extra_repr(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/pooling.pyi----------------------------------------
torch.nn.modules.pooling.AdaptiveAvgPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveAvgPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveAvgPool3d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.AdaptiveMaxPool3d.__call__(self,input:Tensor)
torch.nn.modules.pooling.LPPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.LPPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.MaxPool1d.__call__(self,input:Tensor)
torch.nn.modules.pooling.MaxPool2d.__call__(self,input:Tensor)
torch.nn.modules.pooling.MaxPool3d.__call__(self,input:Tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/activation.py----------------------------------------
A:torch.nn.modules.activation.self.in_proj_weight->Parameter(torch.empty(3 * embed_dim, embed_dim))
A:torch.nn.modules.activation.self.q_proj_weight->Parameter(torch.Tensor(embed_dim, embed_dim))
A:torch.nn.modules.activation.self.k_proj_weight->Parameter(torch.Tensor(embed_dim, self.kdim))
A:torch.nn.modules.activation.self.v_proj_weight->Parameter(torch.Tensor(embed_dim, self.vdim))
A:torch.nn.modules.activation.self.in_proj_bias->Parameter(torch.empty(3 * embed_dim))
A:torch.nn.modules.activation.self.out_proj->Linear(embed_dim, embed_dim, bias=bias)
A:torch.nn.modules.activation.self.bias_k->Parameter(torch.empty(1, 1, embed_dim))
A:torch.nn.modules.activation.self.bias_v->Parameter(torch.empty(1, 1, embed_dim))
A:torch.nn.modules.activation.self.weight->Parameter(torch.Tensor(num_parameters).fill_(init))
torch.nn.CELU(self,alpha=1.0,inplace=False)
torch.nn.CELU.extra_repr(self)
torch.nn.CELU.forward(self,input)
torch.nn.ELU(self,alpha=1.0,inplace=False)
torch.nn.ELU.extra_repr(self)
torch.nn.ELU.forward(self,input)
torch.nn.GLU(self,dim=-1)
torch.nn.GLU.extra_repr(self)
torch.nn.GLU.forward(self,input)
torch.nn.Hardshrink(self,lambd=0.5)
torch.nn.Hardshrink.extra_repr(self)
torch.nn.Hardshrink.forward(self,input)
torch.nn.Hardtanh(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.Hardtanh.extra_repr(self)
torch.nn.Hardtanh.forward(self,input)
torch.nn.LeakyReLU(self,negative_slope=0.01,inplace=False)
torch.nn.LeakyReLU.extra_repr(self)
torch.nn.LeakyReLU.forward(self,input)
torch.nn.LogSigmoid(Module)
torch.nn.LogSigmoid.forward(self,input)
torch.nn.LogSoftmax(self,dim=None)
torch.nn.LogSoftmax.__setstate__(self,state)
torch.nn.LogSoftmax.forward(self,input)
torch.nn.MultiheadAttention(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)
torch.nn.MultiheadAttention._reset_parameters(self)
torch.nn.MultiheadAttention.forward(self,query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)
torch.nn.PReLU(self,num_parameters=1,init=0.25)
torch.nn.PReLU.extra_repr(self)
torch.nn.PReLU.forward(self,input)
torch.nn.RReLU(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.RReLU.extra_repr(self)
torch.nn.RReLU.forward(self,input)
torch.nn.ReLU(self,inplace=False)
torch.nn.ReLU.extra_repr(self)
torch.nn.ReLU.forward(self,input)
torch.nn.ReLU6(self,inplace=False)
torch.nn.ReLU6.extra_repr(self)
torch.nn.SELU(self,inplace=False)
torch.nn.SELU.extra_repr(self)
torch.nn.SELU.forward(self,input)
torch.nn.Sigmoid(Module)
torch.nn.Sigmoid.forward(self,input)
torch.nn.Softmax(self,dim=None)
torch.nn.Softmax.__setstate__(self,state)
torch.nn.Softmax.extra_repr(self)
torch.nn.Softmax.forward(self,input)
torch.nn.Softmax2d(Module)
torch.nn.Softmax2d.forward(self,input)
torch.nn.Softmin(self,dim=None)
torch.nn.Softmin.forward(self,input)
torch.nn.Softplus(self,beta=1,threshold=20)
torch.nn.Softplus.extra_repr(self)
torch.nn.Softplus.forward(self,input)
torch.nn.Softshrink(self,lambd=0.5)
torch.nn.Softshrink.extra_repr(self)
torch.nn.Softshrink.forward(self,input)
torch.nn.Softsign(Module)
torch.nn.Softsign.forward(self,input)
torch.nn.Tanh(Module)
torch.nn.Tanh.forward(self,input)
torch.nn.Tanhshrink(Module)
torch.nn.Tanhshrink.forward(self,input)
torch.nn.Threshold(self,threshold,value,inplace=False)
torch.nn.Threshold.extra_repr(self)
torch.nn.Threshold.forward(self,input)
torch.nn.modules.activation.CELU(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.CELU.__init__(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.CELU.extra_repr(self)
torch.nn.modules.activation.CELU.forward(self,input)
torch.nn.modules.activation.ELU(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.ELU.__init__(self,alpha=1.0,inplace=False)
torch.nn.modules.activation.ELU.extra_repr(self)
torch.nn.modules.activation.ELU.forward(self,input)
torch.nn.modules.activation.GLU(self,dim=-1)
torch.nn.modules.activation.GLU.__init__(self,dim=-1)
torch.nn.modules.activation.GLU.extra_repr(self)
torch.nn.modules.activation.GLU.forward(self,input)
torch.nn.modules.activation.Hardshrink(self,lambd=0.5)
torch.nn.modules.activation.Hardshrink.__init__(self,lambd=0.5)
torch.nn.modules.activation.Hardshrink.extra_repr(self)
torch.nn.modules.activation.Hardshrink.forward(self,input)
torch.nn.modules.activation.Hardtanh(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.modules.activation.Hardtanh.__init__(self,min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)
torch.nn.modules.activation.Hardtanh.extra_repr(self)
torch.nn.modules.activation.Hardtanh.forward(self,input)
torch.nn.modules.activation.LeakyReLU(self,negative_slope=0.01,inplace=False)
torch.nn.modules.activation.LeakyReLU.__init__(self,negative_slope=0.01,inplace=False)
torch.nn.modules.activation.LeakyReLU.extra_repr(self)
torch.nn.modules.activation.LeakyReLU.forward(self,input)
torch.nn.modules.activation.LogSigmoid(Module)
torch.nn.modules.activation.LogSigmoid.forward(self,input)
torch.nn.modules.activation.LogSoftmax(self,dim=None)
torch.nn.modules.activation.LogSoftmax.__init__(self,dim=None)
torch.nn.modules.activation.LogSoftmax.__setstate__(self,state)
torch.nn.modules.activation.LogSoftmax.forward(self,input)
torch.nn.modules.activation.MultiheadAttention(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)
torch.nn.modules.activation.MultiheadAttention.__init__(self,embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)
torch.nn.modules.activation.MultiheadAttention._reset_parameters(self)
torch.nn.modules.activation.MultiheadAttention.forward(self,query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)
torch.nn.modules.activation.PReLU(self,num_parameters=1,init=0.25)
torch.nn.modules.activation.PReLU.__init__(self,num_parameters=1,init=0.25)
torch.nn.modules.activation.PReLU.extra_repr(self)
torch.nn.modules.activation.PReLU.forward(self,input)
torch.nn.modules.activation.RReLU(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.modules.activation.RReLU.__init__(self,lower=1.0/8,upper=1.0/3,inplace=False)
torch.nn.modules.activation.RReLU.extra_repr(self)
torch.nn.modules.activation.RReLU.forward(self,input)
torch.nn.modules.activation.ReLU(self,inplace=False)
torch.nn.modules.activation.ReLU.__init__(self,inplace=False)
torch.nn.modules.activation.ReLU.extra_repr(self)
torch.nn.modules.activation.ReLU.forward(self,input)
torch.nn.modules.activation.ReLU6(self,inplace=False)
torch.nn.modules.activation.ReLU6.__init__(self,inplace=False)
torch.nn.modules.activation.ReLU6.extra_repr(self)
torch.nn.modules.activation.SELU(self,inplace=False)
torch.nn.modules.activation.SELU.__init__(self,inplace=False)
torch.nn.modules.activation.SELU.extra_repr(self)
torch.nn.modules.activation.SELU.forward(self,input)
torch.nn.modules.activation.Sigmoid(Module)
torch.nn.modules.activation.Sigmoid.forward(self,input)
torch.nn.modules.activation.Softmax(self,dim=None)
torch.nn.modules.activation.Softmax.__init__(self,dim=None)
torch.nn.modules.activation.Softmax.__setstate__(self,state)
torch.nn.modules.activation.Softmax.extra_repr(self)
torch.nn.modules.activation.Softmax.forward(self,input)
torch.nn.modules.activation.Softmax2d(Module)
torch.nn.modules.activation.Softmax2d.forward(self,input)
torch.nn.modules.activation.Softmin(self,dim=None)
torch.nn.modules.activation.Softmin.__init__(self,dim=None)
torch.nn.modules.activation.Softmin.forward(self,input)
torch.nn.modules.activation.Softplus(self,beta=1,threshold=20)
torch.nn.modules.activation.Softplus.__init__(self,beta=1,threshold=20)
torch.nn.modules.activation.Softplus.extra_repr(self)
torch.nn.modules.activation.Softplus.forward(self,input)
torch.nn.modules.activation.Softshrink(self,lambd=0.5)
torch.nn.modules.activation.Softshrink.__init__(self,lambd=0.5)
torch.nn.modules.activation.Softshrink.extra_repr(self)
torch.nn.modules.activation.Softshrink.forward(self,input)
torch.nn.modules.activation.Softsign(Module)
torch.nn.modules.activation.Softsign.forward(self,input)
torch.nn.modules.activation.Tanh(Module)
torch.nn.modules.activation.Tanh.forward(self,input)
torch.nn.modules.activation.Tanhshrink(Module)
torch.nn.modules.activation.Tanhshrink.forward(self,input)
torch.nn.modules.activation.Threshold(self,threshold,value,inplace=False)
torch.nn.modules.activation.Threshold.__init__(self,threshold,value,inplace=False)
torch.nn.modules.activation.Threshold.extra_repr(self)
torch.nn.modules.activation.Threshold.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/activation.pyi----------------------------------------
torch.nn.modules.activation.LogSigmoid.__call__(self,input:Tensor)
torch.nn.modules.activation.Sigmoid.__call__(self,input:Tensor)
torch.nn.modules.activation.Softmax2d.__call__(self,input:Tensor)
torch.nn.modules.activation.Softsign.__call__(self,input:Tensor)
torch.nn.modules.activation.Tanh.__call__(self,input:Tensor)
torch.nn.modules.activation.Tanhshrink.__call__(self,input:Tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/adaptive.py----------------------------------------
A:torch.nn.modules.adaptive._ASMoutput->namedtuple('ASMoutput', ['output', 'loss'])
A:torch.nn.modules.adaptive.cutoffs->list(cutoffs)
A:torch.nn.modules.adaptive.self.head->Linear(self.in_features, self.head_size, bias=self.head_bias)
A:torch.nn.modules.adaptive.self.tail->ModuleList()
A:torch.nn.modules.adaptive.hsz->int(self.in_features // self.div_value ** (i + 1))
A:torch.nn.modules.adaptive.projection->Sequential(Linear(self.in_features, hsz, bias=False), Linear(hsz, osz, bias=False))
A:torch.nn.modules.adaptive.batch_size->target.size(0)
A:torch.nn.modules.adaptive.output->torch.argmax(head_output, dim=1)
A:torch.nn.modules.adaptive.gather_inds->target.new_empty(batch_size)
A:torch.nn.modules.adaptive.row_indices->target_mask.nonzero().squeeze()
A:torch.nn.modules.adaptive.input_subset->input.index_select(0, row_indices)
A:torch.nn.modules.adaptive.cluster_output->self.tail[i](input)
A:torch.nn.modules.adaptive.cluster_logprob->log_softmax(cluster_output, dim=1)
A:torch.nn.modules.adaptive.local_logprob->log_softmax(cluster_output, dim=1).gather(1, relative_target.unsqueeze(1))
A:torch.nn.modules.adaptive.head_output->self.head(input)
A:torch.nn.modules.adaptive.head_logprob->log_softmax(head_output, dim=1)
A:torch.nn.modules.adaptive.loss->(-output).mean()
A:torch.nn.modules.adaptive.out->input.new_empty((head_output.size(0), self.n_classes))
A:torch.nn.modules.adaptive.log_prob->self._get_full_log_prob(input[not_in_shortlist], head_output[not_in_shortlist])
A:torch.nn.modules.adaptive.output[not_in_shortlist]->torch.argmax(log_prob, dim=1)
torch.nn.AdaptiveLogSoftmaxWithLoss(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.AdaptiveLogSoftmaxWithLoss.forward(self,input,target)
torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob(self,input)
torch.nn.AdaptiveLogSoftmaxWithLoss.predict(self,input)
torch.nn.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.__init__(self,in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss._get_full_log_prob(self,input,head_output)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.forward(self,input,target)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.log_prob(self,input)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.predict(self,input)
torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.reset_parameters(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/adaptive.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/_functions.py----------------------------------------
A:torch.nn.modules._functions.input->input.contiguous().contiguous()
A:torch.nn.modules._functions.count->torch.Tensor([input.numel() // input.size(1)]).to(input.device)
A:torch.nn.modules._functions.(mean, invstd)->torch.batch_norm_gather_stats_with_counts(input, mean_all, invstd_all, running_mean, running_var, momentum, eps, count_all.view(-1).long().tolist())
A:torch.nn.modules._functions.count_all->torch.empty(world_size, 1, dtype=count.dtype, device=count.device)
A:torch.nn.modules._functions.mean_all->torch.empty(world_size, mean.size(0), dtype=mean.dtype, device=mean.device)
A:torch.nn.modules._functions.invstd_all->torch.empty(world_size, invstd.size(0), dtype=invstd.dtype, device=invstd.device)
A:torch.nn.modules._functions.count_l->list(count_all.unbind(0))
A:torch.nn.modules._functions.mean_l->list(mean_all.unbind(0))
A:torch.nn.modules._functions.invstd_l->list(invstd_all.unbind(0))
A:torch.nn.modules._functions.count_all_reduce->torch.distributed.all_gather(count_l, count, process_group, async_op=True)
A:torch.nn.modules._functions.mean_all_reduce->torch.distributed.all_gather(mean_l, mean, process_group, async_op=True)
A:torch.nn.modules._functions.invstd_all_reduce->torch.distributed.all_gather(invstd_l, invstd, process_group, async_op=True)
A:torch.nn.modules._functions.out->torch.batch_norm_elemt(input, weight, bias, mean, invstd, eps)
A:torch.nn.modules._functions.grad_output->grad_output.contiguous().contiguous()
A:torch.nn.modules._functions.(mean_dy, mean_dy_xmu, grad_weight, grad_bias)->torch.batch_norm_backward_reduce(grad_output, saved_input, mean, invstd, weight, self.needs_input_grad[0], self.needs_input_grad[1], self.needs_input_grad[2])
A:torch.nn.modules._functions.mean_dy_all_reduce->torch.distributed.all_reduce(mean_dy, torch.distributed.ReduceOp.SUM, process_group, async_op=True)
A:torch.nn.modules._functions.mean_dy_xmu_all_reduce->torch.distributed.all_reduce(mean_dy_xmu, torch.distributed.ReduceOp.SUM, process_group, async_op=True)
A:torch.nn.modules._functions.grad_input->grad_output.contiguous().contiguous().new()
A:torch.nn.modules._functions.output->input.contiguous().contiguous().new()
A:torch.nn.modules._functions.batch_size->input.contiguous().contiguous().size(0)
A:torch.nn.modules._functions.channels->input.contiguous().contiguous().size(1)
A:torch.nn.modules._functions.input_height->input.contiguous().contiguous().size(2)
A:torch.nn.modules._functions.input_width->input.contiguous().contiguous().size(3)
A:torch.nn.modules._functions.pre_pad->int((ctx.size - 1) / 2 + 1)
A:torch.nn.modules._functions.scale_first->ctx.scale.select(1, 0)
A:torch.nn.modules._functions.scale_previous->ctx.scale.select(1, c - 1)
A:torch.nn.modules._functions.scale_current->ctx.scale.select(1, c)
A:torch.nn.modules._functions.square_next->input_square.select(1, c + pre_pad - 1)
A:torch.nn.modules._functions.square_previous->input_square.select(1, c - pre_pad)
A:torch.nn.modules._functions.paddded_ratio->input.contiguous().contiguous().new(channels + ctx.size - 1, input_height, input_width)
A:torch.nn.modules._functions.accum_ratio->input.contiguous().contiguous().new(input_height, input_width)
A:torch.nn.modules._functions.inversePrePad->int(ctx.size - (ctx.size - 1) / 2)
A:torch.nn.modules._functions.padded_ratio_center->input.contiguous().contiguous().new(channels + ctx.size - 1, input_height, input_width).narrow(0, inversePrePad, channels)
torch.nn._functions.CrossMapLRN2d(Function)
torch.nn._functions.CrossMapLRN2d.backward(ctx,grad_output)
torch.nn._functions.CrossMapLRN2d.forward(ctx,input,size,alpha=0.0001,beta=0.75,k=1)
torch.nn._functions.SyncBatchNorm(Function)
torch.nn._functions.SyncBatchNorm.backward(self,grad_output)
torch.nn._functions.SyncBatchNorm.forward(self,input,weight,bias,running_mean,running_var,eps,momentum,process_group,world_size)
torch.nn.modules._functions.CrossMapLRN2d(Function)
torch.nn.modules._functions.CrossMapLRN2d.backward(ctx,grad_output)
torch.nn.modules._functions.CrossMapLRN2d.forward(ctx,input,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules._functions.SyncBatchNorm(Function)
torch.nn.modules._functions.SyncBatchNorm.backward(self,grad_output)
torch.nn.modules._functions.SyncBatchNorm.forward(self,input,weight,bias,running_mean,running_var,eps,momentum,process_group,world_size)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/dropout.py----------------------------------------
torch.nn.AlphaDropout(_DropoutNd)
torch.nn.AlphaDropout.forward(self,input)
torch.nn.Dropout(_DropoutNd)
torch.nn.Dropout.forward(self,input)
torch.nn.Dropout2d(_DropoutNd)
torch.nn.Dropout2d.forward(self,input)
torch.nn.Dropout3d(_DropoutNd)
torch.nn.Dropout3d.forward(self,input)
torch.nn.FeatureAlphaDropout(_DropoutNd)
torch.nn.FeatureAlphaDropout.forward(self,input)
torch.nn.dropout._DropoutNd(self,p=0.5,inplace=False)
torch.nn.dropout._DropoutNd.extra_repr(self)
torch.nn.modules.dropout.AlphaDropout(_DropoutNd)
torch.nn.modules.dropout.AlphaDropout.forward(self,input)
torch.nn.modules.dropout.Dropout(_DropoutNd)
torch.nn.modules.dropout.Dropout.forward(self,input)
torch.nn.modules.dropout.Dropout2d(_DropoutNd)
torch.nn.modules.dropout.Dropout2d.forward(self,input)
torch.nn.modules.dropout.Dropout3d(_DropoutNd)
torch.nn.modules.dropout.Dropout3d.forward(self,input)
torch.nn.modules.dropout.FeatureAlphaDropout(_DropoutNd)
torch.nn.modules.dropout.FeatureAlphaDropout.forward(self,input)
torch.nn.modules.dropout._DropoutNd(self,p=0.5,inplace=False)
torch.nn.modules.dropout._DropoutNd.__init__(self,p=0.5,inplace=False)
torch.nn.modules.dropout._DropoutNd.extra_repr(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/dropout.pyi----------------------------------------
torch.nn.modules.dropout.AlphaDropout.__call__(self,input:Tensor)
torch.nn.modules.dropout.Dropout.__call__(self,input:Tensor)
torch.nn.modules.dropout.Dropout2d.__call__(self,input:Tensor)
torch.nn.modules.dropout.Dropout3d.__call__(self,input:Tensor)
torch.nn.modules.dropout.FeatureAlphaDropout.__call__(self,input:Tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/module.py----------------------------------------
A:torch.nn.modules.module.s->'\n'.join(s)
A:torch.nn.modules.module.first->'\n'.join(s).pop(0)
A:torch.nn.modules.module.self._parameters->OrderedDict()
A:torch.nn.modules.module.self._buffers->OrderedDict()
A:torch.nn.modules.module.self._backward_hooks->OrderedDict()
A:torch.nn.modules.module.self._forward_hooks->OrderedDict()
A:torch.nn.modules.module.self._forward_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._state_dict_hooks->OrderedDict()
A:torch.nn.modules.module.self._load_state_dict_pre_hooks->OrderedDict()
A:torch.nn.modules.module.self._modules->OrderedDict()
A:torch.nn.modules.module.param_applied->fn(param)
A:torch.nn.modules.module.should_use_set_data->compute_should_use_set_data(param.grad, grad_applied)
A:torch.nn.modules.module.self._parameters[key]->Parameter(param_applied, param.requires_grad)
A:torch.nn.modules.module.grad_applied->fn(param.grad)
A:torch.nn.modules.module.self._parameters[key].grad->fn(param.grad).requires_grad_(param.grad.requires_grad)
A:torch.nn.modules.module.self._buffers[key]->fn(buf)
A:torch.nn.modules.module.(device, dtype, non_blocking)->torch._C._nn._parse_to(*args, **kwargs)
A:torch.nn.modules.module.handle->torch.utils.hooks.RemovableHandle(self._load_state_dict_pre_hooks)
A:torch.nn.modules.module.tracing_state->torch._C._get_tracing_state()
A:torch.nn.modules.module.name->self._tracing_name(tracing_state)
A:torch.nn.modules.module.result->self.forward(*input, **kwargs)
A:torch.nn.modules.module.hook_result->hook(self, destination, prefix, local_metadata)
A:torch.nn.modules.module.var->next((v for v in var.values() if isinstance(v, torch.Tensor)))
A:torch.nn.modules.module.wrapper->functools.partial(hook, self)
A:torch.nn.modules.module.params->self.__dict__.get('_parameters')
A:torch.nn.modules.module.modules->list(self._modules.keys())
A:torch.nn.modules.module.buffers->list(self._buffers.keys())
A:torch.nn.modules.module.destination->OrderedDict()
A:torch.nn.modules.module.destination._metadata->OrderedDict()
A:torch.nn.modules.module.destination._metadata[prefix[:-1]]local_metadata->dict(version=self._version)
A:torch.nn.modules.module.local_name_params->itertools.chain(self._parameters.items(), self._buffers.items())
A:torch.nn.modules.module.metadata->getattr(state_dict, '_metadata', None)
A:torch.nn.modules.module.state_dict->state_dict.copy().copy()
A:torch.nn.modules.module.memo->set()
A:torch.nn.modules.module.members->get_members_fn(module)
A:torch.nn.modules.module.gen->self._named_members(lambda module: module._buffers.items(), prefix=prefix, recurse=recurse)
A:torch.nn.modules.module.extra_repr->self.extra_repr()
A:torch.nn.modules.module.extra_lines->self.extra_repr().split('\n')
A:torch.nn.modules.module.mod_str->_addindent(mod_str, 2)
A:torch.nn.modules.module.module_attrs->dir(self.__class__)
A:torch.nn.modules.module.attrs->list(self.__dict__.keys())
A:torch.nn.modules.module.parameters->list(self._parameters.keys())
torch.nn.Module(self)
torch.nn.Module.__delattr__(self,name)
torch.nn.Module.__dir__(self)
torch.nn.Module.__getattr__(self,name)
torch.nn.Module.__repr__(self)
torch.nn.Module.__setattr__(self,name,value)
torch.nn.Module.__setstate__(self,state)
torch.nn.Module._apply(self,fn)
torch.nn.Module._get_name(self)
torch.nn.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.Module._register_load_state_dict_pre_hook(self,hook)
torch.nn.Module._register_state_dict_hook(self,hook)
torch.nn.Module._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.Module._slow_forward(self,*input,**kwargs)
torch.nn.Module._tracing_name(self,tracing_state)
torch.nn.Module.add_module(self,name,module)
torch.nn.Module.apply(self,fn)
torch.nn.Module.buffers(self,recurse=True)
torch.nn.Module.children(self)
torch.nn.Module.cpu(self)
torch.nn.Module.cuda(self,device=None)
torch.nn.Module.double(self)
torch.nn.Module.eval(self)
torch.nn.Module.extra_repr(self)
torch.nn.Module.float(self)
torch.nn.Module.forward(self,*input)
torch.nn.Module.half(self)
torch.nn.Module.load_state_dict(self,state_dict,strict=True)
torch.nn.Module.modules(self)
torch.nn.Module.named_buffers(self,prefix='',recurse=True)
torch.nn.Module.named_children(self)
torch.nn.Module.named_modules(self,memo=None,prefix='')
torch.nn.Module.named_parameters(self,prefix='',recurse=True)
torch.nn.Module.parameters(self,recurse=True)
torch.nn.Module.register_backward_hook(self,hook)
torch.nn.Module.register_buffer(self,name,tensor)
torch.nn.Module.register_forward_hook(self,hook)
torch.nn.Module.register_forward_pre_hook(self,hook)
torch.nn.Module.register_parameter(self,name,param)
torch.nn.Module.requires_grad_(self,requires_grad=True)
torch.nn.Module.share_memory(self)
torch.nn.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.Module.to(self,*args,**kwargs)
torch.nn.Module.train(self,mode=True)
torch.nn.Module.type(self,dst_type)
torch.nn.Module.zero_grad(self)
torch.nn.module._IncompatibleKeys(namedtuple('IncompatibleKeys',['missing_keys','unexpected_keys']))
torch.nn.module._IncompatibleKeys.__repr__(self)
torch.nn.module._addindent(s_,numSpaces)
torch.nn.modules.module.Module(self)
torch.nn.modules.module.Module.__delattr__(self,name)
torch.nn.modules.module.Module.__dir__(self)
torch.nn.modules.module.Module.__getattr__(self,name)
torch.nn.modules.module.Module.__init__(self)
torch.nn.modules.module.Module.__repr__(self)
torch.nn.modules.module.Module.__setattr__(self,name,value)
torch.nn.modules.module.Module.__setstate__(self,state)
torch.nn.modules.module.Module._apply(self,fn)
torch.nn.modules.module.Module._get_name(self)
torch.nn.modules.module.Module._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
torch.nn.modules.module.Module._named_members(self,get_members_fn,prefix='',recurse=True)
torch.nn.modules.module.Module._register_load_state_dict_pre_hook(self,hook)
torch.nn.modules.module.Module._register_state_dict_hook(self,hook)
torch.nn.modules.module.Module._save_to_state_dict(self,destination,prefix,keep_vars)
torch.nn.modules.module.Module._slow_forward(self,*input,**kwargs)
torch.nn.modules.module.Module._tracing_name(self,tracing_state)
torch.nn.modules.module.Module.add_module(self,name,module)
torch.nn.modules.module.Module.apply(self,fn)
torch.nn.modules.module.Module.buffers(self,recurse=True)
torch.nn.modules.module.Module.children(self)
torch.nn.modules.module.Module.cpu(self)
torch.nn.modules.module.Module.cuda(self,device=None)
torch.nn.modules.module.Module.double(self)
torch.nn.modules.module.Module.eval(self)
torch.nn.modules.module.Module.extra_repr(self)
torch.nn.modules.module.Module.float(self)
torch.nn.modules.module.Module.forward(self,*input)
torch.nn.modules.module.Module.half(self)
torch.nn.modules.module.Module.load_state_dict(self,state_dict,strict=True)
torch.nn.modules.module.Module.modules(self)
torch.nn.modules.module.Module.named_buffers(self,prefix='',recurse=True)
torch.nn.modules.module.Module.named_children(self)
torch.nn.modules.module.Module.named_modules(self,memo=None,prefix='')
torch.nn.modules.module.Module.named_parameters(self,prefix='',recurse=True)
torch.nn.modules.module.Module.parameters(self,recurse=True)
torch.nn.modules.module.Module.register_backward_hook(self,hook)
torch.nn.modules.module.Module.register_buffer(self,name,tensor)
torch.nn.modules.module.Module.register_forward_hook(self,hook)
torch.nn.modules.module.Module.register_forward_pre_hook(self,hook)
torch.nn.modules.module.Module.register_parameter(self,name,param)
torch.nn.modules.module.Module.requires_grad_(self,requires_grad=True)
torch.nn.modules.module.Module.share_memory(self)
torch.nn.modules.module.Module.state_dict(self,destination=None,prefix='',keep_vars=False)
torch.nn.modules.module.Module.to(self,*args,**kwargs)
torch.nn.modules.module.Module.train(self,mode=True)
torch.nn.modules.module.Module.type(self,dst_type)
torch.nn.modules.module.Module.zero_grad(self)
torch.nn.modules.module._IncompatibleKeys(namedtuple('IncompatibleKeys',['missing_keys','unexpected_keys']))
torch.nn.modules.module._IncompatibleKeys.__repr__(self)
torch.nn.modules.module._addindent(s_,numSpaces)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/module.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/conv.py----------------------------------------
A:torch.nn.modules.conv.self.weight->Parameter(torch.Tensor(out_channels, in_channels // groups, *kernel_size))
A:torch.nn.modules.conv.self.bias->Parameter(torch.Tensor(out_channels))
A:torch.nn.modules.conv.(fan_in, _)->init._calculate_fan_in_and_fan_out(self.weight)
A:torch.nn.modules.conv.kernel_size->_triple(kernel_size)
A:torch.nn.modules.conv.stride->_triple(stride)
A:torch.nn.modules.conv.padding->_triple(padding)
A:torch.nn.modules.conv.dilation->_triple(dilation)
A:torch.nn.modules.conv.ret->_single(self.output_padding)
A:torch.nn.modules.conv.min_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.max_sizes->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.res->torch.jit.annotate(List[int], [])
A:torch.nn.modules.conv.output_padding->self._output_padding(input, output_size, self.stride, self.padding, self.kernel_size)
torch.nn.Conv1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.Conv1d.forward(self,input)
torch.nn.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.Conv2d.conv2d_forward(self,input,weight)
torch.nn.Conv2d.forward(self,input)
torch.nn.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.Conv3d.forward(self,input)
torch.nn.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.ConvTranspose1d.forward(self,input,output_size=None)
torch.nn.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.ConvTranspose2d.forward(self,input,output_size=None)
torch.nn.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.ConvTranspose3d.forward(self,input,output_size=None)
torch.nn.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.conv._ConvNd.__setstate__(self,state)
torch.nn.conv._ConvNd.extra_repr(self)
torch.nn.conv._ConvNd.reset_parameters(self)
torch.nn.conv._ConvTransposeMixin(object)
torch.nn.conv._ConvTransposeMixin._output_padding(self,input,output_size,stride,padding,kernel_size)
torch.nn.modules.conv.Conv1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv1d.forward(self,input)
torch.nn.modules.conv.Conv2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv2d.conv2d_forward(self,input,weight)
torch.nn.modules.conv.Conv2d.forward(self,input)
torch.nn.modules.conv.Conv3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')
torch.nn.modules.conv.Conv3d.forward(self,input)
torch.nn.modules.conv.ConvTranspose1d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose1d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose1d.forward(self,input,output_size=None)
torch.nn.modules.conv.ConvTranspose2d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose2d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose2d.forward(self,input,output_size=None)
torch.nn.modules.conv.ConvTranspose3d(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose3d.__init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')
torch.nn.modules.conv.ConvTranspose3d.forward(self,input,output_size=None)
torch.nn.modules.conv._ConvNd(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.modules.conv._ConvNd.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,transposed,output_padding,groups,bias,padding_mode)
torch.nn.modules.conv._ConvNd.__setstate__(self,state)
torch.nn.modules.conv._ConvNd.extra_repr(self)
torch.nn.modules.conv._ConvNd.reset_parameters(self)
torch.nn.modules.conv._ConvTransposeMixin(object)
torch.nn.modules.conv._ConvTransposeMixin._output_padding(self,input,output_size,stride,padding,kernel_size)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/conv.pyi----------------------------------------
torch.nn.conv._ConvTransposeMixin.forward(self,input:Tensor,output_size:Optional[List[int]]=...)
torch.nn.modules.conv._ConvTransposeMixin.__call__(self,input:Tensor,output_size:Optional[List[int]]=...)
torch.nn.modules.conv._ConvTransposeMixin.forward(self,input:Tensor,output_size:Optional[List[int]]=...)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/padding.py----------------------------------------
A:torch.nn.modules.padding.self.padding->_ntuple(6)(padding)
torch.nn.ConstantPad1d(self,padding,value)
torch.nn.ConstantPad2d(self,padding,value)
torch.nn.ConstantPad3d(self,padding,value)
torch.nn.ReflectionPad1d(self,padding)
torch.nn.ReflectionPad2d(self,padding)
torch.nn.ReplicationPad1d(self,padding)
torch.nn.ReplicationPad2d(self,padding)
torch.nn.ReplicationPad3d(self,padding)
torch.nn.ZeroPad2d(self,padding)
torch.nn.modules.padding.ConstantPad1d(self,padding,value)
torch.nn.modules.padding.ConstantPad1d.__init__(self,padding,value)
torch.nn.modules.padding.ConstantPad2d(self,padding,value)
torch.nn.modules.padding.ConstantPad2d.__init__(self,padding,value)
torch.nn.modules.padding.ConstantPad3d(self,padding,value)
torch.nn.modules.padding.ConstantPad3d.__init__(self,padding,value)
torch.nn.modules.padding.ReflectionPad1d(self,padding)
torch.nn.modules.padding.ReflectionPad1d.__init__(self,padding)
torch.nn.modules.padding.ReflectionPad2d(self,padding)
torch.nn.modules.padding.ReflectionPad2d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad1d(self,padding)
torch.nn.modules.padding.ReplicationPad1d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad2d(self,padding)
torch.nn.modules.padding.ReplicationPad2d.__init__(self,padding)
torch.nn.modules.padding.ReplicationPad3d(self,padding)
torch.nn.modules.padding.ReplicationPad3d.__init__(self,padding)
torch.nn.modules.padding.ZeroPad2d(self,padding)
torch.nn.modules.padding.ZeroPad2d.__init__(self,padding)
torch.nn.modules.padding._ConstantPadNd(self,value)
torch.nn.modules.padding._ConstantPadNd.__init__(self,value)
torch.nn.modules.padding._ConstantPadNd.extra_repr(self)
torch.nn.modules.padding._ConstantPadNd.forward(self,input)
torch.nn.modules.padding._ReflectionPadNd(Module)
torch.nn.modules.padding._ReflectionPadNd.extra_repr(self)
torch.nn.modules.padding._ReflectionPadNd.forward(self,input)
torch.nn.modules.padding._ReplicationPadNd(Module)
torch.nn.modules.padding._ReplicationPadNd.extra_repr(self)
torch.nn.modules.padding._ReplicationPadNd.forward(self,input)
torch.nn.padding._ConstantPadNd(self,value)
torch.nn.padding._ConstantPadNd.extra_repr(self)
torch.nn.padding._ConstantPadNd.forward(self,input)
torch.nn.padding._ReflectionPadNd(Module)
torch.nn.padding._ReflectionPadNd.extra_repr(self)
torch.nn.padding._ReflectionPadNd.forward(self,input)
torch.nn.padding._ReplicationPadNd(Module)
torch.nn.padding._ReplicationPadNd.extra_repr(self)
torch.nn.padding._ReplicationPadNd.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/padding.pyi----------------------------------------
torch.nn.modules.padding._ReflectionPadNd.__call__(self,input:Tensor)
torch.nn.modules.padding._ReplicationPadNd.__call__(self,input:Tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/normalization.py----------------------------------------
A:torch.nn.modules.normalization.self.normalized_shape->tuple(normalized_shape)
A:torch.nn.modules.normalization.self.weight->Parameter(torch.Tensor(num_channels))
A:torch.nn.modules.normalization.self.bias->Parameter(torch.Tensor(num_channels))
torch.nn.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.CrossMapLRN2d.extra_repr(self)
torch.nn.CrossMapLRN2d.forward(self,input)
torch.nn.GroupNorm(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.GroupNorm.extra_repr(self)
torch.nn.GroupNorm.forward(self,input)
torch.nn.GroupNorm.reset_parameters(self)
torch.nn.LayerNorm(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.LayerNorm.extra_repr(self)
torch.nn.LayerNorm.forward(self,input)
torch.nn.LayerNorm.reset_parameters(self)
torch.nn.LocalResponseNorm(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.LocalResponseNorm.extra_repr(self)
torch.nn.LocalResponseNorm.forward(self,input)
torch.nn.modules.normalization.CrossMapLRN2d(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules.normalization.CrossMapLRN2d.__init__(self,size,alpha=0.0001,beta=0.75,k=1)
torch.nn.modules.normalization.CrossMapLRN2d.extra_repr(self)
torch.nn.modules.normalization.CrossMapLRN2d.forward(self,input)
torch.nn.modules.normalization.GroupNorm(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.modules.normalization.GroupNorm.__init__(self,num_groups,num_channels,eps=1e-05,affine=True)
torch.nn.modules.normalization.GroupNorm.extra_repr(self)
torch.nn.modules.normalization.GroupNorm.forward(self,input)
torch.nn.modules.normalization.GroupNorm.reset_parameters(self)
torch.nn.modules.normalization.LayerNorm(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.modules.normalization.LayerNorm.__init__(self,normalized_shape,eps=1e-05,elementwise_affine=True)
torch.nn.modules.normalization.LayerNorm.extra_repr(self)
torch.nn.modules.normalization.LayerNorm.forward(self,input)
torch.nn.modules.normalization.LayerNorm.reset_parameters(self)
torch.nn.modules.normalization.LocalResponseNorm(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.modules.normalization.LocalResponseNorm.__init__(self,size,alpha=0.0001,beta=0.75,k=1.0)
torch.nn.modules.normalization.LocalResponseNorm.extra_repr(self)
torch.nn.modules.normalization.LocalResponseNorm.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/normalization.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/pixelshuffle.py----------------------------------------
torch.nn.PixelShuffle(self,upscale_factor)
torch.nn.PixelShuffle.extra_repr(self)
torch.nn.PixelShuffle.forward(self,input)
torch.nn.modules.pixelshuffle.PixelShuffle(self,upscale_factor)
torch.nn.modules.pixelshuffle.PixelShuffle.__init__(self,upscale_factor)
torch.nn.modules.pixelshuffle.PixelShuffle.extra_repr(self)
torch.nn.modules.pixelshuffle.PixelShuffle.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/pixelshuffle.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/fold.py----------------------------------------
torch.nn.Fold(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.Fold.extra_repr(self)
torch.nn.Fold.forward(self,input)
torch.nn.Unfold(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.Unfold.extra_repr(self)
torch.nn.Unfold.forward(self,input)
torch.nn.modules.fold.Fold(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Fold.__init__(self,output_size,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Fold.extra_repr(self)
torch.nn.modules.fold.Fold.forward(self,input)
torch.nn.modules.fold.Unfold(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Unfold.__init__(self,kernel_size,dilation=1,padding=0,stride=1)
torch.nn.modules.fold.Unfold.extra_repr(self)
torch.nn.modules.fold.Unfold.forward(self,input)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/fold.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/upsampling.py----------------------------------------
A:torch.nn.modules.upsampling.self.scale_factor->tuple((float(factor) for factor in scale_factor))
torch.nn.Upsample(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.Upsample.extra_repr(self)
torch.nn.Upsample.forward(self,input)
torch.nn.UpsamplingBilinear2d(self,size=None,scale_factor=None)
torch.nn.UpsamplingNearest2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.Upsample(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.modules.upsampling.Upsample.__init__(self,size=None,scale_factor=None,mode='nearest',align_corners=None)
torch.nn.modules.upsampling.Upsample.extra_repr(self)
torch.nn.modules.upsampling.Upsample.forward(self,input)
torch.nn.modules.upsampling.UpsamplingBilinear2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingBilinear2d.__init__(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingNearest2d(self,size=None,scale_factor=None)
torch.nn.modules.upsampling.UpsamplingNearest2d.__init__(self,size=None,scale_factor=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/nn/modules/upsampling.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/multiprocessing/__init__.py----------------------------------------
torch.multiprocessing.__init__.get_all_sharing_strategies()
torch.multiprocessing.__init__.get_sharing_strategy()
torch.multiprocessing.__init__.set_sharing_strategy(new_strategy)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/multiprocessing/queue.py----------------------------------------
A:torch.multiprocessing.queue.buf->self.recv_bytes()
A:torch.multiprocessing.queue.self._reader->ConnectionWrapper(self._reader)
A:torch.multiprocessing.queue.self._writer->ConnectionWrapper(self._writer)
torch.multiprocessing.Queue(self,*args,**kwargs)
torch.multiprocessing.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.SimpleQueue._make_methods(self)
torch.multiprocessing.queue.ConnectionWrapper(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.__getattr__(self,name)
torch.multiprocessing.queue.ConnectionWrapper.__init__(self,conn)
torch.multiprocessing.queue.ConnectionWrapper.recv(self)
torch.multiprocessing.queue.ConnectionWrapper.send(self,obj)
torch.multiprocessing.queue.Queue(self,*args,**kwargs)
torch.multiprocessing.queue.Queue.__init__(self,*args,**kwargs)
torch.multiprocessing.queue.SimpleQueue(multiprocessing.queues.SimpleQueue)
torch.multiprocessing.queue.SimpleQueue._make_methods(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/multiprocessing/spawn.py----------------------------------------
A:torch.multiprocessing.spawn.ready->multiprocessing.connection.wait(self.sentinels.keys(), timeout=timeout)
A:torch.multiprocessing.spawn.index->self.sentinels.pop(sentinel)
A:torch.multiprocessing.spawn.original_trace->self.error_queues[error_index].get()
A:torch.multiprocessing.spawn.mp->multiprocessing.get_context('spawn')
A:torch.multiprocessing.spawn.error_queue->multiprocessing.get_context('spawn').SimpleQueue()
A:torch.multiprocessing.spawn.process->multiprocessing.get_context('spawn').Process(target=_wrap, args=(fn, i, args, error_queue), daemon=daemon)
A:torch.multiprocessing.spawn.spawn_context->SpawnContext(processes, error_queues)
torch.multiprocessing.SpawnContext(self,processes,error_queues)
torch.multiprocessing.SpawnContext.join(self,timeout=None)
torch.multiprocessing.SpawnContext.pids(self)
torch.multiprocessing.spawn(fn,args=(),nprocs=1,join=True,daemon=False)
torch.multiprocessing.spawn.SpawnContext(self,processes,error_queues)
torch.multiprocessing.spawn.SpawnContext.__init__(self,processes,error_queues)
torch.multiprocessing.spawn.SpawnContext.join(self,timeout=None)
torch.multiprocessing.spawn.SpawnContext.pids(self)
torch.multiprocessing.spawn._python_version_check()
torch.multiprocessing.spawn._wrap(fn,i,args,error_queue)
torch.multiprocessing.spawn.spawn(fn,args=(),nprocs=1,join=True,daemon=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/multiprocessing/pool.py----------------------------------------
A:torch.multiprocessing.pool.self._inqueue->SimpleQueue()
A:torch.multiprocessing.pool.self._outqueue->SimpleQueue()
A:torch.multiprocessing.pool.w->self.Process(target=clean_worker, args=args)
A:torch.multiprocessing.pool.w.name->self.Process(target=clean_worker, args=args).name.replace('Process', 'PoolWorker')
torch.multiprocessing.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.Pool._repopulate_pool(self)
torch.multiprocessing.Pool._setup_queues(self)
torch.multiprocessing.pool.Pool(multiprocessing.pool.Pool)
torch.multiprocessing.pool.Pool._repopulate_pool(self)
torch.multiprocessing.pool.Pool._setup_queues(self)
torch.multiprocessing.pool.clean_worker(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/multiprocessing/reductions.py----------------------------------------
A:torch.multiprocessing.reductions.self.cdata->cls._new_shared_filename(manager, handle, size)._weak_ref()
A:torch.multiprocessing.reductions.self.lock->threading.Lock()
A:torch.multiprocessing.reductions.self.limit->max(128, live * 2)
A:torch.multiprocessing.reductions.shared_cache->SharedCache()
A:torch.multiprocessing.reductions.handle->event.ipc_handle()
A:torch.multiprocessing.reductions.t->torch.nn.parameter.Parameter(t)
A:torch.multiprocessing.reductions.storage->cls._new_shared_filename(manager, handle, size)
A:torch.multiprocessing.reductions.shared_cache[storage_handle, storage_offset_bytes]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.(device, handle, storage_size_bytes, storage_offset_bytes, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required)->cls._new_shared_filename(manager, handle, size)._share_cuda_()
A:torch.multiprocessing.reductions.tensor_offset->tensor.storage_offset()
A:torch.multiprocessing.reductions.shared_cache[handle]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.stat->os.fstat(fd)
A:torch.multiprocessing.reductions.storage_ref->SharedCache().get(key)
A:torch.multiprocessing.reductions.fd->multiprocessing.reduction.DupFd(fd).detach()
A:torch.multiprocessing.reductions.shared_cache[fd_id(fd)]->StorageWeakRef(storage)
A:torch.multiprocessing.reductions.metadata->cls._new_shared_filename(manager, handle, size)._share_filename_()
A:torch.multiprocessing.reductions.(fd, size)->cls._new_shared_filename(manager, handle, size)._share_fd_()
A:torch.multiprocessing.reductions.df->multiprocessing.reduction.DupFd(fd)
A:torch.multiprocessing.reductions.cache_key->fd_id(fd)
A:torch.multiprocessing.reductions.shared_cache[cache_key]->StorageWeakRef(storage)
torch.multiprocessing.init_reductions()
torch.multiprocessing.reductions.SharedCache(self)
torch.multiprocessing.reductions.SharedCache.__init__(self)
torch.multiprocessing.reductions.SharedCache.__setitem__(self,key,storage_ref)
torch.multiprocessing.reductions.SharedCache._after_fork(self)
torch.multiprocessing.reductions.SharedCache.free_dead_references(self)
torch.multiprocessing.reductions.StorageWeakRef(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.__del__(self)
torch.multiprocessing.reductions.StorageWeakRef.__init__(self,storage)
torch.multiprocessing.reductions.StorageWeakRef.expired(self)
torch.multiprocessing.reductions.fd_id(fd)
torch.multiprocessing.reductions.init_reductions()
torch.multiprocessing.reductions.rebuild_cuda_tensor(tensor_cls,tensor_size,tensor_stride,tensor_offset,storage_cls,storage_device,storage_handle,storage_size_bytes,storage_offset_bytes,requires_grad,ref_counter_handle,ref_counter_offset,event_handle,event_sync_required)
torch.multiprocessing.reductions.rebuild_event(device,handle)
torch.multiprocessing.reductions.rebuild_storage_empty(cls)
torch.multiprocessing.reductions.rebuild_storage_fd(cls,df,size)
torch.multiprocessing.reductions.rebuild_storage_filename(cls,manager,handle,size)
torch.multiprocessing.reductions.rebuild_tensor(cls,storage,metadata)
torch.multiprocessing.reductions.reduce_event(event)
torch.multiprocessing.reductions.reduce_storage(storage)
torch.multiprocessing.reductions.reduce_tensor(tensor)
torch.multiprocessing.reductions.storage_from_cache(cls,key)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/for_onnx/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/throughput_benchmark.py----------------------------------------
A:torch.utils.throughput_benchmark.self._benchmark->torch._C.ThroughputBenchmark(module)
A:torch.utils.throughput_benchmark.config->torch._C.BenchmarkConfig()
A:torch.utils.throughput_benchmark.c_stats->self._benchmark.benchmark(config)
torch.utils.ThroughputBenchmark(self,module)
torch.utils.ThroughputBenchmark.add_input(self,*args,**kwargs)
torch.utils.ThroughputBenchmark.benchmark(self,num_calling_threads=1,num_warmup_iters=10,num_iters=100)
torch.utils.ThroughputBenchmark.run_once(self,*args,**kwargs)
torch.utils.throughput_benchmark.ExecutionStats(self,c_stats,benchmark_config)
torch.utils.throughput_benchmark.ExecutionStats.__init__(self,c_stats,benchmark_config)
torch.utils.throughput_benchmark.ExecutionStats.__str__(self)
torch.utils.throughput_benchmark.ExecutionStats.iters_per_second(self)
torch.utils.throughput_benchmark.ExecutionStats.latency_avg_ms(self)
torch.utils.throughput_benchmark.ExecutionStats.num_iters(self)
torch.utils.throughput_benchmark.ExecutionStats.total_time_seconds(self)
torch.utils.throughput_benchmark.ThroughputBenchmark(self,module)
torch.utils.throughput_benchmark.ThroughputBenchmark.__init__(self,module)
torch.utils.throughput_benchmark.ThroughputBenchmark.add_input(self,*args,**kwargs)
torch.utils.throughput_benchmark.ThroughputBenchmark.benchmark(self,num_calling_threads=1,num_warmup_iters=10,num_iters=100)
torch.utils.throughput_benchmark.ThroughputBenchmark.run_once(self,*args,**kwargs)
torch.utils.throughput_benchmark.format_time(time_us=None,time_ms=None,time_s=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/checkpoint.py----------------------------------------
A:torch.utils.checkpoint.x->inp.detach()
A:torch.utils.checkpoint.fwd_gpu_devices->list(set((arg.get_device() for arg in args if isinstance(arg, torch.Tensor) and arg.is_cuda)))
A:torch.utils.checkpoint.ctx.fwd_cpu_state->torch.get_rng_state()
A:torch.utils.checkpoint.(ctx.fwd_gpu_devices, ctx.fwd_gpu_states)->get_device_states(*args)
A:torch.utils.checkpoint.outputs->ctx.run_function(*detached_inputs)
A:torch.utils.checkpoint.detached_inputs->detach_variable(inputs)
A:torch.utils.checkpoint.grads->tuple((inp.grad if isinstance(inp, torch.Tensor) else inp for inp in detached_inputs))
A:torch.utils.checkpoint.preserve->kwargs.pop('preserve_rng_state', True)
A:torch.utils.checkpoint.inputs->checkpoint(run_function(start, end, functions), *inputs, preserve_rng_state=preserve)
A:torch.utils.checkpoint.functions->list(functions.children())
torch.utils.checkpoint.CheckpointFunction(torch.autograd.Function)
torch.utils.checkpoint.CheckpointFunction.backward(ctx,*args)
torch.utils.checkpoint.CheckpointFunction.forward(ctx,run_function,preserve_rng_state,*args)
torch.utils.checkpoint.check_backward_validity(inputs)
torch.utils.checkpoint.checkpoint(function,*args,**kwargs)
torch.utils.checkpoint.checkpoint_sequential(functions,segments,*inputs,**kwargs)
torch.utils.checkpoint.detach_variable(inputs)
torch.utils.checkpoint.get_device_states(*args)
torch.utils.checkpoint.set_device_states(devices,states)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/file_baton.py----------------------------------------
A:torch.utils.file_baton.self.fd->os.open(self.lock_file_path, os.O_CREAT | os.O_EXCL)
torch.utils.file_baton.FileBaton(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.__init__(self,lock_file_path,wait_seconds=0.1)
torch.utils.file_baton.FileBaton.release(self)
torch.utils.file_baton.FileBaton.try_acquire(self)
torch.utils.file_baton.FileBaton.wait(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/mkldnn.py----------------------------------------
A:torch.utils.mkldnn.self.weight->state[0].to_mkldnn()
A:torch.utils.mkldnn.self.bias->state[1].to_mkldnn()
A:torch.utils.mkldnn.y_mkldnn->torch._C._nn.mkldnn_linear(x_mkldnn, self.weight, self.bias)
A:torch.utils.mkldnn.weight->self.weight.to_dense()
A:torch.utils.mkldnn.bias->self.bias.to_dense()
A:torch.utils.mkldnn.running_mean->self.running_mean.to_dense()
A:torch.utils.mkldnn.running_var->self.running_var.to_dense()
A:torch.utils.mkldnn.self.running_mean->state[2].to_mkldnn()
A:torch.utils.mkldnn.self.running_var->state[3].to_mkldnn()
A:torch.utils.mkldnn.new_m->m_fn(m)
torch.utils.mkldnn.MkldnnBatchNorm2d(self,dense_module)
torch.utils.mkldnn.MkldnnBatchNorm2d.__getstate__(self)
torch.utils.mkldnn.MkldnnBatchNorm2d.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnBatchNorm2d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnBatchNorm2d.forward(self,x)
torch.utils.mkldnn.MkldnnConv2d(self,dense_module)
torch.utils.mkldnn.MkldnnConv2d.__getstate__(self)
torch.utils.mkldnn.MkldnnConv2d.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnConv2d.__setstate__(self,state)
torch.utils.mkldnn.MkldnnConv2d.forward(self,x)
torch.utils.mkldnn.MkldnnLinear(self,dense_module)
torch.utils.mkldnn.MkldnnLinear.__getstate__(self)
torch.utils.mkldnn.MkldnnLinear.__init__(self,dense_module)
torch.utils.mkldnn.MkldnnLinear.__setstate__(self,state)
torch.utils.mkldnn.MkldnnLinear.forward(self,x)
torch.utils.mkldnn.to_mkldnn(module)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/collect_env.py----------------------------------------
A:torch.utils.collect_env.SystemEnv->namedtuple('SystemEnv', ['torch_version', 'is_debug_build', 'cuda_compiled_version', 'gcc_version', 'cmake_version', 'os', 'python_version', 'is_cuda_available', 'cuda_runtime_version', 'nvidia_driver_version', 'nvidia_gpu_models', 'cudnn_version', 'pip_version', 'pip_packages', 'conda_packages'])
A:torch.utils.collect_env.p->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
A:torch.utils.collect_env.(output, err)->subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()
A:torch.utils.collect_env.enc->locale.getpreferredencoding()
A:torch.utils.collect_env.output->get_pretty_env_info()
A:torch.utils.collect_env.err->err.decode(enc).decode(enc)
A:torch.utils.collect_env.(rc, out, _)->run_lambda(cudnn_cmd)
A:torch.utils.collect_env.match->re.search(regex, out)
A:torch.utils.collect_env.conda->os.environ.get('CONDA_EXE', 'conda')
A:torch.utils.collect_env.out->run_and_read_all(run_lambda, conda + ' list | ' + grep_cmd)
A:torch.utils.collect_env.comment_regex->re.compile('^#.*\\n')
A:torch.utils.collect_env.smi->get_nvidia_smi()
A:torch.utils.collect_env.uuid_regex->re.compile(' \\(UUID: .+?\\)')
A:torch.utils.collect_env.l->os.environ.get('CUDNN_LIBRARY')
A:torch.utils.collect_env.files->list(sorted(files))
A:torch.utils.collect_env.fn->os.path.realpath(fn)
A:torch.utils.collect_env.result->'\n'.join(files)
A:torch.utils.collect_env.platform->get_platform()
A:torch.utils.collect_env.version->get_mac_version(run_lambda)
A:torch.utils.collect_env.desc->check_release_file(run_lambda)
A:torch.utils.collect_env.out2->run_with_pip('pip')
A:torch.utils.collect_env.out3->run_with_pip('pip3')
A:torch.utils.collect_env.num_pips->len([x for x in [out2, out3] if x is not None])
A:torch.utils.collect_env.(pip_version, pip_list_output)->get_pip_packages(run_lambda)
A:torch.utils.collect_env.cuda_available_str->torch.cuda.is_available()
A:torch.utils.collect_env.env_info_fmt->'\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\n\nOS: {os}\nGCC version: {gcc_version}\nCMake version: {cmake_version}\n\nPython version: {python_version}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n'.strip()
A:torch.utils.collect_env.lines->text.split('\n')
A:torch.utils.collect_env.mutable_dict->replace_nones(mutable_dict)
A:torch.utils.collect_env.mutable_dict['nvidia_gpu_models']->maybe_start_on_next_line(envinfo.nvidia_gpu_models)
A:torch.utils.collect_env.all_dynamic_cuda_fields_missing->all((mutable_dict[field] is None for field in dynamic_cuda_fields))
A:torch.utils.collect_env.mutable_dict['pip_packages']->prepend(mutable_dict['pip_packages'], '[{}] '.format(envinfo.pip_version))
A:torch.utils.collect_env.mutable_dict['conda_packages']->prepend(mutable_dict['conda_packages'], '[conda] ')
torch.utils.collect_env.check_release_file(run_lambda)
torch.utils.collect_env.get_cmake_version(run_lambda)
torch.utils.collect_env.get_conda_packages(run_lambda)
torch.utils.collect_env.get_cudnn_version(run_lambda)
torch.utils.collect_env.get_env_info()
torch.utils.collect_env.get_gcc_version(run_lambda)
torch.utils.collect_env.get_gpu_info(run_lambda)
torch.utils.collect_env.get_lsb_version(run_lambda)
torch.utils.collect_env.get_mac_version(run_lambda)
torch.utils.collect_env.get_nvidia_driver_version(run_lambda)
torch.utils.collect_env.get_nvidia_smi()
torch.utils.collect_env.get_os(run_lambda)
torch.utils.collect_env.get_pip_packages(run_lambda)
torch.utils.collect_env.get_platform()
torch.utils.collect_env.get_pretty_env_info()
torch.utils.collect_env.get_running_cuda_version(run_lambda)
torch.utils.collect_env.get_windows_version(run_lambda)
torch.utils.collect_env.main()
torch.utils.collect_env.pretty_str(envinfo)
torch.utils.collect_env.run(command)
torch.utils.collect_env.run_and_parse_first_match(run_lambda,command,regex)
torch.utils.collect_env.run_and_read_all(run_lambda,command)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/dlpack.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/_cpp_extension_versioner.py----------------------------------------
A:torch.utils._cpp_extension_versioner.Entry->collections.namedtuple('Entry', 'version, hash')
A:torch.utils._cpp_extension_versioner.hash_value->update_hash(hash_value, with_cuda)
A:torch.utils._cpp_extension_versioner.entry->self.entries.get(name)
A:torch.utils._cpp_extension_versioner.self.entries[name]entry->Entry(entry.version + 1, hash_value)
torch.utils._cpp_extension_versioner.ExtensionVersioner(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.__init__(self)
torch.utils._cpp_extension_versioner.ExtensionVersioner.bump_version_if_changed(self,name,source_files,build_arguments,build_directory,with_cuda)
torch.utils._cpp_extension_versioner.ExtensionVersioner.get_version(self,name)
torch.utils._cpp_extension_versioner.hash_build_arguments(hash_value,build_arguments)
torch.utils._cpp_extension_versioner.hash_source_files(hash_value,source_files)
torch.utils._cpp_extension_versioner.update_hash(seed,value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/cpp_extension.py----------------------------------------
A:torch.utils.cpp_extension.nvcc->_join_cuda_home('bin', 'nvcc')
A:torch.utils.cpp_extension.cuda_home->os.path.dirname(os.path.dirname(nvcc))
A:torch.utils.cpp_extension.cuda_homes->glob.glob('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')
A:torch.utils.cpp_extension.CUDA_HOME->_find_cuda_home()
A:torch.utils.cpp_extension.BUILT_FROM_SOURCE_VERSION_PATTERN->re.compile('\\d+\\.\\d+\\.\\d+\\w+\\+\\w+')
A:torch.utils.cpp_extension.JIT_EXTENSION_VERSIONER->ExtensionVersioner()
A:torch.utils.cpp_extension.which->subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.compiler_path->os.path.realpath(which.decode().strip())
A:torch.utils.cpp_extension.version->ExtensionVersioner().bump_version_if_changed(name, sources, build_arguments=[extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths], build_directory=build_directory, with_cuda=with_cuda)
A:torch.utils.cpp_extension.compiler_info->subprocess.check_output(compiler, stderr=subprocess.STDOUT)
A:torch.utils.cpp_extension.match->re.search('(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip())
A:torch.utils.cpp_extension.(_, error, _)->sys.exc_info()
A:torch.utils.cpp_extension.compiler->os.environ.get('CXX', 'c++')
A:torch.utils.cpp_extension.kwargs->kwargs.copy().copy()
A:torch.utils.cpp_extension.self.no_python_abi_suffix->kwargs.copy().copy().get('no_python_abi_suffix', False)
A:torch.utils.cpp_extension.cflags->_nt_quote_args(cflags)
A:torch.utils.cpp_extension.self.cflags->copy.deepcopy(extra_postargs)
A:torch.utils.cpp_extension.src_regex->re.compile('/T(p|c)(.*)')
A:torch.utils.cpp_extension.obj_regex->re.compile('/Fo(.*)')
A:torch.utils.cpp_extension.include_regex->re.compile('((\\-|\\/)I.*)')
A:torch.utils.cpp_extension.ext_filename->'.'.join(without_abi)
A:torch.utils.cpp_extension.ext_filename_parts->'.'.join(without_abi).split('.')
A:torch.utils.cpp_extension.extension.extra_compile_args->copy.deepcopy(extension.extra_compile_args)
A:torch.utils.cpp_extension.names->extension.name.split('.')
A:torch.utils.cpp_extension.define->'-DTORCH_EXTENSION_NAME={}'.format(name)
A:torch.utils.cpp_extension.include_dirs->kwargs.copy().copy().get('include_dirs', [])
A:torch.utils.cpp_extension.library_dirs->kwargs.copy().copy().get('library_dirs', [])
A:torch.utils.cpp_extension.libraries->kwargs.copy().copy().get('libraries', [])
A:torch.utils.cpp_extension.here->os.path.abspath(__file__)
A:torch.utils.cpp_extension.torch_path->os.path.dirname(os.path.dirname(here))
A:torch.utils.cpp_extension.lib_include->os.path.join(torch_path, 'include')
A:torch.utils.cpp_extension.cuda_home_include->_join_cuda_home('include')
A:torch.utils.cpp_extension.lib_path->os.path.join(torch_path, 'lib')
A:torch.utils.cpp_extension.functions->dict(((f, f) for f in functions))
A:torch.utils.cpp_extension.cpp_source_path->os.path.join(build_directory, 'main.cpp')
A:torch.utils.cpp_extension.cuda_source_path->os.path.join(build_directory, 'cuda.cu')
A:torch.utils.cpp_extension.old_version->ExtensionVersioner().get_version(name)
A:torch.utils.cpp_extension.name->'{}_v{}'.format(name, version)
A:torch.utils.cpp_extension.baton->FileBaton(os.path.join(build_directory, 'lock'))
A:torch.utils.cpp_extension.with_cuda->any(map(_is_cuda_file, sources))
A:torch.utils.cpp_extension.extra_ldflags->_prepare_ldflags(extra_ldflags or [], with_cuda, verbose)
A:torch.utils.cpp_extension.build_file_path->os.path.join(build_directory, 'build.ninja')
A:torch.utils.cpp_extension.python_path->os.path.dirname(sys.executable)
A:torch.utils.cpp_extension.python_lib_path->os.path.join(python_path, 'libs')
A:torch.utils.cpp_extension.named_arches->collections.OrderedDict([('Kepler+Tesla', '3.7'), ('Kepler', '3.5+PTX'), ('Maxwell+Tegra', '5.3'), ('Maxwell', '5.0;5.2+PTX'), ('Pascal', '6.0;6.1+PTX'), ('Volta', '7.0+PTX'), ('Turing', '7.5+PTX')])
A:torch.utils.cpp_extension.arch_list->arch_list.split(';').split(';')
A:torch.utils.cpp_extension.capability->torch.cuda.get_device_capability()
A:torch.utils.cpp_extension.root_extensions_directory->get_default_build_root()
A:torch.utils.cpp_extension.build_directory->os.path.join(root_extensions_directory, name)
A:torch.utils.cpp_extension.message->"Error building extension '{}'".format(name)
A:torch.utils.cpp_extension.(file, path, description)->imp.find_module(module_name, [path])
A:torch.utils.cpp_extension.system_includes->include_paths(with_cuda)
A:torch.utils.cpp_extension.cuda_flags->_nt_quote_args(cuda_flags)
A:torch.utils.cpp_extension.ldflags->_nt_quote_args(ldflags)
A:torch.utils.cpp_extension.cl_paths->subprocess.check_output(['where', 'cl']).decode().split('\r\n')
A:torch.utils.cpp_extension.cl_path->os.path.dirname(cl_paths[0]).replace(':', '$:')
A:torch.utils.cpp_extension.target->'{}.o'.format(file_name)
A:torch.utils.cpp_extension.source_file->source_file.replace(' ', '$ ').replace(' ', '$ ')
A:torch.utils.cpp_extension.library_target->'{}.{}'.format(name, ext)
A:torch.utils.cpp_extension.lines->'\n'.join(block)
torch.utils.cpp_extension.BuildExtension(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension.__init__(self,*args,**kwargs)
torch.utils.cpp_extension.BuildExtension._add_compile_flag(self,extension,flag)
torch.utils.cpp_extension.BuildExtension._add_gnu_cpp_abi_flag(self,extension)
torch.utils.cpp_extension.BuildExtension._check_abi(self)
torch.utils.cpp_extension.BuildExtension._define_torch_extension_name(self,extension)
torch.utils.cpp_extension.BuildExtension.build_extensions(self)
torch.utils.cpp_extension.BuildExtension.get_ext_filename(self,ext_name)
torch.utils.cpp_extension.BuildExtension.with_options(cls,**options)
torch.utils.cpp_extension.CUDAExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension.CppExtension(name,sources,*args,**kwargs)
torch.utils.cpp_extension._accepted_compilers_for_platform()
torch.utils.cpp_extension._build_extension_module(name,build_directory,verbose)
torch.utils.cpp_extension._find_cuda_home()
torch.utils.cpp_extension._get_build_directory(name,verbose)
torch.utils.cpp_extension._get_cuda_arch_flags(cflags=None)
torch.utils.cpp_extension._import_module_from_library(module_name,path,is_python_module)
torch.utils.cpp_extension._is_binary_build()
torch.utils.cpp_extension._is_cuda_file(path)
torch.utils.cpp_extension._jit_compile(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory,verbose,with_cuda,is_python_module)
torch.utils.cpp_extension._join_cuda_home(*paths)
torch.utils.cpp_extension._prepare_ldflags(extra_ldflags,with_cuda,verbose)
torch.utils.cpp_extension._write_ninja_file(path,name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,with_cuda)
torch.utils.cpp_extension._write_ninja_file_and_build(name,sources,extra_cflags,extra_cuda_cflags,extra_ldflags,extra_include_paths,build_directory,verbose,with_cuda)
torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)
torch.utils.cpp_extension.check_compiler_ok_for_platform(compiler)
torch.utils.cpp_extension.get_default_build_root()
torch.utils.cpp_extension.include_paths(cuda=False)
torch.utils.cpp_extension.library_paths(cuda=False)
torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)
torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)
torch.utils.cpp_extension.verify_ninja_availability()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/model_zoo.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/hooks.py----------------------------------------
A:torch.utils.hooks.self.hooks_dict_ref->weakref.ref(state[0])
A:torch.utils.hooks.hooks_dict->self.hooks_dict_ref()
A:torch.utils.hooks.RemovableHandle.next_id->max(RemovableHandle.next_id, self.id + 1)
torch.utils.hooks.RemovableHandle(self,hooks_dict)
torch.utils.hooks.RemovableHandle.__enter__(self)
torch.utils.hooks.RemovableHandle.__exit__(self,type,value,tb)
torch.utils.hooks.RemovableHandle.__getstate__(self)
torch.utils.hooks.RemovableHandle.__init__(self,hooks_dict)
torch.utils.hooks.RemovableHandle.__setstate__(self,state)
torch.utils.hooks.RemovableHandle.remove(self)
torch.utils.hooks.unserializable_hook(f)
torch.utils.hooks.warn_if_has_hooks(tensor)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/ffi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/backcompat/__init__.py----------------------------------------
A:torch.utils.backcompat.__init__.enabled->property(get_enabled, set_enabled)
A:torch.utils.backcompat.__init__.broadcast_warning->Warning(_set_backcompat_broadcast_warn, _get_backcompat_broadcast_warn)
A:torch.utils.backcompat.__init__.keepdim_warning->Warning(_set_backcompat_keepdim_warn, _get_backcompat_keepdim_warn)
torch.utils.backcompat.__init__.Warning(self,setter,getter)
torch.utils.backcompat.__init__.Warning.__init__(self,setter,getter)
torch.utils.backcompat.__init__.Warning.get_enabled(self)
torch.utils.backcompat.__init__.Warning.set_enabled(self,value)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py----------------------------------------
A:torch.utils.bottleneck.__main__.env_summary->run_env_analysis()
A:torch.utils.bottleneck.__main__.info->get_env_info()
A:torch.utils.bottleneck.__main__.prof->cProfile.Profile()
A:torch.utils.bottleneck.__main__.cprof_summary->'\n--------------------------------------------------------------------------------\n  cProfile output\n--------------------------------------------------------------------------------\n'.strip()
A:torch.utils.bottleneck.__main__.cprofile_stats->pstats.Stats(prof).sort_stats(sortby)
A:torch.utils.bottleneck.__main__.autograd_prof_summary->'\n--------------------------------------------------------------------------------\n  autograd profiler output ({mode} mode)\n--------------------------------------------------------------------------------\n        {description}\n{cuda_warning}\n{output}\n'.strip()
A:torch.utils.bottleneck.__main__.sorted_events->sorted(prof.function_events, key=lambda x: getattr(x, sortby), reverse=True)
A:torch.utils.bottleneck.__main__.descript->"\n`bottleneck` is a tool that can be used as an initial step for debugging\nbottlenecks in your program.\n\nIt summarizes runs of your script with the Python profiler and PyTorch's\nautograd profiler. Because your script will be profiled, please ensure that it\nexits in a finite amount of time.\n\nFor more complicated uses of the profilers, please see\nhttps://docs.python.org/3/library/profile.html and\nhttps://pytorch.org/docs/master/autograd.html#profiler for more information.\n".strip()
A:torch.utils.bottleneck.__main__.parser->argparse.ArgumentParser(description=descript)
A:torch.utils.bottleneck.__main__.args->parse_args()
A:torch.utils.bottleneck.__main__.code->compile(stream.read(), scriptfile, 'exec')
A:torch.utils.bottleneck.__main__.cprofile_prof->run_cprofile(code, globs)
A:torch.utils.bottleneck.__main__.(autograd_prof_cpu, autograd_prof_cuda)->run_autograd_prof(code, globs)
A:torch.utils.bottleneck.__main__.cuda_prof_exec_time->cpu_time_total(autograd_prof_cuda)
A:torch.utils.bottleneck.__main__.cpu_prof_exec_time->cpu_time_total(autograd_prof_cpu)
torch.utils.bottleneck.__main__.compiled_with_cuda(sysinfo)
torch.utils.bottleneck.__main__.cpu_time_total(autograd_prof)
torch.utils.bottleneck.__main__.main()
torch.utils.bottleneck.__main__.parse_args()
torch.utils.bottleneck.__main__.print_autograd_prof_summary(prof,mode,sortby='cpu_time',topk=15)
torch.utils.bottleneck.__main__.print_cprofile_summary(prof,sortby='tottime',topk=15)
torch.utils.bottleneck.__main__.redirect_argv(new_argv)
torch.utils.bottleneck.__main__.run_autograd_prof(code,globs)
torch.utils.bottleneck.__main__.run_cprofile(code,globs,launch_blocking=False)
torch.utils.bottleneck.__main__.run_env_analysis()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/bottleneck/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/__init__.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/sampler.py----------------------------------------
A:torch.utils.data.sampler.n->len(self.data_source)
A:torch.utils.data.sampler.self.weights->torch.as_tensor(weights, dtype=torch.double)
torch.utils.data.BatchSampler(self,sampler,batch_size,drop_last)
torch.utils.data.BatchSampler.__iter__(self)
torch.utils.data.BatchSampler.__len__(self)
torch.utils.data.RandomSampler(self,data_source,replacement=False,num_samples=None)
torch.utils.data.RandomSampler.__iter__(self)
torch.utils.data.RandomSampler.__len__(self)
torch.utils.data.RandomSampler.num_samples(self)
torch.utils.data.Sampler(self,data_source)
torch.utils.data.Sampler.__iter__(self)
torch.utils.data.SequentialSampler(self,data_source)
torch.utils.data.SequentialSampler.__iter__(self)
torch.utils.data.SequentialSampler.__len__(self)
torch.utils.data.SubsetRandomSampler(self,indices)
torch.utils.data.SubsetRandomSampler.__iter__(self)
torch.utils.data.SubsetRandomSampler.__len__(self)
torch.utils.data.WeightedRandomSampler(self,weights,num_samples,replacement=True)
torch.utils.data.WeightedRandomSampler.__iter__(self)
torch.utils.data.WeightedRandomSampler.__len__(self)
torch.utils.data.sampler.BatchSampler(self,sampler,batch_size,drop_last)
torch.utils.data.sampler.BatchSampler.__init__(self,sampler,batch_size,drop_last)
torch.utils.data.sampler.BatchSampler.__iter__(self)
torch.utils.data.sampler.BatchSampler.__len__(self)
torch.utils.data.sampler.RandomSampler(self,data_source,replacement=False,num_samples=None)
torch.utils.data.sampler.RandomSampler.__init__(self,data_source,replacement=False,num_samples=None)
torch.utils.data.sampler.RandomSampler.__iter__(self)
torch.utils.data.sampler.RandomSampler.__len__(self)
torch.utils.data.sampler.RandomSampler.num_samples(self)
torch.utils.data.sampler.Sampler(self,data_source)
torch.utils.data.sampler.Sampler.__init__(self,data_source)
torch.utils.data.sampler.Sampler.__iter__(self)
torch.utils.data.sampler.SequentialSampler(self,data_source)
torch.utils.data.sampler.SequentialSampler.__init__(self,data_source)
torch.utils.data.sampler.SequentialSampler.__iter__(self)
torch.utils.data.sampler.SequentialSampler.__len__(self)
torch.utils.data.sampler.SubsetRandomSampler(self,indices)
torch.utils.data.sampler.SubsetRandomSampler.__init__(self,indices)
torch.utils.data.sampler.SubsetRandomSampler.__iter__(self)
torch.utils.data.sampler.SubsetRandomSampler.__len__(self)
torch.utils.data.sampler.WeightedRandomSampler(self,weights,num_samples,replacement=True)
torch.utils.data.sampler.WeightedRandomSampler.__init__(self,weights,num_samples,replacement=True)
torch.utils.data.sampler.WeightedRandomSampler.__iter__(self)
torch.utils.data.sampler.WeightedRandomSampler.__len__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/sampler.pyi----------------------------------------
torch.utils.data.Sampler.__len__(self)->int
torch.utils.data.sampler.Sampler.__len__(self)->int


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/dataset.py----------------------------------------
A:torch.utils.data.dataset.l->len(e)
A:torch.utils.data.dataset.self.datasets->list(datasets)
A:torch.utils.data.dataset.self.cumulative_sizes->self.cumsum(self.datasets)
A:torch.utils.data.dataset.dataset_idx->bisect.bisect_right(self.cumulative_sizes, idx)
A:torch.utils.data.dataset.indices->randperm(sum(lengths)).tolist()
torch.utils.data.ChainDataset(self,datasets)
torch.utils.data.ChainDataset.__iter__(self)
torch.utils.data.ChainDataset.__len__(self)
torch.utils.data.ConcatDataset(self,datasets)
torch.utils.data.ConcatDataset.__getitem__(self,idx)
torch.utils.data.ConcatDataset.__len__(self)
torch.utils.data.ConcatDataset.cummulative_sizes(self)
torch.utils.data.ConcatDataset.cumsum(sequence)
torch.utils.data.Dataset(object)
torch.utils.data.Dataset.__add__(self,other)
torch.utils.data.Dataset.__getitem__(self,index)
torch.utils.data.IterableDataset(Dataset)
torch.utils.data.IterableDataset.__add__(self,other)
torch.utils.data.IterableDataset.__iter__(self)
torch.utils.data.Subset(self,dataset,indices)
torch.utils.data.Subset.__getitem__(self,idx)
torch.utils.data.Subset.__len__(self)
torch.utils.data.TensorDataset(self,*tensors)
torch.utils.data.TensorDataset.__getitem__(self,index)
torch.utils.data.TensorDataset.__len__(self)
torch.utils.data.dataset.ChainDataset(self,datasets)
torch.utils.data.dataset.ChainDataset.__init__(self,datasets)
torch.utils.data.dataset.ChainDataset.__iter__(self)
torch.utils.data.dataset.ChainDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset(self,datasets)
torch.utils.data.dataset.ConcatDataset.__getitem__(self,idx)
torch.utils.data.dataset.ConcatDataset.__init__(self,datasets)
torch.utils.data.dataset.ConcatDataset.__len__(self)
torch.utils.data.dataset.ConcatDataset.cummulative_sizes(self)
torch.utils.data.dataset.ConcatDataset.cumsum(sequence)
torch.utils.data.dataset.Dataset(object)
torch.utils.data.dataset.Dataset.__add__(self,other)
torch.utils.data.dataset.Dataset.__getitem__(self,index)
torch.utils.data.dataset.IterableDataset(Dataset)
torch.utils.data.dataset.IterableDataset.__add__(self,other)
torch.utils.data.dataset.IterableDataset.__iter__(self)
torch.utils.data.dataset.Subset(self,dataset,indices)
torch.utils.data.dataset.Subset.__getitem__(self,idx)
torch.utils.data.dataset.Subset.__init__(self,dataset,indices)
torch.utils.data.dataset.Subset.__len__(self)
torch.utils.data.dataset.TensorDataset(self,*tensors)
torch.utils.data.dataset.TensorDataset.__getitem__(self,index)
torch.utils.data.dataset.TensorDataset.__init__(self,*tensors)
torch.utils.data.dataset.TensorDataset.__len__(self)
torch.utils.data.dataset.random_split(dataset,lengths)
torch.utils.data.random_split(dataset,lengths)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/dataset.pyi----------------------------------------
torch.utils.data.Dataset.__len__(self)->int
torch.utils.data.dataset.Dataset.__len__(self)->int


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/dataloader.py----------------------------------------
A:torch.utils.data.dataloader.sampler->SequentialSampler(dataset)
A:torch.utils.data.dataloader.batch_sampler->BatchSampler(sampler, batch_size, drop_last)
A:torch.utils.data.dataloader.valid_start_methods->torch.multiprocessing.get_all_start_methods()
A:torch.utils.data.dataloader.multiprocessing_context->torch.multiprocessing.get_context(multiprocessing_context)
A:torch.utils.data.dataloader.self._sampler_iter->iter(self._index_sampler)
A:torch.utils.data.dataloader.self._base_seed->torch.empty((), dtype=torch.int64).random_().item()
A:torch.utils.data.dataloader.self._dataset_fetcher->_DatasetKind.create_fetcher(self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)
A:torch.utils.data.dataloader.index->self._next_index()
A:torch.utils.data.dataloader.data->self._data_queue.get(timeout=timeout)
A:torch.utils.data.dataloader.self._worker_queue_idx_cycle->itertools.cycle(range(self._num_workers))
A:torch.utils.data.dataloader.self._worker_result_queue->torch.multiprocessing.get_context(multiprocessing_context).Queue()
A:torch.utils.data.dataloader.self._workers_done_event->torch.multiprocessing.get_context(multiprocessing_context).Event()
A:torch.utils.data.dataloader.index_queue->torch.multiprocessing.get_context(multiprocessing_context).Queue()
A:torch.utils.data.dataloader.w->torch.multiprocessing.get_context(multiprocessing_context).Process(target=_utils.worker._worker_loop, args=(self._dataset_kind, self._dataset, index_queue, self._worker_result_queue, self._workers_done_event, self._auto_collation, self._collate_fn, self._drop_last, self._base_seed + i, self._worker_init_fn, i, self._num_workers))
A:torch.utils.data.dataloader.self._pin_memory_thread_done_event->threading.Event()
A:torch.utils.data.dataloader.self._data_queue->torch._six.queue.Queue()
A:torch.utils.data.dataloader.pin_memory_thread->threading.Thread(target=_utils.pin_memory._pin_memory_loop, args=(self._worker_result_queue, self._data_queue, torch.cuda.current_device(), self._pin_memory_thread_done_event))
A:torch.utils.data.dataloader.pids_str->', '.join((str(w.pid) for w in failed_workers))
A:torch.utils.data.dataloader.(success, data)->self._try_get_data()
A:torch.utils.data.dataloader.(idx, data)->self._get_data()
A:torch.utils.data.dataloader.worker_queue_idx->next(self._worker_queue_idx_cycle)
torch.utils.data.DataLoader(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)
torch.utils.data.DataLoader.__iter__(self)
torch.utils.data.DataLoader.__len__(self)
torch.utils.data.DataLoader.__setattr__(self,attr,val)
torch.utils.data.DataLoader._auto_collation(self)
torch.utils.data.DataLoader._index_sampler(self)
torch.utils.data.DataLoader.multiprocessing_context(self)
torch.utils.data.DataLoader.multiprocessing_context(self,multiprocessing_context)
torch.utils.data._DatasetKind(object)
torch.utils.data._DatasetKind.create_fetcher(kind,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data.dataloader.DataLoader(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)
torch.utils.data.dataloader.DataLoader.__init__(self,dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)
torch.utils.data.dataloader.DataLoader.__iter__(self)
torch.utils.data.dataloader.DataLoader.__len__(self)
torch.utils.data.dataloader.DataLoader.__setattr__(self,attr,val)
torch.utils.data.dataloader.DataLoader._auto_collation(self)
torch.utils.data.dataloader.DataLoader._index_sampler(self)
torch.utils.data.dataloader.DataLoader.multiprocessing_context(self)
torch.utils.data.dataloader.DataLoader.multiprocessing_context(self,multiprocessing_context)
torch.utils.data.dataloader._BaseDataLoaderIter(self,loader)
torch.utils.data.dataloader._BaseDataLoaderIter.__getstate__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._BaseDataLoaderIter.__iter__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__len__(self)
torch.utils.data.dataloader._BaseDataLoaderIter.__next__(self)
torch.utils.data.dataloader._BaseDataLoaderIter._next_index(self)
torch.utils.data.dataloader._DatasetKind(object)
torch.utils.data.dataloader._DatasetKind.create_fetcher(kind,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data.dataloader._InfiniteConstantSampler(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__init__(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__iter__(self)
torch.utils.data.dataloader._InfiniteConstantSampler.__len__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter(self,loader)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__del__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter.__next__(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._get_data(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._process_data(self,data)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._shutdown_worker(self,worker_id)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._shutdown_workers(self)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._try_get_data(self,timeout=_utils.MP_STATUS_CHECK_INTERVAL)
torch.utils.data.dataloader._MultiProcessingDataLoaderIter._try_put_index(self)
torch.utils.data.dataloader._SingleProcessDataLoaderIter(self,loader)
torch.utils.data.dataloader._SingleProcessDataLoaderIter.__init__(self,loader)
torch.utils.data.dataloader._SingleProcessDataLoaderIter.__next__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/dataloader.pyi----------------------------------------
torch.utils.data.dataloader._DataLoaderIter(self,loader:DataLoader)
torch.utils.data.dataloader._DataLoaderIter.__init__(self,loader:DataLoader)
torch.utils.data.dataloader._DataLoaderIter.__iter__(self)->_DataLoaderIter
torch.utils.data.dataloader._DataLoaderIter.__len__(self)->int
torch.utils.data.dataloader._DataLoaderIter.__next__(self)->Any


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/distributed.py----------------------------------------
A:torch.utils.data.distributed.num_replicas->torch.distributed.get_world_size()
A:torch.utils.data.distributed.rank->torch.distributed.get_rank()
A:torch.utils.data.distributed.self.num_samples->int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
A:torch.utils.data.distributed.g->torch.Generator()
A:torch.utils.data.distributed.indices->list(range(len(self.dataset)))
torch.utils.data.DistributedSampler(self,dataset,num_replicas=None,rank=None,shuffle=True)
torch.utils.data.DistributedSampler.__iter__(self)
torch.utils.data.DistributedSampler.__len__(self)
torch.utils.data.DistributedSampler.set_epoch(self,epoch)
torch.utils.data.distributed.DistributedSampler(self,dataset,num_replicas=None,rank=None,shuffle=True)
torch.utils.data.distributed.DistributedSampler.__init__(self,dataset,num_replicas=None,rank=None,shuffle=True)
torch.utils.data.distributed.DistributedSampler.__iter__(self)
torch.utils.data.distributed.DistributedSampler.__len__(self)
torch.utils.data.distributed.DistributedSampler.set_epoch(self,epoch)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/distributed.pyi----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py----------------------------------------
A:torch.utils.data._utils.fetch.self.dataset_iter->iter(dataset)
A:torch.utils.data._utils.fetch.data->next(self.dataset_iter)
torch.utils.data._utils.fetch._BaseDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._BaseDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._BaseDatasetFetcher.fetch(self,possibly_batched_index)
torch.utils.data._utils.fetch._IterableDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._IterableDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._IterableDatasetFetcher.fetch(self,possibly_batched_index)
torch.utils.data._utils.fetch._MapDatasetFetcher(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._MapDatasetFetcher.__init__(self,dataset,auto_collation,collate_fn,drop_last)
torch.utils.data._utils.fetch._MapDatasetFetcher.fetch(self,possibly_batched_index)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/_utils/__init__.py----------------------------------------
torch.utils.data._utils.__init__._set_python_exit_flag()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py----------------------------------------
A:torch.utils.data._utils.pin_memory.r->in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data._utils.pin_memory.data->ExceptionWrapper(where='in pin memory thread for device {}'.format(device_id))
torch.utils.data._utils.pin_memory._pin_memory_loop(in_queue,out_queue,device_id,done_event)
torch.utils.data._utils.pin_memory.pin_memory(data)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py----------------------------------------
A:torch.utils.data._utils.signal_handling.previous_handler->signal.getsignal(signal.SIGCHLD)
torch.utils.data._utils.signal_handling._set_SIGCHLD_handler()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py----------------------------------------
A:torch.utils.data._utils.worker.self.manager_pid->os.getppid()
A:torch.utils.data._utils.worker.self.kernel32->ctypes.WinDLL('kernel32', use_last_error=True)
A:torch.utils.data._utils.worker.self.manager_handle->self.kernel32.OpenProcess(SYNCHRONIZE, 0, self.manager_pid)
A:torch.utils.data._utils.worker._IterableDatasetStopIteration->namedtuple('_IterableDatasetStopIteration', ['worker_id'])
A:torch.utils.data._utils.worker._worker_info->WorkerInfo(id=worker_id, num_workers=num_workers, seed=seed, dataset=dataset)
A:torch.utils.data._utils.worker.fetcher->torch.utils.data._DatasetKind.create_fetcher(dataset_kind, dataset, auto_collation, collate_fn, drop_last)
A:torch.utils.data._utils.worker.init_exception->ExceptionWrapper(where='in DataLoader worker process {}'.format(worker_id))
A:torch.utils.data._utils.worker.watchdog->ManagerWatchdog()
A:torch.utils.data._utils.worker.r->index_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
A:torch.utils.data._utils.worker.data->ExceptionWrapper(where='in DataLoader worker process {}'.format(worker_id))
torch.utils.data._utils.worker.WorkerInfo(self,**kwargs)
torch.utils.data._utils.worker.WorkerInfo.__init__(self,**kwargs)
torch.utils.data._utils.worker.WorkerInfo.__setattr__(self,key,val)
torch.utils.data._utils.worker._worker_loop(dataset_kind,dataset,index_queue,data_queue,done_event,auto_collation,collate_fn,drop_last,seed,init_fn,worker_id,num_workers)
torch.utils.data._utils.worker.get_worker_info()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py----------------------------------------
A:torch.utils.data._utils.collate.np_str_obj_array_pattern->re.compile('[SaUO]')
A:torch.utils.data._utils.collate.elem_type->type(elem)
A:torch.utils.data._utils.collate.numel->sum([x.numel() for x in batch])
A:torch.utils.data._utils.collate.storage->elem.storage()._new_shared(numel)
A:torch.utils.data._utils.collate.out->elem.new(storage)
A:torch.utils.data._utils.collate.transposed->zip(*batch)
torch.utils.data._utils.collate.default_collate(batch)
torch.utils.data._utils.collate.default_convert(data)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/summary.py----------------------------------------
A:torch.utils.tensorboard.summary._INVALID_TAG_CHARACTERS->re.compile('[^-/\\w\\.]')
A:torch.utils.tensorboard.summary.new_name->new_name.lstrip('/').lstrip('/')
A:torch.utils.tensorboard.summary.font->PIL.ImageFont.load_default()
A:torch.utils.tensorboard.summary.draw->PIL.ImageDraw.Draw(image)
A:torch.utils.tensorboard.summary.(text_width, text_height)->PIL.ImageFont.load_default().getsize(display_str)
A:torch.utils.tensorboard.summary.margin->numpy.ceil(0.05 * text_height)
A:torch.utils.tensorboard.summary.exp->Summary(value=[Summary.Value(tag=EXPERIMENT_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.content->HParamsPluginData(session_end_info=sei, version=PLUGIN_DATA_VERSION)
A:torch.utils.tensorboard.summary.smd->SummaryMetadata(plugin_data=plugin_data)
A:torch.utils.tensorboard.summary.ssi->Summary(value=[Summary.Value(tag=SESSION_START_INFO_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.sei->Summary(value=[Summary.Value(tag=SESSION_END_INFO_TAG, metadata=smd)])
A:torch.utils.tensorboard.summary.name->_clean_tag(name)
A:torch.utils.tensorboard.summary.scalar->float(scalar)
A:torch.utils.tensorboard.summary.hist->make_histogram(values.astype(float), bins, max_bins)
A:torch.utils.tensorboard.summary.values->values.reshape(-1).reshape(-1)
A:torch.utils.tensorboard.summary.(counts, limits)->numpy.histogram(values, bins=bins)
A:torch.utils.tensorboard.summary.num_bins->len(counts)
A:torch.utils.tensorboard.summary.counts->counts.reshape(-1, subsampling).sum(axis=-1).reshape(-1, subsampling).sum(axis=-1)
A:torch.utils.tensorboard.summary.new_limits->numpy.empty((counts.size + 1,), limits.dtype)
A:torch.utils.tensorboard.summary.cum_counts->numpy.cumsum(np.greater(counts, 0, dtype=np.int32))
A:torch.utils.tensorboard.summary.(start, end)->numpy.searchsorted(cum_counts, [0, cum_counts[-1] - 1], side='right')
A:torch.utils.tensorboard.summary.start->int(start)
A:torch.utils.tensorboard.summary.sum_sq->values.reshape(-1).reshape(-1).dot(values)
A:torch.utils.tensorboard.summary.tag->_clean_tag(tag)
A:torch.utils.tensorboard.summary.tensor->TensorProto(dtype='DT_FLOAT', float_val=tensor.reshape(-1).tolist(), tensor_shape=TensorShapeProto(dim=[TensorShapeProto.Dim(size=tensor.shape[0]), TensorShapeProto.Dim(size=tensor.shape[1]), TensorShapeProto.Dim(size=tensor.shape[2])]))
A:torch.utils.tensorboard.summary.scale_factor->_calc_scale_factor(tensor)
A:torch.utils.tensorboard.summary.image->image.resize((scaled_width, scaled_height), Image.ANTIALIAS).resize((scaled_width, scaled_height), Image.ANTIALIAS)
A:torch.utils.tensorboard.summary.tensor_image->convert_to_HWC(tensor_image, dataformats)
A:torch.utils.tensorboard.summary.tensor_boxes->make_np(tensor_boxes)
A:torch.utils.tensorboard.summary.list_gt->range(num_boxes)
A:torch.utils.tensorboard.summary.disp_image->_draw_single_box(disp_image, boxes[i, 0], boxes[i, 1], boxes[i, 2], boxes[i, 3], display_str=None, color='Red')
A:torch.utils.tensorboard.summary.scaled_height->int(height * rescale)
A:torch.utils.tensorboard.summary.scaled_width->int(width * rescale)
A:torch.utils.tensorboard.summary.output->io.BytesIO()
A:torch.utils.tensorboard.summary.image_string->io.BytesIO().getvalue()
A:torch.utils.tensorboard.summary.video->make_video(tensor, fps)
A:torch.utils.tensorboard.summary.clip->moviepy.editor.ImageSequenceClip(list(tensor), fps=fps)
A:torch.utils.tensorboard.summary.tensor_string->f.read()
A:torch.utils.tensorboard.summary.fio->io.BytesIO()
A:torch.utils.tensorboard.summary.wave_write->wave.open(fio, 'wb')
A:torch.utils.tensorboard.summary.audio_string->io.BytesIO().getvalue()
A:torch.utils.tensorboard.summary.audio->tensorboard.compat.proto.summary_pb2.Summary.Audio(sample_rate=sample_rate, num_channels=1, length_frames=len(tensor_list), encoded_audio_string=audio_string, content_type='audio/wav')
A:torch.utils.tensorboard.summary.mgcc->tensorboard.plugins.custom_scalar.layout_pb2.MarginChartContent(series=[layout_pb2.MarginChartContent.Series(value=tags[0], lower=tags[1], upper=tags[2])])
A:torch.utils.tensorboard.summary.chart->tensorboard.plugins.custom_scalar.layout_pb2.Chart(title=chart_name, multiline=mlcc)
A:torch.utils.tensorboard.summary.mlcc->tensorboard.plugins.custom_scalar.layout_pb2.MultilineChartContent(tag=tags)
A:torch.utils.tensorboard.summary.layout->tensorboard.plugins.custom_scalar.layout_pb2.Layout(category=categories)
A:torch.utils.tensorboard.summary.plugin_data->tensorboard.compat.proto.summary_pb2.SummaryMetadata.PluginData(plugin_name='pr_curves', content=pr_curve_plugin_data)
A:torch.utils.tensorboard.summary.data->compute_curve(labels, predictions, num_thresholds=num_thresholds, weights=weights)
A:torch.utils.tensorboard.summary.pr_curve_plugin_data->PrCurvePluginData(version=0, num_thresholds=num_thresholds).SerializeToString()
A:torch.utils.tensorboard.summary.num_thresholds->min(num_thresholds, 127)
A:torch.utils.tensorboard.summary.bucket_indices->numpy.int32(np.floor(predictions * (num_thresholds - 1)))
A:torch.utils.tensorboard.summary.float_labels->labels.astype(np.float)
A:torch.utils.tensorboard.summary.(tp_buckets, _)->numpy.histogram(bucket_indices, bins=num_thresholds, range=histogram_range, weights=float_labels * weights)
A:torch.utils.tensorboard.summary.(fp_buckets, _)->numpy.histogram(bucket_indices, bins=num_thresholds, range=histogram_range, weights=(1.0 - float_labels) * weights)
A:torch.utils.tensorboard.summary.tensor_metadata->tensorboard.plugins.mesh.metadata.create_summary_metadata(name, display_name, content_type, components, tensor.shape, description, json_config=json_config)
A:torch.utils.tensorboard.summary.tensor_summary->tensorboard.compat.proto.summary_pb2.Summary.Value(tag=metadata.get_instance_name(name, content_type), tensor=tensor, metadata=tensor_metadata)
A:torch.utils.tensorboard.summary.json_config->_get_json_config(config_dict)
A:torch.utils.tensorboard.summary.components->tensorboard.plugins.mesh.metadata.get_components_bitmask([content_type for (tensor, content_type) in tensors])
torch.utils.tensorboard.summary._calc_scale_factor(tensor)
torch.utils.tensorboard.summary._clean_tag(name)
torch.utils.tensorboard.summary._draw_single_box(image,xmin,ymin,xmax,ymax,display_str,color='black',color_text='black',thickness=2)
torch.utils.tensorboard.summary._get_json_config(config_dict)
torch.utils.tensorboard.summary._get_tensor_summary(name,display_name,description,tensor,content_type,components,json_config)
torch.utils.tensorboard.summary.audio(tag,tensor,sample_rate=44100)
torch.utils.tensorboard.summary.compute_curve(labels,predictions,num_thresholds=None,weights=None)
torch.utils.tensorboard.summary.custom_scalars(layout)
torch.utils.tensorboard.summary.draw_boxes(disp_image,boxes)
torch.utils.tensorboard.summary.histogram(name,values,bins,max_bins=None)
torch.utils.tensorboard.summary.histogram_raw(name,min,max,num,sum,sum_squares,bucket_limits,bucket_counts)
torch.utils.tensorboard.summary.hparams(hparam_dict=None,metric_dict=None)
torch.utils.tensorboard.summary.image(tag,tensor,rescale=1,dataformats='NCHW')
torch.utils.tensorboard.summary.image_boxes(tag,tensor_image,tensor_boxes,rescale=1,dataformats='CHW')
torch.utils.tensorboard.summary.make_histogram(values,bins,max_bins=None)
torch.utils.tensorboard.summary.make_image(tensor,rescale=1,rois=None)
torch.utils.tensorboard.summary.make_video(tensor,fps)
torch.utils.tensorboard.summary.mesh(tag,vertices,colors,faces,config_dict,display_name=None,description=None)
torch.utils.tensorboard.summary.pr_curve(tag,labels,predictions,num_thresholds=127,weights=None)
torch.utils.tensorboard.summary.pr_curve_raw(tag,tp,fp,tn,fn,precision,recall,num_thresholds=127,weights=None)
torch.utils.tensorboard.summary.scalar(name,scalar,collections=None)
torch.utils.tensorboard.summary.text(tag,text)
torch.utils.tensorboard.summary.video(tag,tensor,fps=4)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_caffe2_graph.py----------------------------------------
A:torch.utils.tensorboard._caffe2_graph.WEIGHT->re.compile('(_w)$')
A:torch.utils.tensorboard._caffe2_graph.WEIGHT_->re.compile('(_w_)')
A:torch.utils.tensorboard._caffe2_graph.BN->re.compile('(_bn)$')
A:torch.utils.tensorboard._caffe2_graph.BN_->re.compile('(_bn_)')
A:torch.utils.tensorboard._caffe2_graph.BIAS->re.compile('(_b)$')
A:torch.utils.tensorboard._caffe2_graph.BIAS_->re.compile('(_b_)')
A:torch.utils.tensorboard._caffe2_graph.SCALE->re.compile('(_s)$')
A:torch.utils.tensorboard._caffe2_graph.SCALE_->re.compile('(_s_)')
A:torch.utils.tensorboard._caffe2_graph.SUM->re.compile('(_sum)$')
A:torch.utils.tensorboard._caffe2_graph.SUM_->re.compile('(_sum_)')
A:torch.utils.tensorboard._caffe2_graph.BRANCH->re.compile('(_branch)')
A:torch.utils.tensorboard._caffe2_graph.inter_name->re.compile('(_sum_)').sub('/sum_', SUM.sub('/sum', inter_name))
A:torch.utils.tensorboard._caffe2_graph.new_name->_make_unique_name(seen, rename_fn(name))
A:torch.utils.tensorboard._caffe2_graph.ir->caffe2.python.core.IR(ops)
A:torch.utils.tensorboard._caffe2_graph.seen->set(input_blobs)
A:torch.utils.tensorboard._caffe2_graph.inputs->list(op.input)
A:torch.utils.tensorboard._caffe2_graph.outputs->list(op.output)
A:torch.utils.tensorboard._caffe2_graph.names->set()
A:torch.utils.tensorboard._caffe2_graph.op.name->_make_unique_name(seen, name)
A:torch.utils.tensorboard._caffe2_graph.scope->os.path.commonprefix(name_list)
A:torch.utils.tensorboard._caffe2_graph.name->os.path.join(scope, op.type)
A:torch.utils.tensorboard._caffe2_graph.shape_proto->TensorShapeProto()
A:torch.utils.tensorboard._caffe2_graph.dim->tensorboard.compat.proto.tensor_shape_pb2.TensorShapeProto.Dim()
A:torch.utils.tensorboard._caffe2_graph.n->NodeDef()
A:torch.utils.tensorboard._caffe2_graph.n.device->_tf_device(device)
A:torch.utils.tensorboard._caffe2_graph.len_outputs->len(outputs)
A:torch.utils.tensorboard._caffe2_graph.device->_tf_device(op.device_option)
A:torch.utils.tensorboard._caffe2_graph.produced_by->producing_ops.get(name, [])
A:torch.utils.tensorboard._caffe2_graph.in_blobs->set()
A:torch.utils.tensorboard._caffe2_graph.out_blobs->set()
A:torch.utils.tensorboard._caffe2_graph.input_blobs->list(in_blobs.difference(out_blobs))
A:torch.utils.tensorboard._caffe2_graph.output_blobs->list(out_blobs.difference(in_blobs))
A:torch.utils.tensorboard._caffe2_graph.ops->_filter_ops(ops, _check_if_cpu, show_simplified)
A:torch.utils.tensorboard._caffe2_graph.blobs->set()
A:torch.utils.tensorboard._caffe2_graph.(input_blobs, inter_blobs, _)->_compute_in_out(ops)
A:torch.utils.tensorboard._caffe2_graph.current_graph->GraphDef()
A:torch.utils.tensorboard._caffe2_graph.(shapes, _)->caffe2.python.workspace.InferShapesAndTypes(nets)
A:torch.utils.tensorboard._caffe2_graph.shapes->copy.deepcopy(shapes or {})
torch.utils.tensorboard._caffe2_graph._add_gradient_scope(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._add_tf_shape(attr_dict,ints)
torch.utils.tensorboard._caffe2_graph._blob_to_node(producing_ops,shapes,name)
torch.utils.tensorboard._caffe2_graph._check_if_cpu(blob)
torch.utils.tensorboard._caffe2_graph._check_if_forward(blob)
torch.utils.tensorboard._caffe2_graph._clear_debug_info(ops,perform_clear)
torch.utils.tensorboard._caffe2_graph._compute_in_out(ops)
torch.utils.tensorboard._caffe2_graph._convert_to_ssa(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._fill_missing_operator_names(ops)
torch.utils.tensorboard._caffe2_graph._filter_ops(ops,filter_fn,perform_filter)
torch.utils.tensorboard._caffe2_graph._get_blob_names(ops)
torch.utils.tensorboard._caffe2_graph._make_unique_name(seen,name,min_version=0)
torch.utils.tensorboard._caffe2_graph._operator_to_node(shapes,op)
torch.utils.tensorboard._caffe2_graph._operator_to_node_simp(op,inter_blobs,seen)
torch.utils.tensorboard._caffe2_graph._operators_to_graph_def(shapes,ops,colon_replacement='$',with_ssa=True,with_gradient_scope=True,blob_name_tracker=None,show_simplified=False,custom_rename=None)
torch.utils.tensorboard._caffe2_graph._propagate_device_option(net_def)
torch.utils.tensorboard._caffe2_graph._remap_keys(old_dict,rename_fn)
torch.utils.tensorboard._caffe2_graph._rename_all(shapes,blob_name_tracker,ops,rename_fn)
torch.utils.tensorboard._caffe2_graph._rename_tensorflow_style(shapes,blob_name_tracker,ops)
torch.utils.tensorboard._caffe2_graph._replace_colons(shapes,blob_name_tracker,ops,repl)
torch.utils.tensorboard._caffe2_graph._set_tf_attr(attr_dict,arg)
torch.utils.tensorboard._caffe2_graph._tf_device(device_option)
torch.utils.tensorboard._caffe2_graph._try_get_shapes(nets)
torch.utils.tensorboard._caffe2_graph.model_to_graph_def(model,**kwargs)
torch.utils.tensorboard._caffe2_graph.nets_to_graph_def(nets,shapes=None,**kwargs)
torch.utils.tensorboard._caffe2_graph.protos_to_graph_def(net_defs,shapes=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py----------------------------------------
A:torch.utils.tensorboard._pytorch_graph.list_of_node->list(getattr(node_cpp, m)())
A:torch.utils.tensorboard._pytorch_graph.tensor_size->node_cpp.type().sizes()
A:torch.utils.tensorboard._pytorch_graph.self.attributes->str({k: node_cpp[k] for k in node_cpp.attributeNames()}).replace("'", ' ')
A:torch.utils.tensorboard._pytorch_graph.self.kind->node_cpp.kind()
A:torch.utils.tensorboard._pytorch_graph.self.nodes_io->OrderedDict()
A:torch.utils.tensorboard._pytorch_graph.self.nodes_io[node_output]->NodeBase(node_output, x.inputs, x.scopeName, outputSize, op_type=x.kind, attributes=x.attributes)
A:torch.utils.tensorboard._pytorch_graph.n_inputs->len(args)
A:torch.utils.tensorboard._pytorch_graph.nodes_py->GraphPy()
A:torch.utils.tensorboard._pytorch_graph.trace->torch.jit.trace(model, args)
A:torch.utils.tensorboard._pytorch_graph.list_of_nodes->parse(graph, args)
A:torch.utils.tensorboard._pytorch_graph.stepstats->RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device='/device:CPU:0')]))
torch.utils.tensorboard._pytorch_graph.GraphPy(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.__init__(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.append(self,x)
torch.utils.tensorboard._pytorch_graph.GraphPy.find_common_root(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.populate_namespace_from_OP_to_IO(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.printall(self)
torch.utils.tensorboard._pytorch_graph.GraphPy.to_proto(self)
torch.utils.tensorboard._pytorch_graph.NodeBase(self,debugName=None,inputs=None,scope=None,tensor_size=None,op_type='UnSpecified',attributes='')
torch.utils.tensorboard._pytorch_graph.NodeBase.__init__(self,debugName=None,inputs=None,scope=None,tensor_size=None,op_type='UnSpecified',attributes='')
torch.utils.tensorboard._pytorch_graph.NodeBase.__repr__(self)
torch.utils.tensorboard._pytorch_graph.NodePy(self,node_cpp,valid_methods)
torch.utils.tensorboard._pytorch_graph.NodePy.__init__(self,node_cpp,valid_methods)
torch.utils.tensorboard._pytorch_graph.NodePyIO(self,node_cpp,input_or_output=None)
torch.utils.tensorboard._pytorch_graph.NodePyIO.__init__(self,node_cpp,input_or_output=None)
torch.utils.tensorboard._pytorch_graph.NodePyOP(self,node_cpp)
torch.utils.tensorboard._pytorch_graph.NodePyOP.__init__(self,node_cpp)
torch.utils.tensorboard._pytorch_graph.graph(model,args,verbose=False)
torch.utils.tensorboard._pytorch_graph.parse(graph,args=None,omit_useless_nodes=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_utils.py----------------------------------------
A:torch.utils.tensorboard._utils.canvas->numpy.zeros((3, H * nrows, W * ncols))
A:torch.utils.tensorboard._utils.data->numpy.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)
A:torch.utils.tensorboard._utils.(w, h)->figure.canvas.get_width_height()
A:torch.utils.tensorboard._utils.image_chw->numpy.moveaxis(image_hwc, source=2, destination=0)
A:torch.utils.tensorboard._utils.image->render_to_rgb(figures)
A:torch.utils.tensorboard._utils.len_addition->int(2 ** V.shape[0].bit_length() - V.shape[0])
A:torch.utils.tensorboard._utils.V->numpy.reshape(V, newshape=(t, n_rows * h, n_cols * w, c))
A:torch.utils.tensorboard._utils.I->numpy.concatenate([I, I, I], 1)
A:torch.utils.tensorboard._utils.ncols->min(nimg, ncols)
A:torch.utils.tensorboard._utils.nrows->int(np.ceil(float(nimg) / ncols))
A:torch.utils.tensorboard._utils.input_format->input_format.upper().upper()
A:torch.utils.tensorboard._utils.tensor_NCHW->numpy.stack([tensor, tensor, tensor], 2).transpose(index)
A:torch.utils.tensorboard._utils.tensor_CHW->make_grid(tensor_NCHW)
A:torch.utils.tensorboard._utils.tensor_HWC->numpy.concatenate([tensor_HWC, tensor_HWC, tensor_HWC], 2)
A:torch.utils.tensorboard._utils.tensor->numpy.stack([tensor, tensor, tensor], 2)
torch.utils.tensorboard._utils._prepare_video(V)
torch.utils.tensorboard._utils.convert_to_HWC(tensor,input_format)
torch.utils.tensorboard._utils.figure_to_image(figures,close=True)
torch.utils.tensorboard._utils.make_grid(I,ncols=8)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_onnx_graph.py----------------------------------------
A:torch.utils.tensorboard._onnx_graph.m->onnx.load(fname)
A:torch.utils.tensorboard._onnx_graph.shapeproto->TensorShapeProto(dim=[TensorShapeProto.Dim(size=d.dim_value) for d in node.type.tensor_type.shape.dim])
A:torch.utils.tensorboard._onnx_graph.attr->', '.join(attr).encode(encoding='utf_8')
torch.utils.tensorboard._onnx_graph.load_onnx_graph(fname)
torch.utils.tensorboard._onnx_graph.parse(graph)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_embedding.py----------------------------------------
A:torch.utils.tensorboard._embedding.nrow->int(math.ceil(label_img.size(0) ** 0.5))
A:torch.utils.tensorboard._embedding.arranged_img_CHW->make_grid(make_np(label_img), ncols=nrow)
A:torch.utils.tensorboard._embedding.arranged_augment_square_HWC->numpy.ndarray((arranged_img_CHW.shape[2], arranged_img_CHW.shape[2], 3))
A:torch.utils.tensorboard._embedding.arranged_img_HWC->make_grid(make_np(label_img), ncols=nrow).transpose(1, 2, 0)
A:torch.utils.tensorboard._embedding.im->PIL.Image.fromarray(np.uint8((arranged_augment_square_HWC * 255).clip(0, 255)))
torch.utils.tensorboard._embedding.append_pbtxt(metadata,label_img,save_path,subdir,global_step,tag)
torch.utils.tensorboard._embedding.make_mat(matlist,save_path)
torch.utils.tensorboard._embedding.make_sprite(label_img,save_path)
torch.utils.tensorboard._embedding.make_tsv(metadata,save_path,metadata_header=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_proto_graph.py----------------------------------------
A:torch.utils.tensorboard._proto_graph.attr['attr']->AttrValue(s=s.encode(encoding='utf_8'))
A:torch.utils.tensorboard._proto_graph.shapeproto->tensor_shape_proto(shape)
A:torch.utils.tensorboard._proto_graph.attr['_output_shapes']->AttrValue(list=AttrValue.ListValue(shape=[shapeproto]))
torch.utils.tensorboard._proto_graph.attr_value_proto(dtype,shape,s)
torch.utils.tensorboard._proto_graph.node_proto(name,op='UnSpecified',input=None,dtype=None,shape=None,outputsize=None,attributes='')
torch.utils.tensorboard._proto_graph.tensor_shape_proto(outputsize)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/_convert_np.py----------------------------------------
A:torch.utils.tensorboard._convert_np.x->caffe2.python.workspace.FetchBlob(x)
torch.utils.tensorboard._convert_np._prepare_caffe2(x)
torch.utils.tensorboard._convert_np._prepare_pytorch(x)
torch.utils.tensorboard._convert_np.make_np(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py----------------------------------------
A:torch.utils.tensorboard.writer.log_dir->os.path.join('runs', current_time + '_' + socket.gethostname() + comment)
A:torch.utils.tensorboard.writer.self.event_writer->EventFileWriter(log_dir, max_queue, flush_secs, filename_suffix)
A:torch.utils.tensorboard.writer.event.step->int(step)
A:torch.utils.tensorboard.writer.event->tensorboard.compat.proto.event_pb2.Event(graph_def=current_graph.SerializeToString())
A:torch.utils.tensorboard.writer.trm->tensorboard.compat.proto.event_pb2.TaggedRunMetadata(tag='step1', run_metadata=stepstats.SerializeToString())
A:torch.utils.tensorboard.writer.current_time->datetime.datetime.now().strftime('%b%d_%H-%M-%S')
A:torch.utils.tensorboard.writer.self.file_writer->FileWriter(self.log_dir, self.max_queue, self.flush_secs, self.filename_suffix)
A:torch.utils.tensorboard.writer.(exp, ssi, sei)->hparams(hparam_dict, metric_dict)
A:torch.utils.tensorboard.writer.scalar_value->workspace.FetchBlob(scalar_value)
A:torch.utils.tensorboard.writer.fw_logdir->self._get_file_writer().get_logdir()
A:torch.utils.tensorboard.writer.fw->FileWriter(fw_tag, self.max_queue, self.flush_secs, self.filename_suffix)
A:torch.utils.tensorboard.writer.values->workspace.FetchBlob(values)
A:torch.utils.tensorboard.writer.img_tensor->workspace.FetchBlob(img_tensor)
A:torch.utils.tensorboard.writer.box_tensor->workspace.FetchBlob(box_tensor)
A:torch.utils.tensorboard.writer.snd_tensor->workspace.FetchBlob(snd_tensor)
A:torch.utils.tensorboard.writer.current_graph->model_to_graph_def(model)
A:torch.utils.tensorboard.writer.retval->retval.replace('\\', '%%%02x' % ord('\\')).replace('\\', '%%%02x' % ord('\\'))
A:torch.utils.tensorboard.writer.mat->make_np(mat)
A:torch.utils.tensorboard.writer.save_path->os.path.join(self._get_file_writer().get_logdir(), subdir)
torch.utils.tensorboard.FileWriter(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.FileWriter.add_event(self,event,step=None,walltime=None)
torch.utils.tensorboard.FileWriter.add_graph(self,graph_profile,walltime=None)
torch.utils.tensorboard.FileWriter.add_onnx_graph(self,graph,walltime=None)
torch.utils.tensorboard.FileWriter.add_summary(self,summary,global_step=None,walltime=None)
torch.utils.tensorboard.FileWriter.close(self)
torch.utils.tensorboard.FileWriter.flush(self)
torch.utils.tensorboard.FileWriter.get_logdir(self)
torch.utils.tensorboard.FileWriter.reopen(self)
torch.utils.tensorboard.SummaryWriter(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.SummaryWriter.__enter__(self)
torch.utils.tensorboard.SummaryWriter.__exit__(self,exc_type,exc_val,exc_tb)
torch.utils.tensorboard.SummaryWriter._check_caffe2_blob(self,item)
torch.utils.tensorboard.SummaryWriter._encode(rawstr)
torch.utils.tensorboard.SummaryWriter._get_file_writer(self)
torch.utils.tensorboard.SummaryWriter.add_audio(self,tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_custom_scalars(self,layout)
torch.utils.tensorboard.SummaryWriter.add_custom_scalars_marginchart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.SummaryWriter.add_custom_scalars_multilinechart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.SummaryWriter.add_embedding(self,mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)
torch.utils.tensorboard.SummaryWriter.add_figure(self,tag,figure,global_step=None,close=True,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_graph(self,model,input_to_model=None,verbose=False)
torch.utils.tensorboard.SummaryWriter.add_histogram(self,tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)
torch.utils.tensorboard.SummaryWriter.add_histogram_raw(self,tag,min,max,num,sum,sum_squares,bucket_limits,bucket_counts,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_hparams(self,hparam_dict=None,metric_dict=None)
torch.utils.tensorboard.SummaryWriter.add_image(self,tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')
torch.utils.tensorboard.SummaryWriter.add_image_with_boxes(self,tag,img_tensor,box_tensor,global_step=None,walltime=None,rescale=1,dataformats='CHW')
torch.utils.tensorboard.SummaryWriter.add_images(self,tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')
torch.utils.tensorboard.SummaryWriter.add_mesh(self,tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_onnx_graph(self,prototxt)
torch.utils.tensorboard.SummaryWriter.add_pr_curve(self,tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_pr_curve_raw(self,tag,true_positive_counts,false_positive_counts,true_negative_counts,false_negative_counts,precision,recall,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_scalar(self,tag,scalar_value,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_scalars(self,main_tag,tag_scalar_dict,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_text(self,tag,text_string,global_step=None,walltime=None)
torch.utils.tensorboard.SummaryWriter.add_video(self,tag,vid_tensor,global_step=None,fps=4,walltime=None)
torch.utils.tensorboard.SummaryWriter.close(self)
torch.utils.tensorboard.SummaryWriter.flush(self)
torch.utils.tensorboard.SummaryWriter.get_logdir(self)
torch.utils.tensorboard.writer.FileWriter(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.FileWriter.__init__(self,log_dir,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.FileWriter.add_event(self,event,step=None,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_graph(self,graph_profile,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_onnx_graph(self,graph,walltime=None)
torch.utils.tensorboard.writer.FileWriter.add_summary(self,summary,global_step=None,walltime=None)
torch.utils.tensorboard.writer.FileWriter.close(self)
torch.utils.tensorboard.writer.FileWriter.flush(self)
torch.utils.tensorboard.writer.FileWriter.get_logdir(self)
torch.utils.tensorboard.writer.FileWriter.reopen(self)
torch.utils.tensorboard.writer.SummaryWriter(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.SummaryWriter.__enter__(self)
torch.utils.tensorboard.writer.SummaryWriter.__exit__(self,exc_type,exc_val,exc_tb)
torch.utils.tensorboard.writer.SummaryWriter.__init__(self,log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')
torch.utils.tensorboard.writer.SummaryWriter._check_caffe2_blob(self,item)
torch.utils.tensorboard.writer.SummaryWriter._encode(rawstr)
torch.utils.tensorboard.writer.SummaryWriter._get_file_writer(self)
torch.utils.tensorboard.writer.SummaryWriter.add_audio(self,tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars(self,layout)
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars_marginchart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars_multilinechart(self,tags,category='default',title='untitled')
torch.utils.tensorboard.writer.SummaryWriter.add_embedding(self,mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)
torch.utils.tensorboard.writer.SummaryWriter.add_figure(self,tag,figure,global_step=None,close=True,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_graph(self,model,input_to_model=None,verbose=False)
torch.utils.tensorboard.writer.SummaryWriter.add_histogram(self,tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)
torch.utils.tensorboard.writer.SummaryWriter.add_histogram_raw(self,tag,min,max,num,sum,sum_squares,bucket_limits,bucket_counts,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_hparams(self,hparam_dict=None,metric_dict=None)
torch.utils.tensorboard.writer.SummaryWriter.add_image(self,tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')
torch.utils.tensorboard.writer.SummaryWriter.add_image_with_boxes(self,tag,img_tensor,box_tensor,global_step=None,walltime=None,rescale=1,dataformats='CHW')
torch.utils.tensorboard.writer.SummaryWriter.add_images(self,tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')
torch.utils.tensorboard.writer.SummaryWriter.add_mesh(self,tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_onnx_graph(self,prototxt)
torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve(self,tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve_raw(self,tag,true_positive_counts,false_positive_counts,true_negative_counts,false_negative_counts,precision,recall,global_step=None,num_thresholds=127,weights=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_scalar(self,tag,scalar_value,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_scalars(self,main_tag,tag_scalar_dict,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_text(self,tag,text_string,global_step=None,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.add_video(self,tag,vid_tensor,global_step=None,fps=4,walltime=None)
torch.utils.tensorboard.writer.SummaryWriter.close(self)
torch.utils.tensorboard.writer.SummaryWriter.flush(self)
torch.utils.tensorboard.writer.SummaryWriter.get_logdir(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/contrib/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/contrib/_tensorboard_vis.py----------------------------------------
A:torch.contrib._tensorboard_vis.pb_graph->visualize(graph_executor)
A:torch.contrib._tensorboard_vis.evt->tensorflow.core.util.event_pb2.Event(wall_time=time.time(), graph_def=pb_graph.SerializeToString())
A:torch.contrib._tensorboard_vis.input_node->visualize(graph_executor).node.add(op='input', name=name_prefix + 'input')
A:torch.contrib._tensorboard_vis.return_node->visualize(graph_executor).node.add(op='output', name=name_prefix + 'output')
A:torch.contrib._tensorboard_vis.input_kinds->visualize(graph_executor).node.add(op='INPUT_KIND', name=subgraph_name)
A:torch.contrib._tensorboard_vis.input_kinds.attr['inputs'].s->repr(arg_spec).encode('ascii')
A:torch.contrib._tensorboard_vis.op_id_counter->defaultdict(int)
A:torch.contrib._tensorboard_vis.(op, name)->name_for(node)
A:torch.contrib._tensorboard_vis.ge->next(executors_it)
A:torch.contrib._tensorboard_vis.pb_node->visualize(graph_executor).node.add(op=op, name=name)
torch.contrib._tensorboard_vis.dump_tensorboard_summary(graph_executor,logdir)
torch.contrib._tensorboard_vis.visualize(graph,name_prefix='',pb_graph=None,executors_it=None)
torch.contrib._tensorboard_vis.visualize_graph_executor(state,name_prefix,pb_graph,inline_graph)
torch.contrib._tensorboard_vis.visualize_rec(graph,value_map,name_prefix,pb_graph,executors_it=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/__init__.py----------------------------------------
A:torch.onnx.__init__.result->torch.onnx.utils._export(*args, **kwargs)
torch.onnx.__init__.ExportTypes
torch.onnx.__init__._export(*args,**kwargs)
torch.onnx.__init__._export_to_pretty_string(*args,**kwargs)
torch.onnx.__init__._optimize_trace(trace,operator_export_type)
torch.onnx.__init__._run_symbolic_function(*args,**kwargs)
torch.onnx.__init__._run_symbolic_method(*args,**kwargs)
torch.onnx.__init__.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=False,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None)
torch.onnx.__init__.export_to_pretty_string(*args,**kwargs)
torch.onnx.__init__.is_in_onnx_export()
torch.onnx.__init__.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)
torch.onnx.__init__.set_training(model,mode)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/operators.py----------------------------------------
torch.onnx.operators.reshape_from_tensor_shape(x,shape)
torch.onnx.operators.shape_as_tensor(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_opset7.py----------------------------------------
A:torch.onnx.symbolic_opset7.vars()[black_listed_op]->_black_list_in_opset(black_listed_op)
torch.onnx.symbolic_opset7.max(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset7.min(g,self,dim_or_y=None,keepdim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_opset10.py----------------------------------------
A:torch.onnx.symbolic_opset10.shape_->g.op('Shape', self)
A:torch.onnx.symbolic_opset10.axis->g.op('Constant', value_t=torch.tensor(0, dtype=torch.int64))
A:torch.onnx.symbolic_opset10.start->g.op('Constant', value_t=torch.tensor(dim, dtype=torch.int64))
A:torch.onnx.symbolic_opset10.end->g.op('Constant', value_t=torch.tensor(dim + 1, dtype=torch.int64))
A:torch.onnx.symbolic_opset10.slice_->torch.onnx.symbolic_helper._slice_helper(g, shape_, axes=axis, starts=start, ends=end, steps=None, dynamic_slice=True)
A:torch.onnx.symbolic_opset10.k->unsqueeze(g, k, 0)
A:torch.onnx.symbolic_opset10.kwargs['dilations_i']->tuple_fn(dilation)
A:torch.onnx.symbolic_opset10.(r, indices)->g.op('MaxPool', input, outputs=2, **kwargs)
A:torch.onnx.symbolic_opset10.(_, flattened_indices)->g.op('MaxPool', input, outputs=2, kernel_shape_i=[1 for _ in range(ndims)], strides_i=[1 for _ in range(ndims)])
A:torch.onnx.symbolic_opset10.s->torch.onnx.symbolic_helper._slice_helper(g, flattened_indices, axes=[2 + i for i in range(ndims)], starts=tuple_fn(0), ends=tuple_fn(1))
A:torch.onnx.symbolic_opset10.indices->sub(g, indices, s)
A:torch.onnx.symbolic_opset10.r->g.op('MaxPool', input, outputs=1, **kwargs)
A:torch.onnx.symbolic_opset10.max_pool1d->_max_pool('max_pool1d', _single, 1, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool2d->_max_pool('max_pool2d', _pair, 2, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool3d->_max_pool('max_pool3d', _triple, 3, return_indices=False)
A:torch.onnx.symbolic_opset10.max_pool1d_with_indices->_max_pool('max_pool1d_with_indices', _single, 1, return_indices=True)
A:torch.onnx.symbolic_opset10.max_pool2d_with_indices->_max_pool('max_pool2d_with_indices', _pair, 2, return_indices=True)
A:torch.onnx.symbolic_opset10.max_pool3d_with_indices->_max_pool('max_pool3d_with_indices', _triple, 3, return_indices=True)
A:torch.onnx.symbolic_opset10.padding->tuple(tuple_fn(padding))
A:torch.onnx.symbolic_opset10.input->g.op('Pad', input, pads_i=((0,) * 2 + padding) * 2, mode_s='constant', value_f=0.0)
A:torch.onnx.symbolic_opset10.output->g.op('AveragePool', input, kernel_shape_i=tuple_fn(kernel_size), strides_i=tuple_fn(stride), pads_i=padding * 2, ceil_mode_i=ceil_mode)
A:torch.onnx.symbolic_opset10.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset10.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset10.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset10.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset10.scales->torch.onnx.symbolic_helper._interpolate_size_to_scales(g, input, output_size, dim)
A:torch.onnx.symbolic_opset10.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset10.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset10.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset10.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset10.starts->g.op('Constant', value_t=torch.tensor(starts))
A:torch.onnx.symbolic_opset10.ends->g.op('Constant', value_t=torch.tensor(ends))
A:torch.onnx.symbolic_opset10.axes->g.op('Constant', value_t=torch.tensor(axes))
A:torch.onnx.symbolic_opset10.steps->g.op('Constant', value_t=torch.tensor(steps))
torch.onnx.symbolic_opset10._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset10._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset10._max_pool(name,tuple_fn,ndims,return_indices)
torch.onnx.symbolic_opset10._slice(g,input,axes,starts,ends,steps=None,dynamic_slice=False)
torch.onnx.symbolic_opset10.flip(g,input,dims)
torch.onnx.symbolic_opset10.slice(g,self,dim,start,end,step)
torch.onnx.symbolic_opset10.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset10.topk(g,self,k,dim,largest,sorted,out=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_opset11.py----------------------------------------
A:torch.onnx.symbolic_opset11.vars()[black_listed_op]->_black_list_in_opset(black_listed_op)
A:torch.onnx.symbolic_opset11.dtype->self.type().scalarType()
A:torch.onnx.symbolic_opset11.min->_cast_if_not_none(min, dtype)
A:torch.onnx.symbolic_opset11.max->_cast_if_not_none(max, dtype)
A:torch.onnx.symbolic_opset11.dims->self.type().sizes()
A:torch.onnx.symbolic_opset11.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset11.output_size->g.op('Constant', value_t=torch.tensor(output_size))
A:torch.onnx.symbolic_opset11.offsets->g.op('Constant', value_t=torch.ones(2, dtype=torch.int64))
A:torch.onnx.symbolic_opset11.empty_tensor->g.op('Constant', value_t=torch.tensor([], dtype=torch.float32))
A:torch.onnx.symbolic_opset11.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset11.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset11.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset11.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset11.upsample_bicubic2d->_interpolate('upsample_bicubic2d', 4, 'cubic')
A:torch.onnx.symbolic_opset11.dim_tensor->g.op('Constant', value_t=torch.tensor(dim))
A:torch.onnx.symbolic_opset11.csum->g.op('Cast', csum, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset11.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset11.(u, indices, inverse_indices, counts)->g.op('Unique', self, axis_i=dim, sorted_i=sorted, outputs=4)
torch.onnx.symbolic_opset11._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset11._unique2(g,self,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset11.clamp(g,self,min,max)
torch.onnx.symbolic_opset11.cumsum(g,self,dim,dtype=None)
torch.onnx.symbolic_opset11.gather(g,self,dim,index,sparse_grad=False)
torch.onnx.symbolic_opset11.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic_opset11.round(g,self)
torch.onnx.symbolic_opset11.scatter(g,self,dim,index,src)
torch.onnx.symbolic_opset11.unique_dim(g,self,dim,sorted,return_inverse,return_counts)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/utils.py----------------------------------------
A:torch.onnx.utils.output_type->node.output().type()
A:torch.onnx.utils.lc->g.create('prim::ListConstruct', inputs).insertBefore(node).output().setType(ListType.ofTensors())
A:torch.onnx.utils.graph->_assign_output_shapes(graph, out_vars)
A:torch.onnx.utils.(trace, torch_out, inputs_states)->torch.jit.get_trace_graph(model, args, _force_outplace=True, _return_inputs_states=True)
A:torch.onnx.utils.orig_state_dict_keys->_unique_state_dict(model).keys()
A:torch.onnx.utils.(method_graph, params)->model.forward._lowered_graph()
A:torch.onnx.utils.(in_vars, in_desc)->torch.jit._flatten(tuple(args))
A:torch.onnx.utils.(graph, torch_out)->_trace_and_get_graph_from_model(model, args, training)
A:torch.onnx.utils.state_dict->_unique_state_dict(model)
A:torch.onnx.utils.params->list(state_dict.values())
A:torch.onnx.utils.graph_inputs->list(graph.inputs())
A:torch.onnx.utils.param_names->list(state_dict.keys())
A:torch.onnx.utils.(out_vars, _)->torch.jit._flatten(tuple(example_outputs))
A:torch.onnx.utils.(output_tensors, _)->torch._C._jit_flatten(torch_out)
A:torch.onnx.utils.(flatten_args, _)->torch._C._jit_flatten(args)
A:torch.onnx.utils.params_dict->torch._C._jit_pass_onnx_constant_fold(graph, params_dict, _export_onnx_opset_version)
A:torch.onnx.utils.(graph, params_dict, torch_out)->_model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, fixed_batch_size=fixed_batch_size)
A:torch.onnx.utils.(proto, export_map)->_assign_output_shapes(graph, out_vars)._export_onnx({}, opset_version, dynamic_axes, False, operator_export_type, strip_doc_string, val_keep_init_as_ip)
A:torch.onnx.utils.model_proto_file->os.path.join(f, ONNX_ARCHIVE_MODEL_PROTO_NAME)
A:torch.onnx.utils.weight_proto_file->os.path.join(f, k)
A:torch.onnx.utils.attr_pattern->re.compile('^(.+)_([ifstgz])$')
A:torch.onnx.utils.m->re.compile('^(.+)_([ifstgz])$').match(key)
A:torch.onnx.utils.value->_scalar(value)
A:torch.onnx.utils.aten->dict(((k, v) for (k, v) in kwargs.items() if v is not None)).pop('aten', False)
A:torch.onnx.utils.n->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs))
A:torch.onnx.utils.outputs->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).outputsSize()
A:torch.onnx.utils.kwargs->dict(((k, v) for (k, v) in kwargs.items() if v is not None))
A:torch.onnx.utils.args->list((const_if_tensor(arg) for arg in raw_args))
A:torch.onnx.utils.ns_op_name->g.insertNode(_newNode(g, opname, outputs, *args, **kwargs)).kind()
A:torch.onnx.utils.(ns, op_name)->symbolic_name.split('::')
A:torch.onnx.utils.is_exportable_aten_op->torch.onnx.symbolic_registry.is_registered_op(op_name, '', opset_version)
A:torch.onnx.utils.op_fn->torch.onnx.symbolic_registry.get_registered_op(op_name, '', opset_version)
A:torch.onnx.utils.new_op_outputs->g.op(op_name, *inputs, outputs=n.outputsSize())
A:torch.onnx.utils.new_block->new_node.addBlock()
A:torch.onnx.utils.is_exportable->torch.onnx.symbolic_registry.is_registered_op(symbolic_name, '', opset_version)
A:torch.onnx.utils.symbolic_fn->torch.onnx.symbolic_registry.get_registered_op(op_name, ns, opset_version)
A:torch.onnx.utils.type->type.lower().lower()
A:torch.onnx.utils.tensor->torch.DoubleTensor(*dims)
A:torch.onnx.utils.sel->self.kindOf(k)
A:torch.onnx.utils.valid_names->set((input_names or []) + (output_names or []))
torch.onnx.utils._add_attribute(node,key,value,aten)
torch.onnx.utils._export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=None,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,opset_version=None,_retain_param_name=False,do_constant_folding=False,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,fixed_batch_size=False)
torch.onnx.utils._export_to_pretty_string(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,google_printer=False,opset_version=None,_retain_param_name=False,do_constant_folding=False,keep_initializers_as_inputs=None,fixed_batch_size=False)
torch.onnx.utils._graph_at(g,opname,*args,**kwargs)
torch.onnx.utils._graph_constant(g,value,dims,type,*args,**kwargs)
torch.onnx.utils._graph_op(g,opname,*raw_args,**kwargs)
torch.onnx.utils._is_onnx_list(value)
torch.onnx.utils._model_to_graph(model,args,verbose=False,training=False,input_names=None,output_names=None,operator_export_type=OperatorExportTypes.ONNX,example_outputs=None,propagate=False,_retain_param_name=False,do_constant_folding=False,_disable_torch_constant_prop=False,fixed_batch_size=False)
torch.onnx.utils._newNode(g,opname,outputs,*args,**kwargs)
torch.onnx.utils._node_getitem(self,k)
torch.onnx.utils._optimize_graph(graph,operator_export_type,_disable_torch_constant_prop=False,fixed_batch_size=False)
torch.onnx.utils._run_symbolic_function(g,n,inputs,env,operator_export_type=OperatorExportTypes.ONNX)
torch.onnx.utils._run_symbolic_method(op_name,symbolic_fn,args)
torch.onnx.utils._scalar(x)
torch.onnx.utils._set_input_and_output_names(graph,input_names,output_names)
torch.onnx.utils._split_tensor_list_constants(g,block)
torch.onnx.utils._trace(func,args,operator_export_type,return_outs=False)
torch.onnx.utils._trace_and_get_graph_from_model(model,args,training)
torch.onnx.utils._validate_dynamic_axes(dynamic_axes,model,input_names,output_names)
torch.onnx.utils.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=False,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None)
torch.onnx.utils.export_to_pretty_string(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,export_type=ExportTypes.PROTOBUF_FILE,example_outputs=None,propagate=False,google_printer=False,opset_version=None,_retain_param_name=True,keep_initializers_as_inputs=None)
torch.onnx.utils.is_in_onnx_export()
torch.onnx.utils.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)
torch.onnx.utils.set_training(model,mode)
torch.onnx.utils.warn_on_static_input_change(input_states)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_opset9.py----------------------------------------
A:torch.onnx.symbolic_opset9.n->g.op('ConvTranspose' if transposed else 'Conv', *args, **kwargs)
A:torch.onnx.symbolic_opset9.shape->g.op('Constant', value_t=torch.LongTensor(output_dims))
A:torch.onnx.symbolic_opset9.tensors->torch.onnx.symbolic_helper._unpack_list(tensor_list)
A:torch.onnx.symbolic_opset9.C->g.op('Constant', value_t=torch.tensor([1]))
A:torch.onnx.symbolic_opset9.overloads->fn(g, *args)
A:torch.onnx.symbolic_opset9.symbolic->_reduce_op_symbolic(onnx_op, allow_multi_dim_support=allow_multi_dim_support)
A:torch.onnx.symbolic_opset9.sum->g.op('ReduceSum', exp, axes_i=[dim])
A:torch.onnx.symbolic_opset9.mean->g.op('ReduceMean', input, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.prod->_reduce_with_dtype('ReduceProd', 'prod', allow_multi_dim_support=False)
A:torch.onnx.symbolic_opset9.size->torch.onnx.symbolic_helper._maybe_get_const(size, 'is')
A:torch.onnx.symbolic_opset9.rank->g.op('Transpose', self, perm_i=adv_idx_permute).type().dim()
A:torch.onnx.symbolic_opset9.dim->_parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset9.full_shape->g.op('Shape', self)
A:torch.onnx.symbolic_opset9.axes->list(range(self.type().dim()))
A:torch.onnx.symbolic_opset9.self_sizes->g.op('Transpose', self, perm_i=adv_idx_permute).type().sizes()
A:torch.onnx.symbolic_opset9.index_val->_parse_arg(index, 'i')
A:torch.onnx.symbolic_opset9.slice_node->torch.onnx.symbolic_helper._slice_helper(g, self, axes=[dim], starts=[index_val], ends=[index_val + 1])
A:torch.onnx.symbolic_opset9.weight->torch.onnx.symbolic_helper._unpack_list(weight_v)
A:torch.onnx.symbolic_opset9.negative_slope->torch.onnx.symbolic_helper._get_const(negative_slope, 't', 'negative_slope')
A:torch.onnx.symbolic_opset9.(first, second)->g.op('Split', input, axis_i=dim, outputs=2)
A:torch.onnx.symbolic_opset9.input_dim->g.op('Transpose', input, perm_i=[1, 0, 2]).type().dim()
A:torch.onnx.symbolic_opset9.softmax->g.op('Cast', softmax, to_i=sym_help.scalar_type_to_onnx[parsed_dtype])
A:torch.onnx.symbolic_opset9.parsed_dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
A:torch.onnx.symbolic_opset9.exp->g.op('Exp', input)
A:torch.onnx.symbolic_opset9.padding->tuple(tuple_fn(padding))
A:torch.onnx.symbolic_opset9.padding_ceil->get_pool_ceil_padding(input, kernel_size, stride, padding)
A:torch.onnx.symbolic_opset9.(r, indices)->g.op('MaxPool', input, outputs=2, **kwargs)
A:torch.onnx.symbolic_opset9.(_, flattened_indices)->g.op('MaxPool', input, outputs=2, kernel_shape_i=[1 for _ in range(ndims)], strides_i=[1 for _ in range(ndims)])
A:torch.onnx.symbolic_opset9.s->torch.onnx.symbolic_helper._slice_helper(g, flattened_indices, axes=[2 + i for i in range(ndims)], starts=tuple_fn(0), ends=tuple_fn(1))
A:torch.onnx.symbolic_opset9.indices->torch.onnx.symbolic_helper._unpack_list(index)
A:torch.onnx.symbolic_opset9.r->g.op('MaxPool', input, outputs=1, **kwargs)
A:torch.onnx.symbolic_opset9.max_pool1d->_max_pool('max_pool1d', _single, 1, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool2d->_max_pool('max_pool2d', _pair, 2, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool3d->_max_pool('max_pool3d', _triple, 3, return_indices=False)
A:torch.onnx.symbolic_opset9.max_pool1d_with_indices->_max_pool('max_pool1d_with_indices', _single, 1, return_indices=True)
A:torch.onnx.symbolic_opset9.max_pool2d_with_indices->_max_pool('max_pool2d_with_indices', _pair, 2, return_indices=True)
A:torch.onnx.symbolic_opset9.max_pool3d_with_indices->_max_pool('max_pool3d_with_indices', _triple, 3, return_indices=True)
A:torch.onnx.symbolic_opset9.input->g.op('Transpose', input, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.output->g.op('IsNaN', input)
A:torch.onnx.symbolic_opset9.avg_pool1d->_avg_pool('avg_pool1d', _single)
A:torch.onnx.symbolic_opset9.avg_pool2d->_avg_pool('avg_pool2d', _pair)
A:torch.onnx.symbolic_opset9.avg_pool3d->_avg_pool('avg_pool3d', _triple)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool1d->_adaptive_pool('adaptive_avg_pool1d', 'AveragePool', _single)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool2d->_adaptive_pool('adaptive_avg_pool2d', 'AveragePool', _pair)
A:torch.onnx.symbolic_opset9.adaptive_avg_pool3d->_adaptive_pool('adaptive_avg_pool3d', 'AveragePool', _triple)
A:torch.onnx.symbolic_opset9.adaptive_max_pool1d->_adaptive_pool('adaptive_max_pool1d', 'MaxPool', _single, max_pool1d_with_indices)
A:torch.onnx.symbolic_opset9.adaptive_max_pool2d->_adaptive_pool('adaptive_max_pool2d', 'MaxPool', _pair, max_pool2d_with_indices)
A:torch.onnx.symbolic_opset9.adaptive_max_pool3d->_adaptive_pool('adaptive_max_pool3d', 'MaxPool', _triple, max_pool3d_with_indices)
A:torch.onnx.symbolic_opset9.paddings->prepare_onnx_paddings(input.type().dim(), padding)
A:torch.onnx.symbolic_opset9.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset9.scales->torch.onnx.symbolic_helper._interpolate_size_to_scales(g, input, output_size, dim)
A:torch.onnx.symbolic_opset9.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset9.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset9.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset9.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset9.from_cast_func->wrap_logical_op_with_cast_to(input.type().scalarType())(fn)
A:torch.onnx.symbolic_opset9.return_op->g.op('Cast', return_op, to_i=sym_help.scalar_type_to_onnx[dtype])
A:torch.onnx.symbolic_opset9.weight_size->torch.onnx.symbolic_helper._unpack_list(weight_v).type().sizes()
A:torch.onnx.symbolic_opset9.input_sizes->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic_opset9.weight_value->torch.tensor([1.0] * input_sizes[1]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_opset9.bias_value->torch.tensor([0.0] * input_sizes[1]).type('torch.' + input.type().scalarType() + 'Tensor')
A:torch.onnx.symbolic_opset9.bias->g.op('Constant', value_t=bias_value)
A:torch.onnx.symbolic_opset9.out->g.op('Squeeze', out, axes_i=[2])
A:torch.onnx.symbolic_opset9.res->g.op('Squeeze', res, axes_i=[2])
A:torch.onnx.symbolic_opset9.two_cst->g.op('Constant', value_t=torch.tensor(2.0))
A:torch.onnx.symbolic_opset9.eps_cst->g.op('Constant', value_t=torch.tensor(eps))
A:torch.onnx.symbolic_opset9.numerator->sub(g, input, mean)
A:torch.onnx.symbolic_opset9.variance->g.op('ReduceMean', pow(g, numerator, two_cst), axes_i=axes)
A:torch.onnx.symbolic_opset9.denominator->sqrt(g, add(g, variance, eps_cst))
A:torch.onnx.symbolic_opset9.layer_norm->add(g, layer_norm, bias)
A:torch.onnx.symbolic_opset9.index_const->torch.onnx.symbolic_helper._maybe_get_scalar(index)
A:torch.onnx.symbolic_opset9.index_dim->squeeze(g, nonzero(g, index), dim=1).type().dim()
A:torch.onnx.symbolic_opset9.index->squeeze(g, nonzero(g, index), dim=1)
A:torch.onnx.symbolic_opset9.indices_list->torch.onnx.symbolic_helper._unpack_list(indices_list_value)
A:torch.onnx.symbolic_opset9.dim_value->torch.onnx.symbolic_helper._parse_arg(dim, 'i')
A:torch.onnx.symbolic_opset9.self_dim->g.op('Transpose', self, perm_i=adv_idx_permute).type().dim()
A:torch.onnx.symbolic_opset9.unsqueezed_index->g.op('Unsqueeze', index, axes_i=[i for i in range(self_dim) if i != dim_value])
A:torch.onnx.symbolic_opset9.expanded_index_shape->g.op('Scatter', g.op('Shape', self), g.op('Unsqueeze', dim, axes_i=[0]), g.op('Shape', index), axis_i=0)
A:torch.onnx.symbolic_opset9.expanded_index->expand(g, unsqueezed_index, expanded_index_shape, None)
A:torch.onnx.symbolic_opset9.value->torch.onnx.symbolic_helper._maybe_get_scalar(value)
A:torch.onnx.symbolic_opset9.expanded_value->expand(g, value, expanded_index_shape, None)
A:torch.onnx.symbolic_opset9.other_type_name->other.type().scalarType()
A:torch.onnx.symbolic_opset9.min->g.op('ReduceMin', self, axes_i=[dim], keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.max->g.op('ReduceMax', self, axes_i=[dim], keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.keepdim->_parse_arg(keepdim, 'i')
A:torch.onnx.symbolic_opset9.(r, _)->g.op('Dropout', input, ratio_f=p, outputs=2)
A:torch.onnx.symbolic_opset9.feature_dropout->_unsupported_dropout('feature_dropout')
A:torch.onnx.symbolic_opset9.alpha_dropout->_unsupported_dropout('alpha_dropout')
A:torch.onnx.symbolic_opset9.feature_alpha_dropout->_unsupported_dropout('feature_alpha_dropout')
A:torch.onnx.symbolic_opset9.f->_reduce_op_symbolic('ReduceL2')
A:torch.onnx.symbolic_opset9.name->'_cast_{}'.format(k)
A:torch.onnx.symbolic_opset9.globals()[name]->parse_args('v', 'i')(partial(sym_help._cast_func_template, v))
A:torch.onnx.symbolic_opset9.const_value->torch.onnx.symbolic_helper._maybe_get_const(value, 't')
A:torch.onnx.symbolic_opset9.tmp->zeros(g, sizes, dtype, layout, device)
A:torch.onnx.symbolic_opset9.dtype->g.op('Transpose', self, perm_i=adv_idx_permute).type().scalarType()
A:torch.onnx.symbolic_opset9.start_unsqueezed->g.op('Unsqueeze', start, axes_i=[0])
A:torch.onnx.symbolic_opset9.end_unsqueezed->g.op('Unsqueeze', end, axes_i=[0])
A:torch.onnx.symbolic_opset9.dim_unsqueezed->g.op('Unsqueeze', dim, axes_i=[0])
A:torch.onnx.symbolic_opset9.start->g.op('Unsqueeze', args[0], axes_i=[0])
A:torch.onnx.symbolic_opset9.end->g.op('Unsqueeze', args[1], axes_i=[0])
A:torch.onnx.symbolic_opset9.repeats->g.op('Constant', value_t=torch.LongTensor(repeats))
A:torch.onnx.symbolic_opset9.const_repeats->torch.onnx.symbolic_helper._maybe_get_const(repeats, 'is')
A:torch.onnx.symbolic_opset9.sizes->g.op('Transpose', self, perm_i=adv_idx_permute).type().sizes()
A:torch.onnx.symbolic_opset9.self->g.op('Transpose', self, perm_i=adv_idx_permute)
A:torch.onnx.symbolic_opset9.dims->g.op('Transpose', self, perm_i=adv_idx_permute).type().sizes()
A:torch.onnx.symbolic_opset9.after_view->view(g, self, [-1, output_channel, upscale_factor, upscale_factor, dims[2], dims[3]])
A:torch.onnx.symbolic_opset9.after_transpose->g.op('Transpose', after_view, perm_i=[0, 1, 4, 2, 5, 3])
A:torch.onnx.symbolic_opset9.variantToOnnxActivationMap->dict(zip([act_fun.lower() for act_fun in onnxActivations], onnxActivations))
A:torch.onnx.symbolic_opset9.bias_concat->g.op('Concat', bias_f, bias_b, axis_i=0)
A:torch.onnx.symbolic_opset9.(weight_ih, weight_hh, bias_concat)->transform_weights(i)
A:torch.onnx.symbolic_opset9.(weight_ih_f, weight_hh_f, bias_f)->transform_weights(2 * i)
A:torch.onnx.symbolic_opset9.(weight_ih_b, weight_hh_b, bias_b)->transform_weights(2 * i + 1)
A:torch.onnx.symbolic_opset9.weight_ih->g.op('Concat', weight_ih_f, weight_ih_b, axis_i=0)
A:torch.onnx.symbolic_opset9.weight_hh->g.op('Concat', weight_hh_f, weight_hh_b, axis_i=0)
A:torch.onnx.symbolic_opset9.(prev_output, h_out)->g.op('GRU', *inputs, outputs=2, hidden_size_i=hidden_size, linear_before_reset_i=1, **extra_kwargs)
A:torch.onnx.symbolic_opset9.(prev_output, h_out, c_out)->g.op('LSTM', *inputs, outputs=3, hidden_size_i=hidden_size, **extra_kwargs)
A:torch.onnx.symbolic_opset9.prev_output->g.op('Transpose', prev_output, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.gru->_one_hidden_rnn('GRU')
A:torch.onnx.symbolic_opset9.rnn_tanh->_one_hidden_rnn('RNN_TANH')
A:torch.onnx.symbolic_opset9.rnn_relu->_one_hidden_rnn('RNN_RELU')
A:torch.onnx.symbolic_opset9.like_shape->g.op('Shape', like)
A:torch.onnx.symbolic_opset9.stop->g.op('Gather', like_shape, g.op('Constant', value_t=torch.tensor(dim)), axis_i=0)
A:torch.onnx.symbolic_opset9.lengths->_cast_Int(g, lengths, False)
A:torch.onnx.symbolic_opset9.(data, lengths)->g.op('prim::PadPacked', data, batch_sizes, outputs=2)
A:torch.onnx.symbolic_opset9.data->g.op('Transpose', data, perm_i=[1, 0, 2])
A:torch.onnx.symbolic_opset9.shapes_list->list(shapes)
A:torch.onnx.symbolic_opset9.p->_reshape_from_tensor(g, input, shape)
A:torch.onnx.symbolic_opset9.input_dims->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic_opset9.flattened->reshape(g, input, (-1,))
A:torch.onnx.symbolic_opset9.to_add->torch.onnx.symbolic_helper._scatter_helper(g, to_add, dim, index, src)
A:torch.onnx.symbolic_opset9.values->g.op('Constant', value_t=torch.LongTensor([0, 1]))
A:torch.onnx.symbolic_opset9.depth->size(g, self, g.op('Constant', value_t=torch.LongTensor([dim])))
A:torch.onnx.symbolic_opset9.mul->g.op('Mul', var, g.op('Constant', value_t=torch.tensor(count, dtype=torch.float)))
A:torch.onnx.symbolic_opset9.sqrd->g.op('Mul', input, input)
A:torch.onnx.symbolic_opset9.sqrdmean->g.op('ReduceMean', sqrd, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.redudced_dims->g.op('Transpose', input, perm_i=[1, 0, 2]).type().sizes()
A:torch.onnx.symbolic_opset9.meansqrd->g.op('Mul', mean, mean)
A:torch.onnx.symbolic_opset9.var->g.op('Div', mul, g.op('Constant', value_t=torch.tensor(count - 1, dtype=torch.float)))
A:torch.onnx.symbolic_opset9.count->numpy.prod(redudced_dims)
A:torch.onnx.symbolic_opset9.std->g.op('Sqrt', var)
A:torch.onnx.symbolic_opset9.arange_tensor->g.op('Add', g.op('Mul', arange_tensor, step), start)
A:torch.onnx.symbolic_opset9.range_tensor->g.op('Div', g.op('Sub', end, start), step)
A:torch.onnx.symbolic_opset9.step->g.op('Unsqueeze', args[2], axes_i=[0])
A:torch.onnx.symbolic_opset9.mask->_cast_Bool(g, mask, False)
A:torch.onnx.symbolic_opset9.adv_idx_count->len(adv_idx_indices)
A:torch.onnx.symbolic_opset9.shape_tensor->_shape_as_tensor(g, self)
A:torch.onnx.symbolic_opset9.adv_index->g.op('Mul', indices[adv_idx_indices[i]], multiplier)
A:torch.onnx.symbolic_opset9.cum_adv_index->g.op('Add', cum_adv_index, adv_index)
A:torch.onnx.symbolic_opset9.multiplier->g.op('Mul', multiplier, dim_tensor_list[adv_idx_indices[i]])
A:torch.onnx.symbolic_opset9.cum_adv_index_shape_tensor->_shape_as_tensor(g, cum_adv_index)
A:torch.onnx.symbolic_opset9.folded_adv_idx_shape->g.op('Concat', *folded_adv_idx_shape_list, axis_i=0)
A:torch.onnx.symbolic_opset9.final_shape->g.op('Concat', cum_adv_index_shape_tensor, *[dim_tensor_list[i] for i in range(rank) if i not in adv_idx_indices], axis_i=0)
A:torch.onnx.symbolic_opset9.sqr->g.op('Mul', self, self)
A:torch.onnx.symbolic_opset9.sumsqr->g.op('ReduceSum', sqr, axes_i=dim, keepdims_i=keepdim)
A:torch.onnx.symbolic_opset9.log_input->log(g, input)
A:torch.onnx.symbolic_opset9.batch_mul->matmul(g, batch1, batch2)
A:torch.onnx.symbolic_opset9.mul_a->mul(g, batch_mul, g.op('Cast', alpha, to_i=sym_help.cast_pytorch_to_onnx[dtype]))
A:torch.onnx.symbolic_opset9.mul_b->mul(g, self, g.op('Cast', beta, to_i=sym_help.cast_pytorch_to_onnx[dtype]))
A:torch.onnx.symbolic_opset9.erf->g.op('Erf', div(g, self, torch.tensor(_sqrt2)))
A:torch.onnx.symbolic_opset9.erf_plusone->add(g, erf, g.op('Constant', value_t=torch.tensor(1, dtype=torch.float)))
torch.onnx.symbolic_opset9.__and_(g,input,other)
torch.onnx.symbolic_opset9.__or_(g,input,other)
torch.onnx.symbolic_opset9._adaptive_pool(name,type,tuple_fn,fn=None)
torch.onnx.symbolic_opset9._avg_pool(name,tuple_fn)
torch.onnx.symbolic_opset9._convolution(g,input,weight,bias,stride,padding,dilation,transposed,output_padding,groups,benchmark,deterministic,cudnn_enabled)
torch.onnx.symbolic_opset9._dim_arange(g,like,dim)
torch.onnx.symbolic_opset9._generic_rnn(g,variant,input,initial_states,all_weights,has_biases,num_layers,dropout,train,bidirectional,batch_first=None,batch_sizes=None)
torch.onnx.symbolic_opset9._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset9._lstm_full(g,input,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional,batch_first)
torch.onnx.symbolic_opset9._lstm_packed(g,input,batch_sizes,hidden_v,weight_v,has_biases,num_layers,dropout,train,bidirectional)
torch.onnx.symbolic_opset9._max_pool(name,tuple_fn,ndims,return_indices)
torch.onnx.symbolic_opset9._one_hidden_rnn(kind)
torch.onnx.symbolic_opset9._pack_padded_sequence(g,input,lengths,batch_first)
torch.onnx.symbolic_opset9._pad_packed_sequence(g,data,batch_sizes,batch_first,padding_value,total_length)
torch.onnx.symbolic_opset9._reduce_op_symbolic(onnx_op_name,allow_multi_dim_support=True)
torch.onnx.symbolic_opset9._reduce_with_dtype(onnx_op,name,allow_multi_dim_support=True)
torch.onnx.symbolic_opset9._reshape_from_tensor(g,input,shape)
torch.onnx.symbolic_opset9._sample_dirichlet(g,self,generator)
torch.onnx.symbolic_opset9._shape_as_tensor(g,input)
torch.onnx.symbolic_opset9._slice(g,input,axes,starts,ends)
torch.onnx.symbolic_opset9._standard_gamma(g,self,generator)
torch.onnx.symbolic_opset9._std(g,input,dim,unbiased,keepdim)
torch.onnx.symbolic_opset9._unique(g,input,sorted,return_inverse)
torch.onnx.symbolic_opset9._unique2(g,input,sorted,return_inverse,return_counts)
torch.onnx.symbolic_opset9._unsupported_dropout(name)
torch.onnx.symbolic_opset9._weight_norm(graph,v,g,dim)
torch.onnx.symbolic_opset9.abs(g,self)
torch.onnx.symbolic_opset9.acos(g,self)
torch.onnx.symbolic_opset9.add(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic_opset9.alias(g,self)
torch.onnx.symbolic_opset9.arange(g,*args)
torch.onnx.symbolic_opset9.argmax(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.argmin(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.asin(g,self)
torch.onnx.symbolic_opset9.atan(g,self)
torch.onnx.symbolic_opset9.baddbmm(g,self,batch1,batch2,beta,alpha)
torch.onnx.symbolic_opset9.batch_norm(g,input,weight,bias,running_mean,running_var,training,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.bmm(g,self,other)
torch.onnx.symbolic_opset9.cat(g,tensor_list,dim)
torch.onnx.symbolic_opset9.ceil(g,input)
torch.onnx.symbolic_opset9.clamp(g,self,min,max)
torch.onnx.symbolic_opset9.clamp_max(g,self,max)
torch.onnx.symbolic_opset9.clamp_min(g,self,min)
torch.onnx.symbolic_opset9.clone(g,input)
torch.onnx.symbolic_opset9.constant_pad_nd(g,input,padding,value)
torch.onnx.symbolic_opset9.contiguous(g,input,memory_format)
torch.onnx.symbolic_opset9.conv_tbc(g,input,weight,bias,pad)
torch.onnx.symbolic_opset9.cos(g,self)
torch.onnx.symbolic_opset9.cosine_similarity(g,x1,x2,dim,eps)
torch.onnx.symbolic_opset9.cumsum(g,input,dim,dtype)
torch.onnx.symbolic_opset9.detach(g,input)
torch.onnx.symbolic_opset9.div(g,self,other)
torch.onnx.symbolic_opset9.dropout(g,input,p,train)
torch.onnx.symbolic_opset9.elu(g,input,alpha,scale,input_scale)
torch.onnx.symbolic_opset9.embedding(g,weight,indices,padding_idx,scale_grad_by_freq,sparse)
torch.onnx.symbolic_opset9.embedding_bag(g,embedding_matrix,indices,offsets,scale_grad_by_freq,mode,sparse,per_sample_weights)
torch.onnx.symbolic_opset9.empty(g,sizes,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.empty_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset9.eq(g,self,other)
torch.onnx.symbolic_opset9.erf(g,input)
torch.onnx.symbolic_opset9.exp(g,self)
torch.onnx.symbolic_opset9.expand(g,self,size,implicit)
torch.onnx.symbolic_opset9.expand_as(g,self,other)
torch.onnx.symbolic_opset9.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset9.floor(g,input)
torch.onnx.symbolic_opset9.frobenius_norm(g,self,dim=None,keepdim=False)
torch.onnx.symbolic_opset9.full(g,sizes,value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.full_like(g,input,fill_value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.gather(g,self,dim,index,sparse_grad=False)
torch.onnx.symbolic_opset9.ge(g,input,other)
torch.onnx.symbolic_opset9.gelu(g,self)
torch.onnx.symbolic_opset9.get_pool_ceil_padding(input,kernel_size,stride,padding)
torch.onnx.symbolic_opset9.glu(g,input,dim)
torch.onnx.symbolic_opset9.group_norm(g,input,num_groups,weight,bias,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.gt(g,input,other)
torch.onnx.symbolic_opset9.gt_impl(g,input,other)
torch.onnx.symbolic_opset9.hardtanh(g,self,min_val,max_val)
torch.onnx.symbolic_opset9.index(g,self,index)
torch.onnx.symbolic_opset9.index_copy(g,self,dim,index,source)
torch.onnx.symbolic_opset9.index_fill(g,self,dim,index,value)
torch.onnx.symbolic_opset9.index_put(g,self,indices_list_value,values,accumulate)
torch.onnx.symbolic_opset9.index_select(g,self,dim,index)
torch.onnx.symbolic_opset9.instance_norm(g,input,weight,bias,running_mean,running_var,use_input_stats,momentum,eps,cudnn_enabled)
torch.onnx.symbolic_opset9.isnan(g,input)
torch.onnx.symbolic_opset9.layer_norm(g,input,normalized_shape,weight,bias,eps,cudnn_enable)
torch.onnx.symbolic_opset9.le(g,input,other)
torch.onnx.symbolic_opset9.leaky_relu(g,input,negative_slope,inplace=False)
torch.onnx.symbolic_opset9.log(g,self)
torch.onnx.symbolic_opset9.log1p(g,self)
torch.onnx.symbolic_opset9.log2(g,self)
torch.onnx.symbolic_opset9.log_sigmoid(g,input)
torch.onnx.symbolic_opset9.log_softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset9.logsumexp(g,input,dim,keepdim)
torch.onnx.symbolic_opset9.lstm(g,*args)
torch.onnx.symbolic_opset9.lt(g,input,other)
torch.onnx.symbolic_opset9.lt_impl(g,input,other)
torch.onnx.symbolic_opset9.masked_fill(g,self,mask,value)
torch.onnx.symbolic_opset9.matmul(g,self,other)
torch.onnx.symbolic_opset9.max(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset9.min(g,self,dim_or_y=None,keepdim=None)
torch.onnx.symbolic_opset9.mm(g,self,other)
torch.onnx.symbolic_opset9.mul(g,self,other)
torch.onnx.symbolic_opset9.multinomial(g,input,num_samples,replacement=False,generator=None)
torch.onnx.symbolic_opset9.narrow(g,input,dim,start,length)
torch.onnx.symbolic_opset9.ne(g,self,other)
torch.onnx.symbolic_opset9.neg(g,self)
torch.onnx.symbolic_opset9.nonzero(g,input)
torch.onnx.symbolic_opset9.norm(g,self,p,dim,keepdim)
torch.onnx.symbolic_opset9.ones(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.ones_like(g,input,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.overload_by_arg_count(fn)
torch.onnx.symbolic_opset9.permute(g,self,dims)
torch.onnx.symbolic_opset9.pixel_shuffle(g,self,upscale_factor)
torch.onnx.symbolic_opset9.pow(g,self,exponent)
torch.onnx.symbolic_opset9.prelu(g,self,weight)
torch.onnx.symbolic_opset9.prim_ConstantChunk(g,self,chunks,dim)
torch.onnx.symbolic_opset9.prim_ConstantSplit(g,self,split_size,dim)
torch.onnx.symbolic_opset9.prim_shape(g,self)
torch.onnx.symbolic_opset9.rand(g,*shapes)
torch.onnx.symbolic_opset9.randn(g,*shapes)
torch.onnx.symbolic_opset9.randn_like(g,self,*others)
torch.onnx.symbolic_opset9.reciprocal(g,self)
torch.onnx.symbolic_opset9.reflection_pad(g,input,padding)
torch.onnx.symbolic_opset9.relu(g,input)
torch.onnx.symbolic_opset9.repeat(g,self,repeats)
torch.onnx.symbolic_opset9.replication_pad(g,input,padding)
torch.onnx.symbolic_opset9.reshape(g,self,shape)
torch.onnx.symbolic_opset9.reshape_as(g,self,other)
torch.onnx.symbolic_opset9.rrelu(g,input,lower,upper,training,generator)
torch.onnx.symbolic_opset9.rsqrt(g,self)
torch.onnx.symbolic_opset9.rsub(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.scatter(g,self,dim,index,src)
torch.onnx.symbolic_opset9.scatter_add(g,self,dim,index,src)
torch.onnx.symbolic_opset9.select(g,self,dim,index)
torch.onnx.symbolic_opset9.selu(g,input)
torch.onnx.symbolic_opset9.sigmoid(g,self)
torch.onnx.symbolic_opset9.sign(g,self)
torch.onnx.symbolic_opset9.sin(g,self)
torch.onnx.symbolic_opset9.size(g,self,dim)
torch.onnx.symbolic_opset9.slice(g,self,dim,start,end,step)
torch.onnx.symbolic_opset9.softmax(g,input,dim,dtype=None)
torch.onnx.symbolic_opset9.softplus(g,self,beta,threshold)
torch.onnx.symbolic_opset9.sort(g,self,dim,decending,out=None)
torch.onnx.symbolic_opset9.split(g,self,split_size,dim)
torch.onnx.symbolic_opset9.split_with_sizes(g,self,split_sizes,dim)
torch.onnx.symbolic_opset9.sqrt(g,self)
torch.onnx.symbolic_opset9.squeeze(g,self,dim=None)
torch.onnx.symbolic_opset9.stack(g,tensor_list,dim)
torch.onnx.symbolic_opset9.std(g,input,*args)
torch.onnx.symbolic_opset9.sub(g,self,other,alpha=None)
torch.onnx.symbolic_opset9.t(g,self)
torch.onnx.symbolic_opset9.tan(g,self)
torch.onnx.symbolic_opset9.tanh(g,self)
torch.onnx.symbolic_opset9.threshold(g,self,threshold,value)
torch.onnx.symbolic_opset9.to(g,self,*args)
torch.onnx.symbolic_opset9.topk(g,self,k,dim,largest,sorted,out=None)
torch.onnx.symbolic_opset9.transpose(g,self,dim0,dim1)
torch.onnx.symbolic_opset9.type_as(g,self,other)
torch.onnx.symbolic_opset9.unfold(g,input,dimension,size,step)
torch.onnx.symbolic_opset9.unsqueeze(g,self,dim)
torch.onnx.symbolic_opset9.unused(g)
torch.onnx.symbolic_opset9.view(g,self,size)
torch.onnx.symbolic_opset9.where(g,condition,self,other)
torch.onnx.symbolic_opset9.wrap_logical_op_with_cast_to(to_type)
torch.onnx.symbolic_opset9.wrap_logical_op_with_cast_to_and_from(to_type)
torch.onnx.symbolic_opset9.wrap_logical_op_with_negation(func)
torch.onnx.symbolic_opset9.zeros(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset9.zeros_like(g,input,dtype,layout,device,pin_memory=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_helper.py----------------------------------------
A:torch.onnx.symbolic_helper.value_t->_maybe_get_const(value, 't')
A:torch.onnx.symbolic_helper.list_node->list_value.node()
A:torch.onnx.symbolic_helper.wrapper->wraps(fn)(wrapper)
A:torch.onnx.symbolic_helper.scalar_type->tensor.type().scalarType()
A:torch.onnx.symbolic_helper.ty->tensor.type().scalarType().lower()
A:torch.onnx.symbolic_helper.output_size->_maybe_get_const(output_size, 'is')
A:torch.onnx.symbolic_helper.offsets->g.op('Constant', value_t=torch.ones(offset))
A:torch.onnx.symbolic_helper.dividend->g.op('Cast', output_size, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.divisor->g.op('Cast', divisor, to_i=cast_pytorch_to_onnx['Float'])
A:torch.onnx.symbolic_helper.scale_dims->g.op('Div', dividend, divisor)
A:torch.onnx.symbolic_helper.scales->g.op('Constant', value_t=torch.tensor(scales_constant))
torch.onnx.symbolic_helper._black_list_in_opset(name)
torch.onnx.symbolic_helper._cast_func_template(to_i,g,input,non_blocking)
torch.onnx.symbolic_helper._get_const(value,desc,arg_name)
torch.onnx.symbolic_helper._if_scalar_type_as(g,self,tensor)
torch.onnx.symbolic_helper._interpolate_size_to_scales(g,input,output_size,dim)
torch.onnx.symbolic_helper._interpolate_warning(interpolate_mode)
torch.onnx.symbolic_helper._is_packed_list(list_value)
torch.onnx.symbolic_helper._is_tensor_list(x)
torch.onnx.symbolic_helper._is_value(x)
torch.onnx.symbolic_helper._maybe_get_const(value,desc)
torch.onnx.symbolic_helper._maybe_get_scalar(value)
torch.onnx.symbolic_helper._parse_arg(value,desc)
torch.onnx.symbolic_helper._scalar(x)
torch.onnx.symbolic_helper._scatter_helper(g,self,dim,index,src)
torch.onnx.symbolic_helper._set_operator_export_type(operator_export_type)
torch.onnx.symbolic_helper._set_opset_version(opset_version)
torch.onnx.symbolic_helper._slice_helper(g,input,axes,starts,ends,steps=None,dynamic_slice=False)
torch.onnx.symbolic_helper._try_get_scalar_type(*args)
torch.onnx.symbolic_helper._unimplemented(op,msg)
torch.onnx.symbolic_helper._unpack_list(list_value)
torch.onnx.symbolic_helper.parse_args(*arg_descriptors)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_registry.py----------------------------------------
A:torch.onnx.symbolic_registry.module->importlib.import_module('torch.onnx.symbolic_opset{}'.format(opset_version))
A:torch.onnx.symbolic_registry.version_ops->get_ops_in_version(iter_version)
torch.onnx.symbolic_registry.get_ops_in_version(version)
torch.onnx.symbolic_registry.get_registered_op(opname,domain,version)
torch.onnx.symbolic_registry.is_registered_op(opname,domain,version)
torch.onnx.symbolic_registry.is_registered_version(domain,version)
torch.onnx.symbolic_registry.register_op(opname,op,domain,version)
torch.onnx.symbolic_registry.register_ops_helper(domain,version,iter_version)
torch.onnx.symbolic_registry.register_ops_in_version(domain,version)
torch.onnx.symbolic_registry.register_version(domain,version)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/onnx/symbolic_opset8.py----------------------------------------
A:torch.onnx.symbolic_opset8.vars()[black_listed_op]->_black_list_in_opset(black_listed_op)
A:torch.onnx.symbolic_opset8.align_corners->torch.onnx.symbolic_helper._maybe_get_scalar(align_corners)
A:torch.onnx.symbolic_opset8.output_size->torch.onnx.symbolic_helper._maybe_get_const(output_size, 'is')
A:torch.onnx.symbolic_opset8.upsample_nearest1d->_interpolate('upsample_nearest1d', 3, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_nearest2d->_interpolate('upsample_nearest2d', 4, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_nearest3d->_interpolate('upsample_nearest3d', 5, 'nearest')
A:torch.onnx.symbolic_opset8.upsample_linear1d->_interpolate('upsample_linear1d', 3, 'linear')
A:torch.onnx.symbolic_opset8.upsample_bilinear2d->_interpolate('upsample_bilinear2d', 4, 'linear')
A:torch.onnx.symbolic_opset8.upsample_trilinear3d->_interpolate('upsample_trilinear3d', 5, 'linear')
A:torch.onnx.symbolic_opset8.arg0_type->args[0].type().scalarType()
A:torch.onnx.symbolic_opset8.args->tuple((_cast_Float(g, arg, False) for arg in args))
A:torch.onnx.symbolic_opset8.other->torch.onnx.symbolic_helper._if_scalar_type_as(g, other, input)
A:torch.onnx.symbolic_opset8.(_, input, other)->_try_cast_integer_to_float(g, input, other)
A:torch.onnx.symbolic_opset8.(old_type, self, other)->_try_cast_integer_to_float(g, self, other)
A:torch.onnx.symbolic_opset8.self_sizes->self.type().sizes()
A:torch.onnx.symbolic_opset8.weight->g.op('Unsqueeze', weight, axes_i=list(range(1, len(self_sizes) - 1)))
A:torch.onnx.symbolic_opset8.(old_type, self, weight)->_try_cast_integer_to_float(g, self, weight)
A:torch.onnx.symbolic_opset8.ty->torch.onnx.symbolic_helper._try_get_scalar_type(self, other).lower()
A:torch.onnx.symbolic_opset8.C->g.constant(0, [1], ty)
A:torch.onnx.symbolic_opset8.(old_type, self, other, C)->_try_cast_integer_to_float(g, self, other, C)
A:torch.onnx.symbolic_opset8.(old_type, self, mat1, mat2)->_try_cast_integer_to_float(g, self, mat1, mat2)
A:torch.onnx.symbolic_opset8.size->torch.onnx.symbolic_helper._maybe_get_const(size, 'is')
A:torch.onnx.symbolic_opset8.(old_type, self)->_try_cast_integer_to_float(g, self)
A:torch.onnx.symbolic_opset8.shape->g.op('Shape', input)
A:torch.onnx.symbolic_opset8.start_dim_i->torch.onnx.symbolic_helper._get_const(start_dim, 'i', 'start_dim')
A:torch.onnx.symbolic_opset8.end_dim_i->torch.onnx.symbolic_helper._get_const(end_dim, 'i', 'end_dim')
A:torch.onnx.symbolic_opset8.dim->input.type().dim()
A:torch.onnx.symbolic_opset8.(old_type, input)->_try_cast_integer_to_float(g, input)
A:torch.onnx.symbolic_opset8.result->g.op('ConstantFill', sizes, dtype_i=sym_help.cast_pytorch_to_onnx['Float'], input_as_shape_i=1, value_f=const_value)
A:torch.onnx.symbolic_opset8.const_value->torch.onnx.symbolic_helper._maybe_get_const(value, 't')
A:torch.onnx.symbolic_opset8.tmp->zeros(g, sizes, dtype, layout, device)
A:torch.onnx.symbolic_opset8.dtype->torch.onnx.symbolic_helper._get_const(dtype, 'i', 'dtype')
torch.onnx.symbolic_opset8._cast_to_type(g,input,to_type)
torch.onnx.symbolic_opset8._comparison_operator(g,input,other,op_name)
torch.onnx.symbolic_opset8._constant_fill(g,sizes,dtype,const_value)
torch.onnx.symbolic_opset8._interpolate(name,dim,interpolate_mode)
torch.onnx.symbolic_opset8._try_cast_integer_to_float(g,*args)
torch.onnx.symbolic_opset8.addmm(g,self,mat1,mat2,beta,alpha)
torch.onnx.symbolic_opset8.bmm(g,self,other)
torch.onnx.symbolic_opset8.empty(g,sizes,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.empty_like(g,input,dtype,layout,device,pin_memory=False,memory_format=None)
torch.onnx.symbolic_opset8.flatten(g,input,start_dim,end_dim)
torch.onnx.symbolic_opset8.full(g,sizes,value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.full_like(g,input,fill_value,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.gt(g,input,other)
torch.onnx.symbolic_opset8.lt(g,input,other)
torch.onnx.symbolic_opset8.matmul(g,self,other)
torch.onnx.symbolic_opset8.mm(g,self,other)
torch.onnx.symbolic_opset8.ones(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.ones_like(g,input,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.prelu(g,self,weight)
torch.onnx.symbolic_opset8.view(g,self,size)
torch.onnx.symbolic_opset8.zeros(g,sizes,dtype,layout,device,pin_memory=False)
torch.onnx.symbolic_opset8.zeros_like(g,input,dtype,layout,device,pin_memory=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py----------------------------------------
A:torch.distributed.distributed_c10d.value->getattr(Backend, name.upper(), Backend.UNDEFINED)
A:torch.distributed.distributed_c10d.reduce_op->reduce_op()
A:torch.distributed.distributed_c10d.WORLD->object()
A:torch.distributed.distributed_c10d.NON_GROUP_MEMBER->object()
A:torch.distributed.distributed_c10d._default_pg_timeout->timedelta(minutes=30)
A:torch.distributed.distributed_c10d.backend->Backend(backend)
A:torch.distributed.distributed_c10d._default_pg->_new_process_group_helper(world_size, rank, [], backend, store, group_name=group_name, timeout=timeout)
A:torch.distributed.distributed_c10d.(store, rank, world_size)->next(rendezvous(url))
A:torch.distributed.distributed_c10d.group_name->str(_group_count)
A:torch.distributed.distributed_c10d.pg->_new_process_group_helper(group_world_size, group_rank, ranks, backend, default_store, timeout=timeout)
A:torch.distributed.distributed_c10d.global_rank->_new_process_group_helper(world_size, rank, [], backend, store, group_name=group_name, timeout=timeout).rank()
A:torch.distributed.distributed_c10d.prefix_store->PrefixStore(group_name, store)
A:torch.distributed.distributed_c10d.group_dst_rank->_get_group_rank(group, dst)
A:torch.distributed.distributed_c10d.group_src_rank->_get_group_rank(group, src)
A:torch.distributed.distributed_c10d.work->group.barrier()
A:torch.distributed.distributed_c10d.src_rank->group.barrier().source_rank()
A:torch.distributed.distributed_c10d.opts->ReduceScatterOptions()
A:torch.distributed.distributed_c10d.my_rank->get_rank()
A:torch.distributed.distributed_c10d.global_world_size->_new_process_group_helper(world_size, rank, [], backend, store, group_name=group_name, timeout=timeout).size()
A:torch.distributed.distributed_c10d.ranks->list(range(global_world_size))
A:torch.distributed.distributed_c10d.group_world_size->len(ranks)
A:torch.distributed.distributed_c10d.group_rank->list(range(global_world_size)).index(global_rank)
torch.distributed.Backend(cls,name)
torch.distributed.GroupMember(object)
torch.distributed._check_default_pg()
torch.distributed._check_single_tensor(param,param_name)
torch.distributed._check_tensor_list(param,param_name)
torch.distributed._get_default_group()
torch.distributed._get_default_store()
torch.distributed._get_global_rank(group,group_rank)
torch.distributed._get_group_rank(group,rank)
torch.distributed._get_group_size(group)
torch.distributed._new_process_group_helper(world_size,rank,group_ranks,backend,store,group_name=None,timeout=_default_pg_timeout)
torch.distributed._rank_not_in_group(group)
torch.distributed.all_gather(tensor_list,tensor,group=group.WORLD,async_op=False)
torch.distributed.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.all_reduce_coalesced(tensors,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.barrier(group=group.WORLD,async_op=False)
torch.distributed.broadcast(tensor,src,group=group.WORLD,async_op=False)
torch.distributed.broadcast_multigpu(tensor_list,src,group=group.WORLD,async_op=False,src_tensor=0)
torch.distributed.destroy_process_group(group=group.WORLD)
torch.distributed.distributed_c10d.Backend(cls,name)
torch.distributed.distributed_c10d.Backend.__new__(cls,name)
torch.distributed.distributed_c10d.GroupMember(object)
torch.distributed.distributed_c10d._check_default_pg()
torch.distributed.distributed_c10d._check_single_tensor(param,param_name)
torch.distributed.distributed_c10d._check_tensor_list(param,param_name)
torch.distributed.distributed_c10d._get_default_group()
torch.distributed.distributed_c10d._get_default_store()
torch.distributed.distributed_c10d._get_global_rank(group,group_rank)
torch.distributed.distributed_c10d._get_group_rank(group,rank)
torch.distributed.distributed_c10d._get_group_size(group)
torch.distributed.distributed_c10d._new_process_group_helper(world_size,rank,group_ranks,backend,store,group_name=None,timeout=_default_pg_timeout)
torch.distributed.distributed_c10d._rank_not_in_group(group)
torch.distributed.distributed_c10d.all_gather(tensor_list,tensor,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce(tensor,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce_coalesced(tensors,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.barrier(group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.broadcast(tensor,src,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.broadcast_multigpu(tensor_list,src,group=group.WORLD,async_op=False,src_tensor=0)
torch.distributed.distributed_c10d.destroy_process_group(group=group.WORLD)
torch.distributed.distributed_c10d.gather(tensor,gather_list=None,dst=0,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.get_backend(group=group.WORLD)
torch.distributed.distributed_c10d.get_rank(group=group.WORLD)
torch.distributed.distributed_c10d.get_world_size(group=group.WORLD)
torch.distributed.distributed_c10d.group(object)
torch.distributed.distributed_c10d.init_process_group(backend,init_method=None,timeout=_default_pg_timeout,world_size=-1,rank=-1,store=None,group_name='')
torch.distributed.distributed_c10d.irecv(tensor,src,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.is_gloo_available()
torch.distributed.distributed_c10d.is_initialized()
torch.distributed.distributed_c10d.is_mpi_available()
torch.distributed.distributed_c10d.is_nccl_available()
torch.distributed.distributed_c10d.isend(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.new_group(ranks=None,timeout=_default_pg_timeout,backend=None)
torch.distributed.distributed_c10d.recv(tensor,src=None,group=group.WORLD,tag=0)
torch.distributed.distributed_c10d.reduce(tensor,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False,dst_tensor=0)
torch.distributed.distributed_c10d.reduce_op(self)
torch.distributed.distributed_c10d.reduce_op.__getattribute__(self,key)
torch.distributed.distributed_c10d.reduce_op.__init__(self)
torch.distributed.distributed_c10d.reduce_scatter(output,input_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.reduce_scatter_multigpu(output_tensor_list,input_tensor_lists,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.scatter(tensor,scatter_list=None,src=0,group=group.WORLD,async_op=False)
torch.distributed.distributed_c10d.send(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.gather(tensor,gather_list=None,dst=0,group=group.WORLD,async_op=False)
torch.distributed.get_backend(group=group.WORLD)
torch.distributed.get_rank(group=group.WORLD)
torch.distributed.get_world_size(group=group.WORLD)
torch.distributed.group(object)
torch.distributed.init_process_group(backend,init_method=None,timeout=_default_pg_timeout,world_size=-1,rank=-1,store=None,group_name='')
torch.distributed.irecv(tensor,src,group=group.WORLD,tag=0)
torch.distributed.is_gloo_available()
torch.distributed.is_initialized()
torch.distributed.is_mpi_available()
torch.distributed.is_nccl_available()
torch.distributed.isend(tensor,dst,group=group.WORLD,tag=0)
torch.distributed.new_group(ranks=None,timeout=_default_pg_timeout,backend=None)
torch.distributed.recv(tensor,src=None,group=group.WORLD,tag=0)
torch.distributed.reduce(tensor,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=group.WORLD,async_op=False,dst_tensor=0)
torch.distributed.reduce_op(self)
torch.distributed.reduce_op.__getattribute__(self,key)
torch.distributed.reduce_scatter(output,input_list,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.reduce_scatter_multigpu(output_tensor_list,input_tensor_lists,op=ReduceOp.SUM,group=group.WORLD,async_op=False)
torch.distributed.scatter(tensor,scatter_list=None,src=0,group=group.WORLD,async_op=False)
torch.distributed.send(tensor,dst,group=group.WORLD,tag=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/__init__.py----------------------------------------
torch.distributed.__init__.is_available()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/rpc.py----------------------------------------
A:torch.distributed.rpc.group->_get_default_group()
A:torch.distributed.rpc._agent->init_rpc_backend(backend, self_name=self_name, init_method=init_method)
A:torch.distributed.rpc.qualified_name->torch.jit._find_builtin(func)
A:torch.distributed.rpc.fut->invoke_rpc_python_udf(_agent, _to_worker_id(to), serialize(PythonUDF(func, args, kwargs)))
torch.distributed.RpcBackend(Enum)
torch.distributed._init_rpc(backend=RpcBackend.PROCESS_GROUP,self_name=None,self_rank=-1,init_method=None,num_send_recv_threads=4)
torch.distributed._require_initialized(func)
torch.distributed._to_worker_id(name_or_id)
torch.distributed.get_worker_id(worker_name=None)
torch.distributed.join_rpc()
torch.distributed.remote(to,func,args=None,kwargs=None)
torch.distributed.rpc(to,func,args=None,kwargs=None,async_call=False)
torch.distributed.rpc.RpcBackend(Enum)
torch.distributed.rpc._init_rpc(backend=RpcBackend.PROCESS_GROUP,self_name=None,self_rank=-1,init_method=None,num_send_recv_threads=4)
torch.distributed.rpc._require_initialized(func)
torch.distributed.rpc._to_worker_id(name_or_id)
torch.distributed.rpc.get_worker_id(worker_name=None)
torch.distributed.rpc.join_rpc()
torch.distributed.rpc.remote(to,func,args=None,kwargs=None)
torch.distributed.rpc.rpc(to,func,args=None,kwargs=None,async_call=False)
torch.distributed.rpc.sync_rpc()
torch.distributed.sync_rpc()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/rendezvous.py----------------------------------------
A:torch.distributed.rendezvous.result->urlparse(url)
A:torch.distributed.rendezvous.query->dict((pair.split('=') for pair in filter(None, result.query.split('&'))))
A:torch.distributed.rendezvous.rank->int(rank)
A:torch.distributed.rendezvous.world_size->int(world_size)
A:torch.distributed.rendezvous.store->TCPStore(master_addr, master_port, world_size, start_daemon)
A:torch.distributed.rendezvous.master_addr->os.environ.get('MASTER_ADDR', None)
A:torch.distributed.rendezvous.master_port->int(master_port)
torch.distributed.rendezvous._env_rendezvous_handler(url)
torch.distributed.rendezvous._file_rendezvous_handler(url)
torch.distributed.rendezvous._rendezvous_error(msg)
torch.distributed.rendezvous._tcp_rendezvous_handler(url)
torch.distributed.rendezvous.register_rendezvous_handler(scheme,handler)
torch.distributed.rendezvous.rendezvous(url,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/internal_rpc_utils.py----------------------------------------
A:torch.distributed.internal_rpc_utils.f->io.BytesIO()
A:torch.distributed.internal_rpc_utils.p->pickle.Pickler(f)
A:torch.distributed.internal_rpc_utils.p.dispatch_table->copyreg.dispatch_table.copy()
A:torch.distributed.internal_rpc_utils.python_udf->pickle.loads(pickled_python_udf)
A:torch.distributed.internal_rpc_utils.result->pickle.loads(pickled_python_result)
A:torch.distributed.internal_rpc_utils.except_str->'{}\n{}'.format(repr(e), traceback.format_exc())
A:torch.distributed.internal_rpc_utils.PythonUDF->collections.namedtuple('PythonUDF', ['func', 'args', 'kwargs'])
A:torch.distributed.internal_rpc_utils.RemoteException->collections.namedtuple('RemoteException', ['msg'])
torch.distributed.internal_rpc_utils.load_python_udf_result_internal(pickled_python_result)
torch.distributed.internal_rpc_utils.run_python_udf_internal(pickled_python_udf)
torch.distributed.internal_rpc_utils.serialize(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/rpc_backend_registry.py----------------------------------------
A:torch.distributed.rpc_backend_registry.rpc_backend_registry->_get_rpc_backend_registry()
torch.distributed.rpc_backend_registry._get_rpc_backend_registry()
torch.distributed.rpc_backend_registry.init_rpc_backend(backend_name,*args,**kwargs)
torch.distributed.rpc_backend_registry.is_rpc_backend_registered(backend_name)
torch.distributed.rpc_backend_registry.register_rpc_backend(backend_name,init_rpc_backend_handler)


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/launch.py----------------------------------------
A:torch.distributed.launch.parser->ArgumentParser(description='PyTorch distributed training launch helper utilty that will spawn up multiple distributed processes')
A:torch.distributed.launch.args->parse_args()
A:torch.distributed.launch.current_env->os.environ.copy()
A:torch.distributed.launch.current_env['MASTER_PORT']->str(args.master_port)
A:torch.distributed.launch.current_env['WORLD_SIZE']->str(dist_world_size)
A:torch.distributed.launch.current_env['OMP_NUM_THREADS']->str(1)
A:torch.distributed.launch.current_env['RANK']->str(dist_rank)
A:torch.distributed.launch.current_env['LOCAL_RANK']->str(local_rank)
A:torch.distributed.launch.process->subprocess.Popen(cmd, env=current_env)
torch.distributed.launch.main()
torch.distributed.launch.parse_args()


----------------------------------------/dataset/nuaa/anaconda3/envs/torch1.3.0/lib/python3.6/site-packages/torch/distributed/autograd/__init__.py----------------------------------------
A:torch.distributed.autograd.__init__.self.autograd_context->_new_context()
torch.distributed.autograd.__init__.context(object)
torch.distributed.autograd.__init__.context.__enter__(self)
torch.distributed.autograd.__init__.context.__exit__(self,type,value,traceback)

