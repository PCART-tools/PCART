
----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_xlnet.py----------------------------------------
A:transformers.modeling_tf_xlnet.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_xlnet.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_xlnet.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_xlnet.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.modeling_tf_xlnet.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.modeling_tf_xlnet.initializer->get_initializer(self.initializer_range)
A:transformers.modeling_tf_xlnet.self.q->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='q')
A:transformers.modeling_tf_xlnet.self.k->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='k')
A:transformers.modeling_tf_xlnet.self.v->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='v')
A:transformers.modeling_tf_xlnet.self.o->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='o')
A:transformers.modeling_tf_xlnet.self.r->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='r')
A:transformers.modeling_tf_xlnet.self.r_r_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_r_bias')
A:transformers.modeling_tf_xlnet.self.r_s_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_s_bias')
A:transformers.modeling_tf_xlnet.self.r_w_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_w_bias')
A:transformers.modeling_tf_xlnet.self.seg_embed->self.add_weight(shape=(2, self.n_head, self.d_head), initializer=initializer, trainable=True, name='seg_embed')
A:transformers.modeling_tf_xlnet.x_size->shape_list(x)
A:transformers.modeling_tf_xlnet.x->tensorflow.reshape(x, (x_size[0], x_size[1] - 1, x_size[2], x_size[3]))
A:transformers.modeling_tf_xlnet.ac->tensorflow.einsum('ibnd,jbnd->ijbn', q_head + self.r_w_bias, k_head_h)
A:transformers.modeling_tf_xlnet.bd->self.rel_shift(bd, klen=ac.shape[1])
A:transformers.modeling_tf_xlnet.ef->tensorflow.einsum('ijbs,ibns->ijbn', seg_mat, ef)
A:transformers.modeling_tf_xlnet.attn_prob->self.dropout(attn_prob, training=training)
A:transformers.modeling_tf_xlnet.attn_vec->self.rel_attn_core([q_head_h, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_h, head_mask], training=training)
A:transformers.modeling_tf_xlnet.attn_out->self.dropout(attn_out, training=training)
A:transformers.modeling_tf_xlnet.output->self.sequence_summary(output)
A:transformers.modeling_tf_xlnet.cat->tensorflow.concat([mems, h], axis=0)
A:transformers.modeling_tf_xlnet.k_head_h->tensorflow.einsum('ibh,hnd->ibnd', cat, self.k)
A:transformers.modeling_tf_xlnet.v_head_h->tensorflow.einsum('ibh,hnd->ibnd', cat, self.v)
A:transformers.modeling_tf_xlnet.k_head_r->tensorflow.einsum('ibh,hnd->ibnd', r, self.r)
A:transformers.modeling_tf_xlnet.q_head_h->tensorflow.einsum('ibh,hnd->ibnd', h, self.q)
A:transformers.modeling_tf_xlnet.attn_vec_h->self.rel_attn_core([q_head_h, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_h, head_mask], training=training)
A:transformers.modeling_tf_xlnet.output_h->self.dropout(word_emb_k, training=training)
A:transformers.modeling_tf_xlnet.q_head_g->tensorflow.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)
A:transformers.modeling_tf_xlnet.attn_vec_g->self.rel_attn_core([q_head_g, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_g, head_mask], training=training)
A:transformers.modeling_tf_xlnet.output_g->self.dropout(word_emb_q, training=training)
A:transformers.modeling_tf_xlnet.self.layer_1->tensorflow.keras.layers.Dense(config.d_inner, kernel_initializer=get_initializer(config.initializer_range), name='layer_1')
A:transformers.modeling_tf_xlnet.self.layer_2->tensorflow.keras.layers.Dense(config.d_model, kernel_initializer=get_initializer(config.initializer_range), name='layer_2')
A:transformers.modeling_tf_xlnet.self.rel_attn->TFXLNetRelativeAttention(config, name='rel_attn')
A:transformers.modeling_tf_xlnet.self.ff->TFXLNetFeedForward(config, name='ff')
A:transformers.modeling_tf_xlnet.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_xlnet.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_xlnet.hidden_states->tuple((tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states))
A:transformers.modeling_tf_xlnet.self.word_embedding->TFSharedEmbeddings(config.n_token, config.d_model, initializer_range=config.initializer_range, name='word_embedding')
A:transformers.modeling_tf_xlnet.self.mask_emb->self.add_weight(shape=(1, 1, self.d_model), initializer=initializer, trainable=True, name='mask_emb')
A:transformers.modeling_tf_xlnet.attn_mask->tensorflow.cast(attn_mask > 0, dtype=dtype_float)
A:transformers.modeling_tf_xlnet.mask_u->tensorflow.matrix_band_part(attn_mask, 0, -1)
A:transformers.modeling_tf_xlnet.mask_dia->tensorflow.matrix_band_part(attn_mask, 0, 0)
A:transformers.modeling_tf_xlnet.attn_mask_pad->tensorflow.zeros([qlen, mlen], dtype=dtype)
A:transformers.modeling_tf_xlnet.ret->tensorflow.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)
A:transformers.modeling_tf_xlnet.mask_l->tensorflow.matrix_band_part(attn_mask, -1, 0)
A:transformers.modeling_tf_xlnet.sinusoid_inp->tensorflow.einsum('i,d->id', pos_seq, inv_freq)
A:transformers.modeling_tf_xlnet.pos_emb->self.dropout(pos_emb, training=training)
A:transformers.modeling_tf_xlnet.freq_seq->tensorflow.cast(freq_seq, dtype=dtype)
A:transformers.modeling_tf_xlnet.fwd_pos_seq->tensorflow.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len)
A:transformers.modeling_tf_xlnet.bwd_pos_seq->tensorflow.clip_by_value(bwd_pos_seq, -self.clamp_len, self.clamp_len)
A:transformers.modeling_tf_xlnet.fwd_pos_emb->self.positional_embedding(fwd_pos_seq, inv_freq)
A:transformers.modeling_tf_xlnet.bwd_pos_emb->self.positional_embedding(bwd_pos_seq, inv_freq)
A:transformers.modeling_tf_xlnet.input_ids->tensorflow.transpose(input_ids, perm=(1, 0))
A:transformers.modeling_tf_xlnet.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_xlnet.mems->inputs.get('mems', mems)
A:transformers.modeling_tf_xlnet.perm_mask->inputs.get('perm_mask', perm_mask)
A:transformers.modeling_tf_xlnet.target_mapping->inputs.get('target_mapping', target_mapping)
A:transformers.modeling_tf_xlnet.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_xlnet.input_mask->inputs.get('input_mask', input_mask)
A:transformers.modeling_tf_xlnet.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_tf_xlnet.mems_mask->tensorflow.zeros([tf.shape(data_mask)[0], mlen, bsz], dtype=dtype_float)
A:transformers.modeling_tf_xlnet.data_mask->tensorflow.concat([mems_mask, data_mask], axis=1)
A:transformers.modeling_tf_xlnet.non_tgt_mask->tensorflow.cast(attn_mask + non_tgt_mask[:, :, None, None] > 0, dtype=dtype_float)
A:transformers.modeling_tf_xlnet.word_emb_k->self.word_embedding(input_ids)
A:transformers.modeling_tf_xlnet.word_emb_q->tensorflow.tile(self.mask_emb, [tf.shape(target_mapping)[0], bsz, 1])
A:transformers.modeling_tf_xlnet.mem_pad->tensorflow.zeros([mlen, bsz], dtype=tf.int32)
A:transformers.modeling_tf_xlnet.cat_ids->tensorflow.concat([mem_pad, token_type_ids], 0)
A:transformers.modeling_tf_xlnet.seg_mat->tensorflow.one_hot(seg_mat, 2, dtype=dtype_float)
A:transformers.modeling_tf_xlnet.attentions->tuple((tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions))
A:transformers.modeling_tf_xlnet.self.transformer->TFXLNetMainLayer(config, name='transformer')
A:transformers.modeling_tf_xlnet.self.lm_loss->TFXLNetLMHead(config, self.transformer.word_embedding, name='lm_loss')
A:transformers.modeling_tf_xlnet.transformer_outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_xlnet.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_xlnet.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.initializer_range, name='sequence_summary')
A:transformers.modeling_tf_xlnet.self.logits_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='logits_proj')
A:transformers.modeling_tf_xlnet.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_xlnet.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_xlnet.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_xlnet.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFXLNetForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFXLNetForQuestionAnsweringSimple.call(self,inputs,**kwargs)
transformers.TFXLNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFXLNetForSequenceClassification.call(self,inputs,**kwargs)
transformers.TFXLNetLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFXLNetLMHeadModel.call(self,inputs,**kwargs)
transformers.TFXLNetMainLayer(self,config,**kwargs)
transformers.TFXLNetMainLayer._prune_heads(self,heads_to_prune)
transformers.TFXLNetMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFXLNetMainLayer.build(self,input_shape)
transformers.TFXLNetMainLayer.cache_mem(self,curr_out,prev_mem)
transformers.TFXLNetMainLayer.call(self,inputs,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,training=False)
transformers.TFXLNetMainLayer.create_mask(self,qlen,mlen,dtype=tf.float32)
transformers.TFXLNetMainLayer.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.TFXLNetMainLayer.relative_positional_encoding(self,qlen,klen,bsz=None,dtype=None)
transformers.TFXLNetModel(self,config,*inputs,**kwargs)
transformers.TFXLNetModel.call(self,inputs,**kwargs)
transformers.TFXLNetPreTrainedModel(TFPreTrainedModel)
transformers.load_xlnet_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_xlnet.TFXLNetFeedForward(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetFeedForward.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetFeedForward.call(self,inp,training=False)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.call(self,inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassification.call(self,inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHead.build(self,input_shape)
transformers.modeling_tf_xlnet.TFXLNetLMHead.call(self,hidden_states)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLayer(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLayer.call(self,inputs,training=False)
transformers.modeling_tf_xlnet.TFXLNetMainLayer(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_xlnet.TFXLNetMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.build(self,input_shape)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.cache_mem(self,curr_out,prev_mem)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.call(self,inputs,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.create_mask(self,qlen,mlen,dtype=tf.float32)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.relative_positional_encoding(self,qlen,klen,bsz=None,dtype=None)
transformers.modeling_tf_xlnet.TFXLNetModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.build(self,input_shape)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.call(self,inputs,training=False)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.post_attention(self,inputs,residual=True,training=False)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.prune_heads(self,heads)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.rel_attn_core(self,inputs,training=False)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.rel_shift(x,klen=-1)
transformers.modeling_tf_xlnet.gelu(x)
transformers.modeling_tf_xlnet.load_xlnet_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_xlnet.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_gpt2_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.config->transformers.GPT2Config.from_json_file(gpt2_config_file)
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.model->GPT2Model(config)
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch(gpt2_checkpoint_path,gpt2_config_file,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_utils.py----------------------------------------
A:transformers.tokenization_utils.logger->logging.getLogger(__name__)
A:transformers.tokenization_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.tokenization_utils.force_download->kwargs.pop('force_download', False)
A:transformers.tokenization_utils.proxies->kwargs.pop('proxies', None)
A:transformers.tokenization_utils.s3_models->list(cls.max_model_input_sizes.keys())
A:transformers.tokenization_utils.full_file_name->os.path.join(saved_directory, file_name)
A:transformers.tokenization_utils.saved_directory->os.path.dirname(saved_directory)
A:transformers.tokenization_utils.resolved_vocab_files[file_id]->cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
A:transformers.tokenization_utils.tokenizer_config_file->os.path.join(save_directory, TOKENIZER_CONFIG_FILE)
A:transformers.tokenization_utils.init_kwargs->json.load(open(tokenizer_config_file, encoding='utf-8'))
A:transformers.tokenization_utils.saved_init_inputs->json.load(open(tokenizer_config_file, encoding='utf-8')).pop('init_inputs', ())
A:transformers.tokenization_utils.init_kwargs['max_len']->min(init_kwargs.get('max_len', int(1000000000000.0)), max_len)
A:transformers.tokenization_utils.added_tokens_file->os.path.join(save_directory, ADDED_TOKENS_FILE)
A:transformers.tokenization_utils.special_tokens_map_file->os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)
A:transformers.tokenization_utils.special_tokens_map->json.load(open(special_tokens_map_file, encoding='utf-8'))
A:transformers.tokenization_utils.tokenizer->cls(*init_inputs, **init_kwargs)
A:transformers.tokenization_utils.added_tok_encoder->dict(((tok, len(self) + i) for (i, tok) in enumerate(to_add_tokens)))
A:transformers.tokenization_utils.tokenizer_config->copy.deepcopy(self.init_kwargs)
A:transformers.tokenization_utils.tokenizer_config['init_inputs']->copy.deepcopy(self.init_inputs)
A:transformers.tokenization_utils.out_str->json.dumps(self.added_tokens_encoder, ensure_ascii=False)
A:transformers.tokenization_utils.vocab_files->self.save_vocabulary(save_directory)
A:transformers.tokenization_utils.split_text->''.join(sub_texts).split(tok)
A:transformers.tokenization_utils.sub_text->sub_text.strip().strip()
A:transformers.tokenization_utils.tokenized_text->split_on_tokens(added_tokens, text)
A:transformers.tokenization_utils.encoded_inputs->self.encode_plus(text, text_pair=text_pair, max_length=max_length, add_special_tokens=add_special_tokens, stride=stride, truncation_strategy=truncation_strategy, return_tensors=return_tensors, **kwargs)
A:transformers.tokenization_utils.first_ids->get_input_ids(text)
A:transformers.tokenization_utils.pair->bool(pair_ids is not None)
A:transformers.tokenization_utils.len_ids->len(ids)
A:transformers.tokenization_utils.(ids, pair_ids, overflowing_tokens)->self.truncate_sequences(ids, pair_ids=pair_ids, num_tokens_to_remove=total_len - max_length, truncation_strategy=truncation_strategy, stride=stride)
A:transformers.tokenization_utils.sequence->torch.tensor([sequence])
A:transformers.tokenization_utils.token_type_ids->torch.tensor([token_type_ids])
A:transformers.tokenization_utils.encoded_inputs['special_tokens_mask']->self.get_special_tokens_mask(ids, pair_ids)
A:transformers.tokenization_utils.window_len->min(len(pair_ids), stride + num_tokens_to_remove)
A:transformers.tokenization_utils.filtered_tokens->self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
A:transformers.tokenization_utils.text->''.join(sub_texts)
A:transformers.tokenization_utils.clean_text->self.clean_up_tokenization(text)
A:transformers.tokenization_utils.attr_value->getattr(self, '_' + attr)
A:transformers.tokenization_utils.all_toks->list(set(all_toks))
A:transformers.tokenization_utils.all_ids->list((self._convert_token_to_id(t) for t in all_toks))
A:transformers.tokenization_utils.out_string->out_string.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ',').replace(" ' ", "'").replace(" n't", "n't").replace(" 'm", "'m").replace(' do not', " don't").replace(" 's", "'s").replace(" 've", "'ve").replace(" 're", "'re").replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ',').replace(" ' ", "'").replace(" n't", "n't").replace(" 'm", "'m").replace(' do not', " don't").replace(" 's", "'s").replace(" 've", "'ve").replace(" 're", "'re")
transformers.PreTrainedTokenizer(self,max_len=None,**kwargs)
transformers.PreTrainedTokenizer.__len__(self)
transformers.PreTrainedTokenizer._convert_id_to_token(self,index)
transformers.PreTrainedTokenizer._convert_token_to_id(self,token)
transformers.PreTrainedTokenizer._convert_token_to_id_with_added_voc(self,token)
transformers.PreTrainedTokenizer._from_pretrained(cls,pretrained_model_name_or_path,*init_inputs,**kwargs)
transformers.PreTrainedTokenizer._tokenize(self,text,**kwargs)
transformers.PreTrainedTokenizer.add_special_tokens(self,special_tokens_dict)
transformers.PreTrainedTokenizer.add_tokens(self,new_tokens)
transformers.PreTrainedTokenizer.additional_special_tokens(self)
transformers.PreTrainedTokenizer.additional_special_tokens(self,value)
transformers.PreTrainedTokenizer.additional_special_tokens_ids(self)
transformers.PreTrainedTokenizer.all_special_ids(self)
transformers.PreTrainedTokenizer.all_special_tokens(self)
transformers.PreTrainedTokenizer.bos_token(self)
transformers.PreTrainedTokenizer.bos_token(self,value)
transformers.PreTrainedTokenizer.bos_token_id(self)
transformers.PreTrainedTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.PreTrainedTokenizer.clean_up_tokenization(out_string)
transformers.PreTrainedTokenizer.cls_token(self)
transformers.PreTrainedTokenizer.cls_token(self,value)
transformers.PreTrainedTokenizer.cls_token_id(self)
transformers.PreTrainedTokenizer.convert_ids_to_tokens(self,ids,skip_special_tokens=False)
transformers.PreTrainedTokenizer.convert_tokens_to_ids(self,tokens)
transformers.PreTrainedTokenizer.convert_tokens_to_string(self,tokens)
transformers.PreTrainedTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.PreTrainedTokenizer.decode(self,token_ids,skip_special_tokens=False,clean_up_tokenization_spaces=True)
transformers.PreTrainedTokenizer.encode(self,text,text_pair=None,add_special_tokens=False,max_length=None,stride=0,truncation_strategy='longest_first',return_tensors=None,**kwargs)
transformers.PreTrainedTokenizer.encode_plus(self,text,text_pair=None,add_special_tokens=False,max_length=None,stride=0,truncation_strategy='longest_first',return_tensors=None,**kwargs)
transformers.PreTrainedTokenizer.eos_token(self)
transformers.PreTrainedTokenizer.eos_token(self,value)
transformers.PreTrainedTokenizer.eos_token_id(self)
transformers.PreTrainedTokenizer.from_pretrained(cls,*inputs,**kwargs)
transformers.PreTrainedTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.PreTrainedTokenizer.mask_token(self)
transformers.PreTrainedTokenizer.mask_token(self,value)
transformers.PreTrainedTokenizer.mask_token_id(self)
transformers.PreTrainedTokenizer.num_added_tokens(self,pair=False)
transformers.PreTrainedTokenizer.pad_token(self)
transformers.PreTrainedTokenizer.pad_token(self,value)
transformers.PreTrainedTokenizer.pad_token_id(self)
transformers.PreTrainedTokenizer.prepare_for_model(self,ids,pair_ids=None,max_length=None,add_special_tokens=False,stride=0,truncation_strategy='longest_first',return_tensors=None)
transformers.PreTrainedTokenizer.save_pretrained(self,save_directory)
transformers.PreTrainedTokenizer.save_vocabulary(self,save_directory)
transformers.PreTrainedTokenizer.sep_token(self)
transformers.PreTrainedTokenizer.sep_token(self,value)
transformers.PreTrainedTokenizer.sep_token_id(self)
transformers.PreTrainedTokenizer.special_tokens_map(self)
transformers.PreTrainedTokenizer.tokenize(self,text,**kwargs)
transformers.PreTrainedTokenizer.truncate_sequences(self,ids,pair_ids=None,num_tokens_to_remove=0,truncation_strategy='longest_first',stride=0)
transformers.PreTrainedTokenizer.unk_token(self)
transformers.PreTrainedTokenizer.unk_token(self,value)
transformers.PreTrainedTokenizer.unk_token_id(self)
transformers.PreTrainedTokenizer.vocab_size(self)
transformers.tokenization_utils.PreTrainedTokenizer(self,max_len=None,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.__init__(self,max_len=None,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.__len__(self)
transformers.tokenization_utils.PreTrainedTokenizer._convert_id_to_token(self,index)
transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id(self,token)
transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc(self,token)
transformers.tokenization_utils.PreTrainedTokenizer._from_pretrained(cls,pretrained_model_name_or_path,*init_inputs,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer._tokenize(self,text,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.add_special_tokens(self,special_tokens_dict)
transformers.tokenization_utils.PreTrainedTokenizer.add_tokens(self,new_tokens)
transformers.tokenization_utils.PreTrainedTokenizer.additional_special_tokens(self)
transformers.tokenization_utils.PreTrainedTokenizer.additional_special_tokens(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.additional_special_tokens_ids(self)
transformers.tokenization_utils.PreTrainedTokenizer.all_special_ids(self)
transformers.tokenization_utils.PreTrainedTokenizer.all_special_tokens(self)
transformers.tokenization_utils.PreTrainedTokenizer.bos_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.bos_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.bos_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization(out_string)
transformers.tokenization_utils.PreTrainedTokenizer.cls_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.cls_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.cls_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens(self,ids,skip_special_tokens=False)
transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids(self,tokens)
transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.tokenization_utils.PreTrainedTokenizer.decode(self,token_ids,skip_special_tokens=False,clean_up_tokenization_spaces=True)
transformers.tokenization_utils.PreTrainedTokenizer.encode(self,text,text_pair=None,add_special_tokens=False,max_length=None,stride=0,truncation_strategy='longest_first',return_tensors=None,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.encode_plus(self,text,text_pair=None,add_special_tokens=False,max_length=None,stride=0,truncation_strategy='longest_first',return_tensors=None,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.eos_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.eos_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.eos_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.from_pretrained(cls,*inputs,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.tokenization_utils.PreTrainedTokenizer.mask_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.mask_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.mask_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.num_added_tokens(self,pair=False)
transformers.tokenization_utils.PreTrainedTokenizer.pad_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.pad_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.pad_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.prepare_for_model(self,ids,pair_ids=None,max_length=None,add_special_tokens=False,stride=0,truncation_strategy='longest_first',return_tensors=None)
transformers.tokenization_utils.PreTrainedTokenizer.save_pretrained(self,save_directory)
transformers.tokenization_utils.PreTrainedTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_utils.PreTrainedTokenizer.sep_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.sep_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.sep_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.special_tokens_map(self)
transformers.tokenization_utils.PreTrainedTokenizer.tokenize(self,text,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.truncate_sequences(self,ids,pair_ids=None,num_tokens_to_remove=0,truncation_strategy='longest_first',stride=0)
transformers.tokenization_utils.PreTrainedTokenizer.unk_token(self)
transformers.tokenization_utils.PreTrainedTokenizer.unk_token(self,value)
transformers.tokenization_utils.PreTrainedTokenizer.unk_token_id(self)
transformers.tokenization_utils.PreTrainedTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/file_utils.py----------------------------------------
A:transformers.file_utils.logger->logging.getLogger(__name__)
A:transformers.file_utils.torch_cache_home->os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))
A:transformers.file_utils.default_cache_path->os.path.join(torch_cache_home, 'transformers')
A:transformers.file_utils.PYTORCH_PRETRAINED_BERT_CACHE->os.getenv('PYTORCH_TRANSFORMERS_CACHE', os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path))
A:transformers.file_utils.url_bytes->url.encode('utf-8')
A:transformers.file_utils.url_hash->sha256(url_bytes)
A:transformers.file_utils.filename->url_to_filename(url, etag)
A:transformers.file_utils.etag_bytes->etag.decode('utf-8').encode('utf-8')
A:transformers.file_utils.etag_hash->sha256(etag_bytes)
A:transformers.file_utils.cache_dir->str(cache_dir)
A:transformers.file_utils.cache_path->os.path.join(cache_dir, matching_files[-1])
A:transformers.file_utils.metadata->json.load(meta_file)
A:transformers.file_utils.url_or_filename->str(url_or_filename)
A:transformers.file_utils.parsed->urlparse(url)
A:transformers.file_utils.s3_resource->boto3.resource('s3', config=Config(proxies=proxies))
A:transformers.file_utils.(bucket_name, s3_path)->split_s3_path(url)
A:transformers.file_utils.s3_object->boto3.resource('s3', config=Config(proxies=proxies)).Object(bucket_name, s3_path)
A:transformers.file_utils.req->requests.get(url, stream=True, proxies=proxies)
A:transformers.file_utils.content_length->requests.get(url, stream=True, proxies=proxies).headers.get('Content-Length')
A:transformers.file_utils.progress->tqdm(unit='B', total=total)
A:transformers.file_utils.etag->etag.decode('utf-8').decode('utf-8')
A:transformers.file_utils.response->requests.head(url, allow_redirects=True, proxies=proxies)
A:transformers.file_utils.matching_files->list(filter(lambda s: not s.endswith('.json'), matching_files))
A:transformers.file_utils.output_string->unicode(output_string, 'utf-8')
transformers.cached_path(url_or_filename,cache_dir=None,force_download=False,proxies=None)
transformers.file_utils.cached_path(url_or_filename,cache_dir=None,force_download=False,proxies=None)
transformers.file_utils.filename_to_url(filename,cache_dir=None)
transformers.file_utils.get_from_cache(url,cache_dir=None,force_download=False,proxies=None)
transformers.file_utils.http_get(url,temp_file,proxies=None)
transformers.file_utils.is_tf_available()
transformers.file_utils.is_torch_available()
transformers.file_utils.s3_etag(url,proxies=None)
transformers.file_utils.s3_get(url,temp_file,proxies=None)
transformers.file_utils.s3_request(func)
transformers.file_utils.split_s3_path(url)
transformers.file_utils.url_to_filename(url,etag=None)
transformers.is_tf_available()
transformers.is_torch_available()


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_ctrl.py----------------------------------------
A:transformers.configuration_ctrl.logger->logging.getLogger(__name__)
A:transformers.configuration_ctrl.json_config->json.loads(reader.read())
transformers.CTRLConfig(self,vocab_size_or_config_json_file=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.CTRLConfig.hidden_size(self)
transformers.CTRLConfig.max_position_embeddings(self)
transformers.CTRLConfig.num_attention_heads(self)
transformers.CTRLConfig.num_hidden_layers(self)
transformers.configuration_ctrl.CTRLConfig(self,vocab_size_or_config_json_file=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_ctrl.CTRLConfig.__init__(self,vocab_size_or_config_json_file=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_ctrl.CTRLConfig.hidden_size(self)
transformers.configuration_ctrl.CTRLConfig.max_position_embeddings(self)
transformers.configuration_ctrl.CTRLConfig.num_attention_heads(self)
transformers.configuration_ctrl.CTRLConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_gpt2.py----------------------------------------
A:transformers.configuration_gpt2.logger->logging.getLogger(__name__)
A:transformers.configuration_gpt2.json_config->json.loads(reader.read())
transformers.GPT2Config(self,vocab_size_or_config_json_file=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.GPT2Config.hidden_size(self)
transformers.GPT2Config.max_position_embeddings(self)
transformers.GPT2Config.num_attention_heads(self)
transformers.GPT2Config.num_hidden_layers(self)
transformers.configuration_gpt2.GPT2Config(self,vocab_size_or_config_json_file=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_gpt2.GPT2Config.__init__(self,vocab_size_or_config_json_file=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_gpt2.GPT2Config.hidden_size(self)
transformers.configuration_gpt2.GPT2Config.max_position_embeddings(self)
transformers.configuration_gpt2.GPT2Config.num_attention_heads(self)
transformers.configuration_gpt2.GPT2Config.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_transfo_xl_utilities.py----------------------------------------
A:transformers.modeling_transfo_xl_utilities.self.cluster_weight->torch.nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))
A:transformers.modeling_transfo_xl_utilities.self.cluster_bias->torch.nn.Parameter(torch.zeros(self.n_clusters))
A:transformers.modeling_transfo_xl_utilities.self.out_layers->torch.nn.ModuleList()
A:transformers.modeling_transfo_xl_utilities.self.out_projs->torch.nn.ParameterList()
A:transformers.modeling_transfo_xl_utilities.logit->self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])
A:transformers.modeling_transfo_xl_utilities.proj_hid->torch.nn.functional.linear(hidden, proj.t().contiguous())
A:transformers.modeling_transfo_xl_utilities.labels->labels.view(-1).view(-1)
A:transformers.modeling_transfo_xl_utilities.out->hidden.new_empty((head_logit.size(0), self.n_token))
A:transformers.modeling_transfo_xl_utilities.weight_i->torch.cat([weight_i, self.cluster_weight], dim=0)
A:transformers.modeling_transfo_xl_utilities.bias_i->torch.cat([bias_i, self.cluster_bias], dim=0)
A:transformers.modeling_transfo_xl_utilities.head_logit->self._compute_logit(hidden, head_weight, head_bias, head_proj)
A:transformers.modeling_transfo_xl_utilities.head_logprob->torch.nn.functional.log_softmax(head_logit, dim=1)
A:transformers.modeling_transfo_xl_utilities.indices_i->mask_i.nonzero().squeeze()
A:transformers.modeling_transfo_xl_utilities.head_logprob_i->torch.nn.functional.log_softmax(head_logit, dim=1).index_select(0, indices_i)
A:transformers.modeling_transfo_xl_utilities.hidden_i->hidden.index_select(0, indices_i)
A:transformers.modeling_transfo_xl_utilities.logprob_i->torch.nn.functional.log_softmax(head_logit, dim=1).index_select(0, indices_i).gather(1, target_i[:, None]).squeeze(1)
A:transformers.modeling_transfo_xl_utilities.tail_logit_i->self._compute_logit(hidden, weight_i, bias_i, proj_i)
A:transformers.modeling_transfo_xl_utilities.tail_logprob_i->torch.nn.functional.log_softmax(tail_logit_i, dim=1)
A:transformers.modeling_transfo_xl_utilities.log_indices->torch.arange(1.0, range_max + 2.0, 1.0).log_()
A:transformers.modeling_transfo_xl_utilities.self.log_q->(-(-self.dist.double().log1p_() * 2 * n_sample).expm1_()).log_().float()
A:transformers.modeling_transfo_xl_utilities.neg_samples->neg_samples.to(device).to(device)
A:transformers.modeling_transfo_xl_utilities.true_log_probs->self.log_q[labels].to(device)
A:transformers.modeling_transfo_xl_utilities.samp_log_probs->self.log_q[neg_samples].to(device)
A:transformers.modeling_transfo_xl_utilities.(true_log_probs, samp_log_probs, neg_samples)->sampler.sample(labels)
A:transformers.modeling_transfo_xl_utilities.n_sample->neg_samples.to(device).to(device).size(0)
A:transformers.modeling_transfo_xl_utilities.all_ids->torch.cat([labels.view(-1), neg_samples])
A:transformers.modeling_transfo_xl_utilities.all_w->embedding(all_ids)
A:transformers.modeling_transfo_xl_utilities.true_w->all_w[:-n_sample].view(b1, b2, -1)
A:transformers.modeling_transfo_xl_utilities.sample_w->all_w[-n_sample:].view(n_sample, -1)
A:transformers.modeling_transfo_xl_utilities.true_b->all_b[:-n_sample].view(b1, b2)
A:transformers.modeling_transfo_xl_utilities.hit->(labels[:, :, None] == neg_samples).detach()
A:transformers.modeling_transfo_xl_utilities.logits->torch.cat([true_logits[:, :, None], sample_logits], -1)
transformers.modeling_transfo_xl_utilities.LogUniformSampler(self,range_max,n_sample)
transformers.modeling_transfo_xl_utilities.LogUniformSampler.__init__(self,range_max,n_sample)
transformers.modeling_transfo_xl_utilities.LogUniformSampler.sample(self,labels)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit(self,hidden,weight,bias,proj)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.forward(self,hidden,labels=None,keep_order=False)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.log_prob(self,hidden)
transformers.modeling_transfo_xl_utilities.sample_logits(embedding,bias,labels,inputs,sampler)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_roberta.py----------------------------------------
A:transformers.modeling_roberta.logger->logging.getLogger(__name__)
A:transformers.modeling_roberta.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.modeling_roberta.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.modeling_roberta.seq_length->input_ids.size(1)
A:transformers.modeling_roberta.position_ids->position_ids.unsqueeze(0).expand_as(input_ids).unsqueeze(0).expand_as(input_ids)
A:transformers.modeling_roberta.self.embeddings->RobertaEmbeddings(config)
A:transformers.modeling_roberta.self.roberta->RobertaModel(config)
A:transformers.modeling_roberta.self.lm_head->RobertaLMHead(config)
A:transformers.modeling_roberta.outputs->self.roberta(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids, attention_mask=flat_attention_mask, head_mask=head_mask)
A:transformers.modeling_roberta.prediction_scores->self.lm_head(sequence_output)
A:transformers.modeling_roberta.loss_fct->CrossEntropyLoss()
A:transformers.modeling_roberta.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))
A:transformers.modeling_roberta.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_roberta.self.layer_norm->BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_roberta.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.modeling_roberta.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_roberta.x->self.out_proj(x)
A:transformers.modeling_roberta.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_roberta.logits->self.classifier(pooled_output)
A:transformers.modeling_roberta.loss->loss_fct(reshaped_logits, labels)
A:transformers.modeling_roberta.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_roberta.flat_input_ids->input_ids.view(-1, input_ids.size(-1))
A:transformers.modeling_roberta.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_roberta.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
A:transformers.modeling_roberta.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
transformers.RobertaForMaskedLM(self,config)
transformers.RobertaForMaskedLM.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,masked_lm_labels=None)
transformers.RobertaForMaskedLM.tie_weights(self)
transformers.RobertaForMultipleChoice(self,config)
transformers.RobertaForMultipleChoice.forward(self,input_ids,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None)
transformers.RobertaForSequenceClassification(self,config)
transformers.RobertaForSequenceClassification.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.RobertaModel(self,config)
transformers.RobertaModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.modeling_roberta.RobertaClassificationHead(self,config)
transformers.modeling_roberta.RobertaClassificationHead.__init__(self,config)
transformers.modeling_roberta.RobertaClassificationHead.forward(self,features,**kwargs)
transformers.modeling_roberta.RobertaEmbeddings(self,config)
transformers.modeling_roberta.RobertaEmbeddings.__init__(self,config)
transformers.modeling_roberta.RobertaEmbeddings.forward(self,input_ids,token_type_ids=None,position_ids=None)
transformers.modeling_roberta.RobertaForMaskedLM(self,config)
transformers.modeling_roberta.RobertaForMaskedLM.__init__(self,config)
transformers.modeling_roberta.RobertaForMaskedLM.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,masked_lm_labels=None)
transformers.modeling_roberta.RobertaForMaskedLM.tie_weights(self)
transformers.modeling_roberta.RobertaForMultipleChoice(self,config)
transformers.modeling_roberta.RobertaForMultipleChoice.__init__(self,config)
transformers.modeling_roberta.RobertaForMultipleChoice.forward(self,input_ids,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None)
transformers.modeling_roberta.RobertaForSequenceClassification(self,config)
transformers.modeling_roberta.RobertaForSequenceClassification.__init__(self,config)
transformers.modeling_roberta.RobertaForSequenceClassification.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_roberta.RobertaLMHead(self,config)
transformers.modeling_roberta.RobertaLMHead.__init__(self,config)
transformers.modeling_roberta.RobertaLMHead.forward(self,features,**kwargs)
transformers.modeling_roberta.RobertaModel(self,config)
transformers.modeling_roberta.RobertaModel.__init__(self,config)
transformers.modeling_roberta.RobertaModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_ctrl.py----------------------------------------
A:transformers.modeling_ctrl.logger->logging.getLogger(__name__)
A:transformers.modeling_ctrl.angle_rads->angle_defn(torch.arange(position, dtype=dtype).unsqueeze(1), torch.arange(d_model_size, dtype=dtype).unsqueeze(0), d_model_size)
A:transformers.modeling_ctrl.sines->torch.sin(angle_rads[:, 0::2])
A:transformers.modeling_ctrl.cosines->torch.cos(angle_rads[:, 1::2])
A:transformers.modeling_ctrl.pos_encoding->torch.cat([sines, cosines], dim=-1)
A:transformers.modeling_ctrl.matmul_qk->torch.matmul(q, k.permute(0, 1, 3, 2))
A:transformers.modeling_ctrl.attention_weights->torch.softmax(scaled_attention_logits, dim=-1)
A:transformers.modeling_ctrl.output->self.dense(original_size_attention)
A:transformers.modeling_ctrl.self.depth->int(d_model_size / self.num_heads)
A:transformers.modeling_ctrl.self.Wq->torch.nn.Linear(d_model_size, d_model_size)
A:transformers.modeling_ctrl.self.Wk->torch.nn.Linear(d_model_size, d_model_size)
A:transformers.modeling_ctrl.self.Wv->torch.nn.Linear(d_model_size, d_model_size)
A:transformers.modeling_ctrl.self.dense->torch.nn.Linear(d_model_size, d_model_size)
A:transformers.modeling_ctrl.x->x.reshape(batch_size, -1, self.num_heads, self.depth).reshape(batch_size, -1, self.num_heads, self.depth)
A:transformers.modeling_ctrl.q->self.split_into_heads(q, batch_size)
A:transformers.modeling_ctrl.k->torch.cat((past_key, k), dim=-2)
A:transformers.modeling_ctrl.v->torch.cat((past_value, v), dim=-2)
A:transformers.modeling_ctrl.present->torch.stack((k, v))
A:transformers.modeling_ctrl.scaled_attention->output[0].permute([0, 2, 1, 3])
A:transformers.modeling_ctrl.original_size_attention->output[0].permute([0, 2, 1, 3]).reshape(batch_size, -1, self.d_model_size)
A:transformers.modeling_ctrl.self.multi_head_attention->MultiHeadAttention(d_model_size, num_heads, output_attentions)
A:transformers.modeling_ctrl.self.ffn->point_wise_feed_forward_network(d_model_size, dff)
A:transformers.modeling_ctrl.self.layernorm1->torch.nn.LayerNorm(d_model_size, eps=1e-06)
A:transformers.modeling_ctrl.self.layernorm2->torch.nn.LayerNorm(d_model_size, eps=1e-06)
A:transformers.modeling_ctrl.self.dropout1->torch.nn.Dropout(rate)
A:transformers.modeling_ctrl.self.dropout2->torch.nn.Dropout(rate)
A:transformers.modeling_ctrl.normed->self.layernorm1(x)
A:transformers.modeling_ctrl.attn_outputs->self.multi_head_attention(normed, normed, normed, mask, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask)
A:transformers.modeling_ctrl.attn_output->self.dropout1(attn_output)
A:transformers.modeling_ctrl.out2->self.layernorm2(out1)
A:transformers.modeling_ctrl.ffn_output->self.dropout2(ffn_output)
A:transformers.modeling_ctrl.self.pos_encoding->positional_encoding(config.n_positions, self.d_model_size, torch.float)
A:transformers.modeling_ctrl.self.w->self._get_resized_embeddings(self.w, new_num_tokens)
A:transformers.modeling_ctrl.self.dropout->torch.nn.Dropout(config.embd_pdrop)
A:transformers.modeling_ctrl.self.h->torch.nn.ModuleList([EncoderLayer(config.n_embd, config.n_head, config.dff, config.resid_pdrop, config.output_attentions) for _ in range(config.n_layer)])
A:transformers.modeling_ctrl.self.layernorm->torch.nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
A:transformers.modeling_ctrl.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.modeling_ctrl.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_ctrl.past_length->past[0][0].size(-2)
A:transformers.modeling_ctrl.position_ids->position_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_ctrl.attention_mask->attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_ctrl.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_ctrl.token_type_ids->token_type_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_ctrl.token_type_embeds->self.w(token_type_ids)
A:transformers.modeling_ctrl.inputs_embeds->self.w(input_ids)
A:transformers.modeling_ctrl.mask->torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device)
A:transformers.modeling_ctrl.pos_embeds->self.pos_encoding[position_ids, :].to(inputs_embeds.device)
A:transformers.modeling_ctrl.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.modeling_ctrl.outputs->h(hidden_states, mask, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i])
A:transformers.modeling_ctrl.all_attentions->tuple((t.view(*attention_output_shape) for t in all_attentions))
A:transformers.modeling_ctrl.self.transformer->CTRLModel(config)
A:transformers.modeling_ctrl.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=True)
A:transformers.modeling_ctrl.transformer_outputs->self.transformer(input_ids, past=past, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)
A:transformers.modeling_ctrl.lm_logits->self.lm_head(hidden_states)
A:transformers.modeling_ctrl.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.modeling_ctrl.shift_labels->labels[..., 1:].contiguous()
A:transformers.modeling_ctrl.loss_fct->CrossEntropyLoss(ignore_index=-1)
A:transformers.modeling_ctrl.loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
transformers.CTRLLMHeadModel(self,config)
transformers.CTRLLMHeadModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.CTRLLMHeadModel.tie_weights(self)
transformers.CTRLModel(self,config)
transformers.CTRLModel._prune_heads(self,heads_to_prune)
transformers.CTRLModel._resize_token_embeddings(self,new_num_tokens)
transformers.CTRLModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.CTRLPreTrainedModel(PreTrainedModel)
transformers.CTRLPreTrainedModel._init_weights(self,module)
transformers.modeling_ctrl.CTRLLMHeadModel(self,config)
transformers.modeling_ctrl.CTRLLMHeadModel.__init__(self,config)
transformers.modeling_ctrl.CTRLLMHeadModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_ctrl.CTRLLMHeadModel.tie_weights(self)
transformers.modeling_ctrl.CTRLModel(self,config)
transformers.modeling_ctrl.CTRLModel.__init__(self,config)
transformers.modeling_ctrl.CTRLModel._prune_heads(self,heads_to_prune)
transformers.modeling_ctrl.CTRLModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_ctrl.CTRLModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.modeling_ctrl.CTRLPreTrainedModel(PreTrainedModel)
transformers.modeling_ctrl.CTRLPreTrainedModel._init_weights(self,module)
transformers.modeling_ctrl.EncoderLayer(self,d_model_size,num_heads,dff,rate=0.1,output_attentions=False)
transformers.modeling_ctrl.EncoderLayer.__init__(self,d_model_size,num_heads,dff,rate=0.1,output_attentions=False)
transformers.modeling_ctrl.EncoderLayer.forward(self,x,mask,layer_past=None,attention_mask=None,head_mask=None)
transformers.modeling_ctrl.MultiHeadAttention(self,d_model_size,num_heads,output_attentions=False)
transformers.modeling_ctrl.MultiHeadAttention.__init__(self,d_model_size,num_heads,output_attentions=False)
transformers.modeling_ctrl.MultiHeadAttention.forward(self,v,k,q,mask,layer_past=None,attention_mask=None,head_mask=None)
transformers.modeling_ctrl.MultiHeadAttention.split_into_heads(self,x,batch_size)
transformers.modeling_ctrl.angle_defn(pos,i,d_model_size)
transformers.modeling_ctrl.point_wise_feed_forward_network(d_model_size,dff)
transformers.modeling_ctrl.positional_encoding(position,d_model_size,dtype)
transformers.modeling_ctrl.scaled_dot_product_attention(q,k,v,mask,attention_mask=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_bert.py----------------------------------------
A:transformers.tokenization_bert.logger->logging.getLogger(__name__)
A:transformers.tokenization_bert.vocab->collections.OrderedDict()
A:transformers.tokenization_bert.tokens->unicodedata.normalize('NFD', text).split()
A:transformers.tokenization_bert.token->self._run_strip_accents(token)
A:transformers.tokenization_bert.text->unicodedata.normalize('NFD', text)
A:transformers.tokenization_bert.self.vocab->load_vocab(vocab_file)
A:transformers.tokenization_bert.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.tokenization_bert.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars)
A:transformers.tokenization_bert.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.tokenization_bert.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.tokenization_bert.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.tokenization_bert.vocab_file->os.path.join(vocab_path, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_bert.orig_tokens->whitespace_tokenize(text)
A:transformers.tokenization_bert.output_tokens->whitespace_tokenize(' '.join(split_tokens))
A:transformers.tokenization_bert.cat->unicodedata.category(char)
A:transformers.tokenization_bert.chars->list(token)
A:transformers.tokenization_bert.cp->ord(char)
A:transformers.tokenization_bert.end->len(chars)
A:transformers.tokenization_bert.substr->''.join(chars[start:end])
transformers.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True)
transformers.BasicTokenizer._clean_text(self,text)
transformers.BasicTokenizer._is_chinese_char(self,cp)
transformers.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.BasicTokenizer._run_strip_accents(self,text)
transformers.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.BertTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,**kwargs)
transformers.BertTokenizer._convert_id_to_token(self,index)
transformers.BertTokenizer._convert_token_to_id(self,token)
transformers.BertTokenizer._tokenize(self,text)
transformers.BertTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.BertTokenizer.convert_tokens_to_string(self,tokens)
transformers.BertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.BertTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.BertTokenizer.save_vocabulary(self,vocab_path)
transformers.BertTokenizer.vocab_size(self)
transformers.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.WordpieceTokenizer.tokenize(self,text)
transformers.tokenization_bert.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True)
transformers.tokenization_bert.BasicTokenizer.__init__(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True)
transformers.tokenization_bert.BasicTokenizer._clean_text(self,text)
transformers.tokenization_bert.BasicTokenizer._is_chinese_char(self,cp)
transformers.tokenization_bert.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.tokenization_bert.BasicTokenizer._run_strip_accents(self,text)
transformers.tokenization_bert.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.tokenization_bert.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.tokenization_bert.BertTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,**kwargs)
transformers.tokenization_bert.BertTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,**kwargs)
transformers.tokenization_bert.BertTokenizer._convert_id_to_token(self,index)
transformers.tokenization_bert.BertTokenizer._convert_token_to_id(self,token)
transformers.tokenization_bert.BertTokenizer._tokenize(self,text)
transformers.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_bert.BertTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.tokenization_bert.BertTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.tokenization_bert.BertTokenizer.save_vocabulary(self,vocab_path)
transformers.tokenization_bert.BertTokenizer.vocab_size(self)
transformers.tokenization_bert.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.tokenization_bert.WordpieceTokenizer.__init__(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.tokenization_bert.WordpieceTokenizer.tokenize(self,text)
transformers.tokenization_bert._is_control(char)
transformers.tokenization_bert._is_punctuation(char)
transformers.tokenization_bert._is_whitespace(char)
transformers.tokenization_bert.load_vocab(vocab_file)
transformers.tokenization_bert.whitespace_tokenize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_xlm.py----------------------------------------
A:transformers.configuration_xlm.logger->logging.getLogger(__name__)
A:transformers.configuration_xlm.json_config->json.loads(reader.read())
transformers.XLMConfig(self,vocab_size_or_config_json_file=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,finetuning_task=None,num_labels=2,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,**kwargs)
transformers.XLMConfig.hidden_size(self)
transformers.XLMConfig.num_attention_heads(self)
transformers.XLMConfig.num_hidden_layers(self)
transformers.XLMConfig.vocab_size(self)
transformers.XLMConfig.vocab_size(self,value)
transformers.configuration_xlm.XLMConfig(self,vocab_size_or_config_json_file=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,finetuning_task=None,num_labels=2,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,**kwargs)
transformers.configuration_xlm.XLMConfig.__init__(self,vocab_size_or_config_json_file=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,finetuning_task=None,num_labels=2,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,**kwargs)
transformers.configuration_xlm.XLMConfig.hidden_size(self)
transformers.configuration_xlm.XLMConfig.num_attention_heads(self)
transformers.configuration_xlm.XLMConfig.num_hidden_layers(self)
transformers.configuration_xlm.XLMConfig.vocab_size(self)
transformers.configuration_xlm.XLMConfig.vocab_size(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_bert.py----------------------------------------
A:transformers.configuration_bert.logger->logging.getLogger(__name__)
A:transformers.configuration_bert.json_config->json.loads(reader.read())
transformers.BertConfig(self,vocab_size_or_config_json_file=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,**kwargs)
transformers.configuration_bert.BertConfig(self,vocab_size_or_config_json_file=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,**kwargs)
transformers.configuration_bert.BertConfig.__init__(self,vocab_size_or_config_json_file=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_gpt2.py----------------------------------------
A:transformers.tokenization_gpt2.logger->logging.getLogger(__name__)
A:transformers.tokenization_gpt2.pairs->get_pairs(word)
A:transformers.tokenization_gpt2.self.encoder->json.load(open(vocab_file, encoding='utf-8'))
A:transformers.tokenization_gpt2.self.byte_encoder->bytes_to_unicode()
A:transformers.tokenization_gpt2.self.bpe_ranks->dict(zip(bpe_merges, range(len(bpe_merges))))
A:transformers.tokenization_gpt2.self.pat->regex.compile("'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+")
A:transformers.tokenization_gpt2.word->' '.join(word)
A:transformers.tokenization_gpt2.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_gpt2.j->' '.join(word).index(first, i)
A:transformers.tokenization_gpt2.new_word->tuple(new_word)
A:transformers.tokenization_gpt2.token->''.join((self.byte_encoder[b] for b in token.encode('utf-8')))
A:transformers.tokenization_gpt2.text->bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)
A:transformers.tokenization_gpt2.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_gpt2.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.GPT2Tokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',**kwargs)
transformers.GPT2Tokenizer._convert_id_to_token(self,index)
transformers.GPT2Tokenizer._convert_token_to_id(self,token)
transformers.GPT2Tokenizer._tokenize(self,text,add_prefix_space=False)
transformers.GPT2Tokenizer.bpe(self,token)
transformers.GPT2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.GPT2Tokenizer.save_vocabulary(self,save_directory)
transformers.GPT2Tokenizer.vocab_size(self)
transformers.tokenization_gpt2.GPT2Tokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',**kwargs)
transformers.tokenization_gpt2.GPT2Tokenizer.__init__(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',**kwargs)
transformers.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token(self,index)
transformers.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id(self,token)
transformers.tokenization_gpt2.GPT2Tokenizer._tokenize(self,text,add_prefix_space=False)
transformers.tokenization_gpt2.GPT2Tokenizer.bpe(self,token)
transformers.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_gpt2.GPT2Tokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_gpt2.GPT2Tokenizer.vocab_size(self)
transformers.tokenization_gpt2.bytes_to_unicode()
transformers.tokenization_gpt2.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_openai.py----------------------------------------
A:transformers.modeling_openai.logger->logging.getLogger(__name__)
A:transformers.modeling_openai.openai_checkpoint_folder_path->os.path.dirname(openai_checkpoint_folder_path)
A:transformers.modeling_openai.names->json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
A:transformers.modeling_openai.shapes->json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
A:transformers.modeling_openai.offsets->numpy.cumsum([np.prod(shape) for shape in shapes])
A:transformers.modeling_openai.model.tokens_embed.weight.data->torch.from_numpy(init_params[1])
A:transformers.modeling_openai.model.positions_embed.weight.data->torch.from_numpy(init_params[0])
A:transformers.modeling_openai.name->name.split('/').split('/')
A:transformers.modeling_openai.l->re.split('(\\d+)', m_name)
A:transformers.modeling_openai.pointer->getattr(pointer, l[0])
A:transformers.modeling_openai.num->int(l[1])
A:transformers.modeling_openai.pointer.data->torch.from_numpy(array)
A:transformers.modeling_openai.self.c_attn->prune_conv1d_layer(self.c_attn, index_attn, dim=1)
A:transformers.modeling_openai.self.c_proj->Conv1D(nx, n_state)
A:transformers.modeling_openai.self.attn_dropout->torch.nn.Dropout(config.attn_pdrop)
A:transformers.modeling_openai.self.resid_dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_openai.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_openai.mask->mask.view(-1).contiguous().eq(1).view(-1).contiguous().eq(1)
A:transformers.modeling_openai.index->torch.arange(len(mask))[mask].long()
A:transformers.modeling_openai.index_attn->torch.cat([index, index + self.split_size, index + 2 * self.split_size])
A:transformers.modeling_openai.w->self.attn_dropout(w)
A:transformers.modeling_openai.x->self.c_attn(x)
A:transformers.modeling_openai.(query, key, value)->self.c_attn(x).split(self.split_size, dim=2)
A:transformers.modeling_openai.query->self.split_heads(query)
A:transformers.modeling_openai.key->self.split_heads(key, k=True)
A:transformers.modeling_openai.value->self.split_heads(value)
A:transformers.modeling_openai.attn_outputs->self.attn(x, attention_mask=attention_mask, head_mask=head_mask)
A:transformers.modeling_openai.a->self.resid_dropout(a)
A:transformers.modeling_openai.self.c_fc->Conv1D(n_state, nx)
A:transformers.modeling_openai.self.dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_openai.h->self.ln_2(n + m)
A:transformers.modeling_openai.h2->self.c_proj(h)
A:transformers.modeling_openai.self.attn->Attention(nx, n_ctx, config, scale)
A:transformers.modeling_openai.self.ln_1->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.modeling_openai.self.mlp->MLP(4 * nx, config)
A:transformers.modeling_openai.self.ln_2->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.modeling_openai.n->self.ln_1(x + a)
A:transformers.modeling_openai.m->self.mlp(n)
A:transformers.modeling_openai.self.tokens_embed->self._get_resized_embeddings(self.tokens_embed, new_num_tokens)
A:transformers.modeling_openai.self.positions_embed->torch.nn.Embedding(config.n_positions, config.n_embd)
A:transformers.modeling_openai.self.drop->torch.nn.Dropout(config.embd_pdrop)
A:transformers.modeling_openai.self.h->torch.nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])
A:transformers.modeling_openai.position_ids->position_ids.view(-1, position_ids.size(-1)).view(-1, position_ids.size(-1))
A:transformers.modeling_openai.attention_mask->attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_openai.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_openai.input_shape->input_ids.view(-1, input_ids.size(-1)).size()
A:transformers.modeling_openai.input_ids->input_ids.view(-1, input_ids.size(-1)).view(-1, input_ids.size(-1))
A:transformers.modeling_openai.inputs_embeds->self.tokens_embed(input_ids)
A:transformers.modeling_openai.position_embeds->self.positions_embed(position_ids)
A:transformers.modeling_openai.token_type_ids->token_type_ids.view(-1, token_type_ids.size(-1)).view(-1, token_type_ids.size(-1))
A:transformers.modeling_openai.token_type_embeds->self.tokens_embed(token_type_ids)
A:transformers.modeling_openai.hidden_states->self.drop(hidden_states)
A:transformers.modeling_openai.outputs->block(hidden_states, attention_mask, head_mask[i])
A:transformers.modeling_openai.self.transformer->OpenAIGPTModel(config)
A:transformers.modeling_openai.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)
A:transformers.modeling_openai.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)
A:transformers.modeling_openai.lm_logits->self.lm_head(hidden_states)
A:transformers.modeling_openai.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.modeling_openai.shift_labels->lm_labels[..., 1:].contiguous()
A:transformers.modeling_openai.loss_fct->CrossEntropyLoss(ignore_index=-1)
A:transformers.modeling_openai.loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
A:transformers.modeling_openai.self.multiple_choice_head->SequenceSummary(config)
A:transformers.modeling_openai.mc_logits->self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)
transformers.OpenAIGPTDoubleHeadsModel(self,config)
transformers.OpenAIGPTDoubleHeadsModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,lm_labels=None,mc_labels=None)
transformers.OpenAIGPTDoubleHeadsModel.tie_weights(self)
transformers.OpenAIGPTLMHeadModel(self,config)
transformers.OpenAIGPTLMHeadModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.OpenAIGPTLMHeadModel.tie_weights(self)
transformers.OpenAIGPTModel(self,config)
transformers.OpenAIGPTModel._prune_heads(self,heads_to_prune)
transformers.OpenAIGPTModel._resize_token_embeddings(self,new_num_tokens)
transformers.OpenAIGPTModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.OpenAIGPTPreTrainedModel(PreTrainedModel)
transformers.OpenAIGPTPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_openai_gpt(model,config,openai_checkpoint_folder_path)
transformers.modeling_openai.Attention(self,nx,n_ctx,config,scale=False)
transformers.modeling_openai.Attention.__init__(self,nx,n_ctx,config,scale=False)
transformers.modeling_openai.Attention._attn(self,q,k,v,attention_mask=None,head_mask=None)
transformers.modeling_openai.Attention.forward(self,x,attention_mask=None,head_mask=None)
transformers.modeling_openai.Attention.merge_heads(self,x)
transformers.modeling_openai.Attention.prune_heads(self,heads)
transformers.modeling_openai.Attention.split_heads(self,x,k=False)
transformers.modeling_openai.Block(self,n_ctx,config,scale=False)
transformers.modeling_openai.Block.__init__(self,n_ctx,config,scale=False)
transformers.modeling_openai.Block.forward(self,x,attention_mask=None,head_mask=None)
transformers.modeling_openai.MLP(self,n_state,config)
transformers.modeling_openai.MLP.__init__(self,n_state,config)
transformers.modeling_openai.MLP.forward(self,x)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel(self,config)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel.__init__(self,config)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,lm_labels=None,mc_labels=None)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel.tie_weights(self)
transformers.modeling_openai.OpenAIGPTLMHeadModel(self,config)
transformers.modeling_openai.OpenAIGPTLMHeadModel.__init__(self,config)
transformers.modeling_openai.OpenAIGPTLMHeadModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_openai.OpenAIGPTLMHeadModel.tie_weights(self)
transformers.modeling_openai.OpenAIGPTModel(self,config)
transformers.modeling_openai.OpenAIGPTModel.__init__(self,config)
transformers.modeling_openai.OpenAIGPTModel._prune_heads(self,heads_to_prune)
transformers.modeling_openai.OpenAIGPTModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_openai.OpenAIGPTModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.modeling_openai.OpenAIGPTPreTrainedModel(PreTrainedModel)
transformers.modeling_openai.OpenAIGPTPreTrainedModel._init_weights(self,module)
transformers.modeling_openai.gelu(x)
transformers.modeling_openai.load_tf_weights_in_openai_gpt(model,config,openai_checkpoint_folder_path)
transformers.modeling_openai.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.corpus->pickle.load(fp, encoding='latin1')
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.config_path->os.path.abspath(transfo_xl_config_file)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.config->transformers.TransfoXLConfig.from_json_file(transfo_xl_config_file)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.model->load_tf_weights_in_transfo_xl(model, config, tf_path)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.pytorch_config_dump_path->os.path.join(pytorch_dump_folder_path, CONFIG_NAME)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,transfo_xl_config_file,pytorch_dump_folder_path,transfo_xl_dataset_file)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_utils.py----------------------------------------
A:transformers.modeling_utils.logger->logging.getLogger(__name__)
A:transformers.modeling_utils.(old_num_tokens, old_embedding_dim)->old_embeddings.weight.size()
A:transformers.modeling_utils.new_embeddings->torch.nn.Embedding(new_num_tokens, old_embedding_dim)
A:transformers.modeling_utils.num_tokens_to_copy->min(old_num_tokens, new_num_tokens)
A:transformers.modeling_utils.first_module.weight->torch.nn.Parameter(second_module.weight.clone())
A:transformers.modeling_utils.first_module.bias.data->torch.nn.functional.pad(first_module.bias.data, (0, first_module.weight.shape[0] - first_module.bias.shape[0]), 'constant', 0)
A:transformers.modeling_utils.base_model->getattr(self, self.base_model_prefix, self)
A:transformers.modeling_utils.model_embeds->getattr(self, self.base_model_prefix, self)._resize_token_embeddings(new_num_tokens)
A:transformers.modeling_utils.self.config.pruned_heads[layer]->list(union_heads)
A:transformers.modeling_utils.output_model_file->os.path.join(save_directory, WEIGHTS_NAME)
A:transformers.modeling_utils.config->kwargs.pop('config', None)
A:transformers.modeling_utils.state_dict->state_dict.copy().copy()
A:transformers.modeling_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_utils.from_tf->kwargs.pop('from_tf', False)
A:transformers.modeling_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_utils.output_loading_info->kwargs.pop('output_loading_info', False)
A:transformers.modeling_utils.(config, model_kwargs)->cls.config_class.from_pretrained(pretrained_model_name_or_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, **kwargs)
A:transformers.modeling_utils.archive_file->os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)
A:transformers.modeling_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
A:transformers.modeling_utils.model->load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)
A:transformers.modeling_utils.new_key->key.replace('beta', 'bias')
A:transformers.modeling_utils.state_dict[new_key]->state_dict.copy().copy().pop(old_key)
A:transformers.modeling_utils.metadata->getattr(state_dict, '_metadata', None)
A:transformers.modeling_utils.model_to_load->getattr(model, cls.base_model_prefix)
A:transformers.modeling_utils.w->torch.empty(nx, nf)
A:transformers.modeling_utils.self.weight->torch.nn.Parameter(w)
A:transformers.modeling_utils.self.bias->torch.nn.Parameter(torch.zeros(nf))
A:transformers.modeling_utils.x->self.dense_1(x).squeeze(-1)
A:transformers.modeling_utils.self.dense->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_utils.self.dense_0->torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
A:transformers.modeling_utils.self.activation->torch.nn.Tanh()
A:transformers.modeling_utils.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_utils.self.dense_1->torch.nn.Linear(config.hidden_size, 1, bias=False)
A:transformers.modeling_utils.start_positions->start_positions[:, None, None].expand(-1, -1, hsz)
A:transformers.modeling_utils.start_states->torch.einsum('blh,bl->bh', hidden_states, start_log_probs)
A:transformers.modeling_utils.cls_index->cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),)).expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))
A:transformers.modeling_utils.cls_token_state->hidden_states.gather(-2, cls_index).squeeze(-2)
A:transformers.modeling_utils.self.start_logits->PoolerStartLogits(config)
A:transformers.modeling_utils.self.end_logits->PoolerEndLogits(config)
A:transformers.modeling_utils.self.answer_class->PoolerAnswerClass(config)
A:transformers.modeling_utils.start_logits->self.start_logits(hidden_states, p_mask=p_mask)
A:transformers.modeling_utils.end_logits->self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)
A:transformers.modeling_utils.loss_fct->CrossEntropyLoss()
A:transformers.modeling_utils.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_utils.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_utils.cls_logits->self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)
A:transformers.modeling_utils.loss_fct_cls->torch.nn.BCEWithLogitsLoss()
A:transformers.modeling_utils.cls_loss->loss_fct_cls(cls_logits, is_impossible)
A:transformers.modeling_utils.(bsz, slen, hsz)->hidden_states.size()
A:transformers.modeling_utils.start_log_probs->torch.nn.functional.softmax(start_logits, dim=-1)
A:transformers.modeling_utils.(start_top_log_probs, start_top_index)->torch.topk(start_log_probs, self.start_n_top, dim=-1)
A:transformers.modeling_utils.start_top_index_exp->start_top_index.unsqueeze(-1).expand(-1, -1, hsz)
A:transformers.modeling_utils.hidden_states_expanded->hidden_states.unsqueeze(2).expand_as(start_states)
A:transformers.modeling_utils.end_log_probs->torch.nn.functional.softmax(end_logits, dim=1)
A:transformers.modeling_utils.(end_top_log_probs, end_top_index)->torch.topk(end_log_probs, self.end_n_top, dim=1)
A:transformers.modeling_utils.end_top_log_probs->end_top_log_probs.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_utils.end_top_index->end_top_index.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_utils.self.summary->torch.nn.Linear(config.hidden_size, num_classes)
A:transformers.modeling_utils.self.first_dropout->torch.nn.Dropout(config.summary_first_dropout)
A:transformers.modeling_utils.self.last_dropout->torch.nn.Dropout(config.summary_last_dropout)
A:transformers.modeling_utils.output->self.last_dropout(output)
A:transformers.modeling_utils.index->index.to(layer.weight.device).to(layer.weight.device)
A:transformers.modeling_utils.W->layer.weight.index_select(dim, index).clone().detach()
A:transformers.modeling_utils.b->layer.bias[index].clone().detach()
A:transformers.modeling_utils.new_size->list(layer.weight.size())
A:transformers.modeling_utils.new_size[dim]->len(index)
A:transformers.modeling_utils.new_layer->Conv1D(new_size[1], new_size[0]).to(layer.weight.device)
transformers.Conv1D(self,nf,nx)
transformers.Conv1D.forward(self,x)
transformers.PreTrainedModel(self,config,*inputs,**kwargs)
transformers.PreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)
transformers.PreTrainedModel._tie_or_clone_weights(self,first_module,second_module)
transformers.PreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.PreTrainedModel.init_weights(self)
transformers.PreTrainedModel.prune_heads(self,heads_to_prune)
transformers.PreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)
transformers.PreTrainedModel.save_pretrained(self,save_directory)
transformers.modeling_utils.Conv1D(self,nf,nx)
transformers.modeling_utils.Conv1D.__init__(self,nf,nx)
transformers.modeling_utils.Conv1D.forward(self,x)
transformers.modeling_utils.PoolerAnswerClass(self,config)
transformers.modeling_utils.PoolerAnswerClass.__init__(self,config)
transformers.modeling_utils.PoolerAnswerClass.forward(self,hidden_states,start_states=None,start_positions=None,cls_index=None)
transformers.modeling_utils.PoolerEndLogits(self,config)
transformers.modeling_utils.PoolerEndLogits.__init__(self,config)
transformers.modeling_utils.PoolerEndLogits.forward(self,hidden_states,start_states=None,start_positions=None,p_mask=None)
transformers.modeling_utils.PoolerStartLogits(self,config)
transformers.modeling_utils.PoolerStartLogits.__init__(self,config)
transformers.modeling_utils.PoolerStartLogits.forward(self,hidden_states,p_mask=None)
transformers.modeling_utils.PreTrainedModel(self,config,*inputs,**kwargs)
transformers.modeling_utils.PreTrainedModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_utils.PreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)
transformers.modeling_utils.PreTrainedModel._tie_or_clone_weights(self,first_module,second_module)
transformers.modeling_utils.PreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_utils.PreTrainedModel.init_weights(self)
transformers.modeling_utils.PreTrainedModel.prune_heads(self,heads_to_prune)
transformers.modeling_utils.PreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)
transformers.modeling_utils.PreTrainedModel.save_pretrained(self,save_directory)
transformers.modeling_utils.SQuADHead(self,config)
transformers.modeling_utils.SQuADHead.__init__(self,config)
transformers.modeling_utils.SQuADHead.forward(self,hidden_states,start_positions=None,end_positions=None,cls_index=None,is_impossible=None,p_mask=None)
transformers.modeling_utils.SequenceSummary(self,config)
transformers.modeling_utils.SequenceSummary.__init__(self,config)
transformers.modeling_utils.SequenceSummary.forward(self,hidden_states,cls_index=None)
transformers.modeling_utils.prune_conv1d_layer(layer,index,dim=1)
transformers.modeling_utils.prune_layer(layer,index,dim=None)
transformers.modeling_utils.prune_linear_layer(layer,index,dim=0)
transformers.prune_layer(layer,index,dim=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_xlm.py----------------------------------------
A:transformers.modeling_tf_xlm.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_xlm.inputs_list->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.modeling_tf_xlm.attns_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_tf_xlm.langs_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_tf_xlm.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_xlm.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_tf_xlm.out[:, 0::2]->tensorflow.constant(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_tf_xlm.out[:, 1::2]->tensorflow.constant(np.cos(position_enc[:, 1::2]))
A:transformers.modeling_tf_xlm.alen->tensorflow.range(slen)
A:transformers.modeling_tf_xlm.mask->tensorflow.reshape(mask, mask_reshape)
A:transformers.modeling_tf_xlm.attn_mask->tensorflow.cast(attn_mask, dtype=dtype)
A:transformers.modeling_tf_xlm.NEW_ID->itertools.count()
A:transformers.modeling_tf_xlm.self.layer_id->next(TFMultiHeadAttention.NEW_ID)
A:transformers.modeling_tf_xlm.self.q_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='q_lin')
A:transformers.modeling_tf_xlm.self.k_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='k_lin')
A:transformers.modeling_tf_xlm.self.v_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='v_lin')
A:transformers.modeling_tf_xlm.self.out_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='out_lin')
A:transformers.modeling_tf_xlm.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.modeling_tf_xlm.self.pruned_heads->set()
A:transformers.modeling_tf_xlm.(bs, qlen, dim)->shape_list(input)
A:transformers.modeling_tf_xlm.q->shape(self.q_lin(input))
A:transformers.modeling_tf_xlm.k->tensorflow.concat([k_, k], axis=2)
A:transformers.modeling_tf_xlm.v->tensorflow.concat([v_, v], axis=2)
A:transformers.modeling_tf_xlm.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.modeling_tf_xlm.weights->self.dropout(weights, training=training)
A:transformers.modeling_tf_xlm.context->unshape(context)
A:transformers.modeling_tf_xlm.self.lin1->tensorflow.keras.layers.Dense(dim_hidden, kernel_initializer=get_initializer(config.init_std), name='lin1')
A:transformers.modeling_tf_xlm.self.lin2->tensorflow.keras.layers.Dense(out_dim, kernel_initializer=get_initializer(config.init_std), name='lin2')
A:transformers.modeling_tf_xlm.x->self.dropout(x, training=training)
A:transformers.modeling_tf_xlm.self.attention_dropout->tensorflow.keras.layers.Dropout(config.attention_dropout)
A:transformers.modeling_tf_xlm.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, self.dim, embeddings_initializer=get_initializer(config.embed_init_std), name='position_embeddings')
A:transformers.modeling_tf_xlm.self.lang_embeddings->tensorflow.keras.layers.Embedding(self.n_langs, self.dim, embeddings_initializer=get_initializer(config.embed_init_std), name='lang_embeddings')
A:transformers.modeling_tf_xlm.self.embeddings->TFSharedEmbeddings(self.n_words, self.dim, initializer_range=config.embed_init_std, name='embeddings')
A:transformers.modeling_tf_xlm.self.layer_norm_emb->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm_emb')
A:transformers.modeling_tf_xlm.pruned_heads->config.pruned_heads.copy().items()
A:transformers.modeling_tf_xlm.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_xlm.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_xlm.langs->inputs.get('langs', langs)
A:transformers.modeling_tf_xlm.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_xlm.position_ids->tensorflow.expand_dims(tf.range(slen), axis=0)
A:transformers.modeling_tf_xlm.lengths->tensorflow.reduce_sum(tf.cast(tf.not_equal(input_ids, self.pad_index), dtype=tf.int32), axis=1)
A:transformers.modeling_tf_xlm.cache->inputs.get('cache', cache)
A:transformers.modeling_tf_xlm.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_xlm.(bs, slen)->shape_list(input_ids)
A:transformers.modeling_tf_xlm.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.modeling_tf_xlm.tensor->self.layer_norm2[i](tensor)
A:transformers.modeling_tf_xlm.attn_outputs->self.attentions[i]([tensor, attn_mask, None, cache, head_mask[i]], training=training)
A:transformers.modeling_tf_xlm.attn->self.dropout(attn, training=training)
A:transformers.modeling_tf_xlm.self.transformer->TFXLMMainLayer(config, name='transformer')
A:transformers.modeling_tf_xlm.outputs->self.pred_layer(output)
A:transformers.modeling_tf_xlm.self.bias->self.add_weight(shape=(self.n_words,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_xlm.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_xlm.self.pred_layer->TFXLMPredLayer(config, self.transformer.embeddings, name='pred_layer_._proj')
A:transformers.modeling_tf_xlm.transformer_outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_xlm.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.init_std, name='sequence_summary')
A:transformers.modeling_tf_xlm.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_xlm.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.init_std), name='qa_outputs')
A:transformers.modeling_tf_xlm.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_xlm.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_xlm.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFXLMForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFXLMForQuestionAnsweringSimple.call(self,inputs,**kwargs)
transformers.TFXLMForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFXLMForSequenceClassification.call(self,inputs,**kwargs)
transformers.TFXLMMainLayer(self,config,**kwargs)
transformers.TFXLMMainLayer._prune_heads(self,heads_to_prune)
transformers.TFXLMMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFXLMMainLayer.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,training=False)
transformers.TFXLMModel(self,config,*inputs,**kwargs)
transformers.TFXLMModel.call(self,inputs,**kwargs)
transformers.TFXLMPreTrainedModel(TFPreTrainedModel)
transformers.TFXLMWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFXLMWithLMHeadModel.call(self,inputs,**kwargs)
transformers.load_xlm_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_xlm.TFMultiHeadAttention(self,n_heads,dim,config,**kwargs)
transformers.modeling_tf_xlm.TFMultiHeadAttention.__init__(self,n_heads,dim,config,**kwargs)
transformers.modeling_tf_xlm.TFMultiHeadAttention.call(self,inputs,training=False)
transformers.modeling_tf_xlm.TFMultiHeadAttention.prune_heads(self,heads)
transformers.modeling_tf_xlm.TFTransformerFFN(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.modeling_tf_xlm.TFTransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.modeling_tf_xlm.TFTransformerFFN.call(self,input,training=False)
transformers.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.call(self,inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForSequenceClassification.call(self,inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMMainLayer(self,config,**kwargs)
transformers.modeling_tf_xlm.TFXLMMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_xlm.TFXLMMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_xlm.TFXLMMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_xlm.TFXLMMainLayer.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,training=False)
transformers.modeling_tf_xlm.TFXLMModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_xlm.TFXLMPredLayer(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlm.TFXLMPredLayer.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlm.TFXLMPredLayer.build(self,input_shape)
transformers.modeling_tf_xlm.TFXLMPredLayer.call(self,hidden_states)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlm.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.modeling_tf_xlm.gelu(x)
transformers.modeling_tf_xlm.get_masks(slen,lengths,causal,padding_mask=None,dtype=tf.float32)
transformers.modeling_tf_xlm.load_xlm_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_transfo_xl.py----------------------------------------
A:transformers.configuration_transfo_xl.logger->logging.getLogger(__name__)
A:transformers.configuration_transfo_xl.json_config->json.loads(reader.read())
transformers.TransfoXLConfig(self,vocab_size_or_config_json_file=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,tgt_len=128,ext_len=0,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,tie_weight=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,**kwargs)
transformers.TransfoXLConfig.hidden_size(self)
transformers.TransfoXLConfig.max_position_embeddings(self)
transformers.TransfoXLConfig.num_attention_heads(self)
transformers.TransfoXLConfig.num_hidden_layers(self)
transformers.TransfoXLConfig.vocab_size(self)
transformers.TransfoXLConfig.vocab_size(self,value)
transformers.configuration_transfo_xl.TransfoXLConfig(self,vocab_size_or_config_json_file=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,tgt_len=128,ext_len=0,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,tie_weight=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,**kwargs)
transformers.configuration_transfo_xl.TransfoXLConfig.__init__(self,vocab_size_or_config_json_file=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,tgt_len=128,ext_len=0,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,tie_weight=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,**kwargs)
transformers.configuration_transfo_xl.TransfoXLConfig.hidden_size(self)
transformers.configuration_transfo_xl.TransfoXLConfig.max_position_embeddings(self)
transformers.configuration_transfo_xl.TransfoXLConfig.num_attention_heads(self)
transformers.configuration_transfo_xl.TransfoXLConfig.num_hidden_layers(self)
transformers.configuration_transfo_xl.TransfoXLConfig.vocab_size(self)
transformers.configuration_transfo_xl.TransfoXLConfig.vocab_size(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_transfo_xl.py----------------------------------------
A:transformers.modeling_tf_transfo_xl.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_transfo_xl.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_transfo_xl.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_transfo_xl.sinusoid_inp->tensorflow.einsum('i,j->ij', pos_seq, self.inv_freq)
A:transformers.modeling_tf_transfo_xl.pos_emb->self.drop(pos_emb, training=training)
A:transformers.modeling_tf_transfo_xl.self.layer_1->tensorflow.keras.layers.Dense(d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name='CoreNet_._0')
A:transformers.modeling_tf_transfo_xl.self.drop_1->tensorflow.keras.layers.Dropout(dropout)
A:transformers.modeling_tf_transfo_xl.self.layer_2->tensorflow.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name='CoreNet_._3')
A:transformers.modeling_tf_transfo_xl.self.drop_2->tensorflow.keras.layers.Dropout(dropout)
A:transformers.modeling_tf_transfo_xl.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layer_norm')
A:transformers.modeling_tf_transfo_xl.core_out->self.drop(core_out, training=training)
A:transformers.modeling_tf_transfo_xl.output->self.layer_norm(inp + core_out)
A:transformers.modeling_tf_transfo_xl.self.qkv_net->tensorflow.keras.layers.Dense(3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name='qkv_net')
A:transformers.modeling_tf_transfo_xl.self.drop->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.modeling_tf_transfo_xl.self.dropatt->tensorflow.keras.layers.Dropout(dropatt)
A:transformers.modeling_tf_transfo_xl.self.o_net->tensorflow.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name='o_net')
A:transformers.modeling_tf_transfo_xl.self.r_net->tensorflow.keras.layers.Dense(self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name='r_net')
A:transformers.modeling_tf_transfo_xl.self.r_r_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_r_bias')
A:transformers.modeling_tf_transfo_xl.self.r_w_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_w_bias')
A:transformers.modeling_tf_transfo_xl.x_size->shape_list(x)
A:transformers.modeling_tf_transfo_xl.x->tensorflow.reshape(x, x_size)
A:transformers.modeling_tf_transfo_xl.cat->tensorflow.concat([mems[i], hids[i]], axis=0)
A:transformers.modeling_tf_transfo_xl.w_heads->self.qkv_net(w)
A:transformers.modeling_tf_transfo_xl.r_head_k->tensorflow.reshape(r_head_k, (rlen, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.(w_head_q, w_head_k, w_head_v)->tensorflow.split(w_heads, 3, axis=-1)
A:transformers.modeling_tf_transfo_xl.w_head_q->tensorflow.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.w_head_k->tensorflow.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.w_head_v->tensorflow.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.AC->tensorflow.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)
A:transformers.modeling_tf_transfo_xl.BD->self._rel_shift(BD)
A:transformers.modeling_tf_transfo_xl.attn_prob->self.dropatt(attn_prob, training=training)
A:transformers.modeling_tf_transfo_xl.attn_vec->tensorflow.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))
A:transformers.modeling_tf_transfo_xl.attn_vec_sizes->shape_list(attn_vec)
A:transformers.modeling_tf_transfo_xl.attn_out->self.drop(attn_out, training=training)
A:transformers.modeling_tf_transfo_xl.self.dec_attn->TFRelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len, dropatt=dropatt, pre_lnorm=pre_lnorm, r_w_bias=r_w_bias, r_r_bias=r_r_bias, init_std=init_std, output_attentions=output_attentions, layer_norm_epsilon=layer_norm_epsilon, name='dec_attn')
A:transformers.modeling_tf_transfo_xl.self.pos_ff->TFPositionwiseFF(d_model, d_inner, dropout, pre_lnorm=pre_lnorm, init_std=init_std, layer_norm_epsilon=layer_norm_epsilon, name='pos_ff')
A:transformers.modeling_tf_transfo_xl.attn_outputs->self.dec_attn([dec_inp, r, dec_attn_mask, mems, head_mask], training=training)
A:transformers.modeling_tf_transfo_xl.ff_output->self.pos_ff(attn_outputs[0], training=training)
A:transformers.modeling_tf_transfo_xl.inp_flat->tensorflow.reshape(inp, (-1,))
A:transformers.modeling_tf_transfo_xl.emb_flat->tensorflow.zeros([shape_list(inp_flat)[0], self.d_proj])
A:transformers.modeling_tf_transfo_xl.emb_i->tensorflow.einsum('id,de->ie', emb_i, self.emb_projs[i])
A:transformers.modeling_tf_transfo_xl.mask_idx->tensorflow.cast(tf.where(mask_i), dtype=tf.int64)
A:transformers.modeling_tf_transfo_xl.embed->tensorflow.reshape(emb_flat, embed_shape)
A:transformers.modeling_tf_transfo_xl.self.word_emb->TFAdaptiveEmbedding(config.n_token, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, init_std=config.init_std, name='word_emb')
A:transformers.modeling_tf_transfo_xl.self.pos_emb->TFPositionalEmbedding(self.d_model, name='pos_emb')
A:transformers.modeling_tf_transfo_xl.empty->tensorflow.zeros([self.mem_len, shape_list(data)[1], self.d_model])
A:transformers.modeling_tf_transfo_xl.beg_idx->max(0, end_idx - self.mem_len)
A:transformers.modeling_tf_transfo_xl.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_transfo_xl.mems->inputs.get('mems', mems)
A:transformers.modeling_tf_transfo_xl.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_transfo_xl.(qlen, bsz)->shape_list(input_ids)
A:transformers.modeling_tf_transfo_xl.word_emb->self.word_emb(input_ids)
A:transformers.modeling_tf_transfo_xl.attn_mask->tensorflow.ones([qlen, qlen])
A:transformers.modeling_tf_transfo_xl.mask_u->tensorflow.linalg.band_part(attn_mask, 0, -1)
A:transformers.modeling_tf_transfo_xl.mask_dia->tensorflow.linalg.band_part(attn_mask, 0, 0)
A:transformers.modeling_tf_transfo_xl.attn_mask_pad->tensorflow.zeros([qlen, mlen])
A:transformers.modeling_tf_transfo_xl.dec_attn_mask->tensorflow.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)
A:transformers.modeling_tf_transfo_xl.mask_l->tensorflow.linalg.band_part(attn_mask, -1, 0)
A:transformers.modeling_tf_transfo_xl.pos_seq->tensorflow.minimum(pos_seq, self.clamp_len)
A:transformers.modeling_tf_transfo_xl.layer_outputs->layer([core_out, pos_emb, dec_attn_mask, mems_i, head_mask[i]], training=training)
A:transformers.modeling_tf_transfo_xl.new_mems->self._update_mems(hids, mems, mlen, qlen)
A:transformers.modeling_tf_transfo_xl.hids->list((tf.transpose(t, perm=(1, 0, 2)) for t in hids))
A:transformers.modeling_tf_transfo_xl.attentions->list((tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions))
A:transformers.modeling_tf_transfo_xl.self.transformer->TFTransfoXLMainLayer(config, name='transformer')
A:transformers.modeling_tf_transfo_xl.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_transfo_xl.self.crit->TFAdaptiveSoftmaxMask(config.n_token, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, name='crit')
A:transformers.modeling_tf_transfo_xl.labels->inputs.get('labels', labels)
A:transformers.modeling_tf_transfo_xl.transformer_outputs->self.transformer([input_ids, mems, head_mask], training=training)
A:transformers.modeling_tf_transfo_xl.softmax_output->self.crit([pred_hid, labels], training=training)
transformers.TFTransfoXLLMHeadModel(self,config)
transformers.TFTransfoXLLMHeadModel.call(self,inputs,mems=None,head_mask=None,labels=None,training=False)
transformers.TFTransfoXLLMHeadModel.init_mems(self,data)
transformers.TFTransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TFTransfoXLMainLayer(self,config,**kwargs)
transformers.TFTransfoXLMainLayer._prune_heads(self,heads)
transformers.TFTransfoXLMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFTransfoXLMainLayer._update_mems(self,hids,mems,qlen,mlen)
transformers.TFTransfoXLMainLayer.backward_compatible(self)
transformers.TFTransfoXLMainLayer.build(self,input_shape)
transformers.TFTransfoXLMainLayer.call(self,inputs,mems=None,head_mask=None,training=False)
transformers.TFTransfoXLMainLayer.init_mems(self,data)
transformers.TFTransfoXLMainLayer.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TFTransfoXLModel(self,config,*inputs,**kwargs)
transformers.TFTransfoXLModel.call(self,inputs,**kwargs)
transformers.TFTransfoXLPreTrainedModel(TFPreTrainedModel)
transformers.load_transfo_xl_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding.call(self,inp)
transformers.modeling_tf_transfo_xl.TFPositionalEmbedding(self,demb,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionalEmbedding.__init__(self,demb,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionalEmbedding.call(self,pos_seq,bsz=None)
transformers.modeling_tf_transfo_xl.TFPositionwiseFF(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionwiseFF.__init__(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionwiseFF.call(self,inp,training=False)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer(self,n_head,d_model,d_head,d_inner,dropout,tgt_len=None,ext_len=None,mem_len=None,dropatt=0.0,pre_lnorm=False,r_w_bias=None,r_r_bias=None,output_attentions=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer.__init__(self,n_head,d_model,d_head,d_inner,dropout,tgt_len=None,ext_len=None,mem_len=None,dropatt=0.0,pre_lnorm=False,r_w_bias=None,r_r_bias=None,output_attentions=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer.call(self,inputs,training=False)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,output_attentions=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.__init__(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,output_attentions=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn._rel_shift(self,x)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.call(self,inputs,training=False)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel(self,config)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.__init__(self,config)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.call(self,inputs,mems=None,head_mask=None,labels=None,training=False)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.init_mems(self,data)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer(self,config,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer._prune_heads(self,heads)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer._update_mems(self,hids,mems,qlen,mlen)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.backward_compatible(self)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.call(self,inputs,mems=None,head_mask=None,training=False)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.init_mems(self,data)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_tf_transfo_xl.TFTransfoXLModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLModel.call(self,inputs,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_transfo_xl.load_transfo_xl_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_openai.py----------------------------------------
A:transformers.tokenization_openai.logger->logging.getLogger(__name__)
A:transformers.tokenization_openai.pairs->get_pairs(word)
A:transformers.tokenization_openai.text->self.nlp(text_standardize(self.fix_text(text)))
A:transformers.tokenization_openai._nlp->English()
A:transformers.tokenization_openai.self.nlp->BasicTokenizer(do_lower_case=True)
A:transformers.tokenization_openai.self.encoder->json.load(open(vocab_file, encoding='utf-8'))
A:transformers.tokenization_openai.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.tokenization_openai.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_openai.j->' '.join(word).index(first, i)
A:transformers.tokenization_openai.new_word->tuple(new_word)
A:transformers.tokenization_openai.word->' '.join(word)
A:transformers.tokenization_openai.out_string->''.join(tokens).replace('</w>', ' ').strip()
A:transformers.tokenization_openai.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_openai.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.OpenAIGPTTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.OpenAIGPTTokenizer._convert_id_to_token(self,index)
transformers.OpenAIGPTTokenizer._convert_token_to_id(self,token)
transformers.OpenAIGPTTokenizer._tokenize(self,text)
transformers.OpenAIGPTTokenizer.bpe(self,token)
transformers.OpenAIGPTTokenizer.convert_tokens_to_string(self,tokens)
transformers.OpenAIGPTTokenizer.save_vocabulary(self,save_directory)
transformers.OpenAIGPTTokenizer.vocab_size(self)
transformers.tokenization_openai.OpenAIGPTTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.OpenAIGPTTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.OpenAIGPTTokenizer._convert_id_to_token(self,index)
transformers.tokenization_openai.OpenAIGPTTokenizer._convert_token_to_id(self,token)
transformers.tokenization_openai.OpenAIGPTTokenizer._tokenize(self,text)
transformers.tokenization_openai.OpenAIGPTTokenizer.bpe(self,token)
transformers.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_openai.OpenAIGPTTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_openai.OpenAIGPTTokenizer.vocab_size(self)
transformers.tokenization_openai.get_pairs(word)
transformers.tokenization_openai.text_standardize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_pytorch_utils.py----------------------------------------
A:transformers.modeling_tf_pytorch_utils.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_pytorch_utils.tf_name->tf_name.replace(start_prefix_to_remove, '', 1).replace(start_prefix_to_remove, '', 1)
A:transformers.modeling_tf_pytorch_utils.transpose->bool(tf_name[-1] == 'kernel' or 'emb_projs' in tf_name or 'out_projs' in tf_name)
A:transformers.modeling_tf_pytorch_utils.pt_path->os.path.abspath(pytorch_checkpoint_path)
A:transformers.modeling_tf_pytorch_utils.pt_state_dict->pt_model.state_dict()
A:transformers.modeling_tf_pytorch_utils.tf_inputs->tensorflow.constant(DUMMY_INPUTS)
A:transformers.modeling_tf_pytorch_utils.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_pytorch_utils.new_key->key.replace('beta', 'bias')
A:transformers.modeling_tf_pytorch_utils.pt_state_dict[new_key]->pt_model.state_dict().pop(old_key)
A:transformers.modeling_tf_pytorch_utils.all_pytorch_weights->set(list(pt_state_dict.keys()))
A:transformers.modeling_tf_pytorch_utils.(name, transpose)->convert_tf_weight_name_to_pt_weight_name(sw_name, start_prefix_to_remove=start_prefix_to_remove)
A:transformers.modeling_tf_pytorch_utils.array->numpy.expand_dims(array, axis=0)
A:transformers.modeling_tf_pytorch_utils.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_tf_pytorch_utils.tf_model_class->getattr(transformers, tf_model_class_name)
A:transformers.modeling_tf_pytorch_utils.tf_model->tf_model_class(pt_model.config)
A:transformers.modeling_tf_pytorch_utils.current_pt_params_dict->dict(pt_model.named_parameters())
A:transformers.modeling_tf_pytorch_utils.(pt_name, transpose)->convert_tf_weight_name_to_pt_weight_name(tf_weight.name, start_prefix_to_remove=start_prefix_to_remove)
A:transformers.modeling_tf_pytorch_utils.all_tf_weights->set(list(tf_weights_map.keys()))
A:transformers.modeling_tf_pytorch_utils.new_pt_params_dict[pt_weight_name]->torch.from_numpy(array)
A:transformers.modeling_tf_pytorch_utils.loaded_pt_weights_data_ptr[pt_weight.data_ptr()]->torch.from_numpy(array)
A:transformers.modeling_tf_pytorch_utils.(missing_keys, unexpected_keys)->pt_model.load_state_dict(new_pt_params_dict, strict=False)
transformers.convert_tf_weight_name_to_pt_weight_name(tf_name,start_prefix_to_remove='')
transformers.load_pytorch_checkpoint_in_tf2_model(tf_model,pytorch_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.load_pytorch_model_in_tf2_model(tf_model,pt_model,tf_inputs=None,allow_missing_keys=False)
transformers.load_pytorch_weights_in_tf2_model(tf_model,pt_state_dict,tf_inputs=None,allow_missing_keys=False)
transformers.load_tf2_checkpoint_in_pytorch_model(pt_model,tf_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.load_tf2_model_in_pytorch_model(pt_model,tf_model,allow_missing_keys=False)
transformers.load_tf2_weights_in_pytorch_model(pt_model,tf_weights,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.convert_tf_weight_name_to_pt_weight_name(tf_name,start_prefix_to_remove='')
transformers.modeling_tf_pytorch_utils.load_pytorch_checkpoint_in_tf2_model(tf_model,pytorch_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_pytorch_model_in_tf2_model(tf_model,pt_model,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_pytorch_weights_in_tf2_model(tf_model,pt_state_dict,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_checkpoint_in_pytorch_model(pt_model,tf_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_model_in_pytorch_model(pt_model,tf_model,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_weights_in_pytorch_model(pt_model,tf_weights,allow_missing_keys=False)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_bert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.config->transformers.BertConfig.from_json_file(bert_config_file)
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.model->BertForPreTraining(config)
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_bert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_transfo_xl_utilities.py----------------------------------------
A:transformers.modeling_tf_transfo_xl_utilities.self.cluster_weight->self.add_weight(shape=(self.n_clusters, self.d_embed), initializer='zeros', trainable=True, name='cluster_weight')
A:transformers.modeling_tf_transfo_xl_utilities.self.cluster_bias->self.add_weight(shape=(self.n_clusters,), initializer='zeros', trainable=True, name='cluster_bias')
A:transformers.modeling_tf_transfo_xl_utilities.weight->self.add_weight(shape=(r_idx - l_idx, d_emb_i), initializer='zeros', trainable=True, name='out_layers_._{}_._weight'.format(i))
A:transformers.modeling_tf_transfo_xl_utilities.bias->self.add_weight(shape=(r_idx - l_idx,), initializer='zeros', trainable=True, name='out_layers_._{}_._bias'.format(i))
A:transformers.modeling_tf_transfo_xl_utilities.y->tensorflow.einsum('ibd,ed->ibe', y, proj)
A:transformers.modeling_tf_transfo_xl_utilities.lp_size->tensorflow.shape(logprob)
A:transformers.modeling_tf_transfo_xl_utilities.r->tensorflow.range(lp_size[0])
A:transformers.modeling_tf_transfo_xl_utilities.idx->tensorflow.stack([r, target], 1)
A:transformers.modeling_tf_transfo_xl_utilities.softmax_b->tensorflow.get_variable('bias', [n_token], initializer=tf.zeros_initializer())
A:transformers.modeling_tf_transfo_xl_utilities.output->self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])
A:transformers.modeling_tf_transfo_xl_utilities.loss->tensorflow.reduce_mean(loss)
A:transformers.modeling_tf_transfo_xl_utilities.out->tensorflow.concat(out, axis=-1)
A:transformers.modeling_tf_transfo_xl_utilities.hidden_sizes->shape_list(hidden)
A:transformers.modeling_tf_transfo_xl_utilities.mask_idx->tensorflow.where(mask)
A:transformers.modeling_tf_transfo_xl_utilities.cur_W->tensorflow.concat([cur_W, self.cluster_weight], 0)
A:transformers.modeling_tf_transfo_xl_utilities.cur_b->tensorflow.concat([cur_b, self.cluster_bias], 0)
A:transformers.modeling_tf_transfo_xl_utilities.head_logit->self._logit(hidden, cur_W, cur_b, self.out_projs[0])
A:transformers.modeling_tf_transfo_xl_utilities.head_logprob->tensorflow.nn.log_softmax(head_logit)
A:transformers.modeling_tf_transfo_xl_utilities.cur_head_logprob->tensorflow.boolean_mask(head_logprob, mask)
A:transformers.modeling_tf_transfo_xl_utilities.cur_logprob->self._gather_logprob(cur_tail_logprob, cur_target)
A:transformers.modeling_tf_transfo_xl_utilities.tail_logit->self._logit(hidden, cur_W, cur_b, self.out_projs[i])
A:transformers.modeling_tf_transfo_xl_utilities.tail_logprob->tensorflow.nn.log_softmax(tail_logit)
A:transformers.modeling_tf_transfo_xl_utilities.cur_tail_logprob->tensorflow.boolean_mask(tail_logprob, mask)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False,**kwargs)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False,**kwargs)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask._gather_logprob(logprob,target)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask._logit(x,W,b,proj=None)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.build(self,input_shape)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.call(self,inputs,return_mean=True,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_xlm.py----------------------------------------
A:transformers.tokenization_xlm.logger->logging.getLogger(__name__)
A:transformers.tokenization_xlm.pairs->get_pairs(word)
A:transformers.tokenization_xlm.text->lowercase_and_remove_accent(text)
A:transformers.tokenization_xlm.cat->unicodedata.category(char)
A:transformers.tokenization_xlm.self.cache_moses_punct_normalizer->dict()
A:transformers.tokenization_xlm.self.cache_moses_tokenizer->dict()
A:transformers.tokenization_xlm.self.lang_with_custom_tokenizer->set(['zh', 'th', 'ja'])
A:transformers.tokenization_xlm.self.encoder->json.load(open(vocab_file, encoding='utf-8'))
A:transformers.tokenization_xlm.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.tokenization_xlm.punct_normalizer->sacremoses.MosesPunctNormalizer(lang=lang)
A:transformers.tokenization_xlm.moses_tokenizer->sacremoses.MosesTokenizer(lang=lang)
A:transformers.tokenization_xlm.self.ja_word_tokenizer->Mykytea.Mykytea('-model %s/local/share/kytea/model.bin' % os.path.expanduser('~'))
A:transformers.tokenization_xlm.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_xlm.j->' '.join(word).index(first, i)
A:transformers.tokenization_xlm.new_word->tuple(new_word)
A:transformers.tokenization_xlm.word->' '.join(word)
A:transformers.tokenization_xlm.out_string->''.join(tokens).replace('</w>', ' ').strip()
A:transformers.tokenization_xlm.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_xlm.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.XLMTokenizer(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.XLMTokenizer._convert_id_to_token(self,index)
transformers.XLMTokenizer._convert_token_to_id(self,token)
transformers.XLMTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.XLMTokenizer.bpe(self,token)
transformers.XLMTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.XLMTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLMTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.XLMTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.XLMTokenizer.ja_tokenize(self,text)
transformers.XLMTokenizer.moses_pipeline(self,text,lang)
transformers.XLMTokenizer.moses_punct_norm(self,text,lang)
transformers.XLMTokenizer.moses_tokenize(self,text,lang)
transformers.XLMTokenizer.save_vocabulary(self,save_directory)
transformers.XLMTokenizer.vocab_size(self)
transformers.tokenization_xlm.XLMTokenizer(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.tokenization_xlm.XLMTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.tokenization_xlm.XLMTokenizer._convert_id_to_token(self,index)
transformers.tokenization_xlm.XLMTokenizer._convert_token_to_id(self,token)
transformers.tokenization_xlm.XLMTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.tokenization_xlm.XLMTokenizer.bpe(self,token)
transformers.tokenization_xlm.XLMTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_xlm.XLMTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_xlm.XLMTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.tokenization_xlm.XLMTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.tokenization_xlm.XLMTokenizer.ja_tokenize(self,text)
transformers.tokenization_xlm.XLMTokenizer.moses_pipeline(self,text,lang)
transformers.tokenization_xlm.XLMTokenizer.moses_punct_norm(self,text,lang)
transformers.tokenization_xlm.XLMTokenizer.moses_tokenize(self,text,lang)
transformers.tokenization_xlm.XLMTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_xlm.XLMTokenizer.vocab_size(self)
transformers.tokenization_xlm.get_pairs(word)
transformers.tokenization_xlm.lowercase_and_remove_accent(text)
transformers.tokenization_xlm.remove_non_printing_char(text)
transformers.tokenization_xlm.replace_unicode_punct(text)
transformers.tokenization_xlm.romanian_preprocessing(text)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_xlnet.py----------------------------------------
A:transformers.configuration_xlnet.logger->logging.getLogger(__name__)
A:transformers.configuration_xlnet.json_config->json.loads(reader.read())
transformers.XLNetConfig(self,vocab_size_or_config_json_file=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,max_position_embeddings=512,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=None,reuse_len=None,bi_data=False,clamp_len=-1,same_length=False,finetuning_task=None,num_labels=2,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,**kwargs)
transformers.XLNetConfig.hidden_size(self)
transformers.XLNetConfig.max_position_embeddings(self)
transformers.XLNetConfig.num_attention_heads(self)
transformers.XLNetConfig.num_hidden_layers(self)
transformers.XLNetConfig.vocab_size(self)
transformers.XLNetConfig.vocab_size(self,value)
transformers.configuration_xlnet.XLNetConfig(self,vocab_size_or_config_json_file=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,max_position_embeddings=512,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=None,reuse_len=None,bi_data=False,clamp_len=-1,same_length=False,finetuning_task=None,num_labels=2,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,**kwargs)
transformers.configuration_xlnet.XLNetConfig.__init__(self,vocab_size_or_config_json_file=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,max_position_embeddings=512,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=None,reuse_len=None,bi_data=False,clamp_len=-1,same_length=False,finetuning_task=None,num_labels=2,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,**kwargs)
transformers.configuration_xlnet.XLNetConfig.hidden_size(self)
transformers.configuration_xlnet.XLNetConfig.max_position_embeddings(self)
transformers.configuration_xlnet.XLNetConfig.num_attention_heads(self)
transformers.configuration_xlnet.XLNetConfig.num_hidden_layers(self)
transformers.configuration_xlnet.XLNetConfig.vocab_size(self)
transformers.configuration_xlnet.XLNetConfig.vocab_size(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_openai.py----------------------------------------
A:transformers.configuration_openai.logger->logging.getLogger(__name__)
A:transformers.configuration_openai.json_config->json.loads(reader.read())
transformers.OpenAIGPTConfig(self,vocab_size_or_config_json_file=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.OpenAIGPTConfig.hidden_size(self)
transformers.OpenAIGPTConfig.max_position_embeddings(self)
transformers.OpenAIGPTConfig.num_attention_heads(self)
transformers.OpenAIGPTConfig.num_hidden_layers(self)
transformers.configuration_openai.OpenAIGPTConfig(self,vocab_size_or_config_json_file=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_openai.OpenAIGPTConfig.__init__(self,vocab_size_or_config_json_file=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,num_labels=1,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_openai.OpenAIGPTConfig.hidden_size(self)
transformers.configuration_openai.OpenAIGPTConfig.max_position_embeddings(self)
transformers.configuration_openai.OpenAIGPTConfig.num_attention_heads(self)
transformers.configuration_openai.OpenAIGPTConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_roberta.py----------------------------------------
A:transformers.modeling_tf_roberta.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_roberta.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_roberta.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_roberta.self.embeddings->TFRobertaEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_roberta.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_roberta.self.roberta->TFRobertaMainLayer(config, name='roberta')
A:transformers.modeling_tf_roberta.outputs->self.roberta(inputs, **kwargs)
A:transformers.modeling_tf_roberta.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), activation='tanh', name='dense')
A:transformers.modeling_tf_roberta.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.modeling_tf_roberta.self.act->tensorflow.keras.layers.Activation(gelu)
A:transformers.modeling_tf_roberta.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_roberta.x->self.out_proj(x)
A:transformers.modeling_tf_roberta.self.lm_head->TFRobertaLMHead(config, self.roberta.embeddings, name='lm_head')
A:transformers.modeling_tf_roberta.prediction_scores->self.lm_head(sequence_output)
A:transformers.modeling_tf_roberta.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_roberta.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.modeling_tf_roberta.self.classifier->TFRobertaClassificationHead(config, name='classifier')
A:transformers.modeling_tf_roberta.logits->self.classifier(sequence_output, training=kwargs.get('training', False))
transformers.TFRobertaForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFRobertaForMaskedLM.call(self,inputs,**kwargs)
transformers.TFRobertaForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFRobertaForSequenceClassification.call(self,inputs,**kwargs)
transformers.TFRobertaMainLayer(self,config,**kwargs)
transformers.TFRobertaMainLayer.call(self,inputs,**kwargs)
transformers.TFRobertaModel(self,config,*inputs,**kwargs)
transformers.TFRobertaModel.call(self,inputs,**kwargs)
transformers.TFRobertaPreTrainedModel(TFPreTrainedModel)
transformers.load_roberta_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_roberta.TFRobertaClassificationHead(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaClassificationHead.__init__(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaClassificationHead.call(self,features,training=False)
transformers.modeling_tf_roberta.TFRobertaEmbeddings(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaEmbeddings._embedding(self,inputs,training=False)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM.call(self,inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForSequenceClassification.call(self,inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_roberta.TFRobertaLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_roberta.TFRobertaLMHead.build(self,input_shape)
transformers.modeling_tf_roberta.TFRobertaLMHead.call(self,features)
transformers.modeling_tf_roberta.TFRobertaMainLayer(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaMainLayer.call(self,inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaModel.call(self,inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_roberta.load_roberta_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_bert_pytorch_checkpoint_to_original_tf.py----------------------------------------
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.state_dict->transformers.BertModel.from_pretrained(pretrained_model_name_or_path=args.model_name, state_dict=torch.load(args.pytorch_model_path), cache_dir=args.cache_dir).state_dict()
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.name->name.replace(patt, repl).replace(patt, repl)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_dtype->tensorflow.dtypes.as_dtype(tensor.dtype)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_var->create_tf_var(tensor=torch_tensor, name=tf_name, session=session)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_name->to_tf_var_name(var_name)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.torch_tensor->state_dict[var_name].numpy()
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_weight->session.run(tf_var)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.saver->tensorflow.train.Saver(tf.trainable_variables())
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.parser->argparse.ArgumentParser()
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.args->argparse.ArgumentParser().parse_args(raw_args)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.model->transformers.BertModel.from_pretrained(pretrained_model_name_or_path=args.model_name, state_dict=torch.load(args.pytorch_model_path), cache_dir=args.cache_dir)
transformers.convert_bert_pytorch_checkpoint_to_original_tf.convert_pytorch_checkpoint_to_tf(model:BertModel,ckpt_dir:str,model_name:str)
transformers.convert_bert_pytorch_checkpoint_to_original_tf.main(raw_args=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_openai_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.config->transformers.OpenAIGPTConfig.from_json_file(openai_config_file)
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.model->OpenAIGPTModel(config)
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_openai_original_tf_checkpoint_to_pytorch.convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path,openai_config_file,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_auto.py----------------------------------------
A:transformers.modeling_auto.logger->logging.getLogger(__name__)
transformers.AutoModel(self)
transformers.AutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForQuestionAnswering(self)
transformers.AutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForSequenceClassification(self)
transformers.AutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelWithLMHead(self)
transformers.AutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModel(self)
transformers.modeling_auto.AutoModel.__init__(self)
transformers.modeling_auto.AutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForQuestionAnswering(self)
transformers.modeling_auto.AutoModelForQuestionAnswering.__init__(self)
transformers.modeling_auto.AutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForSequenceClassification(self)
transformers.modeling_auto.AutoModelForSequenceClassification.__init__(self)
transformers.modeling_auto.AutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelWithLMHead(self)
transformers.modeling_auto.AutoModelWithLMHead.__init__(self)
transformers.modeling_auto.AutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_openai.py----------------------------------------
A:transformers.modeling_tf_openai.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_openai.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_openai.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_openai.self.c_attn->TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name='c_attn')
A:transformers.modeling_tf_openai.self.c_proj->TFConv1D(nx, n_state, initializer_range=config.initializer_range, name='c_proj')
A:transformers.modeling_tf_openai.self.attn_dropout->tensorflow.keras.layers.Dropout(config.attn_pdrop)
A:transformers.modeling_tf_openai.self.resid_dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_openai.self.pruned_heads->set()
A:transformers.modeling_tf_openai.j->tensorflow.range(ns)
A:transformers.modeling_tf_openai.w->self.attn_dropout(w, training=training)
A:transformers.modeling_tf_openai.dk->tensorflow.cast(tf.shape(k)[-1], tf.float32)
A:transformers.modeling_tf_openai.(_, _, nd, ns)->shape_list(w)
A:transformers.modeling_tf_openai.b->tensorflow.reshape(b, [1, 1, nd, ns])
A:transformers.modeling_tf_openai.x->self.c_attn(x)
A:transformers.modeling_tf_openai.x_shape->shape_list(x)
A:transformers.modeling_tf_openai.(query, key, value)->tensorflow.split(x, 3, axis=2)
A:transformers.modeling_tf_openai.query->self.split_heads(query)
A:transformers.modeling_tf_openai.key->self.split_heads(key)
A:transformers.modeling_tf_openai.value->self.split_heads(value)
A:transformers.modeling_tf_openai.attn_outputs->self._attn([query, key, value, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_openai.a->self.resid_dropout(a, training=training)
A:transformers.modeling_tf_openai.self.c_fc->TFConv1D(n_state, nx, initializer_range=config.initializer_range, name='c_fc')
A:transformers.modeling_tf_openai.self.dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_openai.h->self.ln_2(n + m)
A:transformers.modeling_tf_openai.h2->self.dropout(h2, training=training)
A:transformers.modeling_tf_openai.self.attn->TFAttention(nx, n_ctx, config, scale, name='attn')
A:transformers.modeling_tf_openai.self.ln_1->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_1')
A:transformers.modeling_tf_openai.self.mlp->TFMLP(4 * nx, config, name='mlp')
A:transformers.modeling_tf_openai.self.ln_2->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_2')
A:transformers.modeling_tf_openai.output_attn->self.attn([x, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_openai.n->self.ln_1(x + a)
A:transformers.modeling_tf_openai.m->self.mlp(n, training=training)
A:transformers.modeling_tf_openai.self.tokens_embed->TFSharedEmbeddings(config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name='tokens_embed')
A:transformers.modeling_tf_openai.self.positions_embed->tensorflow.keras.layers.Embedding(config.n_positions, config.n_embd, embeddings_initializer=get_initializer(config.initializer_range), name='positions_embed')
A:transformers.modeling_tf_openai.self.drop->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.modeling_tf_openai.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_openai.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_openai.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_openai.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_openai.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_openai.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_openai.inputs_embeds->self.tokens_embed(input_ids, mode='embedding')
A:transformers.modeling_tf_openai.position_embeds->self.positions_embed(position_ids)
A:transformers.modeling_tf_openai.token_type_embeds->self.tokens_embed(token_type_ids, mode='embedding')
A:transformers.modeling_tf_openai.hidden_states->tensorflow.reshape(hidden_states, input_shapes + shape_list(hidden_states)[-1:])
A:transformers.modeling_tf_openai.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_openai.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.modeling_tf_openai.self.transformer->TFOpenAIGPTMainLayer(config, name='transformer')
A:transformers.modeling_tf_openai.transformer_outputs->self.transformer(flat_inputs, training=training)
A:transformers.modeling_tf_openai.lm_logits->self.transformer.tokens_embed(hidden_states, mode='linear')
A:transformers.modeling_tf_openai.self.multiple_choice_head->TFSequenceSummary(config, initializer_range=config.initializer_range, name='multiple_choice_head')
A:transformers.modeling_tf_openai.mc_token_ids->inputs.get('mc_token_ids', mc_token_ids)
A:transformers.modeling_tf_openai.input_shapes->shape_list(input_ids)
A:transformers.modeling_tf_openai.flat_input_ids->tensorflow.reshape(input_ids, (-1, seq_length))
A:transformers.modeling_tf_openai.mc_logits->tensorflow.squeeze(mc_logits, axis=-1)
transformers.TFOpenAIGPTDoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTDoubleHeadsModel.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,training=False)
transformers.TFOpenAIGPTLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTLMHeadModel.call(self,inputs,**kwargs)
transformers.TFOpenAIGPTMainLayer(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTMainLayer._prune_heads(self,heads_to_prune)
transformers.TFOpenAIGPTMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFOpenAIGPTMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.TFOpenAIGPTModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTModel.call(self,inputs,**kwargs)
transformers.TFOpenAIGPTPreTrainedModel(TFPreTrainedModel)
transformers.load_openai_gpt_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_openai.TFAttention(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFAttention.__init__(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFAttention._attn(self,inputs,training=False)
transformers.modeling_tf_openai.TFAttention.call(self,inputs,training=False)
transformers.modeling_tf_openai.TFAttention.causal_attention_mask(nd,ns,dtype)
transformers.modeling_tf_openai.TFAttention.merge_heads(self,x)
transformers.modeling_tf_openai.TFAttention.prune_heads(self,heads)
transformers.modeling_tf_openai.TFAttention.split_heads(self,x)
transformers.modeling_tf_openai.TFBlock(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFBlock.__init__(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFBlock.call(self,inputs,training=False)
transformers.modeling_tf_openai.TFMLP(self,n_state,config,**kwargs)
transformers.modeling_tf_openai.TFMLP.__init__(self,n_state,config,**kwargs)
transformers.modeling_tf_openai.TFMLP.call(self,x,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel.call(self,inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTModel.call(self,inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_openai.gelu(x)
transformers.modeling_tf_openai.load_openai_gpt_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_openai.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_distilbert.py----------------------------------------
A:transformers.modeling_distilbert.logger->logging.getLogger(__name__)
A:transformers.modeling_distilbert.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_distilbert.out[:, 0::2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_distilbert.out[:, 1::2]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.modeling_distilbert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.dim, padding_idx=0)
A:transformers.modeling_distilbert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.dim)
A:transformers.modeling_distilbert.self.LayerNorm->torch.nn.LayerNorm(config.dim, eps=1e-12)
A:transformers.modeling_distilbert.self.dropout->torch.nn.Dropout(config.qa_dropout)
A:transformers.modeling_distilbert.seq_length->input_ids.size(1)
A:transformers.modeling_distilbert.position_ids->position_ids.unsqueeze(0).expand_as(input_ids).unsqueeze(0).expand_as(input_ids)
A:transformers.modeling_distilbert.word_embeddings->self.word_embeddings(input_ids)
A:transformers.modeling_distilbert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_distilbert.embeddings->self.dropout(embeddings)
A:transformers.modeling_distilbert.self.q_lin->prune_linear_layer(self.q_lin, index)
A:transformers.modeling_distilbert.self.k_lin->prune_linear_layer(self.k_lin, index)
A:transformers.modeling_distilbert.self.v_lin->prune_linear_layer(self.v_lin, index)
A:transformers.modeling_distilbert.self.out_lin->prune_linear_layer(self.out_lin, index, dim=1)
A:transformers.modeling_distilbert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_distilbert.mask->(mask == 0).view(mask_reshp).expand_as(scores)
A:transformers.modeling_distilbert.index->torch.arange(len(mask))[mask].long()
A:transformers.modeling_distilbert.(bs, q_length, dim)->query.size()
A:transformers.modeling_distilbert.k_length->key.size(1)
A:transformers.modeling_distilbert.q->shape(self.q_lin(query))
A:transformers.modeling_distilbert.k->shape(self.k_lin(key))
A:transformers.modeling_distilbert.v->shape(self.v_lin(value))
A:transformers.modeling_distilbert.scores->torch.matmul(q, k.transpose(2, 3))
A:transformers.modeling_distilbert.weights->self.dropout(weights)
A:transformers.modeling_distilbert.context->self.out_lin(context)
A:transformers.modeling_distilbert.self.lin1->torch.nn.Linear(in_features=config.dim, out_features=config.hidden_dim)
A:transformers.modeling_distilbert.self.lin2->torch.nn.Linear(in_features=config.hidden_dim, out_features=config.dim)
A:transformers.modeling_distilbert.x->self.dropout(x)
A:transformers.modeling_distilbert.self.attention->MultiHeadSelfAttention(config)
A:transformers.modeling_distilbert.self.sa_layer_norm->torch.nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)
A:transformers.modeling_distilbert.self.ffn->FFN(config)
A:transformers.modeling_distilbert.self.output_layer_norm->torch.nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)
A:transformers.modeling_distilbert.sa_output->self.sa_layer_norm(sa_output + x)
A:transformers.modeling_distilbert.ffn_output->self.output_layer_norm(ffn_output + sa_output)
A:transformers.modeling_distilbert.layer->TransformerBlock(config)
A:transformers.modeling_distilbert.self.layer->torch.nn.ModuleList([copy.deepcopy(layer) for _ in range(config.n_layers)])
A:transformers.modeling_distilbert.layer_outputs->layer_module(x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i])
A:transformers.modeling_distilbert.self.embeddings->Embeddings(config)
A:transformers.modeling_distilbert.self.transformer->Transformer(config)
A:transformers.modeling_distilbert.new_embeddings->self._get_resized_embeddings(old_embeddings, new_num_tokens)
A:transformers.modeling_distilbert.attention_mask->torch.ones_like(input_ids)
A:transformers.modeling_distilbert.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_distilbert.embedding_output->self.embeddings(input_ids)
A:transformers.modeling_distilbert.tfmr_output->self.transformer(x=embedding_output, attn_mask=attention_mask, head_mask=head_mask)
A:transformers.modeling_distilbert.self.distilbert->DistilBertModel(config)
A:transformers.modeling_distilbert.self.vocab_transform->torch.nn.Linear(config.dim, config.dim)
A:transformers.modeling_distilbert.self.vocab_layer_norm->torch.nn.LayerNorm(config.dim, eps=1e-12)
A:transformers.modeling_distilbert.self.vocab_projector->torch.nn.Linear(config.dim, config.vocab_size)
A:transformers.modeling_distilbert.self.mlm_loss_fct->torch.nn.CrossEntropyLoss(ignore_index=-1)
A:transformers.modeling_distilbert.dlbrt_output->self.distilbert(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask)
A:transformers.modeling_distilbert.prediction_logits->self.vocab_projector(prediction_logits)
A:transformers.modeling_distilbert.mlm_loss->self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)), masked_lm_labels.view(-1))
A:transformers.modeling_distilbert.self.pre_classifier->torch.nn.Linear(config.dim, config.dim)
A:transformers.modeling_distilbert.self.classifier->torch.nn.Linear(config.dim, config.num_labels)
A:transformers.modeling_distilbert.distilbert_output->self.distilbert(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask)
A:transformers.modeling_distilbert.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_distilbert.logits->self.qa_outputs(hidden_states)
A:transformers.modeling_distilbert.loss_fct->torch.nn.CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_distilbert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_distilbert.self.qa_outputs->torch.nn.Linear(config.dim, config.num_labels)
A:transformers.modeling_distilbert.hidden_states->self.dropout(hidden_states)
A:transformers.modeling_distilbert.(start_logits, end_logits)->self.qa_outputs(hidden_states).split(1, dim=-1)
A:transformers.modeling_distilbert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_distilbert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_distilbert.end_loss->loss_fct(end_logits, end_positions)
transformers.DistilBertForMaskedLM(self,config)
transformers.DistilBertForMaskedLM.forward(self,input_ids,attention_mask=None,head_mask=None,masked_lm_labels=None)
transformers.DistilBertForMaskedLM.tie_weights(self)
transformers.DistilBertForQuestionAnswering(self,config)
transformers.DistilBertForQuestionAnswering.forward(self,input_ids,attention_mask=None,head_mask=None,start_positions=None,end_positions=None)
transformers.DistilBertForSequenceClassification(self,config)
transformers.DistilBertForSequenceClassification.forward(self,input_ids,attention_mask=None,head_mask=None,labels=None)
transformers.DistilBertModel(self,config)
transformers.DistilBertModel._prune_heads(self,heads_to_prune)
transformers.DistilBertModel._resize_token_embeddings(self,new_num_tokens)
transformers.DistilBertModel.forward(self,input_ids,attention_mask=None,head_mask=None)
transformers.modeling_distilbert.DistilBertForMaskedLM(self,config)
transformers.modeling_distilbert.DistilBertForMaskedLM.__init__(self,config)
transformers.modeling_distilbert.DistilBertForMaskedLM.forward(self,input_ids,attention_mask=None,head_mask=None,masked_lm_labels=None)
transformers.modeling_distilbert.DistilBertForMaskedLM.tie_weights(self)
transformers.modeling_distilbert.DistilBertForQuestionAnswering(self,config)
transformers.modeling_distilbert.DistilBertForQuestionAnswering.__init__(self,config)
transformers.modeling_distilbert.DistilBertForQuestionAnswering.forward(self,input_ids,attention_mask=None,head_mask=None,start_positions=None,end_positions=None)
transformers.modeling_distilbert.DistilBertForSequenceClassification(self,config)
transformers.modeling_distilbert.DistilBertForSequenceClassification.__init__(self,config)
transformers.modeling_distilbert.DistilBertForSequenceClassification.forward(self,input_ids,attention_mask=None,head_mask=None,labels=None)
transformers.modeling_distilbert.DistilBertModel(self,config)
transformers.modeling_distilbert.DistilBertModel.__init__(self,config)
transformers.modeling_distilbert.DistilBertModel._prune_heads(self,heads_to_prune)
transformers.modeling_distilbert.DistilBertModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_distilbert.DistilBertModel.forward(self,input_ids,attention_mask=None,head_mask=None)
transformers.modeling_distilbert.DistilBertPreTrainedModel(self,*inputs,**kwargs)
transformers.modeling_distilbert.DistilBertPreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.modeling_distilbert.DistilBertPreTrainedModel._init_weights(self,module)
transformers.modeling_distilbert.Embeddings(self,config)
transformers.modeling_distilbert.Embeddings.__init__(self,config)
transformers.modeling_distilbert.Embeddings.forward(self,input_ids)
transformers.modeling_distilbert.FFN(self,config)
transformers.modeling_distilbert.FFN.__init__(self,config)
transformers.modeling_distilbert.FFN.forward(self,input)
transformers.modeling_distilbert.MultiHeadSelfAttention(self,config)
transformers.modeling_distilbert.MultiHeadSelfAttention.__init__(self,config)
transformers.modeling_distilbert.MultiHeadSelfAttention.forward(self,query,key,value,mask,head_mask=None)
transformers.modeling_distilbert.MultiHeadSelfAttention.prune_heads(self,heads)
transformers.modeling_distilbert.Transformer(self,config)
transformers.modeling_distilbert.Transformer.__init__(self,config)
transformers.modeling_distilbert.Transformer.forward(self,x,attn_mask=None,head_mask=None)
transformers.modeling_distilbert.TransformerBlock(self,config)
transformers.modeling_distilbert.TransformerBlock.__init__(self,config)
transformers.modeling_distilbert.TransformerBlock.forward(self,x,attn_mask=None,head_mask=None)
transformers.modeling_distilbert.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.modeling_distilbert.gelu(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_utils.py----------------------------------------
A:transformers.configuration_utils.logger->logging.getLogger(__name__)
A:transformers.configuration_utils.self.finetuning_task->kwargs.pop('finetuning_task', None)
A:transformers.configuration_utils.self.num_labels->kwargs.pop('num_labels', 2)
A:transformers.configuration_utils.self.output_attentions->kwargs.pop('output_attentions', False)
A:transformers.configuration_utils.self.output_hidden_states->kwargs.pop('output_hidden_states', False)
A:transformers.configuration_utils.self.torchscript->kwargs.pop('torchscript', False)
A:transformers.configuration_utils.self.use_bfloat16->kwargs.pop('use_bfloat16', False)
A:transformers.configuration_utils.self.pruned_heads->kwargs.pop('pruned_heads', {})
A:transformers.configuration_utils.output_config_file->os.path.join(save_directory, CONFIG_NAME)
A:transformers.configuration_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.configuration_utils.force_download->kwargs.pop('force_download', False)
A:transformers.configuration_utils.proxies->kwargs.pop('proxies', None)
A:transformers.configuration_utils.return_unused_kwargs->kwargs.pop('return_unused_kwargs', False)
A:transformers.configuration_utils.config_file->os.path.join(pretrained_model_name_or_path, CONFIG_NAME)
A:transformers.configuration_utils.resolved_config_file->cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
A:transformers.configuration_utils.config->cls(vocab_size_or_config_json_file=-1)
A:transformers.configuration_utils.config.pruned_heads->dict(((int(key), set(value)) for (key, value) in config.pruned_heads.items()))
A:transformers.configuration_utils.text->reader.read()
A:transformers.configuration_utils.output->copy.deepcopy(self.__dict__)
transformers.PretrainedConfig(self,**kwargs)
transformers.PretrainedConfig.__eq__(self,other)
transformers.PretrainedConfig.__repr__(self)
transformers.PretrainedConfig.from_dict(cls,json_object)
transformers.PretrainedConfig.from_json_file(cls,json_file)
transformers.PretrainedConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.PretrainedConfig.save_pretrained(self,save_directory)
transformers.PretrainedConfig.to_dict(self)
transformers.PretrainedConfig.to_json_file(self,json_file_path)
transformers.PretrainedConfig.to_json_string(self)
transformers.configuration_utils.PretrainedConfig(self,**kwargs)
transformers.configuration_utils.PretrainedConfig.__eq__(self,other)
transformers.configuration_utils.PretrainedConfig.__init__(self,**kwargs)
transformers.configuration_utils.PretrainedConfig.__repr__(self)
transformers.configuration_utils.PretrainedConfig.from_dict(cls,json_object)
transformers.configuration_utils.PretrainedConfig.from_json_file(cls,json_file)
transformers.configuration_utils.PretrainedConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.configuration_utils.PretrainedConfig.save_pretrained(self,save_directory)
transformers.configuration_utils.PretrainedConfig.to_dict(self)
transformers.configuration_utils.PretrainedConfig.to_json_file(self,json_file_path)
transformers.configuration_utils.PretrainedConfig.to_json_string(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_gpt2.py----------------------------------------
A:transformers.modeling_tf_gpt2.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_gpt2.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_gpt2.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_gpt2.self.c_attn->TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name='c_attn')
A:transformers.modeling_tf_gpt2.self.c_proj->TFConv1D(nx, n_state, initializer_range=config.initializer_range, name='c_proj')
A:transformers.modeling_tf_gpt2.self.attn_dropout->tensorflow.keras.layers.Dropout(config.attn_pdrop)
A:transformers.modeling_tf_gpt2.self.resid_dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_gpt2.self.pruned_heads->set()
A:transformers.modeling_tf_gpt2.j->tensorflow.range(ns)
A:transformers.modeling_tf_gpt2.w->self.attn_dropout(w, training=training)
A:transformers.modeling_tf_gpt2.dk->tensorflow.cast(tf.shape(k)[-1], tf.float32)
A:transformers.modeling_tf_gpt2.(_, _, nd, ns)->shape_list(w)
A:transformers.modeling_tf_gpt2.b->tensorflow.reshape(b, [1, 1, nd, ns])
A:transformers.modeling_tf_gpt2.x->self.c_attn(x)
A:transformers.modeling_tf_gpt2.x_shape->shape_list(x)
A:transformers.modeling_tf_gpt2.(query, key, value)->tensorflow.split(x, 3, axis=2)
A:transformers.modeling_tf_gpt2.query->self.split_heads(query)
A:transformers.modeling_tf_gpt2.key->tensorflow.concat([past_key, key], axis=-2)
A:transformers.modeling_tf_gpt2.value->tensorflow.concat([past_value, value], axis=-2)
A:transformers.modeling_tf_gpt2.(past_key, past_value)->tensorflow.unstack(layer_past, axis=1)
A:transformers.modeling_tf_gpt2.present->tensorflow.stack([key, value], axis=1)
A:transformers.modeling_tf_gpt2.attn_outputs->self._attn([query, key, value, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_gpt2.a->self.ln_1(x)
A:transformers.modeling_tf_gpt2.self.c_fc->TFConv1D(n_state, nx, initializer_range=config.initializer_range, name='c_fc')
A:transformers.modeling_tf_gpt2.self.dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_gpt2.h->self.act(self.c_fc(x))
A:transformers.modeling_tf_gpt2.h2->self.dropout(h2, training=training)
A:transformers.modeling_tf_gpt2.self.ln_1->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_1')
A:transformers.modeling_tf_gpt2.self.attn->TFAttention(nx, n_ctx, config, scale, name='attn')
A:transformers.modeling_tf_gpt2.self.ln_2->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_2')
A:transformers.modeling_tf_gpt2.self.mlp->TFMLP(4 * nx, config, name='mlp')
A:transformers.modeling_tf_gpt2.output_attn->self.attn([a, layer_past, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_gpt2.m->self.mlp(m, training=training)
A:transformers.modeling_tf_gpt2.self.wte->TFSharedEmbeddings(config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name='wte')
A:transformers.modeling_tf_gpt2.self.wpe->tensorflow.keras.layers.Embedding(config.n_positions, config.n_embd, embeddings_initializer=get_initializer(config.initializer_range), name='wpe')
A:transformers.modeling_tf_gpt2.self.drop->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.modeling_tf_gpt2.self.ln_f->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_f')
A:transformers.modeling_tf_gpt2.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_gpt2.past->inputs.get('past', past)
A:transformers.modeling_tf_gpt2.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_gpt2.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_gpt2.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_gpt2.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_gpt2.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_gpt2.inputs_embeds->self.wte(input_ids, mode='embedding')
A:transformers.modeling_tf_gpt2.position_embeds->self.wpe(position_ids)
A:transformers.modeling_tf_gpt2.token_type_embeds->self.wte(token_type_ids, mode='embedding')
A:transformers.modeling_tf_gpt2.hidden_states->tensorflow.reshape(hidden_states, input_shapes + shape_list(hidden_states)[-1:])
A:transformers.modeling_tf_gpt2.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_gpt2.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.modeling_tf_gpt2.self.transformer->TFGPT2MainLayer(config, name='transformer')
A:transformers.modeling_tf_gpt2.transformer_outputs->self.transformer(flat_inputs, training=training)
A:transformers.modeling_tf_gpt2.lm_logits->self.transformer.wte(hidden_states, mode='linear')
A:transformers.modeling_tf_gpt2.self.multiple_choice_head->TFSequenceSummary(config, initializer_range=config.initializer_range, name='multiple_choice_head')
A:transformers.modeling_tf_gpt2.mc_token_ids->inputs.get('mc_token_ids', mc_token_ids)
A:transformers.modeling_tf_gpt2.input_shapes->shape_list(input_ids)
A:transformers.modeling_tf_gpt2.flat_input_ids->tensorflow.reshape(input_ids, (-1, seq_length))
A:transformers.modeling_tf_gpt2.mc_logits->tensorflow.squeeze(mc_logits, axis=-1)
transformers.TFGPT2DoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.TFGPT2DoubleHeadsModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,training=False)
transformers.TFGPT2LMHeadModel(self,config,*inputs,**kwargs)
transformers.TFGPT2LMHeadModel.call(self,inputs,**kwargs)
transformers.TFGPT2MainLayer(self,config,*inputs,**kwargs)
transformers.TFGPT2MainLayer._prune_heads(self,heads_to_prune)
transformers.TFGPT2MainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFGPT2MainLayer.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.TFGPT2Model(self,config,*inputs,**kwargs)
transformers.TFGPT2Model.call(self,inputs,**kwargs)
transformers.TFGPT2PreTrainedModel(TFPreTrainedModel)
transformers.load_gpt2_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_gpt2.TFAttention(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFAttention.__init__(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFAttention._attn(self,inputs,training=False)
transformers.modeling_tf_gpt2.TFAttention.call(self,inputs,training=False)
transformers.modeling_tf_gpt2.TFAttention.causal_attention_mask(nd,ns,dtype)
transformers.modeling_tf_gpt2.TFAttention.merge_heads(self,x)
transformers.modeling_tf_gpt2.TFAttention.prune_heads(self,heads)
transformers.modeling_tf_gpt2.TFAttention.split_heads(self,x)
transformers.modeling_tf_gpt2.TFBlock(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFBlock.__init__(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFBlock.call(self,inputs,training=False)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,training=False)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel.call(self,inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2MainLayer(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2MainLayer.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2MainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_gpt2.TFGPT2MainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_gpt2.TFGPT2MainLayer.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.modeling_tf_gpt2.TFGPT2Model(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2Model.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2Model.call(self,inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2PreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_gpt2.TFMLP(self,n_state,config,**kwargs)
transformers.modeling_tf_gpt2.TFMLP.__init__(self,n_state,config,**kwargs)
transformers.modeling_tf_gpt2.TFMLP.call(self,x,training=False)
transformers.modeling_tf_gpt2.gelu(x)
transformers.modeling_tf_gpt2.load_gpt2_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_xlm_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.chkpt->torch.load(xlm_checkpoint_path, map_location='cpu')
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.config->dict(((n, v) for (n, v) in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray))))
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.vocab->dict(((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for (s, i) in vocab.items()))
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/optimization.py----------------------------------------
A:transformers.optimization.logger->logging.getLogger(__name__)
A:transformers.optimization.defaults->dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)
A:transformers.optimization.loss->closure()
A:transformers.optimization.state['exp_avg']->torch.zeros_like(p.data)
A:transformers.optimization.state['exp_avg_sq']->torch.zeros_like(p.data)
A:transformers.optimization.denom->exp_avg_sq.sqrt().add_(group['eps'])
transformers.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-06,weight_decay=0.0,correct_bias=True)
transformers.AdamW.step(self,closure=None)
transformers.ConstantLRSchedule(self,optimizer,last_epoch=-1)
transformers.WarmupConstantSchedule(self,optimizer,warmup_steps,last_epoch=-1)
transformers.WarmupConstantSchedule.lr_lambda(self,step)
transformers.WarmupCosineSchedule(self,optimizer,warmup_steps,t_total,cycles=0.5,last_epoch=-1)
transformers.WarmupCosineSchedule.lr_lambda(self,step)
transformers.WarmupCosineWithHardRestartsSchedule(self,optimizer,warmup_steps,t_total,cycles=1.0,last_epoch=-1)
transformers.WarmupCosineWithHardRestartsSchedule.lr_lambda(self,step)
transformers.WarmupLinearSchedule(self,optimizer,warmup_steps,t_total,last_epoch=-1)
transformers.WarmupLinearSchedule.lr_lambda(self,step)
transformers.optimization.AdamW(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-06,weight_decay=0.0,correct_bias=True)
transformers.optimization.AdamW.__init__(self,params,lr=0.001,betas=(0.9,0.999),eps=1e-06,weight_decay=0.0,correct_bias=True)
transformers.optimization.AdamW.step(self,closure=None)
transformers.optimization.ConstantLRSchedule(self,optimizer,last_epoch=-1)
transformers.optimization.ConstantLRSchedule.__init__(self,optimizer,last_epoch=-1)
transformers.optimization.WarmupConstantSchedule(self,optimizer,warmup_steps,last_epoch=-1)
transformers.optimization.WarmupConstantSchedule.__init__(self,optimizer,warmup_steps,last_epoch=-1)
transformers.optimization.WarmupConstantSchedule.lr_lambda(self,step)
transformers.optimization.WarmupCosineSchedule(self,optimizer,warmup_steps,t_total,cycles=0.5,last_epoch=-1)
transformers.optimization.WarmupCosineSchedule.__init__(self,optimizer,warmup_steps,t_total,cycles=0.5,last_epoch=-1)
transformers.optimization.WarmupCosineSchedule.lr_lambda(self,step)
transformers.optimization.WarmupCosineWithHardRestartsSchedule(self,optimizer,warmup_steps,t_total,cycles=1.0,last_epoch=-1)
transformers.optimization.WarmupCosineWithHardRestartsSchedule.__init__(self,optimizer,warmup_steps,t_total,cycles=1.0,last_epoch=-1)
transformers.optimization.WarmupCosineWithHardRestartsSchedule.lr_lambda(self,step)
transformers.optimization.WarmupLinearSchedule(self,optimizer,warmup_steps,t_total,last_epoch=-1)
transformers.optimization.WarmupLinearSchedule.__init__(self,optimizer,warmup_steps,t_total,last_epoch=-1)
transformers.optimization.WarmupLinearSchedule.lr_lambda(self,step)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_auto.py----------------------------------------
A:transformers.configuration_auto.logger->logging.getLogger(__name__)
transformers.AutoConfig(self)
transformers.AutoConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.configuration_auto.AutoConfig(self)
transformers.configuration_auto.AutoConfig.__init__(self)
transformers.configuration_auto.AutoConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_ctrl.py----------------------------------------
A:transformers.tokenization_ctrl.logger->logging.getLogger(__name__)
A:transformers.tokenization_ctrl.text->self.nlp(text_standardize(self.fix_text(text)))
A:transformers.tokenization_ctrl.pairs->get_pairs(word)
A:transformers.tokenization_ctrl._nlp->English()
A:transformers.tokenization_ctrl.self.nlp->BasicTokenizer(do_lower_case=True)
A:transformers.tokenization_ctrl.self.encoder->json.load(open(vocab_file, encoding='utf-8'))
A:transformers.tokenization_ctrl.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.tokenization_ctrl.word->'@@ '.join(word)
A:transformers.tokenization_ctrl.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_ctrl.j->'@@ '.join(word).index(first, i)
A:transformers.tokenization_ctrl.new_word->tuple(new_word)
A:transformers.tokenization_ctrl.out_string->' '.join(tokens).replace('@@ ', '').strip()
A:transformers.tokenization_ctrl.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_ctrl.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.CTRLTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.CTRLTokenizer._convert_id_to_token(self,index)
transformers.CTRLTokenizer._convert_token_to_id(self,token)
transformers.CTRLTokenizer._tokenize(self,text)
transformers.CTRLTokenizer.bpe(self,token)
transformers.CTRLTokenizer.convert_tokens_to_string(self,tokens)
transformers.CTRLTokenizer.save_vocabulary(self,save_directory)
transformers.CTRLTokenizer.vocab_size(self)
transformers.tokenization_ctrl.CTRLTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_ctrl.CTRLTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_ctrl.CTRLTokenizer._convert_id_to_token(self,index)
transformers.tokenization_ctrl.CTRLTokenizer._convert_token_to_id(self,token)
transformers.tokenization_ctrl.CTRLTokenizer._tokenize(self,text)
transformers.tokenization_ctrl.CTRLTokenizer.bpe(self,token)
transformers.tokenization_ctrl.CTRLTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_ctrl.CTRLTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_ctrl.CTRLTokenizer.vocab_size(self)
transformers.tokenization_ctrl.get_pairs(word)
transformers.tokenization_ctrl.text_standardize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_roberta.py----------------------------------------
A:transformers.tokenization_roberta.logger->logging.getLogger(__name__)
transformers.RobertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.RobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.RobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.RobertaTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.tokenization_roberta.RobertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.tokenization_roberta.RobertaTokenizer.__init__(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.tokenization_roberta.RobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_xlm.py----------------------------------------
A:transformers.modeling_xlm.logger->logging.getLogger(__name__)
A:transformers.modeling_xlm.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_xlm.out[:, 0::2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_xlm.out[:, 1::2]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.modeling_xlm.bs->(input_ids != self.pad_index).sum(dim=1).long().size(0)
A:transformers.modeling_xlm.alen->torch.arange(slen, dtype=torch.long, device=lengths.device)
A:transformers.modeling_xlm.NEW_ID->itertools.count()
A:transformers.modeling_xlm.self.layer_id->next(MultiHeadAttention.NEW_ID)
A:transformers.modeling_xlm.self.q_lin->prune_linear_layer(self.q_lin, index)
A:transformers.modeling_xlm.self.k_lin->prune_linear_layer(self.k_lin, index)
A:transformers.modeling_xlm.self.v_lin->prune_linear_layer(self.v_lin, index)
A:transformers.modeling_xlm.self.out_lin->prune_linear_layer(self.out_lin, index, dim=1)
A:transformers.modeling_xlm.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_xlm.mask->(mask == 0).view(mask_reshape).expand_as(scores)
A:transformers.modeling_xlm.index->torch.arange(len(mask))[mask].long()
A:transformers.modeling_xlm.(bs, qlen, dim)->input.size()
A:transformers.modeling_xlm.klen->kv.size(1)
A:transformers.modeling_xlm.q->shape(self.q_lin(input))
A:transformers.modeling_xlm.k->torch.cat([k_, k], dim=2)
A:transformers.modeling_xlm.v->torch.cat([v_, v], dim=2)
A:transformers.modeling_xlm.scores->self.proj.log_prob(x)
A:transformers.modeling_xlm.weights->torch.nn.functional.dropout(weights, p=self.dropout, training=self.training)
A:transformers.modeling_xlm.context->unshape(context)
A:transformers.modeling_xlm.self.lin1->torch.nn.Linear(in_dim, dim_hidden)
A:transformers.modeling_xlm.self.lin2->torch.nn.Linear(dim_hidden, out_dim)
A:transformers.modeling_xlm.x->torch.nn.functional.dropout(x, p=self.dropout, training=self.training)
A:transformers.modeling_xlm.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, self.dim)
A:transformers.modeling_xlm.self.lang_embeddings->torch.nn.Embedding(self.n_langs, self.dim)
A:transformers.modeling_xlm.self.embeddings->self._get_resized_embeddings(self.embeddings, new_num_tokens)
A:transformers.modeling_xlm.self.layer_norm_emb->torch.nn.LayerNorm(self.dim, eps=config.layer_norm_eps)
A:transformers.modeling_xlm.self.attentions->torch.nn.ModuleList()
A:transformers.modeling_xlm.self.layer_norm1->torch.nn.ModuleList()
A:transformers.modeling_xlm.self.ffns->torch.nn.ModuleList()
A:transformers.modeling_xlm.self.layer_norm2->torch.nn.ModuleList()
A:transformers.modeling_xlm.pruned_heads->config.pruned_heads.copy().items()
A:transformers.modeling_xlm.lengths->(input_ids != self.pad_index).sum(dim=1).long()
A:transformers.modeling_xlm.(bs, slen)->input_ids.size()
A:transformers.modeling_xlm.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.modeling_xlm.position_ids->torch.arange(slen, out=position_ids).unsqueeze(0)
A:transformers.modeling_xlm.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_xlm.tensor->self.layer_norm2[i](tensor)
A:transformers.modeling_xlm.attn_outputs->self.attentions[i](tensor, attn_mask, cache=cache, head_mask=head_mask[i])
A:transformers.modeling_xlm.attn->torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)
A:transformers.modeling_xlm.self.proj->torch.nn.AdaptiveLogSoftmaxWithLoss(in_features=dim, n_classes=config.n_words, cutoffs=config.asm_cutoffs, div_value=config.asm_div_value, head_bias=True)
A:transformers.modeling_xlm.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_xlm.(_, loss)->self.proj(x, y)
A:transformers.modeling_xlm.self.transformer->XLMModel(config)
A:transformers.modeling_xlm.self.pred_layer->XLMPredLayer(config)
A:transformers.modeling_xlm.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask)
A:transformers.modeling_xlm.outputs->self.qa_outputs(output, start_positions=start_positions, end_positions=end_positions, cls_index=cls_index, is_impossible=is_impossible, p_mask=p_mask)
A:transformers.modeling_xlm.self.sequence_summary->SequenceSummary(config)
A:transformers.modeling_xlm.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_xlm.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_xlm.self.qa_outputs->SQuADHead(config)
A:transformers.modeling_xlm.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_xlm.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_xlm.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_xlm.end_loss->loss_fct(end_logits, end_positions)
transformers.XLMForQuestionAnswering(self,config)
transformers.XLMForQuestionAnswering.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None)
transformers.XLMForQuestionAnsweringSimple(self,config)
transformers.XLMForQuestionAnsweringSimple.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,start_positions=None,end_positions=None)
transformers.XLMForSequenceClassification(self,config)
transformers.XLMForSequenceClassification.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,labels=None)
transformers.XLMModel(self,config)
transformers.XLMModel._prune_heads(self,heads_to_prune)
transformers.XLMModel._resize_token_embeddings(self,new_num_tokens)
transformers.XLMModel.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None)
transformers.XLMPreTrainedModel(self,*inputs,**kwargs)
transformers.XLMPreTrainedModel._init_weights(self,module)
transformers.XLMWithLMHeadModel(self,config)
transformers.XLMWithLMHeadModel.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,labels=None)
transformers.XLMWithLMHeadModel.tie_weights(self)
transformers.modeling_xlm.MultiHeadAttention(self,n_heads,dim,config)
transformers.modeling_xlm.MultiHeadAttention.__init__(self,n_heads,dim,config)
transformers.modeling_xlm.MultiHeadAttention.forward(self,input,mask,kv=None,cache=None,head_mask=None)
transformers.modeling_xlm.MultiHeadAttention.prune_heads(self,heads)
transformers.modeling_xlm.TransformerFFN(self,in_dim,dim_hidden,out_dim,config)
transformers.modeling_xlm.TransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config)
transformers.modeling_xlm.TransformerFFN.forward(self,input)
transformers.modeling_xlm.XLMForQuestionAnswering(self,config)
transformers.modeling_xlm.XLMForQuestionAnswering.__init__(self,config)
transformers.modeling_xlm.XLMForQuestionAnswering.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None)
transformers.modeling_xlm.XLMForQuestionAnsweringSimple(self,config)
transformers.modeling_xlm.XLMForQuestionAnsweringSimple.__init__(self,config)
transformers.modeling_xlm.XLMForQuestionAnsweringSimple.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,start_positions=None,end_positions=None)
transformers.modeling_xlm.XLMForSequenceClassification(self,config)
transformers.modeling_xlm.XLMForSequenceClassification.__init__(self,config)
transformers.modeling_xlm.XLMForSequenceClassification.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,labels=None)
transformers.modeling_xlm.XLMModel(self,config)
transformers.modeling_xlm.XLMModel.__init__(self,config)
transformers.modeling_xlm.XLMModel._prune_heads(self,heads_to_prune)
transformers.modeling_xlm.XLMModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_xlm.XLMModel.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None)
transformers.modeling_xlm.XLMPreTrainedModel(self,*inputs,**kwargs)
transformers.modeling_xlm.XLMPreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.modeling_xlm.XLMPreTrainedModel._init_weights(self,module)
transformers.modeling_xlm.XLMPredLayer(self,config)
transformers.modeling_xlm.XLMPredLayer.__init__(self,config)
transformers.modeling_xlm.XLMPredLayer.forward(self,x,y=None)
transformers.modeling_xlm.XLMWithLMHeadModel(self,config)
transformers.modeling_xlm.XLMWithLMHeadModel.__init__(self,config)
transformers.modeling_xlm.XLMWithLMHeadModel.forward(self,input_ids,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,labels=None)
transformers.modeling_xlm.XLMWithLMHeadModel.tie_weights(self)
transformers.modeling_xlm.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.modeling_xlm.gelu(x)
transformers.modeling_xlm.get_masks(slen,lengths,causal,padding_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_bert.py----------------------------------------
A:transformers.modeling_bert.logger->logging.getLogger(__name__)
A:transformers.modeling_bert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_bert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_bert.array->numpy.transpose(array)
A:transformers.modeling_bert.name->name.split('/').split('/')
A:transformers.modeling_bert.l->re.split('_(\\d+)', m_name)
A:transformers.modeling_bert.pointer->getattr(pointer, 'weight')
A:transformers.modeling_bert.num->int(l[1])
A:transformers.modeling_bert.pointer.data->torch.from_numpy(array)
A:transformers.modeling_bert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
A:transformers.modeling_bert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.modeling_bert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.modeling_bert.self.LayerNorm->BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_bert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_bert.seq_length->input_ids.view(-1, input_ids.size(-1)).size(1)
A:transformers.modeling_bert.position_ids->position_ids.unsqueeze(0).expand_as(input_ids).unsqueeze(0).expand_as(input_ids)
A:transformers.modeling_bert.token_type_ids->torch.zeros_like(input_ids)
A:transformers.modeling_bert.words_embeddings->self.word_embeddings(input_ids)
A:transformers.modeling_bert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_bert.embeddings->self.dropout(embeddings)
A:transformers.modeling_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_bert.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.modeling_bert.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.modeling_bert.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.modeling_bert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.modeling_bert.mixed_query_layer->self.query(hidden_states)
A:transformers.modeling_bert.mixed_key_layer->self.key(hidden_states)
A:transformers.modeling_bert.mixed_value_layer->self.value(hidden_states)
A:transformers.modeling_bert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.modeling_bert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.modeling_bert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.modeling_bert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.modeling_bert.attention_probs->self.dropout(attention_probs)
A:transformers.modeling_bert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.modeling_bert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_bert.hidden_states->self.transform(hidden_states)
A:transformers.modeling_bert.self.self->BertSelfAttention(config)
A:transformers.modeling_bert.self.output->BertOutput(config)
A:transformers.modeling_bert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_bert.mask->mask.view(-1).contiguous().eq(1).view(-1).contiguous().eq(1)
A:transformers.modeling_bert.index->torch.arange(len(mask))[mask].long()
A:transformers.modeling_bert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.modeling_bert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.modeling_bert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.modeling_bert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.modeling_bert.self_outputs->self.self(input_tensor, attention_mask, head_mask)
A:transformers.modeling_bert.attention_output->self.output(self_outputs[0], input_tensor)
A:transformers.modeling_bert.self.attention->BertAttention(config)
A:transformers.modeling_bert.self.intermediate->BertIntermediate(config)
A:transformers.modeling_bert.attention_outputs->self.attention(hidden_states, attention_mask, head_mask)
A:transformers.modeling_bert.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_bert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.modeling_bert.self.layer->torch.nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.modeling_bert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i])
A:transformers.modeling_bert.self.activation->torch.nn.Tanh()
A:transformers.modeling_bert.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_bert.self.transform->BertPredictionHeadTransform(config)
A:transformers.modeling_bert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.modeling_bert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_bert.self.predictions->BertLMPredictionHead(config)
A:transformers.modeling_bert.prediction_scores->self.cls(sequence_output)
A:transformers.modeling_bert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.modeling_bert.seq_relationship_score->self.cls(pooled_output)
A:transformers.modeling_bert.self.embeddings->BertEmbeddings(config)
A:transformers.modeling_bert.self.encoder->BertEncoder(config)
A:transformers.modeling_bert.self.pooler->BertPooler(config)
A:transformers.modeling_bert.new_embeddings->self._get_resized_embeddings(old_embeddings, new_num_tokens)
A:transformers.modeling_bert.attention_mask->torch.ones_like(input_ids)
A:transformers.modeling_bert.extended_attention_mask->extended_attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_bert.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_bert.embedding_output->self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)
A:transformers.modeling_bert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask)
A:transformers.modeling_bert.self.bert->BertModel(config)
A:transformers.modeling_bert.self.cls->BertOnlyNSPHead(config)
A:transformers.modeling_bert.outputs->self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)
A:transformers.modeling_bert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.modeling_bert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_bert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))
A:transformers.modeling_bert.next_sentence_loss->loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
A:transformers.modeling_bert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_bert.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_bert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_bert.input_ids->input_ids.view(-1, input_ids.size(-1)).view(-1, input_ids.size(-1))
A:transformers.modeling_bert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.modeling_bert.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_bert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_bert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_bert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_bert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_bert.end_loss->loss_fct(end_logits, end_positions)
transformers.BertForMaskedLM(self,config)
transformers.BertForMaskedLM.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,masked_lm_labels=None)
transformers.BertForMaskedLM.tie_weights(self)
transformers.BertForMultipleChoice(self,config)
transformers.BertForMultipleChoice.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.BertForNextSentencePrediction(self,config)
transformers.BertForNextSentencePrediction.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,next_sentence_label=None)
transformers.BertForPreTraining(self,config)
transformers.BertForPreTraining.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,masked_lm_labels=None,next_sentence_label=None)
transformers.BertForPreTraining.tie_weights(self)
transformers.BertForQuestionAnswering(self,config)
transformers.BertForQuestionAnswering.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,start_positions=None,end_positions=None)
transformers.BertForSequenceClassification(self,config)
transformers.BertForSequenceClassification.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.BertForTokenClassification(self,config)
transformers.BertForTokenClassification.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.BertModel(self,config)
transformers.BertModel._prune_heads(self,heads_to_prune)
transformers.BertModel._resize_token_embeddings(self,new_num_tokens)
transformers.BertModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.BertPreTrainedModel(PreTrainedModel)
transformers.BertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_bert(model,config,tf_checkpoint_path)
transformers.modeling_bert.BertAttention(self,config)
transformers.modeling_bert.BertAttention.__init__(self,config)
transformers.modeling_bert.BertAttention.forward(self,input_tensor,attention_mask=None,head_mask=None)
transformers.modeling_bert.BertAttention.prune_heads(self,heads)
transformers.modeling_bert.BertEmbeddings(self,config)
transformers.modeling_bert.BertEmbeddings.__init__(self,config)
transformers.modeling_bert.BertEmbeddings.forward(self,input_ids,token_type_ids=None,position_ids=None)
transformers.modeling_bert.BertEncoder(self,config)
transformers.modeling_bert.BertEncoder.__init__(self,config)
transformers.modeling_bert.BertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None)
transformers.modeling_bert.BertForMaskedLM(self,config)
transformers.modeling_bert.BertForMaskedLM.__init__(self,config)
transformers.modeling_bert.BertForMaskedLM.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,masked_lm_labels=None)
transformers.modeling_bert.BertForMaskedLM.tie_weights(self)
transformers.modeling_bert.BertForMultipleChoice(self,config)
transformers.modeling_bert.BertForMultipleChoice.__init__(self,config)
transformers.modeling_bert.BertForMultipleChoice.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_bert.BertForNextSentencePrediction(self,config)
transformers.modeling_bert.BertForNextSentencePrediction.__init__(self,config)
transformers.modeling_bert.BertForNextSentencePrediction.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,next_sentence_label=None)
transformers.modeling_bert.BertForPreTraining(self,config)
transformers.modeling_bert.BertForPreTraining.__init__(self,config)
transformers.modeling_bert.BertForPreTraining.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,masked_lm_labels=None,next_sentence_label=None)
transformers.modeling_bert.BertForPreTraining.tie_weights(self)
transformers.modeling_bert.BertForQuestionAnswering(self,config)
transformers.modeling_bert.BertForQuestionAnswering.__init__(self,config)
transformers.modeling_bert.BertForQuestionAnswering.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,start_positions=None,end_positions=None)
transformers.modeling_bert.BertForSequenceClassification(self,config)
transformers.modeling_bert.BertForSequenceClassification.__init__(self,config)
transformers.modeling_bert.BertForSequenceClassification.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_bert.BertForTokenClassification(self,config)
transformers.modeling_bert.BertForTokenClassification.__init__(self,config)
transformers.modeling_bert.BertForTokenClassification.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_bert.BertIntermediate(self,config)
transformers.modeling_bert.BertIntermediate.__init__(self,config)
transformers.modeling_bert.BertIntermediate.forward(self,hidden_states)
transformers.modeling_bert.BertLMPredictionHead(self,config)
transformers.modeling_bert.BertLMPredictionHead.__init__(self,config)
transformers.modeling_bert.BertLMPredictionHead.forward(self,hidden_states)
transformers.modeling_bert.BertLayer(self,config)
transformers.modeling_bert.BertLayer.__init__(self,config)
transformers.modeling_bert.BertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None)
transformers.modeling_bert.BertModel(self,config)
transformers.modeling_bert.BertModel.__init__(self,config)
transformers.modeling_bert.BertModel._prune_heads(self,heads_to_prune)
transformers.modeling_bert.BertModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_bert.BertModel.forward(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.modeling_bert.BertOnlyMLMHead(self,config)
transformers.modeling_bert.BertOnlyMLMHead.__init__(self,config)
transformers.modeling_bert.BertOnlyMLMHead.forward(self,sequence_output)
transformers.modeling_bert.BertOnlyNSPHead(self,config)
transformers.modeling_bert.BertOnlyNSPHead.__init__(self,config)
transformers.modeling_bert.BertOnlyNSPHead.forward(self,pooled_output)
transformers.modeling_bert.BertOutput(self,config)
transformers.modeling_bert.BertOutput.__init__(self,config)
transformers.modeling_bert.BertOutput.forward(self,hidden_states,input_tensor)
transformers.modeling_bert.BertPooler(self,config)
transformers.modeling_bert.BertPooler.__init__(self,config)
transformers.modeling_bert.BertPooler.forward(self,hidden_states)
transformers.modeling_bert.BertPreTrainedModel(PreTrainedModel)
transformers.modeling_bert.BertPreTrainedModel._init_weights(self,module)
transformers.modeling_bert.BertPreTrainingHeads(self,config)
transformers.modeling_bert.BertPreTrainingHeads.__init__(self,config)
transformers.modeling_bert.BertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.modeling_bert.BertPredictionHeadTransform(self,config)
transformers.modeling_bert.BertPredictionHeadTransform.__init__(self,config)
transformers.modeling_bert.BertPredictionHeadTransform.forward(self,hidden_states)
transformers.modeling_bert.BertSelfAttention(self,config)
transformers.modeling_bert.BertSelfAttention.__init__(self,config)
transformers.modeling_bert.BertSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None)
transformers.modeling_bert.BertSelfAttention.transpose_for_scores(self,x)
transformers.modeling_bert.BertSelfOutput(self,config)
transformers.modeling_bert.BertSelfOutput.__init__(self,config)
transformers.modeling_bert.BertSelfOutput.forward(self,hidden_states,input_tensor)
transformers.modeling_bert.gelu(x)
transformers.modeling_bert.gelu_new(x)
transformers.modeling_bert.load_tf_weights_in_bert(model,config,tf_checkpoint_path)
transformers.modeling_bert.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_roberta.py----------------------------------------
A:transformers.configuration_roberta.logger->logging.getLogger(__name__)
transformers.RobertaConfig(BertConfig)
transformers.configuration_roberta.RobertaConfig(BertConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/__main__.py----------------------------------------
A:transformers.__main__.PYTORCH_DUMP_OUTPUT->sys.argv.pop()
A:transformers.__main__.TF_CONFIG->sys.argv.pop()
A:transformers.__main__.TF_CHECKPOINT->sys.argv.pop()
transformers.__main__.main()


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/configuration_distilbert.py----------------------------------------
A:transformers.configuration_distilbert.logger->logging.getLogger(__name__)
A:transformers.configuration_distilbert.json_config->json.loads(reader.read())
transformers.DistilBertConfig(self,vocab_size_or_config_json_file=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,tie_weights_=True,qa_dropout=0.1,seq_classif_dropout=0.2,**kwargs)
transformers.DistilBertConfig.hidden_size(self)
transformers.DistilBertConfig.num_attention_heads(self)
transformers.DistilBertConfig.num_hidden_layers(self)
transformers.configuration_distilbert.DistilBertConfig(self,vocab_size_or_config_json_file=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,tie_weights_=True,qa_dropout=0.1,seq_classif_dropout=0.2,**kwargs)
transformers.configuration_distilbert.DistilBertConfig.__init__(self,vocab_size_or_config_json_file=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,tie_weights_=True,qa_dropout=0.1,seq_classif_dropout=0.2,**kwargs)
transformers.configuration_distilbert.DistilBertConfig.hidden_size(self)
transformers.configuration_distilbert.DistilBertConfig.num_attention_heads(self)
transformers.configuration_distilbert.DistilBertConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_distilbert.py----------------------------------------
A:transformers.tokenization_distilbert.logger->logging.getLogger(__name__)
transformers.DistilBertTokenizer(BertTokenizer)
transformers.tokenization_distilbert.DistilBertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_xlnet.py----------------------------------------
A:transformers.tokenization_xlnet.logger->logging.getLogger(__name__)
A:transformers.tokenization_xlnet.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_xlnet.state->self.__dict__.copy()
A:transformers.tokenization_xlnet.outputs->outputs.lower().lower()
A:transformers.tokenization_xlnet.text->text.encode('utf-8').encode('utf-8')
A:transformers.tokenization_xlnet.pieces->self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)
A:transformers.tokenization_xlnet.cur_pieces->self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, ''))
A:transformers.tokenization_xlnet.piece->piece.decode('utf-8').decode('utf-8')
A:transformers.tokenization_xlnet.token->token.decode('utf-8').decode('utf-8')
A:transformers.tokenization_xlnet.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.tokenization_xlnet.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
transformers.XLNetTokenizer(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.XLNetTokenizer.__getstate__(self)
transformers.XLNetTokenizer.__setstate__(self,d)
transformers.XLNetTokenizer._convert_id_to_token(self,index,return_unicode=True)
transformers.XLNetTokenizer._convert_token_to_id(self,token)
transformers.XLNetTokenizer._tokenize(self,text,return_unicode=True,sample=False)
transformers.XLNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.XLNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.XLNetTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.XLNetTokenizer.preprocess_text(self,inputs)
transformers.XLNetTokenizer.save_vocabulary(self,save_directory)
transformers.XLNetTokenizer.vocab_size(self)
transformers.tokenization_xlnet.XLNetTokenizer(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.tokenization_xlnet.XLNetTokenizer.__getstate__(self)
transformers.tokenization_xlnet.XLNetTokenizer.__init__(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.tokenization_xlnet.XLNetTokenizer.__setstate__(self,d)
transformers.tokenization_xlnet.XLNetTokenizer._convert_id_to_token(self,index,return_unicode=True)
transformers.tokenization_xlnet.XLNetTokenizer._convert_token_to_id(self,token)
transformers.tokenization_xlnet.XLNetTokenizer._tokenize(self,text,return_unicode=True,sample=False)
transformers.tokenization_xlnet.XLNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_xlnet.XLNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_xlnet.XLNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.tokenization_xlnet.XLNetTokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.tokenization_xlnet.XLNetTokenizer.preprocess_text(self,inputs)
transformers.tokenization_xlnet.XLNetTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_xlnet.XLNetTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/__init__.py----------------------------------------
A:transformers.__init__.logger->logging.getLogger(__name__)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_pytorch_checkpoint_to_tf2.py----------------------------------------
A:transformers.convert_pytorch_checkpoint_to_tf2.config_file->cached_path(config_shortcut_name, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.config->config_class.from_json_file(config_file)
A:transformers.convert_pytorch_checkpoint_to_tf2.tf_model->loading_fct(tf_model, pytorch_checkpoint_path)
A:transformers.convert_pytorch_checkpoint_to_tf2.pytorch_checkpoint_path->cached_path(aws_model_maps[pytorch_checkpoint_path], force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.convert_pytorch_checkpoint_to_tf2.tfo->tf_model(tf_inputs, training=False)
A:transformers.convert_pytorch_checkpoint_to_tf2.pt_model->pt_model_class.from_pretrained(None, config=config, state_dict=torch.load(pytorch_checkpoint_path, map_location='cpu'))
A:transformers.convert_pytorch_checkpoint_to_tf2.pt_inputs->torch.tensor(inputs_list)
A:transformers.convert_pytorch_checkpoint_to_tf2.pto->pt_model(pt_inputs)
A:transformers.convert_pytorch_checkpoint_to_tf2.np_pt->pto[0].detach().numpy()
A:transformers.convert_pytorch_checkpoint_to_tf2.np_tf->tfo[0].numpy()
A:transformers.convert_pytorch_checkpoint_to_tf2.diff->numpy.amax(np.abs(np_pt - np_tf))
A:transformers.convert_pytorch_checkpoint_to_tf2.model_types->list(MODEL_CLASSES.keys())
A:transformers.convert_pytorch_checkpoint_to_tf2.model_shortcut_names_or_path->list(aws_model_maps.keys())
A:transformers.convert_pytorch_checkpoint_to_tf2.model_file->cached_path(model_shortcut_name, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.parser->argparse.ArgumentParser()
A:transformers.convert_pytorch_checkpoint_to_tf2.args->argparse.ArgumentParser().parse_args()
transformers.convert_pytorch_checkpoint_to_tf2.convert_all_pt_checkpoints_to_tf(args_model_type,tf_dump_path,model_shortcut_names_or_path=None,config_shortcut_names_or_path=None,compare_with_pt_model=False,use_cached_models=False,only_convert_finetuned_models=False)
transformers.convert_pytorch_checkpoint_to_tf2.convert_pt_checkpoint_to_tf(model_type,pytorch_checkpoint_path,config_file,tf_dump_path,compare_with_pt_model=False,use_cached_models=True)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_bert.py----------------------------------------
A:transformers.modeling_tf_bert.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_bert.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_bert.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_bert.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.hidden_size, embeddings_initializer=get_initializer(self.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_bert.self.token_type_embeddings->tensorflow.keras.layers.Embedding(config.type_vocab_size, config.hidden_size, embeddings_initializer=get_initializer(self.initializer_range), name='token_type_embeddings')
A:transformers.modeling_tf_bert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.modeling_tf_bert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_bert.self.word_embeddings->self.add_weight('weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_bert.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_bert.words_embeddings->tensorflow.gather(self.word_embeddings, input_ids)
A:transformers.modeling_tf_bert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_tf_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_tf_bert.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_bert.x->tensorflow.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.modeling_tf_bert.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_tf_bert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.modeling_tf_bert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.modeling_tf_bert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.modeling_tf_bert.mixed_query_layer->self.query(hidden_states)
A:transformers.modeling_tf_bert.mixed_key_layer->self.key(hidden_states)
A:transformers.modeling_tf_bert.mixed_value_layer->self.value(hidden_states)
A:transformers.modeling_tf_bert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.modeling_tf_bert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.modeling_tf_bert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.modeling_tf_bert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.modeling_tf_bert.dk->tensorflow.cast(tf.shape(key_layer)[-1], tf.float32)
A:transformers.modeling_tf_bert.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.modeling_tf_bert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.all_head_size))
A:transformers.modeling_tf_bert.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.modeling_tf_bert.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_bert.self.self_attention->TFBertSelfAttention(config, name='self')
A:transformers.modeling_tf_bert.self.dense_output->TFBertSelfOutput(config, name='output')
A:transformers.modeling_tf_bert.self_outputs->self.self_attention([input_tensor, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_bert.attention_output->self.dense_output([self_outputs[0], input_tensor], training=training)
A:transformers.modeling_tf_bert.self.attention->TFBertAttention(config, name='attention')
A:transformers.modeling_tf_bert.self.intermediate->TFBertIntermediate(config, name='intermediate')
A:transformers.modeling_tf_bert.self.bert_output->TFBertOutput(config, name='output')
A:transformers.modeling_tf_bert.attention_outputs->self.attention([hidden_states, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_bert.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_tf_bert.layer_output->self.bert_output([intermediate_output, attention_output], training=training)
A:transformers.modeling_tf_bert.layer_outputs->layer_module([hidden_states, attention_mask, head_mask[i]], training=training)
A:transformers.modeling_tf_bert.pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_bert.self.transform->TFBertPredictionHeadTransform(config, name='transform')
A:transformers.modeling_tf_bert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_bert.self.predictions->TFBertLMPredictionHead(config, input_embeddings, name='predictions')
A:transformers.modeling_tf_bert.prediction_scores->self.mlm(sequence_output, training=kwargs.get('training', False))
A:transformers.modeling_tf_bert.self.seq_relationship->tensorflow.keras.layers.Dense(2, kernel_initializer=get_initializer(config.initializer_range), name='seq_relationship')
A:transformers.modeling_tf_bert.seq_relationship_score->self.nsp(pooled_output)
A:transformers.modeling_tf_bert.self.embeddings->TFBertEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_bert.self.encoder->TFBertEncoder(config, name='encoder')
A:transformers.modeling_tf_bert.self.pooler->TFBertPooler(config, name='pooler')
A:transformers.modeling_tf_bert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_bert.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_bert.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_bert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_bert.extended_attention_mask->tensorflow.cast(extended_attention_mask, tf.float32)
A:transformers.modeling_tf_bert.embedding_output->self.embeddings([input_ids, position_ids, token_type_ids], training=training)
A:transformers.modeling_tf_bert.encoder_outputs->self.encoder([embedding_output, extended_attention_mask, head_mask], training=training)
A:transformers.modeling_tf_bert.self.bert->TFBertMainLayer(config, name='bert')
A:transformers.modeling_tf_bert.outputs->self.bert(inputs, **kwargs)
A:transformers.modeling_tf_bert.self.nsp->TFBertNSPHead(config, name='nsp___cls')
A:transformers.modeling_tf_bert.self.mlm->TFBertMLMHead(config, self.bert.embeddings, name='mlm___cls')
A:transformers.modeling_tf_bert.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_bert.flat_input_ids->tensorflow.reshape(input_ids, (-1, seq_length))
A:transformers.modeling_tf_bert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_bert.sequence_output->self.dropout(sequence_output, training=kwargs.get('training', False))
A:transformers.modeling_tf_bert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_bert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_bert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_bert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFBertEmbeddings(self,config,**kwargs)
transformers.TFBertEmbeddings._embedding(self,inputs,training=False)
transformers.TFBertEmbeddings._linear(self,inputs)
transformers.TFBertEmbeddings.build(self,input_shape)
transformers.TFBertEmbeddings.call(self,inputs,mode='embedding',training=False)
transformers.TFBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFBertForMaskedLM.call(self,inputs,**kwargs)
transformers.TFBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFBertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.TFBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.TFBertForNextSentencePrediction.call(self,inputs,**kwargs)
transformers.TFBertForPreTraining(self,config,*inputs,**kwargs)
transformers.TFBertForPreTraining.call(self,inputs,**kwargs)
transformers.TFBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFBertForQuestionAnswering.call(self,inputs,**kwargs)
transformers.TFBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFBertForSequenceClassification.call(self,inputs,**kwargs)
transformers.TFBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFBertForTokenClassification.call(self,inputs,**kwargs)
transformers.TFBertMainLayer(self,config,**kwargs)
transformers.TFBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFBertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFBertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.TFBertModel(self,config,*inputs,**kwargs)
transformers.TFBertModel.call(self,inputs,**kwargs)
transformers.TFBertPreTrainedModel(TFPreTrainedModel)
transformers.load_bert_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_bert.TFBertAttention(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertAttention.call(self,inputs,training=False)
transformers.modeling_tf_bert.TFBertAttention.prune_heads(self,heads)
transformers.modeling_tf_bert.TFBertEmbeddings(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEmbeddings._embedding(self,inputs,training=False)
transformers.modeling_tf_bert.TFBertEmbeddings._linear(self,inputs)
transformers.modeling_tf_bert.TFBertEmbeddings.build(self,input_shape)
transformers.modeling_tf_bert.TFBertEmbeddings.call(self,inputs,mode='embedding',training=False)
transformers.modeling_tf_bert.TFBertEncoder(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEncoder.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEncoder.call(self,inputs,training=False)
transformers.modeling_tf_bert.TFBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMaskedLM.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.modeling_tf_bert.TFBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForNextSentencePrediction.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForNextSentencePrediction.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForQuestionAnswering.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForSequenceClassification.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForTokenClassification.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertIntermediate(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertIntermediate.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertIntermediate.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertLMPredictionHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertLMPredictionHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertLMPredictionHead.build(self,input_shape)
transformers.modeling_tf_bert.TFBertLMPredictionHead.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertLayer(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertLayer.call(self,inputs,training=False)
transformers.modeling_tf_bert.TFBertMLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertMLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertMLMHead.call(self,sequence_output)
transformers.modeling_tf_bert.TFBertMainLayer(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_bert.TFBertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_bert.TFBertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.modeling_tf_bert.TFBertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertModel.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertNSPHead(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertNSPHead.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertNSPHead.call(self,pooled_output)
transformers.modeling_tf_bert.TFBertOutput(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertOutput.call(self,inputs,training=False)
transformers.modeling_tf_bert.TFBertPooler(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPooler.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPooler.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_bert.TFBertPredictionHeadTransform(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPredictionHeadTransform.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPredictionHeadTransform.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertSelfAttention(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfAttention.call(self,inputs,training=False)
transformers.modeling_tf_bert.TFBertSelfAttention.transpose_for_scores(self,x,batch_size)
transformers.modeling_tf_bert.TFBertSelfOutput(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfOutput.call(self,inputs,training=False)
transformers.modeling_tf_bert.gelu(x)
transformers.modeling_tf_bert.gelu_new(x)
transformers.modeling_tf_bert.load_bert_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_bert.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_transfo_xl.py----------------------------------------
A:transformers.modeling_transfo_xl.logger->logging.getLogger(__name__)
A:transformers.modeling_transfo_xl.tf_to_pt_map->build_tf_to_pytorch_map(model, config)
A:transformers.modeling_transfo_xl.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_transfo_xl.array->numpy.transpose(array)
A:transformers.modeling_transfo_xl.p_i.data->torch.from_numpy(arr_i)
A:transformers.modeling_transfo_xl.pointer.data->torch.from_numpy(array)
A:transformers.modeling_transfo_xl.sinusoid_inp->torch.ger(pos_seq, self.inv_freq)
A:transformers.modeling_transfo_xl.pos_emb->self.drop(pos_emb)
A:transformers.modeling_transfo_xl.self.CoreNet->torch.nn.Sequential(nn.Linear(d_model, d_inner), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(d_inner, d_model), nn.Dropout(dropout))
A:transformers.modeling_transfo_xl.self.layer_norm->torch.nn.LayerNorm(d_model, eps=layer_norm_epsilon)
A:transformers.modeling_transfo_xl.core_out->self.drop(core_out)
A:transformers.modeling_transfo_xl.output->self.layer_norm(inp + core_out)
A:transformers.modeling_transfo_xl.self.qkv_net->torch.nn.Linear(d_model, 3 * n_head * d_head, bias=False)
A:transformers.modeling_transfo_xl.self.drop->torch.nn.Dropout(config.dropout)
A:transformers.modeling_transfo_xl.self.dropatt->torch.nn.Dropout(dropatt)
A:transformers.modeling_transfo_xl.self.o_net->torch.nn.Linear(n_head * d_head, d_model, bias=False)
A:transformers.modeling_transfo_xl.self.r_r_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_transfo_xl.self.r_w_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_transfo_xl.self.r_net->torch.nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)
A:transformers.modeling_transfo_xl.zero_pad->torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)
A:transformers.modeling_transfo_xl.x_padded->x_padded.view(*x_padded_shape).view(*x_padded_shape)
A:transformers.modeling_transfo_xl.x->x_padded[1:].view_as(x)
A:transformers.modeling_transfo_xl.cat->torch.cat([mems[i], hids[i]], dim=0)
A:transformers.modeling_transfo_xl.w_heads->self.qkv_net(w)
A:transformers.modeling_transfo_xl.r_head_k->r_head_k.view(rlen, self.n_head, self.d_head).view(rlen, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.(w_head_q, w_head_k, w_head_v)->torch.chunk(w_heads, 3, dim=-1)
A:transformers.modeling_transfo_xl.klen->w_head_k.view(klen, bsz, self.n_head, self.d_head).size(0)
A:transformers.modeling_transfo_xl.w_head_q->w_head_q.view(qlen, bsz, self.n_head, self.d_head).view(qlen, bsz, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.w_head_k->w_head_k.view(klen, bsz, self.n_head, self.d_head).view(klen, bsz, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.w_head_v->w_head_v.view(klen, bsz, self.n_head, self.d_head).view(klen, bsz, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.AC->torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))
A:transformers.modeling_transfo_xl.BD->self._rel_shift(BD)
A:transformers.modeling_transfo_xl.attn_score->attn_score.float().masked_fill(attn_mask[:, :, :, None], -1e+30).type_as(attn_score).float().masked_fill(attn_mask[:, :, :, None], -1e+30).type_as(attn_score)
A:transformers.modeling_transfo_xl.attn_prob->self.dropatt(attn_prob)
A:transformers.modeling_transfo_xl.attn_vec->attn_vec.contiguous().view(attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head).contiguous().view(attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)
A:transformers.modeling_transfo_xl.attn_out->self.drop(attn_out)
A:transformers.modeling_transfo_xl.self.dec_attn->RelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, layer_norm_epsilon=layer_norm_epsilon, **kwargs)
A:transformers.modeling_transfo_xl.self.pos_ff->PositionwiseFF(d_model, d_inner, dropout, pre_lnorm=kwargs.get('pre_lnorm'), layer_norm_epsilon=layer_norm_epsilon)
A:transformers.modeling_transfo_xl.attn_outputs->self.dec_attn(dec_inp, r, attn_mask=dec_attn_mask, mems=mems, head_mask=head_mask)
A:transformers.modeling_transfo_xl.ff_output->self.pos_ff(attn_outputs[0])
A:transformers.modeling_transfo_xl.self.emb_layers->torch.nn.ModuleList()
A:transformers.modeling_transfo_xl.self.emb_projs->torch.nn.ParameterList()
A:transformers.modeling_transfo_xl.embed->torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device).view(embed_shape)
A:transformers.modeling_transfo_xl.param->next(self.parameters())
A:transformers.modeling_transfo_xl.inp_flat->inp.view(-1)
A:transformers.modeling_transfo_xl.emb_flat->torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device)
A:transformers.modeling_transfo_xl.indices_i->mask_i.nonzero().squeeze()
A:transformers.modeling_transfo_xl.emb_i->torch.nn.functional.linear(emb_i, self.emb_projs[i])
A:transformers.modeling_transfo_xl.self.word_emb->AdaptiveEmbedding(config.n_token, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val)
A:transformers.modeling_transfo_xl.self.layers->torch.nn.ModuleList()
A:transformers.modeling_transfo_xl.self.pos_emb->PositionalEmbedding(self.d_model)
A:transformers.modeling_transfo_xl.empty->torch.zeros(self.mem_len, data.size(1), self.config.d_model, dtype=param.dtype, device=param.device)
A:transformers.modeling_transfo_xl.beg_idx->max(0, end_idx - self.mem_len)
A:transformers.modeling_transfo_xl.input_ids->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.modeling_transfo_xl.mems->self.init_mems(input_ids)
A:transformers.modeling_transfo_xl.(qlen, bsz)->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous().size()
A:transformers.modeling_transfo_xl.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_transfo_xl.word_emb->self.word_emb(input_ids)
A:transformers.modeling_transfo_xl.all_ones->self.word_emb(input_ids).new_ones((qlen, klen), dtype=torch.uint8)
A:transformers.modeling_transfo_xl.pos_seq->torch.arange(klen - 1, -1, -1.0, device=word_emb.device, dtype=word_emb.dtype)
A:transformers.modeling_transfo_xl.layer_outputs->layer(core_out, pos_emb, dec_attn_mask=dec_attn_mask, mems=mems_i, head_mask=head_mask[i])
A:transformers.modeling_transfo_xl.new_mems->self._update_mems(hids, mems, mlen, qlen)
A:transformers.modeling_transfo_xl.hids->list((t.transpose(0, 1).contiguous() for t in hids))
A:transformers.modeling_transfo_xl.attentions->list((t.permute(2, 3, 0, 1).contiguous() for t in attentions))
A:transformers.modeling_transfo_xl.self.transformer->TransfoXLModel(config)
A:transformers.modeling_transfo_xl.self.out_layer->torch.nn.Linear(config.d_model, config.n_token)
A:transformers.modeling_transfo_xl.self.sampler->LogUniformSampler(config.n_token, config.sample_softmax)
A:transformers.modeling_transfo_xl.self.crit->ProjectedAdaptiveLogSoftmax(config.n_token, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val)
A:transformers.modeling_transfo_xl.self.crit.out_projs[i]->torch.nn.Parameter(self.transformer.word_emb.emb_projs[i].clone())
A:transformers.modeling_transfo_xl.bsz->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous().size(0)
A:transformers.modeling_transfo_xl.tgt_len->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous().size(1)
A:transformers.modeling_transfo_xl.transformer_outputs->self.transformer(input_ids, mems=mems, head_mask=head_mask)
A:transformers.modeling_transfo_xl.logit->sample_logits(self.transformer.word_emb, self.out_layer.bias, labels, pred_hid, self.sampler)
A:transformers.modeling_transfo_xl.softmax_output->softmax_output.view(bsz, tgt_len).view(bsz, tgt_len)
transformers.TransfoXLLMHeadModel(self,config)
transformers.TransfoXLLMHeadModel.forward(self,input_ids,mems=None,head_mask=None,labels=None)
transformers.TransfoXLLMHeadModel.init_mems(self,data)
transformers.TransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TransfoXLLMHeadModel.tie_weights(self)
transformers.TransfoXLModel(self,config)
transformers.TransfoXLModel._prune_heads(self,heads)
transformers.TransfoXLModel._resize_token_embeddings(self,new_num_tokens)
transformers.TransfoXLModel._update_mems(self,hids,mems,qlen,mlen)
transformers.TransfoXLModel.backward_compatible(self)
transformers.TransfoXLModel.forward(self,input_ids,mems=None,head_mask=None)
transformers.TransfoXLModel.init_mems(self,data)
transformers.TransfoXLModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TransfoXLPreTrainedModel(PreTrainedModel)
transformers.TransfoXLPreTrainedModel._init_bias(self,bias)
transformers.TransfoXLPreTrainedModel._init_weight(self,weight)
transformers.TransfoXLPreTrainedModel._init_weights(self,m)
transformers.load_tf_weights_in_transfo_xl(model,config,tf_path)
transformers.modeling_transfo_xl.AdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.modeling_transfo_xl.AdaptiveEmbedding.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.modeling_transfo_xl.AdaptiveEmbedding.forward(self,inp)
transformers.modeling_transfo_xl.PositionalEmbedding(self,demb)
transformers.modeling_transfo_xl.PositionalEmbedding.__init__(self,demb)
transformers.modeling_transfo_xl.PositionalEmbedding.forward(self,pos_seq,bsz=None)
transformers.modeling_transfo_xl.PositionwiseFF(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.PositionwiseFF.__init__(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.PositionwiseFF.forward(self,inp)
transformers.modeling_transfo_xl.RelPartialLearnableDecoderLayer(self,n_head,d_model,d_head,d_inner,dropout,layer_norm_epsilon=1e-05,**kwargs)
transformers.modeling_transfo_xl.RelPartialLearnableDecoderLayer.__init__(self,n_head,d_model,d_head,d_inner,dropout,layer_norm_epsilon=1e-05,**kwargs)
transformers.modeling_transfo_xl.RelPartialLearnableDecoderLayer.forward(self,dec_inp,r,dec_attn_mask=None,mems=None,head_mask=None)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,output_attentions=False,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.__init__(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,output_attentions=False,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn._rel_shift(self,x)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.forward(self,w,r,attn_mask=None,mems=None,head_mask=None)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel(self,config)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.__init__(self,config)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.forward(self,input_ids,mems=None,head_mask=None,labels=None)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.init_mems(self,data)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.tie_weights(self)
transformers.modeling_transfo_xl.TransfoXLModel(self,config)
transformers.modeling_transfo_xl.TransfoXLModel.__init__(self,config)
transformers.modeling_transfo_xl.TransfoXLModel._prune_heads(self,heads)
transformers.modeling_transfo_xl.TransfoXLModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_transfo_xl.TransfoXLModel._update_mems(self,hids,mems,qlen,mlen)
transformers.modeling_transfo_xl.TransfoXLModel.backward_compatible(self)
transformers.modeling_transfo_xl.TransfoXLModel.forward(self,input_ids,mems=None,head_mask=None)
transformers.modeling_transfo_xl.TransfoXLModel.init_mems(self,data)
transformers.modeling_transfo_xl.TransfoXLModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel(PreTrainedModel)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._init_bias(self,bias)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._init_weight(self,weight)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._init_weights(self,m)
transformers.modeling_transfo_xl.build_tf_to_pytorch_map(model,config)
transformers.modeling_transfo_xl.load_tf_weights_in_transfo_xl(model,config,tf_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_gpt2.py----------------------------------------
A:transformers.modeling_gpt2.logger->logging.getLogger(__name__)
A:transformers.modeling_gpt2.tf_path->os.path.abspath(gpt2_checkpoint_path)
A:transformers.modeling_gpt2.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_gpt2.array->tensorflow.train.load_variable(tf_path, name)
A:transformers.modeling_gpt2.name->name.split('/').split('/')
A:transformers.modeling_gpt2.l->re.split('(\\d+)', m_name)
A:transformers.modeling_gpt2.pointer->getattr(pointer, l[0])
A:transformers.modeling_gpt2.num->int(l[1])
A:transformers.modeling_gpt2.pointer.data->torch.from_numpy(array)
A:transformers.modeling_gpt2.self.c_attn->prune_conv1d_layer(self.c_attn, index_attn, dim=1)
A:transformers.modeling_gpt2.self.c_proj->Conv1D(nx, n_state)
A:transformers.modeling_gpt2.self.attn_dropout->torch.nn.Dropout(config.attn_pdrop)
A:transformers.modeling_gpt2.self.resid_dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_gpt2.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_gpt2.mask->mask.view(-1).contiguous().eq(1).view(-1).contiguous().eq(1)
A:transformers.modeling_gpt2.index->torch.arange(len(mask))[mask].long()
A:transformers.modeling_gpt2.index_attn->torch.cat([index, index + self.split_size, index + 2 * self.split_size])
A:transformers.modeling_gpt2.w->self.attn_dropout(w)
A:transformers.modeling_gpt2.x->self.c_attn(x)
A:transformers.modeling_gpt2.(query, key, value)->self.c_attn(x).split(self.split_size, dim=2)
A:transformers.modeling_gpt2.query->self.split_heads(query)
A:transformers.modeling_gpt2.key->torch.cat((past_key, key), dim=-1)
A:transformers.modeling_gpt2.value->torch.cat((past_value, value), dim=-2)
A:transformers.modeling_gpt2.present->torch.stack((key.transpose(-2, -1), value))
A:transformers.modeling_gpt2.attn_outputs->self._attn(query, key, value, attention_mask, head_mask)
A:transformers.modeling_gpt2.a->self.resid_dropout(a)
A:transformers.modeling_gpt2.self.c_fc->Conv1D(n_state, nx)
A:transformers.modeling_gpt2.self.dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_gpt2.h->self.act(self.c_fc(x))
A:transformers.modeling_gpt2.h2->self.c_proj(h)
A:transformers.modeling_gpt2.self.ln_1->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.self.attn->Attention(nx, n_ctx, config, scale)
A:transformers.modeling_gpt2.self.ln_2->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.self.mlp->MLP(4 * nx, config)
A:transformers.modeling_gpt2.output_attn->self.attn(self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask)
A:transformers.modeling_gpt2.m->self.mlp(self.ln_2(x))
A:transformers.modeling_gpt2.self.wte->self._get_resized_embeddings(self.wte, new_num_tokens)
A:transformers.modeling_gpt2.self.wpe->torch.nn.Embedding(config.n_positions, config.n_embd)
A:transformers.modeling_gpt2.self.drop->torch.nn.Dropout(config.embd_pdrop)
A:transformers.modeling_gpt2.self.h->torch.nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])
A:transformers.modeling_gpt2.self.ln_f->torch.nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.modeling_gpt2.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_gpt2.token_type_ids->token_type_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_gpt2.position_ids->position_ids.unsqueeze(0).expand_as(input_ids).unsqueeze(0).expand_as(input_ids)
A:transformers.modeling_gpt2.past_length->past[0][0].size(-2)
A:transformers.modeling_gpt2.attention_mask->attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_gpt2.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_gpt2.inputs_embeds->self.wte(input_ids)
A:transformers.modeling_gpt2.position_embeds->self.wpe(position_ids)
A:transformers.modeling_gpt2.token_type_embeds->self.wte(token_type_ids)
A:transformers.modeling_gpt2.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.modeling_gpt2.outputs->block(hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i])
A:transformers.modeling_gpt2.all_attentions->tuple((t.view(*attention_output_shape) for t in all_attentions))
A:transformers.modeling_gpt2.self.transformer->GPT2Model(config)
A:transformers.modeling_gpt2.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)
A:transformers.modeling_gpt2.transformer_outputs->self.transformer(input_ids, past=past, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)
A:transformers.modeling_gpt2.lm_logits->self.lm_head(hidden_states)
A:transformers.modeling_gpt2.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.modeling_gpt2.shift_labels->lm_labels[..., 1:].contiguous()
A:transformers.modeling_gpt2.loss_fct->CrossEntropyLoss(ignore_index=-1)
A:transformers.modeling_gpt2.loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
A:transformers.modeling_gpt2.self.multiple_choice_head->SequenceSummary(config)
A:transformers.modeling_gpt2.mc_logits->self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)
transformers.GPT2DoubleHeadsModel(self,config)
transformers.GPT2DoubleHeadsModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,lm_labels=None,mc_labels=None)
transformers.GPT2DoubleHeadsModel.tie_weights(self)
transformers.GPT2LMHeadModel(self,config)
transformers.GPT2LMHeadModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.GPT2LMHeadModel.tie_weights(self)
transformers.GPT2Model(self,config)
transformers.GPT2Model._prune_heads(self,heads_to_prune)
transformers.GPT2Model._resize_token_embeddings(self,new_num_tokens)
transformers.GPT2Model.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.GPT2PreTrainedModel(self,*inputs,**kwargs)
transformers.GPT2PreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_gpt2(model,config,gpt2_checkpoint_path)
transformers.modeling_gpt2.Attention(self,nx,n_ctx,config,scale=False)
transformers.modeling_gpt2.Attention.__init__(self,nx,n_ctx,config,scale=False)
transformers.modeling_gpt2.Attention._attn(self,q,k,v,attention_mask=None,head_mask=None)
transformers.modeling_gpt2.Attention.forward(self,x,layer_past=None,attention_mask=None,head_mask=None)
transformers.modeling_gpt2.Attention.merge_heads(self,x)
transformers.modeling_gpt2.Attention.prune_heads(self,heads)
transformers.modeling_gpt2.Attention.split_heads(self,x,k=False)
transformers.modeling_gpt2.Block(self,n_ctx,config,scale=False)
transformers.modeling_gpt2.Block.__init__(self,n_ctx,config,scale=False)
transformers.modeling_gpt2.Block.forward(self,x,layer_past=None,attention_mask=None,head_mask=None)
transformers.modeling_gpt2.GPT2DoubleHeadsModel(self,config)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.__init__(self,config)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,mc_token_ids=None,lm_labels=None,mc_labels=None)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.tie_weights(self)
transformers.modeling_gpt2.GPT2LMHeadModel(self,config)
transformers.modeling_gpt2.GPT2LMHeadModel.__init__(self,config)
transformers.modeling_gpt2.GPT2LMHeadModel.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,labels=None)
transformers.modeling_gpt2.GPT2LMHeadModel.tie_weights(self)
transformers.modeling_gpt2.GPT2Model(self,config)
transformers.modeling_gpt2.GPT2Model.__init__(self,config)
transformers.modeling_gpt2.GPT2Model._prune_heads(self,heads_to_prune)
transformers.modeling_gpt2.GPT2Model._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_gpt2.GPT2Model.forward(self,input_ids,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None)
transformers.modeling_gpt2.GPT2PreTrainedModel(self,*inputs,**kwargs)
transformers.modeling_gpt2.GPT2PreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.modeling_gpt2.GPT2PreTrainedModel._init_weights(self,module)
transformers.modeling_gpt2.MLP(self,n_state,config)
transformers.modeling_gpt2.MLP.__init__(self,n_state,config)
transformers.modeling_gpt2.MLP.forward(self,x)
transformers.modeling_gpt2.gelu(x)
transformers.modeling_gpt2.load_tf_weights_in_gpt2(model,config,gpt2_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_xlnet_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.config->transformers.XLNetConfig.from_json_file(bert_config_file)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.model->XLNetLMHeadModel(config)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.pytorch_config_dump_path->os.path.join(pytorch_dump_folder_path, CONFIG_NAME)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.convert_xlnet_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_folder_path,finetuning_task=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/convert_roberta_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.logger->logging.getLogger(__name__)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.roberta->fairseq.models.roberta.RobertaModel.from_pretrained(roberta_checkpoint_path)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.config->BertConfig(vocab_size_or_config_json_file=50265, hidden_size=roberta.args.encoder_embed_dim, num_hidden_layers=roberta.args.encoder_layers, num_attention_heads=roberta.args.encoder_attention_heads, intermediate_size=roberta.args.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.model.roberta.embeddings.token_type_embeddings.weight.data->torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.their_output->fairseq.models.roberta.RobertaModel.from_pretrained(roberta_checkpoint_path).model.classification_heads['mnli'](roberta.extract_features(input_ids))
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.max_absolute_diff->torch.max(torch.abs(our_output - their_output)).item()
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.success->torch.allclose(our_output, their_output, atol=0.001)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.convert_roberta_checkpoint_to_pytorch(roberta_checkpoint_path,pytorch_dump_folder_path,classification_head)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_distilbert.py----------------------------------------
A:transformers.modeling_tf_distilbert.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_distilbert.inputs_list->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.modeling_tf_distilbert.attns_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_tf_distilbert.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_distilbert.self.word_embeddings->self.add_weight('weight', shape=[self.vocab_size, self.dim], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_distilbert.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.dim, embeddings_initializer=get_initializer(config.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_distilbert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='LayerNorm')
A:transformers.modeling_tf_distilbert.self.dropout->tensorflow.keras.layers.Dropout(config.qa_dropout)
A:transformers.modeling_tf_distilbert.word_embeddings->tensorflow.gather(self.word_embeddings, input_ids)
A:transformers.modeling_tf_distilbert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_tf_distilbert.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_distilbert.x->self.dropout(x, training=training)
A:transformers.modeling_tf_distilbert.logits->self.qa_outputs(hidden_states)
A:transformers.modeling_tf_distilbert.self.q_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='q_lin')
A:transformers.modeling_tf_distilbert.self.k_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='k_lin')
A:transformers.modeling_tf_distilbert.self.v_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='v_lin')
A:transformers.modeling_tf_distilbert.self.out_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='out_lin')
A:transformers.modeling_tf_distilbert.self.pruned_heads->set()
A:transformers.modeling_tf_distilbert.(bs, q_length, dim)->shape_list(query)
A:transformers.modeling_tf_distilbert.q->shape(self.q_lin(query))
A:transformers.modeling_tf_distilbert.k->shape(self.k_lin(key))
A:transformers.modeling_tf_distilbert.v->shape(self.v_lin(value))
A:transformers.modeling_tf_distilbert.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.modeling_tf_distilbert.mask->tensorflow.reshape(mask, mask_reshape)
A:transformers.modeling_tf_distilbert.weights->self.dropout(weights, training=training)
A:transformers.modeling_tf_distilbert.context->self.out_lin(context)
A:transformers.modeling_tf_distilbert.self.lin1->tensorflow.keras.layers.Dense(config.hidden_dim, kernel_initializer=get_initializer(config.initializer_range), name='lin1')
A:transformers.modeling_tf_distilbert.self.lin2->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='lin2')
A:transformers.modeling_tf_distilbert.self.attention->TFMultiHeadSelfAttention(config, name='attention')
A:transformers.modeling_tf_distilbert.self.sa_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='sa_layer_norm')
A:transformers.modeling_tf_distilbert.self.ffn->TFFFN(config, name='ffn')
A:transformers.modeling_tf_distilbert.self.output_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='output_layer_norm')
A:transformers.modeling_tf_distilbert.sa_output->self.sa_layer_norm(sa_output + x)
A:transformers.modeling_tf_distilbert.ffn_output->self.output_layer_norm(ffn_output + sa_output)
A:transformers.modeling_tf_distilbert.layer_outputs->layer_module([hidden_state, attn_mask, head_mask[i]], training=training)
A:transformers.modeling_tf_distilbert.self.embeddings->TFEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_distilbert.self.transformer->TFTransformer(config, name='transformer')
A:transformers.modeling_tf_distilbert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_distilbert.attention_mask->tensorflow.cast(attention_mask, dtype=tf.float32)
A:transformers.modeling_tf_distilbert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_distilbert.embedding_output->self.embeddings(input_ids)
A:transformers.modeling_tf_distilbert.tfmr_output->self.transformer([embedding_output, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_distilbert.self.distilbert->TFDistilBertMainLayer(config, name='distilbert')
A:transformers.modeling_tf_distilbert.outputs->self.distilbert(inputs, **kwargs)
A:transformers.modeling_tf_distilbert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_distilbert.hidden_states->self.dropout(hidden_states, training=kwargs.get('training', False))
A:transformers.modeling_tf_distilbert.self.vocab_transform->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='vocab_transform')
A:transformers.modeling_tf_distilbert.self.act->tensorflow.keras.layers.Activation(gelu)
A:transformers.modeling_tf_distilbert.self.vocab_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='vocab_layer_norm')
A:transformers.modeling_tf_distilbert.self.vocab_projector->TFDistilBertLMHead(config, self.distilbert.embeddings, name='vocab_projector')
A:transformers.modeling_tf_distilbert.distilbert_output->self.distilbert(inputs, **kwargs)
A:transformers.modeling_tf_distilbert.prediction_logits->self.vocab_projector(prediction_logits)
A:transformers.modeling_tf_distilbert.self.pre_classifier->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), activation='relu', name='pre_classifier')
A:transformers.modeling_tf_distilbert.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_distilbert.pooled_output->self.dropout(pooled_output, training=kwargs.get('training', False))
A:transformers.modeling_tf_distilbert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_distilbert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_distilbert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_distilbert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFDistilBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFDistilBertForMaskedLM.call(self,inputs,**kwargs)
transformers.TFDistilBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFDistilBertForQuestionAnswering.call(self,inputs,**kwargs)
transformers.TFDistilBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFDistilBertForSequenceClassification.call(self,inputs,**kwargs)
transformers.TFDistilBertMainLayer(self,config,**kwargs)
transformers.TFDistilBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFDistilBertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFDistilBertMainLayer.call(self,inputs,attention_mask=None,head_mask=None,training=False)
transformers.TFDistilBertModel(self,config,*inputs,**kwargs)
transformers.TFDistilBertModel.call(self,inputs,**kwargs)
transformers.TFDistilBertPreTrainedModel(TFPreTrainedModel)
transformers.load_distilbert_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM.call(self,inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.call(self,inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification.call(self,inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertLMHead.build(self,input_shape)
transformers.modeling_tf_distilbert.TFDistilBertLMHead.call(self,hidden_states)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer.call(self,inputs,attention_mask=None,head_mask=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertModel.call(self,inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_distilbert.TFEmbeddings(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFEmbeddings._embedding(self,inputs,training=False)
transformers.modeling_tf_distilbert.TFEmbeddings._linear(self,inputs)
transformers.modeling_tf_distilbert.TFEmbeddings.build(self,input_shape)
transformers.modeling_tf_distilbert.TFEmbeddings.call(self,inputs,mode='embedding',training=False)
transformers.modeling_tf_distilbert.TFFFN(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFFFN.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFFFN.call(self,input,training=False)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention.call(self,inputs,training=False)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention.prune_heads(self,heads)
transformers.modeling_tf_distilbert.TFTransformer(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformer.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformer.call(self,inputs,training=False)
transformers.modeling_tf_distilbert.TFTransformerBlock(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformerBlock.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformerBlock.call(self,inputs,training=False)
transformers.modeling_tf_distilbert.gelu(x)
transformers.modeling_tf_distilbert.gelu_new(x)
transformers.modeling_tf_distilbert.load_distilbert_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_transfo_xl.py----------------------------------------
A:transformers.tokenization_transfo_xl.logger->logging.getLogger(__name__)
A:transformers.tokenization_transfo_xl.self.counter->Counter()
A:transformers.tokenization_transfo_xl.vocab_dict->torch.load(pretrained_vocab_file)
A:transformers.tokenization_transfo_xl.symbols->line.lower().split(self.delimiter)
A:transformers.tokenization_transfo_xl.self.sym2idx->OrderedDict()
A:transformers.tokenization_transfo_xl.vocab_file->os.path.join(vocab_path, VOCAB_FILES_NAMES['pretrained_vocab_file'])
A:transformers.tokenization_transfo_xl.encoded->torch.cat(encoded)
A:transformers.tokenization_transfo_xl.out_string->' '.join(tokens).strip()
A:transformers.tokenization_transfo_xl.line->line.lower().lower()
A:transformers.tokenization_transfo_xl.data->torch.LongTensor(self.bptt, self.bsz)
A:transformers.tokenization_transfo_xl.self.data->torch.LongTensor(self.bptt, self.bsz).view(bsz, -1).t().contiguous().to(device)
A:transformers.tokenization_transfo_xl.seq_len->min(bptt, self.data.size(0) - 1 - i)
A:transformers.tokenization_transfo_xl.beg_idx->max(0, i - self.ext_len)
A:transformers.tokenization_transfo_xl.data_out->torch.LongTensor(self.bptt, self.bsz).transpose(0, 1).contiguous().to(self.device)
A:transformers.tokenization_transfo_xl.target_out->torch.LongTensor(self.bptt, self.bsz).transpose(0, 1).contiguous().to(self.device)
A:transformers.tokenization_transfo_xl.bptt->min(max_len, max(min_len, int(np.random.normal(bptt, std))))
A:transformers.tokenization_transfo_xl.(data, target, seq_len)->self.get_batch(i, bptt)
A:transformers.tokenization_transfo_xl.target->torch.LongTensor(self.bptt, self.bsz)
A:transformers.tokenization_transfo_xl.streams[i]->next(sent_stream)
A:transformers.tokenization_transfo_xl.n_new->min(len(streams[i]) - 1, self.bptt - n_filled)
A:transformers.tokenization_transfo_xl.n_retain->min(data.size(0), self.ext_len)
A:transformers.tokenization_transfo_xl.sent_stream->self.get_sent_stream(path)
A:transformers.tokenization_transfo_xl.sents->self.vocab.encode_file(path, add_double_eos=True)
A:transformers.tokenization_transfo_xl.vocab->TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
A:transformers.tokenization_transfo_xl.corpus_file->os.path.join(pretrained_model_name_or_path, CORPUS_NAME)
A:transformers.tokenization_transfo_xl.resolved_corpus_file->cached_path(corpus_file, cache_dir=cache_dir)
A:transformers.tokenization_transfo_xl.corpus->TransfoXLCorpus(datadir, dataset, **kwargs)
A:transformers.tokenization_transfo_xl.corpus_dict->torch.load(resolved_corpus_file)
A:transformers.tokenization_transfo_xl.corpus.train->torch.tensor(corpus.train, dtype=torch.long)
A:transformers.tokenization_transfo_xl.corpus.valid->torch.tensor(corpus.valid, dtype=torch.long)
A:transformers.tokenization_transfo_xl.corpus.test->torch.tensor(corpus.test, dtype=torch.long)
A:transformers.tokenization_transfo_xl.self.vocab->TransfoXLTokenizer(*args, **kwargs)
A:transformers.tokenization_transfo_xl.train_path_pattern->os.path.join(path, '1-billion-word-language-modeling-benchmark-r13output', 'training-monolingual.tokenized.shuffled', 'news.en-*')
A:transformers.tokenization_transfo_xl.train_paths->glob.glob(train_path_pattern)
A:transformers.tokenization_transfo_xl.self.train->self.vocab.encode_file(os.path.join(path, 'train.txt'), ordered=True, add_eos=False)
A:transformers.tokenization_transfo_xl.self.valid->self.vocab.encode_file(os.path.join(path, 'valid.txt'), ordered=False, add_double_eos=True)
A:transformers.tokenization_transfo_xl.self.test->self.vocab.encode_file(os.path.join(path, 'test.txt'), ordered=False, add_double_eos=True)
A:transformers.tokenization_transfo_xl.data_iter->LMShuffledIterator(data, *args, **kwargs)
A:transformers.tokenization_transfo_xl.fn->os.path.join(datadir, 'cache.pt')
A:transformers.tokenization_transfo_xl.fn_pickle->os.path.join(datadir, 'cache.pkl')
A:transformers.tokenization_transfo_xl.kwargs['vocab_file']->os.path.join(datadir, '1b_word_vocab.txt')
transformers.TransfoXLCorpus(self,*args,**kwargs)
transformers.TransfoXLCorpus.build_corpus(self,path,dataset)
transformers.TransfoXLCorpus.from_pretrained(cls,pretrained_model_name_or_path,cache_dir=None,*inputs,**kwargs)
transformers.TransfoXLCorpus.get_iterator(self,split,*args,**kwargs)
transformers.TransfoXLTokenizer(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],**kwargs)
transformers.TransfoXLTokenizer._build_from_file(self,vocab_file)
transformers.TransfoXLTokenizer._convert_id_to_token(self,idx)
transformers.TransfoXLTokenizer._convert_token_to_id(self,sym)
transformers.TransfoXLTokenizer._tokenize(self,line,add_eos=False,add_double_eos=False)
transformers.TransfoXLTokenizer.add_special(self,sym)
transformers.TransfoXLTokenizer.add_symbol(self,sym)
transformers.TransfoXLTokenizer.build_vocab(self)
transformers.TransfoXLTokenizer.convert_to_tensor(self,symbols)
transformers.TransfoXLTokenizer.convert_tokens_to_string(self,tokens)
transformers.TransfoXLTokenizer.count_file(self,path,verbose=False,add_eos=False)
transformers.TransfoXLTokenizer.count_sents(self,sents,verbose=False)
transformers.TransfoXLTokenizer.encode_file(self,path,ordered=False,verbose=False,add_eos=True,add_double_eos=False)
transformers.TransfoXLTokenizer.encode_sents(self,sents,ordered=False,verbose=False)
transformers.TransfoXLTokenizer.save_vocabulary(self,vocab_path)
transformers.TransfoXLTokenizer.vocab_size(self)
transformers.tokenization_transfo_xl.LMMultiFileIterator(self,paths,vocab,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMMultiFileIterator.__init__(self,paths,vocab,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMMultiFileIterator.__iter__(self)
transformers.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream(self,path)
transformers.tokenization_transfo_xl.LMOrderedIterator(self,data,bsz,bptt,device='cpu',ext_len=None)
transformers.tokenization_transfo_xl.LMOrderedIterator.__init__(self,data,bsz,bptt,device='cpu',ext_len=None)
transformers.tokenization_transfo_xl.LMOrderedIterator.__iter__(self)
transformers.tokenization_transfo_xl.LMOrderedIterator.get_batch(self,i,bptt=None)
transformers.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter(self,start=0)
transformers.tokenization_transfo_xl.LMOrderedIterator.get_varlen_iter(self,start=0,std=5,min_len=5,max_deviation=3)
transformers.tokenization_transfo_xl.LMShuffledIterator(self,data,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMShuffledIterator.__init__(self,data,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMShuffledIterator.__iter__(self)
transformers.tokenization_transfo_xl.LMShuffledIterator.get_sent_stream(self)
transformers.tokenization_transfo_xl.LMShuffledIterator.stream_iterator(self,sent_stream)
transformers.tokenization_transfo_xl.TransfoXLCorpus(self,*args,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLCorpus.__init__(self,*args,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLCorpus.build_corpus(self,path,dataset)
transformers.tokenization_transfo_xl.TransfoXLCorpus.from_pretrained(cls,pretrained_model_name_or_path,cache_dir=None,*inputs,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLCorpus.get_iterator(self,split,*args,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizer(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.__init__(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file(self,vocab_file)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._convert_id_to_token(self,idx)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._convert_token_to_id(self,sym)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._tokenize(self,line,add_eos=False,add_double_eos=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_special(self,sym)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol(self,sym)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab(self)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor(self,symbols)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file(self,path,verbose=False,add_eos=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_sents(self,sents,verbose=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file(self,path,ordered=False,verbose=False,add_eos=True,add_double_eos=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_sents(self,sents,ordered=False,verbose=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.save_vocabulary(self,vocab_path)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.vocab_size(self)
transformers.tokenization_transfo_xl.get_lm_corpus(datadir,dataset)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_auto.py----------------------------------------
A:transformers.modeling_tf_auto.logger->logging.getLogger(__name__)
transformers.TFAutoModel(self)
transformers.TFAutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForQuestionAnswering(self)
transformers.TFAutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForSequenceClassification(self)
transformers.TFAutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelWithLMHead(self)
transformers.TFAutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModel(self)
transformers.modeling_tf_auto.TFAutoModel.__init__(self)
transformers.modeling_tf_auto.TFAutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering(self)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification(self)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelWithLMHead(self)
transformers.modeling_tf_auto.TFAutoModelWithLMHead.__init__(self)
transformers.modeling_tf_auto.TFAutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_ctrl.py----------------------------------------
A:transformers.modeling_tf_ctrl.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_ctrl.tf_inputs->tensorflow.constant(inputs_list)
A:transformers.modeling_tf_ctrl.tfo->tf_model(tf_inputs, training=False)
A:transformers.modeling_tf_ctrl.angle_rads->angle_defn(np.arange(position)[:, np.newaxis], np.arange(d_model_size)[np.newaxis, :], d_model_size)
A:transformers.modeling_tf_ctrl.sines->numpy.sin(angle_rads[:, 0::2])
A:transformers.modeling_tf_ctrl.cosines->numpy.cos(angle_rads[:, 1::2])
A:transformers.modeling_tf_ctrl.pos_encoding->tensorflow.cast(np.concatenate([sines, cosines], axis=-1), dtype=tf.float32)
A:transformers.modeling_tf_ctrl.matmul_qk->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.modeling_tf_ctrl.dk->tensorflow.cast(shape_list(k)[-1], tf.float32)
A:transformers.modeling_tf_ctrl.attention_weights->tensorflow.nn.softmax(scaled_attention_logits, axis=-1)
A:transformers.modeling_tf_ctrl.output->self.dense(original_size_attention)
A:transformers.modeling_tf_ctrl.self.depth->int(d_model_size / self.num_heads)
A:transformers.modeling_tf_ctrl.self.Wq->tensorflow.keras.layers.Dense(d_model_size, name='Wq')
A:transformers.modeling_tf_ctrl.self.Wk->tensorflow.keras.layers.Dense(d_model_size, name='Wk')
A:transformers.modeling_tf_ctrl.self.Wv->tensorflow.keras.layers.Dense(d_model_size, name='Wv')
A:transformers.modeling_tf_ctrl.self.dense->tensorflow.keras.layers.Dense(d_model_size, name='dense')
A:transformers.modeling_tf_ctrl.x->tensorflow.reshape(x, (batch_size, -1, self.num_heads, self.depth))
A:transformers.modeling_tf_ctrl.q->self.split_into_heads(q, batch_size)
A:transformers.modeling_tf_ctrl.k->tensorflow.concat((past_key, k), dim=-2)
A:transformers.modeling_tf_ctrl.v->tensorflow.concat((past_value, v), dim=-2)
A:transformers.modeling_tf_ctrl.(past_key, past_value)->tensorflow.unstack(layer_past, axis=1)
A:transformers.modeling_tf_ctrl.present->tensorflow.stack((k, v), axis=1)
A:transformers.modeling_tf_ctrl.scaled_attention->tensorflow.transpose(output[0], perm=[0, 2, 1, 3])
A:transformers.modeling_tf_ctrl.original_size_attention->tensorflow.reshape(scaled_attention, (batch_size, -1, self.d_model_size))
A:transformers.modeling_tf_ctrl.self.multi_head_attention->TFMultiHeadAttention(d_model_size, num_heads, output_attentions, name='multi_head_attention')
A:transformers.modeling_tf_ctrl.self.ffn->point_wise_feed_forward_network(d_model_size, dff, name='ffn')
A:transformers.modeling_tf_ctrl.self.layernorm1->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layernorm1')
A:transformers.modeling_tf_ctrl.self.layernorm2->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layernorm2')
A:transformers.modeling_tf_ctrl.self.dropout1->tensorflow.keras.layers.Dropout(rate)
A:transformers.modeling_tf_ctrl.self.dropout2->tensorflow.keras.layers.Dropout(rate)
A:transformers.modeling_tf_ctrl.normed->self.layernorm1(x)
A:transformers.modeling_tf_ctrl.attn_outputs->self.multi_head_attention([normed, normed, normed, mask, layer_past, attention_mask, head_mask], training=training)
A:transformers.modeling_tf_ctrl.attn_output->self.dropout1(attn_output, training=training)
A:transformers.modeling_tf_ctrl.out2->self.layernorm2(out1)
A:transformers.modeling_tf_ctrl.ffn_output->self.dropout2(ffn_output, training=training)
A:transformers.modeling_tf_ctrl.self.pos_encoding->positional_encoding(config.n_positions, self.d_model_size)
A:transformers.modeling_tf_ctrl.self.w->TFSharedEmbeddings(config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name='w')
A:transformers.modeling_tf_ctrl.self.dropout->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.modeling_tf_ctrl.self.layernorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='layernorm')
A:transformers.modeling_tf_ctrl.input_ids->tensorflow.reshape(input_ids, [-1, input_shape[-1]])
A:transformers.modeling_tf_ctrl.past->inputs.get('past', past)
A:transformers.modeling_tf_ctrl.attention_mask->tensorflow.cast(attention_mask, tf.float32)
A:transformers.modeling_tf_ctrl.token_type_ids->tensorflow.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])
A:transformers.modeling_tf_ctrl.position_ids->tensorflow.reshape(position_ids, [-1, shape_list(position_ids)[-1]])
A:transformers.modeling_tf_ctrl.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_ctrl.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_ctrl.token_type_embeds->self.w(token_type_ids, mode='embedding')
A:transformers.modeling_tf_ctrl.inputs_embeds->self.w(input_ids, mode='embedding')
A:transformers.modeling_tf_ctrl.pos_embeds->tensorflow.gather(self.pos_encoding, position_ids)
A:transformers.modeling_tf_ctrl.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_ctrl.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_ctrl.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.modeling_tf_ctrl.self.transformer->TFCTRLMainLayer(config, name='transformer')
A:transformers.modeling_tf_ctrl.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_ctrl.self.lm_head->TFCTRLLMHead(config, self.transformer.w, name='lm_head')
A:transformers.modeling_tf_ctrl.transformer_outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_ctrl.lm_logits->self.lm_head(hidden_states)
transformers.TFCTRLLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFCTRLLMHeadModel.call(self,inputs,**kwargs)
transformers.TFCTRLModel(self,config,*inputs,**kwargs)
transformers.TFCTRLModel.call(self,inputs,**kwargs)
transformers.TFCTRLPreTrainedModel(TFPreTrainedModel)
transformers.load_ctrl_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_ctrl.TFCTRLLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHead.build(self,input_shape)
transformers.modeling_tf_ctrl.TFCTRLLMHead.call(self,hidden_states)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel.call(self,inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLMainLayer(self,config,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_ctrl.TFCTRLMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_ctrl.TFCTRLMainLayer.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,training=False)
transformers.modeling_tf_ctrl.TFCTRLModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLModel.call(self,inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_ctrl.TFEncoderLayer(self,d_model_size,num_heads,dff,rate=0.1,layer_norm_epsilon=1e-06,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFEncoderLayer.__init__(self,d_model_size,num_heads,dff,rate=0.1,layer_norm_epsilon=1e-06,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFEncoderLayer.call(self,inputs,training=False)
transformers.modeling_tf_ctrl.TFMultiHeadAttention(self,d_model_size,num_heads,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFMultiHeadAttention.__init__(self,d_model_size,num_heads,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFMultiHeadAttention.call(self,inputs,training=False)
transformers.modeling_tf_ctrl.TFMultiHeadAttention.split_into_heads(self,x,batch_size)
transformers.modeling_tf_ctrl.angle_defn(pos,i,d_model_size)
transformers.modeling_tf_ctrl.load_ctrl_pt_weights_in_tf2(tf_model,pytorch_checkpoint_path)
transformers.modeling_tf_ctrl.point_wise_feed_forward_network(d_model_size,dff,name='')
transformers.modeling_tf_ctrl.positional_encoding(position,d_model_size)
transformers.modeling_tf_ctrl.scaled_dot_product_attention(q,k,v,mask,attention_mask=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/tokenization_auto.py----------------------------------------
A:transformers.tokenization_auto.logger->logging.getLogger(__name__)
transformers.AutoTokenizer(self)
transformers.AutoTokenizer.from_pretrained(cls,pretrained_model_name_or_path,*inputs,**kwargs)
transformers.tokenization_auto.AutoTokenizer(self)
transformers.tokenization_auto.AutoTokenizer.__init__(self)
transformers.tokenization_auto.AutoTokenizer.from_pretrained(cls,pretrained_model_name_or_path,*inputs,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_xlnet.py----------------------------------------
A:transformers.modeling_xlnet.logger->logging.getLogger(__name__)
A:transformers.modeling_xlnet.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_xlnet.array->numpy.transpose(array)
A:transformers.modeling_xlnet.tf_to_pt_map->build_tf_xlnet_to_pytorch_map(model, config, tf_weights)
A:transformers.modeling_xlnet.p_i.data->torch.from_numpy(arr_i)
A:transformers.modeling_xlnet.pointer.data->torch.from_numpy(array)
A:transformers.modeling_xlnet.self.q->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.k->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.v->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.o->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r_r_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r_s_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r_w_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.seg_embed->torch.nn.Parameter(torch.FloatTensor(2, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.layer_norm->XLNetLayerNorm(config.d_model, eps=config.layer_norm_eps)
A:transformers.modeling_xlnet.self.dropout->torch.nn.Dropout(config.dropout)
A:transformers.modeling_xlnet.x->torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))
A:transformers.modeling_xlnet.ac->torch.einsum('ibnd,jbnd->ijbn', q_head + self.r_w_bias, k_head_h)
A:transformers.modeling_xlnet.bd->self.rel_shift(bd, klen=ac.shape[1])
A:transformers.modeling_xlnet.ef->torch.einsum('ijbs,ibns->ijbn', seg_mat, ef)
A:transformers.modeling_xlnet.attn_prob->self.dropout(attn_prob)
A:transformers.modeling_xlnet.attn_vec->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask)
A:transformers.modeling_xlnet.attn_out->self.dropout(attn_out)
A:transformers.modeling_xlnet.output->self.sequence_summary(output)
A:transformers.modeling_xlnet.cat->torch.cat([mems, h], dim=0)
A:transformers.modeling_xlnet.k_head_h->torch.einsum('ibh,hnd->ibnd', cat, self.k)
A:transformers.modeling_xlnet.v_head_h->torch.einsum('ibh,hnd->ibnd', cat, self.v)
A:transformers.modeling_xlnet.k_head_r->torch.einsum('ibh,hnd->ibnd', r, self.r)
A:transformers.modeling_xlnet.q_head_h->torch.einsum('ibh,hnd->ibnd', h, self.q)
A:transformers.modeling_xlnet.attn_vec_h->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask)
A:transformers.modeling_xlnet.output_h->self.dropout(word_emb_k)
A:transformers.modeling_xlnet.q_head_g->torch.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)
A:transformers.modeling_xlnet.attn_vec_g->self.rel_attn_core(q_head_g, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_g, head_mask=head_mask)
A:transformers.modeling_xlnet.output_g->self.dropout(word_emb_q)
A:transformers.modeling_xlnet.self.layer_1->torch.nn.Linear(config.d_model, config.d_inner)
A:transformers.modeling_xlnet.self.layer_2->torch.nn.Linear(config.d_inner, config.d_model)
A:transformers.modeling_xlnet.self.rel_attn->XLNetRelativeAttention(config)
A:transformers.modeling_xlnet.self.ff->XLNetFeedForward(config)
A:transformers.modeling_xlnet.outputs->self.transformer(input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask)
A:transformers.modeling_xlnet.self.word_embedding->self._get_resized_embeddings(self.word_embedding, new_num_tokens)
A:transformers.modeling_xlnet.self.mask_emb->torch.nn.Parameter(torch.FloatTensor(1, 1, config.d_model))
A:transformers.modeling_xlnet.self.layer->torch.nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])
A:transformers.modeling_xlnet.attn_mask->(attn_mask > 0).to(dtype_float)
A:transformers.modeling_xlnet.mask_up->torch.triu(attn_mask, diagonal=1)
A:transformers.modeling_xlnet.attn_mask_pad->torch.zeros([qlen, mlen])
A:transformers.modeling_xlnet.ret->ret.to(next(self.parameters())).to(next(self.parameters()))
A:transformers.modeling_xlnet.mask_lo->torch.tril(attn_mask, diagonal=-1)
A:transformers.modeling_xlnet.sinusoid_inp->torch.einsum('i,d->id', pos_seq, inv_freq)
A:transformers.modeling_xlnet.pos_emb->self.dropout(pos_emb)
A:transformers.modeling_xlnet.freq_seq->torch.arange(0, self.d_model, 2.0, dtype=torch.float)
A:transformers.modeling_xlnet.fwd_pos_seq->fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len).clamp(-self.clamp_len, self.clamp_len)
A:transformers.modeling_xlnet.bwd_pos_seq->bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len).clamp(-self.clamp_len, self.clamp_len)
A:transformers.modeling_xlnet.fwd_pos_emb->self.positional_embedding(fwd_pos_seq, inv_freq)
A:transformers.modeling_xlnet.bwd_pos_emb->self.positional_embedding(bwd_pos_seq, inv_freq)
A:transformers.modeling_xlnet.input_ids->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.modeling_xlnet.mems_mask->torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)
A:transformers.modeling_xlnet.data_mask->torch.cat([mems_mask, data_mask], dim=1)
A:transformers.modeling_xlnet.non_tgt_mask->(attn_mask + non_tgt_mask[:, :, None, None] > 0).to(attn_mask)
A:transformers.modeling_xlnet.word_emb_k->self.word_embedding(input_ids)
A:transformers.modeling_xlnet.word_emb_q->self.mask_emb.expand(target_mapping.shape[0], bsz, -1)
A:transformers.modeling_xlnet.mem_pad->torch.zeros([mlen, bsz], dtype=torch.long, device=device)
A:transformers.modeling_xlnet.cat_ids->torch.cat([mem_pad, token_type_ids], dim=0)
A:transformers.modeling_xlnet.seg_mat->torch.nn.functional.one_hot(seg_mat, num_classes=2).to(dtype_float)
A:transformers.modeling_xlnet.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_xlnet.hidden_states->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states))
A:transformers.modeling_xlnet.attentions->tuple((t.permute(2, 3, 0, 1).contiguous() for t in attentions))
A:transformers.modeling_xlnet.self.transformer->XLNetModel(config)
A:transformers.modeling_xlnet.self.lm_loss->torch.nn.Linear(config.d_model, config.n_token, bias=True)
A:transformers.modeling_xlnet.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask)
A:transformers.modeling_xlnet.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_xlnet.loss_fct->CrossEntropyLoss()
A:transformers.modeling_xlnet.loss->loss_fct(reshaped_logits, labels.view(-1))
A:transformers.modeling_xlnet.self.sequence_summary->SequenceSummary(config)
A:transformers.modeling_xlnet.self.logits_proj->torch.nn.Linear(config.d_model, 1)
A:transformers.modeling_xlnet.flat_input_ids->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous().view(-1, input_ids.size(-1))
A:transformers.modeling_xlnet.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.modeling_xlnet.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_xlnet.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_xlnet.start_logits->self.start_logits(hidden_states, p_mask=p_mask)
A:transformers.modeling_xlnet.end_logits->self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)
A:transformers.modeling_xlnet.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlnet.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlnet.ignored_index->self.start_logits(hidden_states, p_mask=p_mask).size(1)
A:transformers.modeling_xlnet.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_xlnet.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_xlnet.self.start_logits->PoolerStartLogits(config)
A:transformers.modeling_xlnet.self.end_logits->PoolerEndLogits(config)
A:transformers.modeling_xlnet.self.answer_class->PoolerAnswerClass(config)
A:transformers.modeling_xlnet.cls_logits->self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)
A:transformers.modeling_xlnet.loss_fct_cls->torch.nn.BCEWithLogitsLoss()
A:transformers.modeling_xlnet.cls_loss->loss_fct_cls(cls_logits, is_impossible)
A:transformers.modeling_xlnet.(bsz, slen, hsz)->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states)).size()
A:transformers.modeling_xlnet.start_log_probs->torch.nn.functional.softmax(start_logits, dim=-1)
A:transformers.modeling_xlnet.(start_top_log_probs, start_top_index)->torch.topk(start_log_probs, self.start_n_top, dim=-1)
A:transformers.modeling_xlnet.start_top_index_exp->start_top_index.unsqueeze(-1).expand(-1, -1, hsz)
A:transformers.modeling_xlnet.start_states->torch.einsum('blh,bl->bh', hidden_states, start_log_probs)
A:transformers.modeling_xlnet.hidden_states_expanded->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states)).unsqueeze(2).expand_as(start_states)
A:transformers.modeling_xlnet.end_log_probs->torch.nn.functional.softmax(end_logits, dim=1)
A:transformers.modeling_xlnet.(end_top_log_probs, end_top_index)->torch.topk(end_log_probs, self.end_n_top, dim=1)
A:transformers.modeling_xlnet.end_top_log_probs->end_top_log_probs.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_xlnet.end_top_index->end_top_index.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
transformers.XLNetForMultipleChoice(self,config)
transformers.XLNetForMultipleChoice.forward(self,input_ids,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,labels=None,head_mask=None)
transformers.XLNetForQuestionAnswering(self,config)
transformers.XLNetForQuestionAnswering.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None)
transformers.XLNetForQuestionAnsweringSimple(self,config)
transformers.XLNetForQuestionAnsweringSimple.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,start_positions=None,end_positions=None)
transformers.XLNetForSequenceClassification(self,config)
transformers.XLNetForSequenceClassification.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,labels=None)
transformers.XLNetLMHeadModel(self,config)
transformers.XLNetLMHeadModel.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,labels=None)
transformers.XLNetLMHeadModel.tie_weights(self)
transformers.XLNetModel(self,config)
transformers.XLNetModel._prune_heads(self,heads_to_prune)
transformers.XLNetModel._resize_token_embeddings(self,new_num_tokens)
transformers.XLNetModel.cache_mem(self,curr_out,prev_mem)
transformers.XLNetModel.create_mask(self,qlen,mlen)
transformers.XLNetModel.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None)
transformers.XLNetModel.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.XLNetModel.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.XLNetPreTrainedModel(PreTrainedModel)
transformers.XLNetPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_xlnet(model,config,tf_path)
transformers.modeling_xlnet.XLNetFeedForward(self,config)
transformers.modeling_xlnet.XLNetFeedForward.__init__(self,config)
transformers.modeling_xlnet.XLNetFeedForward.forward(self,inp)
transformers.modeling_xlnet.XLNetForMultipleChoice(self,config)
transformers.modeling_xlnet.XLNetForMultipleChoice.__init__(self,config)
transformers.modeling_xlnet.XLNetForMultipleChoice.forward(self,input_ids,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,labels=None,head_mask=None)
transformers.modeling_xlnet.XLNetForQuestionAnswering(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnswering.__init__(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnswering.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple.__init__(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,start_positions=None,end_positions=None)
transformers.modeling_xlnet.XLNetForSequenceClassification(self,config)
transformers.modeling_xlnet.XLNetForSequenceClassification.__init__(self,config)
transformers.modeling_xlnet.XLNetForSequenceClassification.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,labels=None)
transformers.modeling_xlnet.XLNetLMHeadModel(self,config)
transformers.modeling_xlnet.XLNetLMHeadModel.__init__(self,config)
transformers.modeling_xlnet.XLNetLMHeadModel.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,labels=None)
transformers.modeling_xlnet.XLNetLMHeadModel.tie_weights(self)
transformers.modeling_xlnet.XLNetLayer(self,config)
transformers.modeling_xlnet.XLNetLayer.__init__(self,config)
transformers.modeling_xlnet.XLNetLayer.forward(self,output_h,output_g,attn_mask_h,attn_mask_g,r,seg_mat,mems=None,target_mapping=None,head_mask=None)
transformers.modeling_xlnet.XLNetModel(self,config)
transformers.modeling_xlnet.XLNetModel.__init__(self,config)
transformers.modeling_xlnet.XLNetModel._prune_heads(self,heads_to_prune)
transformers.modeling_xlnet.XLNetModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_xlnet.XLNetModel.cache_mem(self,curr_out,prev_mem)
transformers.modeling_xlnet.XLNetModel.create_mask(self,qlen,mlen)
transformers.modeling_xlnet.XLNetModel.forward(self,input_ids,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None)
transformers.modeling_xlnet.XLNetModel.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.modeling_xlnet.XLNetModel.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.modeling_xlnet.XLNetPreTrainedModel(PreTrainedModel)
transformers.modeling_xlnet.XLNetPreTrainedModel._init_weights(self,module)
transformers.modeling_xlnet.XLNetRelativeAttention(self,config)
transformers.modeling_xlnet.XLNetRelativeAttention.__init__(self,config)
transformers.modeling_xlnet.XLNetRelativeAttention.forward(self,h,g,attn_mask_h,attn_mask_g,r,seg_mat,mems=None,target_mapping=None,head_mask=None)
transformers.modeling_xlnet.XLNetRelativeAttention.post_attention(self,h,attn_vec,residual=True)
transformers.modeling_xlnet.XLNetRelativeAttention.prune_heads(self,heads)
transformers.modeling_xlnet.XLNetRelativeAttention.rel_attn_core(self,q_head,k_head_h,v_head_h,k_head_r,seg_mat=None,attn_mask=None,head_mask=None)
transformers.modeling_xlnet.XLNetRelativeAttention.rel_shift(x,klen=-1)
transformers.modeling_xlnet.build_tf_xlnet_to_pytorch_map(model,config,tf_weights=None)
transformers.modeling_xlnet.gelu(x)
transformers.modeling_xlnet.load_tf_weights_in_xlnet(model,config,tf_path)
transformers.modeling_xlnet.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/modeling_tf_utils.py----------------------------------------
A:transformers.modeling_tf_utils.logger->logging.getLogger(__name__)
A:transformers.modeling_tf_utils.output_model_file->os.path.join(save_directory, TF2_WEIGHTS_NAME)
A:transformers.modeling_tf_utils.config->kwargs.pop('config', None)
A:transformers.modeling_tf_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_tf_utils.from_pt->kwargs.pop('from_pt', False)
A:transformers.modeling_tf_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_tf_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_tf_utils.(config, model_kwargs)->cls.config_class.from_pretrained(pretrained_model_name_or_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, **kwargs)
A:transformers.modeling_tf_utils.archive_file->os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)
A:transformers.modeling_tf_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
A:transformers.modeling_tf_utils.model->cls(config, *model_args, **model_kwargs)
A:transformers.modeling_tf_utils.inputs->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.modeling_tf_utils.ret->model(inputs, training=False)
A:transformers.modeling_tf_utils.self.weight->self.add_weight('weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_utils.self.bias->self.add_weight('bias', shape=[1, self.nf], initializer=tf.zeros_initializer())
A:transformers.modeling_tf_utils.x->tensorflow.reshape(inputs, [-1, self.hidden_size])
A:transformers.modeling_tf_utils.logits->tensorflow.matmul(x, self.weight, transpose_b=True)
A:transformers.modeling_tf_utils.self.summary->tensorflow.keras.layers.Dense(num_classes, kernel_initializer=get_initializer(initializer_range), name='summary')
A:transformers.modeling_tf_utils.self.first_dropout->tensorflow.keras.layers.Dropout(config.summary_first_dropout)
A:transformers.modeling_tf_utils.self.last_dropout->tensorflow.keras.layers.Dropout(config.summary_last_dropout)
A:transformers.modeling_tf_utils.input_ids->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]).get('input_ids')
A:transformers.modeling_tf_utils.cls_index->tensorflow.fill(hidden_shape[:-2], hidden_shape[-2] - 1)
A:transformers.modeling_tf_utils.output->self.last_dropout(output)
A:transformers.modeling_tf_utils.hidden_shape->shape_list(hidden_states)
A:transformers.modeling_tf_utils.cls_shape->shape_list(cls_index)
A:transformers.modeling_tf_utils.static->tensorflow.reshape(inputs, [-1, self.hidden_size]).shape.as_list()
A:transformers.modeling_tf_utils.dynamic->tensorflow.shape(x)
transformers.TFPreTrainedModel(self,config,*inputs,**kwargs)
transformers.TFPreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)
transformers.TFPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFPreTrainedModel.prune_heads(self,heads_to_prune)
transformers.TFPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)
transformers.TFPreTrainedModel.save_pretrained(self,save_directory)
transformers.TFSequenceSummary(self,config,initializer_range=0.02,**kwargs)
transformers.TFSequenceSummary.call(self,inputs,training=False)
transformers.TFSharedEmbeddings(self,vocab_size,hidden_size,initializer_range=None,**kwargs)
transformers.TFSharedEmbeddings._embedding(self,input_ids)
transformers.TFSharedEmbeddings._linear(self,inputs)
transformers.TFSharedEmbeddings.build(self,input_shape)
transformers.TFSharedEmbeddings.call(self,inputs,mode='embedding')
transformers.modeling_tf_utils.TFConv1D(self,nf,nx,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFConv1D.__init__(self,nf,nx,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFConv1D.build(self,input_shape)
transformers.modeling_tf_utils.TFConv1D.call(self,x)
transformers.modeling_tf_utils.TFPreTrainedModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)
transformers.modeling_tf_utils.TFPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.prune_heads(self,heads_to_prune)
transformers.modeling_tf_utils.TFPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)
transformers.modeling_tf_utils.TFPreTrainedModel.save_pretrained(self,save_directory)
transformers.modeling_tf_utils.TFSequenceSummary(self,config,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFSequenceSummary.__init__(self,config,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFSequenceSummary.call(self,inputs,training=False)
transformers.modeling_tf_utils.TFSharedEmbeddings(self,vocab_size,hidden_size,initializer_range=None,**kwargs)
transformers.modeling_tf_utils.TFSharedEmbeddings.__init__(self,vocab_size,hidden_size,initializer_range=None,**kwargs)
transformers.modeling_tf_utils.TFSharedEmbeddings._embedding(self,input_ids)
transformers.modeling_tf_utils.TFSharedEmbeddings._linear(self,inputs)
transformers.modeling_tf_utils.TFSharedEmbeddings.build(self,input_shape)
transformers.modeling_tf_utils.TFSharedEmbeddings.call(self,inputs,mode='embedding')
transformers.modeling_tf_utils.get_initializer(initializer_range=0.02)
transformers.modeling_tf_utils.shape_list(x)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/data/processors/glue.py----------------------------------------
A:transformers.data.processors.glue.logger->logging.getLogger(__name__)
A:transformers.data.processors.glue.processor->glue_processors[task]()
A:transformers.data.processors.glue.label_list->glue_processors[task]().get_labels()
A:transformers.data.processors.glue.example->glue_processors[task]().get_example_from_tensor_dict(example)
A:transformers.data.processors.glue.inputs->tokenizer.encode_plus(example.text_a, example.text_b, add_special_tokens=True, max_length=max_length)
A:transformers.data.processors.glue.label->float(example.label)
transformers.data.processors.glue.ColaProcessor(DataProcessor)
transformers.data.processors.glue.ColaProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.ColaProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.ColaProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.ColaProcessor.get_labels(self)
transformers.data.processors.glue.ColaProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.MnliMismatchedProcessor(MnliProcessor)
transformers.data.processors.glue.MnliMismatchedProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor(DataProcessor)
transformers.data.processors.glue.MnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.MnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.MnliProcessor.get_labels(self)
transformers.data.processors.glue.MnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor(DataProcessor)
transformers.data.processors.glue.MrpcProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.MrpcProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.MrpcProcessor.get_labels(self)
transformers.data.processors.glue.MrpcProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.QnliProcessor(DataProcessor)
transformers.data.processors.glue.QnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.QnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.QnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.QnliProcessor.get_labels(self)
transformers.data.processors.glue.QnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor(DataProcessor)
transformers.data.processors.glue.QqpProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.QqpProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.QqpProcessor.get_labels(self)
transformers.data.processors.glue.QqpProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor(DataProcessor)
transformers.data.processors.glue.RteProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.RteProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.RteProcessor.get_labels(self)
transformers.data.processors.glue.RteProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor(DataProcessor)
transformers.data.processors.glue.Sst2Processor._create_examples(self,lines,set_type)
transformers.data.processors.glue.Sst2Processor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.Sst2Processor.get_labels(self)
transformers.data.processors.glue.Sst2Processor.get_train_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor(DataProcessor)
transformers.data.processors.glue.StsbProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.StsbProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.StsbProcessor.get_labels(self)
transformers.data.processors.glue.StsbProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor(DataProcessor)
transformers.data.processors.glue.WnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.WnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.WnliProcessor.get_labels(self)
transformers.data.processors.glue.WnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.glue_convert_examples_to_features(examples,tokenizer,max_length=512,task=None,label_list=None,output_mode=None,pad_on_left=False,pad_token=0,pad_token_segment_id=0,mask_padding_with_zero=True)
transformers.glue_convert_examples_to_features(examples,tokenizer,max_length=512,task=None,label_list=None,output_mode=None,pad_on_left=False,pad_token=0,pad_token_segment_id=0,mask_padding_with_zero=True)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/data/processors/utils.py----------------------------------------
A:transformers.data.processors.utils.output->copy.deepcopy(self.__dict__)
A:transformers.data.processors.utils.reader->csv.reader(f, delimiter='\t', quotechar=quotechar)
A:transformers.data.processors.utils.line->list((unicode(cell, 'utf-8') for cell in line))
transformers.DataProcessor(object)
transformers.DataProcessor._read_tsv(cls,input_file,quotechar=None)
transformers.DataProcessor.get_dev_examples(self,data_dir)
transformers.DataProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.DataProcessor.get_labels(self)
transformers.DataProcessor.get_train_examples(self,data_dir)
transformers.InputExample(self,guid,text_a,text_b=None,label=None)
transformers.InputExample.__repr__(self)
transformers.InputExample.to_dict(self)
transformers.InputExample.to_json_string(self)
transformers.InputFeatures(self,input_ids,attention_mask,token_type_ids,label)
transformers.InputFeatures.__repr__(self)
transformers.InputFeatures.to_dict(self)
transformers.InputFeatures.to_json_string(self)
transformers.data.processors.utils.DataProcessor(object)
transformers.data.processors.utils.DataProcessor._read_tsv(cls,input_file,quotechar=None)
transformers.data.processors.utils.DataProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.utils.DataProcessor.get_labels(self)
transformers.data.processors.utils.DataProcessor.get_train_examples(self,data_dir)
transformers.data.processors.utils.InputExample(self,guid,text_a,text_b=None,label=None)
transformers.data.processors.utils.InputExample.__init__(self,guid,text_a,text_b=None,label=None)
transformers.data.processors.utils.InputExample.__repr__(self)
transformers.data.processors.utils.InputExample.to_dict(self)
transformers.data.processors.utils.InputExample.to_json_string(self)
transformers.data.processors.utils.InputFeatures(self,input_ids,attention_mask,token_type_ids,label)
transformers.data.processors.utils.InputFeatures.__init__(self,input_ids,attention_mask,token_type_ids,label)
transformers.data.processors.utils.InputFeatures.__repr__(self)
transformers.data.processors.utils.InputFeatures.to_dict(self)
transformers.data.processors.utils.InputFeatures.to_json_string(self)


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/data/processors/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers2.1.0/data/metrics/__init__.py----------------------------------------
A:transformers.data.metrics.__init__.logger->logging.getLogger(__name__)
A:transformers.data.metrics.__init__.acc->simple_accuracy(preds, labels)
A:transformers.data.metrics.__init__.f1->f1_score(y_true=labels, y_pred=preds)
transformers.data.metrics.__init__.is_sklearn_available()

