
----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/integrations.py----------------------------------------
A:transformers.integrations.logger->utils.logging.get_logger(__name__)
A:transformers.integrations.checkpoint->os.path.join(checkpoint_dir, subdir)
A:transformers.integrations.metrics->local_trainer.evaluate()
A:transformers.integrations.trainer.objective->trainer.compute_objective(metrics)
A:transformers.integrations.timeout->kwargs.pop('timeout', None)
A:transformers.integrations.n_jobs->kwargs.pop('n_jobs', 1)
A:transformers.integrations.study->optuna.create_study(direction=direction, **kwargs)
A:transformers.integrations.local_trainer.objective->local_trainer.compute_objective(metrics)
A:transformers.integrations.trainer._memory_tracker->TrainerMemoryTracker(skip_memory_metrics=True)
A:transformers.integrations._tb_writer->trainer.pop_callback(TensorBoardCallback)
A:transformers.integrations.gpus_per_trial->kwargs['resources_per_trial'].get('gpu', 0)
A:transformers.integrations.kwargs['progress_reporter']->CLIReporter(metric_columns=['objective'])
A:transformers.integrations.analysis->ray.tune.run(ray.tune.with_parameters(_objective, local_trainer=trainer), config=trainer.hp_space(None), num_samples=n_trials, **kwargs)
A:transformers.integrations.best_trial->ray.tune.run(ray.tune.with_parameters(_objective, local_trainer=trainer), config=trainer.hp_space(None), num_samples=n_trials, **kwargs).get_best_trial(metric='objective', mode=direction[:3])
A:transformers.integrations.best_run->BestRun(best_trial.trial_id, best_trial.last_result['objective'], best_trial.config)
A:transformers.integrations.eval_prefix_len->len(eval_prefix)
A:transformers.integrations.has_tensorboard->is_tensorboard_available()
A:transformers.integrations.self.tb_writer->self._SummaryWriter(log_dir=log_dir)
A:transformers.integrations.log_dir->os.path.join(args.logging_dir, trial_name)
A:transformers.integrations.model_config_json->model.config.to_json_string()
A:transformers.integrations.logs->rewrite_logs(logs)
A:transformers.integrations.has_wandb->is_wandb_available()
A:transformers.integrations.model_config->model.config.to_dict()
A:transformers.integrations.fake_trainer->Trainer(args=args, model=model, tokenizer=tokenizer)
A:transformers.integrations.artifact->self._wandb.Artifact(name=f'model-{self._wandb.run.id}', type='model', metadata=metadata)
A:transformers.integrations.comet_mode->os.getenv('COMET_MODE', 'ONLINE').upper()
A:transformers.integrations.experiment->comet_ml.config.get_global_experiment()
A:transformers.integrations.args['offline_directory']->os.getenv('COMET_OFFLINE_DIRECTORY', './')
A:transformers.integrations.self.azureml_run->azureml.core.run.Run.get_context()
A:transformers.integrations.log_artifacts->os.getenv('HF_MLFLOW_LOG_ARTIFACTS', 'FALSE').upper()
A:transformers.integrations.combined_dict->args.to_dict()
A:transformers.integrations.combined_dict_items->list(combined_dict.items())
A:transformers.integrations.self._neptune_run->self._neptune.init(project=os.getenv('NEPTUNE_PROJECT'), api_token=os.getenv('NEPTUNE_API_TOKEN'), mode=os.getenv('NEPTUNE_CONNECTION_MODE', 'async'), name=os.getenv('NEPTUNE_RUN_NAME', None))
A:transformers.integrations.stop_timeout->os.getenv('NEPTUNE_STOP_TIMEOUT')
A:transformers.integrations.self.tracker->self._codecarbon.EmissionsTracker(output_dir=args.output_dir)
transformers.integrations.AzureMLCallback(self,azureml_run=None)
transformers.integrations.AzureMLCallback.__init__(self,azureml_run=None)
transformers.integrations.AzureMLCallback.on_init_end(self,args,state,control,**kwargs)
transformers.integrations.AzureMLCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.integrations.CodeCarbonCallback(self)
transformers.integrations.CodeCarbonCallback.__init__(self)
transformers.integrations.CodeCarbonCallback.on_init_end(self,args,state,control,**kwargs)
transformers.integrations.CodeCarbonCallback.on_train_begin(self,args,state,control,model=None,**kwargs)
transformers.integrations.CodeCarbonCallback.on_train_end(self,args,state,control,**kwargs)
transformers.integrations.CometCallback(self)
transformers.integrations.CometCallback.__init__(self)
transformers.integrations.CometCallback.on_log(self,args,state,control,model=None,logs=None,**kwargs)
transformers.integrations.CometCallback.on_train_begin(self,args,state,control,model=None,**kwargs)
transformers.integrations.CometCallback.setup(self,args,state,model)
transformers.integrations.MLflowCallback(self)
transformers.integrations.MLflowCallback.__del__(self)
transformers.integrations.MLflowCallback.__init__(self)
transformers.integrations.MLflowCallback.on_log(self,args,state,control,logs,model=None,**kwargs)
transformers.integrations.MLflowCallback.on_train_begin(self,args,state,control,model=None,**kwargs)
transformers.integrations.MLflowCallback.on_train_end(self,args,state,control,**kwargs)
transformers.integrations.MLflowCallback.setup(self,args,state,model)
transformers.integrations.NeptuneCallback(self)
transformers.integrations.NeptuneCallback.__del__(self)
transformers.integrations.NeptuneCallback.__init__(self)
transformers.integrations.NeptuneCallback.on_log(self,args,state,control,logs,model=None,**kwargs)
transformers.integrations.NeptuneCallback.on_train_begin(self,args,state,control,model=None,**kwargs)
transformers.integrations.NeptuneCallback.setup(self,args,state,model)
transformers.integrations.TensorBoardCallback(self,tb_writer=None)
transformers.integrations.TensorBoardCallback.__init__(self,tb_writer=None)
transformers.integrations.TensorBoardCallback._init_summary_writer(self,args,log_dir=None)
transformers.integrations.TensorBoardCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.integrations.TensorBoardCallback.on_train_begin(self,args,state,control,**kwargs)
transformers.integrations.TensorBoardCallback.on_train_end(self,args,state,control,**kwargs)
transformers.integrations.WandbCallback(self)
transformers.integrations.WandbCallback.__init__(self)
transformers.integrations.WandbCallback.on_log(self,args,state,control,model=None,logs=None,**kwargs)
transformers.integrations.WandbCallback.on_train_begin(self,args,state,control,model=None,**kwargs)
transformers.integrations.WandbCallback.on_train_end(self,args,state,control,model=None,tokenizer=None,**kwargs)
transformers.integrations.WandbCallback.setup(self,args,state,model,**kwargs)
transformers.integrations.default_hp_search_backend()
transformers.integrations.get_available_reporting_integrations()
transformers.integrations.get_reporting_integration_callbacks(report_to)
transformers.integrations.hp_params(trial)
transformers.integrations.is_azureml_available()
transformers.integrations.is_codecarbon_available()
transformers.integrations.is_comet_available()
transformers.integrations.is_fairscale_available()
transformers.integrations.is_mlflow_available()
transformers.integrations.is_neptune_available()
transformers.integrations.is_optuna_available()
transformers.integrations.is_ray_available()
transformers.integrations.is_ray_tune_available()
transformers.integrations.is_tensorboard_available()
transformers.integrations.is_wandb_available()
transformers.integrations.rewrite_logs(d)
transformers.integrations.run_hp_search_optuna(trainer,n_trials:int,direction:str,**kwargs)->BestRun
transformers.integrations.run_hp_search_ray(trainer,n_trials:int,direction:str,**kwargs)->BestRun
transformers.is_comet_available()
transformers.is_optuna_available()
transformers.is_ray_available()
transformers.is_ray_tune_available()
transformers.is_tensorboard_available()
transformers.is_wandb_available()


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/tokenization_utils_base.py----------------------------------------
A:transformers.tokenization_utils_base.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_utils_base.VERY_LARGE_INTEGER->int(1e+30)
A:transformers.tokenization_utils_base.LARGE_INTEGER->int(1e+20)
A:transformers.tokenization_utils_base.span->self._encodings[batch_index].word_to_tokens(word_index, sequence_index)
A:transformers.tokenization_utils_base.tensor_type->TensorType(tensor_type)
A:transformers.tokenization_utils_base.tensor->as_tensor(value)
A:transformers.tokenization_utils_base.self._bos_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.self._eos_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.self._unk_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.self._sep_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.self._pad_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.self._cls_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.self._mask_token->self.convert_tokens_to_ids(value)
A:transformers.tokenization_utils_base.attr_value->getattr(self, '_' + attr)
A:transformers.tokenization_utils_base.set_attr[attr]->str(attr_value)
A:transformers.tokenization_utils_base.all_toks->list(OrderedDict.fromkeys(all_toks))
A:transformers.tokenization_utils_base.all_ids->self.convert_tokens_to_ids(all_toks)
A:transformers.tokenization_utils_base.self.init_kwargs->copy.deepcopy(kwargs)
A:transformers.tokenization_utils_base.self.name_or_path->kwargs.pop('name_or_path', '')
A:transformers.tokenization_utils_base.model_max_length->kwargs.pop('model_max_length', kwargs.pop('max_len', None))
A:transformers.tokenization_utils_base.self.padding_side->kwargs.pop('padding_side', self.padding_side)
A:transformers.tokenization_utils_base.self.model_input_names->kwargs.pop('model_input_names', self.model_input_names)
A:transformers.tokenization_utils_base.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.tokenization_utils_base.force_download->kwargs.pop('force_download', False)
A:transformers.tokenization_utils_base.resume_download->kwargs.pop('resume_download', False)
A:transformers.tokenization_utils_base.proxies->kwargs.pop('proxies', None)
A:transformers.tokenization_utils_base.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.tokenization_utils_base.use_auth_token->kwargs.pop('use_auth_token', None)
A:transformers.tokenization_utils_base.revision->kwargs.pop('revision', None)
A:transformers.tokenization_utils_base.subfolder->kwargs.pop('subfolder', None)
A:transformers.tokenization_utils_base.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.tokenization_utils_base.from_auto_class->kwargs.pop('_from_auto', False)
A:transformers.tokenization_utils_base.pretrained_model_name_or_path->str(pretrained_model_name_or_path)
A:transformers.tokenization_utils_base.full_file_name->hf_bucket_url(pretrained_model_name_or_path, filename=file_name, subfolder=subfolder, revision=revision, mirror=None)
A:transformers.tokenization_utils_base.resolved_vocab_files[file_id]->cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent)
A:transformers.tokenization_utils_base.from_slow->kwargs.get('from_slow', False)
A:transformers.tokenization_utils_base.slow_tokenizer->cls.slow_tokenizer_class._from_pretrained(copy.deepcopy(resolved_vocab_files), pretrained_model_name_or_path, copy.deepcopy(init_configuration), *init_inputs, **copy.deepcopy(kwargs))
A:transformers.tokenization_utils_base.tokenizer_config_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + TOKENIZER_CONFIG_FILE)
A:transformers.tokenization_utils_base.init_kwargs->convert_added_tokens(init_kwargs)
A:transformers.tokenization_utils_base.saved_init_inputs->convert_added_tokens(init_kwargs).pop('init_inputs', ())
A:transformers.tokenization_utils_base.init_kwargs['model_max_length']->min(init_kwargs.get('model_max_length', int(1e+30)), model_max_length)
A:transformers.tokenization_utils_base.added_tokens_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + ADDED_TOKENS_FILE)
A:transformers.tokenization_utils_base.tokenizer->cls(*init_inputs, **init_kwargs)
A:transformers.tokenization_utils_base.special_tokens_map_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + SPECIAL_TOKENS_MAP_FILE)
A:transformers.tokenization_utils_base.special_tokens_map->json.load(special_tokens_map_handle)
A:transformers.tokenization_utils_base.value->AddedToken(**value)
A:transformers.tokenization_utils_base.added_tok_encoder->json.load(added_tokens_handle)
A:transformers.tokenization_utils_base.added_tok_encoder_sorted->list(sorted(added_tok_encoder.items(), key=lambda x: x[1]))
A:transformers.tokenization_utils_base.added_tokens->cls(*init_inputs, **init_kwargs).sanitize_special_tokens()
A:transformers.tokenization_utils_base.commit_message->kwargs.pop('commit_message', None)
A:transformers.tokenization_utils_base.repo->self._create_or_get_repo(save_directory, **kwargs)
A:transformers.tokenization_utils_base.tokenizer_config->convert_added_tokens(tokenizer_config, add_type_field=True)
A:transformers.tokenization_utils_base.tokenizer_config['init_inputs']->copy.deepcopy(self.init_inputs)
A:transformers.tokenization_utils_base.out->obj.__getstate__()
A:transformers.tokenization_utils_base.write_dict->convert_added_tokens(self.special_tokens_map_extended, add_type_field=False)
A:transformers.tokenization_utils_base.save_files->self._save_pretrained(save_directory=save_directory, file_names=file_names, legacy_format=legacy_format, filename_prefix=filename_prefix)
A:transformers.tokenization_utils_base.url->self._push_to_hub(repo, commit_message=commit_message)
A:transformers.tokenization_utils_base.save_directory->str(save_directory)
A:transformers.tokenization_utils_base.added_vocab->self.get_added_vocab()
A:transformers.tokenization_utils_base.out_str->json.dumps(added_vocab, ensure_ascii=False)
A:transformers.tokenization_utils_base.vocab_files->self.save_vocabulary(save_directory, filename_prefix=filename_prefix)
A:transformers.tokenization_utils_base.encoded_inputs->self.pad(encoded_inputs, max_length=max_length, padding=padding_strategy.value, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.tokenization_utils_base.old_truncation_strategy->kwargs.pop('truncation_strategy', 'do_not_truncate')
A:transformers.tokenization_utils_base.old_pad_to_max_length->kwargs.pop('pad_to_max_length', False)
A:transformers.tokenization_utils_base.padding_strategy->PaddingStrategy(padding)
A:transformers.tokenization_utils_base.truncation_strategy->TruncationStrategy(truncation_strategy)
A:transformers.tokenization_utils_base.is_batched->isinstance(text, (list, tuple))
A:transformers.tokenization_utils_base.(padding_strategy, truncation_strategy, max_length, kwargs)->self._get_padding_truncation_strategies(padding=padding, truncation=truncation, max_length=max_length, pad_to_multiple_of=pad_to_multiple_of, verbose=verbose, **kwargs)
A:transformers.tokenization_utils_base.encoded_inputs[key]->to_py_obj(value)
A:transformers.tokenization_utils_base.(padding_strategy, _, max_length, _)->self._get_padding_truncation_strategies(padding=padding, max_length=max_length, verbose=verbose)
A:transformers.tokenization_utils_base.batch_size->len(required_input)
A:transformers.tokenization_utils_base.max_length->len(required_input)
A:transformers.tokenization_utils_base.inputs->dict(((k, v[i]) for (k, v) in encoded_inputs.items()))
A:transformers.tokenization_utils_base.outputs->self._pad(inputs, max_length=max_length, padding_strategy=padding_strategy, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.tokenization_utils_base.pair->bool(pair_ids is not None)
A:transformers.tokenization_utils_base.len_ids->len(ids)
A:transformers.tokenization_utils_base.(ids, pair_ids, overflowing_tokens)->self.truncate_sequences(ids, pair_ids=pair_ids, num_tokens_to_remove=total_len - max_length, truncation_strategy=truncation_strategy, stride=stride)
A:transformers.tokenization_utils_base.sequence->self.build_inputs_with_special_tokens(ids, pair_ids)
A:transformers.tokenization_utils_base.token_type_ids->self.create_token_type_ids_from_sequences(ids, pair_ids)
A:transformers.tokenization_utils_base.encoded_inputs['special_tokens_mask']->self.get_special_tokens_mask(ids, pair_ids)
A:transformers.tokenization_utils_base.encoded_inputs['length']->len(encoded_inputs['input_ids'])
A:transformers.tokenization_utils_base.batch_outputs->BatchEncoding(encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis)
A:transformers.tokenization_utils_base.window_len->min(len(pair_ids), stride + num_tokens_to_remove)
A:transformers.tokenization_utils_base.token_ids->to_py_obj(token_ids)
A:transformers.tokenization_utils_base.out_string->out_string.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ',').replace(" ' ", "'").replace(" n't", "n't").replace(" 'm", "'m").replace(" 's", "'s").replace(" 've", "'ve").replace(" 're", "'re").replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ',').replace(" ' ", "'").replace(" n't", "n't").replace(" 'm", "'m").replace(" 's", "'s").replace(" 've", "'ve").replace(" 're", "'re")
A:transformers.tokenization_utils_base.model_inputs->self(src_texts, add_special_tokens=True, return_tensors=return_tensors, max_length=max_length, padding=padding, truncation=truncation, **kwargs)
A:transformers.tokenization_utils_base.labels->self(tgt_texts, add_special_tokens=True, return_tensors=return_tensors, padding=padding, max_length=max_target_length, truncation=truncation, **kwargs)
transformers.BatchEncoding(self,data:Optional[Dict[str,Any]]=None,encoding:Optional[Union[EncodingFast,Sequence[EncodingFast]]]=None,tensor_type:Union[None,str,TensorType]=None,prepend_batch_axis:bool=False,n_sequences:Optional[int]=None)
transformers.BatchEncoding.__getattr__(self,item:str)
transformers.BatchEncoding.__getitem__(self,item:Union[int,str])->Union[Any, EncodingFast]
transformers.BatchEncoding.__getstate__(self)
transformers.BatchEncoding.__setstate__(self,state)
transformers.BatchEncoding.char_to_token(self,batch_or_char_index:int,char_index:Optional[int]=None,sequence_index:int=0)->int
transformers.BatchEncoding.char_to_word(self,batch_or_char_index:int,char_index:Optional[int]=None,sequence_index:int=0)->int
transformers.BatchEncoding.convert_to_tensors(self,tensor_type:Optional[Union[str,TensorType]]=None,prepend_batch_axis:bool=False)
transformers.BatchEncoding.encodings(self)->Optional[List[EncodingFast]]
transformers.BatchEncoding.is_fast(self)->bool
transformers.BatchEncoding.items(self)
transformers.BatchEncoding.keys(self)
transformers.BatchEncoding.n_sequences(self)->Optional[int]
transformers.BatchEncoding.sequence_ids(self,batch_index:int=0)->List[Optional[int]]
transformers.BatchEncoding.to(self,device:Union[str,'torch.device'])->'BatchEncoding'
transformers.BatchEncoding.token_to_chars(self,batch_or_token_index:int,token_index:Optional[int]=None)->CharSpan
transformers.BatchEncoding.token_to_sequence(self,batch_or_token_index:int,token_index:Optional[int]=None)->int
transformers.BatchEncoding.token_to_word(self,batch_or_token_index:int,token_index:Optional[int]=None)->int
transformers.BatchEncoding.tokens(self,batch_index:int=0)->List[str]
transformers.BatchEncoding.values(self)
transformers.BatchEncoding.word_ids(self,batch_index:int=0)->List[Optional[int]]
transformers.BatchEncoding.word_to_chars(self,batch_or_word_index:int,word_index:Optional[int]=None,sequence_index:int=0)->CharSpan
transformers.BatchEncoding.word_to_tokens(self,batch_or_word_index:int,word_index:Optional[int]=None,sequence_index:int=0)->Optional[TokenSpan]
transformers.BatchEncoding.words(self,batch_index:int=0)->List[Optional[int]]
transformers.CharSpan(NamedTuple)
transformers.PreTrainedTokenizerBase(self,**kwargs)
transformers.PreTrainedTokenizerBase.__repr__(self)->str
transformers.PreTrainedTokenizerBase._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase._decode(self,token_ids:Union[int,List[int]],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.PreTrainedTokenizerBase._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase._eventual_warn_about_too_long_sequence(self,ids:List[int],max_length:Optional[int],verbose:bool)
transformers.PreTrainedTokenizerBase._from_pretrained(cls,resolved_vocab_files,pretrained_model_name_or_path,init_configuration,*init_inputs,**kwargs)
transformers.PreTrainedTokenizerBase._get_padding_truncation_strategies(self,padding=False,truncation=False,max_length=None,pad_to_multiple_of=None,verbose=True,**kwargs)
transformers.PreTrainedTokenizerBase._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.PreTrainedTokenizerBase._save_pretrained(self,save_directory:Union[str,os.PathLike],file_names:Tuple[str],legacy_format:Optional[bool]=None,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.PreTrainedTokenizerBase.as_target_tokenizer(self)
transformers.PreTrainedTokenizerBase.batch_decode(self,sequences:Union[List[int],List[List[int]],'np.ndarray','torch.Tensor','tf.Tensor'],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->List[str]
transformers.PreTrainedTokenizerBase.batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.PreTrainedTokenizerBase.clean_up_tokenization(out_string:str)->str
transformers.PreTrainedTokenizerBase.convert_tokens_to_string(self,tokens:List[str])->str
transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.PreTrainedTokenizerBase.decode(self,token_ids:Union[int,List[int],'np.ndarray','torch.Tensor','tf.Tensor'],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.PreTrainedTokenizerBase.encode(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,return_tensors:Optional[Union[str,TensorType]]=None,**kwargs)->List[int]
transformers.PreTrainedTokenizerBase.encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],*init_inputs,**kwargs)
transformers.PreTrainedTokenizerBase.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PreTrainedTokenizerBase.get_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizerBase.max_len_sentences_pair(self)->int
transformers.PreTrainedTokenizerBase.max_len_sentences_pair(self,value)->int
transformers.PreTrainedTokenizerBase.max_len_single_sentence(self)->int
transformers.PreTrainedTokenizerBase.max_len_single_sentence(self,value)->int
transformers.PreTrainedTokenizerBase.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.PreTrainedTokenizerBase.pad(self,encoded_inputs:Union[BatchEncoding,List[BatchEncoding],Dict[str,EncodedInput],Dict[str,List[EncodedInput]],List[Dict[str,EncodedInput]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None,verbose:bool=True)->BatchEncoding
transformers.PreTrainedTokenizerBase.prepare_for_model(self,ids:List[int],pair_ids:Optional[List[int]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str=None,truncation:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.save_pretrained(self,save_directory:Union[str,os.PathLike],legacy_format:Optional[bool]=None,filename_prefix:Optional[str]=None,push_to_hub:bool=False,**kwargs)->Tuple[str]
transformers.PreTrainedTokenizerBase.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.PreTrainedTokenizerBase.tokenize(self,text:str,pair:Optional[str]=None,add_special_tokens:bool=False,**kwargs)->List[str]
transformers.PreTrainedTokenizerBase.truncate_sequences(self,ids:List[int],pair_ids:Optional[List[int]]=None,num_tokens_to_remove:int=0,truncation_strategy:Union[str,TruncationStrategy]='longest_first',stride:int=0)->Tuple[List[int], List[int], List[int]]
transformers.SpecialTokensMixin(self,verbose=True,**kwargs)
transformers.SpecialTokensMixin._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.SpecialTokensMixin.add_special_tokens(self,special_tokens_dict:Dict[str,Union[str,AddedToken]])->int
transformers.SpecialTokensMixin.add_tokens(self,new_tokens:Union[str,AddedToken,List[Union[str,AddedToken]]],special_tokens:bool=False)->int
transformers.SpecialTokensMixin.additional_special_tokens(self)->List[str]
transformers.SpecialTokensMixin.additional_special_tokens(self,value)
transformers.SpecialTokensMixin.additional_special_tokens_ids(self)->List[int]
transformers.SpecialTokensMixin.additional_special_tokens_ids(self,values)
transformers.SpecialTokensMixin.all_special_ids(self)->List[int]
transformers.SpecialTokensMixin.all_special_tokens(self)->List[str]
transformers.SpecialTokensMixin.all_special_tokens_extended(self)->List[Union[str, AddedToken]]
transformers.SpecialTokensMixin.bos_token(self)->str
transformers.SpecialTokensMixin.bos_token(self,value)
transformers.SpecialTokensMixin.bos_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.bos_token_id(self,value)
transformers.SpecialTokensMixin.cls_token(self)->str
transformers.SpecialTokensMixin.cls_token(self,value)
transformers.SpecialTokensMixin.cls_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.cls_token_id(self,value)
transformers.SpecialTokensMixin.eos_token(self)->str
transformers.SpecialTokensMixin.eos_token(self,value)
transformers.SpecialTokensMixin.eos_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.eos_token_id(self,value)
transformers.SpecialTokensMixin.mask_token(self)->str
transformers.SpecialTokensMixin.mask_token(self,value)
transformers.SpecialTokensMixin.mask_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.mask_token_id(self,value)
transformers.SpecialTokensMixin.pad_token(self)->str
transformers.SpecialTokensMixin.pad_token(self,value)
transformers.SpecialTokensMixin.pad_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.pad_token_id(self,value)
transformers.SpecialTokensMixin.pad_token_type_id(self)->int
transformers.SpecialTokensMixin.sanitize_special_tokens(self)->int
transformers.SpecialTokensMixin.sep_token(self)->str
transformers.SpecialTokensMixin.sep_token(self,value)
transformers.SpecialTokensMixin.sep_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.sep_token_id(self,value)
transformers.SpecialTokensMixin.special_tokens_map(self)->Dict[str, Union[str, List[str]]]
transformers.SpecialTokensMixin.special_tokens_map_extended(self)->Dict[str, Union[str, AddedToken, List[Union[str, AddedToken]]]]
transformers.SpecialTokensMixin.unk_token(self)->str
transformers.SpecialTokensMixin.unk_token(self,value)
transformers.SpecialTokensMixin.unk_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.unk_token_id(self,value)
transformers.TokenSpan(NamedTuple)
transformers.tokenization_utils_base.BatchEncoding(self,data:Optional[Dict[str,Any]]=None,encoding:Optional[Union[EncodingFast,Sequence[EncodingFast]]]=None,tensor_type:Union[None,str,TensorType]=None,prepend_batch_axis:bool=False,n_sequences:Optional[int]=None)
transformers.tokenization_utils_base.BatchEncoding.__getattr__(self,item:str)
transformers.tokenization_utils_base.BatchEncoding.__getitem__(self,item:Union[int,str])->Union[Any, EncodingFast]
transformers.tokenization_utils_base.BatchEncoding.__getstate__(self)
transformers.tokenization_utils_base.BatchEncoding.__init__(self,data:Optional[Dict[str,Any]]=None,encoding:Optional[Union[EncodingFast,Sequence[EncodingFast]]]=None,tensor_type:Union[None,str,TensorType]=None,prepend_batch_axis:bool=False,n_sequences:Optional[int]=None)
transformers.tokenization_utils_base.BatchEncoding.__setstate__(self,state)
transformers.tokenization_utils_base.BatchEncoding.char_to_token(self,batch_or_char_index:int,char_index:Optional[int]=None,sequence_index:int=0)->int
transformers.tokenization_utils_base.BatchEncoding.char_to_word(self,batch_or_char_index:int,char_index:Optional[int]=None,sequence_index:int=0)->int
transformers.tokenization_utils_base.BatchEncoding.convert_to_tensors(self,tensor_type:Optional[Union[str,TensorType]]=None,prepend_batch_axis:bool=False)
transformers.tokenization_utils_base.BatchEncoding.encodings(self)->Optional[List[EncodingFast]]
transformers.tokenization_utils_base.BatchEncoding.is_fast(self)->bool
transformers.tokenization_utils_base.BatchEncoding.items(self)
transformers.tokenization_utils_base.BatchEncoding.keys(self)
transformers.tokenization_utils_base.BatchEncoding.n_sequences(self)->Optional[int]
transformers.tokenization_utils_base.BatchEncoding.sequence_ids(self,batch_index:int=0)->List[Optional[int]]
transformers.tokenization_utils_base.BatchEncoding.to(self,device:Union[str,'torch.device'])->'BatchEncoding'
transformers.tokenization_utils_base.BatchEncoding.token_to_chars(self,batch_or_token_index:int,token_index:Optional[int]=None)->CharSpan
transformers.tokenization_utils_base.BatchEncoding.token_to_sequence(self,batch_or_token_index:int,token_index:Optional[int]=None)->int
transformers.tokenization_utils_base.BatchEncoding.token_to_word(self,batch_or_token_index:int,token_index:Optional[int]=None)->int
transformers.tokenization_utils_base.BatchEncoding.tokens(self,batch_index:int=0)->List[str]
transformers.tokenization_utils_base.BatchEncoding.values(self)
transformers.tokenization_utils_base.BatchEncoding.word_ids(self,batch_index:int=0)->List[Optional[int]]
transformers.tokenization_utils_base.BatchEncoding.word_to_chars(self,batch_or_word_index:int,word_index:Optional[int]=None,sequence_index:int=0)->CharSpan
transformers.tokenization_utils_base.BatchEncoding.word_to_tokens(self,batch_or_word_index:int,word_index:Optional[int]=None,sequence_index:int=0)->Optional[TokenSpan]
transformers.tokenization_utils_base.BatchEncoding.words(self,batch_index:int=0)->List[Optional[int]]
transformers.tokenization_utils_base.CharSpan(NamedTuple)
transformers.tokenization_utils_base.PreTrainedTokenizerBase(self,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase.__init__(self,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase.__repr__(self)->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase._decode(self,token_ids:Union[int,List[int]],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase._eventual_warn_about_too_long_sequence(self,ids:List[int],max_length:Optional[int],verbose:bool)
transformers.tokenization_utils_base.PreTrainedTokenizerBase._from_pretrained(cls,resolved_vocab_files,pretrained_model_name_or_path,init_configuration,*init_inputs,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase._get_padding_truncation_strategies(self,padding=False,truncation=False,max_length=None,pad_to_multiple_of=None,verbose=True,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.tokenization_utils_base.PreTrainedTokenizerBase._save_pretrained(self,save_directory:Union[str,os.PathLike],file_names:Tuple[str],legacy_format:Optional[bool]=None,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.as_target_tokenizer(self)
transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode(self,sequences:Union[List[int],List[List[int]],'np.ndarray','torch.Tensor','tf.Tensor'],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->List[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.clean_up_tokenization(out_string:str)->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase.convert_tokens_to_string(self,tokens:List[str])->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.decode(self,token_ids:Union[int,List[int],'np.ndarray','torch.Tensor','tf.Tensor'],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,return_tensors:Optional[Union[str,TensorType]]=None,**kwargs)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],*init_inputs,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_vocab(self)->Dict[str, int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair(self)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair(self,value)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence(self)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence(self,value)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.pad(self,encoded_inputs:Union[BatchEncoding,List[BatchEncoding],Dict[str,EncodedInput],Dict[str,List[EncodedInput]],List[Dict[str,EncodedInput]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None,verbose:bool=True)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_for_model(self,ids:List[int],pair_ids:Optional[List[int]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str=None,truncation:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained(self,save_directory:Union[str,os.PathLike],legacy_format:Optional[bool]=None,filename_prefix:Optional[str]=None,push_to_hub:bool=False,**kwargs)->Tuple[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.tokenize(self,text:str,pair:Optional[str]=None,add_special_tokens:bool=False,**kwargs)->List[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.truncate_sequences(self,ids:List[int],pair_ids:Optional[List[int]]=None,num_tokens_to_remove:int=0,truncation_strategy:Union[str,TruncationStrategy]='longest_first',stride:int=0)->Tuple[List[int], List[int], List[int]]
transformers.tokenization_utils_base.SpecialTokensMixin(self,verbose=True,**kwargs)
transformers.tokenization_utils_base.SpecialTokensMixin.__init__(self,verbose=True,**kwargs)
transformers.tokenization_utils_base.SpecialTokensMixin._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.tokenization_utils_base.SpecialTokensMixin.add_special_tokens(self,special_tokens_dict:Dict[str,Union[str,AddedToken]])->int
transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens(self,new_tokens:Union[str,AddedToken,List[Union[str,AddedToken]]],special_tokens:bool=False)->int
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens(self)->List[str]
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens_ids(self)->List[int]
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens_ids(self,values)
transformers.tokenization_utils_base.SpecialTokensMixin.all_special_ids(self)->List[int]
transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens(self)->List[str]
transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens_extended(self)->List[Union[str, AddedToken]]
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token_id(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token_id(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token_id(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token_id(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_id(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_type_id(self)->int
transformers.tokenization_utils_base.SpecialTokensMixin.sanitize_special_tokens(self)->int
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token_id(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map(self)->Dict[str, Union[str, List[str]]]
transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map_extended(self)->Dict[str, Union[str, AddedToken, List[Union[str, AddedToken]]]]
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token_id(self,value)
transformers.tokenization_utils_base.TokenSpan(NamedTuple)
transformers.tokenization_utils_base.TruncationStrategy(ExplicitEnum)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/optimization_tf.py----------------------------------------
A:transformers.optimization_tf.global_step_float->tensorflow.cast(step, tf.float32)
A:transformers.optimization_tf.warmup_steps_float->tensorflow.cast(self.warmup_steps, tf.float32)
A:transformers.optimization_tf.lr_schedule->WarmUp(initial_learning_rate=init_lr, decay_schedule_fn=lr_schedule, warmup_steps=num_warmup_steps)
A:transformers.optimization_tf.optimizer->tensorflow.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=adam_beta1, beta_2=adam_beta2, epsilon=adam_epsilon)
A:transformers.optimization_tf.apply_state[var_device, var_dtype]['weight_decay_rate']->tensorflow.constant(self.weight_decay_rate, name='adam_weight_decay_rate')
A:transformers.optimization_tf.do_decay->self._do_use_weight_decay(var.name)
A:transformers.optimization_tf.(grads, tvars)->list(zip(*grads_and_vars))
A:transformers.optimization_tf.coefficients->self._fallback_apply_state(var_device, var_dtype)
A:transformers.optimization_tf.(lr_t, kwargs)->self._get_lr(var.device, var.dtype.base_dtype, apply_state)
A:transformers.optimization_tf.decay->self._decay_weights_op(var, lr_t, apply_state)
A:transformers.optimization_tf.config->super().get_config()
A:transformers.optimization_tf.self._accum_steps->tensorflow.Variable(tf.constant(0, dtype=tf.int64), trainable=False, synchronization=tf.VariableSynchronization.ON_READ, aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)
transformers.AdamWeightDecay(self,learning_rate:Union[float,tf.keras.optimizers.schedules.LearningRateSchedule]=0.001,beta_1:float=0.9,beta_2:float=0.999,epsilon:float=1e-07,amsgrad:bool=False,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None,exclude_from_weight_decay:Optional[List[str]]=None,name:str='AdamWeightDecay',**kwargs)
transformers.AdamWeightDecay._decay_weights_op(self,var,learning_rate,apply_state)
transformers.AdamWeightDecay._do_use_weight_decay(self,param_name)
transformers.AdamWeightDecay._get_lr(self,var_device,var_dtype,apply_state)
transformers.AdamWeightDecay._prepare_local(self,var_device,var_dtype,apply_state)
transformers.AdamWeightDecay._resource_apply_dense(self,grad,var,apply_state=None)
transformers.AdamWeightDecay._resource_apply_sparse(self,grad,var,indices,apply_state=None)
transformers.AdamWeightDecay.apply_gradients(self,grads_and_vars,name=None,**kwargs)
transformers.AdamWeightDecay.from_config(cls,config)
transformers.AdamWeightDecay.get_config(self)
transformers.GradientAccumulator(self)
transformers.GradientAccumulator.gradients(self)
transformers.GradientAccumulator.reset(self)
transformers.GradientAccumulator.step(self)
transformers.WarmUp(self,initial_learning_rate:float,decay_schedule_fn:Callable,warmup_steps:int,power:float=1.0,name:str=None)
transformers.WarmUp.get_config(self)
transformers.create_optimizer(init_lr:float,num_train_steps:int,num_warmup_steps:int,min_lr_ratio:float=0.0,adam_beta1:float=0.9,adam_beta2:float=0.999,adam_epsilon:float=1e-08,weight_decay_rate:float=0.0,power:float=1.0,include_in_weight_decay:Optional[List[str]]=None)
transformers.optimization_tf.AdamWeightDecay(self,learning_rate:Union[float,tf.keras.optimizers.schedules.LearningRateSchedule]=0.001,beta_1:float=0.9,beta_2:float=0.999,epsilon:float=1e-07,amsgrad:bool=False,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None,exclude_from_weight_decay:Optional[List[str]]=None,name:str='AdamWeightDecay',**kwargs)
transformers.optimization_tf.AdamWeightDecay.__init__(self,learning_rate:Union[float,tf.keras.optimizers.schedules.LearningRateSchedule]=0.001,beta_1:float=0.9,beta_2:float=0.999,epsilon:float=1e-07,amsgrad:bool=False,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None,exclude_from_weight_decay:Optional[List[str]]=None,name:str='AdamWeightDecay',**kwargs)
transformers.optimization_tf.AdamWeightDecay._decay_weights_op(self,var,learning_rate,apply_state)
transformers.optimization_tf.AdamWeightDecay._do_use_weight_decay(self,param_name)
transformers.optimization_tf.AdamWeightDecay._get_lr(self,var_device,var_dtype,apply_state)
transformers.optimization_tf.AdamWeightDecay._prepare_local(self,var_device,var_dtype,apply_state)
transformers.optimization_tf.AdamWeightDecay._resource_apply_dense(self,grad,var,apply_state=None)
transformers.optimization_tf.AdamWeightDecay._resource_apply_sparse(self,grad,var,indices,apply_state=None)
transformers.optimization_tf.AdamWeightDecay.apply_gradients(self,grads_and_vars,name=None,**kwargs)
transformers.optimization_tf.AdamWeightDecay.from_config(cls,config)
transformers.optimization_tf.AdamWeightDecay.get_config(self)
transformers.optimization_tf.GradientAccumulator(self)
transformers.optimization_tf.GradientAccumulator.__init__(self)
transformers.optimization_tf.GradientAccumulator.gradients(self)
transformers.optimization_tf.GradientAccumulator.reset(self)
transformers.optimization_tf.GradientAccumulator.step(self)
transformers.optimization_tf.WarmUp(self,initial_learning_rate:float,decay_schedule_fn:Callable,warmup_steps:int,power:float=1.0,name:str=None)
transformers.optimization_tf.WarmUp.__init__(self,initial_learning_rate:float,decay_schedule_fn:Callable,warmup_steps:int,power:float=1.0,name:str=None)
transformers.optimization_tf.WarmUp.get_config(self)
transformers.optimization_tf.create_optimizer(init_lr:float,num_train_steps:int,num_warmup_steps:int,min_lr_ratio:float=0.0,adam_beta1:float=0.9,adam_beta2:float=0.999,adam_epsilon:float=1e-08,weight_decay_rate:float=0.0,power:float=1.0,include_in_weight_decay:Optional[List[str]]=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/tokenization_utils.py----------------------------------------
A:transformers.tokenization_utils.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_utils.cat->unicodedata.category(char)
A:transformers.tokenization_utils.cp->ord(char)
A:transformers.tokenization_utils.insertion_idx->bisect.bisect_left(token_list, new_token)
A:transformers.tokenization_utils.token->token.lower().lower()
A:transformers.tokenization_utils.added_tok_encoder->dict(((tok, len(self) + i) for (i, tok) in enumerate(tokens_to_add)))
A:transformers.tokenization_utils.self.unique_no_split_tokens->sorted(set(self.unique_no_split_tokens).union(set(tokens_to_add)))
A:transformers.tokenization_utils.all_special_tokens_extended->dict(((str(t), t) for t in self.all_special_tokens_extended if isinstance(t, AddedToken)))
A:transformers.tokenization_utils.(text, kwargs)->self.prepare_for_tokenization(text, **kwargs)
A:transformers.tokenization_utils.text->''.join(sub_texts)
A:transformers.tokenization_utils.tok_extended->dict(((str(t), t) for t in self.all_special_tokens_extended if isinstance(t, AddedToken))).get(tok, None)
A:transformers.tokenization_utils.split_text->''.join(sub_texts).split(tok)
A:transformers.tokenization_utils.sub_text->sub_text.lstrip().lstrip()
A:transformers.tokenization_utils.tokenized_text->split_on_tokens(no_split_token, text)
A:transformers.tokenization_utils.tokens->list(itertools.chain(*(self.tokenize(t, is_split_into_words=True, **kwargs) for t in text)))
A:transformers.tokenization_utils.first_ids->get_input_ids(ids)
A:transformers.tokenization_utils.batch_outputs->BatchEncoding(batch_outputs, tensor_type=return_tensors)
A:transformers.tokenization_utils.outputs->self.prepare_for_model(first_ids, second_ids, add_special_tokens=add_special_tokens, padding=PaddingStrategy.DO_NOT_PAD.value, truncation=truncation_strategy.value, max_length=max_length, stride=stride, pad_to_multiple_of=None, return_attention_mask=False, return_token_type_ids=return_token_type_ids, return_overflowing_tokens=return_overflowing_tokens, return_special_tokens_mask=return_special_tokens_mask, return_length=return_length, return_tensors=None, prepend_batch_axis=False, verbose=verbose)
A:transformers.tokenization_utils.index->int(index)
A:transformers.tokenization_utils.self._decode_use_source_tokenizer->kwargs.pop('use_source_tokenizer', False)
A:transformers.tokenization_utils.filtered_tokens->self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
A:transformers.tokenization_utils.clean_text->self.clean_up_tokenization(text)
transformers.PreTrainedTokenizer(self,**kwargs)
transformers.PreTrainedTokenizer.__len__(self)
transformers.PreTrainedTokenizer._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.PreTrainedTokenizer._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizer._batch_prepare_for_model(self,batch_ids_pairs:List[Union[PreTokenizedInputPair,Tuple[List[int],None]]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.PreTrainedTokenizer._convert_id_to_token(self,index:int)->str
transformers.PreTrainedTokenizer._convert_token_to_id(self,token)
transformers.PreTrainedTokenizer._convert_token_to_id_with_added_voc(self,token)
transformers.PreTrainedTokenizer._decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,spaces_between_special_tokens:bool=True,**kwargs)->str
transformers.PreTrainedTokenizer._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizer._tokenize(self,text,**kwargs)
transformers.PreTrainedTokenizer.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.PreTrainedTokenizer.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.PreTrainedTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.PreTrainedTokenizer.get_added_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PreTrainedTokenizer.is_fast(self)->bool
transformers.PreTrainedTokenizer.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.PreTrainedTokenizer.prepare_for_tokenization(self,text:str,is_split_into_words:bool=False,**kwargs)->Tuple[str, Dict[str, Any]]
transformers.PreTrainedTokenizer.tokenize(self,text:TextInput,**kwargs)->List[str]
transformers.PreTrainedTokenizer.vocab_size(self)->int
transformers.tokenization_utils.PreTrainedTokenizer(self,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.__init__(self,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.__len__(self)
transformers.tokenization_utils.PreTrainedTokenizer._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.tokenization_utils.PreTrainedTokenizer._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils.PreTrainedTokenizer._batch_prepare_for_model(self,batch_ids_pairs:List[Union[PreTokenizedInputPair,Tuple[List[int],None]]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.tokenization_utils.PreTrainedTokenizer._convert_id_to_token(self,index:int)->str
transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id(self,token)
transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc(self,token)
transformers.tokenization_utils.PreTrainedTokenizer._decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,spaces_between_special_tokens:bool=True,**kwargs)->str
transformers.tokenization_utils.PreTrainedTokenizer._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils.PreTrainedTokenizer._tokenize(self,text,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.tokenization_utils.PreTrainedTokenizer.get_added_vocab(self)->Dict[str, int]
transformers.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_utils.PreTrainedTokenizer.is_fast(self)->bool
transformers.tokenization_utils.PreTrainedTokenizer.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization(self,text:str,is_split_into_words:bool=False,**kwargs)->Tuple[str, Dict[str, Any]]
transformers.tokenization_utils.PreTrainedTokenizer.tokenize(self,text:TextInput,**kwargs)->List[str]
transformers.tokenization_utils.PreTrainedTokenizer.vocab_size(self)->int
transformers.tokenization_utils._insert_one_token_to_ordered_list(token_list:List[str],new_token:str)
transformers.tokenization_utils._is_control(char)
transformers.tokenization_utils._is_end_of_word(text)
transformers.tokenization_utils._is_punctuation(char)
transformers.tokenization_utils._is_start_of_word(text)
transformers.tokenization_utils._is_whitespace(char)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/activations_tf.py----------------------------------------
A:transformers.activations_tf.x->tensorflow.convert_to_tensor(x)
A:transformers.activations_tf.pi->tensorflow.cast(math.pi, x.dtype)
A:transformers.activations_tf.coeff->tensorflow.cast(0.044715, x.dtype)
A:transformers.activations_tf.coeff1->tensorflow.cast(0.044715, x.dtype)
A:transformers.activations_tf.coeff2->tensorflow.cast(0.7978845608, x.dtype)
transformers.activations_tf._gelu(x)
transformers.activations_tf._gelu_new(x)
transformers.activations_tf.gelu_fast(x)
transformers.activations_tf.get_tf_activation(activation_string)
transformers.activations_tf.mish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/file_utils.py----------------------------------------
A:transformers.file_utils.logger->utils.logging.get_logger(__name__)
A:transformers.file_utils.ENV_VARS_TRUE_AND_AUTO_VALUES->ENV_VARS_TRUE_VALUES.union({'AUTO'})
A:transformers.file_utils.USE_TF->os.environ.get('USE_TF', 'AUTO').upper()
A:transformers.file_utils.USE_TORCH->os.environ.get('USE_TORCH', 'AUTO').upper()
A:transformers.file_utils.USE_JAX->os.environ.get('USE_FLAX', 'AUTO').upper()
A:transformers.file_utils._torch_version->transformers.utils.versions.importlib_metadata.version('torch')
A:transformers.file_utils._tf_version->transformers.utils.versions.importlib_metadata.version(pkg)
A:transformers.file_utils._jax_version->transformers.utils.versions.importlib_metadata.version('jax')
A:transformers.file_utils._flax_version->transformers.utils.versions.importlib_metadata.version('flax')
A:transformers.file_utils._->transformers.utils.versions.importlib_metadata.version('datasets')
A:transformers.file_utils._datasets_metadata->transformers.utils.versions.importlib_metadata.metadata('datasets')
A:transformers.file_utils._faiss_version->transformers.utils.versions.importlib_metadata.version('faiss-cpu')
A:transformers.file_utils._onxx_version->transformers.utils.versions.importlib_metadata.version('onnx')
A:transformers.file_utils._scatter_version->transformers.utils.versions.importlib_metadata.version('torch_scatter')
A:transformers.file_utils._soundfile_version->transformers.utils.versions.importlib_metadata.version('soundfile')
A:transformers.file_utils._timm_version->transformers.utils.versions.importlib_metadata.version('timm')
A:transformers.file_utils._torchaudio_version->transformers.utils.versions.importlib_metadata.version('torchaudio')
A:transformers.file_utils.torch_cache_home->os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch'))
A:transformers.file_utils.old_default_cache_path->os.path.join(torch_cache_home, 'transformers')
A:transformers.file_utils.hf_cache_home->os.path.expanduser(os.getenv('HF_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'huggingface')))
A:transformers.file_utils.default_cache_path->os.path.join(hf_cache_home, 'transformers')
A:transformers.file_utils.PYTORCH_PRETRAINED_BERT_CACHE->os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path)
A:transformers.file_utils.PYTORCH_TRANSFORMERS_CACHE->os.getenv('PYTORCH_TRANSFORMERS_CACHE', PYTORCH_PRETRAINED_BERT_CACHE)
A:transformers.file_utils.TRANSFORMERS_CACHE->os.getenv('TRANSFORMERS_CACHE', PYTORCH_TRANSFORMERS_CACHE)
A:transformers.file_utils.HUGGINGFACE_CO_RESOLVE_ENDPOINT->os.environ.get('HUGGINGFACE_CO_RESOLVE_ENDPOINT', _default_endpoint)
A:transformers.file_utils.TORCH_FX_REQUIRED_VERSION->packaging.version.parse('1.8')
A:transformers.file_utils.torch_version->packaging.version.parse(importlib_metadata.version('torch'))
A:transformers.file_utils.sagemaker_params->json.loads(os.getenv('SM_FRAMEWORK_PARAMS', '{}'))
A:transformers.file_utils.smp_options->json.loads(smp_options)
A:transformers.file_utils.mpi_options->json.loads(mpi_options)
A:transformers.file_utils.BACKENDS_MAPPING->OrderedDict([('datasets', (is_datasets_available, DATASETS_IMPORT_ERROR)), ('faiss', (is_faiss_available, FAISS_IMPORT_ERROR)), ('flax', (is_flax_available, FLAX_IMPORT_ERROR)), ('pandas', (is_pandas_available, PANDAS_IMPORT_ERROR)), ('protobuf', (is_protobuf_available, PROTOBUF_IMPORT_ERROR)), ('scatter', (is_scatter_available, SCATTER_IMPORT_ERROR)), ('sentencepiece', (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)), ('sklearn', (is_sklearn_available, SKLEARN_IMPORT_ERROR)), ('speech', (is_speech_available, SPEECH_IMPORT_ERROR)), ('tf', (is_tf_available, TENSORFLOW_IMPORT_ERROR)), ('timm', (is_timm_available, TIMM_IMPORT_ERROR)), ('tokenizers', (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)), ('torch', (is_torch_available, PYTORCH_IMPORT_ERROR)), ('vision', (is_vision_available, VISION_IMPORT_ERROR)), ('scipy', (is_scipy_available, SCIPY_IMPORT_ERROR))])
A:transformers.file_utils.search->re.search('^(\\s*)\\S', t)
A:transformers.file_utils.indent->_get_indent(output_args_doc)
A:transformers.file_utils.blocks[i]->re.sub(':\\s*\\n\\s*(\\S)', ' -- \\1', blocks[i])
A:transformers.file_utils.lines->'\n'.join(lines).split('\n')
A:transformers.file_utils.docstrings->'\n'.join(lines)
A:transformers.file_utils.intro->intro.format(full_output_type=full_output_type, config_class=config_class).format(full_output_type=full_output_type, config_class=config_class)
A:transformers.file_utils.doc_kwargs->dict(model_class=model_class, tokenizer_class=tokenizer_class, checkpoint=checkpoint)
A:transformers.file_utils.built_doc->code_sample.format(**doc_kwargs)
A:transformers.file_utils.lines[i]->_prepare_output_docstrings(output_type, config_class)
A:transformers.file_utils.parsed->urlparse(url_or_filename)
A:transformers.file_utils.endpoint->PRESET_MIRROR_DICT.get(mirror, mirror)
A:transformers.file_utils.url_bytes->self._push_to_hub(repo, commit_message=commit_message).encode('utf-8')
A:transformers.file_utils.filename->url_to_filename(url, etag)
A:transformers.file_utils.etag_bytes->etag.encode('utf-8')
A:transformers.file_utils.cache_dir->str(cache_dir)
A:transformers.file_utils.cache_path->os.path.join(cache_dir, filename)
A:transformers.file_utils.metadata->json.load(meta_file)
A:transformers.file_utils.meta_path->os.path.join(cache_dir, file)
A:transformers.file_utils.url_or_filename->str(url_or_filename)
A:transformers.file_utils.output_path->get_from_cache(url_or_filename, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, user_agent=user_agent, use_auth_token=use_auth_token, local_files_only=local_files_only)
A:transformers.file_utils.(output_dir, output_file)->os.path.split(output_path)
A:transformers.file_utils.output_path_extracted->os.path.join(output_dir, output_extract_dir_name)
A:transformers.file_utils.tar_file->tarfile.open(output_path)
A:transformers.file_utils.instance_data->requests.get(os.environ['ECS_CONTAINER_METADATA_URI']).json()
A:transformers.file_utils.headers->copy.deepcopy(headers)
A:transformers.file_utils.r->requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
A:transformers.file_utils.content_length->requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout).headers.get('Content-Length')
A:transformers.file_utils.progress->tqdm(unit='B', unit_scale=True, total=total, initial=resume_size, desc='Downloading', disable=bool(logging.get_verbosity() == logging.NOTSET))
A:transformers.file_utils.token->huggingface_hub.HfFolder.get_token()
A:transformers.file_utils.temp_file_manager->partial(tempfile.NamedTemporaryFile, mode='wb', dir=cache_dir, delete=False)
A:transformers.file_utils.umask->os.umask(438)
A:transformers.file_utils.cached->self.fget(obj)
A:transformers.file_utils.class_fields->fields(self)
A:transformers.file_utils.first_field->getattr(self, class_fields[0].name)
A:transformers.file_utils.other_fields_are_none->all((getattr(self, field.name) is None for field in class_fields[1:]))
A:transformers.file_utils.iterator->iter(first_field)
A:transformers.file_utils.v->getattr(self, field.name)
A:transformers.file_utils.self._modules->set(import_structure.keys())
A:transformers.file_utils.value->getattr(module, name)
A:transformers.file_utils.module->self._get_module(self._class_to_module[name])
A:transformers.file_utils.g->functools.update_wrapper(g, f)
A:transformers.file_utils.test_git->subprocess.run('git branch'.split(), cwd=repo_path)
A:transformers.file_utils.repo_url->cls._get_repo_url_from_name(repo_name, organization=organization, private=private, use_auth_token=use_auth_token)
A:transformers.file_utils.repo_path_or_name->tempfile.mkdtemp()
A:transformers.file_utils.repo->Repository(repo_path_or_name, clone_from=repo_url, use_auth_token=use_auth_token)
A:transformers.file_utils.url->self._push_to_hub(repo, commit_message=commit_message)
transformers.TensorType(ExplicitEnum)
transformers._BaseLazyModule(self,name,import_structure)
transformers._BaseLazyModule.__dir__(self)
transformers._BaseLazyModule.__getattr__(self,name:str)->Any
transformers._BaseLazyModule._get_module(self,module_name:str)->ModuleType
transformers.add_end_docstrings(*docstr)
transformers.add_start_docstrings(*docstr)
transformers.add_start_docstrings_to_model_forward(*docstr)
transformers.cached_path(url_or_filename,cache_dir=None,force_download=False,proxies=None,resume_download=False,user_agent:Union[Dict,str,None]=None,extract_compressed_file=False,force_extract=False,use_auth_token:Union[bool,str,None]=None,local_files_only=False)->Optional[str]
transformers.file_utils.ExplicitEnum(Enum)
transformers.file_utils.ExplicitEnum._missing_(cls,value)
transformers.file_utils.ModelOutput(OrderedDict)
transformers.file_utils.ModelOutput.__delitem__(self,*args,**kwargs)
transformers.file_utils.ModelOutput.__getitem__(self,k)
transformers.file_utils.ModelOutput.__post_init__(self)
transformers.file_utils.ModelOutput.__setattr__(self,name,value)
transformers.file_utils.ModelOutput.__setitem__(self,key,value)
transformers.file_utils.ModelOutput.pop(self,*args,**kwargs)
transformers.file_utils.ModelOutput.setdefault(self,*args,**kwargs)
transformers.file_utils.ModelOutput.to_tuple(self)->Tuple[Any]
transformers.file_utils.ModelOutput.update(self,*args,**kwargs)
transformers.file_utils.PaddingStrategy(ExplicitEnum)
transformers.file_utils.PushToHubMixin
transformers.file_utils.PushToHubMixin._create_or_get_repo(cls,repo_path_or_name:Optional[str]=None,repo_url:Optional[str]=None,organization:Optional[str]=None,private:bool=None,use_auth_token:Optional[Union[bool,str]]=None)->Repository
transformers.file_utils.PushToHubMixin._get_repo_url_from_name(repo_name:str,organization:Optional[str]=None,private:bool=None,use_auth_token:Optional[Union[bool,str]]=None)->str
transformers.file_utils.PushToHubMixin._push_to_hub(cls,repo:Repository,commit_message:Optional[str]=None)->str
transformers.file_utils.PushToHubMixin.push_to_hub(self,repo_path_or_name:Optional[str]=None,repo_url:Optional[str]=None,use_temp_dir:bool=False,commit_message:Optional[str]=None,organization:Optional[str]=None,private:Optional[bool]=None,use_auth_token:Optional[Union[bool,str]]=None)->str
transformers.file_utils.TensorType(ExplicitEnum)
transformers.file_utils._BaseLazyModule(self,name,import_structure)
transformers.file_utils._BaseLazyModule.__dir__(self)
transformers.file_utils._BaseLazyModule.__getattr__(self,name:str)->Any
transformers.file_utils._BaseLazyModule.__init__(self,name,import_structure)
transformers.file_utils._BaseLazyModule._get_module(self,module_name:str)->ModuleType
transformers.file_utils._convert_output_args_doc(output_args_doc)
transformers.file_utils._get_indent(t)
transformers.file_utils._is_jax(x)
transformers.file_utils._is_numpy(x)
transformers.file_utils._is_tensorflow(x)
transformers.file_utils._is_torch(x)
transformers.file_utils._is_torch_device(x)
transformers.file_utils._prepare_output_docstrings(output_type,config_class)
transformers.file_utils.add_code_sample_docstrings(*docstr,tokenizer_class=None,checkpoint=None,output_type=None,config_class=None,mask=None,model_cls=None)
transformers.file_utils.add_end_docstrings(*docstr)
transformers.file_utils.add_start_docstrings(*docstr)
transformers.file_utils.add_start_docstrings_to_model_forward(*docstr)
transformers.file_utils.cached_path(url_or_filename,cache_dir=None,force_download=False,proxies=None,resume_download=False,user_agent:Union[Dict,str,None]=None,extract_compressed_file=False,force_extract=False,use_auth_token:Union[bool,str,None]=None,local_files_only=False)->Optional[str]
transformers.file_utils.cached_property(property)
transformers.file_utils.cached_property.__get__(self,obj,objtype=None)
transformers.file_utils.copy_func(f)
transformers.file_utils.define_sagemaker_information()
transformers.file_utils.filename_to_url(filename,cache_dir=None)
transformers.file_utils.get_cached_models(cache_dir:Union[str,Path]=None)->List[Tuple]
transformers.file_utils.get_from_cache(url:str,cache_dir=None,force_download=False,proxies=None,etag_timeout=10,resume_download=False,user_agent:Union[Dict,str,None]=None,use_auth_token:Union[bool,str,None]=None,local_files_only=False)->Optional[str]
transformers.file_utils.hf_bucket_url(model_id:str,filename:str,subfolder:Optional[str]=None,revision:Optional[str]=None,mirror=None)->str
transformers.file_utils.http_get(url:str,temp_file:BinaryIO,proxies=None,resume_size=0,headers:Optional[Dict[str,str]]=None)
transformers.file_utils.http_user_agent(user_agent:Union[Dict,str,None]=None)->str
transformers.file_utils.is_apex_available()
transformers.file_utils.is_datasets_available()
transformers.file_utils.is_faiss_available()
transformers.file_utils.is_flax_available()
transformers.file_utils.is_in_notebook()
transformers.file_utils.is_local_clone(repo_path,repo_url)
transformers.file_utils.is_offline_mode()
transformers.file_utils.is_onnx_available()
transformers.file_utils.is_pandas_available()
transformers.file_utils.is_protobuf_available()
transformers.file_utils.is_psutil_available()
transformers.file_utils.is_py3nvml_available()
transformers.file_utils.is_remote_url(url_or_filename)
transformers.file_utils.is_sagemaker_dp_enabled()
transformers.file_utils.is_sagemaker_mp_enabled()
transformers.file_utils.is_scatter_available()
transformers.file_utils.is_scipy_available()
transformers.file_utils.is_sentencepiece_available()
transformers.file_utils.is_sklearn_available()
transformers.file_utils.is_soundfile_availble()
transformers.file_utils.is_speech_available()
transformers.file_utils.is_tensor(x)
transformers.file_utils.is_tf_available()
transformers.file_utils.is_timm_available()
transformers.file_utils.is_tokenizers_available()
transformers.file_utils.is_torch_available()
transformers.file_utils.is_torch_cuda_available()
transformers.file_utils.is_torch_fx_available()
transformers.file_utils.is_torch_fx_proxy(x)
transformers.file_utils.is_torch_tpu_available()
transformers.file_utils.is_torchaudio_available()
transformers.file_utils.is_training_run_on_sagemaker()
transformers.file_utils.is_vision_available()
transformers.file_utils.replace_return_docstrings(output_type=None,config_class=None)
transformers.file_utils.requires_backends(obj,backends)
transformers.file_utils.tf_required(func)
transformers.file_utils.to_py_obj(obj)
transformers.file_utils.torch_only_method(fn)
transformers.file_utils.torch_required(func)
transformers.file_utils.url_to_filename(url:str,etag:Optional[str]=None)->str
transformers.is_apex_available()
transformers.is_datasets_available()
transformers.is_faiss_available()
transformers.is_flax_available()
transformers.is_psutil_available()
transformers.is_py3nvml_available()
transformers.is_scipy_available()
transformers.is_sentencepiece_available()
transformers.is_sklearn_available()
transformers.is_speech_available()
transformers.is_tf_available()
transformers.is_timm_available()
transformers.is_tokenizers_available()
transformers.is_torch_available()
transformers.is_torch_tpu_available()
transformers.is_vision_available()


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_tf_utils.py----------------------------------------
A:transformers.generation_tf_utils.logger->utils.logging.get_logger(__name__)
A:transformers.generation_tf_utils.use_cache->getattr(self.config, 'use_cache', False)
A:transformers.generation_tf_utils.input_ids->tensorflow.concat([input_ids, tf.expand_dims(beam_tokens, 1)], axis=-1)
A:transformers.generation_tf_utils.attention_mask->tensorflow.concat([attention_mask, tf.ones((shape_list(attention_mask)[0], 1), dtype=tf.int32)], axis=-1)
A:transformers.generation_tf_utils.encoder->self.get_encoder()
A:transformers.generation_tf_utils.encoder_outputs->encoder(input_ids, attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states)
A:transformers.generation_tf_utils.expanded_batch_idxs->tensorflow.reshape(tf.repeat(tf.expand_dims(tf.range(batch_size), -1), repeats=num_beams * effective_batch_mult, axis=1), shape=(-1,))
A:transformers.generation_tf_utils.output->self._generate_no_beam_search(input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids, pad_token_id=pad_token_id, eos_token_id=eos_token_id, batch_size=effective_batch_size, vocab_size=vocab_size, encoder_outputs=encoder_outputs, attention_mask=attention_mask, use_cache=use_cache, return_dict_in_generate=return_dict_in_generate, **model_kwargs)
A:transformers.generation_tf_utils.unfinished_sents->tensorflow.ones_like(input_ids[:, 0])
A:transformers.generation_tf_utils.model_inputs->self.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **kwargs)
A:transformers.generation_tf_utils.outputs->self(**model_inputs, return_dict=True, output_attentions=kwargs['output_attentions'], output_hidden_states=kwargs['output_hidden_states'])
A:transformers.generation_tf_utils.next_token_logits_penalties->_create_next_token_logits_penalties(input_ids, next_token_logits, repetition_penalty)
A:transformers.generation_tf_utils.next_token_logits->self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len, max_length=max_length, forced_bos_token_id=forced_bos_token_id, forced_eos_token_id=forced_eos_token_id)
A:transformers.generation_tf_utils.banned_tokens->calc_banned_bad_words_ids(input_ids, bad_words_ids)
A:transformers.generation_tf_utils.is_token_logit_eos_token->tensorflow.convert_to_tensor([True if token is eos_token_id else False for token in range(vocab_size)], dtype=tf.bool)
A:transformers.generation_tf_utils.eos_token_indices_mask->tensorflow.broadcast_to(is_token_logit_eos_token, [num_batch_hypotheses, vocab_size])
A:transformers.generation_tf_utils.next_token->tensorflow.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)
A:transformers.generation_tf_utils.is_sents_unfinished_and_token_to_add_is_eos->tensorflow.math.multiply(unfinished_sents, tf.cast(eos_in_sents, tf.int32))
A:transformers.generation_tf_utils.min_sent_length->tensorflow.math.reduce_min(sent_lengths)
A:transformers.generation_tf_utils.max_sent_length->tensorflow.math.reduce_max(sent_lengths)
A:transformers.generation_tf_utils.broad_casted_sent_lengths->tensorflow.broadcast_to(tf.expand_dims(sent_lengths, -1), [batch_size, max_sent_length])
A:transformers.generation_tf_utils.broad_casted_range->tensorflow.transpose(tf.broadcast_to(tf.expand_dims(tf.range(max_sent_length), -1), [max_sent_length, batch_size]))
A:transformers.generation_tf_utils.decoded->tensorflow.stack(best)
A:transformers.generation_tf_utils.beam_scores_begin->tensorflow.zeros((batch_size, 1), dtype=tf.float32)
A:transformers.generation_tf_utils.beam_scores->tensorflow.convert_to_tensor([x[0] for x in next_batch_beam], dtype=tf.float32)
A:transformers.generation_tf_utils.scores->set_tensor_by_indices_to_value(scores, tf.convert_to_tensor(banned_tokens_indices_mask, dtype=tf.bool), -float('inf'))
A:transformers.generation_tf_utils._scores->tensorflow.reshape(_scores, (batch_size, num_beams * vocab_size))
A:transformers.generation_tf_utils.next_tokens->tensorflow.gather(next_tokens, next_scores_indices, batch_dims=1)
A:transformers.generation_tf_utils.next_scores->tensorflow.reshape(next_scores, (batch_size, num_beams * vocab_size))
A:transformers.generation_tf_utils.next_scores_indices->tensorflow.argsort(next_scores, direction='DESCENDING', axis=1)
A:transformers.generation_tf_utils.(next_scores, next_tokens)->tensorflow.math.top_k(next_scores, k=2 * num_beams, sorted=True)
A:transformers.generation_tf_utils.beam_tokens->tensorflow.convert_to_tensor([x[1] for x in next_batch_beam], dtype=tf.int32)
A:transformers.generation_tf_utils.beam_idx->tensorflow.convert_to_tensor([x[2] for x in next_batch_beam], dtype=tf.int32)
A:transformers.generation_tf_utils.past->self._reorder_cache(past, beam_idx)
A:transformers.generation_tf_utils.final_score->beam_scores[effective_beam_id].numpy().item()
A:transformers.generation_tf_utils.sorted_hyps->sorted(hypotheses.beams, key=lambda x: x[0])
A:transformers.generation_tf_utils.sent_lengths->tensorflow.convert_to_tensor(sent_lengths_list, dtype=tf.int32)
A:transformers.generation_tf_utils.sent_max_len->min(tf.reduce_max(sent_lengths).numpy() + 1, max_length)
A:transformers.generation_tf_utils.decoded_slice->tensorflow.where(tf.range(sent_max_len, dtype=tf.int32) == sent_lengths[i], eos_token_id * tf.ones((sent_max_len,), dtype=tf.int32), decoded_slice)
A:transformers.generation_tf_utils.vocab_range->tensorflow.constant(range(self.config.vocab_size))
A:transformers.generation_tf_utils.token_penalties->numpy.ones(shape_list(logits))
A:transformers.generation_tf_utils.logit_penalties->numpy.zeros(logit_penalized.shape)
A:transformers.generation_tf_utils.gen_tokens->prev_input_ids[idx].numpy().tolist()
A:transformers.generation_tf_utils.prev_ngram_tuple->tuple(ngram[:-1])
A:transformers.generation_tf_utils.ngram_idx->tuple(prev_input_ids[hypo_idx, start_idx:cur_len].numpy().tolist())
A:transformers.generation_tf_utils.logits_shape->shape_list(logits)
A:transformers.generation_tf_utils.top_k->min(max(top_k, min_tokens_to_keep), logits_shape[-1])
A:transformers.generation_tf_utils.logits->set_tensor_by_indices_to_value(logits, indices_to_remove, filter_value)
A:transformers.generation_tf_utils.sorted_indices->tensorflow.argsort(logits, direction='DESCENDING')
A:transformers.generation_tf_utils.sorted_logits->tensorflow.gather(logits, sorted_indices, axis=-1, batch_dims=1)
A:transformers.generation_tf_utils.cumulative_probs->tensorflow.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)
A:transformers.generation_tf_utils.sorted_indices_to_remove->tensorflow.concat([tf.zeros_like(sorted_indices_to_remove[:, :1]), sorted_indices_to_remove[:, 1:]], -1)
A:transformers.generation_tf_utils.indices_to_remove->scatter_values_on_batch_indices(sorted_indices_to_remove, sorted_indices)
A:transformers.generation_tf_utils.shape->shape_list(batch_indices)
A:transformers.generation_tf_utils.broad_casted_batch_dims->tensorflow.reshape(tf.broadcast_to(tf.expand_dims(tf.range(shape[0]), axis=-1), shape), [1, -1])
A:transformers.generation_tf_utils.pair_indices->tensorflow.transpose(tf.concat([broad_casted_batch_dims, tf.reshape(batch_indices, [1, -1])], 0))
A:transformers.generation_tf_utils.(_, indices)->tensorflow.nn.top_k(logits + z, num_samples)
A:transformers.generation_tf_utils.static->x.shape.as_list()
A:transformers.generation_tf_utils.dynamic->tensorflow.shape(x)
A:transformers.generation_tf_utils.sorted_scores->sorted([(s, idx) for (idx, (s, _)) in enumerate(self.beams)])
A:transformers.generation_tf_utils.self.worst_score->min(score, self.worst_score)
transformers.generation_tf_utils.BeamHypotheses(self,num_beams,max_length,length_penalty,early_stopping)
transformers.generation_tf_utils.BeamHypotheses.__init__(self,num_beams,max_length,length_penalty,early_stopping)
transformers.generation_tf_utils.BeamHypotheses.__len__(self)
transformers.generation_tf_utils.BeamHypotheses.add(self,hyp,sum_logprobs)
transformers.generation_tf_utils.BeamHypotheses.is_done(self,best_sum_logprobs,cur_len)
transformers.generation_tf_utils.TFBeamSampleDecoderOnlyOutput(ModelOutput)
transformers.generation_tf_utils.TFBeamSampleEncoderDecoderOutput(ModelOutput)
transformers.generation_tf_utils.TFBeamSearchDecoderOnlyOutput(ModelOutput)
transformers.generation_tf_utils.TFBeamSearchEncoderDecoderOutput(ModelOutput)
transformers.generation_tf_utils.TFGenerationMixin
transformers.generation_tf_utils.TFGenerationMixin._generate_beam_search(self,input_ids,cur_len,max_length,min_length,do_sample,early_stopping,temperature,top_k,top_p,repetition_penalty,no_repeat_ngram_size,bad_words_ids,pad_token_id,eos_token_id,batch_size,num_return_sequences,length_penalty,num_beams,vocab_size,encoder_outputs,attention_mask,use_cache,forced_bos_token_id,forced_eos_token_id,return_dict_in_generate,**kwargs)->Union[TFBeamSearchOutput, TFBeamSampleOutput, tf.Tensor]
transformers.generation_tf_utils.TFGenerationMixin._generate_no_beam_search(self,input_ids,cur_len,max_length,min_length,do_sample,temperature,top_k,top_p,repetition_penalty,no_repeat_ngram_size,bad_words_ids,pad_token_id,eos_token_id,batch_size,vocab_size,encoder_outputs,attention_mask,use_cache,return_dict_in_generate,**kwargs)->Union[TFGreedySearchOutput, TFSampleOutput, tf.Tensor]
transformers.generation_tf_utils.TFGenerationMixin._reorder_cache(past,beam_idx)
transformers.generation_tf_utils.TFGenerationMixin._use_cache(self,outputs,use_cache)
transformers.generation_tf_utils.TFGenerationMixin.adjust_logits_during_generation(self,logits,cur_len,max_length,forced_bos_token_id,forced_eos_token_id,**kwargs)
transformers.generation_tf_utils.TFGenerationMixin.generate(self,input_ids=None,max_length=None,min_length=None,do_sample=None,early_stopping=None,num_beams=None,temperature=None,top_k=None,top_p=None,repetition_penalty=None,bad_words_ids=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,length_penalty=None,no_repeat_ngram_size=None,num_return_sequences=None,attention_mask=None,decoder_start_token_id=None,use_cache=None,output_scores=None,output_attentions=None,output_hidden_states=None,return_dict_in_generate=None,forced_bos_token_id=None,forced_eos_token_id=None,**model_kwargs)->Union[TFGreedySearchOutput, TFSampleOutput, TFBeamSearchOutput, TFBeamSampleOutput, tf.Tensor]
transformers.generation_tf_utils.TFGenerationMixin.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.generation_tf_utils.TFGreedySearchDecoderOnlyOutput(ModelOutput)
transformers.generation_tf_utils.TFGreedySearchEncoderDecoderOutput(ModelOutput)
transformers.generation_tf_utils.TFSampleDecoderOnlyOutput(ModelOutput)
transformers.generation_tf_utils.TFSampleEncoderDecoderOutput(ModelOutput)
transformers.generation_tf_utils._create_next_token_logits_penalties(input_ids,logits,repetition_penalty)
transformers.generation_tf_utils.calc_banned_bad_words_ids(prev_input_ids,bad_words_ids)
transformers.generation_tf_utils.calc_banned_ngram_tokens(prev_input_ids,num_hypos,no_repeat_ngram_size,cur_len)
transformers.generation_tf_utils.sample_without_replacement(logits,num_samples)
transformers.generation_tf_utils.scatter_values_on_batch_indices(values,batch_indices)
transformers.generation_tf_utils.set_tensor_by_indices_to_value(tensor,indices,value)
transformers.generation_tf_utils.shape_list(x)
transformers.generation_tf_utils.tf_top_k_top_p_filtering(logits,top_k=0,top_p=1.0,filter_value=-float('Inf'),min_tokens_to_keep=1)
transformers.tf_top_k_top_p_filtering(logits,top_k=0,top_p=1.0,filter_value=-float('Inf'),min_tokens_to_keep=1)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modelcard.py----------------------------------------
A:transformers.modelcard.logger->utils.logging.get_logger(__name__)
A:transformers.modelcard.self.model_details->kwargs.pop('model_details', {})
A:transformers.modelcard.self.intended_use->kwargs.pop('intended_use', {})
A:transformers.modelcard.self.factors->kwargs.pop('factors', {})
A:transformers.modelcard.self.metrics->kwargs.pop('metrics', {})
A:transformers.modelcard.self.evaluation_data->kwargs.pop('evaluation_data', {})
A:transformers.modelcard.self.training_data->kwargs.pop('training_data', {})
A:transformers.modelcard.self.quantitative_analyses->kwargs.pop('quantitative_analyses', {})
A:transformers.modelcard.self.ethical_considerations->kwargs.pop('ethical_considerations', {})
A:transformers.modelcard.self.caveats_and_recommendations->kwargs.pop('caveats_and_recommendations', {})
A:transformers.modelcard.output_model_card_file->os.path.join(save_directory_or_file, MODEL_CARD_NAME)
A:transformers.modelcard.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modelcard.proxies->kwargs.pop('proxies', None)
A:transformers.modelcard.find_from_standard_name->kwargs.pop('find_from_standard_name', True)
A:transformers.modelcard.return_unused_kwargs->kwargs.pop('return_unused_kwargs', False)
A:transformers.modelcard.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.modelcard.model_card_file->model_card_file.replace(TF2_WEIGHTS_NAME, MODEL_CARD_NAME).replace(TF2_WEIGHTS_NAME, MODEL_CARD_NAME)
A:transformers.modelcard.resolved_model_card_file->cached_path(model_card_file, cache_dir=cache_dir, proxies=proxies, user_agent=user_agent)
A:transformers.modelcard.modelcard->cls()
A:transformers.modelcard.text->reader.read()
A:transformers.modelcard.dict_obj->json.loads(text)
A:transformers.modelcard.output->copy.deepcopy(self.__dict__)
A:transformers.modelcard.model_info->HfApi().model_info(self.finetuned_from)
A:transformers.modelcard.dataset_names->_listify(self.dataset)
A:transformers.modelcard.dataset_tags->_listify(self.dataset_tags)
A:transformers.modelcard.dataset_args->_listify(self.dataset_args)
A:transformers.modelcard.metric_mapping->infer_metric_tags_from_eval_results(self.eval_results)
A:transformers.modelcard.metadata->yaml.dump(self.create_metadata(), sort_keys=False)
A:transformers.modelcard.metadata['model_index']->self.create_model_index(metric_mapping)
A:transformers.modelcard.(_, eval_lines, eval_results)->parse_log_history(trainer.state.log_history)
A:transformers.modelcard.hyperparameters->extract_hyperparameters_from_trainer(trainer)
A:transformers.modelcard.metrics->log_history[i].copy()
A:transformers.modelcard._->log_history[i].copy().pop('eval_steps_per_second', None)
A:transformers.modelcard.epoch->log_history[i].copy().pop('epoch', None)
A:transformers.modelcard.step->log_history[i].copy().pop('step', None)
A:transformers.modelcard.splits->k.split('_')
A:transformers.modelcard.name->' '.join([part.capitalize() for part in splits[1:]])
A:transformers.modelcard.camel_cased_key->' '.join([part.capitalize() for part in key.split('_')])
A:transformers.modelcard.col_widths[key]->len(_maybe_round(value))
A:transformers.modelcard.table->_regular_table_line(list(lines[0].keys()), list(col_widths.values()))
transformers.ModelCard(self,**kwargs)
transformers.ModelCard.__eq__(self,other)
transformers.ModelCard.__repr__(self)
transformers.ModelCard.from_dict(cls,json_object)
transformers.ModelCard.from_json_file(cls,json_file)
transformers.ModelCard.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.ModelCard.save_pretrained(self,save_directory_or_file)
transformers.ModelCard.to_dict(self)
transformers.ModelCard.to_json_file(self,json_file_path)
transformers.ModelCard.to_json_string(self)
transformers.modelcard.ModelCard(self,**kwargs)
transformers.modelcard.ModelCard.__eq__(self,other)
transformers.modelcard.ModelCard.__init__(self,**kwargs)
transformers.modelcard.ModelCard.__repr__(self)
transformers.modelcard.ModelCard.from_dict(cls,json_object)
transformers.modelcard.ModelCard.from_json_file(cls,json_file)
transformers.modelcard.ModelCard.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.modelcard.ModelCard.save_pretrained(self,save_directory_or_file)
transformers.modelcard.ModelCard.to_dict(self)
transformers.modelcard.ModelCard.to_json_file(self,json_file_path)
transformers.modelcard.ModelCard.to_json_string(self)
transformers.modelcard.TrainingSummary
transformers.modelcard.TrainingSummary.__post_init__(self)
transformers.modelcard.TrainingSummary.create_metadata(self)
transformers.modelcard.TrainingSummary.create_model_index(self,metric_mapping)
transformers.modelcard.TrainingSummary.from_trainer(cls,trainer,language=None,license=None,tags=None,model_name=None,finetuned_from=None,tasks=None,dataset_tags=None,dataset=None,dataset_args=None)
transformers.modelcard.TrainingSummary.to_model_card(self)
transformers.modelcard._get_mapping_values(mapping)
transformers.modelcard._insert_value(metadata,name,value)
transformers.modelcard._insert_values_as_list(metadata,name,values)
transformers.modelcard._listify(obj)
transformers.modelcard._maybe_round(v,decimals=4)
transformers.modelcard._regular_table_line(values,col_widths)
transformers.modelcard._second_table_line(col_widths)
transformers.modelcard.extract_hyperparameters_from_trainer(trainer)
transformers.modelcard.infer_metric_tags_from_eval_results(eval_results)
transformers.modelcard.is_hf_dataset(dataset)
transformers.modelcard.make_markdown_table(lines)
transformers.modelcard.parse_log_history(log_history)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/debug_utils.py----------------------------------------
A:transformers.debug_utils.logger->utils.logging.get_logger(__name__)
A:transformers.debug_utils.self.frames->collections.deque([], max_frames_to_save)
A:transformers.debug_utils.abs_var->var.abs()
transformers.debug_utils.DebugOption(ExplicitEnum)
transformers.debug_utils.DebugUnderflowOverflow(self,model,max_frames_to_save=21,trace_batch_nums=[],abort_after_batch_num=None)
transformers.debug_utils.DebugUnderflowOverflow.__init__(self,model,max_frames_to_save=21,trace_batch_nums=[],abort_after_batch_num=None)
transformers.debug_utils.DebugUnderflowOverflow._register_forward_hook(self,module)
transformers.debug_utils.DebugUnderflowOverflow.analyse_model(self)
transformers.debug_utils.DebugUnderflowOverflow.analyse_variable(self,var,ctx)
transformers.debug_utils.DebugUnderflowOverflow.batch_end_frame(self)
transformers.debug_utils.DebugUnderflowOverflow.batch_start_frame(self)
transformers.debug_utils.DebugUnderflowOverflow.create_frame(self,module,input,output)
transformers.debug_utils.DebugUnderflowOverflow.dump_saved_frames(self)
transformers.debug_utils.DebugUnderflowOverflow.expand_frame(self,line)
transformers.debug_utils.DebugUnderflowOverflow.forward_hook(self,module,input,output)
transformers.debug_utils.DebugUnderflowOverflow.register_forward_hook(self)
transformers.debug_utils.DebugUnderflowOverflow.reset_saved_frames(self)
transformers.debug_utils.DebugUnderflowOverflow.save_frame(self,frame=None)
transformers.debug_utils.DebugUnderflowOverflow.trace_frames(self)
transformers.debug_utils.detect_overflow(var,ctx)
transformers.debug_utils.get_abs_min_max(var,ctx)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/dependency_versions_check.py----------------------------------------
A:transformers.dependency_versions_check.pkgs_to_check_at_runtime->'python tqdm regex sacremoses requests packaging filelock numpy tokenizers'.split()
transformers.dependency_versions_check.dep_version_check(pkg,hint=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_flax_logits_process.py----------------------------------------
A:transformers.generation_flax_logits_process.logger->get_logger(__name__)
A:transformers.generation_flax_logits_process.scores->jax.numpy.where(apply_penalty, jax.ops.index_update(scores, jax.ops.index[:, self.eos_token_id], -float('inf')), scores)
A:transformers.generation_flax_logits_process.(topk_scores, topk_indices)->jax.lax.top_k(scores, topk)
A:transformers.generation_flax_logits_process.mask_scores->jax.numpy.full_like(scores, self.filter_value)
A:transformers.generation_flax_logits_process.cumulative_probs->jax.nn.softmax(topk_scores, axis=-1).cumsum(axis=-1)
A:transformers.generation_flax_logits_process.score_mask->jax.ops.index_update(score_mask, jax.ops.index[:, :self.min_tokens_to_keep], True)
A:transformers.generation_flax_logits_process.topk_next_scores->jax.numpy.where(score_mask, topk_scores, mask_scores)
A:transformers.generation_flax_logits_process.next_scores_flat->jax.ops.index_update(next_scores_flat, topk_indices_flat, topk_scores_flat)
A:transformers.generation_flax_logits_process.topk->min(max(self.top_k, self.min_tokens_to_keep), scores.shape[-1])
A:transformers.generation_flax_logits_process.shift->jax.numpy.broadcast_to((jnp.arange(batch_size) * vocab_size)[:, None], (batch_size, topk)).flatten()
A:transformers.generation_flax_logits_process.topk_scores_flat->topk_scores.flatten()
A:transformers.generation_flax_logits_process.next_scores->jax.ops.index_update(next_scores_flat, topk_indices_flat, topk_scores_flat).reshape(batch_size, vocab_size)
A:transformers.generation_flax_logits_process.new_scores->jax.numpy.full(scores.shape, -float('inf'))
transformers.FlaxForcedBOSTokenLogitsProcessor(self,bos_token_id:int)
transformers.FlaxForcedEOSTokenLogitsProcessor(self,max_length:int,eos_token_id:int)
transformers.FlaxLogitsProcessor(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray)
transformers.FlaxLogitsProcessorList(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray,cur_len:int,**kwargs)
transformers.FlaxLogitsWarper(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray)
transformers.FlaxMinLengthLogitsProcessor(self,min_length:int,eos_token_id:int)
transformers.FlaxTemperatureLogitsWarper(self,temperature:float)
transformers.FlaxTopKLogitsWarper(self,top_k:int,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.FlaxTopPLogitsWarper(self,top_p:float,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_flax_logits_process.FlaxForcedBOSTokenLogitsProcessor(self,bos_token_id:int)
transformers.generation_flax_logits_process.FlaxForcedBOSTokenLogitsProcessor.__init__(self,bos_token_id:int)
transformers.generation_flax_logits_process.FlaxForcedEOSTokenLogitsProcessor(self,max_length:int,eos_token_id:int)
transformers.generation_flax_logits_process.FlaxForcedEOSTokenLogitsProcessor.__init__(self,max_length:int,eos_token_id:int)
transformers.generation_flax_logits_process.FlaxLogitsProcessor(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray)
transformers.generation_flax_logits_process.FlaxLogitsProcessor.__call__(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray)
transformers.generation_flax_logits_process.FlaxLogitsProcessorList(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray,cur_len:int,**kwargs)
transformers.generation_flax_logits_process.FlaxLogitsProcessorList.__call__(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray,cur_len:int,**kwargs)
transformers.generation_flax_logits_process.FlaxLogitsWarper(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray)
transformers.generation_flax_logits_process.FlaxLogitsWarper.__call__(self,input_ids:jax_xla.DeviceArray,scores:jax_xla.DeviceArray)
transformers.generation_flax_logits_process.FlaxMinLengthLogitsProcessor(self,min_length:int,eos_token_id:int)
transformers.generation_flax_logits_process.FlaxMinLengthLogitsProcessor.__init__(self,min_length:int,eos_token_id:int)
transformers.generation_flax_logits_process.FlaxTemperatureLogitsWarper(self,temperature:float)
transformers.generation_flax_logits_process.FlaxTemperatureLogitsWarper.__init__(self,temperature:float)
transformers.generation_flax_logits_process.FlaxTopKLogitsWarper(self,top_k:int,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_flax_logits_process.FlaxTopKLogitsWarper.__init__(self,top_k:int,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_flax_logits_process.FlaxTopPLogitsWarper(self,top_p:float,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_flax_logits_process.FlaxTopPLogitsWarper.__init__(self,top_p:float,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/training_args.py----------------------------------------
A:transformers.training_args.logger->utils.logging.get_logger(__name__)
A:transformers.training_args.log_levels->utils.logging.get_log_levels_dict().copy()
A:transformers.training_args.trainer_log_levels->dict(**log_levels, passive=-1)
A:transformers.training_args.current_time->datetime.datetime.now().strftime('%b%d_%H-%M-%S')
A:transformers.training_args.env_local_rank->int(os.environ.get('LOCAL_RANK', -1))
A:transformers.training_args.self.output_dir->os.path.expanduser(self.output_dir)
A:transformers.training_args.self.logging_dir->os.path.expanduser(self.logging_dir)
A:transformers.training_args.self.evaluation_strategy->IntervalStrategy(self.evaluation_strategy)
A:transformers.training_args.self.logging_strategy->IntervalStrategy(self.logging_strategy)
A:transformers.training_args.self.save_strategy->IntervalStrategy(self.save_strategy)
A:transformers.training_args.self.lr_scheduler_type->SchedulerType(self.lr_scheduler_type)
A:transformers.training_args.self.report_to->get_available_reporting_integrations()
A:transformers.training_args.self.hf_deepspeed_config->HfTrainerDeepSpeedConfig(self.deepspeed)
A:transformers.training_args.self_as_dict->asdict(self)
A:transformers.training_args.device->torch.device('cuda', self.local_rank)
A:transformers.training_args.local_rank->smdistributed.modelparallel.torch.local_rank()
A:transformers.training_args.self.local_rank->int(os.environ.get('LOCAL_RANK', '-1'))
A:transformers.training_args.self._n_gpu->torch.cuda.device_count()
A:transformers.training_args.d->self.to_dict()
transformers.TrainingArguments
transformers.TrainingArguments.__post_init__(self)
transformers.TrainingArguments.__str__(self)
transformers.TrainingArguments._no_sync_in_gradient_accumulation(self)
transformers.TrainingArguments._setup_devices(self)->'torch.device'
transformers.TrainingArguments.device(self)->'torch.device'
transformers.TrainingArguments.eval_batch_size(self)->int
transformers.TrainingArguments.get_process_log_level(self)
transformers.TrainingArguments.local_process_index(self)
transformers.TrainingArguments.n_gpu(self)
transformers.TrainingArguments.parallel_mode(self)
transformers.TrainingArguments.place_model_on_device(self)
transformers.TrainingArguments.process_index(self)
transformers.TrainingArguments.should_log(self)
transformers.TrainingArguments.should_save(self)
transformers.TrainingArguments.to_dict(self)
transformers.TrainingArguments.to_json_string(self)
transformers.TrainingArguments.to_sanitized_dict(self)->Dict[str, Any]
transformers.TrainingArguments.train_batch_size(self)->int
transformers.TrainingArguments.world_size(self)
transformers.training_args.ParallelMode(Enum)
transformers.training_args.TrainingArguments
transformers.training_args.TrainingArguments.__post_init__(self)
transformers.training_args.TrainingArguments.__str__(self)
transformers.training_args.TrainingArguments._no_sync_in_gradient_accumulation(self)
transformers.training_args.TrainingArguments._setup_devices(self)->'torch.device'
transformers.training_args.TrainingArguments.device(self)->'torch.device'
transformers.training_args.TrainingArguments.eval_batch_size(self)->int
transformers.training_args.TrainingArguments.get_process_log_level(self)
transformers.training_args.TrainingArguments.local_process_index(self)
transformers.training_args.TrainingArguments.n_gpu(self)
transformers.training_args.TrainingArguments.parallel_mode(self)
transformers.training_args.TrainingArguments.place_model_on_device(self)
transformers.training_args.TrainingArguments.process_index(self)
transformers.training_args.TrainingArguments.should_log(self)
transformers.training_args.TrainingArguments.should_save(self)
transformers.training_args.TrainingArguments.to_dict(self)
transformers.training_args.TrainingArguments.to_json_string(self)
transformers.training_args.TrainingArguments.to_sanitized_dict(self)->Dict[str, Any]
transformers.training_args.TrainingArguments.train_batch_size(self)->int
transformers.training_args.TrainingArguments.world_size(self)
transformers.training_args.default_logdir()->str


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/activations.py----------------------------------------
A:transformers.activations.logger->utils.logging.get_logger(__name__)
transformers.activations._gelu_python(x)
transformers.activations._mish_python(x)
transformers.activations._silu_python(x)
transformers.activations.gelu_fast(x)
transformers.activations.gelu_new(x)
transformers.activations.get_activation(activation_string)
transformers.activations.linear_act(x)
transformers.activations.quick_gelu(x)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_fx_utils.py----------------------------------------
A:transformers.modeling_fx_utils.logger->logging.get_logger(__name__)
A:transformers.modeling_fx_utils.method->getattr(torch.Tensor, method_name)
A:transformers.modeling_fx_utils.cache->getattr(model, cache_name)
A:transformers.modeling_fx_utils.res->getattr(model, cache_name).pop(0)
A:transformers.modeling_fx_utils.bound_method->getattr(torch.Tensor, method_name).__get__(proxy, proxy.__class__)
A:transformers.modeling_fx_utils.original_method->getattr(torch.Tensor, method_name)
A:transformers.modeling_fx_utils.cache_names->dict()
A:transformers.modeling_fx_utils.original_methods->dict()
A:transformers.modeling_fx_utils.original_methods[method_name]->getattr(torch.Tensor, method_name)
A:transformers.modeling_fx_utils.torch_version->packaging.version.parse(importlib_metadata.version('torch'))
A:transformers.modeling_fx_utils.p->HFProxy(node, self)
A:transformers.modeling_fx_utils.inputs_dict->dict()
A:transformers.modeling_fx_utils.inputs_dict['labels']->torch.zeros(self.encoder_shape, dtype=torch.long, device=device)
A:transformers.modeling_fx_utils.inputs_dict['start_positions']->torch.zeros(batch_size, dtype=torch.long, device=device)
A:transformers.modeling_fx_utils.inputs_dict['end_positions']->torch.zeros(batch_size, dtype=torch.long, device=device)
A:transformers.modeling_fx_utils.inputs_dict[input_name]->torch.ones(shape, dtype=torch.float, device=device)
A:transformers.modeling_fx_utils.inputs->dict()
A:transformers.modeling_fx_utils.clone->copy.deepcopy(model)
A:transformers.modeling_fx_utils.(cache_names, original_methods)->_monkey_patch_tensor_methods_for_model_recording(clone, method_names)
A:transformers.modeling_fx_utils.sig->inspect.signature(model.forward)
A:transformers.modeling_fx_utils.graph->super().trace(root, concrete_args=concrete_args)
A:transformers.modeling_fx_utils.path->self._insert_module_as_submodule(mod)
A:transformers.modeling_fx_utils.input_names->model.dummy_inputs.keys()
A:transformers.modeling_fx_utils.tracer->HFTracer(batch_size=batch_size, sequence_length=sequence_length, num_choices=num_choices)
A:transformers.modeling_fx_utils.traced_graph->HFTracer(batch_size=batch_size, sequence_length=sequence_length, num_choices=num_choices).trace(model, concrete_args=concrete_args)
A:transformers.modeling_fx_utils.traced->torch.fx.GraphModule(model, traced_graph)
transformers.modeling_fx_utils.HFProxy(self,node:Node,tracer:Optional[Tracer]=None)
transformers.modeling_fx_utils.HFProxy.__contains__(self,key)
transformers.modeling_fx_utils.HFProxy.__init__(self,node:Node,tracer:Optional[Tracer]=None)
transformers.modeling_fx_utils.HFProxy.__setitem__(self,key,value)
transformers.modeling_fx_utils.HFProxy.shape(self)
transformers.modeling_fx_utils.HFTracer(self,batch_size=1,sequence_length=[128,128],num_choices=-1)
transformers.modeling_fx_utils.HFTracer.__init__(self,batch_size=1,sequence_length=[128,128],num_choices=-1)
transformers.modeling_fx_utils.HFTracer._generate_dummy_input(self,model,input_name)
transformers.modeling_fx_utils.HFTracer._insert_module_as_submodule(self,mod)
transformers.modeling_fx_utils.HFTracer.create_arg(self,a:Any)->Argument
transformers.modeling_fx_utils.HFTracer.path_of_module(self,mod:nn.Module)->str
transformers.modeling_fx_utils.HFTracer.proxy(self,node:Node)
transformers.modeling_fx_utils.HFTracer.record(self,model,input_names,method_names=None)
transformers.modeling_fx_utils.HFTracer.trace(self,root:PreTrainedModel,concrete_args:Optional[Dict[str,Any]]=None,method_names=None)->Graph
transformers.modeling_fx_utils._create_recorded_proxy_method(proxy,method_name,cache_name)
transformers.modeling_fx_utils._monkey_patch_tensor_methods_for_model_recording(model,method_names)
transformers.modeling_fx_utils._reset_tensor_methods(original_methods)
transformers.modeling_fx_utils._wrap_method_for_model_recording(model,method_name,cache_name)
transformers.modeling_fx_utils._wrap_method_for_model_tracing(model,method_name,cache_name)
transformers.modeling_fx_utils.symbolic_trace(model:PreTrainedModel,input_names:Optional[List[str]]=None,batch_size:int=1,sequence_length:Union[int,List[int]]=[128,128],num_choices:int=-1)->GraphModule


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_utils.py----------------------------------------
A:transformers.modeling_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_utils.mask->mask.view(-1).contiguous().eq(1).view(-1).contiguous().eq(1)
A:transformers.modeling_utils.gen->parameter._named_members(get_members_fn=find_tensor_attributes)
A:transformers.modeling_utils.first_tuple->next(gen)
A:transformers.modeling_utils.process->psutil.Process(os.getpid())
A:transformers.modeling_utils.mem->psutil.Process(os.getpid()).memory_info()
A:transformers.modeling_utils.encoder_extended_attention_mask->encoder_extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_utils.seq_ids->torch.arange(seq_length, device=device)
A:transformers.modeling_utils.causal_mask->torch.cat([torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype), causal_mask], axis=-1)
A:transformers.modeling_utils.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_utils.head_mask->head_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_utils.base_model->getattr(self, self.base_model_prefix, self)
A:transformers.modeling_utils.output_embeddings->getattr(self, self.base_model_prefix).get_output_embeddings()
A:transformers.modeling_utils.self->getattr(self, self.base_model_prefix)
A:transformers.modeling_utils.all_encoder_weights->set([module_name + '/' + sub_name for sub_name in encoder_modules.keys()])
A:transformers.modeling_utils.encoder_name->str(int(name) + encoder_layer_pos)
A:transformers.modeling_utils.output_embeddings.weight->torch.nn.Parameter(input_embeddings.weight.clone())
A:transformers.modeling_utils.output_embeddings.bias.data->torch.nn.functional.pad(output_embeddings.bias.data, (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0]), 'constant', 0)
A:transformers.modeling_utils.model_embeds->getattr(self, self.base_model_prefix)._resize_token_embeddings(new_num_tokens)
A:transformers.modeling_utils.old_embeddings->getattr(self, self.base_model_prefix).get_input_embeddings()
A:transformers.modeling_utils.new_embeddings->torch.nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device, dtype=old_embeddings.weight.dtype)
A:transformers.modeling_utils.old_lm_head->getattr(self, self.base_model_prefix).get_output_embeddings()
A:transformers.modeling_utils.new_lm_head->torch.nn.Linear(*new_lm_head_shape, bias=has_new_lm_head_bias).to(self.device)
A:transformers.modeling_utils.(old_num_tokens, old_embedding_dim)->getattr(self, self.base_model_prefix).get_input_embeddings().weight.size()
A:transformers.modeling_utils.n->min(old_num_tokens, new_num_tokens)
A:transformers.modeling_utils.num_tokens_to_copy->min(old_num_tokens, new_num_tokens)
A:transformers.modeling_utils.self.config.pruned_heads[layer]->list(union_heads)
A:transformers.modeling_utils.commit_message->kwargs.pop('commit_message', None)
A:transformers.modeling_utils.repo->getattr(self, self.base_model_prefix)._create_or_get_repo(save_directory, **kwargs)
A:transformers.modeling_utils.model_to_save->unwrap_model(self)
A:transformers.modeling_utils.state_dict->state_dict.copy().copy()
A:transformers.modeling_utils.output_model_file->os.path.join(save_directory, WEIGHTS_NAME)
A:transformers.modeling_utils.url->getattr(self, self.base_model_prefix)._push_to_hub(repo, commit_message=commit_message)
A:transformers.modeling_utils.config->kwargs.pop('config', None)
A:transformers.modeling_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_utils.from_tf->kwargs.pop('from_tf', False)
A:transformers.modeling_utils.from_flax->kwargs.pop('from_flax', False)
A:transformers.modeling_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.modeling_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_utils.output_loading_info->kwargs.pop('output_loading_info', False)
A:transformers.modeling_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.modeling_utils.use_auth_token->kwargs.pop('use_auth_token', None)
A:transformers.modeling_utils.revision->kwargs.pop('revision', None)
A:transformers.modeling_utils.mirror->kwargs.pop('mirror', None)
A:transformers.modeling_utils.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.modeling_utils.from_auto_class->kwargs.pop('_from_auto', False)
A:transformers.modeling_utils._fast_init->kwargs.pop('_fast_init', True)
A:transformers.modeling_utils.(config, model_kwargs)->cls.config_class.from_pretrained(config_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only, use_auth_token=use_auth_token, revision=revision, _from_auto=from_auto_class, _from_pipeline=from_pipeline, **kwargs)
A:transformers.modeling_utils.pretrained_model_name_or_path->str(pretrained_model_name_or_path)
A:transformers.modeling_utils.archive_file->hf_bucket_url(pretrained_model_name_or_path, filename=filename, revision=revision, mirror=mirror)
A:transformers.modeling_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent)
A:transformers.modeling_utils.model->load_flax_checkpoint_in_pytorch_model(model, resolved_archive_file)
A:transformers.modeling_utils.(model, missing_keys, unexpected_keys, error_msgs)->cls._load_state_dict_into_model(model, state_dict, pretrained_model_name_or_path, _fast_init=_fast_init)
A:transformers.modeling_utils.new_key->key.replace('beta', 'bias')
A:transformers.modeling_utils.state_dict[new_key]->state_dict.copy().copy().pop(old_key)
A:transformers.modeling_utils.expected_keys->list(model.state_dict().keys())
A:transformers.modeling_utils.loaded_keys->list(state_dict.keys())
A:transformers.modeling_utils.has_prefix_module->any((s.startswith(prefix) for s in loaded_keys))
A:transformers.modeling_utils.expects_prefix_module->any((s.startswith(prefix) for s in expected_keys))
A:transformers.modeling_utils.missing_keys->list(set(expected_keys) - set(loaded_keys))
A:transformers.modeling_utils.unexpected_keys->list(set(loaded_keys) - set(expected_keys))
A:transformers.modeling_utils.unintialized_modules->load_flax_checkpoint_in_pytorch_model(model, resolved_archive_file).retrieve_modules_from_names(missing_keys, add_prefix=add_prefix, remove_prefix=remove_prefix)
A:transformers.modeling_utils.metadata->getattr(state_dict, '_metadata', None)
A:transformers.modeling_utils.model_to_load->getattr(model, cls.base_model_prefix)
A:transformers.modeling_utils.error_msg->'\n\t'.join(error_msgs)
A:transformers.modeling_utils.module_keys->module_keys.union(set(['.'.join(key.split('.')[:-2]) for key in names if key[-1].isdigit()])).union(set(['.'.join(key.split('.')[:-2]) for key in names if key[-1].isdigit()]))
A:transformers.modeling_utils.w->torch.empty(nx, nf)
A:transformers.modeling_utils.self.weight->torch.nn.Parameter(w)
A:transformers.modeling_utils.self.bias->torch.nn.Parameter(torch.zeros(nf))
A:transformers.modeling_utils.x->getattr(self, self.base_model_prefix).dense_1(x).squeeze(-1)
A:transformers.modeling_utils.self.dense->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_utils.self.dense_0->torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
A:transformers.modeling_utils.self.activation->torch.nn.Tanh()
A:transformers.modeling_utils.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_utils.self.dense_1->torch.nn.Linear(config.hidden_size, 1, bias=False)
A:transformers.modeling_utils.start_positions->start_positions[:, None, None].expand(-1, -1, hsz)
A:transformers.modeling_utils.start_states->torch.einsum('blh,bl->bh', hidden_states, start_log_probs)
A:transformers.modeling_utils.cls_index->cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),)).expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))
A:transformers.modeling_utils.cls_token_state->hidden_states.gather(-2, cls_index).squeeze(-2)
A:transformers.modeling_utils.self.start_logits->PoolerStartLogits(config)
A:transformers.modeling_utils.self.end_logits->PoolerEndLogits(config)
A:transformers.modeling_utils.self.answer_class->PoolerAnswerClass(config)
A:transformers.modeling_utils.start_logits->getattr(self, self.base_model_prefix).start_logits(hidden_states, p_mask=p_mask)
A:transformers.modeling_utils.end_logits->getattr(self, self.base_model_prefix).end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)
A:transformers.modeling_utils.loss_fct->CrossEntropyLoss()
A:transformers.modeling_utils.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_utils.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_utils.cls_logits->getattr(self, self.base_model_prefix).answer_class(hidden_states, start_states=start_states, cls_index=cls_index)
A:transformers.modeling_utils.loss_fct_cls->torch.nn.BCEWithLogitsLoss()
A:transformers.modeling_utils.cls_loss->loss_fct_cls(cls_logits, is_impossible)
A:transformers.modeling_utils.(bsz, slen, hsz)->hidden_states.size()
A:transformers.modeling_utils.start_log_probs->torch.nn.functional.softmax(start_logits, dim=-1)
A:transformers.modeling_utils.(start_top_log_probs, start_top_index)->torch.topk(start_log_probs, self.start_n_top, dim=-1)
A:transformers.modeling_utils.start_top_index_exp->start_top_index.unsqueeze(-1).expand(-1, -1, hsz)
A:transformers.modeling_utils.hidden_states_expanded->hidden_states.unsqueeze(2).expand_as(start_states)
A:transformers.modeling_utils.end_log_probs->torch.nn.functional.softmax(end_logits, dim=1)
A:transformers.modeling_utils.(end_top_log_probs, end_top_index)->torch.topk(end_log_probs, self.end_n_top, dim=1)
A:transformers.modeling_utils.end_top_log_probs->end_top_log_probs.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_utils.end_top_index->end_top_index.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_utils.self.summary_type->getattr(config, 'summary_type', 'last')
A:transformers.modeling_utils.self.summary->torch.nn.Linear(config.hidden_size, num_classes)
A:transformers.modeling_utils.activation_string->getattr(config, 'summary_activation', None)
A:transformers.modeling_utils.self.first_dropout->torch.nn.Dropout(config.summary_first_dropout)
A:transformers.modeling_utils.self.last_dropout->torch.nn.Dropout(config.summary_last_dropout)
A:transformers.modeling_utils.output->getattr(self, self.base_model_prefix).last_dropout(output)
A:transformers.modeling_utils.index->index.to(layer.weight.device).to(layer.weight.device)
A:transformers.modeling_utils.W->layer.weight.index_select(dim, index).clone().detach()
A:transformers.modeling_utils.b->layer.bias[index].clone().detach()
A:transformers.modeling_utils.new_size->list(layer.weight.size())
A:transformers.modeling_utils.new_size[dim]->len(index)
A:transformers.modeling_utils.new_layer->Conv1D(new_size[1], new_size[0]).to(layer.weight.device)
A:transformers.modeling_utils.num_args_in_forward_chunk_fn->len(inspect.signature(forward_fn).parameters)
A:transformers.modeling_utils.input_tensors_chunks->tuple((input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors))
A:transformers.modeling_utils.output_chunks->tuple((forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks)))
transformers.Conv1D(self,nf,nx)
transformers.Conv1D.forward(self,x)
transformers.PreTrainedModel(self,config:PretrainedConfig,*inputs,**kwargs)
transformers.PreTrainedModel._get_resized_embeddings(self,old_embeddings:nn.Embedding,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.PreTrainedModel._get_resized_lm_head(self,old_lm_head:nn.Linear,new_num_tokens:Optional[int]=None,transposed:Optional[bool]=False)->nn.Linear
transformers.PreTrainedModel._init_weights(self,module)
transformers.PreTrainedModel._load_state_dict_into_model(cls,model,state_dict,pretrained_model_name_or_path,_fast_init=True)
transformers.PreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.PreTrainedModel._tie_encoder_decoder_weights(encoder:nn.Module,decoder:nn.Module,base_model_prefix:str)
transformers.PreTrainedModel._tie_or_clone_weights(self,output_embeddings,input_embeddings)
transformers.PreTrainedModel.base_model(self)->nn.Module
transformers.PreTrainedModel.dummy_inputs(self)->Dict[str, torch.Tensor]
transformers.PreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.PreTrainedModel.get_input_embeddings(self)->nn.Module
transformers.PreTrainedModel.get_output_embeddings(self)->nn.Module
transformers.PreTrainedModel.init_weights(self)
transformers.PreTrainedModel.prune_heads(self,heads_to_prune:Dict[int,List[int]])
transformers.PreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.PreTrainedModel.retrieve_modules_from_names(self,names,add_prefix=False,remove_prefix=False)
transformers.PreTrainedModel.save_pretrained(self,save_directory:Union[str,os.PathLike],save_config:bool=True,state_dict:Optional[dict]=None,save_function:Callable=torch.save,push_to_hub:bool=False,**kwargs)
transformers.PreTrainedModel.set_input_embeddings(self,value:nn.Module)
transformers.PreTrainedModel.tie_weights(self)
transformers.apply_chunking_to_forward(forward_fn:Callable[...,torch.Tensor],chunk_size:int,chunk_dim:int,*input_tensors)->torch.Tensor
transformers.modeling_utils.Conv1D(self,nf,nx)
transformers.modeling_utils.Conv1D.__init__(self,nf,nx)
transformers.modeling_utils.Conv1D.forward(self,x)
transformers.modeling_utils.ModuleUtilsMixin
transformers.modeling_utils.ModuleUtilsMixin._convert_head_mask_to_5d(self,head_mask,num_hidden_layers)
transformers.modeling_utils.ModuleUtilsMixin._hook_rss_memory_post_forward(module,*args,**kwargs)
transformers.modeling_utils.ModuleUtilsMixin._hook_rss_memory_pre_forward(module,*args,**kwargs)
transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks(self)
transformers.modeling_utils.ModuleUtilsMixin.device(self)->device
transformers.modeling_utils.ModuleUtilsMixin.dtype(self)->dtype
transformers.modeling_utils.ModuleUtilsMixin.estimate_tokens(self,input_dict:Dict[str,Union[torch.Tensor,Any]])->int
transformers.modeling_utils.ModuleUtilsMixin.floating_point_ops(self,input_dict:Dict[str,Union[torch.Tensor,Any]],exclude_embeddings:bool=True)->int
transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask(self,attention_mask:Tensor,input_shape:Tuple[int],device:device)->Tensor
transformers.modeling_utils.ModuleUtilsMixin.get_head_mask(self,head_mask:Optional[Tensor],num_hidden_layers:int,is_attention_chunked:bool=False)->Tensor
transformers.modeling_utils.ModuleUtilsMixin.invert_attention_mask(self,encoder_attention_mask:Tensor)->Tensor
transformers.modeling_utils.ModuleUtilsMixin.num_parameters(self,only_trainable:bool=False,exclude_embeddings:bool=False)->int
transformers.modeling_utils.ModuleUtilsMixin.reset_memory_hooks_state(self)
transformers.modeling_utils.PoolerAnswerClass(self,config)
transformers.modeling_utils.PoolerAnswerClass.__init__(self,config)
transformers.modeling_utils.PoolerAnswerClass.forward(self,hidden_states:torch.FloatTensor,start_states:Optional[torch.FloatTensor]=None,start_positions:Optional[torch.LongTensor]=None,cls_index:Optional[torch.LongTensor]=None)->torch.FloatTensor
transformers.modeling_utils.PoolerEndLogits(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerEndLogits.__init__(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerEndLogits.forward(self,hidden_states:torch.FloatTensor,start_states:Optional[torch.FloatTensor]=None,start_positions:Optional[torch.LongTensor]=None,p_mask:Optional[torch.FloatTensor]=None)->torch.FloatTensor
transformers.modeling_utils.PoolerStartLogits(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerStartLogits.__init__(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerStartLogits.forward(self,hidden_states:torch.FloatTensor,p_mask:Optional[torch.FloatTensor]=None)->torch.FloatTensor
transformers.modeling_utils.PreTrainedModel(self,config:PretrainedConfig,*inputs,**kwargs)
transformers.modeling_utils.PreTrainedModel.__init__(self,config:PretrainedConfig,*inputs,**kwargs)
transformers.modeling_utils.PreTrainedModel._get_resized_embeddings(self,old_embeddings:nn.Embedding,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.modeling_utils.PreTrainedModel._get_resized_lm_head(self,old_lm_head:nn.Linear,new_num_tokens:Optional[int]=None,transposed:Optional[bool]=False)->nn.Linear
transformers.modeling_utils.PreTrainedModel._init_weights(self,module)
transformers.modeling_utils.PreTrainedModel._load_state_dict_into_model(cls,model,state_dict,pretrained_model_name_or_path,_fast_init=True)
transformers.modeling_utils.PreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_utils.PreTrainedModel._tie_encoder_decoder_weights(encoder:nn.Module,decoder:nn.Module,base_model_prefix:str)
transformers.modeling_utils.PreTrainedModel._tie_or_clone_weights(self,output_embeddings,input_embeddings)
transformers.modeling_utils.PreTrainedModel.base_model(self)->nn.Module
transformers.modeling_utils.PreTrainedModel.dummy_inputs(self)->Dict[str, torch.Tensor]
transformers.modeling_utils.PreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.modeling_utils.PreTrainedModel.get_input_embeddings(self)->nn.Module
transformers.modeling_utils.PreTrainedModel.get_output_embeddings(self)->nn.Module
transformers.modeling_utils.PreTrainedModel.init_weights(self)
transformers.modeling_utils.PreTrainedModel.prune_heads(self,heads_to_prune:Dict[int,List[int]])
transformers.modeling_utils.PreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.modeling_utils.PreTrainedModel.retrieve_modules_from_names(self,names,add_prefix=False,remove_prefix=False)
transformers.modeling_utils.PreTrainedModel.save_pretrained(self,save_directory:Union[str,os.PathLike],save_config:bool=True,state_dict:Optional[dict]=None,save_function:Callable=torch.save,push_to_hub:bool=False,**kwargs)
transformers.modeling_utils.PreTrainedModel.set_input_embeddings(self,value:nn.Module)
transformers.modeling_utils.PreTrainedModel.tie_weights(self)
transformers.modeling_utils.SQuADHead(self,config)
transformers.modeling_utils.SQuADHead.__init__(self,config)
transformers.modeling_utils.SQuADHead.forward(self,hidden_states:torch.FloatTensor,start_positions:Optional[torch.LongTensor]=None,end_positions:Optional[torch.LongTensor]=None,cls_index:Optional[torch.LongTensor]=None,is_impossible:Optional[torch.LongTensor]=None,p_mask:Optional[torch.FloatTensor]=None,return_dict:bool=False)->Union[SquadHeadOutput, Tuple[torch.FloatTensor]]
transformers.modeling_utils.SequenceSummary(self,config:PretrainedConfig)
transformers.modeling_utils.SequenceSummary.__init__(self,config:PretrainedConfig)
transformers.modeling_utils.SequenceSummary.forward(self,hidden_states:torch.FloatTensor,cls_index:Optional[torch.LongTensor]=None)->torch.FloatTensor
transformers.modeling_utils.SquadHeadOutput(ModelOutput)
transformers.modeling_utils.apply_chunking_to_forward(forward_fn:Callable[...,torch.Tensor],chunk_size:int,chunk_dim:int,*input_tensors)->torch.Tensor
transformers.modeling_utils.find_pruneable_heads_and_indices(heads:List[int],n_heads:int,head_size:int,already_pruned_heads:Set[int])->Tuple[Set[int], torch.LongTensor]
transformers.modeling_utils.get_parameter_device(parameter:Union[nn.Module,GenerationMixin,'ModuleUtilsMixin'])
transformers.modeling_utils.get_parameter_dtype(parameter:Union[nn.Module,GenerationMixin,'ModuleUtilsMixin'])
transformers.modeling_utils.no_init_weights(_enable=True)
transformers.modeling_utils.prune_conv1d_layer(layer:Conv1D,index:torch.LongTensor,dim:int=1)->Conv1D
transformers.modeling_utils.prune_layer(layer:Union[nn.Linear,Conv1D],index:torch.LongTensor,dim:Optional[int]=None)->Union[nn.Linear, Conv1D]
transformers.modeling_utils.prune_linear_layer(layer:nn.Linear,index:torch.LongTensor,dim:int=0)->nn.Linear
transformers.modeling_utils.unwrap_model(model:nn.Module)->nn.Module
transformers.prune_layer(layer:Union[nn.Linear,Conv1D],index:torch.LongTensor,dim:Optional[int]=None)->Union[nn.Linear, Conv1D]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/trainer_callback.py----------------------------------------
A:transformers.trainer_callback.logger->utils.logging.get_logger(__name__)
A:transformers.trainer_callback.text->f.read()
A:transformers.trainer_callback.result->getattr(callback, event)(args, state, control, model=self.model, tokenizer=self.tokenizer, optimizer=self.optimizer, lr_scheduler=self.lr_scheduler, train_dataloader=self.train_dataloader, eval_dataloader=self.eval_dataloader, **kwargs)
A:transformers.trainer_callback.self.training_bar->tqdm(total=state.max_steps)
A:transformers.trainer_callback.self.prediction_bar->tqdm(total=len(eval_dataloader), leave=self.training_bar is None)
A:transformers.trainer_callback._->logs.pop('total_flos', None)
A:transformers.trainer_callback.metric_value->metrics.get(metric_to_check)
transformers.DefaultFlowCallback(TrainerCallback)
transformers.DefaultFlowCallback.on_epoch_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.DefaultFlowCallback.on_step_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.EarlyStoppingCallback(self,early_stopping_patience:int=1,early_stopping_threshold:Optional[float]=0.0)
transformers.EarlyStoppingCallback.check_metric_value(self,args,state,control,metric_value)
transformers.EarlyStoppingCallback.on_evaluate(self,args,state,control,metrics,**kwargs)
transformers.EarlyStoppingCallback.on_train_begin(self,args,state,control,**kwargs)
transformers.PrinterCallback(TrainerCallback)
transformers.PrinterCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.ProgressCallback(self)
transformers.ProgressCallback.on_evaluate(self,args,state,control,**kwargs)
transformers.ProgressCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.ProgressCallback.on_prediction_step(self,args,state,control,eval_dataloader=None,**kwargs)
transformers.ProgressCallback.on_step_end(self,args,state,control,**kwargs)
transformers.ProgressCallback.on_train_begin(self,args,state,control,**kwargs)
transformers.ProgressCallback.on_train_end(self,args,state,control,**kwargs)
transformers.TrainerCallback
transformers.TrainerCallback.on_epoch_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_epoch_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_evaluate(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_init_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_log(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_prediction_step(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_save(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_step_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_step_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_train_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerCallback.on_train_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.TrainerControl
transformers.TrainerControl._new_epoch(self)
transformers.TrainerControl._new_step(self)
transformers.TrainerControl._new_training(self)
transformers.TrainerState
transformers.TrainerState.__post_init__(self)
transformers.TrainerState.load_from_json(cls,json_path:str)
transformers.TrainerState.save_to_json(self,json_path:str)
transformers.trainer_callback.CallbackHandler(self,callbacks,model,tokenizer,optimizer,lr_scheduler)
transformers.trainer_callback.CallbackHandler.__init__(self,callbacks,model,tokenizer,optimizer,lr_scheduler)
transformers.trainer_callback.CallbackHandler.add_callback(self,callback)
transformers.trainer_callback.CallbackHandler.call_event(self,event,args,state,control,**kwargs)
transformers.trainer_callback.CallbackHandler.callback_list(self)
transformers.trainer_callback.CallbackHandler.on_epoch_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_epoch_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_evaluate(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,metrics)
transformers.trainer_callback.CallbackHandler.on_init_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_log(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,logs)
transformers.trainer_callback.CallbackHandler.on_prediction_step(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_save(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_step_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_step_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_train_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.on_train_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl)
transformers.trainer_callback.CallbackHandler.pop_callback(self,callback)
transformers.trainer_callback.CallbackHandler.remove_callback(self,callback)
transformers.trainer_callback.DefaultFlowCallback(TrainerCallback)
transformers.trainer_callback.DefaultFlowCallback.on_epoch_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.DefaultFlowCallback.on_step_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.EarlyStoppingCallback(self,early_stopping_patience:int=1,early_stopping_threshold:Optional[float]=0.0)
transformers.trainer_callback.EarlyStoppingCallback.__init__(self,early_stopping_patience:int=1,early_stopping_threshold:Optional[float]=0.0)
transformers.trainer_callback.EarlyStoppingCallback.check_metric_value(self,args,state,control,metric_value)
transformers.trainer_callback.EarlyStoppingCallback.on_evaluate(self,args,state,control,metrics,**kwargs)
transformers.trainer_callback.EarlyStoppingCallback.on_train_begin(self,args,state,control,**kwargs)
transformers.trainer_callback.PrinterCallback(TrainerCallback)
transformers.trainer_callback.PrinterCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.trainer_callback.ProgressCallback(self)
transformers.trainer_callback.ProgressCallback.__init__(self)
transformers.trainer_callback.ProgressCallback.on_evaluate(self,args,state,control,**kwargs)
transformers.trainer_callback.ProgressCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.trainer_callback.ProgressCallback.on_prediction_step(self,args,state,control,eval_dataloader=None,**kwargs)
transformers.trainer_callback.ProgressCallback.on_step_end(self,args,state,control,**kwargs)
transformers.trainer_callback.ProgressCallback.on_train_begin(self,args,state,control,**kwargs)
transformers.trainer_callback.ProgressCallback.on_train_end(self,args,state,control,**kwargs)
transformers.trainer_callback.TrainerCallback
transformers.trainer_callback.TrainerCallback.on_epoch_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_epoch_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_evaluate(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_init_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_log(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_prediction_step(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_save(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_step_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_step_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_train_begin(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerCallback.on_train_end(self,args:TrainingArguments,state:TrainerState,control:TrainerControl,**kwargs)
transformers.trainer_callback.TrainerControl
transformers.trainer_callback.TrainerControl._new_epoch(self)
transformers.trainer_callback.TrainerControl._new_step(self)
transformers.trainer_callback.TrainerControl._new_training(self)
transformers.trainer_callback.TrainerState
transformers.trainer_callback.TrainerState.__post_init__(self)
transformers.trainer_callback.TrainerState.load_from_json(cls,json_path:str)
transformers.trainer_callback.TrainerState.save_to_json(self,json_path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/trainer.py----------------------------------------
A:transformers.trainer.logger->utils.logging.get_logger(__name__)
A:transformers.trainer.args->TrainingArguments(output_dir=output_dir)
A:transformers.trainer.self._memory_tracker->TrainerMemoryTracker(self.args.skip_memory_metrics)
A:transformers.trainer.log_level->TrainingArguments(output_dir=output_dir).get_process_log_level()
A:transformers.trainer.model->model.half().to(self.args.device).half().to(self.args.device)
A:transformers.trainer.self.callback_handler->CallbackHandler(callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler)
A:transformers.trainer.self.scaler->torch.cuda.amp.GradScaler()
A:transformers.trainer.self.label_smoother->LabelSmoother(epsilon=self.args.label_smoothing_factor)
A:transformers.trainer.self.state->trainer_callback.TrainerState.load_from_json(os.path.join(resume_from_checkpoint, 'trainer_state.json'))
A:transformers.trainer.self.control->self.callback_handler.on_prediction_step(self.args, self.state, self.control)
A:transformers.trainer.signature->inspect.signature(self.model.forward)
A:transformers.trainer.self._signature_columns->list(signature.parameters.keys())
A:transformers.trainer.ignored_columns->list(set(dataset.column_names) - set(self._signature_columns))
A:transformers.trainer.generator->torch.Generator()
A:transformers.trainer.train_dataset->IterableDatasetShard(train_dataset, batch_size=self.args.train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)
A:transformers.trainer.train_sampler->self._get_train_sampler()
A:transformers.trainer.eval_dataset->IterableDatasetShard(eval_dataset, batch_size=self.args.eval_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)
A:transformers.trainer.eval_sampler->self._get_eval_sampler(eval_dataset)
A:transformers.trainer.test_dataset->IterableDatasetShard(test_dataset, batch_size=self.args.eval_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)
A:transformers.trainer.test_sampler->self._get_eval_sampler(test_dataset)
A:transformers.trainer.decay_parameters->get_parameter_names(self.model, [nn.LayerNorm])
A:transformers.trainer.self.optimizer->smdistributed.modelparallel.torch.DistributedOptimizer(self.optimizer)
A:transformers.trainer.self.lr_scheduler->get_scheduler(self.args.lr_scheduler_type, self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)
A:transformers.trainer.params->self.hp_space(trial)
A:transformers.trainer.old_attr->getattr(self.args, key, None)
A:transformers.trainer.value->type(old_attr)(value)
A:transformers.trainer.self.args.hf_deepspeed_config->HfDeepSpeedConfig(self.args)
A:transformers.trainer.self.objective->self.compute_objective(metrics.copy())
A:transformers.trainer.output_dir->os.path.join(run_dir, checkpoint_folder)
A:transformers.trainer.model_init_argcount->len(inspect.signature(self.model_init).parameters)
A:transformers.trainer.(model, self.optimizer)->apex.amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)
A:transformers.trainer.self.modelmodel->FullyShardedDDP(model, mixed_precision=mixed_precision, reshard_after_forward=zero_3, cpu_offload=cpu_offload).to(self.args.device)
A:transformers.trainer.self.model->self.model.to(args.device)
A:transformers.trainer.resume_from_checkpoint->get_last_checkpoint(args.output_dir)
A:transformers.trainer.config->configuration_utils.PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))
A:transformers.trainer.state_dict->self.model.state_dict()
A:transformers.trainer.train_dataset_is_sized->isinstance(self.train_dataset, collections.abc.Sized)
A:transformers.trainer.train_dataloader->self.get_train_dataloader()
A:transformers.trainer.num_update_steps_per_epoch->max(num_update_steps_per_epoch, 1)
A:transformers.trainer.max_steps->math.ceil(args.num_train_epochs * num_update_steps_per_epoch)
A:transformers.trainer.num_train_epochs->int(args.num_train_epochs)
A:transformers.trainer.debug_overflow->DebugUnderflowOverflow(self.model)
A:transformers.trainer.(deepspeed_engine, optimizer, lr_scheduler)->deepspeed_init(self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint)
A:transformers.trainer.start_time->time.time()
A:transformers.trainer.steps_trained_progress_bar->tqdm(total=steps_trained_in_current_epoch)
A:transformers.trainer.self.state.is_local_process_zero->self.is_local_process_zero()
A:transformers.trainer.self.state.is_world_process_zero->self.is_world_process_zero()
A:transformers.trainer.tr_loss->torch.tensor(0.0).to(args.device)
A:transformers.trainer.parallel_loader->torch_xla.distributed.parallel_loader.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)
A:transformers.trainer.scale_before->self.scaler.get_scale()
A:transformers.trainer.scale_after->self.scaler.get_scale()
A:transformers.trainer.best_model_path->os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)
A:transformers.trainer.metrics->denumpify_detensorize(metrics)
A:transformers.trainer.load_result->self.model.load_state_dict(state_dict, strict=False)
A:transformers.trainer.tr_loss_scalar->torch.tensor(0.0).to(args.device).item()
A:transformers.trainer.logs['loss']->round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
A:transformers.trainer.logs['learning_rate']->self._get_learning_rate()
A:transformers.trainer.rng_file->os.path.join(checkpoint, 'rng_state.pth')
A:transformers.trainer.checkpoint_rng_state->torch.load(rng_file)
A:transformers.trainer.run_id->ray.tune.get_trial_id()
A:transformers.trainer.run_dir->os.path.join(self.args.output_dir, run_name)
A:transformers.trainer.opt_state_dict->self.optimizer.state_dict()
A:transformers.trainer.rng_states['cuda']->torch.cuda.random.get_rng_state()
A:transformers.trainer.rng_states['xla']->torch_xla.core.xla_model.get_rng_state()
A:transformers.trainer.optimizer_state->torch.load(os.path.join(checkpoint, 'optimizer.pt'), map_location='cpu')
A:transformers.trainer.lr_scheduler_state->torch.load(os.path.join(checkpoint, 'scheduler.pt'), map_location='cpu')
A:transformers.trainer.backend->HPSearchBackend(backend)
A:transformers.trainer.best_run->run_hp_search(self, n_trials, direction, **kwargs)
A:transformers.trainer.logs['epoch']->round(self.state.epoch, 2)
A:transformers.trainer.kwargs->dict(device=self.args.device)
A:transformers.trainer.inputs[k]->v.to(**kwargs)
A:transformers.trainer.inputs->self._prepare_inputs(inputs)
A:transformers.trainer.loss_mb->smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps, scaler=scaler)
A:transformers.trainer.loss->loss.mean().detach().mean().detach()
A:transformers.trainer.labels->nested_detach(tuple((inputs.get(name) for name in self.label_names)))
A:transformers.trainer.outputs->model(**inputs)
A:transformers.trainer.file->os.path.join(output_dir, WEIGHTS_NAME)
A:transformers.trainer.regex_match->re.match(f'.*{checkpoint_prefix}-([0-9]+)', path)
A:transformers.trainer.checkpoints_sorted->self._sorted_checkpoints(use_mtime=use_mtime, output_dir=output_dir)
A:transformers.trainer.best_model_index->self._sorted_checkpoints(use_mtime=use_mtime, output_dir=output_dir).index(str(Path(self.state.best_model_checkpoint)))
A:transformers.trainer.number_of_checkpoints_to_delete->max(0, len(checkpoints_sorted) - save_total_limit)
A:transformers.trainer.eval_dataloader->self.get_eval_dataloader(eval_dataset)
A:transformers.trainer.output->eval_loop(test_dataloader, description='Prediction', ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
A:transformers.trainer.test_dataloader->self.get_test_dataloader(test_dataset)
A:transformers.trainer.(deepspeed_engine, _, _)->deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)
A:transformers.trainer.dataloader->torch_xla.distributed.parallel_loader.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)
A:transformers.trainer.observed_batch_size->find_batch_size(inputs)
A:transformers.trainer.(loss, logits, labels)->self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
A:transformers.trainer.losses->loss.mean().detach().mean().detach().repeat(batch_size)
A:transformers.trainer.logits->nested_detach(logits)
A:transformers.trainer.num_samples->len(eval_dataset)
A:transformers.trainer.all_preds->nested_truncate(all_preds, num_samples)
A:transformers.trainer.all_labels->nested_truncate(all_labels, num_samples)
A:transformers.trainer.metrics[f'{metric_key_prefix}_loss']->eval_losses_gatherer.finalize().mean().item()
A:transformers.trainer.metrics[f'{metric_key_prefix}_{key}']->denumpify_detensorize(metrics).pop(key)
A:transformers.trainer.tensors->distributed_concat(tensors)
A:transformers.trainer.sizes->self._nested_gather(size).cpu()
A:transformers.trainer.max_size->max((s[1] for s in sizes))
A:transformers.trainer.new_size->list(old_size)
A:transformers.trainer.has_labels->all((inputs.get(k) is not None for k in self.label_names))
A:transformers.trainer.ignore_keys->getattr(self.model.config, 'keys_to_ignore_at_inference', [])
A:transformers.trainer.raw_outputs->smp_forward_only(model, inputs)
A:transformers.trainer.logits_mb->tuple((v for (k, v) in raw_outputs.items() if k not in ignore_keys))
A:transformers.trainer.(loss, outputs)->self.compute_loss(model, inputs, return_outputs=True)
A:transformers.trainer.repo_url->file_utils.PushToHubMixin._get_repo_url_from_name(self.args.push_to_hub_model_id, organization=self.args.push_to_hub_organization, use_auth_token=use_auth_token)
A:transformers.trainer.self.repo->file_utils.PushToHubMixin._create_or_get_repo(self.args.output_dir, repo_url=repo_url, use_auth_token=use_auth_token)
A:transformers.trainer.training_summary->modelcard.TrainingSummary.from_trainer(self, language=language, license=license, tags=tags, model_name=model_name, finetuned_from=finetuned_from, tasks=tasks, dataset_tags=dataset_tags, dataset=dataset, dataset_args=dataset_args)
A:transformers.trainer.model_card->modelcard.TrainingSummary.from_trainer(self, language=language, license=license, tags=tags, model_name=model_name, finetuned_from=finetuned_from, tasks=tasks, dataset_tags=dataset_tags, dataset=dataset, dataset_args=dataset_args).to_model_card()
A:transformers.trainer.num_examples->self.num_examples(dataloader)
A:transformers.trainer.world_size->max(1, self.args.world_size)
A:transformers.trainer.eval_losses_gatherer->DistributedTensorGatherer(world_size, num_examples, make_multiple_of=batch_size)
A:transformers.trainer.preds_gatherer->DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)
A:transformers.trainer.labels_gatherer->DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)
A:transformers.trainer.eval_loss->DistributedTensorGatherer(world_size, num_examples, make_multiple_of=batch_size).finalize()
transformers.Trainer(self,model:Union[PreTrainedModel,nn.Module]=None,args:TrainingArguments=None,data_collator:Optional[DataCollator]=None,train_dataset:Optional[Dataset]=None,eval_dataset:Optional[Dataset]=None,tokenizer:Optional[PreTrainedTokenizerBase]=None,model_init:Callable[[],PreTrainedModel]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,callbacks:Optional[List[TrainerCallback]]=None,optimizers:Tuple[torch.optim.Optimizer,torch.optim.lr_scheduler.LambdaLR]=(None,None))
transformers.Trainer._gather_and_numpify(self,tensors,name)
transformers.Trainer._get_eval_sampler(self,eval_dataset:Dataset)->Optional[torch.utils.data.sampler.Sampler]
transformers.Trainer._get_train_sampler(self)->Optional[torch.utils.data.sampler.Sampler]
transformers.Trainer._hp_search_setup(self,trial:Union['optuna.Trial',Dict[str,Any]])
transformers.Trainer._load_optimizer_and_scheduler(self,checkpoint)
transformers.Trainer._load_rng_state(self,checkpoint)
transformers.Trainer._load_state_dict_in_model(self,state_dict)
transformers.Trainer._maybe_log_save_evaluate(self,tr_loss,model,trial,epoch)
transformers.Trainer._nested_gather(self,tensors,name=None)
transformers.Trainer._pad_across_processes(self,tensor,pad_index=-100)
transformers.Trainer._prepare_inputs(self,inputs:Dict[str,Union[torch.Tensor,Any]])->Dict[str, Union[torch.Tensor, Any]]
transformers.Trainer._remove_unused_columns(self,dataset:'datasets.Dataset',description:Optional[str]=None)
transformers.Trainer._report_to_hp_search(self,trial:Union['optuna.Trial',Dict[str,Any]],epoch:int,metrics:Dict[str,float])
transformers.Trainer._rotate_checkpoints(self,use_mtime=False,output_dir=None)->None
transformers.Trainer._save(self,output_dir:Optional[str]=None,state_dict=None)
transformers.Trainer._save_checkpoint(self,model,trial,metrics=None)
transformers.Trainer._save_tpu(self,output_dir:Optional[str]=None)
transformers.Trainer._sorted_checkpoints(self,output_dir=None,checkpoint_prefix=PREFIX_CHECKPOINT_DIR,use_mtime=False)->List[str]
transformers.Trainer._tune_save_checkpoint(self)
transformers.Trainer._wrap_model(self,model,training=True)
transformers.Trainer.add_callback(self,callback)
transformers.Trainer.call_model_init(self,trial=None)
transformers.Trainer.compute_loss(self,model,inputs,return_outputs=False)
transformers.Trainer.create_model_card(self,language:Optional[str]=None,license:Optional[str]=None,tags:Optional[str]=None,model_name:Optional[str]=None,finetuned_from:Optional[str]=None,tasks:Optional[str]=None,dataset_tags:Optional[Union[str,List[str]]]=None,dataset:Optional[Union[str,List[str]]]=None,dataset_args:Optional[Union[str,List[str]]]=None)
transformers.Trainer.create_optimizer(self)
transformers.Trainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.Trainer.create_scheduler(self,num_training_steps:int)
transformers.Trainer.evaluate(self,eval_dataset:Optional[Dataset]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval')->Dict[str, float]
transformers.Trainer.evaluation_loop(self,dataloader:DataLoader,description:str,prediction_loss_only:Optional[bool]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval')->EvalLoopOutput
transformers.Trainer.floating_point_ops(self,inputs:Dict[str,Union[torch.Tensor,Any]])
transformers.Trainer.get_eval_dataloader(self,eval_dataset:Optional[Dataset]=None)->DataLoader
transformers.Trainer.get_test_dataloader(self,test_dataset:Dataset)->DataLoader
transformers.Trainer.get_train_dataloader(self)->DataLoader
transformers.Trainer.hyperparameter_search(self,hp_space:Optional[Callable[['optuna.Trial'],Dict[str,float]]]=None,compute_objective:Optional[Callable[[Dict[str,float]],float]]=None,n_trials:int=20,direction:str='minimize',backend:Optional[Union['str',HPSearchBackend]]=None,hp_name:Optional[Callable[['optuna.Trial'],str]]=None,**kwargs)->BestRun
transformers.Trainer.init_git_repo(self)
transformers.Trainer.is_local_process_zero(self)->bool
transformers.Trainer.is_world_process_zero(self)->bool
transformers.Trainer.log(self,logs:Dict[str,float])->None
transformers.Trainer.num_examples(self,dataloader:DataLoader)->int
transformers.Trainer.pop_callback(self,callback)
transformers.Trainer.predict(self,test_dataset:Dataset,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='test')->PredictionOutput
transformers.Trainer.prediction_loop(self,dataloader:DataLoader,description:str,prediction_loss_only:Optional[bool]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval')->PredictionOutput
transformers.Trainer.prediction_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]],prediction_loss_only:bool,ignore_keys:Optional[List[str]]=None)->Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
transformers.Trainer.push_to_hub(self,commit_message:Optional[str]='addmodel',**kwargs)->str
transformers.Trainer.remove_callback(self,callback)
transformers.Trainer.save_model(self,output_dir:Optional[str]=None)
transformers.Trainer.store_flos(self)
transformers.Trainer.train(self,resume_from_checkpoint:Optional[Union[str,bool]]=None,trial:Union['optuna.Trial',Dict[str,Any]]=None,**kwargs)
transformers.Trainer.training_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]])->torch.Tensor
transformers.trainer.Trainer(self,model:Union[PreTrainedModel,nn.Module]=None,args:TrainingArguments=None,data_collator:Optional[DataCollator]=None,train_dataset:Optional[Dataset]=None,eval_dataset:Optional[Dataset]=None,tokenizer:Optional[PreTrainedTokenizerBase]=None,model_init:Callable[[],PreTrainedModel]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,callbacks:Optional[List[TrainerCallback]]=None,optimizers:Tuple[torch.optim.Optimizer,torch.optim.lr_scheduler.LambdaLR]=(None,None))
transformers.trainer.Trainer.__init__(self,model:Union[PreTrainedModel,nn.Module]=None,args:TrainingArguments=None,data_collator:Optional[DataCollator]=None,train_dataset:Optional[Dataset]=None,eval_dataset:Optional[Dataset]=None,tokenizer:Optional[PreTrainedTokenizerBase]=None,model_init:Callable[[],PreTrainedModel]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,callbacks:Optional[List[TrainerCallback]]=None,optimizers:Tuple[torch.optim.Optimizer,torch.optim.lr_scheduler.LambdaLR]=(None,None))
transformers.trainer.Trainer._gather_and_numpify(self,tensors,name)
transformers.trainer.Trainer._get_eval_sampler(self,eval_dataset:Dataset)->Optional[torch.utils.data.sampler.Sampler]
transformers.trainer.Trainer._get_train_sampler(self)->Optional[torch.utils.data.sampler.Sampler]
transformers.trainer.Trainer._hp_search_setup(self,trial:Union['optuna.Trial',Dict[str,Any]])
transformers.trainer.Trainer._load_optimizer_and_scheduler(self,checkpoint)
transformers.trainer.Trainer._load_rng_state(self,checkpoint)
transformers.trainer.Trainer._load_state_dict_in_model(self,state_dict)
transformers.trainer.Trainer._maybe_log_save_evaluate(self,tr_loss,model,trial,epoch)
transformers.trainer.Trainer._nested_gather(self,tensors,name=None)
transformers.trainer.Trainer._pad_across_processes(self,tensor,pad_index=-100)
transformers.trainer.Trainer._prepare_inputs(self,inputs:Dict[str,Union[torch.Tensor,Any]])->Dict[str, Union[torch.Tensor, Any]]
transformers.trainer.Trainer._remove_unused_columns(self,dataset:'datasets.Dataset',description:Optional[str]=None)
transformers.trainer.Trainer._report_to_hp_search(self,trial:Union['optuna.Trial',Dict[str,Any]],epoch:int,metrics:Dict[str,float])
transformers.trainer.Trainer._rotate_checkpoints(self,use_mtime=False,output_dir=None)->None
transformers.trainer.Trainer._save(self,output_dir:Optional[str]=None,state_dict=None)
transformers.trainer.Trainer._save_checkpoint(self,model,trial,metrics=None)
transformers.trainer.Trainer._save_tpu(self,output_dir:Optional[str]=None)
transformers.trainer.Trainer._sorted_checkpoints(self,output_dir=None,checkpoint_prefix=PREFIX_CHECKPOINT_DIR,use_mtime=False)->List[str]
transformers.trainer.Trainer._tune_save_checkpoint(self)
transformers.trainer.Trainer._wrap_model(self,model,training=True)
transformers.trainer.Trainer.add_callback(self,callback)
transformers.trainer.Trainer.call_model_init(self,trial=None)
transformers.trainer.Trainer.compute_loss(self,model,inputs,return_outputs=False)
transformers.trainer.Trainer.create_model_card(self,language:Optional[str]=None,license:Optional[str]=None,tags:Optional[str]=None,model_name:Optional[str]=None,finetuned_from:Optional[str]=None,tasks:Optional[str]=None,dataset_tags:Optional[Union[str,List[str]]]=None,dataset:Optional[Union[str,List[str]]]=None,dataset_args:Optional[Union[str,List[str]]]=None)
transformers.trainer.Trainer.create_optimizer(self)
transformers.trainer.Trainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.trainer.Trainer.create_scheduler(self,num_training_steps:int)
transformers.trainer.Trainer.evaluate(self,eval_dataset:Optional[Dataset]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval')->Dict[str, float]
transformers.trainer.Trainer.evaluation_loop(self,dataloader:DataLoader,description:str,prediction_loss_only:Optional[bool]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval')->EvalLoopOutput
transformers.trainer.Trainer.floating_point_ops(self,inputs:Dict[str,Union[torch.Tensor,Any]])
transformers.trainer.Trainer.get_eval_dataloader(self,eval_dataset:Optional[Dataset]=None)->DataLoader
transformers.trainer.Trainer.get_test_dataloader(self,test_dataset:Dataset)->DataLoader
transformers.trainer.Trainer.get_train_dataloader(self)->DataLoader
transformers.trainer.Trainer.hyperparameter_search(self,hp_space:Optional[Callable[['optuna.Trial'],Dict[str,float]]]=None,compute_objective:Optional[Callable[[Dict[str,float]],float]]=None,n_trials:int=20,direction:str='minimize',backend:Optional[Union['str',HPSearchBackend]]=None,hp_name:Optional[Callable[['optuna.Trial'],str]]=None,**kwargs)->BestRun
transformers.trainer.Trainer.init_git_repo(self)
transformers.trainer.Trainer.is_local_process_zero(self)->bool
transformers.trainer.Trainer.is_world_process_zero(self)->bool
transformers.trainer.Trainer.log(self,logs:Dict[str,float])->None
transformers.trainer.Trainer.num_examples(self,dataloader:DataLoader)->int
transformers.trainer.Trainer.pop_callback(self,callback)
transformers.trainer.Trainer.predict(self,test_dataset:Dataset,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='test')->PredictionOutput
transformers.trainer.Trainer.prediction_loop(self,dataloader:DataLoader,description:str,prediction_loss_only:Optional[bool]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval')->PredictionOutput
transformers.trainer.Trainer.prediction_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]],prediction_loss_only:bool,ignore_keys:Optional[List[str]]=None)->Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
transformers.trainer.Trainer.push_to_hub(self,commit_message:Optional[str]='addmodel',**kwargs)->str
transformers.trainer.Trainer.remove_callback(self,callback)
transformers.trainer.Trainer.save_model(self,output_dir:Optional[str]=None)
transformers.trainer.Trainer.store_flos(self)
transformers.trainer.Trainer.train(self,resume_from_checkpoint:Optional[Union[str,bool]]=None,trial:Union['optuna.Trial',Dict[str,Any]]=None,**kwargs)
transformers.trainer.Trainer.training_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]])->torch.Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_tf_pytorch_utils.py----------------------------------------
A:transformers.modeling_tf_pytorch_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_pytorch_utils.tf_name->tf_name.replace(start_prefix_to_remove, '', 1).replace(start_prefix_to_remove, '', 1)
A:transformers.modeling_tf_pytorch_utils.transpose->bool(tf_name[-1] in ['kernel', 'pointwise_kernel', 'depthwise_kernel'] or 'emb_projs' in tf_name or 'out_projs' in tf_name)
A:transformers.modeling_tf_pytorch_utils.tf_name[-1]->tf_name[-1].replace('_kernel', '.weight').replace('_kernel', '.weight')
A:transformers.modeling_tf_pytorch_utils.pt_path->os.path.abspath(pytorch_checkpoint_path)
A:transformers.modeling_tf_pytorch_utils.pt_state_dict->pt_model.state_dict()
A:transformers.modeling_tf_pytorch_utils.new_key->key.replace('beta', 'bias')
A:transformers.modeling_tf_pytorch_utils.pt_state_dict[new_key]->pt_model.state_dict().pop(old_key)
A:transformers.modeling_tf_pytorch_utils.all_pytorch_weights->set(list(pt_state_dict.keys()))
A:transformers.modeling_tf_pytorch_utils.(name, transpose)->convert_tf_weight_name_to_pt_weight_name(sw_name, start_prefix_to_remove=start_prefix_to_remove)
A:transformers.modeling_tf_pytorch_utils.array->numpy.reshape(array, pt_weight.shape)
A:transformers.modeling_tf_pytorch_utils.unexpected_keys->list(all_pytorch_weights)
A:transformers.modeling_tf_pytorch_utils.tf_model_class->getattr(transformers, tf_model_class_name)
A:transformers.modeling_tf_pytorch_utils.tf_model->tf_model_class(pt_model.config)
A:transformers.modeling_tf_pytorch_utils.current_pt_params_dict->dict(pt_model.named_parameters())
A:transformers.modeling_tf_pytorch_utils.(pt_name, transpose)->convert_tf_weight_name_to_pt_weight_name(tf_weight.name, start_prefix_to_remove=start_prefix_to_remove)
A:transformers.modeling_tf_pytorch_utils.all_tf_weights->set(list(tf_weights_map.keys()))
A:transformers.modeling_tf_pytorch_utils.new_pt_params_dict[pt_weight_name]->torch.from_numpy(array)
A:transformers.modeling_tf_pytorch_utils.loaded_pt_weights_data_ptr[pt_weight.data_ptr()]->torch.from_numpy(array)
A:transformers.modeling_tf_pytorch_utils.(missing_keys, unexpected_keys)->pt_model.load_state_dict(new_pt_params_dict, strict=False)
transformers.convert_tf_weight_name_to_pt_weight_name(tf_name,start_prefix_to_remove='')
transformers.load_pytorch_checkpoint_in_tf2_model(tf_model,pytorch_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.load_pytorch_model_in_tf2_model(tf_model,pt_model,tf_inputs=None,allow_missing_keys=False)
transformers.load_pytorch_weights_in_tf2_model(tf_model,pt_state_dict,tf_inputs=None,allow_missing_keys=False)
transformers.load_tf2_checkpoint_in_pytorch_model(pt_model,tf_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.load_tf2_model_in_pytorch_model(pt_model,tf_model,allow_missing_keys=False)
transformers.load_tf2_weights_in_pytorch_model(pt_model,tf_weights,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.convert_tf_weight_name_to_pt_weight_name(tf_name,start_prefix_to_remove='')
transformers.modeling_tf_pytorch_utils.load_pytorch_checkpoint_in_tf2_model(tf_model,pytorch_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_pytorch_model_in_tf2_model(tf_model,pt_model,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_pytorch_weights_in_tf2_model(tf_model,pt_state_dict,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_checkpoint_in_pytorch_model(pt_model,tf_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_model_in_pytorch_model(pt_model,tf_model,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_weights_in_pytorch_model(pt_model,tf_weights,allow_missing_keys=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/hf_argparser.py----------------------------------------
A:transformers.hf_argparser.DataClass->NewType('DataClass', Any)
A:transformers.hf_argparser.DataClassType->NewType('DataClassType', Any)
A:transformers.hf_argparser.parser->self.add_argument_group(dtype._argument_group_name)
A:transformers.hf_argparser.kwargs->field.metadata.copy()
A:transformers.hf_argparser.typestring->str(field.type)
A:transformers.hf_argparser.kwargs['type']->type(kwargs['choices'][0])
A:transformers.hf_argparser.kwargs['default']->field.default_factory()
A:transformers.hf_argparser.args_file->Path(sys.argv[0]).with_suffix('.args')
A:transformers.hf_argparser.fargs->Path(sys.argv[0]).with_suffix('.args').read_text().split()
A:transformers.hf_argparser.(namespace, remaining_args)->self.parse_known_args(args=args)
A:transformers.hf_argparser.obj->dtype(**inputs)
A:transformers.hf_argparser.data->json.loads(Path(json_file).read_text())
transformers.HfArgumentParser(self,dataclass_types:Union[DataClassType,Iterable[DataClassType]],**kwargs)
transformers.HfArgumentParser._add_dataclass_arguments(self,dtype:DataClassType)
transformers.HfArgumentParser.parse_args_into_dataclasses(self,args=None,return_remaining_strings=False,look_for_args_file=True,args_filename=None)->Tuple[DataClass, ...]
transformers.HfArgumentParser.parse_dict(self,args:dict)->Tuple[DataClass, ...]
transformers.HfArgumentParser.parse_json_file(self,json_file:str)->Tuple[DataClass, ...]
transformers.hf_argparser.HfArgumentParser(self,dataclass_types:Union[DataClassType,Iterable[DataClassType]],**kwargs)
transformers.hf_argparser.HfArgumentParser.__init__(self,dataclass_types:Union[DataClassType,Iterable[DataClassType]],**kwargs)
transformers.hf_argparser.HfArgumentParser._add_dataclass_arguments(self,dtype:DataClassType)
transformers.hf_argparser.HfArgumentParser.parse_args_into_dataclasses(self,args=None,return_remaining_strings=False,look_for_args_file=True,args_filename=None)->Tuple[DataClass, ...]
transformers.hf_argparser.HfArgumentParser.parse_dict(self,args:dict)->Tuple[DataClass, ...]
transformers.hf_argparser.HfArgumentParser.parse_json_file(self,json_file:str)->Tuple[DataClass, ...]
transformers.hf_argparser.string_to_bool(v)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/trainer_utils.py----------------------------------------
A:transformers.trainer_utils._re_checkpoint->re.compile('^' + PREFIX_CHECKPOINT_DIR + '\\-(\\d+)$')
A:transformers.trainer_utils.content->os.listdir(folder)
A:transformers.trainer_utils.metrics->copy.deepcopy(metrics)
A:transformers.trainer_utils.loss->copy.deepcopy(metrics).pop('eval_loss', None)
A:transformers.trainer_utils._->copy.deepcopy(metrics).pop(sm, None)
A:transformers.trainer_utils.result[f'{split}_samples_per_second']->round(samples_per_second, 3)
A:transformers.trainer_utils.result[f'{split}_steps_per_second']->round(steps_per_second, 3)
A:transformers.trainer_utils.self.process->psutil.Process()
A:transformers.trainer_utils.self.cpu_mem_used_peak->max(self.cpu_mem_used(), self.cpu_mem_used_peak)
A:transformers.trainer_utils.stage->self.derive_stage()
A:transformers.trainer_utils.self.gpu_mem_used_at_start->self.torch.cuda.memory_allocated()
A:transformers.trainer_utils.self.cpu_mem_used_at_start->self.cpu_mem_used()
A:transformers.trainer_utils.peak_monitor_thread->threading.Thread(target=self.peak_monitor_func)
A:transformers.trainer_utils.self.gpu_mem_used_now->self.torch.cuda.memory_allocated()
A:transformers.trainer_utils.self.gpu_mem_used_peak->self.torch.cuda.max_memory_allocated()
A:transformers.trainer_utils.self.gpu[self.cur_stage]->dict(alloc=self.gpu_mem_used_now - self.gpu_mem_used_at_start, peaked=max(0, self.gpu_mem_used_peak - self.gpu_mem_used_now))
A:transformers.trainer_utils.self.cpu_mem_used_now->self.cpu_mem_used()
A:transformers.trainer_utils.self.cpu[self.cur_stage]->dict(alloc=self.cpu_mem_used_now - self.cpu_mem_used_at_start, peaked=max(0, self.cpu_mem_used_peak - self.cpu_mem_used_now))
transformers.EvalPrediction(NamedTuple)
transformers.IntervalStrategy(ExplicitEnum)
transformers.SchedulerType(ExplicitEnum)
transformers.set_seed(seed:int)
transformers.trainer_utils.BestRun(NamedTuple)
transformers.trainer_utils.EvalLoopOutput(NamedTuple)
transformers.trainer_utils.EvalPrediction(NamedTuple)
transformers.trainer_utils.EvaluationStrategy(ExplicitEnum)
transformers.trainer_utils.HPSearchBackend(ExplicitEnum)
transformers.trainer_utils.IntervalStrategy(ExplicitEnum)
transformers.trainer_utils.PredictionOutput(NamedTuple)
transformers.trainer_utils.SchedulerType(ExplicitEnum)
transformers.trainer_utils.ShardedDDPOption(ExplicitEnum)
transformers.trainer_utils.TrainOutput(NamedTuple)
transformers.trainer_utils.TrainerMemoryTracker(self,skip_memory_metrics=False)
transformers.trainer_utils.TrainerMemoryTracker.__init__(self,skip_memory_metrics=False)
transformers.trainer_utils.TrainerMemoryTracker.cpu_mem_used(self)
transformers.trainer_utils.TrainerMemoryTracker.derive_stage(self)
transformers.trainer_utils.TrainerMemoryTracker.peak_monitor_func(self)
transformers.trainer_utils.TrainerMemoryTracker.start(self)
transformers.trainer_utils.TrainerMemoryTracker.stop(self,stage)
transformers.trainer_utils.TrainerMemoryTracker.stop_and_update_metrics(self,metrics=None)
transformers.trainer_utils.TrainerMemoryTracker.update_metrics(self,stage,metrics)
transformers.trainer_utils.default_compute_objective(metrics:Dict[str,float])->float
transformers.trainer_utils.default_hp_space_optuna(trial)->Dict[str, float]
transformers.trainer_utils.default_hp_space_ray(trial)->Dict[str, float]
transformers.trainer_utils.denumpify_detensorize(metrics)
transformers.trainer_utils.get_last_checkpoint(folder)
transformers.trainer_utils.is_main_process(local_rank)
transformers.trainer_utils.set_seed(seed:int)
transformers.trainer_utils.speed_metrics(split,start_time,num_samples=None,num_steps=None)
transformers.trainer_utils.total_processes_number(local_rank)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_flax_outputs.py----------------------------------------
transformers.modeling_flax_outputs.FlaxBaseModelOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast(ModelOutput)
transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions(ModelOutput)
transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling(ModelOutput)
transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions(ModelOutput)
transformers.modeling_flax_outputs.FlaxMaskedLMOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput(ModelOutput)
transformers.modeling_flax_outputs.FlaxTokenClassifierOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/testing_utils.py----------------------------------------
A:transformers.testing_utils._value->int(value)
A:transformers.testing_utils._run_slow_tests->parse_flag_from_env('RUN_SLOW', default=False)
A:transformers.testing_utils._run_pt_tf_cross_tests->parse_flag_from_env('RUN_PT_TF_CROSS_TESTS', default=False)
A:transformers.testing_utils._run_pt_flax_cross_tests->parse_flag_from_env('RUN_PT_FLAX_CROSS_TESTS', default=False)
A:transformers.testing_utils._run_custom_tokenizers->parse_flag_from_env('RUN_CUSTOM_TOKENIZERS', default=False)
A:transformers.testing_utils._run_staging->parse_flag_from_env('HUGGINGFACE_CO_STAGING', default=False)
A:transformers.testing_utils._run_pipeline_tests->parse_flag_from_env('RUN_PIPELINE_TESTS', default=False)
A:transformers.testing_utils._run_git_lfs_tests->parse_flag_from_env('RUN_GIT_LFS_TESTS', default=False)
A:transformers.testing_utils._tf_gpu_memory_limit->parse_int_from_env('TF_GPU_MEMORY_LIMIT', default=None)
A:transformers.testing_utils.test_case->unittest.skip('test requires JAX & Flax')(test_case)
A:transformers.testing_utils.tests_dir->os.path.abspath(os.path.dirname(caller__file__))
A:transformers.testing_utils.out_pr->apply_print_resets(out).lower()
A:transformers.testing_utils.match_str->apply_print_resets(out).lower().find(what.lower())
A:transformers.testing_utils.self.out_buf->StringIO()
A:transformers.testing_utils.self.err_buf->StringIO()
A:transformers.testing_utils.self.out->self.io.getvalue()
A:transformers.testing_utils.self.err->self.err_buf.getvalue()
A:transformers.testing_utils.self.io->StringIO()
A:transformers.testing_utils.self.sh->logging.StreamHandler(self.io)
A:transformers.testing_utils.orig_level->transformers.logging.get_verbosity()
A:transformers.testing_utils.path->Path(tmp_dir).resolve()
A:transformers.testing_utils.self._test_file_path->inspect.getfile(self.__class__)
A:transformers.testing_utils.env->os.environ.copy()
A:transformers.testing_utils.env['PYTHONPATH']->':'.join(paths)
A:transformers.testing_utils.tmp_dir->tempfile.mkdtemp()
A:transformers.testing_utils.remove_after->frozenset((k for k in update if k not in env))
A:transformers.testing_utils.orig_writer->config.get_terminal_writer()
A:transformers.testing_utils.reports->tr.getreports('failed')
A:transformers.testing_utils.msg->tr._getfailureheadline(rep)
A:transformers.testing_utils.longrepr->re.sub('.*_ _ _ (_ ){10,}_ _ ', '', rep.longreprtext, 0, re.M | re.S)
A:transformers.testing_utils.tr._tw->create_terminal_writer(config, f)
A:transformers.testing_utils.line->line.decode('utf-8').rstrip().decode('utf-8').rstrip()
A:transformers.testing_utils.loop->asyncio.get_event_loop()
A:transformers.testing_utils.result->asyncio.get_event_loop().run_until_complete(_stream_subprocess(cmd, env=env, stdin=stdin, timeout=timeout, quiet=quiet, echo=echo))
A:transformers.testing_utils.cmd_str->' '.join(cmd)
A:transformers.testing_utils.stderr->'\n'.join(result.stderr)
A:transformers.testing_utils.worker->re.sub('^gw', '', worker, 0, re.M)
A:transformers.testing_utils.uniq_delta->pytest_xdist_worker_id()
transformers.testing_utils.CaptureLogger(self,logger)
transformers.testing_utils.CaptureLogger.__enter__(self)
transformers.testing_utils.CaptureLogger.__exit__(self,*exc)
transformers.testing_utils.CaptureLogger.__init__(self,logger)
transformers.testing_utils.CaptureLogger.__repr__(self)
transformers.testing_utils.CaptureStd(self,out=True,err=True)
transformers.testing_utils.CaptureStd.__enter__(self)
transformers.testing_utils.CaptureStd.__exit__(self,*exc)
transformers.testing_utils.CaptureStd.__init__(self,out=True,err=True)
transformers.testing_utils.CaptureStd.__repr__(self)
transformers.testing_utils.CaptureStderr(self)
transformers.testing_utils.CaptureStderr.__init__(self)
transformers.testing_utils.CaptureStdout(self)
transformers.testing_utils.CaptureStdout.__init__(self)
transformers.testing_utils.ExtendSysPath(path:Union[str,os.PathLike])->Iterator[None]
transformers.testing_utils.LoggingLevel(level)
transformers.testing_utils.TestCasePlus(unittest.TestCase)
transformers.testing_utils.TestCasePlus.examples_dir(self)
transformers.testing_utils.TestCasePlus.examples_dir_str(self)
transformers.testing_utils.TestCasePlus.get_auto_remove_tmp_dir(self,tmp_dir=None,before=None,after=None)
transformers.testing_utils.TestCasePlus.get_env(self)
transformers.testing_utils.TestCasePlus.repo_root_dir(self)
transformers.testing_utils.TestCasePlus.repo_root_dir_str(self)
transformers.testing_utils.TestCasePlus.setUp(self)
transformers.testing_utils.TestCasePlus.src_dir(self)
transformers.testing_utils.TestCasePlus.src_dir_str(self)
transformers.testing_utils.TestCasePlus.tearDown(self)
transformers.testing_utils.TestCasePlus.test_file_dir(self)
transformers.testing_utils.TestCasePlus.test_file_dir_str(self)
transformers.testing_utils.TestCasePlus.test_file_path(self)
transformers.testing_utils.TestCasePlus.test_file_path_str(self)
transformers.testing_utils.TestCasePlus.tests_dir(self)
transformers.testing_utils.TestCasePlus.tests_dir_str(self)
transformers.testing_utils._RunOutput(self,returncode,stdout,stderr)
transformers.testing_utils._RunOutput.__init__(self,returncode,stdout,stderr)
transformers.testing_utils.apply_print_resets(buf)
transformers.testing_utils.assert_screenout(out,what)
transformers.testing_utils.custom_tokenizers(test_case)
transformers.testing_utils.execute_subprocess_async(cmd,env=None,stdin=None,timeout=180,quiet=False,echo=True)->_RunOutput
transformers.testing_utils.get_gpu_count()
transformers.testing_utils.get_tests_dir(append_path=None)
transformers.testing_utils.get_torch_dist_unique_port()
transformers.testing_utils.is_pipeline_test(test_case)
transformers.testing_utils.is_pt_flax_cross_test(test_case)
transformers.testing_utils.is_pt_tf_cross_test(test_case)
transformers.testing_utils.is_staging_test(test_case)
transformers.testing_utils.mockenv(**kwargs)
transformers.testing_utils.mockenv_context(*remove,**update)
transformers.testing_utils.nested_simplify(obj,decimals=3)
transformers.testing_utils.parse_flag_from_env(key,default=False)
transformers.testing_utils.parse_int_from_env(key,default=None)
transformers.testing_utils.pytest_addoption_shared(parser)
transformers.testing_utils.pytest_terminal_summary_main(tr,id)
transformers.testing_utils.pytest_xdist_worker_id()
transformers.testing_utils.require_datasets(test_case)
transformers.testing_utils.require_deepspeed(test_case)
transformers.testing_utils.require_faiss(test_case)
transformers.testing_utils.require_flax(test_case)
transformers.testing_utils.require_git_lfs(test_case)
transformers.testing_utils.require_onnx(test_case)
transformers.testing_utils.require_optuna(test_case)
transformers.testing_utils.require_pandas(test_case)
transformers.testing_utils.require_ray(test_case)
transformers.testing_utils.require_scatter(test_case)
transformers.testing_utils.require_sentencepiece(test_case)
transformers.testing_utils.require_soundfile(test_case)
transformers.testing_utils.require_tf(test_case)
transformers.testing_utils.require_timm(test_case)
transformers.testing_utils.require_tokenizers(test_case)
transformers.testing_utils.require_torch(test_case)
transformers.testing_utils.require_torch_gpu(test_case)
transformers.testing_utils.require_torch_multi_gpu(test_case)
transformers.testing_utils.require_torch_non_multi_gpu(test_case)
transformers.testing_utils.require_torch_scatter(test_case)
transformers.testing_utils.require_torch_tpu(test_case)
transformers.testing_utils.require_torch_up_to_2_gpus(test_case)
transformers.testing_utils.require_torchaudio(test_case)
transformers.testing_utils.require_vision(test_case)
transformers.testing_utils.slow(test_case)
transformers.testing_utils.tooslow(test_case)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/convert_tf_hub_seq_to_seq_bert_to_pytorch.py----------------------------------------
A:transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.bert_config->BertConfig.from_pretrained('bert-large-cased', vocab_size=vocab_size, max_position_embeddings=512, is_decoder=True, add_cross_attention=True)
A:transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.bert_config_dict->BertConfig.from_pretrained('bert-large-cased', vocab_size=vocab_size, max_position_embeddings=512, is_decoder=True, add_cross_attention=True).to_dict()
A:transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.config->BertGenerationConfig(**bert_config_dict)
A:transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.model->BertGenerationDecoder(config)
A:transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_tf_hub_seq_to_seq_bert_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_hub_path,pytorch_dump_path,is_encoder_named_decoder,vocab_size,is_encoder)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/training_args_seq2seq.py----------------------------------------
A:transformers.training_args_seq2seq.logger->logging.getLogger(__name__)
transformers.Seq2SeqTrainingArguments(TrainingArguments)
transformers.training_args_seq2seq.Seq2SeqTrainingArguments(TrainingArguments)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/configuration_utils.py----------------------------------------
A:transformers.configuration_utils.logger->utils.logging.get_logger(__name__)
A:transformers.configuration_utils.self.return_dict->kwargs.pop('return_dict', True)
A:transformers.configuration_utils.self.output_hidden_states->kwargs.pop('output_hidden_states', False)
A:transformers.configuration_utils.self.output_attentions->kwargs.pop('output_attentions', False)
A:transformers.configuration_utils.self.torchscript->kwargs.pop('torchscript', False)
A:transformers.configuration_utils.self.use_bfloat16->kwargs.pop('use_bfloat16', False)
A:transformers.configuration_utils.self.pruned_heads->kwargs.pop('pruned_heads', {})
A:transformers.configuration_utils.self.tie_word_embeddings->kwargs.pop('tie_word_embeddings', True)
A:transformers.configuration_utils.self.is_encoder_decoder->kwargs.pop('is_encoder_decoder', False)
A:transformers.configuration_utils.self.is_decoder->kwargs.pop('is_decoder', False)
A:transformers.configuration_utils.self.add_cross_attention->kwargs.pop('add_cross_attention', False)
A:transformers.configuration_utils.self.tie_encoder_decoder->kwargs.pop('tie_encoder_decoder', False)
A:transformers.configuration_utils.self.max_length->kwargs.pop('max_length', 20)
A:transformers.configuration_utils.self.min_length->kwargs.pop('min_length', 0)
A:transformers.configuration_utils.self.do_sample->kwargs.pop('do_sample', False)
A:transformers.configuration_utils.self.early_stopping->kwargs.pop('early_stopping', False)
A:transformers.configuration_utils.self.num_beams->kwargs.pop('num_beams', 1)
A:transformers.configuration_utils.self.num_beam_groups->kwargs.pop('num_beam_groups', 1)
A:transformers.configuration_utils.self.diversity_penalty->kwargs.pop('diversity_penalty', 0.0)
A:transformers.configuration_utils.self.temperature->kwargs.pop('temperature', 1.0)
A:transformers.configuration_utils.self.top_k->kwargs.pop('top_k', 50)
A:transformers.configuration_utils.self.top_p->kwargs.pop('top_p', 1.0)
A:transformers.configuration_utils.self.repetition_penalty->kwargs.pop('repetition_penalty', 1.0)
A:transformers.configuration_utils.self.length_penalty->kwargs.pop('length_penalty', 1.0)
A:transformers.configuration_utils.self.no_repeat_ngram_size->kwargs.pop('no_repeat_ngram_size', 0)
A:transformers.configuration_utils.self.encoder_no_repeat_ngram_size->kwargs.pop('encoder_no_repeat_ngram_size', 0)
A:transformers.configuration_utils.self.bad_words_ids->kwargs.pop('bad_words_ids', None)
A:transformers.configuration_utils.self.num_return_sequences->kwargs.pop('num_return_sequences', 1)
A:transformers.configuration_utils.self.chunk_size_feed_forward->kwargs.pop('chunk_size_feed_forward', 0)
A:transformers.configuration_utils.self.output_scores->kwargs.pop('output_scores', False)
A:transformers.configuration_utils.self.return_dict_in_generate->kwargs.pop('return_dict_in_generate', False)
A:transformers.configuration_utils.self.forced_bos_token_id->kwargs.pop('forced_bos_token_id', None)
A:transformers.configuration_utils.self.forced_eos_token_id->kwargs.pop('forced_eos_token_id', None)
A:transformers.configuration_utils.self.remove_invalid_values->kwargs.pop('remove_invalid_values', False)
A:transformers.configuration_utils.self.architectures->kwargs.pop('architectures', None)
A:transformers.configuration_utils.self.finetuning_task->kwargs.pop('finetuning_task', None)
A:transformers.configuration_utils.self.id2label->dict(((int(key), value) for (key, value) in self.id2label.items()))
A:transformers.configuration_utils.self.label2id->dict(zip(self.id2label.values(), self.id2label.keys()))
A:transformers.configuration_utils.self.num_labels->kwargs.pop('num_labels', 2)
A:transformers.configuration_utils.self.tokenizer_class->kwargs.pop('tokenizer_class', None)
A:transformers.configuration_utils.self.prefix->kwargs.pop('prefix', None)
A:transformers.configuration_utils.self.bos_token_id->kwargs.pop('bos_token_id', None)
A:transformers.configuration_utils.self.pad_token_id->kwargs.pop('pad_token_id', None)
A:transformers.configuration_utils.self.eos_token_id->kwargs.pop('eos_token_id', None)
A:transformers.configuration_utils.self.sep_token_id->kwargs.pop('sep_token_id', None)
A:transformers.configuration_utils.self.decoder_start_token_id->kwargs.pop('decoder_start_token_id', None)
A:transformers.configuration_utils.self.task_specific_params->kwargs.pop('task_specific_params', None)
A:transformers.configuration_utils.self.problem_type->kwargs.pop('problem_type', None)
A:transformers.configuration_utils.self._name_or_path->str(value)
A:transformers.configuration_utils.self.transformers_version->kwargs.pop('transformers_version', None)
A:transformers.configuration_utils.commit_message->kwargs.pop('commit_message', None)
A:transformers.configuration_utils.repo->self._create_or_get_repo(save_directory, **kwargs)
A:transformers.configuration_utils.output_config_file->os.path.join(save_directory, CONFIG_NAME)
A:transformers.configuration_utils.url->self._push_to_hub(repo, commit_message=commit_message)
A:transformers.configuration_utils.(config_dict, kwargs)->cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
A:transformers.configuration_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.configuration_utils.force_download->kwargs.pop('force_download', False)
A:transformers.configuration_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.configuration_utils.proxies->kwargs.pop('proxies', None)
A:transformers.configuration_utils.use_auth_token->kwargs.pop('use_auth_token', None)
A:transformers.configuration_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.configuration_utils.revision->kwargs.pop('revision', None)
A:transformers.configuration_utils.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.configuration_utils.from_auto_class->kwargs.pop('_from_auto', False)
A:transformers.configuration_utils.pretrained_model_name_or_path->str(pretrained_model_name_or_path)
A:transformers.configuration_utils.config_file->hf_bucket_url(pretrained_model_name_or_path, filename=CONFIG_NAME, revision=revision, mirror=None)
A:transformers.configuration_utils.resolved_config_file->cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent)
A:transformers.configuration_utils.config_dict->self.to_dict()
A:transformers.configuration_utils.return_unused_kwargs->kwargs.pop('return_unused_kwargs', False)
A:transformers.configuration_utils.config->cls(**config_dict)
A:transformers.configuration_utils.config.pruned_heads->dict(((int(key), value) for (key, value) in config.pruned_heads.items()))
A:transformers.configuration_utils.text->reader.read()
A:transformers.configuration_utils.default_config_dict->PretrainedConfig().to_dict()
A:transformers.configuration_utils.output->copy.deepcopy(self.__dict__)
A:transformers.configuration_utils.d->dict((x.split('=') for x in update_str.split(',')))
A:transformers.configuration_utils.old_v->getattr(self, k)
A:transformers.configuration_utils.v->float(v)
transformers.PretrainedConfig(self,**kwargs)
transformers.PretrainedConfig.__eq__(self,other)
transformers.PretrainedConfig.__repr__(self)
transformers.PretrainedConfig._dict_from_json_file(cls,json_file:Union[str,os.PathLike])
transformers.PretrainedConfig.from_dict(cls,config_dict:Dict[str,Any],**kwargs)->'PretrainedConfig'
transformers.PretrainedConfig.from_json_file(cls,json_file:Union[str,os.PathLike])->'PretrainedConfig'
transformers.PretrainedConfig.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],**kwargs)->'PretrainedConfig'
transformers.PretrainedConfig.get_config_dict(cls,pretrained_model_name_or_path:Union[str,os.PathLike],**kwargs)->Tuple[Dict[str, Any], Dict[str, Any]]
transformers.PretrainedConfig.name_or_path(self)->str
transformers.PretrainedConfig.name_or_path(self,value)
transformers.PretrainedConfig.num_labels(self)->int
transformers.PretrainedConfig.num_labels(self,num_labels:int)
transformers.PretrainedConfig.save_pretrained(self,save_directory:Union[str,os.PathLike],push_to_hub:bool=False,**kwargs)
transformers.PretrainedConfig.to_dict(self)->Dict[str, Any]
transformers.PretrainedConfig.to_diff_dict(self)->Dict[str, Any]
transformers.PretrainedConfig.to_json_file(self,json_file_path:Union[str,os.PathLike],use_diff:bool=True)
transformers.PretrainedConfig.to_json_string(self,use_diff:bool=True)->str
transformers.PretrainedConfig.update(self,config_dict:Dict[str,Any])
transformers.PretrainedConfig.update_from_string(self,update_str:str)
transformers.PretrainedConfig.use_return_dict(self)->bool
transformers.configuration_utils.PretrainedConfig(self,**kwargs)
transformers.configuration_utils.PretrainedConfig.__eq__(self,other)
transformers.configuration_utils.PretrainedConfig.__init__(self,**kwargs)
transformers.configuration_utils.PretrainedConfig.__repr__(self)
transformers.configuration_utils.PretrainedConfig._dict_from_json_file(cls,json_file:Union[str,os.PathLike])
transformers.configuration_utils.PretrainedConfig.from_dict(cls,config_dict:Dict[str,Any],**kwargs)->'PretrainedConfig'
transformers.configuration_utils.PretrainedConfig.from_json_file(cls,json_file:Union[str,os.PathLike])->'PretrainedConfig'
transformers.configuration_utils.PretrainedConfig.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],**kwargs)->'PretrainedConfig'
transformers.configuration_utils.PretrainedConfig.get_config_dict(cls,pretrained_model_name_or_path:Union[str,os.PathLike],**kwargs)->Tuple[Dict[str, Any], Dict[str, Any]]
transformers.configuration_utils.PretrainedConfig.name_or_path(self)->str
transformers.configuration_utils.PretrainedConfig.name_or_path(self,value)
transformers.configuration_utils.PretrainedConfig.num_labels(self)->int
transformers.configuration_utils.PretrainedConfig.num_labels(self,num_labels:int)
transformers.configuration_utils.PretrainedConfig.save_pretrained(self,save_directory:Union[str,os.PathLike],push_to_hub:bool=False,**kwargs)
transformers.configuration_utils.PretrainedConfig.to_dict(self)->Dict[str, Any]
transformers.configuration_utils.PretrainedConfig.to_diff_dict(self)->Dict[str, Any]
transformers.configuration_utils.PretrainedConfig.to_json_file(self,json_file_path:Union[str,os.PathLike],use_diff:bool=True)
transformers.configuration_utils.PretrainedConfig.to_json_string(self,use_diff:bool=True)->str
transformers.configuration_utils.PretrainedConfig.update(self,config_dict:Dict[str,Any])
transformers.configuration_utils.PretrainedConfig.update_from_string(self,update_str:str)
transformers.configuration_utils.PretrainedConfig.use_return_dict(self)->bool


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/hf_api.py----------------------------------------
A:transformers.hf_api.r->requests.delete(path, headers={'authorization': f'Bearer {token}'}, json={'name': name, 'organization': organization})
A:transformers.hf_api.d->requests.delete(path, headers={'authorization': f'Bearer {token}'}, json={'name': name, 'organization': organization}).json()
A:transformers.hf_api.self.pbar->tqdm(total=self.total_size, leave=False)
A:transformers.hf_api.path_token->expanduser('~/.huggingface/token')
transformers.hf_api.HfApi(self,endpoint=None)
transformers.hf_api.HfApi.__init__(self,endpoint=None)
transformers.hf_api.HfApi.create_repo(self,token:str,name:str,organization:Optional[str]=None,private:Optional[bool]=None,exist_ok=False,lfsmultipartthresh:Optional[int]=None)->str
transformers.hf_api.HfApi.delete_repo(self,token:str,name:str,organization:Optional[str]=None)
transformers.hf_api.HfApi.list_repos_objs(self,token:str,organization:Optional[str]=None)->List[RepoObj]
transformers.hf_api.HfApi.login(self,username:str,password:str)->str
transformers.hf_api.HfApi.logout(self,token:str)->None
transformers.hf_api.HfApi.model_list(self)->List[ModelInfo]
transformers.hf_api.HfApi.whoami(self,token:str)->Tuple[str, List[str]]
transformers.hf_api.HfFolder
transformers.hf_api.HfFolder.delete_token(cls)
transformers.hf_api.HfFolder.get_token(cls)
transformers.hf_api.HfFolder.save_token(cls,token)
transformers.hf_api.ModelInfo(self,modelId:Optional[str]=None,tags:List[str]=[],pipeline_tag:Optional[str]=None,siblings:Optional[List[Dict]]=None,**kwargs)
transformers.hf_api.ModelInfo.__init__(self,modelId:Optional[str]=None,tags:List[str]=[],pipeline_tag:Optional[str]=None,siblings:Optional[List[Dict]]=None,**kwargs)
transformers.hf_api.ModelSibling(self,rfilename:str,**kwargs)
transformers.hf_api.ModelSibling.__init__(self,rfilename:str,**kwargs)
transformers.hf_api.RepoObj(self,filename:str,lastModified:str,commit:str,size:int,**kwargs)
transformers.hf_api.RepoObj.__init__(self,filename:str,lastModified:str,commit:str,size:int,**kwargs)
transformers.hf_api.TqdmProgressFileReader(self,f:io.BufferedReader)
transformers.hf_api.TqdmProgressFileReader.__init__(self,f:io.BufferedReader)
transformers.hf_api.TqdmProgressFileReader._read(self,n=-1)
transformers.hf_api.TqdmProgressFileReader.close(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_tf_outputs.py----------------------------------------
transformers.modeling_tf_outputs.TFBaseModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFBaseModelOutputWithCrossAttentions(ModelOutput)
transformers.modeling_tf_outputs.TFBaseModelOutputWithPast(ModelOutput)
transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions(ModelOutput)
transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling(ModelOutput)
transformers.modeling_tf_outputs.TFCausalLMOutput(ModelOutput)
transformers.modeling_tf_outputs.TFCausalLMOutputWithPast(ModelOutput)
transformers.modeling_tf_outputs.TFMaskedLMOutput(ModelOutput)
transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFNextSentencePredictorOutput(ModelOutput)
transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqLMOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSequenceClassifierOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSequenceClassifierOutputWithPast(ModelOutput)
transformers.modeling_tf_outputs.TFTokenClassifierOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/feature_extraction_sequence_utils.py----------------------------------------
A:transformers.feature_extraction_sequence_utils.logger->utils.logging.get_logger(__name__)
A:transformers.feature_extraction_sequence_utils.self.padding_side->kwargs.pop('padding_side', 'right')
A:transformers.feature_extraction_sequence_utils.self.return_attention_mask->kwargs.pop('return_attention_mask', True)
A:transformers.feature_extraction_sequence_utils.processed_features[key]->to_py_obj(value)
A:transformers.feature_extraction_sequence_utils.(padding_strategy, max_length, _)->self._get_padding_strategies(padding=padding, max_length=max_length)
A:transformers.feature_extraction_sequence_utils.processed_features->self._pad(processed_features, max_length=max_length, padding_strategy=padding_strategy, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.feature_extraction_sequence_utils.batch_size->len(required_input)
A:transformers.feature_extraction_sequence_utils.max_length->len(required_input)
A:transformers.feature_extraction_sequence_utils.inputs->dict(((k, v[i]) for (k, v) in processed_features.items()))
A:transformers.feature_extraction_sequence_utils.outputs->self._pad(inputs, max_length=max_length, padding_strategy=padding_strategy, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.feature_extraction_sequence_utils.padding_strategy->PaddingStrategy(padding)
transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor(self,feature_size:int,sampling_rate:int,padding_value:float,**kwargs)
transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor.__init__(self,feature_size:int,sampling_rate:int,padding_value:float,**kwargs)
transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor._get_padding_strategies(self,padding=False,max_length=None,pad_to_multiple_of=None,**kwargs)
transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor._pad(self,processed_features:Union[Dict[str,List[float]],BatchFeature],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.feature_extraction_sequence_utils.SequenceFeatureExtractor.pad(self,processed_features:Union[BatchFeature,List[BatchFeature],Dict[str,BatchFeature],Dict[str,List[BatchFeature]],List[Dict[str,BatchFeature]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None)->BatchFeature


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/optimization.py----------------------------------------
A:transformers.optimization.logger->utils.logging.get_logger(__name__)
A:transformers.optimization.name->SchedulerType(name)
A:transformers.optimization.defaults->dict(lr=lr, eps=eps, clip_threshold=clip_threshold, decay_rate=decay_rate, beta1=beta1, weight_decay=weight_decay, scale_parameter=scale_parameter, relative_step=relative_step, warmup_init=warmup_init)
A:transformers.optimization.loss->closure()
A:transformers.optimization.state['exp_avg']->state['exp_avg'].to(grad).to(grad)
A:transformers.optimization.state['exp_avg_sq']->state['exp_avg_sq'].to(grad).to(grad)
A:transformers.optimization.denom->exp_avg_sq.sqrt().add_(group['eps'])
A:transformers.optimization.rel_step_sz->min(min_step, 1.0 / math.sqrt(param_state['step']))
A:transformers.optimization.param_scale->max(param_group['eps'][1], param_state['RMS'])
A:transformers.optimization.r_factor->(exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True)).rsqrt_()
A:transformers.optimization.c_factor->exp_avg_sq_col.rsqrt()
A:transformers.optimization.grad->grad.float().float()
A:transformers.optimization.(factored, use_first_moment)->self._get_options(group, grad_shape)
A:transformers.optimization.state['exp_avg_sq_row']->state['exp_avg_sq_row'].to(grad).to(grad)
A:transformers.optimization.state['exp_avg_sq_col']->state['exp_avg_sq_col'].to(grad).to(grad)
A:transformers.optimization.p_data_fp32->p_data_fp32.float().float()
A:transformers.optimization.state['RMS']->self._rms(p_data_fp32)
A:transformers.optimization.lr->self._get_lr(group, state)
A:transformers.optimization.update->exp_avg_sq.rsqrt().mul_(grad)
transformers.Adafactor(self,params,lr=None,eps=(1e-30,0.001),clip_threshold=1.0,decay_rate=-0.8,beta1=None,weight_decay=0.0,scale_parameter=True,relative_step=True,warmup_init=False)
transformers.Adafactor._approx_sq_grad(exp_avg_sq_row,exp_avg_sq_col)
transformers.Adafactor._get_lr(param_group,param_state)
transformers.Adafactor._get_options(param_group,param_shape)
transformers.Adafactor._rms(tensor)
transformers.Adafactor.step(self,closure=None)
transformers.AdafactorSchedule(self,optimizer,initial_lr=0.0)
transformers.AdafactorSchedule.get_lr(self)
transformers.AdamW(self,params:Iterable[nn.parameter.Parameter],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-06,weight_decay:float=0.0,correct_bias:bool=True)
transformers.AdamW.step(self,closure:Callable=None)
transformers.get_constant_schedule(optimizer:Optimizer,last_epoch:int=-1)
transformers.get_constant_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,last_epoch:int=-1)
transformers.get_cosine_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:float=0.5,last_epoch:int=-1)
transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:int=1,last_epoch:int=-1)
transformers.get_linear_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,last_epoch=-1)
transformers.get_polynomial_decay_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,lr_end=1e-07,power=1.0,last_epoch=-1)
transformers.get_scheduler(name:Union[str,SchedulerType],optimizer:Optimizer,num_warmup_steps:Optional[int]=None,num_training_steps:Optional[int]=None)
transformers.optimization.Adafactor(self,params,lr=None,eps=(1e-30,0.001),clip_threshold=1.0,decay_rate=-0.8,beta1=None,weight_decay=0.0,scale_parameter=True,relative_step=True,warmup_init=False)
transformers.optimization.Adafactor.__init__(self,params,lr=None,eps=(1e-30,0.001),clip_threshold=1.0,decay_rate=-0.8,beta1=None,weight_decay=0.0,scale_parameter=True,relative_step=True,warmup_init=False)
transformers.optimization.Adafactor._approx_sq_grad(exp_avg_sq_row,exp_avg_sq_col)
transformers.optimization.Adafactor._get_lr(param_group,param_state)
transformers.optimization.Adafactor._get_options(param_group,param_shape)
transformers.optimization.Adafactor._rms(tensor)
transformers.optimization.Adafactor.step(self,closure=None)
transformers.optimization.AdafactorSchedule(self,optimizer,initial_lr=0.0)
transformers.optimization.AdafactorSchedule.__init__(self,optimizer,initial_lr=0.0)
transformers.optimization.AdafactorSchedule.get_lr(self)
transformers.optimization.AdamW(self,params:Iterable[nn.parameter.Parameter],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-06,weight_decay:float=0.0,correct_bias:bool=True)
transformers.optimization.AdamW.__init__(self,params:Iterable[nn.parameter.Parameter],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-06,weight_decay:float=0.0,correct_bias:bool=True)
transformers.optimization.AdamW.step(self,closure:Callable=None)
transformers.optimization.get_adafactor_schedule(optimizer,initial_lr=0.0)
transformers.optimization.get_constant_schedule(optimizer:Optimizer,last_epoch:int=-1)
transformers.optimization.get_constant_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,last_epoch:int=-1)
transformers.optimization.get_cosine_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:float=0.5,last_epoch:int=-1)
transformers.optimization.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:int=1,last_epoch:int=-1)
transformers.optimization.get_linear_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,last_epoch=-1)
transformers.optimization.get_polynomial_decay_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,lr_end=1e-07,power=1.0,last_epoch=-1)
transformers.optimization.get_scheduler(name:Union[str,SchedulerType],optimizer:Optimizer,num_warmup_steps:Optional[int]=None,num_training_steps:Optional[int]=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_flax_utils.py----------------------------------------
A:transformers.generation_flax_utils.logger->utils.logging.get_logger(__name__)
A:transformers.generation_flax_utils.state->jax.lax.while_loop(beam_search_cond_fn, beam_search_body_fn, state)
A:transformers.generation_flax_utils.model_kwargs['encoder_outputs']->self.encode(input_ids, return_dict=True, **encoder_kwargs)
A:transformers.generation_flax_utils.model_kwargs->self.prepare_inputs_for_generation(flatten_beam_dim(input_ids), max_length, **model_kwargs)
A:transformers.generation_flax_utils.logits_processor->self._get_logits_processor(no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id)
A:transformers.generation_flax_utils.logits_warper->self._get_logits_warper(top_k=top_k, top_p=top_p, temperature=temperature)
A:transformers.generation_flax_utils.input_ids->self._expand_to_num_beams(input_ids, num_beams=num_beams)
A:transformers.generation_flax_utils.model_kwargs['encoder_outputs']['last_hidden_state']->flatten_beam_dim(model_kwargs['encoder_outputs']['last_hidden_state'])
A:transformers.generation_flax_utils.model_kwargs['attention_mask']->flatten_beam_dim(model_kwargs['attention_mask'])
A:transformers.generation_flax_utils.warpers->FlaxLogitsProcessorList()
A:transformers.generation_flax_utils.processors->FlaxLogitsProcessorList()
A:transformers.generation_flax_utils.eos_token_id->jax.numpy.array(eos_token_id)
A:transformers.generation_flax_utils.pad_token_id->jax.numpy.array(pad_token_id)
A:transformers.generation_flax_utils.cur_len->jax.numpy.array(cur_len)
A:transformers.generation_flax_utils.sequences->jax.numpy.where(none_finished[:, None, None], state.sequences, state.running_sequences)
A:transformers.generation_flax_utils.is_sent_finished->jax.numpy.zeros((batch_size, num_beams), dtype=jnp.bool_)
A:transformers.generation_flax_utils.all_sequence_finished->jax.numpy.all(state.is_sent_finished)
A:transformers.generation_flax_utils.finish_generation->jax.numpy.logical_or(has_reached_max_length, all_sequence_finished)
A:transformers.generation_flax_utils.model_outputs->model(input_token, params=params, **state.model_kwargs)
A:transformers.generation_flax_utils.logits->unflatten_beam_dim(model_outputs.logits[:, 0], batch_size, num_beams)
A:transformers.generation_flax_utils.next_token->jax.random.categorical(prng_key, model_outputs.logits[:, -1], axis=-1)
A:transformers.generation_flax_utils.next_sequences->jax.lax.dynamic_update_slice(state.sequences, next_token, (0, state.cur_len))
A:transformers.generation_flax_utils.next_model_kwargs->self.update_inputs_for_generation(model_outputs, state.model_kwargs)
A:transformers.generation_flax_utils.(prng_key, prng_key_next)->jax.random.split(state.prng_key)
A:transformers.generation_flax_utils.batch_indices->jax.numpy.reshape(jnp.arange(batch_size * new_num_beams) // new_num_beams, (batch_size, new_num_beams))
A:transformers.generation_flax_utils.running_sequences->jax.lax.dynamic_update_slice(sequences, input_ids, (0, 0, 0))
A:transformers.generation_flax_utils.running_scores->jax.numpy.tile(jnp.array([0.0] + [np.array(-10000000.0)] * (num_beams - 1)), [batch_size, 1])
A:transformers.generation_flax_utils.worst_finished_score->jax.numpy.where(state.is_sent_finished, jnp.min(state.scores, axis=1, keepdims=True), np.array(-10000000.0))
A:transformers.generation_flax_utils.improvement_still_possible->jax.numpy.all(worst_finished_score < best_running_score)
A:transformers.generation_flax_utils.input_token->flatten_beam_dim(lax.dynamic_slice(state.running_sequences, (0, 0, state.cur_len - 1), (batch_size, num_beams, 1)))
A:transformers.generation_flax_utils.cache->jax.tree_map(lambda tensor: unflatten_beam_dim(tensor, batch_size, num_beams), model_outputs.past_key_values)
A:transformers.generation_flax_utils.log_probs->log_probs.reshape((batch_size, num_beams * vocab_size)).reshape((batch_size, num_beams * vocab_size))
A:transformers.generation_flax_utils.(topk_log_probs, topk_indices)->jax.lax.top_k(log_probs, k=beams_to_keep)
A:transformers.generation_flax_utils.topk_running_sequences->gather_beams(state.running_sequences, topk_beam_indices, batch_size, beams_to_keep)
A:transformers.generation_flax_utils.topk_ids->jax.numpy.expand_dims(topk_indices % vocab_size, axis=2)
A:transformers.generation_flax_utils.topk_sequences->jax.lax.dynamic_update_slice(topk_running_sequences, topk_ids, (0, 0, state.cur_len))
A:transformers.generation_flax_utils.next_topk_indices->jax.numpy.flip(lax.top_k(topk_log_probs, k=num_beams)[1], axis=1)
A:transformers.generation_flax_utils.(next_running_sequences, next_running_scores)->gather_beams([topk_sequences, topk_log_probs], next_topk_indices, batch_size, num_beams)
A:transformers.generation_flax_utils.merged_sequences->jax.numpy.concatenate([state.sequences, topk_sequences], axis=1)
A:transformers.generation_flax_utils.merged_scores->jax.numpy.concatenate([state.scores, topk_log_probs], axis=1)
A:transformers.generation_flax_utils.merged_is_sent_finished->jax.numpy.concatenate([state.is_sent_finished, did_topk_just_finished], axis=1)
A:transformers.generation_flax_utils.topk_merged_indices->jax.numpy.flip(lax.top_k(merged_scores, k=num_beams)[1], axis=1)
A:transformers.generation_flax_utils.(next_sequences, next_scores, next_is_sent_finished)->gather_beams([merged_sequences, merged_scores, merged_is_sent_finished], topk_merged_indices, batch_size, num_beams)
A:transformers.generation_flax_utils.next_running_indices->gather_beams(topk_beam_indices, next_topk_indices, batch_size, num_beams)
A:transformers.generation_flax_utils.next_cache->gather_beams(cache, next_running_indices, batch_size, num_beams)
A:transformers.generation_flax_utils.model_outputs['past_key_values']->jax.tree_map(lambda x: flatten_beam_dim(x), next_cache)
A:transformers.generation_flax_utils.none_finished->jax.numpy.any(state.is_sent_finished, axis=1)
A:transformers.generation_flax_utils.scores->jax.numpy.where(none_finished[:, None], state.scores, state.running_scores)
transformers.generation_flax_utils.BeamSearchState
transformers.generation_flax_utils.FlaxBeamSearchOutput(ModelOutput)
transformers.generation_flax_utils.FlaxGenerationMixin
transformers.generation_flax_utils.FlaxGenerationMixin._beam_search(self,input_ids:None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,length_penalty:Optional[float]=None,early_stopping:Optional[bool]=None,logits_processor:Optional[FlaxLogitsProcessorList]=None,trace:bool=True,params:Optional[Dict[str,jax_xla.DeviceArray]]=None,model_kwargs:Optional[Dict[str,jax_xla.DeviceArray]]=None)
transformers.generation_flax_utils.FlaxGenerationMixin._expand_to_num_beams(tensor,num_beams)
transformers.generation_flax_utils.FlaxGenerationMixin._get_logits_processor(self,no_repeat_ngram_size:int,min_length:int,max_length:int,eos_token_id:int,forced_bos_token_id:int,forced_eos_token_id:int)->FlaxLogitsProcessorList
transformers.generation_flax_utils.FlaxGenerationMixin._get_logits_warper(self,top_k:int=None,top_p:float=None,temperature:float=None)->FlaxLogitsProcessorList
transformers.generation_flax_utils.FlaxGenerationMixin._greedy_search(self,input_ids:None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,logits_processor:Optional[FlaxLogitsProcessorList]=None,trace:bool=True,params:Optional[Dict[str,jax_xla.DeviceArray]]=None,model_kwargs:Optional[Dict[str,jax_xla.DeviceArray]]=None)
transformers.generation_flax_utils.FlaxGenerationMixin._prepare_encoder_decoder_kwargs_for_generation(self,input_ids,model_kwargs)
transformers.generation_flax_utils.FlaxGenerationMixin._run_loop_in_debug(cond_fn,body_fn,init_state)
transformers.generation_flax_utils.FlaxGenerationMixin._sample(self,input_ids:None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,prng_key:Optional[jax_xla.DeviceArray]=None,logits_processor:Optional[FlaxLogitsProcessorList]=None,logits_warper:Optional[FlaxLogitsProcessorList]=None,trace:bool=True,params:Optional[Dict[str,jax_xla.DeviceArray]]=None,model_kwargs:Optional[Dict[str,jax_xla.DeviceArray]]=None)
transformers.generation_flax_utils.FlaxGenerationMixin.generate(self,input_ids:jax_xla.DeviceArray,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,bos_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,decoder_start_token_id:Optional[int]=None,do_sample:Optional[bool]=None,prng_key:Optional[jax_xla.DeviceArray]=None,top_k:Optional[int]=None,top_p:Optional[float]=None,temperature:Optional[float]=None,num_beams:Optional[int]=None,no_repeat_ngram_size:Optional[int]=None,min_length:Optional[int]=None,forced_bos_token_id:Optional[int]=None,forced_eos_token_id:Optional[int]=None,length_penalty:Optional[float]=None,early_stopping:Optional[bool]=None,trace:bool=True,params:Optional[Dict[str,jax_xla.DeviceArray]]=None,**model_kwargs)
transformers.generation_flax_utils.FlaxGreedySearchOutput(ModelOutput)
transformers.generation_flax_utils.FlaxSampleOutput(ModelOutput)
transformers.generation_flax_utils.GreedyState
transformers.generation_flax_utils.SampleState


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/convert_slow_tokenizers_checkpoints_to_fast.py----------------------------------------
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.logger->utils.logging.get_logger(__name__)
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.checkpoint_names->list(tokenizer_class.max_model_input_sizes.keys())
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.tokenizer->tokenizer_class.from_pretrained(checkpoint, force_download=force_download)
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.(checkpoint_directory, checkpoint_prefix_name)->checkpoint.split('/')
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.dump_path_full->os.path.join(dump_path_full, checkpoint_prefix_name)
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.file_names->tokenizer_class.from_pretrained(checkpoint, force_download=force_download).save_pretrained(dump_path_full, legacy_format=False, filename_prefix=checkpoint_prefix_name)
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.parser->argparse.ArgumentParser()
A:transformers.convert_slow_tokenizers_checkpoints_to_fast.args->argparse.ArgumentParser().parse_args()
transformers.convert_slow_tokenizers_checkpoints_to_fast.convert_slow_checkpoint_to_fast(tokenizer_name,checkpoint_name,dump_path,force_download)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_flax_pytorch_utils.py----------------------------------------
A:transformers.modeling_flax_pytorch_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_flax_pytorch_utils.pt_path->os.path.abspath(pytorch_checkpoint_path)
A:transformers.modeling_flax_pytorch_utils.pt_state_dict->torch.load(pt_path, map_location='cpu')
A:transformers.modeling_flax_pytorch_utils.flax_state_dict->flatten_dict(flax_state)
A:transformers.modeling_flax_pytorch_utils.random_flax_state_dict->flatten_dict(flax_model.params)
A:transformers.modeling_flax_pytorch_utils.pt_tuple_key->tuple(pt_key.split('.'))
A:transformers.modeling_flax_pytorch_utils.pt_tensor->pt_tensor.transpose(2, 3, 1, 0).transpose(2, 3, 1, 0)
A:transformers.modeling_flax_pytorch_utils.flax_state_dict[pt_tuple_key]->jax.numpy.asarray(pt_tensor)
A:transformers.modeling_flax_pytorch_utils.flax_checkpoint_path->os.path.abspath(flax_checkpoint_path)
A:transformers.modeling_flax_pytorch_utils.flax_cls->getattr(transformers, 'Flax' + model.__class__.__name__)
A:transformers.modeling_flax_pytorch_utils.pt_model_dict->pt_model.state_dict()
A:transformers.modeling_flax_pytorch_utils.missing_keys->list(missing_keys)
A:transformers.modeling_flax_pytorch_utils.flax_tensor->jax.numpy.transpose(flax_tensor, (3, 2, 0, 1))
A:transformers.modeling_flax_pytorch_utils.flax_key->'.'.join(flax_key_tuple)
A:transformers.modeling_flax_pytorch_utils.pt_model_dict[flax_key]->torch.from_numpy(flax_tensor)
transformers.modeling_flax_pytorch_utils.convert_pytorch_state_dict_to_flax(pt_state_dict,flax_model)
transformers.modeling_flax_pytorch_utils.load_flax_checkpoint_in_pytorch_model(model,flax_checkpoint_path)
transformers.modeling_flax_pytorch_utils.load_flax_weights_in_pytorch_model(pt_model,flax_state)
transformers.modeling_flax_pytorch_utils.load_pytorch_checkpoint_in_flax_state_dict(flax_model,pytorch_checkpoint_path,allow_missing_keys=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_logits_process.py----------------------------------------
A:transformers.generation_logits_process.logger->get_logger(__name__)
A:transformers.generation_logits_process.scores->scores.masked_fill(banned_mask, -float('inf')).masked_fill(banned_mask, -float('inf'))
A:transformers.generation_logits_process.score->torch.where(score < 0, score * self.penalty, score / self.penalty)
A:transformers.generation_logits_process.(sorted_logits, sorted_indices)->torch.sort(scores, descending=True)
A:transformers.generation_logits_process.cumulative_probs->sorted_logits.softmax(dim=-1).cumsum(dim=-1)
A:transformers.generation_logits_process.sorted_indices_to_remove[..., 1:]->sorted_indices_to_remove[..., :-1].clone()
A:transformers.generation_logits_process.indices_to_remove->sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
A:transformers.generation_logits_process.top_k->min(max(self.top_k, self.min_tokens_to_keep), scores.size(-1))
A:transformers.generation_logits_process.gen_tokens->prev_input_ids[idx].tolist()
A:transformers.generation_logits_process.prev_ngram_tuple->tuple(ngram[:-1])
A:transformers.generation_logits_process.ngram_idx->tuple(prev_input_ids[start_idx:cur_len].tolist())
A:transformers.generation_logits_process.generated_ngrams->_get_ngrams(ngram_size, prev_input_ids, num_hypos)
A:transformers.generation_logits_process.banned_batch_tokens->_calc_banned_ngram_tokens(self.ngram_size, input_ids, num_batch_hypotheses, cur_len)
A:transformers.generation_logits_process.encoder_input_ids->encoder_input_ids.unsqueeze(0).unsqueeze(0)
A:transformers.generation_logits_process.self.generated_ngrams->_get_ngrams(encoder_ngram_size, encoder_input_ids, self.batch_size)
A:transformers.generation_logits_process.self.bad_words_ids->list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))
A:transformers.generation_logits_process.banned_tokens->self._calc_banned_bad_words_ids(input_ids)
A:transformers.generation_logits_process.banned_mask->torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()
A:transformers.generation_logits_process.indices->torch.ones(len(banned_mask))
A:transformers.generation_logits_process.mask->torch.full_like(scores, -math.inf)
A:transformers.generation_logits_process.group_end_idx->min(group_start_idx + self._num_sub_beams, self._num_beams)
A:transformers.generation_logits_process.token_frequency->torch.bincount(previous_group_tokens, minlength=vocab_size).to(scores.device)
transformers.ForcedBOSTokenLogitsProcessor(self,bos_token_id:int)
transformers.ForcedEOSTokenLogitsProcessor(self,max_length:int,eos_token_id:int)
transformers.HammingDiversityLogitsProcessor(self,diversity_penalty:float,num_beams:int,num_beam_groups:int)
transformers.InfNanRemoveLogitsProcessor(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.LogitsProcessor(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.LogitsProcessorList(self,input_ids:torch.LongTensor,scores:torch.FloatTensor,**kwargs)
transformers.LogitsWarper(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.MinLengthLogitsProcessor(self,min_length:int,eos_token_id:int)
transformers.NoBadWordsLogitsProcessor(self,bad_words_ids:Iterable[Iterable[int]],eos_token_id:int)
transformers.NoBadWordsLogitsProcessor._calc_banned_bad_words_ids(self,prev_input_ids:Iterable[int])->Iterable[int]
transformers.NoBadWordsLogitsProcessor._set_scores_to_inf_for_banned_tokens(self,scores:torch.Tensor,banned_tokens:List[List[int]])->None
transformers.NoBadWordsLogitsProcessor._tokens_match(self,prev_tokens:torch.LongTensor,tokens:List[int])->bool
transformers.NoRepeatNGramLogitsProcessor(self,ngram_size:int)
transformers.PrefixConstrainedLogitsProcessor(self,prefix_allowed_tokens_fn:Callable[[int,torch.Tensor],List[int]],num_beams:int)
transformers.RepetitionPenaltyLogitsProcessor(self,penalty:float)
transformers.TemperatureLogitsWarper(self,temperature:float)
transformers.TopKLogitsWarper(self,top_k:int,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.TopPLogitsWarper(self,top_p:float,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_logits_process.EncoderNoRepeatNGramLogitsProcessor(self,encoder_ngram_size:int,encoder_input_ids:torch.LongTensor)
transformers.generation_logits_process.EncoderNoRepeatNGramLogitsProcessor.__init__(self,encoder_ngram_size:int,encoder_input_ids:torch.LongTensor)
transformers.generation_logits_process.ForcedBOSTokenLogitsProcessor(self,bos_token_id:int)
transformers.generation_logits_process.ForcedBOSTokenLogitsProcessor.__init__(self,bos_token_id:int)
transformers.generation_logits_process.ForcedEOSTokenLogitsProcessor(self,max_length:int,eos_token_id:int)
transformers.generation_logits_process.ForcedEOSTokenLogitsProcessor.__init__(self,max_length:int,eos_token_id:int)
transformers.generation_logits_process.HammingDiversityLogitsProcessor(self,diversity_penalty:float,num_beams:int,num_beam_groups:int)
transformers.generation_logits_process.HammingDiversityLogitsProcessor.__init__(self,diversity_penalty:float,num_beams:int,num_beam_groups:int)
transformers.generation_logits_process.InfNanRemoveLogitsProcessor(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.generation_logits_process.InfNanRemoveLogitsProcessor.__call__(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.generation_logits_process.LogitsProcessor(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.generation_logits_process.LogitsProcessor.__call__(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.generation_logits_process.LogitsProcessorList(self,input_ids:torch.LongTensor,scores:torch.FloatTensor,**kwargs)
transformers.generation_logits_process.LogitsProcessorList.__call__(self,input_ids:torch.LongTensor,scores:torch.FloatTensor,**kwargs)
transformers.generation_logits_process.LogitsWarper(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.generation_logits_process.LogitsWarper.__call__(self,input_ids:torch.LongTensor,scores:torch.FloatTensor)
transformers.generation_logits_process.MinLengthLogitsProcessor(self,min_length:int,eos_token_id:int)
transformers.generation_logits_process.MinLengthLogitsProcessor.__init__(self,min_length:int,eos_token_id:int)
transformers.generation_logits_process.NoBadWordsLogitsProcessor(self,bad_words_ids:Iterable[Iterable[int]],eos_token_id:int)
transformers.generation_logits_process.NoBadWordsLogitsProcessor.__init__(self,bad_words_ids:Iterable[Iterable[int]],eos_token_id:int)
transformers.generation_logits_process.NoBadWordsLogitsProcessor._calc_banned_bad_words_ids(self,prev_input_ids:Iterable[int])->Iterable[int]
transformers.generation_logits_process.NoBadWordsLogitsProcessor._set_scores_to_inf_for_banned_tokens(self,scores:torch.Tensor,banned_tokens:List[List[int]])->None
transformers.generation_logits_process.NoBadWordsLogitsProcessor._tokens_match(self,prev_tokens:torch.LongTensor,tokens:List[int])->bool
transformers.generation_logits_process.NoRepeatNGramLogitsProcessor(self,ngram_size:int)
transformers.generation_logits_process.NoRepeatNGramLogitsProcessor.__init__(self,ngram_size:int)
transformers.generation_logits_process.PrefixConstrainedLogitsProcessor(self,prefix_allowed_tokens_fn:Callable[[int,torch.Tensor],List[int]],num_beams:int)
transformers.generation_logits_process.PrefixConstrainedLogitsProcessor.__init__(self,prefix_allowed_tokens_fn:Callable[[int,torch.Tensor],List[int]],num_beams:int)
transformers.generation_logits_process.RepetitionPenaltyLogitsProcessor(self,penalty:float)
transformers.generation_logits_process.RepetitionPenaltyLogitsProcessor.__init__(self,penalty:float)
transformers.generation_logits_process.TemperatureLogitsWarper(self,temperature:float)
transformers.generation_logits_process.TemperatureLogitsWarper.__init__(self,temperature:float)
transformers.generation_logits_process.TopKLogitsWarper(self,top_k:int,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_logits_process.TopKLogitsWarper.__init__(self,top_k:int,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_logits_process.TopPLogitsWarper(self,top_p:float,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_logits_process.TopPLogitsWarper.__init__(self,top_p:float,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)
transformers.generation_logits_process._calc_banned_ngram_tokens(ngram_size:int,prev_input_ids:torch.Tensor,num_hypos:int,cur_len:int)->List[Iterable[int]]
transformers.generation_logits_process._get_generated_ngrams(banned_ngrams,prev_input_ids,ngram_size,cur_len)
transformers.generation_logits_process._get_ngrams(ngram_size:int,prev_input_ids:torch.Tensor,num_hypos:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/convert_graph_to_onnx.py----------------------------------------
A:transformers.convert_graph_to_onnx.ORT_QUANTIZE_MINIMUM_VERSION->parse('1.4.0')
A:transformers.convert_graph_to_onnx.ort_version->parse(onnxruntime.__version__)
A:transformers.convert_graph_to_onnx.tokens->load_graph_from_args(pipeline_name, framework, model, tokenizer, **model_kwargs).tokenizer('This is a sample output', return_tensors=framework)
A:transformers.convert_graph_to_onnx.outputs->outputs.to_tuple().to_tuple()
A:transformers.convert_graph_to_onnx.input_vars->list(tokens.keys())
A:transformers.convert_graph_to_onnx.dynamic_axes->dict(input_dynamic_axes, **output_dynamic_axes)
A:transformers.convert_graph_to_onnx.(input_names, output_names, dynamic_axes, tokens)->infer_shapes(nlp, 'tf')
A:transformers.convert_graph_to_onnx.(ordered_input_names, model_args)->ensure_valid_input(nlp.model, tokens, input_names)
A:transformers.convert_graph_to_onnx.onnx_model->onnx.load(onnx_model_path.as_posix())
A:transformers.convert_graph_to_onnx.nlp->load_graph_from_args(pipeline_name, framework, model, tokenizer, **model_kwargs)
A:transformers.convert_graph_to_onnx.opt_model_path->generate_identified_filename(onnx_model_path, '-optimized')
A:transformers.convert_graph_to_onnx.sess_option->SessionOptions()
A:transformers.convert_graph_to_onnx.sess_option.optimized_model_filepath->generate_identified_filename(onnx_model_path, '-optimized').as_posix()
A:transformers.convert_graph_to_onnx._->InferenceSession(path.as_posix(), onnx_options, providers=['CPUExecutionProvider'])
A:transformers.convert_graph_to_onnx.quantized_model->quantize(model=onnx_model, quantization_mode=QuantizationMode.IntegerOps, force_fusions=True, symmetric_weight=True)
A:transformers.convert_graph_to_onnx.quantized_model_path->generate_identified_filename(onnx_model_path, '-quantized')
A:transformers.convert_graph_to_onnx.onnx_options->SessionOptions()
A:transformers.convert_graph_to_onnx.parser->OnnxConverterArgumentParser()
A:transformers.convert_graph_to_onnx.args->OnnxConverterArgumentParser().parse_args()
A:transformers.convert_graph_to_onnx.args.output->Path(args.output).absolute()
A:transformers.convert_graph_to_onnx.args.optimized_output->optimize(args.output)
A:transformers.convert_graph_to_onnx.args.quantized_output->quantize(args.optimized_output)
transformers.convert_graph_to_onnx.OnnxConverterArgumentParser(self)
transformers.convert_graph_to_onnx.OnnxConverterArgumentParser.__init__(self)
transformers.convert_graph_to_onnx.check_onnxruntime_requirements(minimum_version:Version)
transformers.convert_graph_to_onnx.convert(framework:str,model:str,output:Path,opset:int,tokenizer:Optional[str]=None,use_external_format:bool=False,pipeline_name:str='feature-extraction',**model_kwargs)
transformers.convert_graph_to_onnx.convert_pytorch(nlp:Pipeline,opset:int,output:Path,use_external_format:bool)
transformers.convert_graph_to_onnx.convert_tensorflow(nlp:Pipeline,opset:int,output:Path)
transformers.convert_graph_to_onnx.ensure_valid_input(model,tokens,input_names)
transformers.convert_graph_to_onnx.generate_identified_filename(filename:Path,identifier:str)->Path
transformers.convert_graph_to_onnx.infer_shapes(nlp:Pipeline,framework:str)->Tuple[List[str], List[str], Dict, BatchEncoding]
transformers.convert_graph_to_onnx.load_graph_from_args(pipeline_name:str,framework:str,model:str,tokenizer:Optional[str]=None,**models_kwargs)->Pipeline
transformers.convert_graph_to_onnx.optimize(onnx_model_path:Path)->Path
transformers.convert_graph_to_onnx.quantize(onnx_model_path:Path)->Path
transformers.convert_graph_to_onnx.verify(path:Path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/trainer_seq2seq.py----------------------------------------
A:transformers.trainer_seq2seq.logger->utils.logging.get_logger(__name__)
A:transformers.trainer_seq2seq.inputs->self._prepare_inputs(inputs)
A:transformers.trainer_seq2seq.generated_tokens->self._pad_tensors_to_max_len(generated_tokens, gen_kwargs['max_length'])
A:transformers.trainer_seq2seq.outputs->model(**inputs)
A:transformers.trainer_seq2seq.loss->(outputs['loss'] if isinstance(outputs, dict) else outputs[0]).mean().detach()
A:transformers.trainer_seq2seq.labels->self._pad_tensors_to_max_len(labels, gen_kwargs['max_length'])
transformers.Seq2SeqTrainer(Trainer)
transformers.Seq2SeqTrainer._pad_tensors_to_max_len(self,tensor,max_length)
transformers.Seq2SeqTrainer.evaluate(self,eval_dataset:Optional[Dataset]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval',max_length:Optional[int]=None,num_beams:Optional[int]=None)->Dict[str, float]
transformers.Seq2SeqTrainer.predict(self,test_dataset:Dataset,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval',max_length:Optional[int]=None,num_beams:Optional[int]=None)->PredictionOutput
transformers.Seq2SeqTrainer.prediction_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]],prediction_loss_only:bool,ignore_keys:Optional[List[str]]=None)->Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]
transformers.trainer_seq2seq.Seq2SeqTrainer(Trainer)
transformers.trainer_seq2seq.Seq2SeqTrainer._pad_tensors_to_max_len(self,tensor,max_length)
transformers.trainer_seq2seq.Seq2SeqTrainer.evaluate(self,eval_dataset:Optional[Dataset]=None,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval',max_length:Optional[int]=None,num_beams:Optional[int]=None)->Dict[str, float]
transformers.trainer_seq2seq.Seq2SeqTrainer.predict(self,test_dataset:Dataset,ignore_keys:Optional[List[str]]=None,metric_key_prefix:str='eval',max_length:Optional[int]=None,num_beams:Optional[int]=None)->PredictionOutput
transformers.trainer_seq2seq.Seq2SeqTrainer.prediction_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]],prediction_loss_only:bool,ignore_keys:Optional[List[str]]=None)->Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/image_utils.py----------------------------------------
A:transformers.image_utils.image->self.to_pil_image(image)
A:transformers.image_utils.rescale->isinstance(image.flat[0], np.integer)
A:transformers.image_utils.mean->torch.tensor(mean)
A:transformers.image_utils.std->torch.tensor(std)
A:transformers.image_utils.new_image->self.to_pil_image(image).new_zeros(new_shape)
transformers.ImageFeatureExtractionMixin
transformers.ImageFeatureExtractionMixin._ensure_format_supported(self,image)
transformers.ImageFeatureExtractionMixin.center_crop(self,image,size)
transformers.ImageFeatureExtractionMixin.normalize(self,image,mean,std)
transformers.ImageFeatureExtractionMixin.resize(self,image,size,resample=PIL.Image.BILINEAR)
transformers.ImageFeatureExtractionMixin.to_numpy_array(self,image,rescale=None,channel_first=True)
transformers.ImageFeatureExtractionMixin.to_pil_image(self,image,rescale=None)
transformers.image_utils.ImageFeatureExtractionMixin
transformers.image_utils.ImageFeatureExtractionMixin._ensure_format_supported(self,image)
transformers.image_utils.ImageFeatureExtractionMixin.center_crop(self,image,size)
transformers.image_utils.ImageFeatureExtractionMixin.normalize(self,image,mean,std)
transformers.image_utils.ImageFeatureExtractionMixin.resize(self,image,size,resample=PIL.Image.BILINEAR)
transformers.image_utils.ImageFeatureExtractionMixin.to_numpy_array(self,image,rescale=None,channel_first=True)
transformers.image_utils.ImageFeatureExtractionMixin.to_pil_image(self,image,rescale=None)
transformers.image_utils.is_torch_tensor(obj)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_outputs.py----------------------------------------
transformers.modeling_outputs.BaseModelOutput(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithCrossAttentions(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithPast(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithPooling(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput)
transformers.modeling_outputs.CausalLMOutput(ModelOutput)
transformers.modeling_outputs.CausalLMOutputWithCrossAttentions(ModelOutput)
transformers.modeling_outputs.CausalLMOutputWithPast(ModelOutput)
transformers.modeling_outputs.MaskedLMOutput(ModelOutput)
transformers.modeling_outputs.MultipleChoiceModelOutput(ModelOutput)
transformers.modeling_outputs.NextSentencePredictorOutput(ModelOutput)
transformers.modeling_outputs.QuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqLMOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqModelOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput(ModelOutput)
transformers.modeling_outputs.SequenceClassifierOutput(ModelOutput)
transformers.modeling_outputs.SequenceClassifierOutputWithPast(ModelOutput)
transformers.modeling_outputs.TokenClassifierOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_beam_search.py----------------------------------------
A:transformers.generation_beam_search.self._done->torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)
A:transformers.generation_beam_search.batch_size->len(self._beam_hyps)
A:transformers.generation_beam_search.next_beam_scores->torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)
A:transformers.generation_beam_search.next_beam_tokens->torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)
A:transformers.generation_beam_search.next_beam_indices->torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)
A:transformers.generation_beam_search.final_score->final_beam_scores[batch_beam_idx].item()
A:transformers.generation_beam_search.sent_lengths->input_ids.new(batch_size * self.num_beam_hyps_to_keep)
A:transformers.generation_beam_search.best_scores->torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)
A:transformers.generation_beam_search.sorted_hyps->sorted(beam_hyp.beams, key=lambda x: x[0])
A:transformers.generation_beam_search.best_hyp_tuple->sorted(beam_hyp.beams, key=lambda x: x[0]).pop()
A:transformers.generation_beam_search.sent_lengths[self.num_beam_hyps_to_keep * i + j]->len(best_hyp)
A:transformers.generation_beam_search.sent_max_len->min(sent_lengths.max().item() + 1, max_length)
A:transformers.generation_beam_search.sorted_next_scores->sorted([(s, idx) for (idx, (s, _)) in enumerate(self.beams)])
A:transformers.generation_beam_search.self.worst_score->min(score, self.worst_score)
transformers.BeamScorer(ABC)
transformers.BeamScorer.finalize(self,input_ids:torch.LongTensor,next_scores:torch.FloatTensor,next_tokens:torch.LongTensor,next_indices:torch.LongTensor,max_length:int,**kwargs)->torch.LongTensor
transformers.BeamScorer.process(self,input_ids:torch.LongTensor,next_scores:torch.FloatTensor,next_tokens:torch.LongTensor,next_indices:torch.LongTensor,**kwargs)->Tuple[torch.Tensor]
transformers.BeamSearchScorer(self,batch_size:int,num_beams:int,device:torch.device,length_penalty:Optional[float]=1.0,do_early_stopping:Optional[bool]=False,num_beam_hyps_to_keep:Optional[int]=1,num_beam_groups:Optional[int]=1,**kwargs)
transformers.BeamSearchScorer.finalize(self,input_ids:torch.LongTensor,final_beam_scores:torch.FloatTensor,final_beam_tokens:torch.LongTensor,final_beam_indices:torch.LongTensor,max_length:int,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None)->Tuple[torch.LongTensor]
transformers.BeamSearchScorer.is_done(self)->bool
transformers.BeamSearchScorer.process(self,input_ids:torch.LongTensor,next_scores:torch.FloatTensor,next_tokens:torch.LongTensor,next_indices:torch.LongTensor,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None)->Tuple[torch.Tensor]
transformers.generation_beam_search.BeamHypotheses(self,num_beams:int,length_penalty:float,early_stopping:bool)
transformers.generation_beam_search.BeamHypotheses.__init__(self,num_beams:int,length_penalty:float,early_stopping:bool)
transformers.generation_beam_search.BeamHypotheses.__len__(self)
transformers.generation_beam_search.BeamHypotheses.add(self,hyp:torch.LongTensor,sum_logprobs:float)
transformers.generation_beam_search.BeamHypotheses.is_done(self,best_sum_logprobs:float,cur_len:int)->bool
transformers.generation_beam_search.BeamScorer(ABC)
transformers.generation_beam_search.BeamScorer.finalize(self,input_ids:torch.LongTensor,next_scores:torch.FloatTensor,next_tokens:torch.LongTensor,next_indices:torch.LongTensor,max_length:int,**kwargs)->torch.LongTensor
transformers.generation_beam_search.BeamScorer.process(self,input_ids:torch.LongTensor,next_scores:torch.FloatTensor,next_tokens:torch.LongTensor,next_indices:torch.LongTensor,**kwargs)->Tuple[torch.Tensor]
transformers.generation_beam_search.BeamSearchScorer(self,batch_size:int,num_beams:int,device:torch.device,length_penalty:Optional[float]=1.0,do_early_stopping:Optional[bool]=False,num_beam_hyps_to_keep:Optional[int]=1,num_beam_groups:Optional[int]=1,**kwargs)
transformers.generation_beam_search.BeamSearchScorer.__init__(self,batch_size:int,num_beams:int,device:torch.device,length_penalty:Optional[float]=1.0,do_early_stopping:Optional[bool]=False,num_beam_hyps_to_keep:Optional[int]=1,num_beam_groups:Optional[int]=1,**kwargs)
transformers.generation_beam_search.BeamSearchScorer.finalize(self,input_ids:torch.LongTensor,final_beam_scores:torch.FloatTensor,final_beam_tokens:torch.LongTensor,final_beam_indices:torch.LongTensor,max_length:int,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None)->Tuple[torch.LongTensor]
transformers.generation_beam_search.BeamSearchScorer.is_done(self)->bool
transformers.generation_beam_search.BeamSearchScorer.process(self,input_ids:torch.LongTensor,next_scores:torch.FloatTensor,next_tokens:torch.LongTensor,next_indices:torch.LongTensor,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None)->Tuple[torch.Tensor]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/deepspeed.py----------------------------------------
A:transformers.deepspeed.logger->utils.logging.get_logger(__name__)
A:transformers.deepspeed.config->config.get(node).get(node)
A:transformers.deepspeed.self._stage->self.get_value('zero_optimization.stage', -1)
A:transformers.deepspeed.offload_devices_valid->set(['cpu', 'nvme'])
A:transformers.deepspeed.offload_devices->set([self.get_value('zero_optimization.offload_optimizer.device'), self.get_value('zero_optimization.offload_param.device')])
A:transformers.deepspeed.nodes->ds_key_long.split('.')
A:transformers.deepspeed.ds_key->ds_key_long.split('.').pop()
A:transformers.deepspeed.(config, ds_key)->self.find_config_node(ds_key_long)
A:transformers.deepspeed.value->self.get_value(ds_key_long)
A:transformers.deepspeed.ds_val->config.get(node).get(node).get(ds_key)
A:transformers.deepspeed.fill_only->partialmethod(fill_match, must_match=False)
A:transformers.deepspeed.mismatches->'\n'.join(self.mismatches)
A:transformers.deepspeed._hf_deepspeed_config_weak_ref->weakref.ref(hf_deepspeed_config_obj)
A:transformers.deepspeed.model_parameters->filter(lambda p: p.requires_grad, model.parameters())
A:transformers.deepspeed.(model, optimizer, _, lr_scheduler)->deepspeed.initialize(model=model, model_parameters=model_parameters, config_params=config, optimizer=optimizer, lr_scheduler=lr_scheduler)
A:transformers.deepspeed.deepspeed_checkpoint_dirs->sorted(glob.glob(f'{resume_from_checkpoint}/global_step*'))
A:transformers.deepspeed.(load_path, _)->model.load_checkpoint(resume_from_checkpoint, load_optimizer_states=True, load_lr_scheduler_states=True)
transformers.deepspeed.HfDeepSpeedConfig(self,config_file_or_dict)
transformers.deepspeed.HfDeepSpeedConfig.__init__(self,config_file_or_dict)
transformers.deepspeed.HfDeepSpeedConfig.find_config_node(self,ds_key_long)
transformers.deepspeed.HfDeepSpeedConfig.get_value(self,ds_key_long,default=None)
transformers.deepspeed.HfDeepSpeedConfig.is_false(self,ds_key_long)
transformers.deepspeed.HfDeepSpeedConfig.is_offload(self)
transformers.deepspeed.HfDeepSpeedConfig.is_true(self,ds_key_long)
transformers.deepspeed.HfDeepSpeedConfig.is_zero2(self)
transformers.deepspeed.HfDeepSpeedConfig.is_zero3(self)
transformers.deepspeed.HfTrainerDeepSpeedConfig(self,config_file_or_dict)
transformers.deepspeed.HfTrainerDeepSpeedConfig.__init__(self,config_file_or_dict)
transformers.deepspeed.HfTrainerDeepSpeedConfig.dtype(self)
transformers.deepspeed.HfTrainerDeepSpeedConfig.fill_match(self,ds_key_long,hf_val,hf_key=None,must_match=True)
transformers.deepspeed.HfTrainerDeepSpeedConfig.trainer_config_finalize(self,args,model,num_training_steps)
transformers.deepspeed.HfTrainerDeepSpeedConfig.trainer_config_process(self,args)
transformers.deepspeed.deepspeed_config()
transformers.deepspeed.deepspeed_init(trainer,num_training_steps,resume_from_checkpoint=None)
transformers.deepspeed.is_deepspeed_available()
transformers.deepspeed.is_deepspeed_zero3_enabled()
transformers.deepspeed.set_hf_deepspeed_config(hf_deepspeed_config_obj)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_utils.py----------------------------------------
A:transformers.generation_utils.logger->utils.logging.get_logger(__name__)
A:transformers.generation_utils.encoder->self.get_encoder()
A:transformers.generation_utils.decoder_start_token_id->self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)
A:transformers.generation_utils.expanded_return_idx->torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)
A:transformers.generation_utils.input_ids->torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)
A:transformers.generation_utils.model_kwargs['token_type_ids']->torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)
A:transformers.generation_utils.model_kwargs['attention_mask']->self._prepare_attention_mask_for_generation(input_ids, pad_token_id, eos_token_id)
A:transformers.generation_utils.encoder_outputs['last_hidden_state']->encoder_outputs.last_hidden_state.index_select(0, expanded_return_idx.to(encoder_outputs.last_hidden_state.device))
A:transformers.generation_utils.warpers->LogitsProcessorList()
A:transformers.generation_utils.processors->LogitsProcessorList()
A:transformers.generation_utils.stopping_criteria->validate_stopping_criteria(stopping_criteria, max_length)
A:transformers.generation_utils.model_kwargs->self._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder)
A:transformers.generation_utils.logits_processor->self._get_logits_processor(repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size, encoder_input_ids=encoder_input_ids, bad_words_ids=bad_words_ids, min_length=min_length, max_length=max_length, eos_token_id=eos_token_id, forced_bos_token_id=forced_bos_token_id, forced_eos_token_id=forced_eos_token_id, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn, num_beams=num_beams, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty, remove_invalid_values=remove_invalid_values)
A:transformers.generation_utils.logits_warper->self._get_logits_warper(top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams)
A:transformers.generation_utils.(input_ids, model_kwargs)->self._expand_inputs_for_generation(input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs)
A:transformers.generation_utils.beam_scorer->BeamSearchScorer(batch_size=batch_size, num_beams=num_beams, device=self.device, length_penalty=length_penalty, do_early_stopping=early_stopping)
A:transformers.generation_utils.diverse_beam_scorer->BeamSearchScorer(batch_size=batch_size, num_beams=num_beams, max_length=stopping_criteria.max_length, device=self.device, length_penalty=length_penalty, do_early_stopping=early_stopping, num_beam_hyps_to_keep=num_return_sequences, num_beam_groups=num_beam_groups)
A:transformers.generation_utils.unfinished_sequences->unfinished_sequences.mul((next_tokens != eos_token_id).long()).mul((next_tokens != eos_token_id).long())
A:transformers.generation_utils.this_peer_finished_flag->torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)
A:transformers.generation_utils.model_inputs->self.prepare_inputs_for_generation(input_ids, **model_kwargs)
A:transformers.generation_utils.outputs->self(**model_inputs, return_dict=True, output_attentions=output_attentions, output_hidden_states=output_hidden_states)
A:transformers.generation_utils.next_tokens_scores->logits_processor(input_ids, next_token_logits)
A:transformers.generation_utils.next_tokens->torch.gather(next_tokens, -1, _indices)
A:transformers.generation_utils.next_token_scores->next_token_scores.view(batch_size, group_size * vocab_size).view(batch_size, group_size * vocab_size)
A:transformers.generation_utils.probs->torch.nn.functional.softmax(next_token_scores, dim=-1)
A:transformers.generation_utils.batch_size->len(beam_scorer._beam_hyps)
A:transformers.generation_utils.beam_scores->beam_scores.view((batch_size * num_beams,)).view((batch_size * num_beams,))
A:transformers.generation_utils.next_token_logits->self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)
A:transformers.generation_utils.(next_token_scores, next_tokens)->torch.topk(next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True)
A:transformers.generation_utils.beam_outputs->BeamSearchScorer(batch_size=batch_size, num_beams=num_beams, device=self.device, length_penalty=length_penalty, do_early_stopping=early_stopping).process(group_input_ids, next_token_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id)
A:transformers.generation_utils.model_kwargs['past']->self._reorder_cache(model_kwargs['past'], reordering_indices)
A:transformers.generation_utils.sequence_outputs->BeamSearchScorer(batch_size=batch_size, num_beams=num_beams, device=self.device, length_penalty=length_penalty, do_early_stopping=early_stopping).finalize(input_ids, beam_scores, next_tokens, next_indices, pad_token_id=pad_token_id, eos_token_id=eos_token_id, max_length=stopping_criteria.max_length)
A:transformers.generation_utils.(next_token_scores, _indices)->torch.sort(next_token_scores, descending=True, dim=1)
A:transformers.generation_utils.current_tokens->torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)
A:transformers.generation_utils.reordering_indices->torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)
A:transformers.generation_utils.group_end_idx->min(group_start_idx + num_sub_beams, num_beams)
A:transformers.generation_utils.processed_score->torch.zeros_like(outputs.logits[:, -1, :])
A:transformers.generation_utils.group_input_ids->torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)
A:transformers.generation_utils.logits->TopPLogitsWarper(top_p=top_p, min_tokens_to_keep=min_tokens_to_keep)(None, logits)
transformers.generation_utils.BeamSampleDecoderOnlyOutput(ModelOutput)
transformers.generation_utils.BeamSampleEncoderDecoderOutput(ModelOutput)
transformers.generation_utils.BeamSearchDecoderOnlyOutput(ModelOutput)
transformers.generation_utils.BeamSearchEncoderDecoderOutput(ModelOutput)
transformers.generation_utils.GenerationMixin
transformers.generation_utils.GenerationMixin._expand_inputs_for_generation(input_ids:torch.LongTensor,expand_size:int=1,is_encoder_decoder:bool=False,attention_mask:torch.LongTensor=None,encoder_outputs:ModelOutput=None,**model_kwargs)->Tuple[torch.LongTensor, Dict[str, Any]]
transformers.generation_utils.GenerationMixin._get_decoder_start_token_id(self,decoder_start_token_id:int=None,bos_token_id:int=None)->int
transformers.generation_utils.GenerationMixin._get_logits_processor(self,repetition_penalty:float,no_repeat_ngram_size:int,encoder_no_repeat_ngram_size:int,encoder_input_ids:torch.LongTensor,bad_words_ids:List[List[int]],min_length:int,max_length:int,eos_token_id:int,forced_bos_token_id:int,forced_eos_token_id:int,prefix_allowed_tokens_fn:Callable[[int,torch.Tensor],List[int]],num_beams:int,num_beam_groups:int,diversity_penalty:float,remove_invalid_values:bool)->LogitsProcessorList
transformers.generation_utils.GenerationMixin._get_logits_warper(self,top_k:int=None,top_p:float=None,temperature:float=None,num_beams:int=None)->LogitsProcessorList
transformers.generation_utils.GenerationMixin._get_pad_token_id(self,pad_token_id:int=None,eos_token_id:int=None)->int
transformers.generation_utils.GenerationMixin._get_stopping_criteria(self,max_length:Optional[int],max_time:Optional[float],max_new_tokens:Optional[int],start_length:int)->StoppingCriteriaList
transformers.generation_utils.GenerationMixin._prepare_attention_mask_for_generation(self,input_ids:torch.Tensor,pad_token_id:int,eos_token_id:int)->torch.LongTensor
transformers.generation_utils.GenerationMixin._prepare_decoder_input_ids_for_generation(self,input_ids:torch.LongTensor,decoder_start_token_id:int=None,bos_token_id:int=None)->torch.LongTensor
transformers.generation_utils.GenerationMixin._prepare_encoder_decoder_kwargs_for_generation(self,input_ids:torch.LongTensor,model_kwargs)->Dict[str, Any]
transformers.generation_utils.GenerationMixin._prepare_input_ids_for_generation(self,bos_token_id:Optional[int],encoder_outputs:Optional[ModelOutput])->torch.LongTensor
transformers.generation_utils.GenerationMixin._reorder_cache(self,past,beam_idx)
transformers.generation_utils.GenerationMixin._update_model_kwargs_for_generation(outputs:ModelOutput,model_kwargs:Dict[str,Any],is_encoder_decoder:bool=False)->Dict[str, Any]
transformers.generation_utils.GenerationMixin.adjust_logits_during_generation(self,logits:torch.FloatTensor,**kwargs)->torch.FloatTensor
transformers.generation_utils.GenerationMixin.beam_sample(self,input_ids:torch.LongTensor,beam_scorer:BeamScorer,logits_processor:Optional[LogitsProcessorList]=None,stopping_criteria:Optional[StoppingCriteriaList]=None,logits_warper:Optional[LogitsProcessorList]=None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,output_scores:Optional[bool]=None,return_dict_in_generate:Optional[bool]=None,synced_gpus:Optional[bool]=None,**model_kwargs)->Union[BeamSampleOutput, torch.LongTensor]
transformers.generation_utils.GenerationMixin.beam_search(self,input_ids:torch.LongTensor,beam_scorer:BeamScorer,logits_processor:Optional[LogitsProcessorList]=None,stopping_criteria:Optional[StoppingCriteriaList]=None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,output_scores:Optional[bool]=None,return_dict_in_generate:Optional[bool]=None,synced_gpus:Optional[bool]=None,**model_kwargs)->Union[BeamSearchOutput, torch.LongTensor]
transformers.generation_utils.GenerationMixin.generate(self,input_ids:Optional[torch.LongTensor]=None,max_length:Optional[int]=None,min_length:Optional[int]=None,do_sample:Optional[bool]=None,early_stopping:Optional[bool]=None,num_beams:Optional[int]=None,temperature:Optional[float]=None,top_k:Optional[int]=None,top_p:Optional[float]=None,repetition_penalty:Optional[float]=None,bad_words_ids:Optional[Iterable[int]]=None,bos_token_id:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,length_penalty:Optional[float]=None,no_repeat_ngram_size:Optional[int]=None,encoder_no_repeat_ngram_size:Optional[int]=None,num_return_sequences:Optional[int]=None,max_time:Optional[float]=None,max_new_tokens:Optional[int]=None,decoder_start_token_id:Optional[int]=None,use_cache:Optional[bool]=None,num_beam_groups:Optional[int]=None,diversity_penalty:Optional[float]=None,prefix_allowed_tokens_fn:Optional[Callable[[int,torch.Tensor],List[int]]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,output_scores:Optional[bool]=None,return_dict_in_generate:Optional[bool]=None,forced_bos_token_id:Optional[int]=None,forced_eos_token_id:Optional[int]=None,remove_invalid_values:Optional[bool]=None,synced_gpus:Optional[bool]=None,**model_kwargs)->Union[GreedySearchOutput, SampleOutput, BeamSearchOutput, BeamSampleOutput, torch.LongTensor]
transformers.generation_utils.GenerationMixin.greedy_search(self,input_ids:torch.LongTensor,logits_processor:Optional[LogitsProcessorList]=None,stopping_criteria:Optional[StoppingCriteriaList]=None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,output_scores:Optional[bool]=None,return_dict_in_generate:Optional[bool]=None,synced_gpus:Optional[bool]=None,**model_kwargs)->Union[GreedySearchOutput, torch.LongTensor]
transformers.generation_utils.GenerationMixin.group_beam_search(self,input_ids:torch.LongTensor,beam_scorer:BeamScorer,logits_processor:Optional[LogitsProcessorList]=None,stopping_criteria:Optional[StoppingCriteriaList]=None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,output_scores:Optional[bool]=None,return_dict_in_generate:Optional[bool]=None,synced_gpus:Optional[bool]=None,**model_kwargs)
transformers.generation_utils.GenerationMixin.prepare_inputs_for_generation(self,input_ids:torch.LongTensor,**kwargs)->Dict[str, Any]
transformers.generation_utils.GenerationMixin.sample(self,input_ids:torch.LongTensor,logits_processor:Optional[LogitsProcessorList]=None,stopping_criteria:Optional[StoppingCriteriaList]=None,logits_warper:Optional[LogitsProcessorList]=None,max_length:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,output_scores:Optional[bool]=None,return_dict_in_generate:Optional[bool]=None,synced_gpus:Optional[bool]=None,**model_kwargs)->Union[SampleOutput, torch.LongTensor]
transformers.generation_utils.GreedySearchDecoderOnlyOutput(ModelOutput)
transformers.generation_utils.GreedySearchEncoderDecoderOutput(ModelOutput)
transformers.generation_utils.SampleDecoderOnlyOutput(ModelOutput)
transformers.generation_utils.SampleEncoderDecoderOutput(ModelOutput)
transformers.generation_utils.top_k_top_p_filtering(logits:torch.FloatTensor,top_k:int=0,top_p:float=1.0,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)->torch.FloatTensor
transformers.top_k_top_p_filtering(logits:torch.FloatTensor,top_k:int=0,top_p:float=1.0,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)->torch.FloatTensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/tokenization_utils_fast.py----------------------------------------
A:transformers.tokenization_utils_fast.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_utils_fast.tokenizer_object->kwargs.pop('tokenizer_object', None)
A:transformers.tokenization_utils_fast.slow_tokenizer->self.slow_tokenizer_class(*args, **kwargs)
A:transformers.tokenization_utils_fast.fast_tokenizer_file->kwargs.pop('tokenizer_file', None)
A:transformers.tokenization_utils_fast.from_slow->kwargs.pop('from_slow', False)
A:transformers.tokenization_utils_fast.fast_tokenizer->convert_slow_tokenizer(slow_tokenizer)
A:transformers.tokenization_utils_fast.base_vocab->self._tokenizer.get_vocab(with_added_tokens=False)
A:transformers.tokenization_utils_fast.full_vocab->self._tokenizer.get_vocab(with_added_tokens=True)
A:transformers.tokenization_utils_fast.added_vocab->self.get_added_vocab()
A:transformers.tokenization_utils_fast.encoding_dict->defaultdict(list)
A:transformers.tokenization_utils_fast.index->int(index)
A:transformers.tokenization_utils_fast.encodings->self._tokenizer.encode_batch(batch_text_or_text_pairs, add_special_tokens=add_special_tokens, is_pretokenized=is_split_into_words)
A:transformers.tokenization_utils_fast.batched_output->BatchEncoding({key: value[0] if len(value) > 0 and isinstance(value[0], list) else value for (key, value) in batched_output.items()}, batched_output.encodings)
A:transformers.tokenization_utils_fast.self._decode_use_source_tokenizer->kwargs.pop('use_source_tokenizer', False)
A:transformers.tokenization_utils_fast.text->self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
A:transformers.tokenization_utils_fast.clean_text->self.clean_up_tokenization(text)
A:transformers.tokenization_utils_fast.save_directory->str(save_directory)
A:transformers.tokenization_utils_fast.added_tokens_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + ADDED_TOKENS_FILE)
A:transformers.tokenization_utils_fast.out_str->json.dumps(added_vocab, ensure_ascii=False)
A:transformers.tokenization_utils_fast.vocab_files->self.save_vocabulary(save_directory, filename_prefix=filename_prefix)
A:transformers.tokenization_utils_fast.tokenizer_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + TOKENIZER_FILE)
transformers.PreTrainedTokenizerFast(self,*args,**kwargs)
transformers.PreTrainedTokenizerFast.__len__(self)->int
transformers.PreTrainedTokenizerFast._add_tokens(self,new_tokens:List[Union[str,AddedToken]],special_tokens=False)->int
transformers.PreTrainedTokenizerFast._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.PreTrainedTokenizerFast._convert_encoding(self,encoding:EncodingFast,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True)->Tuple[Dict[str, Any], List[EncodingFast]]
transformers.PreTrainedTokenizerFast._convert_id_to_token(self,index:int)->Optional[str]
transformers.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self,token:str)->int
transformers.PreTrainedTokenizerFast._decode(self,token_ids:Union[int,List[int]],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.PreTrainedTokenizerFast._encode_plus(self,text:Union[TextInput,PreTokenizedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[bool]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerFast._save_pretrained(self,save_directory:Union[str,os.PathLike],file_names:Tuple[str],legacy_format:Optional[bool]=None,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.PreTrainedTokenizerFast.backend_tokenizer(self)->TokenizerFast
transformers.PreTrainedTokenizerFast.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.PreTrainedTokenizerFast.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.PreTrainedTokenizerFast.convert_tokens_to_string(self,tokens:List[str])->str
transformers.PreTrainedTokenizerFast.decoder(self)->DecoderFast
transformers.PreTrainedTokenizerFast.get_added_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizerFast.get_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizerFast.is_fast(self)->bool
transformers.PreTrainedTokenizerFast.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.PreTrainedTokenizerFast.set_truncation_and_padding(self,padding_strategy:PaddingStrategy,truncation_strategy:TruncationStrategy,max_length:int,stride:int,pad_to_multiple_of:Optional[int])
transformers.PreTrainedTokenizerFast.tokenize(self,text:str,pair:Optional[str]=None,add_special_tokens:bool=False,**kwargs)->List[str]
transformers.PreTrainedTokenizerFast.vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizerFast.vocab_size(self)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast(self,*args,**kwargs)
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.__init__(self,*args,**kwargs)
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.__len__(self)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._add_tokens(self,new_tokens:List[Union[str,AddedToken]],special_tokens=False)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._convert_encoding(self,encoding:EncodingFast,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True)->Tuple[Dict[str, Any], List[EncodingFast]]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._convert_id_to_token(self,index:int)->Optional[str]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self,token:str)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._decode(self,token_ids:Union[int,List[int]],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._encode_plus(self,text:Union[TextInput,PreTokenizedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_split_into_words:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[bool]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._save_pretrained(self,save_directory:Union[str,os.PathLike],file_names:Tuple[str],legacy_format:Optional[bool]=None,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.backend_tokenizer(self)->TokenizerFast
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.convert_tokens_to_string(self,tokens:List[str])->str
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.decoder(self)->DecoderFast
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.get_added_vocab(self)->Dict[str, int]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.get_vocab(self)->Dict[str, int]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.is_fast(self)->bool
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.set_truncation_and_padding(self,padding_strategy:PaddingStrategy,truncation_strategy:TruncationStrategy,max_length:int,stride:int,pad_to_multiple_of:Optional[int])
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.tokenize(self,text:str,pair:Optional[str]=None,add_special_tokens:bool=False,**kwargs)->List[str]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.vocab(self)->Dict[str, int]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.vocab_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/convert_slow_tokenizer.py----------------------------------------
A:transformers.convert_slow_tokenizer.self.sp->SentencePieceProcessor()
A:transformers.convert_slow_tokenizer.piece_id->self.vocab(proto).get(merge, None)
A:transformers.convert_slow_tokenizer.merges->list(self.original_tokenizer.bpe_ranks.keys())
A:transformers.convert_slow_tokenizer.tokenizer->Tokenizer(BPE(vocab=vocab, merges=merges, dropout=None, continuing_subword_prefix='', end_of_word_suffix='</w>', fuse_unk=False))
A:transformers.convert_slow_tokenizer.tokenizer.normalizer->self.normalizer(self.proto)
A:transformers.convert_slow_tokenizer.tokenizer.pre_tokenizer->tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=self.original_tokenizer.add_prefix_space)
A:transformers.convert_slow_tokenizer.cls->str(self.original_tokenizer.cls_token)
A:transformers.convert_slow_tokenizer.sep->str(self.original_tokenizer.sep_token)
A:transformers.convert_slow_tokenizer.tokenizer.post_processor->tokenizers.processors.ByteLevel(trim_offsets=False)
A:transformers.convert_slow_tokenizer.tokenizer.decoder->tokenizers.decoders.ByteLevel()
A:transformers.convert_slow_tokenizer.m->utils.sentencepiece_model_pb2.ModelProto()
A:transformers.convert_slow_tokenizer.vocab->self.vocab(proto)
A:transformers.convert_slow_tokenizer.unk_id->self.unk_id(proto)
A:transformers.convert_slow_tokenizer.(_, merges)->SentencePieceExtractor(self.original_tokenizer.vocab_file).extract()
A:transformers.convert_slow_tokenizer.post_processor->self.post_processor()
transformers.convert_slow_tokenizer(transformer_tokenizer)->Tokenizer
transformers.convert_slow_tokenizer.AlbertConverter(SpmConverter)
transformers.convert_slow_tokenizer.AlbertConverter.normalizer(self,proto)
transformers.convert_slow_tokenizer.AlbertConverter.post_processor(self)
transformers.convert_slow_tokenizer.AlbertConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.BarthezConverter(SpmConverter)
transformers.convert_slow_tokenizer.BarthezConverter.post_processor(self)
transformers.convert_slow_tokenizer.BarthezConverter.unk_id(self,proto)
transformers.convert_slow_tokenizer.BertConverter(Converter)
transformers.convert_slow_tokenizer.BertConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.BertGenerationConverter(SpmConverter)
transformers.convert_slow_tokenizer.BigBirdConverter(SpmConverter)
transformers.convert_slow_tokenizer.BigBirdConverter.post_processor(self)
transformers.convert_slow_tokenizer.CLIPConverter(Converter)
transformers.convert_slow_tokenizer.CLIPConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.CamembertConverter(SpmConverter)
transformers.convert_slow_tokenizer.CamembertConverter.post_processor(self)
transformers.convert_slow_tokenizer.CamembertConverter.unk_id(self,proto)
transformers.convert_slow_tokenizer.CamembertConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.Converter(self,original_tokenizer)
transformers.convert_slow_tokenizer.Converter.__init__(self,original_tokenizer)
transformers.convert_slow_tokenizer.Converter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.DebertaConverter(Converter)
transformers.convert_slow_tokenizer.DebertaConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.FunnelConverter(Converter)
transformers.convert_slow_tokenizer.FunnelConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.GPT2Converter(Converter)
transformers.convert_slow_tokenizer.GPT2Converter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.HerbertConverter(Converter)
transformers.convert_slow_tokenizer.HerbertConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.MBart50Converter(SpmConverter)
transformers.convert_slow_tokenizer.MBart50Converter.post_processor(self)
transformers.convert_slow_tokenizer.MBart50Converter.unk_id(self,proto)
transformers.convert_slow_tokenizer.MBart50Converter.vocab(self,proto)
transformers.convert_slow_tokenizer.MBartConverter(SpmConverter)
transformers.convert_slow_tokenizer.MBartConverter.post_processor(self)
transformers.convert_slow_tokenizer.MBartConverter.unk_id(self,proto)
transformers.convert_slow_tokenizer.MBartConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.MPNetConverter(Converter)
transformers.convert_slow_tokenizer.MPNetConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.OpenAIGPTConverter(Converter)
transformers.convert_slow_tokenizer.OpenAIGPTConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.PegasusConverter(SpmConverter)
transformers.convert_slow_tokenizer.PegasusConverter.post_processor(self)
transformers.convert_slow_tokenizer.PegasusConverter.pre_tokenizer(self,replacement,add_prefix_space)
transformers.convert_slow_tokenizer.PegasusConverter.unk_id(self,proto)
transformers.convert_slow_tokenizer.PegasusConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.ReformerConverter(SpmConverter)
transformers.convert_slow_tokenizer.RoFormerConverter(Converter)
transformers.convert_slow_tokenizer.RoFormerConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.RobertaConverter(Converter)
transformers.convert_slow_tokenizer.RobertaConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.SentencePieceExtractor(self,model:str)
transformers.convert_slow_tokenizer.SentencePieceExtractor.__init__(self,model:str)
transformers.convert_slow_tokenizer.SentencePieceExtractor.extract(self)->Tuple[Dict[str, int], List[Tuple]]
transformers.convert_slow_tokenizer.SpmConverter(self,*args)
transformers.convert_slow_tokenizer.SpmConverter.__init__(self,*args)
transformers.convert_slow_tokenizer.SpmConverter.converted(self)->Tokenizer
transformers.convert_slow_tokenizer.SpmConverter.normalizer(self,proto)
transformers.convert_slow_tokenizer.SpmConverter.post_processor(self)
transformers.convert_slow_tokenizer.SpmConverter.pre_tokenizer(self,replacement,add_prefix_space)
transformers.convert_slow_tokenizer.SpmConverter.tokenizer(self,proto)
transformers.convert_slow_tokenizer.SpmConverter.unk_id(self,proto)
transformers.convert_slow_tokenizer.SpmConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.T5Converter(SpmConverter)
transformers.convert_slow_tokenizer.T5Converter.post_processor(self)
transformers.convert_slow_tokenizer.T5Converter.vocab(self,proto)
transformers.convert_slow_tokenizer.XLMRobertaConverter(SpmConverter)
transformers.convert_slow_tokenizer.XLMRobertaConverter.post_processor(self)
transformers.convert_slow_tokenizer.XLMRobertaConverter.unk_id(self,proto)
transformers.convert_slow_tokenizer.XLMRobertaConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.XLNetConverter(SpmConverter)
transformers.convert_slow_tokenizer.XLNetConverter.normalizer(self,proto)
transformers.convert_slow_tokenizer.XLNetConverter.post_processor(self)
transformers.convert_slow_tokenizer.XLNetConverter.vocab(self,proto)
transformers.convert_slow_tokenizer.check_number_comma(piece:str)->bool
transformers.convert_slow_tokenizer.convert_slow_tokenizer(transformer_tokenizer)->Tokenizer


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/__init__.py----------------------------------------
A:transformers.__init__.logger->utils.logging.get_logger(__name__)
A:transformers.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/convert_pytorch_checkpoint_to_tf2.py----------------------------------------
A:transformers.convert_pytorch_checkpoint_to_tf2.config_file->cached_path(config_shortcut_name, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.config->config_class.from_json_file(config_file)
A:transformers.convert_pytorch_checkpoint_to_tf2.tf_model->load_pytorch_checkpoint_in_tf2_model(tf_model, pytorch_checkpoint_path)
A:transformers.convert_pytorch_checkpoint_to_tf2.pytorch_checkpoint_url->hf_bucket_url(pytorch_checkpoint_path, filename=WEIGHTS_NAME)
A:transformers.convert_pytorch_checkpoint_to_tf2.pytorch_checkpoint_path->cached_path(pytorch_checkpoint_url, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.tfo->tf_model(tf_model.dummy_inputs, training=False)
A:transformers.convert_pytorch_checkpoint_to_tf2.state_dict->torch.load(pytorch_checkpoint_path, map_location='cpu')
A:transformers.convert_pytorch_checkpoint_to_tf2.pt_model->pt_model_class.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=state_dict)
A:transformers.convert_pytorch_checkpoint_to_tf2.pto->pt_model(**pt_model.dummy_inputs)
A:transformers.convert_pytorch_checkpoint_to_tf2.np_pt->pto[0].numpy()
A:transformers.convert_pytorch_checkpoint_to_tf2.np_tf->tfo[0].numpy()
A:transformers.convert_pytorch_checkpoint_to_tf2.diff->numpy.amax(np.abs(np_pt - np_tf))
A:transformers.convert_pytorch_checkpoint_to_tf2.model_types->list(MODEL_CLASSES.keys())
A:transformers.convert_pytorch_checkpoint_to_tf2.model_shortcut_names_or_path->list(aws_model_maps.keys())
A:transformers.convert_pytorch_checkpoint_to_tf2.model_file->cached_path(model_shortcut_name, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.parser->argparse.ArgumentParser()
A:transformers.convert_pytorch_checkpoint_to_tf2.args->argparse.ArgumentParser().parse_args()
transformers.convert_pytorch_checkpoint_to_tf2.convert_all_pt_checkpoints_to_tf(args_model_type,tf_dump_path,model_shortcut_names_or_path=None,config_shortcut_names_or_path=None,compare_with_pt_model=False,use_cached_models=False,remove_cached_files=False,only_convert_finetuned_models=False)
transformers.convert_pytorch_checkpoint_to_tf2.convert_pt_checkpoint_to_tf(model_type,pytorch_checkpoint_path,config_file,tf_dump_path,compare_with_pt_model=False,use_cached_models=True)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/feature_extraction_utils.py----------------------------------------
A:transformers.feature_extraction_utils.logger->utils.logging.get_logger(__name__)
A:transformers.feature_extraction_utils.tensor_type->TensorType(tensor_type)
A:transformers.feature_extraction_utils.tensor->as_tensor(value)
A:transformers.feature_extraction_utils.(feature_extractor_dict, kwargs)->cls.get_feature_extractor_dict(pretrained_model_name_or_path, **kwargs)
A:transformers.feature_extraction_utils.output_feature_extractor_file->os.path.join(save_directory, FEATURE_EXTRACTOR_NAME)
A:transformers.feature_extraction_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.feature_extraction_utils.force_download->kwargs.pop('force_download', False)
A:transformers.feature_extraction_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.feature_extraction_utils.proxies->kwargs.pop('proxies', None)
A:transformers.feature_extraction_utils.use_auth_token->kwargs.pop('use_auth_token', None)
A:transformers.feature_extraction_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.feature_extraction_utils.revision->kwargs.pop('revision', None)
A:transformers.feature_extraction_utils.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.feature_extraction_utils.from_auto_class->kwargs.pop('_from_auto', False)
A:transformers.feature_extraction_utils.pretrained_model_name_or_path->str(pretrained_model_name_or_path)
A:transformers.feature_extraction_utils.feature_extractor_file->hf_bucket_url(pretrained_model_name_or_path, filename=FEATURE_EXTRACTOR_NAME, revision=revision, mirror=None)
A:transformers.feature_extraction_utils.resolved_feature_extractor_file->cached_path(feature_extractor_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent)
A:transformers.feature_extraction_utils.text->reader.read()
A:transformers.feature_extraction_utils.feature_extractor_dict->json.loads(text)
A:transformers.feature_extraction_utils.return_unused_kwargs->kwargs.pop('return_unused_kwargs', False)
A:transformers.feature_extraction_utils.feature_extractor->cls(**feature_extractor_dict)
A:transformers.feature_extraction_utils.output->copy.deepcopy(self.__dict__)
transformers.BatchFeature(self,data:Optional[Dict[str,Any]]=None,tensor_type:Union[None,str,TensorType]=None)
transformers.BatchFeature.__getattr__(self,item:str)
transformers.BatchFeature.__getitem__(self,item:str)->Union[Any]
transformers.BatchFeature.__getstate__(self)
transformers.BatchFeature.__setstate__(self,state)
transformers.BatchFeature.convert_to_tensors(self,tensor_type:Optional[Union[str,TensorType]]=None)
transformers.BatchFeature.items(self)
transformers.BatchFeature.keys(self)
transformers.BatchFeature.to(self,device:Union[str,'torch.device'])->'BatchFeature'
transformers.BatchFeature.values(self)
transformers.feature_extraction_utils.BatchFeature(self,data:Optional[Dict[str,Any]]=None,tensor_type:Union[None,str,TensorType]=None)
transformers.feature_extraction_utils.BatchFeature.__getattr__(self,item:str)
transformers.feature_extraction_utils.BatchFeature.__getitem__(self,item:str)->Union[Any]
transformers.feature_extraction_utils.BatchFeature.__getstate__(self)
transformers.feature_extraction_utils.BatchFeature.__init__(self,data:Optional[Dict[str,Any]]=None,tensor_type:Union[None,str,TensorType]=None)
transformers.feature_extraction_utils.BatchFeature.__setstate__(self,state)
transformers.feature_extraction_utils.BatchFeature.convert_to_tensors(self,tensor_type:Optional[Union[str,TensorType]]=None)
transformers.feature_extraction_utils.BatchFeature.items(self)
transformers.feature_extraction_utils.BatchFeature.keys(self)
transformers.feature_extraction_utils.BatchFeature.to(self,device:Union[str,'torch.device'])->'BatchFeature'
transformers.feature_extraction_utils.BatchFeature.values(self)
transformers.feature_extraction_utils.FeatureExtractionMixin(self,**kwargs)
transformers.feature_extraction_utils.FeatureExtractionMixin.__init__(self,**kwargs)
transformers.feature_extraction_utils.FeatureExtractionMixin.__repr__(self)
transformers.feature_extraction_utils.FeatureExtractionMixin.from_dict(cls,feature_extractor_dict:Dict[str,Any],**kwargs)->PreTrainedFeatureExtractor
transformers.feature_extraction_utils.FeatureExtractionMixin.from_json_file(cls,json_file:Union[str,os.PathLike])->PreTrainedFeatureExtractor
transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],**kwargs)->PreTrainedFeatureExtractor
transformers.feature_extraction_utils.FeatureExtractionMixin.get_feature_extractor_dict(cls,pretrained_model_name_or_path:Union[str,os.PathLike],**kwargs)->Tuple[Dict[str, Any], Dict[str, Any]]
transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained(self,save_directory:Union[str,os.PathLike])
transformers.feature_extraction_utils.FeatureExtractionMixin.to_dict(self)->Dict[str, Any]
transformers.feature_extraction_utils.FeatureExtractionMixin.to_json_file(self,json_file_path:Union[str,os.PathLike])
transformers.feature_extraction_utils.FeatureExtractionMixin.to_json_string(self)->str


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/training_args_tf.py----------------------------------------
A:transformers.training_args_tf.logger->utils.logging.get_logger(__name__)
A:transformers.training_args_tf.gpus->tensorflow.config.list_physical_devices('GPU')
A:transformers.training_args_tf.policy->tensorflow.keras.mixed_precision.experimental.Policy('mixed_bfloat16')
A:transformers.training_args_tf.strategy->tensorflow.distribute.MirroredStrategy()
A:transformers.training_args_tf.tpu->tensorflow.distribute.cluster_resolver.TPUClusterResolver()
transformers.TFTrainingArguments(TrainingArguments)
transformers.TFTrainingArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', int]
transformers.TFTrainingArguments.eval_batch_size(self)->int
transformers.TFTrainingArguments.n_gpu(self)->int
transformers.TFTrainingArguments.n_replicas(self)->int
transformers.TFTrainingArguments.strategy(self)->'tf.distribute.Strategy'
transformers.TFTrainingArguments.train_batch_size(self)->int
transformers.training_args_tf.TFTrainingArguments(TrainingArguments)
transformers.training_args_tf.TFTrainingArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', int]
transformers.training_args_tf.TFTrainingArguments.eval_batch_size(self)->int
transformers.training_args_tf.TFTrainingArguments.n_gpu(self)->int
transformers.training_args_tf.TFTrainingArguments.n_replicas(self)->int
transformers.training_args_tf.TFTrainingArguments.strategy(self)->'tf.distribute.Strategy'
transformers.training_args_tf.TFTrainingArguments.train_batch_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/trainer_tf.py----------------------------------------
A:transformers.trainer_tf.logger->utils.logging.get_logger(__name__)
A:transformers.trainer_tf.self.gradient_accumulator->GradientAccumulator()
A:transformers.trainer_tf.self.eval_loss->tensorflow.keras.metrics.Sum()
A:transformers.trainer_tf.self.tb_writer->tensorflow.summary.create_file_writer(self.args.logging_dir)
A:transformers.trainer_tf.self.num_train_examples->self.train_dataset.cardinality().numpy()
A:transformers.trainer_tf.ds->test_dataset.batch(self.args.eval_batch_size).prefetch(tf.data.experimental.AUTOTUNE)
A:transformers.trainer_tf.num_examples->test_dataset.cardinality().numpy()
A:transformers.trainer_tf.steps->math.ceil(num_examples / self.args.eval_batch_size)
A:transformers.trainer_tf.(self.optimizer, self.lr_scheduler)->create_optimizer(self.args.learning_rate, num_training_steps, warmup_steps, adam_beta1=self.args.adam_beta1, adam_beta2=self.args.adam_beta2, adam_epsilon=self.args.adam_epsilon, weight_decay_rate=self.args.weight_decay, power=self.args.poly_power)
A:transformers.trainer_tf.comet_mode->os.getenv('COMET_MODE', 'ONLINE').upper()
A:transformers.trainer_tf.experiment->comet_ml.config.get_global_experiment()
A:transformers.trainer_tf.args['offline_directory']->os.getenv('COMET_OFFLINE_DIRECTORY', './')
A:transformers.trainer_tf.logits->self.args.strategy.run(self.prediction_step, inputs)
A:transformers.trainer_tf.preds->numpy.append(preds, logits.numpy(), axis=0)
A:transformers.trainer_tf.label_ids->numpy.append(label_ids, labels.numpy(), axis=0)
A:transformers.trainer_tf.metrics->self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))
A:transformers.trainer_tf.metrics[f'eval_{key}']->self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids)).pop(key)
A:transformers.trainer_tf.(eval_ds, steps, num_examples)->self.get_eval_tfdataset(eval_dataset)
A:transformers.trainer_tf.output->self.prediction_loop(eval_ds, steps, num_examples, description='Evaluation')
A:transformers.trainer_tf.(per_example_loss, logits)->self.run_model(features, labels, False)
A:transformers.trainer_tf.nb_instances_in_batch->self._compute_nb_instances(batch)
A:transformers.trainer_tf.inputs->self._get_step_inputs(batch, nb_instances_in_batch)
A:transformers.trainer_tf.train_ds->self.get_train_tfdataset()
A:transformers.trainer_tf.num_update_steps_per_epoch->max(num_update_steps_per_epoch, 1)
A:transformers.trainer_tf.epochs->float(epochs)
A:transformers.trainer_tf.folder->os.path.join(self.args.output_dir, PREFIX_CHECKPOINT_DIR)
A:transformers.trainer_tf.ckpt->tensorflow.train.Checkpoint(optimizer=self.optimizer, model=self.model)
A:transformers.trainer_tf.self.model.ckpt_manager->tensorflow.train.CheckpointManager(ckpt, folder, max_to_keep=self.args.save_total_limit)
A:transformers.trainer_tf.self.global_step->iterations.numpy()
A:transformers.trainer_tf.self.train_loss->tensorflow.keras.metrics.Sum()
A:transformers.trainer_tf.start_time->datetime.datetime.now()
A:transformers.trainer_tf.logs['loss']->training_loss.numpy()
A:transformers.trainer_tf.logs['learning_rate']->self.lr_scheduler(self.global_step).numpy()
A:transformers.trainer_tf.ckpt_save_path->self.model.ckpt_manager.save()
A:transformers.trainer_tf.end_time->datetime.datetime.now()
A:transformers.trainer_tf.(per_example_loss, _)->self.run_model(features, labels, True)
A:transformers.trainer_tf.gradients->self.training_step(features, labels, nb_instances_in_global_batch)
A:transformers.trainer_tf.labels->tensorflow.concat(labels.values, axis=0)
A:transformers.trainer_tf.nb_instances->PerReplica([nb_instances] * len(labels.values))
A:transformers.trainer_tf.(test_ds, steps, num_examples)->self.get_test_tfdataset(test_dataset)
transformers.TFTrainer(self,model:TFPreTrainedModel,args:TFTrainingArguments,train_dataset:Optional[tf.data.Dataset]=None,eval_dataset:Optional[tf.data.Dataset]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional[tf.summary.SummaryWriter]=None,optimizers:Tuple[tf.keras.optimizers.Optimizer,tf.keras.optimizers.schedules.LearningRateSchedule]=(None,None))
transformers.TFTrainer._compute_nb_instances(batch)
transformers.TFTrainer._get_step_inputs(batch,nb_instances)
transformers.TFTrainer.apply_gradients(self,features,labels,nb_instances_in_global_batch)
transformers.TFTrainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.TFTrainer.distributed_prediction_steps(self,batch)
transformers.TFTrainer.distributed_training_steps(self,batch)
transformers.TFTrainer.evaluate(self,eval_dataset:Optional[tf.data.Dataset]=None)->Dict[str, float]
transformers.TFTrainer.get_eval_tfdataset(self,eval_dataset:Optional[tf.data.Dataset]=None)->tf.data.Dataset
transformers.TFTrainer.get_test_tfdataset(self,test_dataset:tf.data.Dataset)->tf.data.Dataset
transformers.TFTrainer.get_train_tfdataset(self)->tf.data.Dataset
transformers.TFTrainer.log(self,logs:Dict[str,float])->None
transformers.TFTrainer.predict(self,test_dataset:tf.data.Dataset)->PredictionOutput
transformers.TFTrainer.prediction_loop(self,dataset:tf.data.Dataset,steps:int,num_examples:int,description:str,prediction_loss_only:Optional[bool]=None)->PredictionOutput
transformers.TFTrainer.prediction_step(self,features:tf.Tensor,labels:tf.Tensor,nb_instances_in_global_batch:tf.Tensor)->tf.Tensor
transformers.TFTrainer.run_model(self,features,labels,training)
transformers.TFTrainer.save_model(self,output_dir:Optional[str]=None)
transformers.TFTrainer.setup_comet(self)
transformers.TFTrainer.setup_wandb(self)
transformers.TFTrainer.train(self)->None
transformers.TFTrainer.training_step(self,features,labels,nb_instances_in_global_batch)
transformers.trainer_tf.TFTrainer(self,model:TFPreTrainedModel,args:TFTrainingArguments,train_dataset:Optional[tf.data.Dataset]=None,eval_dataset:Optional[tf.data.Dataset]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional[tf.summary.SummaryWriter]=None,optimizers:Tuple[tf.keras.optimizers.Optimizer,tf.keras.optimizers.schedules.LearningRateSchedule]=(None,None))
transformers.trainer_tf.TFTrainer.__init__(self,model:TFPreTrainedModel,args:TFTrainingArguments,train_dataset:Optional[tf.data.Dataset]=None,eval_dataset:Optional[tf.data.Dataset]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional[tf.summary.SummaryWriter]=None,optimizers:Tuple[tf.keras.optimizers.Optimizer,tf.keras.optimizers.schedules.LearningRateSchedule]=(None,None))
transformers.trainer_tf.TFTrainer._compute_nb_instances(batch)
transformers.trainer_tf.TFTrainer._get_step_inputs(batch,nb_instances)
transformers.trainer_tf.TFTrainer.apply_gradients(self,features,labels,nb_instances_in_global_batch)
transformers.trainer_tf.TFTrainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.trainer_tf.TFTrainer.distributed_prediction_steps(self,batch)
transformers.trainer_tf.TFTrainer.distributed_training_steps(self,batch)
transformers.trainer_tf.TFTrainer.evaluate(self,eval_dataset:Optional[tf.data.Dataset]=None)->Dict[str, float]
transformers.trainer_tf.TFTrainer.get_eval_tfdataset(self,eval_dataset:Optional[tf.data.Dataset]=None)->tf.data.Dataset
transformers.trainer_tf.TFTrainer.get_test_tfdataset(self,test_dataset:tf.data.Dataset)->tf.data.Dataset
transformers.trainer_tf.TFTrainer.get_train_tfdataset(self)->tf.data.Dataset
transformers.trainer_tf.TFTrainer.log(self,logs:Dict[str,float])->None
transformers.trainer_tf.TFTrainer.predict(self,test_dataset:tf.data.Dataset)->PredictionOutput
transformers.trainer_tf.TFTrainer.prediction_loop(self,dataset:tf.data.Dataset,steps:int,num_examples:int,description:str,prediction_loss_only:Optional[bool]=None)->PredictionOutput
transformers.trainer_tf.TFTrainer.prediction_step(self,features:tf.Tensor,labels:tf.Tensor,nb_instances_in_global_batch:tf.Tensor)->tf.Tensor
transformers.trainer_tf.TFTrainer.run_model(self,features,labels,training)
transformers.trainer_tf.TFTrainer.save_model(self,output_dir:Optional[str]=None)
transformers.trainer_tf.TFTrainer.setup_comet(self)
transformers.trainer_tf.TFTrainer.setup_wandb(self)
transformers.trainer_tf.TFTrainer.train(self)->None
transformers.trainer_tf.TFTrainer.training_step(self,features,labels,nb_instances_in_global_batch)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/dependency_versions_table.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/trainer_pt_utils.py----------------------------------------
A:transformers.trainer_pt_utils.logger->utils.logging.get_logger(__name__)
A:transformers.trainer_pt_utils.result->numpy.full_like(arrays, padding_index, shape=(arrays.shape[0], new_seq_length) + arrays.shape[2:])
A:transformers.trainer_pt_utils.concat->torch.cat(output_tensors, dim=0)
A:transformers.trainer_pt_utils.tensorized_scalar->torch.tensor(scalars).cuda()
A:transformers.trainer_pt_utils.indices->list(range(len(self.dataset)))
A:transformers.trainer_pt_utils.num_replicas->torch.distributed.get_world_size()
A:transformers.trainer_pt_utils.rank->torch.distributed.get_rank()
A:transformers.trainer_pt_utils.num_samples->len(self.dataset)
A:transformers.trainer_pt_utils.self.num_samples->math.ceil(len(self.dataset) / self.num_replicas)
A:transformers.trainer_pt_utils.self._storage->nested_new_like(arrays, self.total_samples, padding_index=self.padding_index)
A:transformers.trainer_pt_utils.self._offsets->list(range(0, self.total_samples, self.process_length))
A:transformers.trainer_pt_utils.(slice_len, self._storage)->self._nested_set_tensors(self._storage, arrays)
A:transformers.trainer_pt_utils.storage->expand_like(storage, arrays.shape[1], padding_index=self.padding_index)
A:transformers.trainer_pt_utils.labels->labels.unsqueeze(-1).unsqueeze(-1)
A:transformers.trainer_pt_utils.padding_mask->labels.unsqueeze(-1).unsqueeze(-1).eq(self.ignore_index)
A:transformers.trainer_pt_utils.nll_loss->log_probs.gather(dim=-1, index=labels)
A:transformers.trainer_pt_utils.smoothed_loss->log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)
A:transformers.trainer_pt_utils.mega_batch_mult->min(len(lengths) // (batch_size * 4), 50)
A:transformers.trainer_pt_utils.max_idx->torch.argmax(torch.tensor(megabatch_maximums)).item()
A:transformers.trainer_pt_utils.g->torch.Generator()
A:transformers.trainer_pt_utils.process_slice->range(self.process_index * self.batch_size, (self.process_index + 1) * self.batch_size)
A:transformers.trainer_pt_utils.first_batch->current_batch.copy()
A:transformers.trainer_pt_utils.msec->int(abs(secs - int(secs)) * 100)
A:transformers.trainer_pt_utils.metrics_copy->metrics.copy()
A:transformers.trainer_pt_utils.metrics_copy[k]->round(v, 4)
A:transformers.trainer_pt_utils.metrics_formatted->self.metrics_format(metrics)
A:transformers.trainer_pt_utils.k_width->max((len(str(x)) for x in metrics_formatted.keys()))
A:transformers.trainer_pt_utils.v_width->max((len(str(x)) for x in metrics_formatted.values()))
A:transformers.trainer_pt_utils.path->os.path.join(self.args.output_dir, 'trainer_state.json')
A:transformers.trainer_pt_utils.all_metrics->json.load(f)
A:transformers.trainer_pt_utils.outputs->model(**inputs)
A:transformers.trainer_pt_utils.loss->scaler.scale(loss).squeeze()
A:transformers.trainer_pt_utils.all_tensors->smdistributed.modelparallel.torch.allgather(tensor, smp.CommGroup.DP_GROUP)
transformers.torch_distributed_zero_first(local_rank:int)
transformers.trainer_pt_utils.DistributedLengthGroupedSampler(self,dataset:Dataset,batch_size:int,num_replicas:Optional[int]=None,rank:Optional[int]=None,seed:int=0,drop_last:bool=False,lengths:Optional[List[int]]=None,model_input_name:Optional[str]=None)
transformers.trainer_pt_utils.DistributedLengthGroupedSampler.__init__(self,dataset:Dataset,batch_size:int,num_replicas:Optional[int]=None,rank:Optional[int]=None,seed:int=0,drop_last:bool=False,lengths:Optional[List[int]]=None,model_input_name:Optional[str]=None)
transformers.trainer_pt_utils.DistributedLengthGroupedSampler.__iter__(self)->Iterator
transformers.trainer_pt_utils.DistributedSamplerWithLoop(self,dataset,batch_size,**kwargs)
transformers.trainer_pt_utils.DistributedSamplerWithLoop.__init__(self,dataset,batch_size,**kwargs)
transformers.trainer_pt_utils.DistributedSamplerWithLoop.__iter__(self)
transformers.trainer_pt_utils.DistributedTensorGatherer(self,world_size,num_samples,make_multiple_of=None,padding_index=-100)
transformers.trainer_pt_utils.DistributedTensorGatherer.__init__(self,world_size,num_samples,make_multiple_of=None,padding_index=-100)
transformers.trainer_pt_utils.DistributedTensorGatherer._nested_set_tensors(self,storage,arrays)
transformers.trainer_pt_utils.DistributedTensorGatherer.add_arrays(self,arrays)
transformers.trainer_pt_utils.DistributedTensorGatherer.finalize(self)
transformers.trainer_pt_utils.IterableDatasetShard(self,dataset:IterableDataset,batch_size:int=1,drop_last:bool=False,num_processes:int=1,process_index:int=0,seed:int=0)
transformers.trainer_pt_utils.IterableDatasetShard.__init__(self,dataset:IterableDataset,batch_size:int=1,drop_last:bool=False,num_processes:int=1,process_index:int=0,seed:int=0)
transformers.trainer_pt_utils.IterableDatasetShard.__iter__(self)
transformers.trainer_pt_utils.IterableDatasetShard.set_epoch(self,epoch)
transformers.trainer_pt_utils.LabelSmoother(self,model_output,labels)
transformers.trainer_pt_utils.LabelSmoother.__call__(self,model_output,labels)
transformers.trainer_pt_utils.LengthGroupedSampler(self,dataset:Dataset,batch_size:int,lengths:Optional[List[int]]=None,model_input_name:Optional[str]=None,generator=None)
transformers.trainer_pt_utils.LengthGroupedSampler.__init__(self,dataset:Dataset,batch_size:int,lengths:Optional[List[int]]=None,model_input_name:Optional[str]=None,generator=None)
transformers.trainer_pt_utils.LengthGroupedSampler.__iter__(self)
transformers.trainer_pt_utils.LengthGroupedSampler.__len__(self)
transformers.trainer_pt_utils.SequentialDistributedSampler(self,dataset,num_replicas=None,rank=None,batch_size=None)
transformers.trainer_pt_utils.SequentialDistributedSampler.__init__(self,dataset,num_replicas=None,rank=None,batch_size=None)
transformers.trainer_pt_utils.SequentialDistributedSampler.__iter__(self)
transformers.trainer_pt_utils.SequentialDistributedSampler.__len__(self)
transformers.trainer_pt_utils.ShardSampler(self,dataset:Dataset,batch_size:int=1,drop_last:bool=False,num_processes:int=1,process_index:int=0)
transformers.trainer_pt_utils.ShardSampler.__init__(self,dataset:Dataset,batch_size:int=1,drop_last:bool=False,num_processes:int=1,process_index:int=0)
transformers.trainer_pt_utils.ShardSampler.__iter__(self)
transformers.trainer_pt_utils.ShardSampler.__len__(self)
transformers.trainer_pt_utils._get_learning_rate(self)
transformers.trainer_pt_utils._secs2timedelta(secs)
transformers.trainer_pt_utils.distributed_broadcast_scalars(scalars:List[Union[int,float]],num_total_examples:Optional[int]=None)->torch.Tensor
transformers.trainer_pt_utils.distributed_concat(tensor:'torch.Tensor',num_total_examples:Optional[int]=None)->torch.Tensor
transformers.trainer_pt_utils.expand_like(arrays,new_seq_length,padding_index=-100)
transformers.trainer_pt_utils.find_batch_size(tensors)
transformers.trainer_pt_utils.get_length_grouped_indices(lengths,batch_size,mega_batch_mult=None,generator=None)
transformers.trainer_pt_utils.get_parameter_names(model,forbidden_layer_types)
transformers.trainer_pt_utils.get_tpu_sampler(dataset:torch.utils.data.dataset.Dataset,bach_size:int)
transformers.trainer_pt_utils.log_metrics(self,split,metrics)
transformers.trainer_pt_utils.metrics_format(self,metrics:Dict[str,float])->Dict[str, float]
transformers.trainer_pt_utils.nested_concat(tensors,new_tensors,padding_index=-100)
transformers.trainer_pt_utils.nested_detach(tensors)
transformers.trainer_pt_utils.nested_new_like(arrays,num_samples,padding_index=-100)
transformers.trainer_pt_utils.nested_numpify(tensors)
transformers.trainer_pt_utils.nested_truncate(tensors,limit)
transformers.trainer_pt_utils.nested_xla_mesh_reduce(tensors,name)
transformers.trainer_pt_utils.numpy_pad_and_concatenate(array1,array2,padding_index=-100)
transformers.trainer_pt_utils.reissue_pt_warnings(caught_warnings)
transformers.trainer_pt_utils.save_metrics(self,split,metrics,combined=True)
transformers.trainer_pt_utils.save_state(self)
transformers.trainer_pt_utils.torch_distributed_zero_first(local_rank:int)
transformers.trainer_pt_utils.torch_pad_and_concatenate(tensor1,tensor2,padding_index=-100)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_flax_utils.py----------------------------------------
A:transformers.modeling_flax_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_flax_utils.self.key->PRNGKey(seed)
A:transformers.modeling_flax_utils.random_params->self.init_weights(self.key, input_shape)
A:transformers.modeling_flax_utils.self._required_params->set(flatten_dict(unfreeze(random_params)).keys())
A:transformers.modeling_flax_utils.params->unfreeze(params)
A:transformers.modeling_flax_utils.param_keys->set(flatten_dict(params).keys())
A:transformers.modeling_flax_utils.config->kwargs.pop('config', None)
A:transformers.modeling_flax_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_flax_utils.from_pt->kwargs.pop('from_pt', False)
A:transformers.modeling_flax_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_flax_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.modeling_flax_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_flax_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.modeling_flax_utils.use_auth_token->kwargs.pop('use_auth_token', None)
A:transformers.modeling_flax_utils.revision->kwargs.pop('revision', None)
A:transformers.modeling_flax_utils.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.modeling_flax_utils.from_auto_class->kwargs.pop('_from_auto', False)
A:transformers.modeling_flax_utils.(config, model_kwargs)->cls.config_class.from_pretrained(config_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only, use_auth_token=use_auth_token, revision=revision, _from_auto=from_auto_class, _from_pipeline=from_pipeline, **kwargs)
A:transformers.modeling_flax_utils.archive_file->hf_bucket_url(pretrained_model_name_or_path, filename=WEIGHTS_NAME if from_pt else FLAX_WEIGHTS_NAME, revision=revision)
A:transformers.modeling_flax_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent)
A:transformers.modeling_flax_utils.model->cls(config, *model_args, **model_kwargs)
A:transformers.modeling_flax_utils.state->flatten_dict(state)
A:transformers.modeling_flax_utils.random_state->flatten_dict(unfreeze(model.params))
A:transformers.modeling_flax_utils.model.params->unflatten_dict(state)
A:transformers.modeling_flax_utils.commit_message->kwargs.pop('commit_message', None)
A:transformers.modeling_flax_utils.repo->self._create_or_get_repo(save_directory, **kwargs)
A:transformers.modeling_flax_utils.save_directory->os.path.abspath(save_directory)
A:transformers.modeling_flax_utils.output_model_file->os.path.join(save_directory, FLAX_WEIGHTS_NAME)
A:transformers.modeling_flax_utils.model_bytes->to_bytes(params)
A:transformers.modeling_flax_utils.url->self._push_to_hub(repo, commit_message=commit_message)
A:transformers.modeling_flax_utils.model_class.__call__->replace_return_docstrings(output_type=output_type, config_class=config_class)(model_class.__call__)
transformers.FlaxPreTrainedModel(self,config:PretrainedConfig,module:nn.Module,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32)
transformers.FlaxPreTrainedModel.config(self)->PretrainedConfig
transformers.FlaxPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],dtype:jnp.dtype=jnp.float32,*model_args,**kwargs)
transformers.FlaxPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->Dict
transformers.FlaxPreTrainedModel.module(self)->nn.Module
transformers.FlaxPreTrainedModel.params(self)->Union[Dict, FrozenDict]
transformers.FlaxPreTrainedModel.params(self,params:Union[Dict,FrozenDict])
transformers.FlaxPreTrainedModel.required_params(self)->Set
transformers.FlaxPreTrainedModel.save_pretrained(self,save_directory:Union[str,os.PathLike],params=None,push_to_hub=False,**kwargs)
transformers.modeling_flax_utils.FlaxPreTrainedModel(self,config:PretrainedConfig,module:nn.Module,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32)
transformers.modeling_flax_utils.FlaxPreTrainedModel.__init__(self,config:PretrainedConfig,module:nn.Module,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32)
transformers.modeling_flax_utils.FlaxPreTrainedModel.config(self)->PretrainedConfig
transformers.modeling_flax_utils.FlaxPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path:Union[str,os.PathLike],dtype:jnp.dtype=jnp.float32,*model_args,**kwargs)
transformers.modeling_flax_utils.FlaxPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->Dict
transformers.modeling_flax_utils.FlaxPreTrainedModel.module(self)->nn.Module
transformers.modeling_flax_utils.FlaxPreTrainedModel.params(self)->Union[Dict, FrozenDict]
transformers.modeling_flax_utils.FlaxPreTrainedModel.params(self,params:Union[Dict,FrozenDict])
transformers.modeling_flax_utils.FlaxPreTrainedModel.required_params(self)->Set
transformers.modeling_flax_utils.FlaxPreTrainedModel.save_pretrained(self,save_directory:Union[str,os.PathLike],params=None,push_to_hub=False,**kwargs)
transformers.modeling_flax_utils.append_call_sample_docstring(model_class,tokenizer_class,checkpoint,output_type,config_class,mask=None)
transformers.modeling_flax_utils.append_replace_return_docstrings(model_class,output_type,config_class)
transformers.modeling_flax_utils.overwrite_call_docstring(model_class,docstring)
transformers.modeling_flax_utils.quick_gelu(x)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/generation_stopping_criteria.py----------------------------------------
A:transformers.generation_stopping_criteria.new_stopping_criteria->deepcopy(stopping_criteria)
transformers.MaxLengthCriteria(self,max_length:int)
transformers.MaxTimeCriteria(self,max_time:float,initial_timestamp:Optional[float]=None)
transformers.StoppingCriteria(self,input_ids:torch.LongTensor,score:torch.FloatTensor,**kwargs)
transformers.StoppingCriteriaList(self,input_ids:torch.LongTensor,scores:torch.FloatTensor,**kwargs)
transformers.StoppingCriteriaList.max_length(self)->Optional[int]
transformers.generation_stopping_criteria.MaxLengthCriteria(self,max_length:int)
transformers.generation_stopping_criteria.MaxLengthCriteria.__init__(self,max_length:int)
transformers.generation_stopping_criteria.MaxNewTokensCriteria(self,start_length:int,max_new_tokens:int)
transformers.generation_stopping_criteria.MaxNewTokensCriteria.__init__(self,start_length:int,max_new_tokens:int)
transformers.generation_stopping_criteria.MaxTimeCriteria(self,max_time:float,initial_timestamp:Optional[float]=None)
transformers.generation_stopping_criteria.MaxTimeCriteria.__init__(self,max_time:float,initial_timestamp:Optional[float]=None)
transformers.generation_stopping_criteria.StoppingCriteria(self,input_ids:torch.LongTensor,score:torch.FloatTensor,**kwargs)
transformers.generation_stopping_criteria.StoppingCriteria.__call__(self,input_ids:torch.LongTensor,score:torch.FloatTensor,**kwargs)
transformers.generation_stopping_criteria.StoppingCriteriaList(self,input_ids:torch.LongTensor,scores:torch.FloatTensor,**kwargs)
transformers.generation_stopping_criteria.StoppingCriteriaList.__call__(self,input_ids:torch.LongTensor,scores:torch.FloatTensor,**kwargs)
transformers.generation_stopping_criteria.StoppingCriteriaList.max_length(self)->Optional[int]
transformers.generation_stopping_criteria.validate_stopping_criteria(stopping_criteria:StoppingCriteriaList,max_length:int)->StoppingCriteriaList


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/modeling_tf_utils.py----------------------------------------
A:transformers.modeling_tf_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_utils.tf_logger->tensorflow.get_logger()
A:transformers.modeling_tf_utils.config_class->getattr(cls, 'config_class', None)
A:transformers.modeling_tf_utils.config->kwargs.pop('config', None)
A:transformers.modeling_tf_utils.cfg->super(cls, self).get_config()
A:transformers.modeling_tf_utils.cfg['config']->self._config.to_dict()
A:transformers.modeling_tf_utils.cls->tensorflow.keras.utils.register_keras_serializable()(cls)
A:transformers.modeling_tf_utils.loss_fn->tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
A:transformers.modeling_tf_utils.active_loss->tensorflow.not_equal(tf.reshape(labels, (-1,)), -100)
A:transformers.modeling_tf_utils.reduced_logits->tensorflow.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)
A:transformers.modeling_tf_utils.labels->tensorflow.boolean_mask(tf.reshape(labels, (-1,)), active_loss)
A:transformers.modeling_tf_utils.start_loss->loss_fn(labels['start_position'], logits[0])
A:transformers.modeling_tf_utils.end_loss->loss_fn(labels['end_position'], logits[1])
A:transformers.modeling_tf_utils.next_sentence_active_loss->tensorflow.not_equal(tf.reshape(labels, (-1,)), -100)
A:transformers.modeling_tf_utils.next_sentence_reduced_logits->tensorflow.boolean_mask(tf.reshape(logits, (-1, 2)), next_sentence_active_loss)
A:transformers.modeling_tf_utils.next_sentence_label->tensorflow.boolean_mask(tf.reshape(labels, (-1,)), next_sentence_active_loss)
A:transformers.modeling_tf_utils.signature->dict(inspect.signature(func).parameters)
A:transformers.modeling_tf_utils.parameter_names->list(signature.keys())
A:transformers.modeling_tf_utils.output['input_ids']->input_ids.pop('inputs')
A:transformers.modeling_tf_utils.output['past_key_values']->input_ids.pop('decoder_cached_states')
A:transformers.modeling_tf_utils.output[name]->kwargs.pop(name, signature[name].default)
A:transformers.modeling_tf_utils.saved_h5_model_layers_name->set(hdf5_format.load_attributes_from_hdf5_group(f, 'layer_names'))
A:transformers.modeling_tf_utils.missing_layers->list(set([layer.name for layer in model.layers]) - saved_h5_model_layers_name)
A:transformers.modeling_tf_utils.unexpected_layers->list(saved_h5_model_layers_name - set([layer.name for layer in model.layers]))
A:transformers.modeling_tf_utils.saved_weight_names_set->set()
A:transformers.modeling_tf_utils.symbolic_weights_names->set()
A:transformers.modeling_tf_utils.name->'/'.join(weight_name.split('/')[1:])
A:transformers.modeling_tf_utils.saved_weights[name]->numpy.asarray(h5_layer_object[weight_name])
A:transformers.modeling_tf_utils.delimeter->len(_prefix.split('/'))
A:transformers.modeling_tf_utils.symbolic_weight_name->'/'.join(symbolic_weight.name.split('/')[1:])
A:transformers.modeling_tf_utils.saved_weight_value->saved_weights.get(symbolic_weight_name, None)
A:transformers.modeling_tf_utils.array->numpy.reshape(saved_weight_value, K.int_shape(symbolic_weight))
A:transformers.modeling_tf_utils.(old_num_tokens, old_embedding_dim)->shape_list(old_embeddings)
A:transformers.modeling_tf_utils.current_weights->tensorflow.slice(old_embeddings.value(), tf.convert_to_tensor([0, 0]), tf.convert_to_tensor([new_num_tokens, old_embedding_dim]))
A:transformers.modeling_tf_utils.num_tokens_to_copy->min(old_num_tokens, new_num_tokens)
A:transformers.modeling_tf_utils.mask->tensorflow.fill(tf.convert_to_tensor([new_num_tokens, 1]), True)
A:transformers.modeling_tf_utils.output->self.last_dropout(output, training=training)
A:transformers.modeling_tf_utils.main_layer->getattr(self, self.base_model_prefix)
A:transformers.modeling_tf_utils.lm_head->self.get_lm_head()
A:transformers.modeling_tf_utils.model_embeds->self._resize_token_embeddings(new_num_tokens)
A:transformers.modeling_tf_utils.embeds->getattr(embedding_layer, 'decoder', None)
A:transformers.modeling_tf_utils.old_embeddings->self._get_word_embedding_weight(self.get_input_embeddings())
A:transformers.modeling_tf_utils.new_embeddings->self.add_weight(name=old_embeddings.name.split(':')[0], shape=[new_num_tokens, old_embedding_dim], initializer=get_initializer(init_range), dtype=tf.float32)
A:transformers.modeling_tf_utils.old_lm_head_bias->self.get_bias()
A:transformers.modeling_tf_utils.new_lm_head_bias->self._get_resized_lm_head_bias(old_lm_head_bias, new_num_tokens)
A:transformers.modeling_tf_utils.old_lm_head_decoder->self._get_word_embedding_weight(self.get_output_embeddings())
A:transformers.modeling_tf_utils.new_lm_head_decoder->self.add_weight(shape=(new_num_tokens, old_embedding_dim), initializer='zeros', trainable=True, name=old_lm_head_decoder.name.split(':')[0])
A:transformers.modeling_tf_utils.current_bias->tensorflow.slice(weight.value(), tf.convert_to_tensor(slice_from), tf.convert_to_tensor(final_shape))
A:transformers.modeling_tf_utils.bias_mask->tensorflow.fill(tf.convert_to_tensor(final_shape), True)
A:transformers.modeling_tf_utils.new_bias->self.add_weight(shape=final_shape, initializer='zeros', trainable=True, name=weight.name.split(':')[0])
A:transformers.modeling_tf_utils.init_bias->tensorflow.where(bias_mask, current_bias, new_bias.value())
A:transformers.modeling_tf_utils.is_input_output_equals->tensorflow.reduce_any(self._get_word_embedding_weight(self.get_input_embeddings()) == old_lm_head_decoder)
A:transformers.modeling_tf_utils.(decoder_mask, current_decoder)->init_copy_embeddings(old_lm_head_decoder, new_num_tokens)
A:transformers.modeling_tf_utils.init_decoder->tensorflow.where(decoder_mask, current_decoder, new_lm_head_decoder.value())
A:transformers.modeling_tf_utils.init_range->getattr(self.config, 'initializer_range', 0.02)
A:transformers.modeling_tf_utils.(embeddings_mask, current_embeddings)->init_copy_embeddings(old_embeddings, new_num_tokens)
A:transformers.modeling_tf_utils.init_embeddings->tensorflow.where(embeddings_mask, current_embeddings, new_embeddings.value())
A:transformers.modeling_tf_utils.commit_message->kwargs.pop('commit_message', None)
A:transformers.modeling_tf_utils.repo->self._create_or_get_repo(save_directory, **kwargs)
A:transformers.modeling_tf_utils.saved_model_dir->os.path.join(save_directory, 'saved_model', str(version))
A:transformers.modeling_tf_utils.output_model_file->os.path.join(save_directory, TF2_WEIGHTS_NAME)
A:transformers.modeling_tf_utils.url->self._push_to_hub(repo, commit_message=commit_message)
A:transformers.modeling_tf_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_tf_utils.from_pt->kwargs.pop('from_pt', False)
A:transformers.modeling_tf_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_tf_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.modeling_tf_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_tf_utils.output_loading_info->kwargs.pop('output_loading_info', False)
A:transformers.modeling_tf_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.modeling_tf_utils.use_auth_token->kwargs.pop('use_auth_token', None)
A:transformers.modeling_tf_utils.revision->kwargs.pop('revision', None)
A:transformers.modeling_tf_utils.mirror->kwargs.pop('mirror', None)
A:transformers.modeling_tf_utils.load_weight_prefix->kwargs.pop('load_weight_prefix', None)
A:transformers.modeling_tf_utils.from_pipeline->kwargs.pop('_from_pipeline', None)
A:transformers.modeling_tf_utils.from_auto_class->kwargs.pop('_from_auto', False)
A:transformers.modeling_tf_utils.(config, model_kwargs)->tensorflow.keras.utils.register_keras_serializable()(cls).config_class.from_pretrained(config_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only, use_auth_token=use_auth_token, revision=revision, _from_auto=from_auto_class, _from_pipeline=from_pipeline, **kwargs)
A:transformers.modeling_tf_utils.archive_file->hf_bucket_url(pretrained_model_name_or_path, filename=WEIGHTS_NAME if from_pt else TF2_WEIGHTS_NAME, revision=revision, mirror=mirror)
A:transformers.modeling_tf_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent)
A:transformers.modeling_tf_utils.model->cls(config, *model_args, **model_kwargs)
A:transformers.modeling_tf_utils.(missing_keys, unexpected_keys)->load_tf_weights(model, resolved_archive_file, load_weight_prefix)
A:transformers.modeling_tf_utils.self.weight->self.add_weight('weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_utils.self.bias->self.add_weight('bias', shape=[1, self.nf], initializer=tf.zeros_initializer())
A:transformers.modeling_tf_utils.x->tensorflow.reshape(inputs, [-1, self.hidden_size])
A:transformers.modeling_tf_utils.base_config->super().get_config()
A:transformers.modeling_tf_utils.logits->tensorflow.matmul(x, self.weight, transpose_b=True)
A:transformers.modeling_tf_utils.self.summary->tensorflow.keras.layers.Dense(num_classes, kernel_initializer=get_initializer(initializer_range), name='summary')
A:transformers.modeling_tf_utils.self.first_dropout->tensorflow.keras.layers.Dropout(config.summary_first_dropout)
A:transformers.modeling_tf_utils.self.last_dropout->tensorflow.keras.layers.Dropout(config.summary_last_dropout)
A:transformers.modeling_tf_utils.hidden_states->inputs.get('hidden_states')
A:transformers.modeling_tf_utils.cls_index->tensorflow.expand_dims(cls_index, axis=-1)
A:transformers.modeling_tf_utils.hidden_shape->shape_list(hidden_states)
A:transformers.modeling_tf_utils.cls_shape->shape_list(cls_index)
A:transformers.modeling_tf_utils.dynamic->tensorflow.shape(tensor)
A:transformers.modeling_tf_utils.static->tensor.shape.as_list()
transformers.TFPreTrainedModel(self,config,*inputs,**kwargs)
transformers.TFPreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)->tf.Variable
transformers.TFPreTrainedModel._get_resized_lm_head_bias(self,old_lm_head_bias,new_num_tokens)
transformers.TFPreTrainedModel._get_resized_lm_head_decoder(self,old_lm_head_decoder,new_num_tokens)
transformers.TFPreTrainedModel._get_word_embedding_weight(model,embedding_layer)
transformers.TFPreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.TFPreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.TFPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFPreTrainedModel.get_bias(self)->Union[None, Dict[str, tf.Variable]]
transformers.TFPreTrainedModel.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.TFPreTrainedModel.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFPreTrainedModel.get_output_embeddings(self)->Union[None, tf.keras.layers.Layer]
transformers.TFPreTrainedModel.get_output_layer_with_bias(self)->Union[None, tf.keras.layers.Layer]
transformers.TFPreTrainedModel.get_prefix_bias_name(self)->Union[None, str]
transformers.TFPreTrainedModel.prune_heads(self,heads_to_prune)
transformers.TFPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)->tf.Variable
transformers.TFPreTrainedModel.save_pretrained(self,save_directory,saved_model=False,version=1,push_to_hub=False,**kwargs)
transformers.TFPreTrainedModel.serving(self,inputs)
transformers.TFPreTrainedModel.serving_output(output)
transformers.TFPreTrainedModel.set_bias(self,value)
transformers.TFPreTrainedModel.set_input_embeddings(self,value)
transformers.TFPreTrainedModel.set_output_embeddings(self,value)
transformers.TFSequenceSummary(self,config:PretrainedConfig,initializer_range:float=0.02,**kwargs)
transformers.TFSequenceSummary.call(self,inputs,cls_index=None,training=False)
transformers.TFSharedEmbeddings(self,vocab_size:int,hidden_size:int,initializer_range:Optional[float]=None,**kwargs)
transformers.TFSharedEmbeddings._embedding(self,input_ids)
transformers.TFSharedEmbeddings._linear(self,inputs)
transformers.TFSharedEmbeddings.build(self,input_shape)
transformers.TFSharedEmbeddings.call(self,inputs:tf.Tensor,mode:str='embedding')->tf.Tensor
transformers.TFSharedEmbeddings.get_config(self)
transformers.modeling_tf_utils.TFCausalLanguageModelingLoss
transformers.modeling_tf_utils.TFCausalLanguageModelingLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFConv1D(self,nf,nx,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFConv1D.__init__(self,nf,nx,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFConv1D.build(self,input_shape)
transformers.modeling_tf_utils.TFConv1D.call(self,x)
transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss(TFCausalLanguageModelingLoss)
transformers.modeling_tf_utils.TFModelUtilsMixin
transformers.modeling_tf_utils.TFModelUtilsMixin.num_parameters(self,only_trainable:bool=False)->int
transformers.modeling_tf_utils.TFMultipleChoiceLoss(TFSequenceClassificationLoss)
transformers.modeling_tf_utils.TFNextSentencePredictionLoss
transformers.modeling_tf_utils.TFNextSentencePredictionLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFPreTrainedModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)->tf.Variable
transformers.modeling_tf_utils.TFPreTrainedModel._get_resized_lm_head_bias(self,old_lm_head_bias,new_num_tokens)
transformers.modeling_tf_utils.TFPreTrainedModel._get_resized_lm_head_decoder(self,old_lm_head_decoder,new_num_tokens)
transformers.modeling_tf_utils.TFPreTrainedModel._get_word_embedding_weight(model,embedding_layer)
transformers.modeling_tf_utils.TFPreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_utils.TFPreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.modeling_tf_utils.TFPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.get_bias(self)->Union[None, Dict[str, tf.Variable]]
transformers.modeling_tf_utils.TFPreTrainedModel.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.modeling_tf_utils.TFPreTrainedModel.get_lm_head(self)->tf.keras.layers.Layer
transformers.modeling_tf_utils.TFPreTrainedModel.get_output_embeddings(self)->Union[None, tf.keras.layers.Layer]
transformers.modeling_tf_utils.TFPreTrainedModel.get_output_layer_with_bias(self)->Union[None, tf.keras.layers.Layer]
transformers.modeling_tf_utils.TFPreTrainedModel.get_prefix_bias_name(self)->Union[None, str]
transformers.modeling_tf_utils.TFPreTrainedModel.prune_heads(self,heads_to_prune)
transformers.modeling_tf_utils.TFPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)->tf.Variable
transformers.modeling_tf_utils.TFPreTrainedModel.save_pretrained(self,save_directory,saved_model=False,version=1,push_to_hub=False,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.serving(self,inputs)
transformers.modeling_tf_utils.TFPreTrainedModel.serving_output(output)
transformers.modeling_tf_utils.TFPreTrainedModel.set_bias(self,value)
transformers.modeling_tf_utils.TFPreTrainedModel.set_input_embeddings(self,value)
transformers.modeling_tf_utils.TFPreTrainedModel.set_output_embeddings(self,value)
transformers.modeling_tf_utils.TFQuestionAnsweringLoss
transformers.modeling_tf_utils.TFQuestionAnsweringLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFSequenceClassificationLoss
transformers.modeling_tf_utils.TFSequenceClassificationLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFSequenceSummary(self,config:PretrainedConfig,initializer_range:float=0.02,**kwargs)
transformers.modeling_tf_utils.TFSequenceSummary.__init__(self,config:PretrainedConfig,initializer_range:float=0.02,**kwargs)
transformers.modeling_tf_utils.TFSequenceSummary.call(self,inputs,cls_index=None,training=False)
transformers.modeling_tf_utils.TFSharedEmbeddings(self,vocab_size:int,hidden_size:int,initializer_range:Optional[float]=None,**kwargs)
transformers.modeling_tf_utils.TFSharedEmbeddings.__init__(self,vocab_size:int,hidden_size:int,initializer_range:Optional[float]=None,**kwargs)
transformers.modeling_tf_utils.TFSharedEmbeddings._embedding(self,input_ids)
transformers.modeling_tf_utils.TFSharedEmbeddings._linear(self,inputs)
transformers.modeling_tf_utils.TFSharedEmbeddings.build(self,input_shape)
transformers.modeling_tf_utils.TFSharedEmbeddings.call(self,inputs:tf.Tensor,mode:str='embedding')->tf.Tensor
transformers.modeling_tf_utils.TFSharedEmbeddings.get_config(self)
transformers.modeling_tf_utils.TFTokenClassificationLoss
transformers.modeling_tf_utils.TFTokenClassificationLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFWrappedEmbeddings(self,layer,abs_scope_name=None)
transformers.modeling_tf_utils.TFWrappedEmbeddings.__init__(self,layer,abs_scope_name=None)
transformers.modeling_tf_utils.TFWrappedEmbeddings.call(self,inputs,mode='embedding')
transformers.modeling_tf_utils.booleans_processing(config,**kwargs)
transformers.modeling_tf_utils.get_initializer(initializer_range:float=0.02)->tf.initializers.TruncatedNormal
transformers.modeling_tf_utils.init_copy_embeddings(old_embeddings,new_num_tokens)
transformers.modeling_tf_utils.input_processing(func,config,input_ids,**kwargs)
transformers.modeling_tf_utils.keras_serializable(cls)
transformers.modeling_tf_utils.load_tf_weights(model,resolved_archive_file,_prefix=None)
transformers.modeling_tf_utils.shape_list(tensor:tf.Tensor)->List[int]
transformers.shape_list(tensor:tf.Tensor)->List[int]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/text_classification.py----------------------------------------
A:transformers.pipelines.text_classification.outputs->super().__call__(*args, **kwargs)
transformers.TextClassificationPipeline(self,return_all_scores:bool=False,**kwargs)
transformers.pipelines.text_classification.TextClassificationPipeline(self,return_all_scores:bool=False,**kwargs)
transformers.pipelines.text_classification.TextClassificationPipeline.__init__(self,return_all_scores:bool=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/question_answering.py----------------------------------------
A:transformers.pipelines.question_answering.inputs->list(args)
A:transformers.pipelines.question_answering.inputs[i]->self.normalize(item)
A:transformers.pipelines.question_answering.self._args_parser->QuestionAnsweringArgumentHandler()
A:transformers.pipelines.question_answering.examples->self._args_parser(*args, **kwargs)
A:transformers.pipelines.question_answering.question_first->bool(self.tokenizer.padding_side == 'right')
A:transformers.pipelines.question_answering.encoded_inputs->self.tokenizer(text=example.question_text if question_first else example.context_text, text_pair=example.context_text if question_first else example.question_text, padding=kwargs['padding'], truncation='only_second' if question_first else 'only_first', max_length=kwargs['max_seq_len'], stride=kwargs['doc_stride'], return_tensors='np', return_token_type_ids=True, return_overflowing_tokens=True, return_offsets_mapping=True, return_special_tokens_mask=True)
A:transformers.pipelines.question_answering.num_spans->len(encoded_inputs['input_ids'])
A:transformers.pipelines.question_answering.p_mask->numpy.asarray([[tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)] for span_id in range(num_spans)])
A:transformers.pipelines.question_answering.cls_index->numpy.nonzero(encoded_inputs['input_ids'] == self.tokenizer.cls_token_id)
A:transformers.pipelines.question_answering.start_->numpy.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))
A:transformers.pipelines.question_answering.end_->numpy.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))
A:transformers.pipelines.question_answering.min_null_score->min(min_null_score, (start_[0] * end_[0]).item())
A:transformers.pipelines.question_answering.(starts, ends, scores)->self.decode(start_, end_, kwargs['topk'], kwargs['max_answer_len'], undesired_tokens)
A:transformers.pipelines.question_answering.char_to_word->numpy.array(example.char_to_word_offset)
A:transformers.pipelines.question_answering.outer->numpy.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))
A:transformers.pipelines.question_answering.candidates->numpy.tril(np.triu(outer), max_answer_len - 1)
A:transformers.pipelines.question_answering.scores_flat->numpy.tril(np.triu(outer), max_answer_len - 1).flatten()
A:transformers.pipelines.question_answering.idx_sort->numpy.argsort(-scores_flat)
A:transformers.pipelines.question_answering.token->self.tokenizer.tokenize(word)
transformers.QuestionAnsweringPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,device:int=-1,task:str='',**kwargs)
transformers.QuestionAnsweringPipeline.create_sample(question:Union[str,List[str]],context:Union[str,List[str]])->Union[SquadExample, List[SquadExample]]
transformers.QuestionAnsweringPipeline.decode(self,start:np.ndarray,end:np.ndarray,topk:int,max_answer_len:int,undesired_tokens:np.ndarray)->Tuple
transformers.QuestionAnsweringPipeline.span_to_answer(self,text:str,start:int,end:int)->Dict[str, Union[str, int]]
transformers.pipelines.QuestionAnsweringArgumentHandler(self,*args,**kwargs)
transformers.pipelines.QuestionAnsweringArgumentHandler.normalize(self,item)
transformers.pipelines.question_answering.QuestionAnsweringArgumentHandler(self,*args,**kwargs)
transformers.pipelines.question_answering.QuestionAnsweringArgumentHandler.__call__(self,*args,**kwargs)
transformers.pipelines.question_answering.QuestionAnsweringArgumentHandler.normalize(self,item)
transformers.pipelines.question_answering.QuestionAnsweringPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,device:int=-1,task:str='',**kwargs)
transformers.pipelines.question_answering.QuestionAnsweringPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,device:int=-1,task:str='',**kwargs)
transformers.pipelines.question_answering.QuestionAnsweringPipeline.create_sample(question:Union[str,List[str]],context:Union[str,List[str]])->Union[SquadExample, List[SquadExample]]
transformers.pipelines.question_answering.QuestionAnsweringPipeline.decode(self,start:np.ndarray,end:np.ndarray,topk:int,max_answer_len:int,undesired_tokens:np.ndarray)->Tuple
transformers.pipelines.question_answering.QuestionAnsweringPipeline.span_to_answer(self,text:str,start:int,end:int)->Dict[str, Union[str, int]]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/text2text_generation.py----------------------------------------
A:transformers.pipelines.text2text_generation.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.text2text_generation.inputs->self._parse_and_tokenize(*args, truncation=truncation, src_lang=src_lang, tgt_lang=tgt_lang)
A:transformers.pipelines.text2text_generation.input_length->tensorflow.shape(inputs['input_ids'])[-1].numpy()
A:transformers.pipelines.text2text_generation.min_length->generate_kwargs.get('min_length', self.model.config.min_length)
A:transformers.pipelines.text2text_generation.max_length->generate_kwargs.get('max_length', self.model.config.max_length)
A:transformers.pipelines.text2text_generation.generations->self.model.generate(**generate_kwargs)
A:transformers.pipelines.text2text_generation.record[f'{self.return_name}_text']->self.tokenizer.decode(generation, skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces)
A:transformers.pipelines.text2text_generation.task->kwargs.get('task', '')
A:transformers.pipelines.text2text_generation.items->kwargs.get('task', '').split('_')
transformers.SummarizationPipeline(self,*args,**kwargs)
transformers.SummarizationPipeline.check_inputs(self,input_length:int,min_length:int,max_length:int)->bool
transformers.Text2TextGenerationPipeline(self,*args,**kwargs)
transformers.Text2TextGenerationPipeline._generate(self,inputs,return_tensors:bool,return_text:bool,clean_up_tokenization_spaces:bool,generate_kwargs)
transformers.Text2TextGenerationPipeline._parse_and_tokenize(self,*args,truncation)
transformers.Text2TextGenerationPipeline.check_inputs(self,input_length:int,min_length:int,max_length:int)
transformers.TranslationPipeline(self,*args,src_lang=None,tgt_lang=None,**kwargs)
transformers.TranslationPipeline._parse_and_tokenize(self,*args,src_lang,tgt_lang,truncation)
transformers.TranslationPipeline.check_inputs(self,input_length:int,min_length:int,max_length:int)
transformers.pipelines.text2text_generation.SummarizationPipeline(self,*args,**kwargs)
transformers.pipelines.text2text_generation.SummarizationPipeline.__call__(self,*args,**kwargs)
transformers.pipelines.text2text_generation.SummarizationPipeline.check_inputs(self,input_length:int,min_length:int,max_length:int)->bool
transformers.pipelines.text2text_generation.Text2TextGenerationPipeline(self,*args,**kwargs)
transformers.pipelines.text2text_generation.Text2TextGenerationPipeline.__init__(self,*args,**kwargs)
transformers.pipelines.text2text_generation.Text2TextGenerationPipeline._generate(self,inputs,return_tensors:bool,return_text:bool,clean_up_tokenization_spaces:bool,generate_kwargs)
transformers.pipelines.text2text_generation.Text2TextGenerationPipeline._parse_and_tokenize(self,*args,truncation)
transformers.pipelines.text2text_generation.Text2TextGenerationPipeline.check_inputs(self,input_length:int,min_length:int,max_length:int)
transformers.pipelines.text2text_generation.TranslationPipeline(self,*args,src_lang=None,tgt_lang=None,**kwargs)
transformers.pipelines.text2text_generation.TranslationPipeline.__init__(self,*args,src_lang=None,tgt_lang=None,**kwargs)
transformers.pipelines.text2text_generation.TranslationPipeline._parse_and_tokenize(self,*args,src_lang,tgt_lang,truncation)
transformers.pipelines.text2text_generation.TranslationPipeline.check_inputs(self,input_length:int,min_length:int,max_length:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/token_classification.py----------------------------------------
A:transformers.pipelines.token_classification.inputs->list(inputs)
A:transformers.pipelines.token_classification.batch_size->len(inputs)
A:transformers.pipelines.token_classification.offset_mapping->kwargs.get('offset_mapping')
A:transformers.pipelines.token_classification.self._basic_tokenizer->BasicTokenizer(do_lower_case=False)
A:transformers.pipelines.token_classification.(_inputs, offset_mappings)->self._args_parser(inputs, **kwargs)
A:transformers.pipelines.token_classification.tokens->self.ensure_tensor_on_device(**tokens)
A:transformers.pipelines.token_classification.entities->self.aggregate_words(pre_entities, aggregation_strategy)
A:transformers.pipelines.token_classification.pre_entities->self.gather_pre_entities(sentence, input_ids, scores, offset_mapping, special_tokens_mask)
A:transformers.pipelines.token_classification.grouped_entities->self.aggregate(pre_entities, self.aggregation_strategy)
A:transformers.pipelines.token_classification.word->self.tokenizer.convert_tokens_to_string([entity['word'] for entity in entities])
A:transformers.pipelines.token_classification.entity_idx->numpy.nanmean(scores, axis=0).argmax()
A:transformers.pipelines.token_classification.idx->numpy.nanmean([entity['score'] for entity in entities]).argmax()
A:transformers.pipelines.token_classification.max_entity->max(entities, key=lambda entity: entity['scores'].max())
A:transformers.pipelines.token_classification.scores->numpy.nanmean([entity['score'] for entity in entities])
A:transformers.pipelines.token_classification.average_scores->numpy.nanmean(scores, axis=0)
A:transformers.pipelines.token_classification.(bi, tag)->self.get_tag(entity['entity'])
A:transformers.pipelines.token_classification.(last_bi, last_tag)->self.get_tag(entity_group_disagg[-1]['entity'])
transformers.TokenClassificationPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=TokenClassificationArgumentHandler(),device:int=-1,binary_output:bool=False,ignore_labels=['O'],task:str='',grouped_entities:Optional[bool]=None,ignore_subwords:Optional[bool]=None,aggregation_strategy:Optional[AggregationStrategy]=None)
transformers.TokenClassificationPipeline.aggregate(self,pre_entities:List[dict],aggregation_strategy:AggregationStrategy)->List[dict]
transformers.TokenClassificationPipeline.aggregate_word(self,entities:List[dict],aggregation_strategy:AggregationStrategy)->dict
transformers.TokenClassificationPipeline.aggregate_words(self,entities:List[dict],aggregation_strategy:AggregationStrategy)->List[dict]
transformers.TokenClassificationPipeline.gather_pre_entities(self,sentence:str,input_ids:np.ndarray,scores:np.ndarray,offset_mapping:Optional[List[Tuple[int,int]]],special_tokens_mask:np.ndarray)->List[dict]
transformers.TokenClassificationPipeline.get_tag(self,entity_name:str)->Tuple[str, str]
transformers.TokenClassificationPipeline.group_entities(self,entities:List[dict])->List[dict]
transformers.TokenClassificationPipeline.group_sub_entities(self,entities:List[dict])->dict
transformers.pipelines.AggregationStrategy(ExplicitEnum)
transformers.pipelines.TokenClassificationArgumentHandler(self,inputs:Union[str,List[str]],**kwargs)
transformers.pipelines.token_classification.AggregationStrategy(ExplicitEnum)
transformers.pipelines.token_classification.TokenClassificationArgumentHandler(self,inputs:Union[str,List[str]],**kwargs)
transformers.pipelines.token_classification.TokenClassificationArgumentHandler.__call__(self,inputs:Union[str,List[str]],**kwargs)
transformers.pipelines.token_classification.TokenClassificationPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=TokenClassificationArgumentHandler(),device:int=-1,binary_output:bool=False,ignore_labels=['O'],task:str='',grouped_entities:Optional[bool]=None,ignore_subwords:Optional[bool]=None,aggregation_strategy:Optional[AggregationStrategy]=None)
transformers.pipelines.token_classification.TokenClassificationPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=TokenClassificationArgumentHandler(),device:int=-1,binary_output:bool=False,ignore_labels=['O'],task:str='',grouped_entities:Optional[bool]=None,ignore_subwords:Optional[bool]=None,aggregation_strategy:Optional[AggregationStrategy]=None)
transformers.pipelines.token_classification.TokenClassificationPipeline.aggregate(self,pre_entities:List[dict],aggregation_strategy:AggregationStrategy)->List[dict]
transformers.pipelines.token_classification.TokenClassificationPipeline.aggregate_word(self,entities:List[dict],aggregation_strategy:AggregationStrategy)->dict
transformers.pipelines.token_classification.TokenClassificationPipeline.aggregate_words(self,entities:List[dict],aggregation_strategy:AggregationStrategy)->List[dict]
transformers.pipelines.token_classification.TokenClassificationPipeline.gather_pre_entities(self,sentence:str,input_ids:np.ndarray,scores:np.ndarray,offset_mapping:Optional[List[Tuple[int,int]]],special_tokens_mask:np.ndarray)->List[dict]
transformers.pipelines.token_classification.TokenClassificationPipeline.get_tag(self,entity_name:str)->Tuple[str, str]
transformers.pipelines.token_classification.TokenClassificationPipeline.group_entities(self,entities:List[dict])->List[dict]
transformers.pipelines.token_classification.TokenClassificationPipeline.group_sub_entities(self,entities:List[dict])->dict


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/automatic_speech_recognition.py----------------------------------------
A:transformers.pipelines.automatic_speech_recognition.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.automatic_speech_recognition.ffmpeg_process->subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
A:transformers.pipelines.automatic_speech_recognition.output_stream->subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE).communicate(bpayload)
A:transformers.pipelines.automatic_speech_recognition.audio->numpy.frombuffer(out_bytes, np.float32)
A:transformers.pipelines.automatic_speech_recognition.inputs->ffmpeg_read(inputs, self.feature_extractor.sampling_rate)
A:transformers.pipelines.automatic_speech_recognition.processed->self.ensure_tensor_on_device(**processed)
A:transformers.pipelines.automatic_speech_recognition.tokens->self.model(**processed).logits.squeeze(0).argmax(dim=-1)
A:transformers.pipelines.automatic_speech_recognition.outputs->self.model(**processed)
A:transformers.pipelines.automatic_speech_recognition.recognized_string->self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)
transformers.AutomaticSpeechRecognitionPipeline(self,feature_extractor:'SequenceFeatureExtractor',*args,**kwargs)
transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline(self,feature_extractor:'SequenceFeatureExtractor',*args,**kwargs)
transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline.__init__(self,feature_extractor:'SequenceFeatureExtractor',*args,**kwargs)
transformers.pipelines.automatic_speech_recognition.ffmpeg_read(bpayload:bytes,sampling_rate:int)->np.array


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/image_classification.py----------------------------------------
A:transformers.pipelines.image_classification.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.image_classification.is_batched->isinstance(images, list)
A:transformers.pipelines.image_classification.inputs->self.feature_extractor(images=images, return_tensors='pt')
A:transformers.pipelines.image_classification.outputs->self.model(**inputs)
A:transformers.pipelines.image_classification.probs->self.model(**inputs).logits.softmax(-1)
A:transformers.pipelines.image_classification.(scores, ids)->self.model(**inputs).logits.softmax(-1).topk(top_k)
A:transformers.pipelines.image_classification.scores->scores.tolist().tolist()
A:transformers.pipelines.image_classification.ids->ids.tolist().tolist()
transformers.ImageClassificationPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],feature_extractor:PreTrainedFeatureExtractor,framework:Optional[str]=None,**kwargs)
transformers.ImageClassificationPipeline.load_image(image:Union[str,'Image.Image'])
transformers.pipelines.image_classification.ImageClassificationPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],feature_extractor:PreTrainedFeatureExtractor,framework:Optional[str]=None,**kwargs)
transformers.pipelines.image_classification.ImageClassificationPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],feature_extractor:PreTrainedFeatureExtractor,framework:Optional[str]=None,**kwargs)
transformers.pipelines.image_classification.ImageClassificationPipeline.load_image(image:Union[str,'Image.Image'])


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/conversational.py----------------------------------------
A:transformers.pipelines.conversational.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.conversational.conversation_id->uuid.uuid4()
A:transformers.pipelines.conversational.inputs->self.tokenizer.pad({'input_ids': input_ids}, padding='longest', return_attention_mask=True, return_tensors=self.framework)
A:transformers.pipelines.conversational.input_length->tensorflow.shape(inputs['input_ids'])[-1].numpy()
A:transformers.pipelines.conversational.generated_responses->self.model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generate_kwargs)
A:transformers.pipelines.conversational.history->self._clean_padding_history(history)
transformers.Conversation(self,text:str=None,conversation_id:uuid.UUID=None,past_user_inputs=None,generated_responses=None)
transformers.Conversation.__eq__(self,other)
transformers.Conversation.__repr__(self)
transformers.Conversation.add_user_input(self,text:str,overwrite:bool=False)
transformers.Conversation.append_response(self,response:str)
transformers.Conversation.iter_texts(self)
transformers.Conversation.mark_processed(self)
transformers.ConversationalPipeline(self,min_length_for_response=32,*args,**kwargs)
transformers.ConversationalPipeline._clean_padding_history(self,generated_tensor)->List[List[int]]
transformers.ConversationalPipeline._legacy_parse_and_tokenize(self,conversation:List[Conversation])->List[int]
transformers.ConversationalPipeline._parse_and_tokenize(self,conversations:List[Conversation])->Dict[str, Any]
transformers.pipelines.conversational.Conversation(self,text:str=None,conversation_id:uuid.UUID=None,past_user_inputs=None,generated_responses=None)
transformers.pipelines.conversational.Conversation.__eq__(self,other)
transformers.pipelines.conversational.Conversation.__init__(self,text:str=None,conversation_id:uuid.UUID=None,past_user_inputs=None,generated_responses=None)
transformers.pipelines.conversational.Conversation.__repr__(self)
transformers.pipelines.conversational.Conversation.add_user_input(self,text:str,overwrite:bool=False)
transformers.pipelines.conversational.Conversation.append_response(self,response:str)
transformers.pipelines.conversational.Conversation.iter_texts(self)
transformers.pipelines.conversational.Conversation.mark_processed(self)
transformers.pipelines.conversational.ConversationalPipeline(self,min_length_for_response=32,*args,**kwargs)
transformers.pipelines.conversational.ConversationalPipeline.__init__(self,min_length_for_response=32,*args,**kwargs)
transformers.pipelines.conversational.ConversationalPipeline._clean_padding_history(self,generated_tensor)->List[List[int]]
transformers.pipelines.conversational.ConversationalPipeline._legacy_parse_and_tokenize(self,conversation:List[Conversation])->List[int]
transformers.pipelines.conversational.ConversationalPipeline._parse_and_tokenize(self,conversations:List[Conversation])->Dict[str, Any]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/table_question_answering.py----------------------------------------
A:transformers.pipelines.table_question_answering.tqa_pipeline_input['table']->pandas.DataFrame(tqa_pipeline_input['table'])
A:transformers.pipelines.table_question_answering.input_ids->inputs['input_ids'].to(self.device)
A:transformers.pipelines.table_question_answering.attention_mask->inputs['attention_mask'].to(self.device)
A:transformers.pipelines.table_question_answering.token_type_ids->inputs['token_type_ids'].to(self.device)
A:transformers.pipelines.table_question_answering.model_labels->numpy.zeros_like(prev_labels_example.cpu().numpy())
A:transformers.pipelines.table_question_answering.model_labels[i]->int(prev_answers[col_id, row_id])
A:transformers.pipelines.table_question_answering.token_type_ids_example[:, 3]->torch.from_numpy(model_labels).type(torch.long).to(self.device)
A:transformers.pipelines.table_question_answering.outputs->self.model(input_ids=input_ids_example.unsqueeze(0), attention_mask=attention_mask_example.unsqueeze(0), token_type_ids=token_type_ids_example.unsqueeze(0))
A:transformers.pipelines.table_question_answering.dist_per_token->torch.distributions.Bernoulli(logits=logits)
A:transformers.pipelines.table_question_answering.coords_to_probs->collections.defaultdict(list)
A:transformers.pipelines.table_question_answering.logits_batch->torch.cat(tuple(all_logits), 0)
A:transformers.pipelines.table_question_answering.(pipeline_inputs, sequential, padding, truncation)->self._args_parser(*args, **kwargs)
A:transformers.pipelines.table_question_answering.inputs->self.tokenizer(table, query, return_tensors=self.framework, truncation='drop_rows_to_fit', padding=padding)
A:transformers.pipelines.table_question_answering.predictions->self.tokenizer.convert_logits_to_predictions(inputs, logits.detach())
A:transformers.pipelines.table_question_answering.aggregator->aggregators.get(index, '')
A:transformers.pipelines.table_question_answering.aggregator_prefix->aggregators_prefix.get(index, '')
transformers.TableQuestionAnsweringPipeline(self,args_parser=TableQuestionAnsweringArgumentHandler(),*args,**kwargs)
transformers.TableQuestionAnsweringPipeline.batch_inference(self,**inputs)
transformers.TableQuestionAnsweringPipeline.sequential_inference(self,**inputs)
transformers.pipelines.TableQuestionAnsweringArgumentHandler(self,table=None,query=None,sequential=False,padding=True,truncation=True)
transformers.pipelines.table_question_answering.TableQuestionAnsweringArgumentHandler(self,table=None,query=None,sequential=False,padding=True,truncation=True)
transformers.pipelines.table_question_answering.TableQuestionAnsweringArgumentHandler.__call__(self,table=None,query=None,sequential=False,padding=True,truncation=True)
transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline(self,args_parser=TableQuestionAnsweringArgumentHandler(),*args,**kwargs)
transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline.__init__(self,args_parser=TableQuestionAnsweringArgumentHandler(),*args,**kwargs)
transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline.batch_inference(self,**inputs)
transformers.pipelines.table_question_answering.TableQuestionAnsweringPipeline.sequential_inference(self,**inputs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/base.py----------------------------------------
A:transformers.pipelines.base.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.base.transformers_module->importlib.import_module('transformers')
A:transformers.pipelines.base._class->getattr(transformers_module, f'TF{architecture}', None)
A:transformers.pipelines.base.kwargs->model_kwargs.copy()
A:transformers.pipelines.base.model->models.auto.modeling_tf_auto.TFAutoModel.from_pretrained(model, revision=revision)
A:transformers.pipelines.base.config->models.auto.configuration_auto.AutoConfig.from_pretrained(model, _from_pipeline=task, **model_kwargs)
A:transformers.pipelines.base.(path, _)->os.path.splitext(self.output_path)
A:transformers.pipelines.base.binary_path->os.path.extsep.join((path, 'pickle'))
A:transformers.pipelines.base.reader->csv.DictReader(f)
A:transformers.pipelines.base.writer->csv.DictWriter(f, list(data[0].keys()))
A:transformers.pipelines.base.self._entries->json.load(f)
A:transformers.pipelines.base.line->line.split('\t').split('\t')
A:transformers.pipelines.base.(framework, model)->infer_framework_load_model(model, config=model.config)
A:transformers.pipelines.base.self.model->self.model.to(self.device)
A:transformers.pipelines.base.inputs->self.ensure_tensor_on_device(**inputs)
A:transformers.pipelines.base.predictions->self.model(**inputs)[0].cpu()
transformers.CsvPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.CsvPipelineDataFormat.__iter__(self)
transformers.CsvPipelineDataFormat.save(self,data:List[dict])
transformers.JsonPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.JsonPipelineDataFormat.__iter__(self)
transformers.JsonPipelineDataFormat.save(self,data:dict)
transformers.PipedPipelineDataFormat(PipelineDataFormat)
transformers.PipedPipelineDataFormat.__iter__(self)
transformers.PipedPipelineDataFormat.save(self,data:dict)
transformers.PipedPipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.Pipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:Optional[PreTrainedTokenizer]=None,feature_extractor:Optional[PreTrainedFeatureExtractor]=None,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,task:str='',args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False)
transformers.Pipeline._forward(self,inputs,return_tensors=False)
transformers.Pipeline._parse_and_tokenize(self,inputs,padding=True,add_special_tokens=True,truncation=TruncationStrategy.DO_NOT_TRUNCATE,**kwargs)
transformers.Pipeline.check_model_type(self,supported_models:Union[List[str],dict])
transformers.Pipeline.device_placement(self)
transformers.Pipeline.ensure_tensor_on_device(self,**inputs)
transformers.Pipeline.predict(self,X)
transformers.Pipeline.save_pretrained(self,save_directory:str)
transformers.Pipeline.transform(self,X)
transformers.PipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite:bool=False)
transformers.PipelineDataFormat.__iter__(self)
transformers.PipelineDataFormat.from_str(format:str,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)->'PipelineDataFormat'
transformers.PipelineDataFormat.save(self,data:Union[dict,List[dict]])
transformers.PipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.PipelineException(self,task:str,model:str,reason:str)
transformers.pipelines.ArgumentHandler(self,*args,**kwargs)
transformers.pipelines.base.ArgumentHandler(self,*args,**kwargs)
transformers.pipelines.base.ArgumentHandler.__call__(self,*args,**kwargs)
transformers.pipelines.base.CsvPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.base.CsvPipelineDataFormat.__init__(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.base.CsvPipelineDataFormat.__iter__(self)
transformers.pipelines.base.CsvPipelineDataFormat.save(self,data:List[dict])
transformers.pipelines.base.JsonPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.base.JsonPipelineDataFormat.__init__(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.base.JsonPipelineDataFormat.__iter__(self)
transformers.pipelines.base.JsonPipelineDataFormat.save(self,data:dict)
transformers.pipelines.base.PipedPipelineDataFormat(PipelineDataFormat)
transformers.pipelines.base.PipedPipelineDataFormat.__iter__(self)
transformers.pipelines.base.PipedPipelineDataFormat.save(self,data:dict)
transformers.pipelines.base.PipedPipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.pipelines.base.Pipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:Optional[PreTrainedTokenizer]=None,feature_extractor:Optional[PreTrainedFeatureExtractor]=None,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,task:str='',args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False)
transformers.pipelines.base.Pipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:Optional[PreTrainedTokenizer]=None,feature_extractor:Optional[PreTrainedFeatureExtractor]=None,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,task:str='',args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False)
transformers.pipelines.base.Pipeline._forward(self,inputs,return_tensors=False)
transformers.pipelines.base.Pipeline._parse_and_tokenize(self,inputs,padding=True,add_special_tokens=True,truncation=TruncationStrategy.DO_NOT_TRUNCATE,**kwargs)
transformers.pipelines.base.Pipeline.check_model_type(self,supported_models:Union[List[str],dict])
transformers.pipelines.base.Pipeline.device_placement(self)
transformers.pipelines.base.Pipeline.ensure_tensor_on_device(self,**inputs)
transformers.pipelines.base.Pipeline.predict(self,X)
transformers.pipelines.base.Pipeline.save_pretrained(self,save_directory:str)
transformers.pipelines.base.Pipeline.transform(self,X)
transformers.pipelines.base.PipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite:bool=False)
transformers.pipelines.base.PipelineDataFormat.__init__(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite:bool=False)
transformers.pipelines.base.PipelineDataFormat.__iter__(self)
transformers.pipelines.base.PipelineDataFormat.from_str(format:str,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)->'PipelineDataFormat'
transformers.pipelines.base.PipelineDataFormat.save(self,data:Union[dict,List[dict]])
transformers.pipelines.base.PipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.pipelines.base.PipelineException(self,task:str,model:str,reason:str)
transformers.pipelines.base.PipelineException.__init__(self,task:str,model:str,reason:str)
transformers.pipelines.base._ScikitCompat(ABC)
transformers.pipelines.base._ScikitCompat.predict(self,X)
transformers.pipelines.base._ScikitCompat.transform(self,X)
transformers.pipelines.base.get_default_model(targeted_task:Dict,framework:Optional[str],task_options:Optional[Any])->str
transformers.pipelines.base.get_framework(model,revision:Optional[str]=None)
transformers.pipelines.base.infer_framework_from_model(model,model_classes:Optional[Dict[str,Tuple[type]]]=None,task:Optional[str]=None,framework:Optional[str]=None,**model_kwargs)
transformers.pipelines.base.infer_framework_load_model(model,config:AutoConfig,model_classes:Optional[Dict[str,Tuple[type]]]=None,task:Optional[str]=None,framework:Optional[str]=None,**model_kwargs)
transformers.pipelines.get_default_model(targeted_task:Dict,framework:Optional[str],task_options:Optional[Any])->str
transformers.pipelines.infer_framework_load_model(model,config:AutoConfig,model_classes:Optional[Dict[str,Tuple[type]]]=None,task:Optional[str]=None,framework:Optional[str]=None,**model_kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/feature_extraction.py----------------------------------------
transformers.FeatureExtractionPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,task:str='')
transformers.pipelines.feature_extraction.FeatureExtractionPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,task:str='')
transformers.pipelines.feature_extraction.FeatureExtractionPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,task:str='')


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/zero_shot_classification.py----------------------------------------
A:transformers.pipelines.zero_shot_classification.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.zero_shot_classification.labels->self._parse_labels(labels)
A:transformers.pipelines.zero_shot_classification.sequence_pairs->self._args_parser(sequences, candidate_labels, hypothesis_template)
A:transformers.pipelines.zero_shot_classification.inputs->self.tokenizer(sequence_pairs, add_special_tokens=add_special_tokens, return_tensors=self.framework, padding=padding, truncation=truncation)
A:transformers.pipelines.zero_shot_classification.multi_label->kwargs.pop('multi_class')
A:transformers.pipelines.zero_shot_classification.outputs->super().__call__(sequences, candidate_labels, hypothesis_template)
A:transformers.pipelines.zero_shot_classification.num_sequences->len(sequences)
A:transformers.pipelines.zero_shot_classification.candidate_labels->self._args_parser._parse_labels(candidate_labels)
A:transformers.pipelines.zero_shot_classification.reshaped_outputs->super().__call__(sequences, candidate_labels, hypothesis_template).reshape((num_sequences, len(candidate_labels), -1))
A:transformers.pipelines.zero_shot_classification.top_inds->list(reversed(scores[iseq].argsort()))
transformers.ZeroShotClassificationPipeline(self,args_parser=ZeroShotClassificationArgumentHandler(),*args,**kwargs)
transformers.ZeroShotClassificationPipeline._parse_and_tokenize(self,sequences,candidate_labels,hypothesis_template,padding=True,add_special_tokens=True,truncation=TruncationStrategy.ONLY_FIRST,**kwargs)
transformers.ZeroShotClassificationPipeline.entailment_id(self)
transformers.pipelines.ZeroShotClassificationArgumentHandler(self,sequences,labels,hypothesis_template)
transformers.pipelines.ZeroShotClassificationArgumentHandler._parse_labels(self,labels)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler(self,sequences,labels,hypothesis_template)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler.__call__(self,sequences,labels,hypothesis_template)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler._parse_labels(self,labels)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline(self,args_parser=ZeroShotClassificationArgumentHandler(),*args,**kwargs)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline.__init__(self,args_parser=ZeroShotClassificationArgumentHandler(),*args,**kwargs)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline._parse_and_tokenize(self,sequences,candidate_labels,hypothesis_template,padding=True,add_special_tokens=True,truncation=TruncationStrategy.ONLY_FIRST,**kwargs)
transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline.entailment_id(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/fill_mask.py----------------------------------------
A:transformers.pipelines.fill_mask.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.fill_mask.numel->numpy.prod(masked_index.shape)
A:transformers.pipelines.fill_mask.inputs->self._parse_and_tokenize(*args, **kwargs)
A:transformers.pipelines.fill_mask.outputs->self._forward(inputs, return_tensors=True)
A:transformers.pipelines.fill_mask.target_enc->self.tokenizer.tokenize(target)
A:transformers.pipelines.fill_mask.target_inds->numpy.array(self.tokenizer.convert_tokens_to_ids(targets_proc))
A:transformers.pipelines.fill_mask.masked_index->torch.nonzero(input_ids == self.tokenizer.mask_token_id, as_tuple=False)
A:transformers.pipelines.fill_mask.probs->logits.softmax(dim=0)
A:transformers.pipelines.fill_mask.topk->tensorflow.math.top_k(probs, k=top_k if top_k is not None else self.top_k)
A:transformers.pipelines.fill_mask.values->tensorflow.gather_nd(values, tf.reshape(sort_inds, (-1, 1))).numpy()
A:transformers.pipelines.fill_mask.sort_inds->list(reversed(values.argsort(dim=-1)))
A:transformers.pipelines.fill_mask.(values, predictions)->logits.softmax(dim=0).topk(top_k if top_k is not None else self.top_k)
A:transformers.pipelines.fill_mask.tokens->input_ids.numpy()
transformers.FillMaskPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,top_k=5,task:str='')
transformers.FillMaskPipeline.ensure_exactly_one_mask_token(self,masked_index:np.ndarray)
transformers.pipelines.fill_mask.FillMaskPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,top_k=5,task:str='')
transformers.pipelines.fill_mask.FillMaskPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,top_k=5,task:str='')
transformers.pipelines.fill_mask.FillMaskPipeline.ensure_exactly_one_mask_token(self,masked_index:np.ndarray)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/__init__.py----------------------------------------
A:transformers.pipelines.__init__.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.__init__.tokens->task.split('_')
A:transformers.pipelines.__init__.(targeted_task, task_options)->check_task(task)
A:transformers.pipelines.__init__.model->get_default_model(targeted_task, framework, task_options)
A:transformers.pipelines.__init__.config->models.auto.configuration_auto.AutoConfig.from_pretrained(model, revision=revision, _from_pipeline=task, **model_kwargs)
A:transformers.pipelines.__init__.model_kwargs['use_auth_token']->model_kwargs.get('use_auth_token', use_auth_token)
A:transformers.pipelines.__init__.(framework, model)->infer_framework_load_model(model, model_classes=model_classes, config=config, framework=framework, revision=revision, task=task)
A:transformers.pipelines.__init__.use_fast->tokenizer[1].pop('use_fast', use_fast)
A:transformers.pipelines.__init__.tokenizer->models.auto.tokenization_auto.AutoTokenizer.from_pretrained(tokenizer_identifier, revision=revision, use_fast=use_fast, _from_pipeline=task, **tokenizer_kwargs)
A:transformers.pipelines.__init__.feature_extractor->models.auto.feature_extraction_auto.AutoFeatureExtractor.from_pretrained(feature_extractor, revision=revision, _from_pipeline=task, **model_kwargs)
transformers.pipelines.__init__.check_task(task:str)->Tuple[Dict, Any]
transformers.pipelines.__init__.pipeline(task:str,model:Optional=None,config:Optional[Union[str,PretrainedConfig]]=None,tokenizer:Optional[Union[str,PreTrainedTokenizer]]=None,feature_extractor:Optional[Union[str,PreTrainedFeatureExtractor]]=None,framework:Optional[str]=None,revision:Optional[str]=None,use_fast:bool=True,use_auth_token:Optional[Union[str,bool]]=None,model_kwargs:Dict[str,Any]={},**kwargs)->Pipeline


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/pipelines/text_generation.py----------------------------------------
A:transformers.pipelines.text_generation.prefix_inputs->self._parse_and_tokenize(prefix, padding=False, add_special_tokens=False)
A:transformers.pipelines.text_generation.inputs->self.ensure_tensor_on_device(**inputs)
A:transformers.pipelines.text_generation.output_sequences->self.model.generate(input_ids=input_ids, **generate_kwargs)
A:transformers.pipelines.text_generation.generated_sequence->generated_sequence.numpy().tolist().numpy().tolist()
A:transformers.pipelines.text_generation.text->self.tokenizer.decode(generated_sequence, skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces)
A:transformers.pipelines.text_generation.prompt_length->len(self.tokenizer.decode(input_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces))
transformers.TextGenerationPipeline(self,*args,return_full_text=True,**kwargs)
transformers.TextGenerationPipeline._parse_and_tokenize(self,*args,**kwargs)
transformers.pipelines.text_generation.TextGenerationPipeline(self,*args,return_full_text=True,**kwargs)
transformers.pipelines.text_generation.TextGenerationPipeline.__init__(self,*args,return_full_text=True,**kwargs)
transformers.pipelines.text_generation.TextGenerationPipeline._parse_and_tokenize(self,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_sentencepiece_and_tokenizers_objects.py----------------------------------------
transformers.dummy_sentencepiece_and_tokenizers_objects.convert_slow_tokenizer(*args,**kwargs)
transformers.utils.dummy_sentencepiece_and_tokenizers_objects.convert_slow_tokenizer(*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_sentencepiece_objects.py----------------------------------------
transformers.dummy_sentencepiece_objects.AlbertTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.AlbertTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.BarthezTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.BarthezTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.BertGenerationTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.BertGenerationTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.CamembertTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.CamembertTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.DebertaV2Tokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.DebertaV2Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.M2M100Tokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.M2M100Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MBart50Tokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MBart50Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MBartTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MBartTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MT5Tokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MT5Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MarianTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.MarianTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.PegasusTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.PegasusTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.ReformerTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.ReformerTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.Speech2TextTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.Speech2TextTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.T5Tokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.T5Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.XLMProphetNetTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.XLMProphetNetTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.XLMRobertaTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.XLMRobertaTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.dummy_sentencepiece_objects.XLNetTokenizer(self,*args,**kwargs)
transformers.dummy_sentencepiece_objects.XLNetTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.AlbertTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.AlbertTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.AlbertTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.BarthezTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.BarthezTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.BarthezTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.BertGenerationTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.BertGenerationTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.BertGenerationTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.CamembertTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.CamembertTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.CamembertTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.DebertaV2Tokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.DebertaV2Tokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.DebertaV2Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.M2M100Tokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.M2M100Tokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.M2M100Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MBart50Tokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MBart50Tokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MBart50Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MBartTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MBartTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MBartTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MT5Tokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MT5Tokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MT5Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MarianTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MarianTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.MarianTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.PegasusTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.PegasusTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.PegasusTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.ReformerTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.ReformerTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.ReformerTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.Speech2TextTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.Speech2TextTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.Speech2TextTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.T5Tokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.T5Tokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.T5Tokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLMProphetNetTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLMProphetNetTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLMProphetNetTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLMRobertaTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLMRobertaTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLMRobertaTokenizer.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLNetTokenizer(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLNetTokenizer.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_objects.XLNetTokenizer.from_pretrained(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/notebook.py----------------------------------------
A:transformers.utils.notebook.t->int(t)
A:transformers.utils.notebook.self.start_timeself.last_time->time.time()
A:transformers.utils.notebook.current_time->time.time()
A:transformers.utils.notebook.self.wait_for->max(int(self.update_every / self.average_time_per_item), 1)
A:transformers.utils.notebook.self.html_code->html_progress_bar(self.value, self.total, self.prefix, self.label, self.width)
A:transformers.utils.notebook.self.output->IPython.display.display(disp.HTML(self.html_code), display_id=True)
A:transformers.utils.notebook.self.child_bar->NotebookProgressBar(total, prefix=prefix, parent=self, width=width)
A:transformers.utils.notebook.self.training_tracker->NotebookTrainingTracker(state.max_steps, column_names)
A:transformers.utils.notebook.self.prediction_bar->NotebookProgressBar(len(eval_dataloader))
A:transformers.utils.notebook.values['Epoch']->int(state.epoch)
A:transformers.utils.notebook.metric_key_prefix->re.sub('\\_loss$', '', k)
A:transformers.utils.notebook._->metrics.pop(f'{metric_key_prefix}_steps_per_second', None)
A:transformers.utils.notebook.splits->k.split('_')
A:transformers.utils.notebook.name->' '.join([part.capitalize() for part in splits[1:]])
transformers.utils.notebook.NotebookProgressBar(self,total:int,prefix:Optional[str]=None,leave:bool=True,parent:Optional['NotebookTrainingTracker']=None,width:int=300)
transformers.utils.notebook.NotebookProgressBar.__init__(self,total:int,prefix:Optional[str]=None,leave:bool=True,parent:Optional['NotebookTrainingTracker']=None,width:int=300)
transformers.utils.notebook.NotebookProgressBar.close(self)
transformers.utils.notebook.NotebookProgressBar.display(self)
transformers.utils.notebook.NotebookProgressBar.update(self,value:int,force_update:bool=False,comment:str=None)
transformers.utils.notebook.NotebookProgressBar.update_bar(self,value,comment=None)
transformers.utils.notebook.NotebookProgressCallback(self)
transformers.utils.notebook.NotebookProgressCallback.__init__(self)
transformers.utils.notebook.NotebookProgressCallback.on_evaluate(self,args,state,control,metrics=None,**kwargs)
transformers.utils.notebook.NotebookProgressCallback.on_log(self,args,state,control,logs=None,**kwargs)
transformers.utils.notebook.NotebookProgressCallback.on_prediction_step(self,args,state,control,eval_dataloader=None,**kwargs)
transformers.utils.notebook.NotebookProgressCallback.on_step_end(self,args,state,control,**kwargs)
transformers.utils.notebook.NotebookProgressCallback.on_train_begin(self,args,state,control,**kwargs)
transformers.utils.notebook.NotebookProgressCallback.on_train_end(self,args,state,control,**kwargs)
transformers.utils.notebook.NotebookTrainingTracker(self,num_steps,column_names=None)
transformers.utils.notebook.NotebookTrainingTracker.__init__(self,num_steps,column_names=None)
transformers.utils.notebook.NotebookTrainingTracker.add_child(self,total,prefix=None,width=300)
transformers.utils.notebook.NotebookTrainingTracker.display(self)
transformers.utils.notebook.NotebookTrainingTracker.remove_child(self)
transformers.utils.notebook.NotebookTrainingTracker.write_line(self,values)
transformers.utils.notebook.format_time(t)
transformers.utils.notebook.html_progress_bar(value,total,prefix,label,width=300)
transformers.utils.notebook.text_to_html_table(items)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/logging.py----------------------------------------
A:transformers.utils.logging._lock->threading.Lock()
A:transformers.utils.logging.env_level_str->os.getenv('TRANSFORMERS_VERBOSITY', None)
A:transformers.utils.logging._default_handler->logging.StreamHandler()
A:transformers.utils.logging.library_root_logger->_get_library_root_logger()
A:transformers.utils.logging.name->_get_library_name()
A:transformers.utils.logging.formatter->logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s')
transformers.logging._configure_library_root_logger()->None
transformers.logging._get_default_logging_level()
transformers.logging._get_library_name()->str
transformers.logging._get_library_root_logger()->logging.Logger
transformers.logging._reset_library_root_logger()->None
transformers.logging.add_handler(handler:logging.Handler)->None
transformers.logging.disable_default_handler()->None
transformers.logging.disable_propagation()->None
transformers.logging.enable_default_handler()->None
transformers.logging.enable_explicit_format()->None
transformers.logging.enable_propagation()->None
transformers.logging.get_log_levels_dict()
transformers.logging.get_logger(name:Optional[str]=None)->logging.Logger
transformers.logging.get_verbosity()->int
transformers.logging.remove_handler(handler:logging.Handler)->None
transformers.logging.reset_format()->None
transformers.logging.set_verbosity(verbosity:int)->None
transformers.logging.set_verbosity_debug()
transformers.logging.set_verbosity_error()
transformers.logging.set_verbosity_info()
transformers.logging.set_verbosity_warning()
transformers.utils.logging._configure_library_root_logger()->None
transformers.utils.logging._get_default_logging_level()
transformers.utils.logging._get_library_name()->str
transformers.utils.logging._get_library_root_logger()->logging.Logger
transformers.utils.logging._reset_library_root_logger()->None
transformers.utils.logging.add_handler(handler:logging.Handler)->None
transformers.utils.logging.disable_default_handler()->None
transformers.utils.logging.disable_propagation()->None
transformers.utils.logging.enable_default_handler()->None
transformers.utils.logging.enable_explicit_format()->None
transformers.utils.logging.enable_propagation()->None
transformers.utils.logging.get_log_levels_dict()
transformers.utils.logging.get_logger(name:Optional[str]=None)->logging.Logger
transformers.utils.logging.get_verbosity()->int
transformers.utils.logging.remove_handler(handler:logging.Handler)->None
transformers.utils.logging.reset_format()->None
transformers.utils.logging.set_verbosity(verbosity:int)->None
transformers.utils.logging.set_verbosity_debug()
transformers.utils.logging.set_verbosity_error()
transformers.utils.logging.set_verbosity_info()
transformers.utils.logging.set_verbosity_warning()


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_speech_objects.py----------------------------------------
transformers.dummy_speech_objects.Speech2TextFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_speech_objects.Speech2TextFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_speech_objects.Speech2TextFeatureExtractor.__init__(self,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/model_parallel_utils.py----------------------------------------
A:transformers.utils.model_parallel_utils.blocks->list(range(0, num_blocks))
A:transformers.utils.model_parallel_utils.layers->list(range(n_layers))
A:transformers.utils.model_parallel_utils.n_blocks->int(ceil(n_layers / len(devices)))
A:transformers.utils.model_parallel_utils.layers_list->list((layers[i:i + n_blocks] for i in range(0, n_layers, n_blocks)))
transformers.utils.model_parallel_utils.assert_device_map(device_map,num_blocks)
transformers.utils.model_parallel_utils.get_device_map(n_layers,devices)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/fx.py----------------------------------------
A:transformers.utils.fx.logger->logging.get_logger(__name__)
A:transformers.utils.fx.method->getattr(torch.Tensor, method_name)
A:transformers.utils.fx.cache->getattr(model, cache_name)
A:transformers.utils.fx.res->getattr(model, cache_name).pop(0)
A:transformers.utils.fx.bound_method->getattr(torch.Tensor, method_name).__get__(proxy, proxy.__class__)
A:transformers.utils.fx.original_method->getattr(torch.Tensor, method_name)
A:transformers.utils.fx.cache_names->dict()
A:transformers.utils.fx.original_methods->dict()
A:transformers.utils.fx.original_methods[method_name]->getattr(torch.Tensor, method_name)
A:transformers.utils.fx.torch_version->packaging.version.parse(importlib_metadata.version('torch'))
A:transformers.utils.fx.p->HFProxy(node, self)
A:transformers.utils.fx.inputs_dict->dict()
A:transformers.utils.fx.inputs_dict['labels']->torch.zeros(self.encoder_shape, dtype=torch.long, device=device)
A:transformers.utils.fx.inputs_dict['start_positions']->torch.zeros(batch_size, dtype=torch.long, device=device)
A:transformers.utils.fx.inputs_dict['end_positions']->torch.zeros(batch_size, dtype=torch.long, device=device)
A:transformers.utils.fx.inputs_dict[input_name]->torch.ones(shape, dtype=torch.float, device=device)
A:transformers.utils.fx.inputs->dict()
A:transformers.utils.fx.clone->copy.deepcopy(model)
A:transformers.utils.fx.(cache_names, original_methods)->_monkey_patch_tensor_methods_for_model_recording(clone, method_names)
A:transformers.utils.fx.sig->inspect.signature(model.forward)
A:transformers.utils.fx.graph->super().trace(root, concrete_args=concrete_args)
A:transformers.utils.fx.path->self._insert_module_as_submodule(mod)
A:transformers.utils.fx.input_names->model.dummy_inputs.keys()
A:transformers.utils.fx.tracer->HFTracer(batch_size=batch_size, sequence_length=sequence_length, num_choices=num_choices)
A:transformers.utils.fx.traced_graph->HFTracer(batch_size=batch_size, sequence_length=sequence_length, num_choices=num_choices).trace(model, concrete_args=concrete_args)
A:transformers.utils.fx.traced->torch.fx.GraphModule(model, traced_graph)
transformers.utils.fx.HFProxy(self,node:Node,tracer:Optional[Tracer]=None)
transformers.utils.fx.HFProxy.__contains__(self,key)
transformers.utils.fx.HFProxy.__init__(self,node:Node,tracer:Optional[Tracer]=None)
transformers.utils.fx.HFProxy.__setitem__(self,key,value)
transformers.utils.fx.HFProxy.shape(self)
transformers.utils.fx.HFTracer(self,batch_size=1,sequence_length=[128,128],num_choices=-1)
transformers.utils.fx.HFTracer.__init__(self,batch_size=1,sequence_length=[128,128],num_choices=-1)
transformers.utils.fx.HFTracer._generate_dummy_input(self,model,input_name)
transformers.utils.fx.HFTracer._insert_module_as_submodule(self,mod)
transformers.utils.fx.HFTracer.create_arg(self,a:Any)->Argument
transformers.utils.fx.HFTracer.path_of_module(self,mod:nn.Module)->str
transformers.utils.fx.HFTracer.proxy(self,node:Node)
transformers.utils.fx.HFTracer.record(self,model,input_names,method_names=None)
transformers.utils.fx.HFTracer.trace(self,root:PreTrainedModel,concrete_args:Optional[Dict[str,Any]]=None,method_names=None)->Graph
transformers.utils.fx._create_recorded_proxy_method(proxy,method_name,cache_name)
transformers.utils.fx._monkey_patch_tensor_methods_for_model_recording(model,method_names)
transformers.utils.fx._reset_tensor_methods(original_methods)
transformers.utils.fx._wrap_method_for_model_recording(model,method_name,cache_name)
transformers.utils.fx._wrap_method_for_model_tracing(model,method_name,cache_name)
transformers.utils.fx.symbolic_trace(model:PreTrainedModel,input_names:Optional[List[str]]=None,batch_size:int=1,sequence_length:Union[int,List[int]]=[128,128],num_choices:int=-1)->GraphModule


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/coco_classes.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_flax_objects.py----------------------------------------
transformers.dummy_flax_objects.FlaxAutoModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForCausalLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForImageClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForImageClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForMaskedLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForMultipleChoice(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForNextSentencePrediction.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForPreTraining(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForPreTraining.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForSeq2SeqLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForSeq2SeqLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForSequenceClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForTokenClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxAutoModelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartForSequenceClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBartPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForPreTraining(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForMaskedLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForMultipleChoice(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForPreTraining(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForSequenceClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForTokenClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxBigBirdPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPTextModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPTextModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPTextPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPTextPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPVisionModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPVisionModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPVisionPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxCLIPVisionPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForMaskedLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForMultipleChoice(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForPreTraining(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForSequenceClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForTokenClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxElectraPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxForcedBOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxForcedBOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxForcedEOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxForcedEOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxGPT2LMHeadModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxGPT2LMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxGPT2Model(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxGPT2Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxGPT2PreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxGPT2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxLogitsProcessor(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxLogitsProcessorList(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxLogitsProcessorList.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxLogitsWarper(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxMinLengthLogitsProcessor(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxMinLengthLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForMaskedLM(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForMultipleChoice(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForSequenceClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForTokenClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxRobertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxT5ForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxT5Model(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxT5Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxT5PreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxT5PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxTemperatureLogitsWarper(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxTopKLogitsWarper(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxTopPLogitsWarper(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxViTForImageClassification(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxViTModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxViTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_flax_objects.FlaxViTPreTrainedModel(self,*args,**kwargs)
transformers.dummy_flax_objects.FlaxViTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForImageClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForImageClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForImageClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForNextSentencePrediction.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForPreTraining.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForSeq2SeqLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForSeq2SeqLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForSeq2SeqLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxAutoModelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBartPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxBigBirdPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPTextModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPTextModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPTextModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPTextPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPTextPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPTextPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPVisionModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPVisionModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPVisionModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPVisionPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPVisionPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxCLIPVisionPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxElectraPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxForcedBOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxForcedBOSTokenLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxForcedBOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxForcedEOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxForcedEOSTokenLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxForcedEOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2LMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2LMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2LMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2Model(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxGPT2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsProcessorList(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsProcessorList.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsProcessorList.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxMinLengthLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxMinLengthLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxMinLengthLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxRobertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5ForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5ForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5Model(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxT5PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxTemperatureLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxTemperatureLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxTopKLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxTopKLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxTopPLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxTopPLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTForImageClassification(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTForImageClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_flax_objects.FlaxViTPreTrainedModel.from_pretrained(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_timm_and_vision_objects.py----------------------------------------
transformers.utils.dummy_timm_and_vision_objects.DetrForObjectDetection(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrForObjectDetection.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrForObjectDetection.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrForSegmentation(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrForSegmentation.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrForSegmentation.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrModel(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_and_vision_objects.DetrPreTrainedModel.from_pretrained(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/hp_naming.py----------------------------------------
A:transformers.utils.hp_naming.words->param_name.split('_')
A:transformers.utils.hp_naming.shortname->separator.join(shortname_parts)
A:transformers.utils.hp_naming.short_name->TrialShortNamer.shortname_for_key(info, param_name)
A:transformers.utils.hp_naming.info->dict(short_word={}, reverse_short_word={}, short_param={}, reverse_short_param={})
A:transformers.utils.hp_naming.field_keys->list(cls.DEFAULTS.keys())
A:transformers.utils.hp_naming.values->repr.split('_')
A:transformers.utils.hp_naming.(p_k, p_v)->value.split('-')
A:transformers.utils.hp_naming.p_k->re.sub('[0-9.]', '', value)
A:transformers.utils.hp_naming.p_v->float(re.sub('[^0-9.]', '', value))
transformers.utils.hp_naming.TrialShortNamer
transformers.utils.hp_naming.TrialShortNamer.add_new_param_name(info,param_name)
transformers.utils.hp_naming.TrialShortNamer.build_naming_info(cls)
transformers.utils.hp_naming.TrialShortNamer.parse_repr(cls,repr)
transformers.utils.hp_naming.TrialShortNamer.set_defaults(cls,prefix,defaults)
transformers.utils.hp_naming.TrialShortNamer.shortname(cls,params)
transformers.utils.hp_naming.TrialShortNamer.shortname_for_key(info,param_name)
transformers.utils.hp_naming.TrialShortNamer.shortname_for_word(info,word)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/sentencepiece_model_pb2.py----------------------------------------
A:transformers.utils.sentencepiece_model_pb2._sym_db->google.protobuf.symbol_database.Default()
A:transformers.utils.sentencepiece_model_pb2.DESCRIPTOR->google.protobuf.descriptor.FileDescriptor(name='sentencepiece_model.proto', package='sentencepiece', syntax='proto2', serialized_pb=_b('\n\x19sentencepiece_model.proto\x12\rsentencepiece"\x08\n\x0bTrainerSpec\x12\r\n\x05input\x18\x01 \x03(\t\x12\x14\n\x0cinput_format\x18\x07 \x01(\t\x12\x14\n\x0cmodel_prefix\x18\x02 \x01(\t\x12A\n\nmodel_type\x18\x03 \x01(\x0e2$.sentencepiece.TrainerSpec.ModelType:\x07UNIGRAM\x12\x18\n\nvocab_size\x18\x04 \x01(\x05:\x048000\x12\x17\n\x0faccept_language\x18\x05 \x03(\t\x12 \n\x15self_test_sample_size\x18\x06 \x01(\x05:\x010\x12"\n\x12character_coverage\x18\n \x01(\x02:\x060.9995\x12\x1e\n\x13input_sentence_size\x18\x0b \x01(\x05:\x010\x12$\n\x16shuffle_input_sentence\x18\x13 \x01(\x08:\x04true\x12 \n\x14mining_sentence_size\x18\x0c \x01(\x05B\x02\x18\x01\x12"\n\x16training_sentence_size\x18\r \x01(\x05B\x02\x18\x01\x12(\n\x17seed_sentencepiece_size\x18\x0e \x01(\x05:\x071000000\x12\x1e\n\x10shrinking_factor\x18\x0f \x01(\x02:\x040.75\x12!\n\x13max_sentence_length\x18\x12 \x01(\x05:\x044192\x12\x17\n\x0bnum_threads\x18\x10 \x01(\x05:\x0216\x12\x1d\n\x12num_sub_iterations\x18\x11 \x01(\x05:\x012\x12$\n\x18max_sentencepiece_length\x18\x14 \x01(\x05:\x0216\x12%\n\x17split_by_unicode_script\x18\x15 \x01(\x08:\x04true\x12\x1d\n\x0fsplit_by_number\x18\x17 \x01(\x08:\x04true\x12!\n\x13split_by_whitespace\x18\x16 \x01(\x08:\x04true\x12)\n\x1atreat_whitespace_as_suffix\x18\x18 \x01(\x08:\x05false\x12\x17\n\x0fcontrol_symbols\x18\x1e \x03(\t\x12\x1c\n\x14user_defined_symbols\x18\x1f \x03(\t\x12\x1e\n\x10hard_vocab_limit\x18! \x01(\x08:\x04true\x12\x1c\n\ruse_all_vocab\x18" \x01(\x08:\x05false\x12\x11\n\x06unk_id\x18( \x01(\x05:\x010\x12\x11\n\x06bos_id\x18) \x01(\x05:\x011\x12\x11\n\x06eos_id\x18* \x01(\x05:\x012\x12\x12\n\x06pad_id\x18+ \x01(\x05:\x02-1\x12\x18\n\tunk_piece\x18- \x01(\t:\x05<unk>\x12\x16\n\tbos_piece\x18. \x01(\t:\x03<s>\x12\x17\n\teos_piece\x18/ \x01(\t:\x04</s>\x12\x18\n\tpad_piece\x180 \x01(\t:\x05<pad>\x12\x1a\n\x0bunk_surface\x18, \x01(\t:\x05 \x81\x87 "5\n\tModelType\x12\x0b\n\x07UNIGRAM\x10\x01\x12\x07\n\x03BPE\x10\x02\x12\x08\n\x04WORD\x10\x03\x12\x08\n\x04CHAR\x10\x04*\t\x08\x01\x10\x80\x80\x80\x80\x02"\x01\n\x0eNormalizerSpec\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x1c\n\x14precompiled_charsmap\x18\x02 \x01(\x0c\x12\x1e\n\x10add_dummy_prefix\x18\x03 \x01(\x08:\x04true\x12&\n\x18remove_extra_whitespaces\x18\x04 \x01(\x08:\x04true\x12 \n\x12escape_whitespaces\x18\x05 \x01(\x08:\x04true\x12\x1e\n\x16normalization_rule_tsv\x18\x06 \x01(\t*\t\x08\x01\x10\x80\x80\x80\x80\x02"y\n\x0cSelfTestData\x123\n\x07samples\x18\x01 \x03(\x0b2".sentencepiece.SelfTestData.Sample\x1a)\n\x06Sample\x12\r\n\x05input\x18\x01 \x01(\t\x12\x10\n\x08expected\x18\x02 \x01(\t*\t\x08\x01\x10\x80\x80\x80\x80\x02"\x03\n\nModelProto\x127\n\x06pieces\x18\x01 \x03(\x0b2\'.sentencepiece.ModelProto.SentencePiece\x120\n\x0ctrainer_spec\x18\x02 \x01(\x0b2\x1a.sentencepiece.TrainerSpec\x126\n\x0fnormalizer_spec\x18\x03 \x01(\x0b2\x1d.sentencepiece.NormalizerSpec\x123\n\x0eself_test_data\x18\x04 \x01(\x0b2\x1b.sentencepiece.SelfTestData\x1a\x01\n\rSentencePiece\x12\r\n\x05piece\x18\x01 \x01(\t\x12\r\n\x05score\x18\x02 \x01(\x02\x12B\n\x04type\x18\x03 \x01(\x0e2,.sentencepiece.ModelProto.SentencePiece.Type:\x06NORMAL"J\n\x04Type\x12\n\n\x06NORMAL\x10\x01\x12\x0b\n\x07UNKNOWN\x10\x02\x12\x0b\n\x07CONTROL\x10\x03\x12\x10\n\x0cUSER_DEFINED\x10\x04\x12\n\n\x06UNUSED\x10\x05*\t\x08\x01\x10\x80\x80\x80\x80\x02*\t\x08\x01\x10\x80\x80\x80\x80\x02B\x02H\x03'))
A:transformers.utils.sentencepiece_model_pb2._TRAINERSPEC_MODELTYPE->google.protobuf.descriptor.EnumDescriptor(name='ModelType', full_name='sentencepiece.TrainerSpec.ModelType', filename=None, file=DESCRIPTOR, values=[_descriptor.EnumValueDescriptor(name='UNIGRAM', index=0, number=1, options=None, type=None), _descriptor.EnumValueDescriptor(name='BPE', index=1, number=2, options=None, type=None), _descriptor.EnumValueDescriptor(name='WORD', index=2, number=3, options=None, type=None), _descriptor.EnumValueDescriptor(name='CHAR', index=3, number=4, options=None, type=None)], containing_type=None, options=None, serialized_start=1121, serialized_end=1174)
A:transformers.utils.sentencepiece_model_pb2._MODELPROTO_SENTENCEPIECE_TYPE->google.protobuf.descriptor.EnumDescriptor(name='Type', full_name='sentencepiece.ModelProto.SentencePiece.Type', filename=None, file=DESCRIPTOR, values=[_descriptor.EnumValueDescriptor(name='NORMAL', index=0, number=1, options=None, type=None), _descriptor.EnumValueDescriptor(name='UNKNOWN', index=1, number=2, options=None, type=None), _descriptor.EnumValueDescriptor(name='CONTROL', index=2, number=3, options=None, type=None), _descriptor.EnumValueDescriptor(name='USER_DEFINED', index=3, number=4, options=None, type=None), _descriptor.EnumValueDescriptor(name='UNUSED', index=4, number=5, options=None, type=None)], containing_type=None, options=None, serialized_start=1869, serialized_end=1943)
A:transformers.utils.sentencepiece_model_pb2._TRAINERSPEC->google.protobuf.descriptor.Descriptor(name='TrainerSpec', full_name='sentencepiece.TrainerSpec', filename=None, file=DESCRIPTOR, containing_type=None, fields=[_descriptor.FieldDescriptor(name='input', full_name='sentencepiece.TrainerSpec.input', index=0, number=1, type=9, cpp_type=9, label=3, has_default_value=False, default_value=[], message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='input_format', full_name='sentencepiece.TrainerSpec.input_format', index=1, number=7, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='model_prefix', full_name='sentencepiece.TrainerSpec.model_prefix', index=2, number=2, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='model_type', full_name='sentencepiece.TrainerSpec.model_type', index=3, number=3, type=14, cpp_type=8, label=1, has_default_value=True, default_value=1, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='vocab_size', full_name='sentencepiece.TrainerSpec.vocab_size', index=4, number=4, type=5, cpp_type=1, label=1, has_default_value=True, default_value=8000, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='accept_language', full_name='sentencepiece.TrainerSpec.accept_language', index=5, number=5, type=9, cpp_type=9, label=3, has_default_value=False, default_value=[], message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='self_test_sample_size', full_name='sentencepiece.TrainerSpec.self_test_sample_size', index=6, number=6, type=5, cpp_type=1, label=1, has_default_value=True, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='character_coverage', full_name='sentencepiece.TrainerSpec.character_coverage', index=7, number=10, type=2, cpp_type=6, label=1, has_default_value=True, default_value=float(0.9995), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='input_sentence_size', full_name='sentencepiece.TrainerSpec.input_sentence_size', index=8, number=11, type=5, cpp_type=1, label=1, has_default_value=True, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='shuffle_input_sentence', full_name='sentencepiece.TrainerSpec.shuffle_input_sentence', index=9, number=19, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='mining_sentence_size', full_name='sentencepiece.TrainerSpec.mining_sentence_size', index=10, number=12, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b('\x18\x01'))), _descriptor.FieldDescriptor(name='training_sentence_size', full_name='sentencepiece.TrainerSpec.training_sentence_size', index=11, number=13, type=5, cpp_type=1, label=1, has_default_value=False, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b('\x18\x01'))), _descriptor.FieldDescriptor(name='seed_sentencepiece_size', full_name='sentencepiece.TrainerSpec.seed_sentencepiece_size', index=12, number=14, type=5, cpp_type=1, label=1, has_default_value=True, default_value=1000000, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='shrinking_factor', full_name='sentencepiece.TrainerSpec.shrinking_factor', index=13, number=15, type=2, cpp_type=6, label=1, has_default_value=True, default_value=float(0.75), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='max_sentence_length', full_name='sentencepiece.TrainerSpec.max_sentence_length', index=14, number=18, type=5, cpp_type=1, label=1, has_default_value=True, default_value=4192, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='num_threads', full_name='sentencepiece.TrainerSpec.num_threads', index=15, number=16, type=5, cpp_type=1, label=1, has_default_value=True, default_value=16, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='num_sub_iterations', full_name='sentencepiece.TrainerSpec.num_sub_iterations', index=16, number=17, type=5, cpp_type=1, label=1, has_default_value=True, default_value=2, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='max_sentencepiece_length', full_name='sentencepiece.TrainerSpec.max_sentencepiece_length', index=17, number=20, type=5, cpp_type=1, label=1, has_default_value=True, default_value=16, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='split_by_unicode_script', full_name='sentencepiece.TrainerSpec.split_by_unicode_script', index=18, number=21, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='split_by_number', full_name='sentencepiece.TrainerSpec.split_by_number', index=19, number=23, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='split_by_whitespace', full_name='sentencepiece.TrainerSpec.split_by_whitespace', index=20, number=22, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='treat_whitespace_as_suffix', full_name='sentencepiece.TrainerSpec.treat_whitespace_as_suffix', index=21, number=24, type=8, cpp_type=7, label=1, has_default_value=True, default_value=False, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='control_symbols', full_name='sentencepiece.TrainerSpec.control_symbols', index=22, number=30, type=9, cpp_type=9, label=3, has_default_value=False, default_value=[], message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='user_defined_symbols', full_name='sentencepiece.TrainerSpec.user_defined_symbols', index=23, number=31, type=9, cpp_type=9, label=3, has_default_value=False, default_value=[], message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='hard_vocab_limit', full_name='sentencepiece.TrainerSpec.hard_vocab_limit', index=24, number=33, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='use_all_vocab', full_name='sentencepiece.TrainerSpec.use_all_vocab', index=25, number=34, type=8, cpp_type=7, label=1, has_default_value=True, default_value=False, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='unk_id', full_name='sentencepiece.TrainerSpec.unk_id', index=26, number=40, type=5, cpp_type=1, label=1, has_default_value=True, default_value=0, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='bos_id', full_name='sentencepiece.TrainerSpec.bos_id', index=27, number=41, type=5, cpp_type=1, label=1, has_default_value=True, default_value=1, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='eos_id', full_name='sentencepiece.TrainerSpec.eos_id', index=28, number=42, type=5, cpp_type=1, label=1, has_default_value=True, default_value=2, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='pad_id', full_name='sentencepiece.TrainerSpec.pad_id', index=29, number=43, type=5, cpp_type=1, label=1, has_default_value=True, default_value=-1, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='unk_piece', full_name='sentencepiece.TrainerSpec.unk_piece', index=30, number=45, type=9, cpp_type=9, label=1, has_default_value=True, default_value=_b('<unk>').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='bos_piece', full_name='sentencepiece.TrainerSpec.bos_piece', index=31, number=46, type=9, cpp_type=9, label=1, has_default_value=True, default_value=_b('<s>').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='eos_piece', full_name='sentencepiece.TrainerSpec.eos_piece', index=32, number=47, type=9, cpp_type=9, label=1, has_default_value=True, default_value=_b('</s>').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='pad_piece', full_name='sentencepiece.TrainerSpec.pad_piece', index=33, number=48, type=9, cpp_type=9, label=1, has_default_value=True, default_value=_b('<pad>').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='unk_surface', full_name='sentencepiece.TrainerSpec.unk_surface', index=34, number=44, type=9, cpp_type=9, label=1, has_default_value=True, default_value=_b(' \x81\x87 ').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None)], extensions=[], nested_types=[], enum_types=[_TRAINERSPEC_MODELTYPE], options=None, is_extendable=True, syntax='proto2', extension_ranges=[(200, 536870912)], oneofs=[], serialized_start=45, serialized_end=1185)
A:transformers.utils.sentencepiece_model_pb2._NORMALIZERSPEC->google.protobuf.descriptor.Descriptor(name='NormalizerSpec', full_name='sentencepiece.NormalizerSpec', filename=None, file=DESCRIPTOR, containing_type=None, fields=[_descriptor.FieldDescriptor(name='name', full_name='sentencepiece.NormalizerSpec.name', index=0, number=1, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='precompiled_charsmap', full_name='sentencepiece.NormalizerSpec.precompiled_charsmap', index=1, number=2, type=12, cpp_type=9, label=1, has_default_value=False, default_value=_b(''), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='add_dummy_prefix', full_name='sentencepiece.NormalizerSpec.add_dummy_prefix', index=2, number=3, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='remove_extra_whitespaces', full_name='sentencepiece.NormalizerSpec.remove_extra_whitespaces', index=3, number=4, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='escape_whitespaces', full_name='sentencepiece.NormalizerSpec.escape_whitespaces', index=4, number=5, type=8, cpp_type=7, label=1, has_default_value=True, default_value=True, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='normalization_rule_tsv', full_name='sentencepiece.NormalizerSpec.normalization_rule_tsv', index=5, number=6, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None)], extensions=[], nested_types=[], enum_types=[], options=None, is_extendable=True, syntax='proto2', extension_ranges=[(200, 536870912)], oneofs=[], serialized_start=1188, serialized_end=1397)
A:transformers.utils.sentencepiece_model_pb2._SELFTESTDATA_SAMPLE->google.protobuf.descriptor.Descriptor(name='Sample', full_name='sentencepiece.SelfTestData.Sample', filename=None, file=DESCRIPTOR, containing_type=None, fields=[_descriptor.FieldDescriptor(name='input', full_name='sentencepiece.SelfTestData.Sample.input', index=0, number=1, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='expected', full_name='sentencepiece.SelfTestData.Sample.expected', index=1, number=2, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None)], extensions=[], nested_types=[], enum_types=[], options=None, is_extendable=False, syntax='proto2', extension_ranges=[], oneofs=[], serialized_start=1468, serialized_end=1509)
A:transformers.utils.sentencepiece_model_pb2._SELFTESTDATA->google.protobuf.descriptor.Descriptor(name='SelfTestData', full_name='sentencepiece.SelfTestData', filename=None, file=DESCRIPTOR, containing_type=None, fields=[_descriptor.FieldDescriptor(name='samples', full_name='sentencepiece.SelfTestData.samples', index=0, number=1, type=11, cpp_type=10, label=3, has_default_value=False, default_value=[], message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None)], extensions=[], nested_types=[_SELFTESTDATA_SAMPLE], enum_types=[], options=None, is_extendable=True, syntax='proto2', extension_ranges=[(200, 536870912)], oneofs=[], serialized_start=1399, serialized_end=1520)
A:transformers.utils.sentencepiece_model_pb2._MODELPROTO_SENTENCEPIECE->google.protobuf.descriptor.Descriptor(name='SentencePiece', full_name='sentencepiece.ModelProto.SentencePiece', filename=None, file=DESCRIPTOR, containing_type=None, fields=[_descriptor.FieldDescriptor(name='piece', full_name='sentencepiece.ModelProto.SentencePiece.piece', index=0, number=1, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b('').decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='score', full_name='sentencepiece.ModelProto.SentencePiece.score', index=1, number=2, type=2, cpp_type=6, label=1, has_default_value=False, default_value=float(0), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='type', full_name='sentencepiece.ModelProto.SentencePiece.type', index=2, number=3, type=14, cpp_type=8, label=1, has_default_value=True, default_value=1, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None)], extensions=[], nested_types=[], enum_types=[_MODELPROTO_SENTENCEPIECE_TYPE], options=None, is_extendable=True, syntax='proto2', extension_ranges=[(200, 536870912)], oneofs=[], serialized_start=1754, serialized_end=1954)
A:transformers.utils.sentencepiece_model_pb2._MODELPROTO->google.protobuf.descriptor.Descriptor(name='ModelProto', full_name='sentencepiece.ModelProto', filename=None, file=DESCRIPTOR, containing_type=None, fields=[_descriptor.FieldDescriptor(name='pieces', full_name='sentencepiece.ModelProto.pieces', index=0, number=1, type=11, cpp_type=10, label=3, has_default_value=False, default_value=[], message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='trainer_spec', full_name='sentencepiece.ModelProto.trainer_spec', index=1, number=2, type=11, cpp_type=10, label=1, has_default_value=False, default_value=None, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='normalizer_spec', full_name='sentencepiece.ModelProto.normalizer_spec', index=2, number=3, type=11, cpp_type=10, label=1, has_default_value=False, default_value=None, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), _descriptor.FieldDescriptor(name='self_test_data', full_name='sentencepiece.ModelProto.self_test_data', index=3, number=4, type=11, cpp_type=10, label=1, has_default_value=False, default_value=None, message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None)], extensions=[], nested_types=[_MODELPROTO_SENTENCEPIECE], enum_types=[], options=None, is_extendable=True, syntax='proto2', extension_ranges=[(200, 536870912)], oneofs=[], serialized_start=1523, serialized_end=1965)
A:transformers.utils.sentencepiece_model_pb2.TrainerSpec->google.protobuf.reflection.GeneratedProtocolMessageType('TrainerSpec', (_message.Message,), dict(DESCRIPTOR=_TRAINERSPEC, __module__='sentencepiece_model_pb2'))
A:transformers.utils.sentencepiece_model_pb2.NormalizerSpec->google.protobuf.reflection.GeneratedProtocolMessageType('NormalizerSpec', (_message.Message,), dict(DESCRIPTOR=_NORMALIZERSPEC, __module__='sentencepiece_model_pb2'))
A:transformers.utils.sentencepiece_model_pb2.SelfTestData->google.protobuf.reflection.GeneratedProtocolMessageType('SelfTestData', (_message.Message,), dict(Sample=_reflection.GeneratedProtocolMessageType('Sample', (_message.Message,), dict(DESCRIPTOR=_SELFTESTDATA_SAMPLE, __module__='sentencepiece_model_pb2')), DESCRIPTOR=_SELFTESTDATA, __module__='sentencepiece_model_pb2'))
A:transformers.utils.sentencepiece_model_pb2.ModelProto->google.protobuf.reflection.GeneratedProtocolMessageType('ModelProto', (_message.Message,), dict(SentencePiece=_reflection.GeneratedProtocolMessageType('SentencePiece', (_message.Message,), dict(DESCRIPTOR=_MODELPROTO_SENTENCEPIECE, __module__='sentencepiece_model_pb2')), DESCRIPTOR=_MODELPROTO, __module__='sentencepiece_model_pb2'))
A:transformers.utils.sentencepiece_model_pb2.DESCRIPTOR._options->google.protobuf.descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('H\x03'))
A:transformers.utils.sentencepiece_model_pb2._TRAINERSPEC.fields_by_name['mining_sentence_size']._options->google.protobuf.descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b('\x18\x01'))
A:transformers.utils.sentencepiece_model_pb2._TRAINERSPEC.fields_by_name['training_sentence_size']._options->google.protobuf.descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b('\x18\x01'))


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/imagenet_classes.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_pt_objects.py----------------------------------------
transformers.dummy_pt_objects.Adafactor(self,*args,**kwargs)
transformers.dummy_pt_objects.AdamW(self,*args,**kwargs)
transformers.dummy_pt_objects.AdaptiveEmbedding(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AlbertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AlbertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.AlbertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModel(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForImageClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForImageClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForNextSentencePrediction.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForPreTraining.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForSeq2SeqLM(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForSeq2SeqLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForTableQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForTableQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelWithLMHead(self,*args,**kwargs)
transformers.dummy_pt_objects.AutoModelWithLMHead.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BartForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BartForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BartForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.BartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BartForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.BartForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BartForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.BartForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BartModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BartModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BartPretrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BartPretrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BeamScorer(self,*args,**kwargs)
transformers.dummy_pt_objects.BeamSearchScorer(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.BertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertGenerationDecoder(self,*args,**kwargs)
transformers.dummy_pt_objects.BertGenerationEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.BertGenerationPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BertGenerationPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BertLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.BertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPegasusPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BigBirdPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.BlenderbotSmallPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CLIPModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CLIPModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CLIPPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CLIPPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CLIPTextModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CLIPTextModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CLIPVisionModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CLIPVisionModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CTRLForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.CTRLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CTRLLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CTRLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CTRLModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CTRLModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CTRLPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CTRLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.CamembertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.CamembertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Conv1D(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ConvBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DPRContextEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.DPRPretrainedContextEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.DPRPretrainedQuestionEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.DPRPretrainedReader(self,*args,**kwargs)
transformers.dummy_pt_objects.DPRQuestionEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.DPRReader(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollator(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForLanguageModeling(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForLanguageModeling.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForPermutationLanguageModeling(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForPermutationLanguageModeling.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForSOP(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForSeq2Seq(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorForWholeWordMask(self,*args,**kwargs)
transformers.dummy_pt_objects.DataCollatorWithPadding(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2ForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2Model(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2PreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DebertaV2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DeiTForImageClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DeiTForImageClassificationWithTeacher(self,*args,**kwargs)
transformers.dummy_pt_objects.DeiTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DeiTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DeiTPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DeiTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.DistilBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ElectraPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ElectraPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.EncoderDecoderModel(self,*args,**kwargs)
transformers.dummy_pt_objects.EncoderDecoderModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FSMTForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.FSMTForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FSMTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.FSMTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertWithLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.FlaubertWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ForcedBOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.ForcedBOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ForcedEOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.ForcedEOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelBaseModel(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelBaseModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelModel(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.FunnelPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.FunnelPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPT2DoubleHeadsModel(self,*args,**kwargs)
transformers.dummy_pt_objects.GPT2DoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPT2ForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.GPT2ForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPT2LMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.GPT2LMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPT2Model(self,*args,**kwargs)
transformers.dummy_pt_objects.GPT2Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPT2PreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.GPT2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoModel(self,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.GPTNeoPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.GlueDataTrainingArguments(self,*args,**kwargs)
transformers.dummy_pt_objects.GlueDataset(self,*args,**kwargs)
transformers.dummy_pt_objects.HammingDiversityLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.HammingDiversityLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.HubertForCTC(self,*args,**kwargs)
transformers.dummy_pt_objects.HubertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.HubertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.HubertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.HubertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.IBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.IBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.InfNanRemoveLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.InfNanRemoveLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LEDForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.LEDForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LEDForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.LEDForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LEDForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LEDForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LEDModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LEDModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LEDPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LEDPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LayoutLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LineByLineTextDataset(self,*args,**kwargs)
transformers.dummy_pt_objects.LineByLineWithRefDataset(self,*args,**kwargs)
transformers.dummy_pt_objects.LineByLineWithSOPTextDataset(self,*args,**kwargs)
transformers.dummy_pt_objects.LogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.LogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LogitsProcessorList(self,*args,**kwargs)
transformers.dummy_pt_objects.LogitsProcessorList.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LogitsWarper(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LongformerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LongformerSelfAttention(self,*args,**kwargs)
transformers.dummy_pt_objects.LukeForEntityClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LukeForEntityPairClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LukeForEntitySpanClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.LukeModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LukeModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LukePreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LukePreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LxmertEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.LxmertForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.LxmertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.LxmertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LxmertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LxmertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LxmertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.LxmertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.LxmertVisualFeatureEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.LxmertXLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.M2M100ForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.M2M100ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.M2M100Model(self,*args,**kwargs)
transformers.dummy_pt_objects.M2M100Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.M2M100PreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.M2M100PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MBartForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.MBartForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MBartForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.MBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MBartForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.MBartForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MBartForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MBartForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MBartModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MBartModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MBartPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MBartPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MMBTForClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MMBTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MMBTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MPNetPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MPNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MT5EncoderModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MT5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MT5ForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.MT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MT5Model(self,*args,**kwargs)
transformers.dummy_pt_objects.MT5Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MarianForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.MarianForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MarianMTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MarianMTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MarianModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MarianModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MaxLengthCriteria(self,*args,**kwargs)
transformers.dummy_pt_objects.MaxTimeCriteria(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MegatronBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MinLengthLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.MinLengthLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.MobileBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ModalEmbeddings(self,*args,**kwargs)
transformers.dummy_pt_objects.NoBadWordsLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.NoBadWordsLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.NoRepeatNGramLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.NoRepeatNGramLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTDoubleHeadsModel(self,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTDoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.OpenAIGPTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PegasusForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.PegasusForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PegasusForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.PegasusForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PegasusModel(self,*args,**kwargs)
transformers.dummy_pt_objects.PegasusModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PegasusPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.PegasusPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PrefixConstrainedLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.PrefixConstrainedLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PretrainedBartModel(self,*args,**kwargs)
transformers.dummy_pt_objects.PretrainedBartModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PretrainedFSMTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.PretrainedFSMTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetDecoder(self,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ProphetNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.PyTorchBenchmark(self,*args,**kwargs)
transformers.dummy_pt_objects.PyTorchBenchmarkArguments(self,*args,**kwargs)
transformers.dummy_pt_objects.RagModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RagModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RagPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RagPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RagSequenceForGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.RagTokenForGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerAttention(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ReformerForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ReformerForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ReformerLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ReformerModelWithLMHead(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerModelWithLMHead.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ReformerPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ReformerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RepetitionPenaltyLogitsProcessor(self,*args,**kwargs)
transformers.dummy_pt_objects.RepetitionPenaltyLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RetriBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RetriBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RetriBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RetriBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RoFormerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.RobertaPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.RobertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Seq2SeqTrainer(self,*args,**kwargs)
transformers.dummy_pt_objects.Speech2TextForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.Speech2TextForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Speech2TextModel(self,*args,**kwargs)
transformers.dummy_pt_objects.Speech2TextModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Speech2TextPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.Speech2TextPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SquadDataTrainingArguments(self,*args,**kwargs)
transformers.dummy_pt_objects.SquadDataset(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertModule(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.SqueezeBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.StoppingCriteria(self,*args,**kwargs)
transformers.dummy_pt_objects.StoppingCriteriaList(self,*args,**kwargs)
transformers.dummy_pt_objects.T5EncoderModel(self,*args,**kwargs)
transformers.dummy_pt_objects.T5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.T5ForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.T5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.T5Model(self,*args,**kwargs)
transformers.dummy_pt_objects.T5Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.T5PreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.T5PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TapasForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.TapasForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TapasForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.TapasForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TapasForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.TapasForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TapasModel(self,*args,**kwargs)
transformers.dummy_pt_objects.TapasModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TapasPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.TapasPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TemperatureLogitsWarper(self,*args,**kwargs)
transformers.dummy_pt_objects.TextDataset(self,*args,**kwargs)
transformers.dummy_pt_objects.TextDatasetForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_pt_objects.TopKLogitsWarper(self,*args,**kwargs)
transformers.dummy_pt_objects.TopPLogitsWarper(self,*args,**kwargs)
transformers.dummy_pt_objects.Trainer(self,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLModel(self,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.TransfoXLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ViTForImageClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.ViTModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ViTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.ViTPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.ViTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForRegionToPhraseAlignment(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertForVisualReasoning(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertLayer(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertModel(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.VisualBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2ForCTC(self,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2ForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2ForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2ForPreTraining(self,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2Model(self,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2PreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.Wav2Vec2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetDecoder(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetEncoder(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMProphetNetModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForCausalLM(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForMaskedLM(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLMWithLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLMWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForMultipleChoice(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForSequenceClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForTokenClassification(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetLMHeadModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.XLNetPreTrainedModel(self,*args,**kwargs)
transformers.dummy_pt_objects.XLNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_pt_objects.apply_chunking_to_forward(*args,**kwargs)
transformers.dummy_pt_objects.default_data_collator(*args,**kwargs)
transformers.dummy_pt_objects.get_constant_schedule(*args,**kwargs)
transformers.dummy_pt_objects.get_constant_schedule_with_warmup(*args,**kwargs)
transformers.dummy_pt_objects.get_cosine_schedule_with_warmup(*args,**kwargs)
transformers.dummy_pt_objects.get_cosine_with_hard_restarts_schedule_with_warmup(*args,**kwargs)
transformers.dummy_pt_objects.get_linear_schedule_with_warmup(*args,**kwargs)
transformers.dummy_pt_objects.get_polynomial_decay_schedule_with_warmup(*args,**kwargs)
transformers.dummy_pt_objects.get_scheduler(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_albert(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_bert(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_bert_generation(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_big_bird(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_convbert(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_electra(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_funnel(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_gpt2(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_gpt_neo(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_mobilebert(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_openai_gpt(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_roformer(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_t5(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_transfo_xl(*args,**kwargs)
transformers.dummy_pt_objects.load_tf_weights_in_xlnet(*args,**kwargs)
transformers.dummy_pt_objects.prune_layer(*args,**kwargs)
transformers.dummy_pt_objects.top_k_top_p_filtering(*args,**kwargs)
transformers.dummy_pt_objects.torch_distributed_zero_first(*args,**kwargs)
transformers.utils.dummy_pt_objects.Adafactor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Adafactor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AdamW(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AdamW.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AdaptiveEmbedding(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AdaptiveEmbedding.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AlbertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForImageClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForImageClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForImageClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForNextSentencePrediction.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForPreTraining.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForSeq2SeqLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForSeq2SeqLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForSeq2SeqLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForTableQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForTableQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForTableQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelWithLMHead(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelWithLMHead.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.AutoModelWithLMHead.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartPretrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartPretrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BartPretrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BeamScorer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BeamScorer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BeamSearchScorer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BeamSearchScorer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationDecoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationDecoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertGenerationPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPegasusPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BigBirdPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.BlenderbotSmallPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPTextModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPTextModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPTextModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPVisionModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPVisionModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CLIPVisionModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CTRLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.CamembertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Conv1D(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Conv1D.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ConvBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRContextEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRContextEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRPretrainedContextEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRPretrainedContextEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRPretrainedQuestionEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRPretrainedQuestionEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRPretrainedReader(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRPretrainedReader.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRQuestionEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRQuestionEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRReader(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DPRReader.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollator(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollator.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForLanguageModeling(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForLanguageModeling.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForLanguageModeling.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForPermutationLanguageModeling(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForPermutationLanguageModeling.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForPermutationLanguageModeling.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForSOP(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForSOP.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForSeq2Seq(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForSeq2Seq.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForWholeWordMask(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorForWholeWordMask.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorWithPadding(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DataCollatorWithPadding.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2ForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2Model(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DebertaV2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTForImageClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTForImageClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTForImageClassificationWithTeacher(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTForImageClassificationWithTeacher.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DeiTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.DistilBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ElectraPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.EncoderDecoderModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.EncoderDecoderModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.EncoderDecoderModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FSMTForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FSMTForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FSMTForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FSMTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FSMTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FSMTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForQuestionAnsweringSimple.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertWithLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertWithLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FlaubertWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ForcedBOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ForcedBOSTokenLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ForcedBOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ForcedEOSTokenLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ForcedEOSTokenLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ForcedEOSTokenLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelBaseModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelBaseModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelBaseModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.FunnelPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2DoubleHeadsModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2DoubleHeadsModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2DoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2ForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2ForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2ForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2LMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2LMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2LMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2Model(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPT2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GPTNeoPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.GlueDataTrainingArguments(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GlueDataTrainingArguments.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GlueDataset(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.GlueDataset.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HammingDiversityLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HammingDiversityLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HammingDiversityLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertForCTC(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertForCTC.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.HubertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.IBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.InfNanRemoveLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.InfNanRemoveLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.InfNanRemoveLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LEDPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LayoutLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LineByLineTextDataset(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LineByLineTextDataset.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LineByLineWithRefDataset(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LineByLineWithRefDataset.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LineByLineWithSOPTextDataset(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LineByLineWithSOPTextDataset.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsProcessorList(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsProcessorList.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsProcessorList.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerSelfAttention(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LongformerSelfAttention.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeForEntityClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeForEntityClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeForEntityPairClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeForEntityPairClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeForEntitySpanClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeForEntitySpanClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukeModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukePreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukePreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LukePreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertVisualFeatureEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertVisualFeatureEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertXLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.LxmertXLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100ForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100ForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100Model(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.M2M100PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MBartPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MMBTForClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MMBTForClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MMBTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MMBTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MMBTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MPNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5EncoderModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5EncoderModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5ForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5ForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5Model(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MT5Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianMTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianMTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianMTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MarianModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MaxLengthCriteria(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MaxLengthCriteria.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MaxTimeCriteria(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MaxTimeCriteria.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MegatronBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MinLengthLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MinLengthLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MinLengthLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.MobileBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ModalEmbeddings(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ModalEmbeddings.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.NoBadWordsLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.NoBadWordsLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.NoBadWordsLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.NoRepeatNGramLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.NoRepeatNGramLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.NoRepeatNGramLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTDoubleHeadsModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTDoubleHeadsModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTDoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.OpenAIGPTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PegasusPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PrefixConstrainedLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PrefixConstrainedLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PrefixConstrainedLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PretrainedBartModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PretrainedBartModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PretrainedBartModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PretrainedFSMTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PretrainedFSMTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PretrainedFSMTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetDecoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetDecoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ProphetNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.PyTorchBenchmark(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PyTorchBenchmark.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PyTorchBenchmarkArguments(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.PyTorchBenchmarkArguments.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagSequenceForGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagSequenceForGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagTokenForGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RagTokenForGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerAttention(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerAttention.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerModelWithLMHead(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerModelWithLMHead.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerModelWithLMHead.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ReformerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RepetitionPenaltyLogitsProcessor(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RepetitionPenaltyLogitsProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RepetitionPenaltyLogitsProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RetriBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RetriBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RetriBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RetriBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RetriBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RetriBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RoFormerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.RobertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Seq2SeqTrainer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Seq2SeqTrainer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Speech2TextPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SquadDataTrainingArguments(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SquadDataTrainingArguments.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SquadDataset(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SquadDataset.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertModule(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertModule.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.SqueezeBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.StoppingCriteria(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.StoppingCriteria.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.StoppingCriteriaList(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.StoppingCriteriaList.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5EncoderModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5EncoderModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5ForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5ForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5Model(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.T5PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TapasPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TemperatureLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TemperatureLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TextDataset(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TextDataset.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TextDatasetForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TextDatasetForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TopKLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TopKLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TopPLogitsWarper(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TopPLogitsWarper.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Trainer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Trainer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.TransfoXLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTForImageClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTForImageClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.ViTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForRegionToPhraseAlignment(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForRegionToPhraseAlignment.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForVisualReasoning(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertForVisualReasoning.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertLayer(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.VisualBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForCTC(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForCTC.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2ForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2Model(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.Wav2Vec2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForQuestionAnsweringSimple.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetDecoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetDecoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetEncoder(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMProphetNetModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMWithLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMWithLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLMWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForQuestionAnsweringSimple.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_pt_objects.XLNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_pt_objects.apply_chunking_to_forward(*args,**kwargs)
transformers.utils.dummy_pt_objects.default_data_collator(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_constant_schedule(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_constant_schedule_with_warmup(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_cosine_schedule_with_warmup(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_cosine_with_hard_restarts_schedule_with_warmup(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_linear_schedule_with_warmup(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_polynomial_decay_schedule_with_warmup(*args,**kwargs)
transformers.utils.dummy_pt_objects.get_scheduler(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_albert(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_bert(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_bert_generation(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_big_bird(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_convbert(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_electra(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_funnel(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_gpt2(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_gpt_neo(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_mobilebert(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_openai_gpt(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_roformer(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_t5(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_transfo_xl(*args,**kwargs)
transformers.utils.dummy_pt_objects.load_tf_weights_in_xlnet(*args,**kwargs)
transformers.utils.dummy_pt_objects.prune_layer(*args,**kwargs)
transformers.utils.dummy_pt_objects.top_k_top_p_filtering(*args,**kwargs)
transformers.utils.dummy_pt_objects.torch_distributed_zero_first(*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_vision_objects.py----------------------------------------
transformers.dummy_vision_objects.CLIPFeatureExtractor(self,*args,**kwargs)
transformers.dummy_vision_objects.CLIPProcessor(self,*args,**kwargs)
transformers.dummy_vision_objects.CLIPProcessor.from_pretrained(cls,*args,**kwargs)
transformers.dummy_vision_objects.DeiTFeatureExtractor(self,*args,**kwargs)
transformers.dummy_vision_objects.DetrFeatureExtractor(self,*args,**kwargs)
transformers.dummy_vision_objects.ImageFeatureExtractionMixin(self,*args,**kwargs)
transformers.dummy_vision_objects.ViTFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.CLIPFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.CLIPFeatureExtractor.__init__(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.CLIPProcessor(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.CLIPProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.CLIPProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_vision_objects.DeiTFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.DeiTFeatureExtractor.__init__(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.DetrFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.DetrFeatureExtractor.__init__(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.ImageFeatureExtractionMixin(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.ImageFeatureExtractionMixin.__init__(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.ViTFeatureExtractor(self,*args,**kwargs)
transformers.utils.dummy_vision_objects.ViTFeatureExtractor.__init__(self,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_timm_objects.py----------------------------------------
transformers.dummy_timm_objects.DetrForObjectDetection(self,*args,**kwargs)
transformers.dummy_timm_objects.DetrForObjectDetection.from_pretrained(cls,*args,**kwargs)
transformers.dummy_timm_objects.DetrForSegmentation(self,*args,**kwargs)
transformers.dummy_timm_objects.DetrForSegmentation.from_pretrained(cls,*args,**kwargs)
transformers.dummy_timm_objects.DetrModel(self,*args,**kwargs)
transformers.dummy_timm_objects.DetrModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrForObjectDetection(self,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrForObjectDetection.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrForObjectDetection.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrForSegmentation(self,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrForSegmentation.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrForSegmentation.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrModel(self,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_timm_objects.DetrModel.from_pretrained(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_sentencepiece_and_speech_objects.py----------------------------------------
transformers.dummy_sentencepiece_and_speech_objects.Speech2TextProcessor(self,*args,**kwargs)
transformers.dummy_sentencepiece_and_speech_objects.Speech2TextProcessor.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_sentencepiece_and_speech_objects.Speech2TextProcessor(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_and_speech_objects.Speech2TextProcessor.__init__(self,*args,**kwargs)
transformers.utils.dummy_sentencepiece_and_speech_objects.Speech2TextProcessor.from_pretrained(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/__init__.py----------------------------------------
transformers.utils.__init__.check_min_version(min_version)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/versions.py----------------------------------------
A:transformers.utils.versions.match->re.findall('^([\\s!=<>]{1,2})(.+)', w)
A:transformers.utils.versions.want_range->want_full.split(',')
A:transformers.utils.versions.got_ver->importlib.metadata.version(pkg)
transformers.utils.versions._compare_versions(op,got_ver,want_ver,requirement,pkg,hint)
transformers.utils.versions.require_version(requirement:str,hint:Optional[str]=None)->None
transformers.utils.versions.require_version_core(requirement)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_tf_objects.py----------------------------------------
transformers.dummy_tf_objects.AdamWeightDecay(self,*args,**kwargs)
transformers.dummy_tf_objects.GradientAccumulator(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAdaptiveEmbedding(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAlbertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForCausalLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForPreTraining.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForSeq2SeqLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForSeq2SeqLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelWithLMHead(self,*args,**kwargs)
transformers.dummy_tf_objects.TFAutoModelWithLMHead.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBartForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBartModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBartModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBartPretrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBartPretrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertEmbeddings(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotSmallForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotSmallForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotSmallModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotSmallModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotSmallPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFBlenderbotSmallPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCTRLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFCamembertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFConvBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDPRContextEncoder(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDPRPretrainedContextEncoder(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDPRPretrainedQuestionEncoder(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDPRPretrainedReader(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDPRQuestionEncoder(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDPRReader(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFDistilBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFElectraPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertWithLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFlaubertWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelBaseModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelBaseModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFFunnelPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2DoubleHeadsModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2DoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2ForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2ForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2LMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2LMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2MainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2Model(self,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2PreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFGPT2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLEDForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLEDForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLEDModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLEDModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLEDPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLEDPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLayoutLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLongformerSelfAttention(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFLxmertVisualFeatureEncoder(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMBartForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMBartModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMBartModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMBartPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMBartPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMPNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMT5EncoderModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMT5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMT5ForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMT5Model(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMT5Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMarianMTModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMarianMTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMarianModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMarianModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMarianPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMarianPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForNextSentencePrediction(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForPreTraining(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFMobileBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTDoubleHeadsModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTDoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFOpenAIGPTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFPegasusForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFPegasusForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFPegasusModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFPegasusModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFPegasusPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFPegasusPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRagModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRagModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRagPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRagPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRagSequenceForGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRagTokenForGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForCausalLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRoFormerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFRobertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFSequenceSummary(self,*args,**kwargs)
transformers.dummy_tf_objects.TFSharedEmbeddings(self,*args,**kwargs)
transformers.dummy_tf_objects.TFT5EncoderModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFT5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFT5ForConditionalGeneration(self,*args,**kwargs)
transformers.dummy_tf_objects.TFT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFT5Model(self,*args,**kwargs)
transformers.dummy_tf_objects.TFT5Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFT5PreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFT5PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFTrainer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFTransfoXLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFWav2Vec2ForCTC(self,*args,**kwargs)
transformers.dummy_tf_objects.TFWav2Vec2Model(self,*args,**kwargs)
transformers.dummy_tf_objects.TFWav2Vec2Model.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFWav2Vec2PreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFWav2Vec2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForMaskedLM(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMWithLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLMWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForMultipleChoice(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForSequenceClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForTokenClassification(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetLMHeadModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetMainLayer(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetPreTrainedModel(self,*args,**kwargs)
transformers.dummy_tf_objects.TFXLNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tf_objects.TensorFlowBenchmark(self,*args,**kwargs)
transformers.dummy_tf_objects.TensorFlowBenchmarkArguments(self,*args,**kwargs)
transformers.dummy_tf_objects.WarmUp(self,*args,**kwargs)
transformers.dummy_tf_objects.create_optimizer(*args,**kwargs)
transformers.dummy_tf_objects.shape_list(*args,**kwargs)
transformers.dummy_tf_objects.tf_top_k_top_p_filtering(*args,**kwargs)
transformers.utils.dummy_tf_objects.AdamWeightDecay(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.AdamWeightDecay.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.GradientAccumulator(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.GradientAccumulator.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAdaptiveEmbedding(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAdaptiveEmbedding.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAlbertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForPreTraining.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForSeq2SeqLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForSeq2SeqLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForSeq2SeqLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelWithLMHead(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelWithLMHead.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFAutoModelWithLMHead.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartPretrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartPretrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBartPretrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertEmbeddings(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertEmbeddings.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFBlenderbotSmallPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCTRLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFCamembertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFConvBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRContextEncoder(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRContextEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRPretrainedContextEncoder(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRPretrainedContextEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRPretrainedQuestionEncoder(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRPretrainedQuestionEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRPretrainedReader(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRPretrainedReader.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRQuestionEncoder(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRQuestionEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRReader(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDPRReader.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFDistilBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFElectraPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForQuestionAnsweringSimple.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertWithLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertWithLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFlaubertWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelBaseModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelBaseModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelBaseModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFFunnelPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2DoubleHeadsModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2DoubleHeadsModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2DoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2ForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2ForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2ForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2LMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2LMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2LMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2MainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2MainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2Model(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFGPT2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLEDPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLayoutLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerSelfAttention(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLongformerSelfAttention.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertVisualFeatureEncoder(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFLxmertVisualFeatureEncoder.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMBartPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMPNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5EncoderModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5EncoderModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5ForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5ForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5Model(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMT5Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianMTModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianMTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianMTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMarianPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForNextSentencePrediction(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForNextSentencePrediction.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForPreTraining(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForPreTraining.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFMobileBertPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTDoubleHeadsModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTDoubleHeadsModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTDoubleHeadsModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFOpenAIGPTPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPegasusPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagSequenceForGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagSequenceForGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagTokenForGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRagTokenForGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForCausalLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForCausalLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForCausalLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRoFormerPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFRobertaPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFSequenceSummary(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFSequenceSummary.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFSharedEmbeddings(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFSharedEmbeddings.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5EncoderModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5EncoderModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5EncoderModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5ForConditionalGeneration(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5ForConditionalGeneration.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5ForConditionalGeneration.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5Model(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFT5PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTrainer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTrainer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFTransfoXLPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2ForCTC(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2ForCTC.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2Model(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2Model.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2Model.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2PreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2PreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFWav2Vec2PreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForQuestionAnsweringSimple.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForMaskedLM(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForMaskedLM.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForMaskedLM.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForQuestionAnswering(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForQuestionAnswering.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForQuestionAnswering.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMRobertaModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMWithLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMWithLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLMWithLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForMultipleChoice(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForMultipleChoice.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForMultipleChoice.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForQuestionAnsweringSimple(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForQuestionAnsweringSimple.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForQuestionAnsweringSimple.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForSequenceClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForSequenceClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForSequenceClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForTokenClassification(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForTokenClassification.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetForTokenClassification.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetLMHeadModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetLMHeadModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetLMHeadModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetMainLayer(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetMainLayer.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetPreTrainedModel(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetPreTrainedModel.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TFXLNetPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tf_objects.TensorFlowBenchmark(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TensorFlowBenchmark.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TensorFlowBenchmarkArguments(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.TensorFlowBenchmarkArguments.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.WarmUp(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.WarmUp.__init__(self,*args,**kwargs)
transformers.utils.dummy_tf_objects.create_optimizer(*args,**kwargs)
transformers.utils.dummy_tf_objects.shape_list(*args,**kwargs)
transformers.utils.dummy_tf_objects.tf_top_k_top_p_filtering(*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/modeling_auto_mapping.py----------------------------------------
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForQuestionAnswering'), ('BigBirdPegasusConfig', 'BigBirdPegasusForQuestionAnswering'), ('BigBirdConfig', 'BigBirdForQuestionAnswering'), ('ConvBertConfig', 'ConvBertForQuestionAnswering'), ('LEDConfig', 'LEDForQuestionAnswering'), ('DistilBertConfig', 'DistilBertForQuestionAnswering'), ('AlbertConfig', 'AlbertForQuestionAnswering'), ('CamembertConfig', 'CamembertForQuestionAnswering'), ('BartConfig', 'BartForQuestionAnswering'), ('MBartConfig', 'MBartForQuestionAnswering'), ('LongformerConfig', 'LongformerForQuestionAnswering'), ('XLMRobertaConfig', 'XLMRobertaForQuestionAnswering'), ('RobertaConfig', 'RobertaForQuestionAnswering'), ('SqueezeBertConfig', 'SqueezeBertForQuestionAnswering'), ('BertConfig', 'BertForQuestionAnswering'), ('XLNetConfig', 'XLNetForQuestionAnsweringSimple'), ('FlaubertConfig', 'FlaubertForQuestionAnsweringSimple'), ('MegatronBertConfig', 'MegatronBertForQuestionAnswering'), ('MobileBertConfig', 'MobileBertForQuestionAnswering'), ('XLMConfig', 'XLMForQuestionAnsweringSimple'), ('ElectraConfig', 'ElectraForQuestionAnswering'), ('ReformerConfig', 'ReformerForQuestionAnswering'), ('FunnelConfig', 'FunnelForQuestionAnswering'), ('LxmertConfig', 'LxmertForQuestionAnswering'), ('MPNetConfig', 'MPNetForQuestionAnswering'), ('DebertaConfig', 'DebertaForQuestionAnswering'), ('DebertaV2Config', 'DebertaV2ForQuestionAnswering'), ('IBertConfig', 'IBertForQuestionAnswering')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_CAUSAL_LM_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForCausalLM'), ('BigBirdPegasusConfig', 'BigBirdPegasusForCausalLM'), ('GPTNeoConfig', 'GPTNeoForCausalLM'), ('BigBirdConfig', 'BigBirdForCausalLM'), ('CamembertConfig', 'CamembertForCausalLM'), ('XLMRobertaConfig', 'XLMRobertaForCausalLM'), ('RobertaConfig', 'RobertaForCausalLM'), ('BertConfig', 'BertLMHeadModel'), ('OpenAIGPTConfig', 'OpenAIGPTLMHeadModel'), ('GPT2Config', 'GPT2LMHeadModel'), ('TransfoXLConfig', 'TransfoXLLMHeadModel'), ('XLNetConfig', 'XLNetLMHeadModel'), ('XLMConfig', 'XLMWithLMHeadModel'), ('CTRLConfig', 'CTRLLMHeadModel'), ('ReformerConfig', 'ReformerModelWithLMHead'), ('BertGenerationConfig', 'BertGenerationDecoder'), ('XLMProphetNetConfig', 'XLMProphetNetForCausalLM'), ('ProphetNetConfig', 'ProphetNetForCausalLM'), ('BartConfig', 'BartForCausalLM'), ('MBartConfig', 'MBartForCausalLM'), ('PegasusConfig', 'PegasusForCausalLM'), ('MarianConfig', 'MarianForCausalLM'), ('BlenderbotConfig', 'BlenderbotForCausalLM'), ('BlenderbotSmallConfig', 'BlenderbotSmallForCausalLM'), ('MegatronBertConfig', 'MegatronBertForCausalLM')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES->OrderedDict([('ViTConfig', 'ViTForImageClassification'), ('DeiTConfig', "('DeiTForImageClassification', 'DeiTForImageClassificationWithTeacher')")])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_MASKED_LM_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForMaskedLM'), ('BigBirdConfig', 'BigBirdForMaskedLM'), ('Wav2Vec2Config', 'Wav2Vec2ForMaskedLM'), ('ConvBertConfig', 'ConvBertForMaskedLM'), ('LayoutLMConfig', 'LayoutLMForMaskedLM'), ('DistilBertConfig', 'DistilBertForMaskedLM'), ('AlbertConfig', 'AlbertForMaskedLM'), ('BartConfig', 'BartForConditionalGeneration'), ('MBartConfig', 'MBartForConditionalGeneration'), ('CamembertConfig', 'CamembertForMaskedLM'), ('XLMRobertaConfig', 'XLMRobertaForMaskedLM'), ('LongformerConfig', 'LongformerForMaskedLM'), ('RobertaConfig', 'RobertaForMaskedLM'), ('SqueezeBertConfig', 'SqueezeBertForMaskedLM'), ('BertConfig', 'BertForMaskedLM'), ('MegatronBertConfig', 'MegatronBertForMaskedLM'), ('MobileBertConfig', 'MobileBertForMaskedLM'), ('FlaubertConfig', 'FlaubertWithLMHeadModel'), ('XLMConfig', 'XLMWithLMHeadModel'), ('ElectraConfig', 'ElectraForMaskedLM'), ('ReformerConfig', 'ReformerForMaskedLM'), ('FunnelConfig', 'FunnelForMaskedLM'), ('MPNetConfig', 'MPNetForMaskedLM'), ('TapasConfig', 'TapasForMaskedLM'), ('DebertaConfig', 'DebertaForMaskedLM'), ('DebertaV2Config', 'DebertaV2ForMaskedLM'), ('IBertConfig', 'IBertForMaskedLM')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForMultipleChoice'), ('BigBirdConfig', 'BigBirdForMultipleChoice'), ('ConvBertConfig', 'ConvBertForMultipleChoice'), ('CamembertConfig', 'CamembertForMultipleChoice'), ('ElectraConfig', 'ElectraForMultipleChoice'), ('XLMRobertaConfig', 'XLMRobertaForMultipleChoice'), ('LongformerConfig', 'LongformerForMultipleChoice'), ('RobertaConfig', 'RobertaForMultipleChoice'), ('SqueezeBertConfig', 'SqueezeBertForMultipleChoice'), ('BertConfig', 'BertForMultipleChoice'), ('DistilBertConfig', 'DistilBertForMultipleChoice'), ('MegatronBertConfig', 'MegatronBertForMultipleChoice'), ('MobileBertConfig', 'MobileBertForMultipleChoice'), ('XLNetConfig', 'XLNetForMultipleChoice'), ('AlbertConfig', 'AlbertForMultipleChoice'), ('XLMConfig', 'XLMForMultipleChoice'), ('FlaubertConfig', 'FlaubertForMultipleChoice'), ('FunnelConfig', 'FunnelForMultipleChoice'), ('MPNetConfig', 'MPNetForMultipleChoice'), ('IBertConfig', 'IBertForMultipleChoice')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES->OrderedDict([('BertConfig', 'BertForNextSentencePrediction'), ('MegatronBertConfig', 'MegatronBertForNextSentencePrediction'), ('MobileBertConfig', 'MobileBertForNextSentencePrediction')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES->OrderedDict([('DetrConfig', 'DetrForObjectDetection')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES->OrderedDict([('BigBirdPegasusConfig', 'BigBirdPegasusForConditionalGeneration'), ('M2M100Config', 'M2M100ForConditionalGeneration'), ('LEDConfig', 'LEDForConditionalGeneration'), ('BlenderbotSmallConfig', 'BlenderbotSmallForConditionalGeneration'), ('MT5Config', 'MT5ForConditionalGeneration'), ('T5Config', 'T5ForConditionalGeneration'), ('PegasusConfig', 'PegasusForConditionalGeneration'), ('MarianConfig', 'MarianMTModel'), ('MBartConfig', 'MBartForConditionalGeneration'), ('BlenderbotConfig', 'BlenderbotForConditionalGeneration'), ('BartConfig', 'BartForConditionalGeneration'), ('FSMTConfig', 'FSMTForConditionalGeneration'), ('EncoderDecoderConfig', 'EncoderDecoderModel'), ('XLMProphetNetConfig', 'XLMProphetNetForConditionalGeneration'), ('ProphetNetConfig', 'ProphetNetForConditionalGeneration')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForSequenceClassification'), ('BigBirdPegasusConfig', 'BigBirdPegasusForSequenceClassification'), ('BigBirdConfig', 'BigBirdForSequenceClassification'), ('ConvBertConfig', 'ConvBertForSequenceClassification'), ('LEDConfig', 'LEDForSequenceClassification'), ('DistilBertConfig', 'DistilBertForSequenceClassification'), ('AlbertConfig', 'AlbertForSequenceClassification'), ('CamembertConfig', 'CamembertForSequenceClassification'), ('XLMRobertaConfig', 'XLMRobertaForSequenceClassification'), ('MBartConfig', 'MBartForSequenceClassification'), ('BartConfig', 'BartForSequenceClassification'), ('LongformerConfig', 'LongformerForSequenceClassification'), ('RobertaConfig', 'RobertaForSequenceClassification'), ('SqueezeBertConfig', 'SqueezeBertForSequenceClassification'), ('LayoutLMConfig', 'LayoutLMForSequenceClassification'), ('BertConfig', 'BertForSequenceClassification'), ('XLNetConfig', 'XLNetForSequenceClassification'), ('MegatronBertConfig', 'MegatronBertForSequenceClassification'), ('MobileBertConfig', 'MobileBertForSequenceClassification'), ('FlaubertConfig', 'FlaubertForSequenceClassification'), ('XLMConfig', 'XLMForSequenceClassification'), ('ElectraConfig', 'ElectraForSequenceClassification'), ('FunnelConfig', 'FunnelForSequenceClassification'), ('DebertaConfig', 'DebertaForSequenceClassification'), ('DebertaV2Config', 'DebertaV2ForSequenceClassification'), ('GPT2Config', 'GPT2ForSequenceClassification'), ('GPTNeoConfig', 'GPTNeoForSequenceClassification'), ('OpenAIGPTConfig', 'OpenAIGPTForSequenceClassification'), ('ReformerConfig', 'ReformerForSequenceClassification'), ('CTRLConfig', 'CTRLForSequenceClassification'), ('TransfoXLConfig', 'TransfoXLForSequenceClassification'), ('MPNetConfig', 'MPNetForSequenceClassification'), ('TapasConfig', 'TapasForSequenceClassification'), ('IBertConfig', 'IBertForSequenceClassification')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES->OrderedDict([('TapasConfig', 'TapasForQuestionAnswering')])
A:transformers.utils.modeling_auto_mapping.MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForTokenClassification'), ('BigBirdConfig', 'BigBirdForTokenClassification'), ('ConvBertConfig', 'ConvBertForTokenClassification'), ('LayoutLMConfig', 'LayoutLMForTokenClassification'), ('DistilBertConfig', 'DistilBertForTokenClassification'), ('CamembertConfig', 'CamembertForTokenClassification'), ('FlaubertConfig', 'FlaubertForTokenClassification'), ('XLMConfig', 'XLMForTokenClassification'), ('XLMRobertaConfig', 'XLMRobertaForTokenClassification'), ('LongformerConfig', 'LongformerForTokenClassification'), ('RobertaConfig', 'RobertaForTokenClassification'), ('SqueezeBertConfig', 'SqueezeBertForTokenClassification'), ('BertConfig', 'BertForTokenClassification'), ('MegatronBertConfig', 'MegatronBertForTokenClassification'), ('MobileBertConfig', 'MobileBertForTokenClassification'), ('XLNetConfig', 'XLNetForTokenClassification'), ('AlbertConfig', 'AlbertForTokenClassification'), ('ElectraConfig', 'ElectraForTokenClassification'), ('FunnelConfig', 'FunnelForTokenClassification'), ('MPNetConfig', 'MPNetForTokenClassification'), ('DebertaConfig', 'DebertaForTokenClassification'), ('DebertaV2Config', 'DebertaV2ForTokenClassification'), ('IBertConfig', 'IBertForTokenClassification')])
A:transformers.utils.modeling_auto_mapping.MODEL_MAPPING_NAMES->OrderedDict([('VisualBertConfig', 'VisualBertModel'), ('RoFormerConfig', 'RoFormerModel'), ('CLIPConfig', 'CLIPModel'), ('BigBirdPegasusConfig', 'BigBirdPegasusModel'), ('DeiTConfig', 'DeiTModel'), ('LukeConfig', 'LukeModel'), ('DetrConfig', 'DetrModel'), ('GPTNeoConfig', 'GPTNeoModel'), ('BigBirdConfig', 'BigBirdModel'), ('Speech2TextConfig', 'Speech2TextModel'), ('ViTConfig', 'ViTModel'), ('Wav2Vec2Config', 'Wav2Vec2Model'), ('HubertConfig', 'HubertModel'), ('M2M100Config', 'M2M100Model'), ('ConvBertConfig', 'ConvBertModel'), ('LEDConfig', 'LEDModel'), ('BlenderbotSmallConfig', 'BlenderbotSmallModel'), ('RetriBertConfig', 'RetriBertModel'), ('MT5Config', 'MT5Model'), ('T5Config', 'T5Model'), ('PegasusConfig', 'PegasusModel'), ('MarianConfig', 'MarianModel'), ('MBartConfig', 'MBartModel'), ('BlenderbotConfig', 'BlenderbotModel'), ('DistilBertConfig', 'DistilBertModel'), ('AlbertConfig', 'AlbertModel'), ('CamembertConfig', 'CamembertModel'), ('XLMRobertaConfig', 'XLMRobertaModel'), ('BartConfig', 'BartModel'), ('LongformerConfig', 'LongformerModel'), ('RobertaConfig', 'RobertaModel'), ('LayoutLMConfig', 'LayoutLMModel'), ('SqueezeBertConfig', 'SqueezeBertModel'), ('BertConfig', 'BertModel'), ('OpenAIGPTConfig', 'OpenAIGPTModel'), ('GPT2Config', 'GPT2Model'), ('MegatronBertConfig', 'MegatronBertModel'), ('MobileBertConfig', 'MobileBertModel'), ('TransfoXLConfig', 'TransfoXLModel'), ('XLNetConfig', 'XLNetModel'), ('FlaubertConfig', 'FlaubertModel'), ('FSMTConfig', 'FSMTModel'), ('XLMConfig', 'XLMModel'), ('CTRLConfig', 'CTRLModel'), ('ElectraConfig', 'ElectraModel'), ('ReformerConfig', 'ReformerModel'), ('FunnelConfig', "('FunnelModel', 'FunnelBaseModel')"), ('LxmertConfig', 'LxmertModel'), ('BertGenerationConfig', 'BertGenerationEncoder'), ('DebertaConfig', 'DebertaModel'), ('DebertaV2Config', 'DebertaV2Model'), ('DPRConfig', 'DPRQuestionEncoder'), ('XLMProphetNetConfig', 'XLMProphetNetModel'), ('ProphetNetConfig', 'ProphetNetModel'), ('MPNetConfig', 'MPNetModel'), ('TapasConfig', 'TapasModel'), ('IBertConfig', 'IBertModel')])
A:transformers.utils.modeling_auto_mapping.MODEL_WITH_LM_HEAD_MAPPING_NAMES->OrderedDict([('RoFormerConfig', 'RoFormerForMaskedLM'), ('BigBirdPegasusConfig', 'BigBirdPegasusForConditionalGeneration'), ('GPTNeoConfig', 'GPTNeoForCausalLM'), ('BigBirdConfig', 'BigBirdForMaskedLM'), ('Speech2TextConfig', 'Speech2TextForConditionalGeneration'), ('Wav2Vec2Config', 'Wav2Vec2ForMaskedLM'), ('M2M100Config', 'M2M100ForConditionalGeneration'), ('ConvBertConfig', 'ConvBertForMaskedLM'), ('LEDConfig', 'LEDForConditionalGeneration'), ('BlenderbotSmallConfig', 'BlenderbotSmallForConditionalGeneration'), ('LayoutLMConfig', 'LayoutLMForMaskedLM'), ('T5Config', 'T5ForConditionalGeneration'), ('DistilBertConfig', 'DistilBertForMaskedLM'), ('AlbertConfig', 'AlbertForMaskedLM'), ('CamembertConfig', 'CamembertForMaskedLM'), ('XLMRobertaConfig', 'XLMRobertaForMaskedLM'), ('MarianConfig', 'MarianMTModel'), ('FSMTConfig', 'FSMTForConditionalGeneration'), ('BartConfig', 'BartForConditionalGeneration'), ('LongformerConfig', 'LongformerForMaskedLM'), ('RobertaConfig', 'RobertaForMaskedLM'), ('SqueezeBertConfig', 'SqueezeBertForMaskedLM'), ('BertConfig', 'BertForMaskedLM'), ('OpenAIGPTConfig', 'OpenAIGPTLMHeadModel'), ('GPT2Config', 'GPT2LMHeadModel'), ('MegatronBertConfig', 'MegatronBertForCausalLM'), ('MobileBertConfig', 'MobileBertForMaskedLM'), ('TransfoXLConfig', 'TransfoXLLMHeadModel'), ('XLNetConfig', 'XLNetLMHeadModel'), ('FlaubertConfig', 'FlaubertWithLMHeadModel'), ('XLMConfig', 'XLMWithLMHeadModel'), ('CTRLConfig', 'CTRLLMHeadModel'), ('ElectraConfig', 'ElectraForMaskedLM'), ('EncoderDecoderConfig', 'EncoderDecoderModel'), ('ReformerConfig', 'ReformerModelWithLMHead'), ('FunnelConfig', 'FunnelForMaskedLM'), ('MPNetConfig', 'MPNetForMaskedLM'), ('TapasConfig', 'TapasForMaskedLM'), ('DebertaConfig', 'DebertaForMaskedLM'), ('DebertaV2Config', 'DebertaV2ForMaskedLM'), ('IBertConfig', 'IBertForMaskedLM')])


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/utils/dummy_tokenizers_objects.py----------------------------------------
transformers.dummy_tokenizers_objects.AlbertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.AlbertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.BartTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.BartTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.BarthezTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.BarthezTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.BertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.BertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.BigBirdTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.BigBirdTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.CLIPTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.CLIPTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.CamembertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.CamembertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.ConvBertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.ConvBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.DPRContextEncoderTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.DPRContextEncoderTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.DPRQuestionEncoderTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.DPRQuestionEncoderTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.DPRReaderTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.DPRReaderTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.DebertaTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.DebertaTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.DistilBertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.DistilBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.ElectraTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.ElectraTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.FunnelTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.FunnelTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.GPT2TokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.GPT2TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.HerbertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.HerbertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.LEDTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.LEDTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.LayoutLMTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.LayoutLMTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.LongformerTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.LongformerTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.LxmertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.LxmertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.MBart50TokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.MBart50TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.MBartTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.MBartTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.MPNetTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.MPNetTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.MT5TokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.MT5TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.MobileBertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.MobileBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.OpenAIGPTTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.OpenAIGPTTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.PegasusTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.PegasusTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.PreTrainedTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.PreTrainedTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.ReformerTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.ReformerTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.RetriBertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.RetriBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.RoFormerTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.RoFormerTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.RobertaTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.RobertaTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.SqueezeBertTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.SqueezeBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.T5TokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.T5TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.XLMRobertaTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.XLMRobertaTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.dummy_tokenizers_objects.XLNetTokenizerFast(self,*args,**kwargs)
transformers.dummy_tokenizers_objects.XLNetTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.AlbertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.AlbertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.AlbertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BartTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BartTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BartTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BarthezTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BarthezTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BarthezTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BigBirdTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BigBirdTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.BigBirdTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.CLIPTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.CLIPTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.CLIPTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.CamembertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.CamembertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.CamembertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ConvBertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ConvBertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ConvBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRContextEncoderTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRContextEncoderTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRContextEncoderTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRQuestionEncoderTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRQuestionEncoderTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRQuestionEncoderTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRReaderTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRReaderTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DPRReaderTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DebertaTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DebertaTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DebertaTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DistilBertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DistilBertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.DistilBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ElectraTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ElectraTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ElectraTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.FunnelTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.FunnelTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.FunnelTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.GPT2TokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.GPT2TokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.GPT2TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.HerbertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.HerbertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.HerbertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LEDTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LEDTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LEDTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LayoutLMTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LayoutLMTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LayoutLMTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LongformerTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LongformerTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LongformerTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LxmertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LxmertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.LxmertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MBart50TokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MBart50TokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MBart50TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MBartTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MBartTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MBartTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MPNetTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MPNetTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MPNetTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MT5TokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MT5TokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MT5TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MobileBertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MobileBertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.MobileBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.OpenAIGPTTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.OpenAIGPTTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.OpenAIGPTTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.PegasusTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.PegasusTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.PegasusTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.PreTrainedTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.PreTrainedTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.PreTrainedTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ReformerTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ReformerTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.ReformerTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RetriBertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RetriBertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RetriBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RoFormerTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RoFormerTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RoFormerTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RobertaTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RobertaTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.RobertaTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.SqueezeBertTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.SqueezeBertTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.SqueezeBertTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.T5TokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.T5TokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.T5TokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.XLMRobertaTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.XLMRobertaTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.XLMRobertaTokenizerFast.from_pretrained(cls,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.XLNetTokenizerFast(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.XLNetTokenizerFast.__init__(self,*args,**kwargs)
transformers.utils.dummy_tokenizers_objects.XLNetTokenizerFast.from_pretrained(cls,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/sagemaker/training_args_sm.py----------------------------------------
A:transformers.sagemaker.training_args_sm.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.sagemaker.training_args_sm.smp_options->json.loads(smp_options)
A:transformers.sagemaker.training_args_sm.mpi_options->json.loads(mpi_options)
A:transformers.sagemaker.training_args_sm.device->torch.device('cuda', self.local_rank)
A:transformers.sagemaker.training_args_sm.local_rank->smdistributed.modelparallel.torch.local_rank()
A:transformers.sagemaker.training_args_sm.self.local_rank->smdistributed.dataparallel.torch.distributed.get_local_rank()
A:transformers.sagemaker.training_args_sm.self._n_gpu->torch.cuda.device_count()
transformers.sagemaker.SageMakerTrainingArguments(TrainingArguments)
transformers.sagemaker.SageMakerTrainingArguments.__post_init__(self)
transformers.sagemaker.SageMakerTrainingArguments._no_sync_in_gradient_accumulation(self)
transformers.sagemaker.SageMakerTrainingArguments._setup_devices(self)->'torch.device'
transformers.sagemaker.SageMakerTrainingArguments.place_model_on_device(self)
transformers.sagemaker.SageMakerTrainingArguments.world_size(self)
transformers.sagemaker.training_args_sm.SageMakerTrainingArguments(TrainingArguments)
transformers.sagemaker.training_args_sm.SageMakerTrainingArguments.__post_init__(self)
transformers.sagemaker.training_args_sm.SageMakerTrainingArguments._no_sync_in_gradient_accumulation(self)
transformers.sagemaker.training_args_sm.SageMakerTrainingArguments._setup_devices(self)->'torch.device'
transformers.sagemaker.training_args_sm.SageMakerTrainingArguments.place_model_on_device(self)
transformers.sagemaker.training_args_sm.SageMakerTrainingArguments.world_size(self)
transformers.sagemaker.training_args_sm.is_sagemaker_model_parallel_available()


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/sagemaker/trainer_sm.py----------------------------------------
A:transformers.sagemaker.trainer_sm.logger->utils.logging.get_logger(__name__)
transformers.sagemaker.SageMakerTrainer(self,args=None,**kwargs)
transformers.sagemaker.trainer_sm.SageMakerTrainer(self,args=None,**kwargs)
transformers.sagemaker.trainer_sm.SageMakerTrainer.__init__(self,args=None,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/sagemaker/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/add_new_model.py----------------------------------------
A:transformers.commands.add_new_model.logger->utils.logging.get_logger(__name__)
A:transformers.commands.add_new_model.add_new_model_parser->parser.add_parser('add-new-model')
A:transformers.commands.add_new_model.testing_configuration->json.load(configuration_file)
A:transformers.commands.add_new_model.configuration->json.load(configuration_file)
A:transformers.commands.add_new_model.lines->f.readlines()
A:transformers.commands.add_new_model.(fh, abs_path)->mkstemp()
A:transformers.commands.add_new_model.skip_file->skip_units(line)
A:transformers.commands.add_new_model.skip_snippet->skip_units(line)
transformers.commands.add_new_model.AddNewModelCommand(self,testing:bool,testing_file:str,path=None,*args)
transformers.commands.add_new_model.AddNewModelCommand.__init__(self,testing:bool,testing_file:str,path=None,*args)
transformers.commands.add_new_model.AddNewModelCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.add_new_model.AddNewModelCommand.run(self)
transformers.commands.add_new_model.add_new_model_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/serving.py----------------------------------------
A:transformers.commands.serving.logger->utils.logging.get_logger('transformers-cli/serving')
A:transformers.commands.serving.nlp->pipeline(task=args.task, model=args.model if args.model else None, config=args.config, tokenizer=args.tokenizer, device=args.device)
A:transformers.commands.serving.serve_parser->parser.add_parser('serve', help='CLI tool to run inference requests through REST and GraphQL endpoints.')
A:transformers.commands.serving.self._app->FastAPI(routes=[APIRoute('/', self.model_info, response_model=ServeModelInfoResult, response_class=JSONResponse, methods=['GET']), APIRoute('/tokenize', self.tokenize, response_model=ServeTokenizeResult, response_class=JSONResponse, methods=['POST']), APIRoute('/detokenize', self.detokenize, response_model=ServeDeTokenizeResult, response_class=JSONResponse, methods=['POST']), APIRoute('/forward', self.forward, response_model=ServeForwardResult, response_class=JSONResponse, methods=['POST'])], timeout=600)
A:transformers.commands.serving.tokens_txt->self._pipeline.tokenizer.tokenize(text_input)
A:transformers.commands.serving.tokens_ids->self._pipeline.tokenizer.convert_tokens_to_ids(tokens_txt)
A:transformers.commands.serving.decoded_str->self._pipeline.tokenizer.decode(tokens_ids, skip_special_tokens, cleanup_tokenization_spaces)
A:transformers.commands.serving.output->self._pipeline(inputs)
transformers.commands.serving.ServeCommand(self,pipeline:Pipeline,host:str,port:int,workers:int)
transformers.commands.serving.ServeCommand.__init__(self,pipeline:Pipeline,host:str,port:int,workers:int)
transformers.commands.serving.ServeCommand.detokenize(self,tokens_ids:List[int]=Body(None,embed=True),skip_special_tokens:bool=Body(False,embed=True),cleanup_tokenization_spaces:bool=Body(True,embed=True))
transformers.commands.serving.ServeCommand.model_info(self)
transformers.commands.serving.ServeCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.serving.ServeCommand.run(self)
transformers.commands.serving.ServeCommand.tokenize(self,text_input:str=Body(None,embed=True),return_ids:bool=Body(False,embed=True))
transformers.commands.serving.ServeDeTokenizeResult(BaseModel)
transformers.commands.serving.ServeForwardResult(BaseModel)
transformers.commands.serving.ServeModelInfoResult(BaseModel)
transformers.commands.serving.ServeTokenizeResult(BaseModel)
transformers.commands.serving.serve_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/run.py----------------------------------------
A:transformers.commands.run.logger->utils.logging.get_logger(__name__)
A:transformers.commands.run.nlp->pipeline(task=args.task, model=args.model if args.model else None, config=args.config, tokenizer=args.tokenizer, device=args.device)
A:transformers.commands.run.reader->pipelines.PipelineDataFormat.from_str(format=format, output_path=args.output, input_path=args.input, column=args.column if args.column else nlp.default_input_names, overwrite=args.overwrite)
A:transformers.commands.run.run_parser->parser.add_parser('run', help='Run a pipeline through the CLI')
A:transformers.commands.run.binary_path->self._reader.save_binary(outputs)
transformers.commands.run.RunCommand(self,nlp:Pipeline,reader:PipelineDataFormat)
transformers.commands.run.RunCommand.__init__(self,nlp:Pipeline,reader:PipelineDataFormat)
transformers.commands.run.RunCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.run.RunCommand.run(self)
transformers.commands.run.run_command_factory(args)
transformers.commands.run.try_infer_format_from_ext(path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/user.py----------------------------------------
A:transformers.commands.user.login_parser->parser.add_parser('login', help='Log in using the same credentials as on huggingface.co')
A:transformers.commands.user.whoami_parser->parser.add_parser('whoami', help='Find out which huggingface.co account you are logged in as.')
A:transformers.commands.user.logout_parser->parser.add_parser('logout', help='Log out')
A:transformers.commands.user.s3_parser->parser.add_parser('s3_datasets', help='{ls, rm} Commands to interact with the files you upload on S3.')
A:transformers.commands.user.s3_subparsers->parser.add_parser('s3_datasets', help='{ls, rm} Commands to interact with the files you upload on S3.').add_subparsers(help='s3 related commands')
A:transformers.commands.user.ls_parser->repo_parser.add_subparsers(help='Deprecated: use `huggingface-cli` instead. huggingface.co repos related commands').add_parser('ls-files', help='Deprecated: use `huggingface-cli` instead. List all your files on huggingface.co')
A:transformers.commands.user.rm_parser->parser.add_parser('s3_datasets', help='{ls, rm} Commands to interact with the files you upload on S3.').add_subparsers(help='s3 related commands').add_parser('rm')
A:transformers.commands.user.upload_parser->parser.add_parser('upload', help='Deprecated: used to be the way to upload a model to S3. We now use a git-based system for storing models and other artifacts. Use the `repo create` command instead.')
A:transformers.commands.user.repo_parser->parser.add_parser('repo', help='Deprecated: use `huggingface-cli` instead. {create, ls-files} Commands to interact with your huggingface.co repos.')
A:transformers.commands.user.repo_subparsers->parser.add_parser('repo', help='Deprecated: use `huggingface-cli` instead. {create, ls-files} Commands to interact with your huggingface.co repos.').add_subparsers(help='Deprecated: use `huggingface-cli` instead. huggingface.co repos related commands')
A:transformers.commands.user.repo_create_parser->parser.add_parser('repo', help='Deprecated: use `huggingface-cli` instead. {create, ls-files} Commands to interact with your huggingface.co repos.').add_subparsers(help='Deprecated: use `huggingface-cli` instead. huggingface.co repos related commands').add_parser('create', help='Deprecated: use `huggingface-cli` instead. Create a new repo on huggingface.co')
A:transformers.commands.user.row_format->('{{:{}}} ' * len(headers)).format(*col_widths)
A:transformers.commands.user.self._api->HfApi()
A:transformers.commands.user.username->input('Username: ')
A:transformers.commands.user.password->getpass()
A:transformers.commands.user.token->hf_api.HfFolder.get_token()
A:transformers.commands.user.(user, orgs)->self._api.whoami(token)
A:transformers.commands.user.objs->self._api.list_repos_objs(token, organization=self.args.organization)
A:transformers.commands.user.stdout->subprocess.check_output(['git-lfs', '--version']).decode('utf-8')
A:transformers.commands.user.(user, _)->self._api.whoami(token)
A:transformers.commands.user.choice->input('Proceed? [Y/n] ').lower()
A:transformers.commands.user.url->self._api.create_repo(token, name=self.args.name, organization=self.args.organization)
A:transformers.commands.user.local_path->os.path.abspath(self.args.path)
A:transformers.commands.user.rel_path->os.path.basename(local_path)
A:transformers.commands.user.files->self.walk_dir(rel_path)
A:transformers.commands.user.access_url->self._api.presign_and_upload(token=token, filename=filename, filepath=filepath, organization=self.args.organization)
transformers.commands.user.ANSI
transformers.commands.user.ANSI.bold(cls,s)
transformers.commands.user.ANSI.gray(cls,s)
transformers.commands.user.ANSI.red(cls,s)
transformers.commands.user.BaseUserCommand(self,args)
transformers.commands.user.BaseUserCommand.__init__(self,args)
transformers.commands.user.DeleteObjCommand(BaseUserCommand)
transformers.commands.user.DeleteObjCommand.run(self)
transformers.commands.user.DeprecatedUploadCommand(BaseUserCommand)
transformers.commands.user.DeprecatedUploadCommand.run(self)
transformers.commands.user.ListObjsCommand(BaseUserCommand)
transformers.commands.user.ListObjsCommand.run(self)
transformers.commands.user.ListReposObjsCommand(BaseUserCommand)
transformers.commands.user.ListReposObjsCommand.run(self)
transformers.commands.user.LoginCommand(BaseUserCommand)
transformers.commands.user.LoginCommand.run(self)
transformers.commands.user.LogoutCommand(BaseUserCommand)
transformers.commands.user.LogoutCommand.run(self)
transformers.commands.user.RepoCreateCommand(BaseUserCommand)
transformers.commands.user.RepoCreateCommand.run(self)
transformers.commands.user.UploadCommand(BaseUserCommand)
transformers.commands.user.UploadCommand.run(self)
transformers.commands.user.UploadCommand.walk_dir(self,rel_path)
transformers.commands.user.UserCommands(BaseTransformersCLICommand)
transformers.commands.user.UserCommands.register_subcommand(parser:ArgumentParser)
transformers.commands.user.WhoamiCommand(BaseUserCommand)
transformers.commands.user.WhoamiCommand.run(self)
transformers.commands.user.tabulate(rows:List[List[Union[str,int]]],headers:List[str])->str


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/env.py----------------------------------------
A:transformers.commands.env.download_parser->parser.add_parser('env')
A:transformers.commands.env.pt_cuda_available->torch.cuda.is_available()
A:transformers.commands.env.tf_cuda_available->bool(tf.config.list_physical_devices('GPU'))
transformers.commands.env.EnvironmentCommand(BaseTransformersCLICommand)
transformers.commands.env.EnvironmentCommand.format_dict(d)
transformers.commands.env.EnvironmentCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.env.EnvironmentCommand.run(self)
transformers.commands.env.info_command_factory(_)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/train.py----------------------------------------
A:transformers.commands.train.train_parser->parser.add_parser('train', help='CLI tool to train a model on a task.')
A:transformers.commands.train.self.logger->utils.logging.get_logger('transformers-cli/training')
A:transformers.commands.train.self.pipeline->pipelines.TextClassificationPipeline.from_pretrained(args.model)
A:transformers.commands.train.self.train_dataset->data.SingleSentenceClassificationProcessor.create_from_csv(args.train_data, column_label=args.column_label, column_text=args.column_text, column_id=args.column_id, skip_first_row=args.skip_first_row)
A:transformers.commands.train.self.valid_dataset->data.SingleSentenceClassificationProcessor.create_from_csv(args.validation_data, column_label=args.column_label, column_text=args.column_text, column_id=args.column_id, skip_first_row=args.skip_first_row)
transformers.commands.train.TrainCommand(self,args:Namespace)
transformers.commands.train.TrainCommand.__init__(self,args:Namespace)
transformers.commands.train.TrainCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.train.TrainCommand.run(self)
transformers.commands.train.TrainCommand.run_tf(self)
transformers.commands.train.TrainCommand.run_torch(self)
transformers.commands.train.train_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/download.py----------------------------------------
A:transformers.commands.download.download_parser->parser.add_parser('download')
transformers.commands.download.DownloadCommand(self,model:str,cache:str,force:bool)
transformers.commands.download.DownloadCommand.__init__(self,model:str,cache:str,force:bool)
transformers.commands.download.DownloadCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.download.DownloadCommand.run(self)
transformers.commands.download.download_command_factory(args)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/__init__.py----------------------------------------
transformers.commands.__init__.BaseTransformersCLICommand(ABC)
transformers.commands.__init__.BaseTransformersCLICommand.register_subcommand(parser:ArgumentParser)
transformers.commands.__init__.BaseTransformersCLICommand.run(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/lfs.py----------------------------------------
A:transformers.commands.lfs.logger->utils.logging.get_logger(__name__)
A:transformers.commands.lfs.enable_parser->parser.add_parser('lfs-enable-largefiles', help='Deprecated: use `huggingface-cli` instead. Configure your repository to enable upload of files > 5GB.')
A:transformers.commands.lfs.upload_parser->parser.add_parser(LFS_MULTIPART_UPLOAD_COMMAND, help='Deprecated: use `huggingface-cli` instead. Command will get called by git-lfs, do not call it directly.')
A:transformers.commands.lfs.local_path->os.path.abspath(self.args.path)
A:transformers.commands.lfs.msg->read_msg()
A:transformers.commands.lfs.self.f->open(self.filepath, 'rb')
A:transformers.commands.lfs.data->self.f.read(remaining_amount if n < 0 else min(n, remaining_amount))
A:transformers.commands.lfs.init_msg->json.loads(sys.stdin.readline().strip())
A:transformers.commands.lfs.chunk_size->int(header.pop('chunk_size'))
A:transformers.commands.lfs.r->requests.post(completion_url, json={'oid': oid, 'parts': parts})
transformers.commands.lfs.FileSlice(self,filepath:str,seek_from:int,read_limit:int)
transformers.commands.lfs.FileSlice.__enter__(self)
transformers.commands.lfs.FileSlice.__exit__(self,*args)
transformers.commands.lfs.FileSlice.__init__(self,filepath:str,seek_from:int,read_limit:int)
transformers.commands.lfs.FileSlice.__iter__(self)
transformers.commands.lfs.FileSlice.__len__(self)
transformers.commands.lfs.FileSlice.read(self,n=-1)
transformers.commands.lfs.LfsCommands(BaseTransformersCLICommand)
transformers.commands.lfs.LfsCommands.register_subcommand(parser:ArgumentParser)
transformers.commands.lfs.LfsEnableCommand(self,args)
transformers.commands.lfs.LfsEnableCommand.__init__(self,args)
transformers.commands.lfs.LfsEnableCommand.run(self)
transformers.commands.lfs.LfsUploadCommand(self,args)
transformers.commands.lfs.LfsUploadCommand.__init__(self,args)
transformers.commands.lfs.LfsUploadCommand.run(self)
transformers.commands.lfs.read_msg()->Optional[Dict]
transformers.commands.lfs.write_msg(msg:Dict)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/convert.py----------------------------------------
A:transformers.commands.convert.train_parser->parser.add_parser('convert', help='CLI tool to run convert model from original author checkpoints to Transformers PyTorch checkpoints.')
A:transformers.commands.convert.self._logger->utils.logging.get_logger('transformers-cli/converting')
transformers.commands.convert.ConvertCommand(self,model_type:str,tf_checkpoint:str,pytorch_dump_output:str,config:str,finetuning_task_name:str,*args)
transformers.commands.convert.ConvertCommand.__init__(self,model_type:str,tf_checkpoint:str,pytorch_dump_output:str,config:str,finetuning_task_name:str,*args)
transformers.commands.convert.ConvertCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.convert.ConvertCommand.run(self)
transformers.commands.convert.convert_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/commands/transformers_cli.py----------------------------------------
A:transformers.commands.transformers_cli.parser->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]')
A:transformers.commands.transformers_cli.commands_parser->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]').add_subparsers(help='transformers-cli command helpers')
A:transformers.commands.transformers_cli.args->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]').parse_args()
A:transformers.commands.transformers_cli.service->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]').parse_args().func(args)
transformers.commands.transformers_cli.main()


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/benchmark_tf.py----------------------------------------
A:transformers.benchmark.benchmark_tf.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_tf.rng->random.Random()
A:transformers.benchmark.benchmark_tf._inference->self._prepare_inference_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_tf._train->self._prepare_train_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_tf.transformers_module->__import__('transformers', fromlist=[model_class])
A:transformers.benchmark.benchmark_tf.model_cls->getattr(transformers_module, model_class)
A:transformers.benchmark.benchmark_tf.model->TF_MODEL_WITH_LM_HEAD_MAPPING[config.__class__](config)
A:transformers.benchmark.benchmark_tf.input_ids->random_input_ids(batch_size, sequence_length, vocab_size)
A:transformers.benchmark.benchmark_tf.gradients->tensorflow.gradients(loss, model.trainable_variables)
A:transformers.benchmark.benchmark_tf.runtimes->timeit.repeat(func, repeat=self.args.repeat, number=10)
A:transformers.benchmark.benchmark_tf.trace->start_memory_tracing('transformers')
A:transformers.benchmark.benchmark_tf.handle->py3nvml.py3nvml.nvmlDeviceGetHandleByIndex(self.args.device_idx)
A:transformers.benchmark.benchmark_tf.meminfo->py3nvml.py3nvml.nvmlDeviceGetMemoryInfo(handle)
A:transformers.benchmark.benchmark_tf.memory->Memory(max_bytes_in_use)
A:transformers.benchmark.benchmark_tf.memory_bytes->measure_peak_memory_cpu(func)
A:transformers.benchmark.benchmark_tf.summary->stop_memory_tracing(trace)
transformers.TensorFlowBenchmark(Benchmark)
transformers.TensorFlowBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.TensorFlowBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.TensorFlowBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.TensorFlowBenchmark._measure_speed(self,func)->float
transformers.TensorFlowBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.TensorFlowBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.TensorFlowBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.TensorFlowBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.TensorFlowBenchmark.framework_version(self)
transformers.benchmark.benchmark_tf.TensorFlowBenchmark(Benchmark)
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._measure_speed(self,func)->float
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_tf.TensorFlowBenchmark.framework_version(self)
transformers.benchmark.benchmark_tf.random_input_ids(batch_size:int,sequence_length:int,vocab_size:int)->['tf.Tensor']
transformers.benchmark.benchmark_tf.run_with_tf_optimizations(do_eager_mode:bool,use_xla:bool)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/benchmark.py----------------------------------------
A:transformers.benchmark.benchmark.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark._inference->self._prepare_inference_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark._train->self._prepare_train_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark.transformers_module->__import__('transformers', fromlist=[model_class])
A:transformers.benchmark.benchmark.model_cls->getattr(transformers_module, model_class)
A:transformers.benchmark.benchmark.model->MODEL_WITH_LM_HEAD_MAPPING[config.__class__](config)
A:transformers.benchmark.benchmark.input_ids->torch.randint(vocab_size, (batch_size, sequence_length), dtype=torch.long, device=self.args.device)
A:transformers.benchmark.benchmark.inference_model->torch.jit.trace(model, input_ids)
A:transformers.benchmark.benchmark.outputs->inference_model(input_ids)
A:transformers.benchmark.benchmark.runtimes->timeit.repeat(func, repeat=self.args.repeat, number=10)
A:transformers.benchmark.benchmark.trace->start_memory_tracing('transformers')
A:transformers.benchmark.benchmark.handle->py3nvml.py3nvml.nvmlDeviceGetHandleByIndex(self.args.device_idx)
A:transformers.benchmark.benchmark.meminfo->py3nvml.py3nvml.nvmlDeviceGetMemoryInfo(handle)
A:transformers.benchmark.benchmark.memory->Memory(max_bytes_in_use)
A:transformers.benchmark.benchmark.memory_bytes->measure_peak_memory_cpu(func)
A:transformers.benchmark.benchmark.summary->stop_memory_tracing(trace)
transformers.PyTorchBenchmark(Benchmark)
transformers.PyTorchBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.PyTorchBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.PyTorchBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.PyTorchBenchmark._measure_speed(self,func)->float
transformers.PyTorchBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.PyTorchBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.PyTorchBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.PyTorchBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.PyTorchBenchmark.framework_version(self)
transformers.benchmark.benchmark.PyTorchBenchmark(Benchmark)
transformers.benchmark.benchmark.PyTorchBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark.PyTorchBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark.PyTorchBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.benchmark.benchmark.PyTorchBenchmark._measure_speed(self,func)->float
transformers.benchmark.benchmark.PyTorchBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark.PyTorchBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark.PyTorchBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark.PyTorchBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark.PyTorchBenchmark.framework_version(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/benchmark_args.py----------------------------------------
A:transformers.benchmark.benchmark_args.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_args.self.torchscript->kwargs.pop('torchscript', self.torchscript)
A:transformers.benchmark.benchmark_args.self.torch_xla_tpu_print_metrics->kwargs.pop('torch_xla_tpu_print_metrics', self.torch_xla_tpu_print_metrics)
A:transformers.benchmark.benchmark_args.self.fp16_opt_level->kwargs.pop('fp16_opt_level', self.fp16_opt_level)
A:transformers.benchmark.benchmark_args.device->torch.device('cuda' if torch.cuda.is_available() else 'cpu')
A:transformers.benchmark.benchmark_args.n_gpu->torch.cuda.device_count()
transformers.PyTorchBenchmarkArguments(self,**kwargs)
transformers.PyTorchBenchmarkArguments._setup_devices(self)->Tuple['torch.device', int]
transformers.PyTorchBenchmarkArguments.device(self)->'torch.device'
transformers.PyTorchBenchmarkArguments.device_idx(self)->int
transformers.PyTorchBenchmarkArguments.is_gpu(self)
transformers.PyTorchBenchmarkArguments.is_tpu(self)
transformers.PyTorchBenchmarkArguments.n_gpu(self)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments(self,**kwargs)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.__init__(self,**kwargs)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments._setup_devices(self)->Tuple['torch.device', int]
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.device(self)->'torch.device'
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.device_idx(self)->int
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.is_gpu(self)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.is_tpu(self)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.n_gpu(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/benchmark_utils.py----------------------------------------
A:transformers.benchmark.benchmark_utils.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_utils.BenchmarkOutput->namedtuple('BenchmarkOutput', ['time_inference_result', 'memory_inference_result', 'time_train_result', 'memory_train_result', 'inference_summary', 'train_summary'])
A:transformers.benchmark.benchmark_utils.result->str(result)
A:transformers.benchmark.benchmark_utils.queue->Queue()
A:transformers.benchmark.benchmark_utils.p->Process(target=wrapper_func, args=[queue] + list(args))
A:transformers.benchmark.benchmark_utils.process->psutil.Process(os.getpid())
A:transformers.benchmark.benchmark_utils.self.mem_usage->max(self.mem_usage, get_cpu_memory(self.process_id))
A:transformers.benchmark.benchmark_utils.stop->self.connection.poll(self.interval)
A:transformers.benchmark.benchmark_utils.(child_connection, parent_connection)->Pipe()
A:transformers.benchmark.benchmark_utils.mem_process->MemoryMeasureProcess(os.getpid(), child_connection, interval)
A:transformers.benchmark.benchmark_utils.max_memory->parent_connection.recv()
A:transformers.benchmark.benchmark_utils.num_measurements->parent_connection.recv()
A:transformers.benchmark.benchmark_utils.parent->psutil.Process(os.getpid())
A:transformers.benchmark.benchmark_utils.line->linecache.getline(filename, lineno).rstrip()
A:transformers.benchmark.benchmark_utils.traced_state->Frame(filename, name, lineno, event, line)
A:transformers.benchmark.benchmark_utils.mem->psutil.Process(os.getpid()).memory_info()
A:transformers.benchmark.benchmark_utils.handle->py3nvml.py3nvml.nvmlDeviceGetHandleByIndex(self.args.device_idx)
A:transformers.benchmark.benchmark_utils.meminfo->py3nvml.py3nvml.nvmlDeviceGetMemoryInfo(handle)
A:transformers.benchmark.benchmark_utils.mem_state->UsedMemoryState(traced_state, cpu_mem, gpu_mem)
A:transformers.benchmark.benchmark_utils.cumulative_memory_dict->defaultdict(lambda : [0, 0, 0])
A:transformers.benchmark.benchmark_utils.cumulative_memory->list((MemoryState(frame=frame, cpu=Memory(cpu_mem_inc), gpu=Memory(gpu_mem_inc), cpu_gpu=Memory(cpu_gpu_mem_inc)) for (frame, (cpu_mem_inc, gpu_mem_inc, cpu_gpu_mem_inc)) in cumulative_memory))
A:transformers.benchmark.benchmark_utils.memory_curr_trace->sorted(memory_curr_trace, key=lambda x: x.cpu_gpu.bytes, reverse=True)
A:transformers.benchmark.benchmark_utils.total_memory->Memory(total_memory)
A:transformers.benchmark.benchmark_utils.inference_result_time->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.inference_result_memory->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.train_result_time->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.train_result_memory->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.inference_result_time[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.inference_result_memory[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.train_result_time[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.train_result_memory[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.(memory, inference_summary)->self.inference_memory(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_utils.time->self.train_speed(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_utils.(memory, train_summary)->self.train_memory(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_utils.writer->csv.DictWriter(csv_file, fieldnames=fieldnames + ['result'])
A:transformers.benchmark.benchmark_utils.info['python_version']->platform.python_version()
A:transformers.benchmark.benchmark_utils.info['system']->platform.system()
A:transformers.benchmark.benchmark_utils.info['cpu']->platform.processor()
A:transformers.benchmark.benchmark_utils.info['date']->datetime.datetime.date(datetime.now())
A:transformers.benchmark.benchmark_utils.info['time']->datetime.datetime.time(datetime.now())
A:transformers.benchmark.benchmark_utils.info['cpu_ram_mb']->bytes_to_mega_bytes(psutil.virtual_memory().total)
A:transformers.benchmark.benchmark_utils.info['gpu']->py3nvml.py3nvml.nvmlDeviceGetName(handle)
A:transformers.benchmark.benchmark_utils.info['gpu_ram_mb']->bytes_to_mega_bytes(nvml.nvmlDeviceGetMemoryInfo(handle).total)
A:transformers.benchmark.benchmark_utils.info['gpu_performance_state']->py3nvml.py3nvml.nvmlDeviceGetPerformanceState(handle)
transformers.benchmark.benchmark_utils.Benchmark(self,args:BenchmarkArguments=None,configs:PretrainedConfig=None)
transformers.benchmark.benchmark_utils.Benchmark.__init__(self,args:BenchmarkArguments=None,configs:PretrainedConfig=None)
transformers.benchmark.benchmark_utils.Benchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_utils.Benchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_utils.Benchmark.environment_info(self)
transformers.benchmark.benchmark_utils.Benchmark.framework_version(self)
transformers.benchmark.benchmark_utils.Benchmark.inference_memory(self,*args,**kwargs)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark.inference_speed(self,*args,**kwargs)->float
transformers.benchmark.benchmark_utils.Benchmark.print_fn(self)
transformers.benchmark.benchmark_utils.Benchmark.print_memory_trace_statistics(self,summary:MemorySummary)
transformers.benchmark.benchmark_utils.Benchmark.print_results(self,result_dict,type_label)
transformers.benchmark.benchmark_utils.Benchmark.run(self)
transformers.benchmark.benchmark_utils.Benchmark.save_to_csv(self,result_dict,filename)
transformers.benchmark.benchmark_utils.Benchmark.train_memory(self,*args,**kwargs)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark.train_speed(self,*args,**kwargs)->float
transformers.benchmark.benchmark_utils.Frame(NamedTuple)
transformers.benchmark.benchmark_utils.Memory(NamedTuple)
transformers.benchmark.benchmark_utils.Memory.__repr__(self)->str
transformers.benchmark.benchmark_utils.MemoryState(NamedTuple)
transformers.benchmark.benchmark_utils.MemorySummary(NamedTuple)
transformers.benchmark.benchmark_utils.UsedMemoryState(NamedTuple)
transformers.benchmark.benchmark_utils.bytes_to_mega_bytes(memory_amount:int)->int
transformers.benchmark.benchmark_utils.is_memory_tracing_enabled()
transformers.benchmark.benchmark_utils.measure_peak_memory_cpu(function:Callable[[],None],interval=0.5,device_idx=None)->int
transformers.benchmark.benchmark_utils.separate_process_wrapper_fn(func:Callable[[],None],do_multi_processing:bool)->Callable[[], None]
transformers.benchmark.benchmark_utils.start_memory_tracing(modules_to_trace:Optional[Union[str,Iterable[str]]]=None,modules_not_to_trace:Optional[Union[str,Iterable[str]]]=None,events_to_trace:str='line',gpus_to_trace:Optional[List[int]]=None)->MemoryTrace
transformers.benchmark.benchmark_utils.stop_memory_tracing(memory_trace:Optional[MemoryTrace]=None,ignore_released_memory:bool=True)->Optional[MemorySummary]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/benchmark_args_tf.py----------------------------------------
A:transformers.benchmark.benchmark_args_tf.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_args_tf.self.tpu_name->kwargs.pop('tpu_name', self.tpu_name)
A:transformers.benchmark.benchmark_args_tf.self.device_idx->kwargs.pop('device_idx', self.device_idx)
A:transformers.benchmark.benchmark_args_tf.self.eager_mode->kwargs.pop('eager_mode', self.eager_mode)
A:transformers.benchmark.benchmark_args_tf.self.use_xla->kwargs.pop('use_xla', self.use_xla)
A:transformers.benchmark.benchmark_args_tf.tpu->tensorflow.distribute.cluster_resolver.TPUClusterResolver()
A:transformers.benchmark.benchmark_args_tf.strategy->tensorflow.distribute.OneDeviceStrategy(device=f'/cpu:{self.device_idx}')
transformers.TensorFlowBenchmarkArguments(self,**kwargs)
transformers.TensorFlowBenchmarkArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', 'tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.TensorFlowBenchmarkArguments._setup_tpu(self)->Tuple['tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.TensorFlowBenchmarkArguments.gpu_list(self)
transformers.TensorFlowBenchmarkArguments.is_gpu(self)->bool
transformers.TensorFlowBenchmarkArguments.is_tpu(self)->bool
transformers.TensorFlowBenchmarkArguments.n_gpu(self)->int
transformers.TensorFlowBenchmarkArguments.strategy(self)->'tf.distribute.Strategy'
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments(self,**kwargs)
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.__init__(self,**kwargs)
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', 'tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments._setup_tpu(self)->Tuple['tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.gpu_list(self)
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.is_gpu(self)->bool
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.is_tpu(self)->bool
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.n_gpu(self)->int
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.strategy(self)->'tf.distribute.Strategy'


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/benchmark_args_utils.py----------------------------------------
A:transformers.benchmark.benchmark_args_utils.logger->utils.logging.get_logger(__name__)
transformers.benchmark.benchmark_args_utils.BenchmarkArguments
transformers.benchmark.benchmark_args_utils.BenchmarkArguments.do_multi_processing(self)
transformers.benchmark.benchmark_args_utils.BenchmarkArguments.model_names(self)
transformers.benchmark.benchmark_args_utils.BenchmarkArguments.to_json_string(self)
transformers.benchmark.benchmark_args_utils.list_field(default=None,metadata=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/benchmark/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/distilbert/tokenization_distilbert_fast.py----------------------------------------
A:transformers.models.distilbert.tokenization_distilbert_fast.logger->utils.logging.get_logger(__name__)
transformers.DistilBertTokenizerFast(BertTokenizerFast)
transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/distilbert/modeling_distilbert.py----------------------------------------
A:transformers.models.distilbert.modeling_distilbert.logger->utils.logging.get_logger(__name__)
A:transformers.models.distilbert.modeling_distilbert.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.distilbert.modeling_distilbert.out[:, 0::2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.models.distilbert.modeling_distilbert.out[:, 1::2]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.models.distilbert.modeling_distilbert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)
A:transformers.models.distilbert.modeling_distilbert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.dim)
A:transformers.models.distilbert.modeling_distilbert.self.LayerNorm->torch.nn.LayerNorm(config.dim, eps=1e-12)
A:transformers.models.distilbert.modeling_distilbert.self.dropout->torch.nn.Dropout(config.seq_classif_dropout)
A:transformers.models.distilbert.modeling_distilbert.seq_length->input_ids.size(1)
A:transformers.models.distilbert.modeling_distilbert.position_ids->position_ids.unsqueeze(0).expand_as(input_ids).unsqueeze(0).expand_as(input_ids)
A:transformers.models.distilbert.modeling_distilbert.word_embeddings->self.word_embeddings(input_ids)
A:transformers.models.distilbert.modeling_distilbert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.distilbert.modeling_distilbert.embeddings->self.dropout(embeddings)
A:transformers.models.distilbert.modeling_distilbert.self.q_lin->prune_linear_layer(self.q_lin, index)
A:transformers.models.distilbert.modeling_distilbert.self.k_lin->prune_linear_layer(self.k_lin, index)
A:transformers.models.distilbert.modeling_distilbert.self.v_lin->prune_linear_layer(self.v_lin, index)
A:transformers.models.distilbert.modeling_distilbert.self.out_lin->prune_linear_layer(self.out_lin, index, dim=1)
A:transformers.models.distilbert.modeling_distilbert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.distilbert.modeling_distilbert.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)
A:transformers.models.distilbert.modeling_distilbert.(bs, q_length, dim)->query.size()
A:transformers.models.distilbert.modeling_distilbert.k_length->key.size(1)
A:transformers.models.distilbert.modeling_distilbert.q->shape(self.q_lin(query))
A:transformers.models.distilbert.modeling_distilbert.k->shape(self.k_lin(key))
A:transformers.models.distilbert.modeling_distilbert.v->shape(self.v_lin(value))
A:transformers.models.distilbert.modeling_distilbert.scores->torch.matmul(q, k.transpose(2, 3))
A:transformers.models.distilbert.modeling_distilbert.mask->(mask == 0).view(mask_reshp).expand_as(scores)
A:transformers.models.distilbert.modeling_distilbert.weights->self.dropout(weights)
A:transformers.models.distilbert.modeling_distilbert.context->self.out_lin(context)
A:transformers.models.distilbert.modeling_distilbert.self.lin1->torch.nn.Linear(in_features=config.dim, out_features=config.hidden_dim)
A:transformers.models.distilbert.modeling_distilbert.self.lin2->torch.nn.Linear(in_features=config.hidden_dim, out_features=config.dim)
A:transformers.models.distilbert.modeling_distilbert.x->self.dropout(x)
A:transformers.models.distilbert.modeling_distilbert.self.attention->MultiHeadSelfAttention(config)
A:transformers.models.distilbert.modeling_distilbert.self.sa_layer_norm->torch.nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)
A:transformers.models.distilbert.modeling_distilbert.self.ffn->FFN(config)
A:transformers.models.distilbert.modeling_distilbert.self.output_layer_norm->torch.nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)
A:transformers.models.distilbert.modeling_distilbert.sa_output->self.sa_layer_norm(sa_output + x)
A:transformers.models.distilbert.modeling_distilbert.ffn_output->self.output_layer_norm(ffn_output + sa_output)
A:transformers.models.distilbert.modeling_distilbert.layer->TransformerBlock(config)
A:transformers.models.distilbert.modeling_distilbert.self.layer->torch.nn.ModuleList([copy.deepcopy(layer) for _ in range(config.n_layers)])
A:transformers.models.distilbert.modeling_distilbert.layer_outputs->layer_module(x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions)
A:transformers.models.distilbert.modeling_distilbert.self.embeddings->Embeddings(config)
A:transformers.models.distilbert.modeling_distilbert.self.transformer->Transformer(config)
A:transformers.models.distilbert.modeling_distilbert.input_shape->input_ids.size()
A:transformers.models.distilbert.modeling_distilbert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.distilbert.modeling_distilbert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.distilbert.modeling_distilbert.inputs_embeds->self.embeddings(input_ids)
A:transformers.models.distilbert.modeling_distilbert.self.distilbert->DistilBertModel(config)
A:transformers.models.distilbert.modeling_distilbert.self.vocab_transform->torch.nn.Linear(config.dim, config.dim)
A:transformers.models.distilbert.modeling_distilbert.self.vocab_layer_norm->torch.nn.LayerNorm(config.dim, eps=1e-12)
A:transformers.models.distilbert.modeling_distilbert.self.vocab_projector->torch.nn.Linear(config.dim, config.vocab_size)
A:transformers.models.distilbert.modeling_distilbert.self.mlm_loss_fct->torch.nn.CrossEntropyLoss()
A:transformers.models.distilbert.modeling_distilbert.dlbrt_output->self.distilbert(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.distilbert.modeling_distilbert.prediction_logits->self.vocab_projector(prediction_logits)
A:transformers.models.distilbert.modeling_distilbert.mlm_loss->self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))
A:transformers.models.distilbert.modeling_distilbert.self.pre_classifier->torch.nn.Linear(config.dim, config.dim)
A:transformers.models.distilbert.modeling_distilbert.self.classifier->torch.nn.Linear(config.dim, 1)
A:transformers.models.distilbert.modeling_distilbert.distilbert_output->self.distilbert(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.distilbert.modeling_distilbert.pooled_output->self.dropout(pooled_output)
A:transformers.models.distilbert.modeling_distilbert.logits->self.classifier(pooled_output)
A:transformers.models.distilbert.modeling_distilbert.loss_fct->CrossEntropyLoss()
A:transformers.models.distilbert.modeling_distilbert.loss->loss_fct(reshaped_logits, labels)
A:transformers.models.distilbert.modeling_distilbert.self.qa_outputs->torch.nn.Linear(config.dim, config.num_labels)
A:transformers.models.distilbert.modeling_distilbert.hidden_states->self.dropout(hidden_states)
A:transformers.models.distilbert.modeling_distilbert.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.models.distilbert.modeling_distilbert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.distilbert.modeling_distilbert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.distilbert.modeling_distilbert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.distilbert.modeling_distilbert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.distilbert.modeling_distilbert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.distilbert.modeling_distilbert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.distilbert.modeling_distilbert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.distilbert.modeling_distilbert.outputs->self.distilbert(input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.distilbert.modeling_distilbert.sequence_output->self.dropout(sequence_output)
A:transformers.models.distilbert.modeling_distilbert.active_logits->self.classifier(pooled_output).view(-1, self.num_labels)
A:transformers.models.distilbert.modeling_distilbert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.distilbert.modeling_distilbert.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.DistilBertForMaskedLM(self,config)
transformers.DistilBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForMaskedLM.get_output_embeddings(self)
transformers.DistilBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.DistilBertForMultipleChoice(self,config)
transformers.DistilBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForQuestionAnswering(self,config)
transformers.DistilBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForSequenceClassification(self,config)
transformers.DistilBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForTokenClassification(self,config)
transformers.DistilBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertModel(self,config)
transformers.DistilBertModel._prune_heads(self,heads_to_prune)
transformers.DistilBertModel.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertModel.get_input_embeddings(self)
transformers.DistilBertModel.set_input_embeddings(self,new_embeddings)
transformers.DistilBertPreTrainedModel(PreTrainedModel)
transformers.DistilBertPreTrainedModel._init_weights(self,module)
transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM.get_output_embeddings(self)
transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.distilbert.modeling_distilbert.DistilBertForMultipleChoice(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForMultipleChoice.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.distilbert.modeling_distilbert.DistilBertForTokenClassification(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForTokenClassification.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.distilbert.modeling_distilbert.DistilBertModel(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertModel.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.DistilBertModel._prune_heads(self,heads_to_prune)
transformers.models.distilbert.modeling_distilbert.DistilBertModel.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.distilbert.modeling_distilbert.DistilBertModel.get_input_embeddings(self)
transformers.models.distilbert.modeling_distilbert.DistilBertModel.set_input_embeddings(self,new_embeddings)
transformers.models.distilbert.modeling_distilbert.DistilBertPreTrainedModel(PreTrainedModel)
transformers.models.distilbert.modeling_distilbert.DistilBertPreTrainedModel._init_weights(self,module)
transformers.models.distilbert.modeling_distilbert.Embeddings(self,config)
transformers.models.distilbert.modeling_distilbert.Embeddings.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.Embeddings.forward(self,input_ids)
transformers.models.distilbert.modeling_distilbert.FFN(self,config)
transformers.models.distilbert.modeling_distilbert.FFN.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.FFN.ff_chunk(self,input)
transformers.models.distilbert.modeling_distilbert.FFN.forward(self,input)
transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention(self,config)
transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention.forward(self,query,key,value,mask,head_mask=None,output_attentions=False)
transformers.models.distilbert.modeling_distilbert.MultiHeadSelfAttention.prune_heads(self,heads)
transformers.models.distilbert.modeling_distilbert.Transformer(self,config)
transformers.models.distilbert.modeling_distilbert.Transformer.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.Transformer.forward(self,x,attn_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=None)
transformers.models.distilbert.modeling_distilbert.TransformerBlock(self,config)
transformers.models.distilbert.modeling_distilbert.TransformerBlock.__init__(self,config)
transformers.models.distilbert.modeling_distilbert.TransformerBlock.forward(self,x,attn_mask=None,head_mask=None,output_attentions=False)
transformers.models.distilbert.modeling_distilbert.create_sinusoidal_embeddings(n_pos,dim,out)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/distilbert/configuration_distilbert.py----------------------------------------
A:transformers.models.distilbert.configuration_distilbert.logger->utils.logging.get_logger(__name__)
transformers.DistilBertConfig(self,vocab_size=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,qa_dropout=0.1,seq_classif_dropout=0.2,pad_token_id=0,**kwargs)
transformers.DistilBertConfig.hidden_size(self)
transformers.DistilBertConfig.num_attention_heads(self)
transformers.DistilBertConfig.num_hidden_layers(self)
transformers.models.distilbert.configuration_distilbert.DistilBertConfig(self,vocab_size=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,qa_dropout=0.1,seq_classif_dropout=0.2,pad_token_id=0,**kwargs)
transformers.models.distilbert.configuration_distilbert.DistilBertConfig.__init__(self,vocab_size=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,qa_dropout=0.1,seq_classif_dropout=0.2,pad_token_id=0,**kwargs)
transformers.models.distilbert.configuration_distilbert.DistilBertConfig.hidden_size(self)
transformers.models.distilbert.configuration_distilbert.DistilBertConfig.num_attention_heads(self)
transformers.models.distilbert.configuration_distilbert.DistilBertConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/distilbert/tokenization_distilbert.py----------------------------------------
A:transformers.models.distilbert.tokenization_distilbert.logger->utils.logging.get_logger(__name__)
transformers.DistilBertTokenizer(BertTokenizer)
transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/distilbert/__init__.py----------------------------------------
A:transformers.models.distilbert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/distilbert/modeling_tf_distilbert.py----------------------------------------
A:transformers.models.distilbert.modeling_tf_distilbert.logger->utils.logging.get_logger(__name__)
A:transformers.models.distilbert.modeling_tf_distilbert.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.distilbert.modeling_tf_distilbert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='LayerNorm')
A:transformers.models.distilbert.modeling_tf_distilbert.self.dropout->tensorflow.keras.layers.Dropout(config.qa_dropout)
A:transformers.models.distilbert.modeling_tf_distilbert.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.dim], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.distilbert.modeling_tf_distilbert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.dim], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.distilbert.modeling_tf_distilbert.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.distilbert.modeling_tf_distilbert.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.distilbert.modeling_tf_distilbert.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.distilbert.modeling_tf_distilbert.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.distilbert.modeling_tf_distilbert.self.q_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='q_lin')
A:transformers.models.distilbert.modeling_tf_distilbert.self.k_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='k_lin')
A:transformers.models.distilbert.modeling_tf_distilbert.self.v_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='v_lin')
A:transformers.models.distilbert.modeling_tf_distilbert.self.out_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='out_lin')
A:transformers.models.distilbert.modeling_tf_distilbert.self.pruned_heads->set()
A:transformers.models.distilbert.modeling_tf_distilbert.(bs, q_length, dim)->shape_list(query)
A:transformers.models.distilbert.modeling_tf_distilbert.dim_per_head->tensorflow.cast(dim_per_head, dtype=tf.int32)
A:transformers.models.distilbert.modeling_tf_distilbert.q->tensorflow.multiply(q, tf.math.rsqrt(tf.cast(dim_per_head, dtype=tf.float32)))
A:transformers.models.distilbert.modeling_tf_distilbert.k->tensorflow.cast(k, dtype=q.dtype)
A:transformers.models.distilbert.modeling_tf_distilbert.v->shape(self.v_lin(value))
A:transformers.models.distilbert.modeling_tf_distilbert.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.models.distilbert.modeling_tf_distilbert.mask->tensorflow.cast(mask, dtype=scores.dtype)
A:transformers.models.distilbert.modeling_tf_distilbert.weights->self.dropout(weights, training=training)
A:transformers.models.distilbert.modeling_tf_distilbert.context->self.out_lin(context)
A:transformers.models.distilbert.modeling_tf_distilbert.self.lin1->tensorflow.keras.layers.Dense(config.hidden_dim, kernel_initializer=get_initializer(config.initializer_range), name='lin1')
A:transformers.models.distilbert.modeling_tf_distilbert.self.lin2->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='lin2')
A:transformers.models.distilbert.modeling_tf_distilbert.self.activation->get_tf_activation(config.activation)
A:transformers.models.distilbert.modeling_tf_distilbert.x->self.dropout(x, training=training)
A:transformers.models.distilbert.modeling_tf_distilbert.self.attention->TFMultiHeadSelfAttention(config, name='attention')
A:transformers.models.distilbert.modeling_tf_distilbert.self.sa_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='sa_layer_norm')
A:transformers.models.distilbert.modeling_tf_distilbert.self.ffn->TFFFN(config, name='ffn')
A:transformers.models.distilbert.modeling_tf_distilbert.self.output_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='output_layer_norm')
A:transformers.models.distilbert.modeling_tf_distilbert.sa_output->self.sa_layer_norm(sa_output + x)
A:transformers.models.distilbert.modeling_tf_distilbert.ffn_output->self.output_layer_norm(ffn_output + sa_output)
A:transformers.models.distilbert.modeling_tf_distilbert.layer_outputs->layer_module(hidden_state, attn_mask, head_mask[i], output_attentions, training=training)
A:transformers.models.distilbert.modeling_tf_distilbert.self.embeddings->TFEmbeddings(config, name='embeddings')
A:transformers.models.distilbert.modeling_tf_distilbert.self.transformer->TFTransformer(config, name='transformer')
A:transformers.models.distilbert.modeling_tf_distilbert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.distilbert.modeling_tf_distilbert.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.distilbert.modeling_tf_distilbert.inputs['attention_mask']->tensorflow.cast(inputs['attention_mask'], dtype=tf.float32)
A:transformers.models.distilbert.modeling_tf_distilbert.embedding_output->self.embeddings(inputs['input_ids'], inputs_embeds=inputs['inputs_embeds'])
A:transformers.models.distilbert.modeling_tf_distilbert.tfmr_output->self.transformer(embedding_output, inputs['attention_mask'], inputs['head_mask'], inputs['output_attentions'], inputs['output_hidden_states'], inputs['return_dict'], training=inputs['training'])
A:transformers.models.distilbert.modeling_tf_distilbert.output->self.call(inputs)
A:transformers.models.distilbert.modeling_tf_distilbert.self.distilbert->TFDistilBertMainLayer(config, name='distilbert')
A:transformers.models.distilbert.modeling_tf_distilbert.outputs->self.distilbert(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.distilbert.modeling_tf_distilbert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.distilbert.modeling_tf_distilbert.hidden_states->self.dropout(hidden_states, training=inputs['training'])
A:transformers.models.distilbert.modeling_tf_distilbert.self.vocab_transform->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='vocab_transform')
A:transformers.models.distilbert.modeling_tf_distilbert.self.act->get_tf_activation('gelu')
A:transformers.models.distilbert.modeling_tf_distilbert.self.vocab_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='vocab_layer_norm')
A:transformers.models.distilbert.modeling_tf_distilbert.self.vocab_projector->TFDistilBertLMHead(config, self.distilbert.embeddings, name='vocab_projector')
A:transformers.models.distilbert.modeling_tf_distilbert.distilbert_output->self.distilbert(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.distilbert.modeling_tf_distilbert.prediction_logits->self.vocab_projector(prediction_logits)
A:transformers.models.distilbert.modeling_tf_distilbert.self.pre_classifier->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), activation='relu', name='pre_classifier')
A:transformers.models.distilbert.modeling_tf_distilbert.self.classifier->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.distilbert.modeling_tf_distilbert.pooled_output->self.dropout(pooled_output, training=inputs['training'])
A:transformers.models.distilbert.modeling_tf_distilbert.logits->self.qa_outputs(hidden_states)
A:transformers.models.distilbert.modeling_tf_distilbert.sequence_output->self.dropout(sequence_output, training=inputs['training'])
A:transformers.models.distilbert.modeling_tf_distilbert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.distilbert.modeling_tf_distilbert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.distilbert.modeling_tf_distilbert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.distilbert.modeling_tf_distilbert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.distilbert.modeling_tf_distilbert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.distilbert.modeling_tf_distilbert.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFDistilBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFDistilBertForMaskedLM.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFDistilBertForMaskedLM.get_lm_head(self)
transformers.TFDistilBertForMaskedLM.get_prefix_bias_name(self)
transformers.TFDistilBertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFDistilBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFDistilBertForMultipleChoice.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFDistilBertForMultipleChoice.dummy_inputs(self)
transformers.TFDistilBertForMultipleChoice.serving(self,inputs)
transformers.TFDistilBertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFDistilBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFDistilBertForQuestionAnswering.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFDistilBertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFDistilBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFDistilBertForSequenceClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFDistilBertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFDistilBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFDistilBertForTokenClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFDistilBertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFDistilBertMainLayer(self,config,**kwargs)
transformers.TFDistilBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFDistilBertMainLayer.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFDistilBertMainLayer.get_input_embeddings(self)
transformers.TFDistilBertMainLayer.set_input_embeddings(self,value)
transformers.TFDistilBertModel(self,config,*inputs,**kwargs)
transformers.TFDistilBertModel.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFDistilBertModel.serving_output(self,output)
transformers.TFDistilBertPreTrainedModel(TFPreTrainedModel)
transformers.TFDistilBertPreTrainedModel.serving(self,inputs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMaskedLM.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMaskedLM.get_lm_head(self)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMaskedLM.get_prefix_bias_name(self)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMultipleChoice.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMultipleChoice.dummy_inputs(self)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMultipleChoice.serving(self,inputs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForTokenClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead(self,config,input_embeddings,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.build(self,input_shape)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.call(self,hidden_states)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.get_bias(self)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.get_output_embeddings(self)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.set_bias(self,value)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertLMHead.set_output_embeddings(self,value)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer.__init__(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer._prune_heads(self,heads_to_prune)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer.get_input_embeddings(self)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer.set_input_embeddings(self,value)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel.__init__(self,config,*inputs,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel.call(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel.serving_output(self,output)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertPreTrainedModel(TFPreTrainedModel)
transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertPreTrainedModel.serving(self,inputs)
transformers.models.distilbert.modeling_tf_distilbert.TFEmbeddings(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFEmbeddings.__init__(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.distilbert.modeling_tf_distilbert.TFEmbeddings.call(self,input_ids=None,position_ids=None,inputs_embeds=None,training=False)
transformers.models.distilbert.modeling_tf_distilbert.TFFFN(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFFFN.__init__(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFFFN.call(self,input,training=False)
transformers.models.distilbert.modeling_tf_distilbert.TFMultiHeadSelfAttention(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFMultiHeadSelfAttention.__init__(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFMultiHeadSelfAttention.call(self,query,key,value,mask,head_mask,output_attentions,training=False)
transformers.models.distilbert.modeling_tf_distilbert.TFMultiHeadSelfAttention.prune_heads(self,heads)
transformers.models.distilbert.modeling_tf_distilbert.TFTransformer(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFTransformer.__init__(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFTransformer.call(self,x,attn_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock.__init__(self,config,**kwargs)
transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock.call(self,x,attn_mask,head_mask,output_attentions,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/modeling_openai.py----------------------------------------
A:transformers.models.openai.modeling_openai.logger->utils.logging.get_logger(__name__)
A:transformers.models.openai.modeling_openai.openai_checkpoint_folder_path->os.path.dirname(openai_checkpoint_folder_path)
A:transformers.models.openai.modeling_openai.names->json.load(names_handle)
A:transformers.models.openai.modeling_openai.shapes->json.load(shapes_handle)
A:transformers.models.openai.modeling_openai.offsets->numpy.cumsum([np.prod(shape) for shape in shapes])
A:transformers.models.openai.modeling_openai.model.tokens_embed.weight.data->torch.from_numpy(init_params[1])
A:transformers.models.openai.modeling_openai.model.positions_embed.weight.data->torch.from_numpy(init_params[0])
A:transformers.models.openai.modeling_openai.name->name.split('/').split('/')
A:transformers.models.openai.modeling_openai.scope_names->re.split('(\\d+)', m_name)
A:transformers.models.openai.modeling_openai.pointer->getattr(pointer, scope_names[0])
A:transformers.models.openai.modeling_openai.num->int(scope_names[1])
A:transformers.models.openai.modeling_openai.pointer.data->torch.from_numpy(array)
A:transformers.models.openai.modeling_openai.self.c_attn->prune_conv1d_layer(self.c_attn, index_attn, dim=1)
A:transformers.models.openai.modeling_openai.self.c_proj->Conv1D(nx, n_state)
A:transformers.models.openai.modeling_openai.self.attn_dropout->torch.nn.Dropout(config.attn_pdrop)
A:transformers.models.openai.modeling_openai.self.resid_dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.models.openai.modeling_openai.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.openai.modeling_openai.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_head, self.split_size // self.n_head, self.pruned_heads)
A:transformers.models.openai.modeling_openai.index_attn->torch.cat([index, index + self.split_size, index + 2 * self.split_size])
A:transformers.models.openai.modeling_openai.w->self.attn_dropout(w)
A:transformers.models.openai.modeling_openai.x->self.c_attn(x)
A:transformers.models.openai.modeling_openai.(query, key, value)->self.c_attn(x).split(self.split_size, dim=2)
A:transformers.models.openai.modeling_openai.query->self.split_heads(query)
A:transformers.models.openai.modeling_openai.key->self.split_heads(key, k=True)
A:transformers.models.openai.modeling_openai.value->self.split_heads(value)
A:transformers.models.openai.modeling_openai.attn_outputs->self.attn(x, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.models.openai.modeling_openai.a->self.resid_dropout(a)
A:transformers.models.openai.modeling_openai.self.c_fc->Conv1D(n_state, nx)
A:transformers.models.openai.modeling_openai.self.dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.models.openai.modeling_openai.h->self.ln_2(n + m)
A:transformers.models.openai.modeling_openai.h2->self.c_proj(h)
A:transformers.models.openai.modeling_openai.self.attn->Attention(nx, n_ctx, config, scale)
A:transformers.models.openai.modeling_openai.self.ln_1->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.models.openai.modeling_openai.self.mlp->MLP(4 * nx, config)
A:transformers.models.openai.modeling_openai.self.ln_2->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.models.openai.modeling_openai.n->self.ln_1(x + a)
A:transformers.models.openai.modeling_openai.m->self.mlp(n)
A:transformers.models.openai.modeling_openai.self.tokens_embed->torch.nn.Embedding(config.vocab_size, config.n_embd)
A:transformers.models.openai.modeling_openai.self.positions_embed->torch.nn.Embedding(config.n_positions, config.n_embd)
A:transformers.models.openai.modeling_openai.self.drop->torch.nn.Dropout(config.embd_pdrop)
A:transformers.models.openai.modeling_openai.self.h->torch.nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])
A:transformers.models.openai.modeling_openai.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.models.openai.modeling_openai.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.openai.modeling_openai.attention_mask->attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.models.openai.modeling_openai.head_mask->self.get_head_mask(head_mask, self.config.n_layer)
A:transformers.models.openai.modeling_openai.inputs_embeds->self.tokens_embed(input_ids)
A:transformers.models.openai.modeling_openai.position_embeds->self.positions_embed(position_ids)
A:transformers.models.openai.modeling_openai.token_type_ids->token_type_ids.view(-1, token_type_ids.size(-1)).view(-1, token_type_ids.size(-1))
A:transformers.models.openai.modeling_openai.token_type_embeds->self.tokens_embed(token_type_ids)
A:transformers.models.openai.modeling_openai.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.models.openai.modeling_openai.outputs->block(hidden_states, attention_mask, head_mask[i], output_attentions=output_attentions)
A:transformers.models.openai.modeling_openai.self.transformer->OpenAIGPTModel(config)
A:transformers.models.openai.modeling_openai.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)
A:transformers.models.openai.modeling_openai.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.openai.modeling_openai.lm_logits->self.lm_head(hidden_states)
A:transformers.models.openai.modeling_openai.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.models.openai.modeling_openai.shift_labels->labels[..., 1:].contiguous()
A:transformers.models.openai.modeling_openai.loss_fct->CrossEntropyLoss()
A:transformers.models.openai.modeling_openai.loss->loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.openai.modeling_openai.self.multiple_choice_head->SequenceSummary(config)
A:transformers.models.openai.modeling_openai.mc_logits->self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)
A:transformers.models.openai.modeling_openai.mc_loss->loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))
A:transformers.models.openai.modeling_openai.lm_loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
A:transformers.models.openai.modeling_openai.self.score->torch.nn.Linear(config.n_embd, self.num_labels, bias=False)
A:transformers.models.openai.modeling_openai.logits->self.score(hidden_states)
transformers.OpenAIGPTDoubleHeadsModel(self,config)
transformers.OpenAIGPTDoubleHeadsModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.OpenAIGPTDoubleHeadsModel.get_output_embeddings(self)
transformers.OpenAIGPTDoubleHeadsModel.set_output_embeddings(self,new_embeddings)
transformers.OpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.OpenAIGPTForSequenceClassification(self,config)
transformers.OpenAIGPTForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.OpenAIGPTLMHeadModel(self,config)
transformers.OpenAIGPTLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.OpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.OpenAIGPTLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.OpenAIGPTModel(self,config)
transformers.OpenAIGPTModel._prune_heads(self,heads_to_prune)
transformers.OpenAIGPTModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.OpenAIGPTModel.get_input_embeddings(self)
transformers.OpenAIGPTModel.set_input_embeddings(self,new_embeddings)
transformers.OpenAIGPTPreTrainedModel(PreTrainedModel)
transformers.OpenAIGPTPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_openai_gpt(model,config,openai_checkpoint_folder_path)
transformers.models.openai.modeling_openai.Attention(self,nx,n_ctx,config,scale=False)
transformers.models.openai.modeling_openai.Attention.__init__(self,nx,n_ctx,config,scale=False)
transformers.models.openai.modeling_openai.Attention._attn(self,q,k,v,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.openai.modeling_openai.Attention.forward(self,x,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.openai.modeling_openai.Attention.merge_heads(self,x)
transformers.models.openai.modeling_openai.Attention.prune_heads(self,heads)
transformers.models.openai.modeling_openai.Attention.split_heads(self,x,k=False)
transformers.models.openai.modeling_openai.Block(self,n_ctx,config,scale=False)
transformers.models.openai.modeling_openai.Block.__init__(self,n_ctx,config,scale=False)
transformers.models.openai.modeling_openai.Block.forward(self,x,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.openai.modeling_openai.MLP(self,n_state,config)
transformers.models.openai.modeling_openai.MLP.__init__(self,n_state,config)
transformers.models.openai.modeling_openai.MLP.forward(self,x)
transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModel(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModel.__init__(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModel.get_output_embeddings(self)
transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModel.set_output_embeddings(self,new_embeddings)
transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.models.openai.modeling_openai.OpenAIGPTForSequenceClassification(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTForSequenceClassification.__init__(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel.__init__(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.openai.modeling_openai.OpenAIGPTModel(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTModel.__init__(self,config)
transformers.models.openai.modeling_openai.OpenAIGPTModel._prune_heads(self,heads_to_prune)
transformers.models.openai.modeling_openai.OpenAIGPTModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.openai.modeling_openai.OpenAIGPTModel.get_input_embeddings(self)
transformers.models.openai.modeling_openai.OpenAIGPTModel.set_input_embeddings(self,new_embeddings)
transformers.models.openai.modeling_openai.OpenAIGPTPreTrainedModel(PreTrainedModel)
transformers.models.openai.modeling_openai.OpenAIGPTPreTrainedModel._init_weights(self,module)
transformers.models.openai.modeling_openai.load_tf_weights_in_openai_gpt(model,config,openai_checkpoint_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/tokenization_openai_fast.py----------------------------------------
A:transformers.models.openai.tokenization_openai_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.openai.tokenization_openai_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
transformers.OpenAIGPTTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<unk>',**kwargs)
transformers.OpenAIGPTTokenizerFast.do_lower_case(self)
transformers.OpenAIGPTTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.openai.tokenization_openai_fast.OpenAIGPTTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<unk>',**kwargs)
transformers.models.openai.tokenization_openai_fast.OpenAIGPTTokenizerFast.__init__(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<unk>',**kwargs)
transformers.models.openai.tokenization_openai_fast.OpenAIGPTTokenizerFast.do_lower_case(self)
transformers.models.openai.tokenization_openai_fast.OpenAIGPTTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/tokenization_openai.py----------------------------------------
A:transformers.models.openai.tokenization_openai.logger->utils.logging.get_logger(__name__)
A:transformers.models.openai.tokenization_openai.pairs->get_pairs(word)
A:transformers.models.openai.tokenization_openai.text->self.nlp(text_standardize(self.fix_text(text)))
A:transformers.models.openai.tokenization_openai._nlp->English()
A:transformers.models.openai.tokenization_openai.self.nlp->BasicTokenizer(do_lower_case=True)
A:transformers.models.openai.tokenization_openai.self.encoder->json.load(vocab_handle)
A:transformers.models.openai.tokenization_openai.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.openai.tokenization_openai.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.openai.tokenization_openai.j->' '.join(word).index(first, i)
A:transformers.models.openai.tokenization_openai.new_word->tuple(new_word)
A:transformers.models.openai.tokenization_openai.word->' '.join(word)
A:transformers.models.openai.tokenization_openai.out_string->''.join(tokens).replace('</w>', ' ').strip()
A:transformers.models.openai.tokenization_openai.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.openai.tokenization_openai.merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
transformers.OpenAIGPTTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.OpenAIGPTTokenizer._convert_id_to_token(self,index)
transformers.OpenAIGPTTokenizer._convert_token_to_id(self,token)
transformers.OpenAIGPTTokenizer._tokenize(self,text)
transformers.OpenAIGPTTokenizer.bpe(self,token)
transformers.OpenAIGPTTokenizer.convert_tokens_to_string(self,tokens)
transformers.OpenAIGPTTokenizer.do_lower_case(self)
transformers.OpenAIGPTTokenizer.get_vocab(self)
transformers.OpenAIGPTTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.OpenAIGPTTokenizer.vocab_size(self)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer._convert_id_to_token(self,index)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer._convert_token_to_id(self,token)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer._tokenize(self,text)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.bpe(self,token)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.do_lower_case(self)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.get_vocab(self)
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.openai.tokenization_openai.OpenAIGPTTokenizer.vocab_size(self)
transformers.models.openai.tokenization_openai.get_pairs(word)
transformers.models.openai.tokenization_openai.text_standardize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/configuration_openai.py----------------------------------------
A:transformers.models.openai.configuration_openai.logger->utils.logging.get_logger(__name__)
transformers.OpenAIGPTConfig(self,vocab_size=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.OpenAIGPTConfig.hidden_size(self)
transformers.OpenAIGPTConfig.max_position_embeddings(self)
transformers.OpenAIGPTConfig.num_attention_heads(self)
transformers.OpenAIGPTConfig.num_hidden_layers(self)
transformers.models.openai.configuration_openai.OpenAIGPTConfig(self,vocab_size=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.models.openai.configuration_openai.OpenAIGPTConfig.__init__(self,vocab_size=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.models.openai.configuration_openai.OpenAIGPTConfig.hidden_size(self)
transformers.models.openai.configuration_openai.OpenAIGPTConfig.max_position_embeddings(self)
transformers.models.openai.configuration_openai.OpenAIGPTConfig.num_attention_heads(self)
transformers.models.openai.configuration_openai.OpenAIGPTConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.openai.convert_openai_original_tf_checkpoint_to_pytorch.config->transformers.OpenAIGPTConfig.from_json_file(openai_config_file)
A:transformers.models.openai.convert_openai_original_tf_checkpoint_to_pytorch.model->OpenAIGPTModel(config)
A:transformers.models.openai.convert_openai_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.openai.convert_openai_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.openai.convert_openai_original_tf_checkpoint_to_pytorch.convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path,openai_config_file,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/modeling_tf_openai.py----------------------------------------
A:transformers.models.openai.modeling_tf_openai.logger->utils.logging.get_logger(__name__)
A:transformers.models.openai.modeling_tf_openai.self.c_attn->TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name='c_attn')
A:transformers.models.openai.modeling_tf_openai.self.c_proj->TFConv1D(nx, n_state, initializer_range=config.initializer_range, name='c_proj')
A:transformers.models.openai.modeling_tf_openai.self.attn_dropout->tensorflow.keras.layers.Dropout(config.attn_pdrop)
A:transformers.models.openai.modeling_tf_openai.self.resid_dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.models.openai.modeling_tf_openai.self.pruned_heads->set()
A:transformers.models.openai.modeling_tf_openai.j->tensorflow.range(ns)
A:transformers.models.openai.modeling_tf_openai.w->self.attn_dropout(w, training=training)
A:transformers.models.openai.modeling_tf_openai.dk->tensorflow.cast(shape_list(k)[-1], dtype=w.dtype)
A:transformers.models.openai.modeling_tf_openai.(_, _, nd, ns)->shape_list(w)
A:transformers.models.openai.modeling_tf_openai.b->tensorflow.reshape(b, [1, 1, nd, ns])
A:transformers.models.openai.modeling_tf_openai.attention_mask->tensorflow.cast(attention_mask, dtype=w.dtype)
A:transformers.models.openai.modeling_tf_openai.x->self.c_attn(x)
A:transformers.models.openai.modeling_tf_openai.x_shape->shape_list(x)
A:transformers.models.openai.modeling_tf_openai.(query, key, value)->tensorflow.split(x, 3, axis=2)
A:transformers.models.openai.modeling_tf_openai.query->self.split_heads(query)
A:transformers.models.openai.modeling_tf_openai.key->self.split_heads(key)
A:transformers.models.openai.modeling_tf_openai.value->self.split_heads(value)
A:transformers.models.openai.modeling_tf_openai.attn_outputs->self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.openai.modeling_tf_openai.a->self.resid_dropout(a, training=training)
A:transformers.models.openai.modeling_tf_openai.self.c_fc->TFConv1D(n_state, nx, initializer_range=config.initializer_range, name='c_fc')
A:transformers.models.openai.modeling_tf_openai.self.act->get_tf_activation('gelu')
A:transformers.models.openai.modeling_tf_openai.self.dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.models.openai.modeling_tf_openai.h->self.ln_2(n + m)
A:transformers.models.openai.modeling_tf_openai.h2->self.dropout(h2, training=training)
A:transformers.models.openai.modeling_tf_openai.self.attn->TFAttention(nx, n_ctx, config, scale, name='attn')
A:transformers.models.openai.modeling_tf_openai.self.ln_1->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_1')
A:transformers.models.openai.modeling_tf_openai.self.mlp->TFMLP(4 * nx, config, name='mlp')
A:transformers.models.openai.modeling_tf_openai.self.ln_2->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_2')
A:transformers.models.openai.modeling_tf_openai.output_attn->self.attn(x, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.openai.modeling_tf_openai.n->self.ln_1(x + a)
A:transformers.models.openai.modeling_tf_openai.m->self.mlp(n, training=training)
A:transformers.models.openai.modeling_tf_openai.self.tokens_embed->TFSharedEmbeddings(config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name='tokens_embed')
A:transformers.models.openai.modeling_tf_openai.self.drop->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.models.openai.modeling_tf_openai.self.positions_embed->self.add_weight(name='embeddings', shape=[self.n_positions, self.n_embd], initializer=get_initializer(self.initializer_range))
A:transformers.models.openai.modeling_tf_openai.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.openai.modeling_tf_openai.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.openai.modeling_tf_openai.inputs['input_ids']->tensorflow.reshape(inputs['input_ids'], [-1, input_shape[-1]])
A:transformers.models.openai.modeling_tf_openai.inputs['position_ids']->tensorflow.reshape(inputs['position_ids'], [-1, shape_list(inputs['position_ids'])[-1]])
A:transformers.models.openai.modeling_tf_openai.inputs['attention_mask']->tensorflow.multiply(tf.subtract(one_cst, inputs['attention_mask']), tf.constant(-10000.0))
A:transformers.models.openai.modeling_tf_openai.one_cst->tensorflow.constant(1.0)
A:transformers.models.openai.modeling_tf_openai.inputs['inputs_embeds']->self.tokens_embed(inputs['input_ids'], mode='embedding')
A:transformers.models.openai.modeling_tf_openai.position_embeds->tensorflow.gather(self.positions_embed, inputs['position_ids'])
A:transformers.models.openai.modeling_tf_openai.inputs['token_type_ids']->tensorflow.reshape(inputs['token_type_ids'], [-1, shape_list(inputs['token_type_ids'])[-1]])
A:transformers.models.openai.modeling_tf_openai.token_type_embeds->self.tokens_embed(inputs['token_type_ids'], mode='embedding')
A:transformers.models.openai.modeling_tf_openai.hidden_states->tensorflow.reshape(hidden_states, input_shapes + shape_list(hidden_states)[-1:])
A:transformers.models.openai.modeling_tf_openai.outputs->self.transformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.openai.modeling_tf_openai.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.models.openai.modeling_tf_openai.output->self.call(inputs)
A:transformers.models.openai.modeling_tf_openai.self.transformer->TFOpenAIGPTMainLayer(config, name='transformer')
A:transformers.models.openai.modeling_tf_openai.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.openai.modeling_tf_openai.logits->self.score(hidden_states)
A:transformers.models.openai.modeling_tf_openai.loss->self.compute_loss(tf.reshape(inputs['labels'], [-1, 1]), tf.reshape(in_logits, [-1, self.num_labels]))
A:transformers.models.openai.modeling_tf_openai.self.multiple_choice_head->TFSequenceSummary(config, initializer_range=config.initializer_range, name='multiple_choice_head')
A:transformers.models.openai.modeling_tf_openai.input_shapes->shape_list(inputs['input_ids'])
A:transformers.models.openai.modeling_tf_openai.lm_logits->self.transformer.tokens_embed(hidden_states, mode='linear')
A:transformers.models.openai.modeling_tf_openai.mc_logits->tensorflow.squeeze(mc_logits, axis=-1)
A:transformers.models.openai.modeling_tf_openai.self.score->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='score', use_bias=False)
A:transformers.models.openai.modeling_tf_openai.in_logits->tensorflow.gather(logits, sequence_lengths, batch_dims=1, axis=1)
transformers.TFOpenAIGPTDoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTDoubleHeadsModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFOpenAIGPTDoubleHeadsModel.serving(self,inputs)
transformers.TFOpenAIGPTDoubleHeadsModel.serving_output(self,output)
transformers.TFOpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.TFOpenAIGPTForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFOpenAIGPTForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFOpenAIGPTLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTLMHeadModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFOpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.TFOpenAIGPTLMHeadModel.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.TFOpenAIGPTLMHeadModel.set_output_embeddings(self,value)
transformers.TFOpenAIGPTMainLayer(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTMainLayer._prune_heads(self,heads_to_prune)
transformers.TFOpenAIGPTMainLayer.build(self,input_shape)
transformers.TFOpenAIGPTMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFOpenAIGPTMainLayer.get_input_embeddings(self)
transformers.TFOpenAIGPTMainLayer.set_input_embeddings(self,value)
transformers.TFOpenAIGPTModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFOpenAIGPTModel.serving_output(self,output)
transformers.TFOpenAIGPTPreTrainedModel(TFPreTrainedModel)
transformers.TFOpenAIGPTPreTrainedModel.serving(self,inputs)
transformers.models.openai.modeling_tf_openai.TFAttention(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFAttention.__init__(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFAttention._attn(self,q,k,v,attention_mask,head_mask,output_attentions,training=False)
transformers.models.openai.modeling_tf_openai.TFAttention.call(self,x,attention_mask,head_mask,output_attentions,training=False)
transformers.models.openai.modeling_tf_openai.TFAttention.causal_attention_mask(nd,ns)
transformers.models.openai.modeling_tf_openai.TFAttention.merge_heads(self,x)
transformers.models.openai.modeling_tf_openai.TFAttention.prune_heads(self,heads)
transformers.models.openai.modeling_tf_openai.TFAttention.split_heads(self,x)
transformers.models.openai.modeling_tf_openai.TFBlock(self,n_ctx,config,scale=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFBlock.__init__(self,n_ctx,config,scale=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFBlock.call(self,x,attention_mask,head_mask,output_attentions,training=False)
transformers.models.openai.modeling_tf_openai.TFMLP(self,n_state,config,**kwargs)
transformers.models.openai.modeling_tf_openai.TFMLP.__init__(self,n_state,config,**kwargs)
transformers.models.openai.modeling_tf_openai.TFMLP.call(self,x,training=False)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.__init__(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.serving(self,inputs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.serving_output(self,output)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTLMHeadModel(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTLMHeadModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTLMHeadModel.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTLMHeadModel.set_output_embeddings(self,value)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer.__init__(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer._prune_heads(self,heads_to_prune)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer.build(self,input_shape)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer.get_input_embeddings(self)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTMainLayer.set_input_embeddings(self,value)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTModel(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTModel.__init__(self,config,*inputs,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTModel.serving_output(self,output)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTPreTrainedModel(TFPreTrainedModel)
transformers.models.openai.modeling_tf_openai.TFOpenAIGPTPreTrainedModel.serving(self,inputs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/openai/__init__.py----------------------------------------
A:transformers.models.openai.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bertweet/__init__.py----------------------------------------
A:transformers.models.bertweet.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bertweet/tokenization_bertweet.py----------------------------------------
A:transformers.models.bertweet.tokenization_bertweet.logger->utils.logging.get_logger(__name__)
A:transformers.models.bertweet.tokenization_bertweet.pairs->get_pairs(word)
A:transformers.models.bertweet.tokenization_bertweet.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.bertweet.tokenization_bertweet.self.tweetPreprocessor->TweetTokenizer()
A:transformers.models.bertweet.tokenization_bertweet.word->'@@ '.join(word)
A:transformers.models.bertweet.tokenization_bertweet.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.bertweet.tokenization_bertweet.j->'@@ '.join(word).index(first, i)
A:transformers.models.bertweet.tokenization_bertweet.new_word->tuple(new_word)
A:transformers.models.bertweet.tokenization_bertweet.text->reduce_lengthening(text)
A:transformers.models.bertweet.tokenization_bertweet.words->list(map(lambda x: x if EMOTICON_RE.search(x) else x.lower(), words))
A:transformers.models.bertweet.tokenization_bertweet.tweet->tweet.replace(punct, self.special_puncts[punct]).replace(punct, self.special_puncts[punct])
A:transformers.models.bertweet.tokenization_bertweet.tokens->self.tweetPreprocessor.tokenize(tweet)
A:transformers.models.bertweet.tokenization_bertweet.normTweet->normTweet.replace(' p . m .', '  p.m.').replace(' p . m ', ' p.m ').replace(' a . m .', ' a.m.').replace(' a . m ', ' a.m ').replace(' p . m .', '  p.m.').replace(' p . m ', ' p.m ').replace(' a . m .', ' a.m.').replace(' a . m ', ' a.m ')
A:transformers.models.bertweet.tokenization_bertweet.lowercased_token->token.lower()
A:transformers.models.bertweet.tokenization_bertweet.out_string->' '.join(tokens).replace('@@ ', '').strip()
A:transformers.models.bertweet.tokenization_bertweet.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.bertweet.tokenization_bertweet.out_merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
A:transformers.models.bertweet.tokenization_bertweet.lines->f.readlines()
A:transformers.models.bertweet.tokenization_bertweet.line->lineTmp.strip()
A:transformers.models.bertweet.tokenization_bertweet.idx->lineTmp.strip().rfind(' ')
A:transformers.models.bertweet.tokenization_bertweet.self.encoder[word]->len(self.encoder)
A:transformers.models.bertweet.tokenization_bertweet.WORD_RE->regex.compile('(%s)' % '|'.join(REGEXPS), regex.VERBOSE | regex.I | regex.UNICODE)
A:transformers.models.bertweet.tokenization_bertweet.HANG_RE->regex.compile('([^a-zA-Z0-9])\\1{3,}')
A:transformers.models.bertweet.tokenization_bertweet.EMOTICON_RE->regex.compile(EMOTICONS, regex.VERBOSE | regex.I | regex.UNICODE)
A:transformers.models.bertweet.tokenization_bertweet.ENT_RE->regex.compile('&(#?(x?))([^&;\\s]+);')
A:transformers.models.bertweet.tokenization_bertweet.entity_body->match.group(3)
A:transformers.models.bertweet.tokenization_bertweet.number->html.entities.name2codepoint.get(entity_body)
A:transformers.models.bertweet.tokenization_bertweet.safe_text->regex.compile('([^a-zA-Z0-9])\\1{3,}').sub('\\1\\1\\1', text)
A:transformers.models.bertweet.tokenization_bertweet.pattern->regex.compile('(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){20}(?!@))|(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){1,19})(?![A-Za-z0-9_]*@)')
transformers.BertweetTokenizer(self,vocab_file,merges_file,normalization=False,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.BertweetTokenizer._convert_id_to_token(self,index)
transformers.BertweetTokenizer._convert_token_to_id(self,token)
transformers.BertweetTokenizer._tokenize(self,text)
transformers.BertweetTokenizer.add_from_file(self,f)
transformers.BertweetTokenizer.bpe(self,token)
transformers.BertweetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertweetTokenizer.convert_tokens_to_string(self,tokens)
transformers.BertweetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertweetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.BertweetTokenizer.get_vocab(self)
transformers.BertweetTokenizer.normalizeToken(self,token)
transformers.BertweetTokenizer.normalizeTweet(self,tweet)
transformers.BertweetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.BertweetTokenizer.vocab_size(self)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer(self,vocab_file,merges_file,normalization=False,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.__init__(self,vocab_file,merges_file,normalization=False,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer._convert_id_to_token(self,index)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer._convert_token_to_id(self,token)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer._tokenize(self,text)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.add_from_file(self,f)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.bpe(self,token)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.get_vocab(self)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.normalizeToken(self,token)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.normalizeTweet(self,tweet)
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.bertweet.tokenization_bertweet.BertweetTokenizer.vocab_size(self)
transformers.models.bertweet.tokenization_bertweet.TweetTokenizer(self,preserve_case=True,reduce_len=False,strip_handles=False)
transformers.models.bertweet.tokenization_bertweet.TweetTokenizer.__init__(self,preserve_case=True,reduce_len=False,strip_handles=False)
transformers.models.bertweet.tokenization_bertweet.TweetTokenizer.tokenize(self,text)
transformers.models.bertweet.tokenization_bertweet._replace_html_entities(text,keep=(),remove_illegal=True,encoding='utf-8')
transformers.models.bertweet.tokenization_bertweet._str_to_unicode(text,encoding=None,errors='strict')
transformers.models.bertweet.tokenization_bertweet.casual_tokenize(text,preserve_case=True,reduce_len=False,strip_handles=False)
transformers.models.bertweet.tokenization_bertweet.get_pairs(word)
transformers.models.bertweet.tokenization_bertweet.reduce_lengthening(text)
transformers.models.bertweet.tokenization_bertweet.remove_handles(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.k->k.replace('norm3', 'final_layer_norm').replace('norm3', 'final_layer_norm')
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.v->sd.pop(k)
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.new_k->rename_state_dict_key(k)
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.model->torch.load(checkpoint_path, map_location='cpu')
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.cfg->transformers.BartConfig.from_json_file(config_json_path)
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.m->BartForConditionalGeneration(cfg)
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.valid_keys->BartForConditionalGeneration(cfg).model.state_dict().keys()
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.convert_parlai_checkpoint(checkpoint_path,pytorch_dump_folder_path,config_json_path)
transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.rename_layernorm_keys(sd)
transformers.models.blenderbot.convert_blenderbot_original_pytorch_checkpoint_to_pytorch.rename_state_dict_key(k)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot/tokenization_blenderbot.py----------------------------------------
A:transformers.models.blenderbot.tokenization_blenderbot.logger->utils.logging.get_logger(__name__)
A:transformers.models.blenderbot.tokenization_blenderbot.full_string->'  '.join(inputs)
A:transformers.models.blenderbot.tokenization_blenderbot.input_ids->self.encode(full_string)
A:transformers.models.blenderbot.tokenization_blenderbot.pairs->set(pairs)
transformers.BlenderbotTokenizer(RobertaTokenizer)
transformers.BlenderbotTokenizer._build_conversation_input_ids(self,conversation:'Conversation')->List[int]
transformers.BlenderbotTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:List[int]=None)
transformers.models.blenderbot.tokenization_blenderbot.BlenderbotTokenizer(RobertaTokenizer)
transformers.models.blenderbot.tokenization_blenderbot.BlenderbotTokenizer._build_conversation_input_ids(self,conversation:'Conversation')->List[int]
transformers.models.blenderbot.tokenization_blenderbot.BlenderbotTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:List[int]=None)
transformers.models.blenderbot.tokenization_blenderbot.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot/configuration_blenderbot.py----------------------------------------
A:transformers.models.blenderbot.configuration_blenderbot.logger->utils.logging.get_logger(__name__)
transformers.BlenderbotConfig(self,vocab_size=8008,max_position_embeddings=128,encoder_layers=2,encoder_ffn_dim=10240,encoder_attention_heads=32,decoder_layers=24,decoder_ffn_dim=10240,decoder_attention_heads=32,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=2560,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=1,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,encoder_no_repeat_ngram_size=3,forced_eos_token_id=2,**kwargs)
transformers.BlenderbotConfig.hidden_size(self)->int
transformers.BlenderbotConfig.num_attention_heads(self)->int
transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig(self,vocab_size=8008,max_position_embeddings=128,encoder_layers=2,encoder_ffn_dim=10240,encoder_attention_heads=32,decoder_layers=24,decoder_ffn_dim=10240,decoder_attention_heads=32,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=2560,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=1,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,encoder_no_repeat_ngram_size=3,forced_eos_token_id=2,**kwargs)
transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig.__init__(self,vocab_size=8008,max_position_embeddings=128,encoder_layers=2,encoder_ffn_dim=10240,encoder_attention_heads=32,decoder_layers=24,decoder_ffn_dim=10240,decoder_attention_heads=32,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=2560,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=1,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,encoder_no_repeat_ngram_size=3,forced_eos_token_id=2,**kwargs)
transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig.hidden_size(self)->int
transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot/modeling_tf_blenderbot.py----------------------------------------
A:transformers.models.blenderbot.modeling_tf_blenderbot.logger->utils.logging.get_logger(__name__)
A:transformers.models.blenderbot.modeling_tf_blenderbot.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.blenderbot.modeling_tf_blenderbot.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.blenderbot.modeling_tf_blenderbot.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.blenderbot.modeling_tf_blenderbot.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.blenderbot.modeling_tf_blenderbot.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.blenderbot.modeling_tf_blenderbot.one_cst->tensorflow.constant(1.0)
A:transformers.models.blenderbot.modeling_tf_blenderbot.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.blenderbot.modeling_tf_blenderbot.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.blenderbot.modeling_tf_blenderbot.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.blenderbot.modeling_tf_blenderbot.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.blenderbot.modeling_tf_blenderbot.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.blenderbot.modeling_tf_blenderbot.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.blenderbot.modeling_tf_blenderbot.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.blenderbot.modeling_tf_blenderbot.attention_mask->_expand_mask(inputs['attention_mask'])
A:transformers.models.blenderbot.modeling_tf_blenderbot.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.blenderbot.modeling_tf_blenderbot.attn_output->self.out_proj(attn_output)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.self_attn->TFBlenderbotAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.blenderbot.modeling_tf_blenderbot.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.blenderbot.modeling_tf_blenderbot.(hidden_states, self_attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.encoder_attn->TFBlenderbotAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.blenderbot.modeling_tf_blenderbot.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.blenderbot.modeling_tf_blenderbot.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.blenderbot.modeling_tf_blenderbot.input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.blenderbot.modeling_tf_blenderbot.decoder_input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.blenderbot.modeling_tf_blenderbot.output->self.call(inputs)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.embed_positions->TFBlenderbotLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layer_norm')
A:transformers.models.blenderbot.modeling_tf_blenderbot.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.blenderbot.modeling_tf_blenderbot.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.blenderbot.modeling_tf_blenderbot.embed_pos->self.embed_positions(input_shape)
A:transformers.models.blenderbot.modeling_tf_blenderbot.dropout_probability->random.uniform(0, 1)
A:transformers.models.blenderbot.modeling_tf_blenderbot.(hidden_states, attn)->encoder_layer(hidden_states, attention_mask, inputs['head_mask'][idx] if inputs['head_mask'] is not None else None)
A:transformers.models.blenderbot.modeling_tf_blenderbot.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.blenderbot.modeling_tf_blenderbot.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.blenderbot.modeling_tf_blenderbot.(hidden_states, layer_self_attn, layer_cross_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, cross_attn_layer_head_mask=inputs['cross_attn_head_mask'][idx] if inputs['cross_attn_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.blenderbot.modeling_tf_blenderbot.all_self_attns->list(all_self_attns)
A:transformers.models.blenderbot.modeling_tf_blenderbot.all_cross_attns->list(all_cross_attns)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='model.shared')
A:transformers.models.blenderbot.modeling_tf_blenderbot.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.encoder->TFBlenderbotEncoder(config, embed_tokens, name='encoder')
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.decoder->TFBlenderbotDecoder(config, embed_tokens, name='decoder')
A:transformers.models.blenderbot.modeling_tf_blenderbot.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.blenderbot.modeling_tf_blenderbot.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.model->TFBlenderbotMainLayer(config, name='model')
A:transformers.models.blenderbot.modeling_tf_blenderbot.outputs->self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], encoder_outputs=inputs['encoder_outputs'], decoder_attention_mask=inputs['decoder_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.blenderbot.modeling_tf_blenderbot.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.blenderbot.modeling_tf_blenderbot.inputs['labels']->tensorflow.where(inputs['labels'] == self.config.pad_token_id, tf.fill(shape_list(inputs['labels']), -100), inputs['labels'])
A:transformers.models.blenderbot.modeling_tf_blenderbot.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.blenderbot.modeling_tf_blenderbot.lm_logits->self.model.shared(outputs[0], mode='linear')
A:transformers.models.blenderbot.modeling_tf_blenderbot.encoder_outputs->TFBaseModelOutput(last_hidden_state=encoder_outputs)
transformers.TFBlenderbotForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFBlenderbotForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.TFBlenderbotForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFBlenderbotForConditionalGeneration.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.TFBlenderbotForConditionalGeneration.get_bias(self)
transformers.TFBlenderbotForConditionalGeneration.get_decoder(self)
transformers.TFBlenderbotForConditionalGeneration.get_encoder(self)
transformers.TFBlenderbotForConditionalGeneration.get_output_embeddings(self)
transformers.TFBlenderbotForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFBlenderbotForConditionalGeneration.serving_output(self,output)
transformers.TFBlenderbotForConditionalGeneration.set_bias(self,value)
transformers.TFBlenderbotForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFBlenderbotModel(self,config:BlenderbotConfig,*inputs,**kwargs)
transformers.TFBlenderbotModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFBlenderbotModel.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.TFBlenderbotModel.get_decoder(self)
transformers.TFBlenderbotModel.get_encoder(self)
transformers.TFBlenderbotModel.serving_output(self,output)
transformers.TFBlenderbotPreTrainedModel(TFPreTrainedModel)
transformers.TFBlenderbotPreTrainedModel.dummy_inputs(self)
transformers.TFBlenderbotPreTrainedModel.serving(self,inputs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoder(self,config:BlenderbotConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoder.__init__(self,config:BlenderbotConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoder.get_embed_tokens(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoderLayer(self,config:BlenderbotConfig,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoderLayer.__init__(self,config:BlenderbotConfig,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,cross_attn_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoder(self,config:BlenderbotConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoder.__init__(self,config:BlenderbotConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoder.get_embed_tokens(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoderLayer(self,config:BlenderbotConfig,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoderLayer.__init__(self,config:BlenderbotConfig,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,training=False)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.get_bias(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.get_decoder(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.get_encoder(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.get_output_embeddings(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.serving_output(self,output)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.set_bias(self,value)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotLearnedPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotMainLayer(self,config:BlenderbotConfig,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotMainLayer.__init__(self,config:BlenderbotConfig,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotMainLayer.get_input_embeddings(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel(self,config:BlenderbotConfig,*inputs,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel.__init__(self,config:BlenderbotConfig,*inputs,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel.get_decoder(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel.get_encoder(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotModel.serving_output(self,output)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotPreTrainedModel(TFPreTrainedModel)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotPreTrainedModel.dummy_inputs(self)
transformers.models.blenderbot.modeling_tf_blenderbot.TFBlenderbotPreTrainedModel.serving(self,inputs)
transformers.models.blenderbot.modeling_tf_blenderbot._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.blenderbot.modeling_tf_blenderbot._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.blenderbot.modeling_tf_blenderbot.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot/modeling_blenderbot.py----------------------------------------
A:transformers.models.blenderbot.modeling_blenderbot.logger->utils.logging.get_logger(__name__)
A:transformers.models.blenderbot.modeling_blenderbot.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.blenderbot.modeling_blenderbot.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.blenderbot.modeling_blenderbot.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.blenderbot.modeling_blenderbot.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.blenderbot.modeling_blenderbot.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.blenderbot.modeling_blenderbot.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.blenderbot.modeling_blenderbot.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.blenderbot.modeling_blenderbot.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot.modeling_blenderbot.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot.modeling_blenderbot.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot.modeling_blenderbot.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot.modeling_blenderbot.(bsz, tgt_len, embed_dim)->self.layer_norm(hidden_states).size()
A:transformers.models.blenderbot.modeling_blenderbot.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.blenderbot.modeling_blenderbot.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.blenderbot.modeling_blenderbot.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.blenderbot.modeling_blenderbot.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.blenderbot.modeling_blenderbot.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.blenderbot.modeling_blenderbot.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.blenderbot.modeling_blenderbot.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.blenderbot.modeling_blenderbot.attn_output->self.out_proj(attn_output)
A:transformers.models.blenderbot.modeling_blenderbot.self.self_attn->BlenderbotAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.blenderbot.modeling_blenderbot.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.blenderbot.modeling_blenderbot.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.blenderbot.modeling_blenderbot.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.blenderbot.modeling_blenderbot.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.blenderbot.modeling_blenderbot.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.blenderbot.modeling_blenderbot.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.blenderbot.modeling_blenderbot.self.encoder_attn->BlenderbotAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.blenderbot.modeling_blenderbot.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.blenderbot.modeling_blenderbot.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.blenderbot.modeling_blenderbot.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.blenderbot.modeling_blenderbot.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.blenderbot.modeling_blenderbot.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.blenderbot.modeling_blenderbot.self.embed_positions->BlenderbotLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model)
A:transformers.models.blenderbot.modeling_blenderbot.self.layers->torch.nn.ModuleList([BlenderbotDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.blenderbot.modeling_blenderbot.self.layer_norm->torch.nn.LayerNorm(config.d_model)
A:transformers.models.blenderbot.modeling_blenderbot.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.blenderbot.modeling_blenderbot.embed_pos->self.embed_positions(input_shape)
A:transformers.models.blenderbot.modeling_blenderbot.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.blenderbot.modeling_blenderbot.dropout_probability->random.uniform(0, 1)
A:transformers.models.blenderbot.modeling_blenderbot.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.blenderbot.modeling_blenderbot.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.blenderbot.modeling_blenderbot.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.blenderbot.modeling_blenderbot.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.blenderbot.modeling_blenderbot.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.blenderbot.modeling_blenderbot.self.encoder->BlenderbotEncoder(config, self.shared)
A:transformers.models.blenderbot.modeling_blenderbot.self.decoder->BlenderbotDecoder(config)
A:transformers.models.blenderbot.modeling_blenderbot.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.blenderbot.modeling_blenderbot.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.blenderbot.modeling_blenderbot.self.model->BlenderbotDecoderWrapper(config)
A:transformers.models.blenderbot.modeling_blenderbot.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.blenderbot.modeling_blenderbot.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.blenderbot.modeling_blenderbot.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.blenderbot.modeling_blenderbot.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.blenderbot.modeling_blenderbot.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.blenderbot.modeling_blenderbot.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.blenderbot.modeling_blenderbot.loss_fct->CrossEntropyLoss()
A:transformers.models.blenderbot.modeling_blenderbot.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.blenderbot.modeling_blenderbot.config->copy.deepcopy(config)
A:transformers.models.blenderbot.modeling_blenderbot.logits->self.lm_head(outputs[0])
A:transformers.models.blenderbot.modeling_blenderbot.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
transformers.BlenderbotForCausalLM(self,config)
transformers.BlenderbotForCausalLM._reorder_cache(past,beam_idx)
transformers.BlenderbotForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BlenderbotForCausalLM.get_decoder(self)
transformers.BlenderbotForCausalLM.get_input_embeddings(self)
transformers.BlenderbotForCausalLM.get_output_embeddings(self)
transformers.BlenderbotForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.BlenderbotForCausalLM.set_decoder(self,decoder)
transformers.BlenderbotForCausalLM.set_input_embeddings(self,value)
transformers.BlenderbotForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.BlenderbotForConditionalGeneration(self,config:BlenderbotConfig)
transformers.BlenderbotForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.BlenderbotForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.BlenderbotForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BlenderbotForConditionalGeneration.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.BlenderbotForConditionalGeneration.get_decoder(self)
transformers.BlenderbotForConditionalGeneration.get_encoder(self)
transformers.BlenderbotForConditionalGeneration.get_output_embeddings(self)
transformers.BlenderbotForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.BlenderbotForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.BlenderbotForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.BlenderbotModel(self,config:BlenderbotConfig)
transformers.BlenderbotModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BlenderbotModel.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.BlenderbotModel.get_decoder(self)
transformers.BlenderbotModel.get_encoder(self)
transformers.BlenderbotModel.get_input_embeddings(self)
transformers.BlenderbotModel.set_input_embeddings(self,value)
transformers.BlenderbotPreTrainedModel(PreTrainedModel)
transformers.BlenderbotPreTrainedModel._init_weights(self,module)
transformers.BlenderbotPreTrainedModel.dummy_inputs(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoder(self,config:BlenderbotConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoder.__init__(self,config:BlenderbotConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoder.get_input_embeddings(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoder.set_input_embeddings(self,value)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoderLayer(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoderLayer.__init__(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoderWrapper(self,config)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoderWrapper.__init__(self,config)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotEncoder(self,config:BlenderbotConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotEncoder.__init__(self,config:BlenderbotConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotEncoderLayer(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotEncoderLayer.__init__(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM(self,config)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.__init__(self,config)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM._reorder_cache(past,beam_idx)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.get_decoder(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.get_input_embeddings(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.get_output_embeddings(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.set_decoder(self,decoder)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.set_input_embeddings(self,value)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.__init__(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.get_decoder(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.get_encoder(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.get_output_embeddings(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.blenderbot.modeling_blenderbot.BlenderbotForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotLearnedPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.__init__(self,config:BlenderbotConfig)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.from_pretrained(cls,pretrained_model_name_or_path:Optional[Union[str,os.PathLike]],*model_args,**kwargs)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.get_decoder(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.get_encoder(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.get_input_embeddings(self)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotModel.set_input_embeddings(self,value)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotPreTrainedModel(PreTrainedModel)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotPreTrainedModel._init_weights(self,module)
transformers.models.blenderbot.modeling_blenderbot.BlenderbotPreTrainedModel.dummy_inputs(self)
transformers.models.blenderbot.modeling_blenderbot._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.blenderbot.modeling_blenderbot._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.blenderbot.modeling_blenderbot.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot/__init__.py----------------------------------------
A:transformers.models.blenderbot.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/byt5/tokenization_byt5.py----------------------------------------
A:transformers.models.byt5.tokenization_byt5.logger->utils.logging.get_logger(__name__)
A:transformers.models.byt5.tokenization_byt5.extra_tokens->len(set(filter(lambda x: bool('extra_id' in str(x)), additional_special_tokens)))
A:transformers.models.byt5.tokenization_byt5.self._num_special_tokens->len(self.special_tokens_encoder)
A:transformers.models.byt5.tokenization_byt5.token_ids_0->self._add_eos_if_not_present(token_ids_0)
A:transformers.models.byt5.tokenization_byt5.token_ids_1->self._add_eos_if_not_present(token_ids_1)
A:transformers.models.byt5.tokenization_byt5.character_list->list(sub_text)
A:transformers.models.byt5.tokenization_byt5.sub_texts->list(filter(None, re.split(pattern, text)))
A:transformers.models.byt5.tokenization_byt5.match->re.match('<extra_id_(\\d+)>', token)
A:transformers.models.byt5.tokenization_byt5.num->int(match.group(1))
A:transformers.models.byt5.tokenization_byt5.token->chr(index - self._num_special_tokens)
A:transformers.models.byt5.tokenization_byt5.byte_string->bytes([ord(char) for char in sub_chars])
transformers.ByT5Tokenizer(self,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=125,additional_special_tokens=None,**kwargs)
transformers.ByT5Tokenizer._add_eos_if_not_present(self,token_ids:List[int])->List[int]
transformers.ByT5Tokenizer._convert_id_to_token(self,index)
transformers.ByT5Tokenizer._convert_token_to_id(self,token)
transformers.ByT5Tokenizer._tokenize(self,text:str)->List[str]
transformers.ByT5Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.ByT5Tokenizer.convert_tokens_to_string(self,tokens)
transformers.ByT5Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.ByT5Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.ByT5Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.ByT5Tokenizer.vocab_size(self)
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer(self,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=125,additional_special_tokens=None,**kwargs)
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.__init__(self,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=125,additional_special_tokens=None,**kwargs)
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer._add_eos_if_not_present(self,token_ids:List[int])->List[int]
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer._convert_id_to_token(self,index)
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer._convert_token_to_id(self,token)
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer._tokenize(self,text:str)->List[str]
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.convert_tokens_to_string(self,tokens)
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.byt5.tokenization_byt5.ByT5Tokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/byt5/__init__.py----------------------------------------
A:transformers.models.byt5.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.byt5.convert_byt5_original_tf_checkpoint_to_pytorch.config->transformers.T5Config.from_json_file(config_file)
A:transformers.models.byt5.convert_byt5_original_tf_checkpoint_to_pytorch.model->T5ForConditionalGeneration(config)
A:transformers.models.byt5.convert_byt5_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.byt5.convert_byt5_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.byt5.convert_byt5_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/modeling_t5.py----------------------------------------
A:transformers.models.t5.modeling_t5.logger->utils.logging.get_logger(__name__)
A:transformers.models.t5.modeling_t5.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.t5.modeling_t5.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.t5.modeling_t5.array->numpy.transpose(array)
A:transformers.models.t5.modeling_t5.name->txt_name.split('/')
A:transformers.models.t5.modeling_t5.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.t5.modeling_t5.pointer->getattr(pointer, 'weight')
A:transformers.models.t5.modeling_t5.num->int(scope_names[1])
A:transformers.models.t5.modeling_t5.pointer.data->torch.from_numpy(array.astype(np.float32))
A:transformers.models.t5.modeling_t5.self.weight->torch.nn.Parameter(torch.ones(hidden_size))
A:transformers.models.t5.modeling_t5.variance->hidden_states.to(self.decoder.first_device).to(torch.float32).pow(2).mean(-1, keepdim=True)
A:transformers.models.t5.modeling_t5.hidden_states->hidden_states.to(self.decoder.first_device).to(self.decoder.first_device)
A:transformers.models.t5.modeling_t5.self.wi->torch.nn.Linear(config.d_model, config.d_ff, bias=False)
A:transformers.models.t5.modeling_t5.self.wo->torch.nn.Linear(config.d_ff, config.d_model, bias=False)
A:transformers.models.t5.modeling_t5.self.dropout->torch.nn.Dropout(config.dropout_rate)
A:transformers.models.t5.modeling_t5.self.wi_0->torch.nn.Linear(config.d_model, config.d_ff, bias=False)
A:transformers.models.t5.modeling_t5.self.wi_1->torch.nn.Linear(config.d_model, config.d_ff, bias=False)
A:transformers.models.t5.modeling_t5.hidden_gelu->self.gelu_act(self.wi_0(hidden_states))
A:transformers.models.t5.modeling_t5.hidden_linear->self.wi_1(hidden_states)
A:transformers.models.t5.modeling_t5.self.DenseReluDense->T5DenseGatedGeluDense(config)
A:transformers.models.t5.modeling_t5.self.layer_norm->T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
A:transformers.models.t5.modeling_t5.forwarded_states->self.DenseReluDense(forwarded_states)
A:transformers.models.t5.modeling_t5.self.q->prune_linear_layer(self.q, index)
A:transformers.models.t5.modeling_t5.self.k->prune_linear_layer(self.k, index)
A:transformers.models.t5.modeling_t5.self.v->prune_linear_layer(self.v, index)
A:transformers.models.t5.modeling_t5.self.o->prune_linear_layer(self.o, index, dim=1)
A:transformers.models.t5.modeling_t5.self.relative_attention_bias->torch.nn.Embedding(self.relative_attention_num_buckets, self.n_heads)
A:transformers.models.t5.modeling_t5.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.t5.modeling_t5.self.gradient_checkpointing->getattr(config, 'gradient_checkpointing', False)
A:transformers.models.t5.modeling_t5.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads)
A:transformers.models.t5.modeling_t5.relative_position->torch.abs(relative_position)
A:transformers.models.t5.modeling_t5.relative_postion_if_large->torch.min(relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1))
A:transformers.models.t5.modeling_t5.relative_position_bucket->relative_position_bucket.to(self.relative_attention_bias.weight.device).to(self.relative_attention_bias.weight.device)
A:transformers.models.t5.modeling_t5.values->values.permute([2, 0, 1]).unsqueeze(0).permute([2, 0, 1]).unsqueeze(0)
A:transformers.models.t5.modeling_t5.int_seq_length->int(seq_length)
A:transformers.models.t5.modeling_t5.query_states->shape(self.q(hidden_states))
A:transformers.models.t5.modeling_t5.key_states->project(hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None)
A:transformers.models.t5.modeling_t5.value_states->project(hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None)
A:transformers.models.t5.modeling_t5.scores->torch.matmul(query_states, key_states.transpose(3, 2))
A:transformers.models.t5.modeling_t5.position_bias->position_bias.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.t5.modeling_t5.attn_weights->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.t5.modeling_t5.attn_output->self.o(attn_output)
A:transformers.models.t5.modeling_t5.self.SelfAttention->T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
A:transformers.models.t5.modeling_t5.normed_hidden_states->self.layer_norm(hidden_states)
A:transformers.models.t5.modeling_t5.attention_output->self.EncDecAttention(normed_hidden_states, mask=attention_mask, key_value_states=key_value_states, position_bias=position_bias, layer_head_mask=layer_head_mask, past_key_value=past_key_value, use_cache=use_cache, query_length=query_length, output_attentions=output_attentions)
A:transformers.models.t5.modeling_t5.self.EncDecAttention->T5Attention(config, has_relative_attention_bias=False)
A:transformers.models.t5.modeling_t5.self.layer->torch.nn.ModuleList()
A:transformers.models.t5.modeling_t5.self_attention_outputs->self.layer[0](hidden_states, attention_mask=attention_mask, position_bias=position_bias, layer_head_mask=layer_head_mask, past_key_value=self_attn_past_key_value, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.t5.modeling_t5.cross_attention_outputs->self.layer[1](hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, position_bias=encoder_decoder_position_bias, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, query_length=query_length, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.t5.modeling_t5.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.t5.modeling_t5.input_mask->torch.tensor(DUMMY_MASK)
A:transformers.models.t5.modeling_t5.shifted_input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.t5.modeling_t5.shifted_input_ids[..., 1:]->input_ids[..., :-1].clone()
A:transformers.models.t5.modeling_t5.self.block->torch.nn.ModuleList([T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)])
A:transformers.models.t5.modeling_t5.self.final_layer_norm->self.final_layer_norm.to('cpu')
A:transformers.models.t5.modeling_t5.self.block[layer]->self.block[layer].to(cuda_device)
A:transformers.models.t5.modeling_t5.self.embed_tokens->self.embed_tokens.to(self.first_device)
A:transformers.models.t5.modeling_t5.self.block[i]->self.block[i].to('cpu')
A:transformers.models.t5.modeling_t5.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.t5.modeling_t5.inputs_embeds->self.embed_tokens(input_ids)
A:transformers.models.t5.modeling_t5.attention_mask->attention_mask.to(self.decoder.first_device).to(self.decoder.first_device)
A:transformers.models.t5.modeling_t5.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=inputs_embeds.device)
A:transformers.models.t5.modeling_t5.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, inputs_embeds.device)
A:transformers.models.t5.modeling_t5.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.to(hidden_states.device).size()
A:transformers.models.t5.modeling_t5.encoder_extended_attention_mask->encoder_extended_attention_mask.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.t5.modeling_t5.head_mask->self.get_head_mask(head_mask, self.config.num_layers)
A:transformers.models.t5.modeling_t5.cross_attn_head_mask->self.get_head_mask(cross_attn_head_mask, self.config.num_layers)
A:transformers.models.t5.modeling_t5.encoder_hidden_states->encoder_hidden_states.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.t5.modeling_t5.encoder_decoder_position_bias->encoder_decoder_position_bias.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.t5.modeling_t5.layer_head_mask->layer_head_mask.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.t5.modeling_t5.cross_attn_layer_head_mask->cross_attn_layer_head_mask.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.t5.modeling_t5.layer_outputs->layer_module(hidden_states, attention_mask=extended_attention_mask, position_bias=position_bias, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, encoder_decoder_position_bias=encoder_decoder_position_bias, layer_head_mask=layer_head_mask, cross_attn_layer_head_mask=cross_attn_layer_head_mask, past_key_value=past_key_value, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.t5.modeling_t5.self.shared->torch.nn.Embedding(config.vocab_size, config.d_model)
A:transformers.models.t5.modeling_t5.encoder_config->copy.deepcopy(config)
A:transformers.models.t5.modeling_t5.self.encoder->self.encoder.to('cpu')
A:transformers.models.t5.modeling_t5.decoder_config->copy.deepcopy(config)
A:transformers.models.t5.modeling_t5.self.decoder->self.decoder.to('cpu')
A:transformers.models.t5.modeling_t5.encoder_outputs->self.encoder(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.t5.modeling_t5.decoder_input_ids->decoder_input_ids.to(self.decoder.first_device).to(self.decoder.first_device)
A:transformers.models.t5.modeling_t5.decoder_attention_mask->decoder_attention_mask.to(self.decoder.first_device).to(self.decoder.first_device)
A:transformers.models.t5.modeling_t5.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, inputs_embeds=decoder_inputs_embeds, past_key_values=past_key_values, encoder_hidden_states=hidden_states, encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.t5.modeling_t5.self.lm_head->self.lm_head.to(self.encoder.first_device)
A:transformers.models.t5.modeling_t5.sequence_output->sequence_output.to(self.lm_head.weight.device).to(self.lm_head.weight.device)
A:transformers.models.t5.modeling_t5.lm_logits->self.lm_head(sequence_output)
A:transformers.models.t5.modeling_t5.loss_fct->CrossEntropyLoss(ignore_index=-100)
A:transformers.models.t5.modeling_t5.loss->loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))
transformers.T5EncoderModel(self,config:T5Config)
transformers.T5EncoderModel._prune_heads(self,heads_to_prune)
transformers.T5EncoderModel.deparallelize(self)
transformers.T5EncoderModel.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.T5EncoderModel.get_encoder(self)
transformers.T5EncoderModel.get_input_embeddings(self)
transformers.T5EncoderModel.parallelize(self,device_map=None)
transformers.T5EncoderModel.set_input_embeddings(self,new_embeddings)
transformers.T5ForConditionalGeneration(self,config)
transformers.T5ForConditionalGeneration._reorder_cache(self,past,beam_idx)
transformers.T5ForConditionalGeneration.deparallelize(self)
transformers.T5ForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.T5ForConditionalGeneration.get_decoder(self)
transformers.T5ForConditionalGeneration.get_encoder(self)
transformers.T5ForConditionalGeneration.get_input_embeddings(self)
transformers.T5ForConditionalGeneration.get_output_embeddings(self)
transformers.T5ForConditionalGeneration.parallelize(self,device_map=None)
transformers.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.T5ForConditionalGeneration.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.T5ForConditionalGeneration.set_input_embeddings(self,new_embeddings)
transformers.T5ForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.T5Model(self,config:T5Config)
transformers.T5Model._prune_heads(self,heads_to_prune)
transformers.T5Model.deparallelize(self)
transformers.T5Model.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.T5Model.get_decoder(self)
transformers.T5Model.get_encoder(self)
transformers.T5Model.get_input_embeddings(self)
transformers.T5Model.parallelize(self,device_map=None)
transformers.T5Model.set_input_embeddings(self,new_embeddings)
transformers.T5PreTrainedModel(PreTrainedModel)
transformers.T5PreTrainedModel._init_weights(self,module)
transformers.T5PreTrainedModel._shift_right(self,input_ids)
transformers.T5PreTrainedModel.dummy_inputs(self)
transformers.load_tf_weights_in_t5(model,config,tf_checkpoint_path)
transformers.models.t5.modeling_t5.T5Attention(self,config:T5Config,has_relative_attention_bias=False)
transformers.models.t5.modeling_t5.T5Attention.__init__(self,config:T5Config,has_relative_attention_bias=False)
transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket(relative_position,bidirectional=True,num_buckets=32,max_distance=128)
transformers.models.t5.modeling_t5.T5Attention.compute_bias(self,query_length,key_length)
transformers.models.t5.modeling_t5.T5Attention.forward(self,hidden_states,mask=None,key_value_states=None,position_bias=None,past_key_value=None,layer_head_mask=None,query_length=None,use_cache=False,output_attentions=False)
transformers.models.t5.modeling_t5.T5Attention.prune_heads(self,heads)
transformers.models.t5.modeling_t5.T5Block(self,config,has_relative_attention_bias=False)
transformers.models.t5.modeling_t5.T5Block.__init__(self,config,has_relative_attention_bias=False)
transformers.models.t5.modeling_t5.T5Block.forward(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,layer_head_mask=None,cross_attn_layer_head_mask=None,past_key_value=None,use_cache=False,output_attentions=False,return_dict=True)
transformers.models.t5.modeling_t5.T5DenseGatedGeluDense(self,config)
transformers.models.t5.modeling_t5.T5DenseGatedGeluDense.__init__(self,config)
transformers.models.t5.modeling_t5.T5DenseGatedGeluDense.forward(self,hidden_states)
transformers.models.t5.modeling_t5.T5DenseReluDense(self,config)
transformers.models.t5.modeling_t5.T5DenseReluDense.__init__(self,config)
transformers.models.t5.modeling_t5.T5DenseReluDense.forward(self,hidden_states)
transformers.models.t5.modeling_t5.T5EncoderModel(self,config:T5Config)
transformers.models.t5.modeling_t5.T5EncoderModel.__init__(self,config:T5Config)
transformers.models.t5.modeling_t5.T5EncoderModel._prune_heads(self,heads_to_prune)
transformers.models.t5.modeling_t5.T5EncoderModel.deparallelize(self)
transformers.models.t5.modeling_t5.T5EncoderModel.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.t5.modeling_t5.T5EncoderModel.get_encoder(self)
transformers.models.t5.modeling_t5.T5EncoderModel.get_input_embeddings(self)
transformers.models.t5.modeling_t5.T5EncoderModel.parallelize(self,device_map=None)
transformers.models.t5.modeling_t5.T5EncoderModel.set_input_embeddings(self,new_embeddings)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration(self,config)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.__init__(self,config)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration._reorder_cache(self,past,beam_idx)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.deparallelize(self)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_decoder(self)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder(self)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_input_embeddings(self)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_output_embeddings(self)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.parallelize(self,device_map=None)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_input_embeddings(self,new_embeddings)
transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.t5.modeling_t5.T5LayerCrossAttention(self,config)
transformers.models.t5.modeling_t5.T5LayerCrossAttention.__init__(self,config)
transformers.models.t5.modeling_t5.T5LayerCrossAttention.forward(self,hidden_states,key_value_states,attention_mask=None,position_bias=None,layer_head_mask=None,past_key_value=None,use_cache=False,query_length=None,output_attentions=False)
transformers.models.t5.modeling_t5.T5LayerFF(self,config)
transformers.models.t5.modeling_t5.T5LayerFF.__init__(self,config)
transformers.models.t5.modeling_t5.T5LayerFF.forward(self,hidden_states)
transformers.models.t5.modeling_t5.T5LayerNorm(self,hidden_size,eps=1e-06)
transformers.models.t5.modeling_t5.T5LayerNorm.__init__(self,hidden_size,eps=1e-06)
transformers.models.t5.modeling_t5.T5LayerNorm.forward(self,hidden_states)
transformers.models.t5.modeling_t5.T5LayerSelfAttention(self,config,has_relative_attention_bias=False)
transformers.models.t5.modeling_t5.T5LayerSelfAttention.__init__(self,config,has_relative_attention_bias=False)
transformers.models.t5.modeling_t5.T5LayerSelfAttention.forward(self,hidden_states,attention_mask=None,position_bias=None,layer_head_mask=None,past_key_value=None,use_cache=False,output_attentions=False)
transformers.models.t5.modeling_t5.T5Model(self,config:T5Config)
transformers.models.t5.modeling_t5.T5Model.__init__(self,config:T5Config)
transformers.models.t5.modeling_t5.T5Model._prune_heads(self,heads_to_prune)
transformers.models.t5.modeling_t5.T5Model.deparallelize(self)
transformers.models.t5.modeling_t5.T5Model.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.t5.modeling_t5.T5Model.get_decoder(self)
transformers.models.t5.modeling_t5.T5Model.get_encoder(self)
transformers.models.t5.modeling_t5.T5Model.get_input_embeddings(self)
transformers.models.t5.modeling_t5.T5Model.parallelize(self,device_map=None)
transformers.models.t5.modeling_t5.T5Model.set_input_embeddings(self,new_embeddings)
transformers.models.t5.modeling_t5.T5PreTrainedModel(PreTrainedModel)
transformers.models.t5.modeling_t5.T5PreTrainedModel._init_weights(self,module)
transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right(self,input_ids)
transformers.models.t5.modeling_t5.T5PreTrainedModel.dummy_inputs(self)
transformers.models.t5.modeling_t5.T5Stack(self,config,embed_tokens=None)
transformers.models.t5.modeling_t5.T5Stack.__init__(self,config,embed_tokens=None)
transformers.models.t5.modeling_t5.T5Stack.deparallelize(self)
transformers.models.t5.modeling_t5.T5Stack.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,inputs_embeds=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.t5.modeling_t5.T5Stack.get_input_embeddings(self)
transformers.models.t5.modeling_t5.T5Stack.parallelize(self,device_map=None)
transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings(self,new_embeddings)
transformers.models.t5.modeling_t5.load_tf_weights_in_t5(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/configuration_t5.py----------------------------------------
A:transformers.models.t5.configuration_t5.logger->utils.logging.get_logger(__name__)
transformers.T5Config(self,vocab_size=32128,d_model=512,d_kv=64,d_ff=2048,num_layers=6,num_decoder_layers=None,num_heads=8,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,feed_forward_proj='relu',is_encoder_decoder=True,use_cache=True,pad_token_id=0,eos_token_id=1,gradient_checkpointing=False,**kwargs)
transformers.T5Config.hidden_size(self)
transformers.T5Config.num_attention_heads(self)
transformers.T5Config.num_hidden_layers(self)
transformers.models.t5.configuration_t5.T5Config(self,vocab_size=32128,d_model=512,d_kv=64,d_ff=2048,num_layers=6,num_decoder_layers=None,num_heads=8,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,feed_forward_proj='relu',is_encoder_decoder=True,use_cache=True,pad_token_id=0,eos_token_id=1,gradient_checkpointing=False,**kwargs)
transformers.models.t5.configuration_t5.T5Config.__init__(self,vocab_size=32128,d_model=512,d_kv=64,d_ff=2048,num_layers=6,num_decoder_layers=None,num_heads=8,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,feed_forward_proj='relu',is_encoder_decoder=True,use_cache=True,pad_token_id=0,eos_token_id=1,gradient_checkpointing=False,**kwargs)
transformers.models.t5.configuration_t5.T5Config.hidden_size(self)
transformers.models.t5.configuration_t5.T5Config.num_attention_heads(self)
transformers.models.t5.configuration_t5.T5Config.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/tokenization_t5.py----------------------------------------
A:transformers.models.t5.tokenization_t5.logger->utils.logging.get_logger(__name__)
A:transformers.models.t5.tokenization_t5.extra_tokens->len(set(filter(lambda x: bool('extra_id' in str(x)), additional_special_tokens)))
A:transformers.models.t5.tokenization_t5.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.t5.tokenization_t5.token_ids_0->self._add_eos_if_not_present(token_ids_0)
A:transformers.models.t5.tokenization_t5.token_ids_1->self._add_eos_if_not_present(token_ids_1)
A:transformers.models.t5.tokenization_t5.state->self.__dict__.copy()
A:transformers.models.t5.tokenization_t5.match->re.match('<extra_id_(\\d+)>', token)
A:transformers.models.t5.tokenization_t5.num->int(match.group(1))
A:transformers.models.t5.tokenization_t5.token->self.sp_model.IdToPiece(index)
A:transformers.models.t5.tokenization_t5.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.T5Tokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.T5Tokenizer.__getstate__(self)
transformers.T5Tokenizer.__setstate__(self,d)
transformers.T5Tokenizer._add_eos_if_not_present(self,token_ids:List[int])->List[int]
transformers.T5Tokenizer._convert_id_to_token(self,index)
transformers.T5Tokenizer._convert_token_to_id(self,token)
transformers.T5Tokenizer._tokenize(self,text:str)->List[str]
transformers.T5Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.T5Tokenizer.convert_tokens_to_string(self,tokens)
transformers.T5Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.T5Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.T5Tokenizer.get_vocab(self)
transformers.T5Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.T5Tokenizer.vocab_size(self)
transformers.models.t5.tokenization_t5.T5Tokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.t5.tokenization_t5.T5Tokenizer.__getstate__(self)
transformers.models.t5.tokenization_t5.T5Tokenizer.__init__(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.t5.tokenization_t5.T5Tokenizer.__setstate__(self,d)
transformers.models.t5.tokenization_t5.T5Tokenizer._add_eos_if_not_present(self,token_ids:List[int])->List[int]
transformers.models.t5.tokenization_t5.T5Tokenizer._convert_id_to_token(self,index)
transformers.models.t5.tokenization_t5.T5Tokenizer._convert_token_to_id(self,token)
transformers.models.t5.tokenization_t5.T5Tokenizer._tokenize(self,text:str)->List[str]
transformers.models.t5.tokenization_t5.T5Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.t5.tokenization_t5.T5Tokenizer.convert_tokens_to_string(self,tokens)
transformers.models.t5.tokenization_t5.T5Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.t5.tokenization_t5.T5Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.t5.tokenization_t5.T5Tokenizer.get_vocab(self)
transformers.models.t5.tokenization_t5.T5Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.t5.tokenization_t5.T5Tokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/tokenization_t5_fast.py----------------------------------------
A:transformers.models.t5.tokenization_t5_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.t5.tokenization_t5_fast.extra_tokens->len(set(filter(lambda x: bool('extra_id_' in str(x)), additional_special_tokens)))
A:transformers.models.t5.tokenization_t5_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.T5TokenizerFast(self,vocab_file,tokenizer_file=None,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,**kwargs)
transformers.T5TokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.T5TokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.T5TokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.t5.tokenization_t5_fast.T5TokenizerFast(self,vocab_file,tokenizer_file=None,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,**kwargs)
transformers.models.t5.tokenization_t5_fast.T5TokenizerFast.__init__(self,vocab_file,tokenizer_file=None,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,**kwargs)
transformers.models.t5.tokenization_t5_fast.T5TokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.t5.tokenization_t5_fast.T5TokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.t5.tokenization_t5_fast.T5TokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.t5.convert_t5_original_tf_checkpoint_to_pytorch.config->transformers.T5Config.from_json_file(config_file)
A:transformers.models.t5.convert_t5_original_tf_checkpoint_to_pytorch.model->T5ForConditionalGeneration(config)
A:transformers.models.t5.convert_t5_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.t5.convert_t5_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.t5.convert_t5_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/modeling_tf_t5.py----------------------------------------
A:transformers.models.t5.modeling_tf_t5.logger->utils.logging.get_logger(__name__)
A:transformers.models.t5.modeling_tf_t5.self.weight->self.add_weight('weight', shape=(input_shape[-1],), initializer='ones')
A:transformers.models.t5.modeling_tf_t5.variance->tensorflow.math.reduce_mean(tf.math.square(hidden_states), axis=-1, keepdims=True)
A:transformers.models.t5.modeling_tf_t5.self.wi->tensorflow.keras.layers.Dense(config.d_ff, use_bias=False, name='wi')
A:transformers.models.t5.modeling_tf_t5.self.wo->tensorflow.keras.layers.Dense(config.d_model, use_bias=False, name='wo')
A:transformers.models.t5.modeling_tf_t5.self.dropout->tensorflow.keras.layers.Dropout(config.dropout_rate)
A:transformers.models.t5.modeling_tf_t5.hidden_states->self.dropout(hidden_states, training=inputs['training'])
A:transformers.models.t5.modeling_tf_t5.self.wi_0->tensorflow.keras.layers.Dense(config.d_ff, use_bias=False, name='wi_0')
A:transformers.models.t5.modeling_tf_t5.self.wi_1->tensorflow.keras.layers.Dense(config.d_ff, use_bias=False, name='wi_1')
A:transformers.models.t5.modeling_tf_t5.self.act->get_tf_activation('gelu_new')
A:transformers.models.t5.modeling_tf_t5.hidden_gelu->self.act(self.wi_0(hidden_states))
A:transformers.models.t5.modeling_tf_t5.hidden_linear->self.wi_1(hidden_states)
A:transformers.models.t5.modeling_tf_t5.self.DenseReluDense->TFT5GatedGeluDense(config, name='DenseReluDense')
A:transformers.models.t5.modeling_tf_t5.self.layer_norm->TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name='layer_norm')
A:transformers.models.t5.modeling_tf_t5.normed_hidden_states->self.layer_norm(hidden_states)
A:transformers.models.t5.modeling_tf_t5.dense_output->self.DenseReluDense(normed_hidden_states, training=training)
A:transformers.models.t5.modeling_tf_t5.NEW_ID->itertools.count()
A:transformers.models.t5.modeling_tf_t5.self.layer_id->next(TFT5Attention.NEW_ID)
A:transformers.models.t5.modeling_tf_t5.self.q->tensorflow.keras.layers.Dense(self.inner_dim, use_bias=False, name='q')
A:transformers.models.t5.modeling_tf_t5.self.k->tensorflow.keras.layers.Dense(self.inner_dim, use_bias=False, name='k')
A:transformers.models.t5.modeling_tf_t5.self.v->tensorflow.keras.layers.Dense(self.inner_dim, use_bias=False, name='v')
A:transformers.models.t5.modeling_tf_t5.self.o->tensorflow.keras.layers.Dense(self.d_model, use_bias=False, name='o')
A:transformers.models.t5.modeling_tf_t5.self.pruned_heads->set()
A:transformers.models.t5.modeling_tf_t5.self.relative_attention_bias->self.add_weight(name='embeddings', shape=[self.relative_attention_num_buckets, self.n_heads])
A:transformers.models.t5.modeling_tf_t5.relative_position->tensorflow.math.abs(relative_position)
A:transformers.models.t5.modeling_tf_t5.is_small->tensorflow.math.less(relative_position, max_exact)
A:transformers.models.t5.modeling_tf_t5.relative_position_if_large->tensorflow.math.minimum(relative_position_if_large, num_buckets - 1)
A:transformers.models.t5.modeling_tf_t5.relative_position_bucket->self._relative_position_bucket(relative_position, bidirectional=not self.is_decoder, num_buckets=self.relative_attention_num_buckets)
A:transformers.models.t5.modeling_tf_t5.values->tensorflow.expand_dims(tf.transpose(values, [2, 0, 1]), axis=0)
A:transformers.models.t5.modeling_tf_t5.query_states->shape(self.q(hidden_states))
A:transformers.models.t5.modeling_tf_t5.key_states->project(hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None)
A:transformers.models.t5.modeling_tf_t5.value_states->project(hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None)
A:transformers.models.t5.modeling_tf_t5.scores->tensorflow.einsum('bnqd,bnkd->bnqk', query_states, key_states)
A:transformers.models.t5.modeling_tf_t5.position_bias->tensorflow.cast(position_bias, dtype=mask.dtype)
A:transformers.models.t5.modeling_tf_t5.weights->self.dropout(weights, training=training)
A:transformers.models.t5.modeling_tf_t5.attn_output->self.o(unshape(attn_output))
A:transformers.models.t5.modeling_tf_t5.self.SelfAttention->TFT5Attention(config, has_relative_attention_bias=has_relative_attention_bias, name='SelfAttention')
A:transformers.models.t5.modeling_tf_t5.attention_output->self.EncDecAttention(normed_hidden_states, mask=attention_mask, key_value_states=key_value_states, position_bias=position_bias, layer_head_mask=layer_head_mask, past_key_value=past_key_value, query_length=query_length, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.models.t5.modeling_tf_t5.self.EncDecAttention->TFT5Attention(config, has_relative_attention_bias=False, name='EncDecAttention')
A:transformers.models.t5.modeling_tf_t5.self_attention_outputs->self.layer[0](hidden_states, attention_mask=attention_mask, position_bias=position_bias, layer_head_mask=layer_head_mask, past_key_value=self_attn_past_key_value, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.models.t5.modeling_tf_t5.cross_attention_outputs->self.layer[1](hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, position_bias=encoder_decoder_position_bias, layer_head_mask=encoder_layer_head_mask, past_key_value=cross_attn_past_key_value, query_length=query_length, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.models.t5.modeling_tf_t5.self.final_layer_norm->TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name='final_layer_norm')
A:transformers.models.t5.modeling_tf_t5.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training, kwargs_call=kwargs)
A:transformers.models.t5.modeling_tf_t5.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.t5.modeling_tf_t5.inputs['input_ids']->tensorflow.reshape(inputs['input_ids'], (-1, input_shape[-1]))
A:transformers.models.t5.modeling_tf_t5.inputs['inputs_embeds']->self.embed_tokens(inputs['input_ids'])
A:transformers.models.t5.modeling_tf_t5.inputs['attention_mask']->tensorflow.cast(inputs['attention_mask'], dtype=inputs['inputs_embeds'].dtype)
A:transformers.models.t5.modeling_tf_t5.inputs['encoder_attention_mask']->tensorflow.cast(inputs['encoder_attention_mask'], dtype=extended_attention_mask.dtype)
A:transformers.models.t5.modeling_tf_t5.num_dims_attention_mask->len(shape_list(inputs['attention_mask']))
A:transformers.models.t5.modeling_tf_t5.seq_ids->tensorflow.range(mask_seq_length)
A:transformers.models.t5.modeling_tf_t5.causal_mask->tensorflow.cast(causal_mask, dtype=inputs['attention_mask'].dtype)
A:transformers.models.t5.modeling_tf_t5.num_dims_encoder_attention_mask->len(shape_list(inputs['encoder_attention_mask']))
A:transformers.models.t5.modeling_tf_t5.layer_outputs->layer_module(hidden_states, attention_mask=extended_attention_mask, position_bias=position_bias, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=encoder_extended_attention_mask, encoder_decoder_position_bias=encoder_decoder_position_bias, layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, encoder_layer_head_mask=inputs['encoder_head_mask'][idx] if inputs['encoder_head_mask'] is not None else None, past_key_value=past_key_value, use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], training=inputs['training'])
A:transformers.models.t5.modeling_tf_t5.input_mask->tensorflow.constant(DUMMY_MASK)
A:transformers.models.t5.modeling_tf_t5.output->self.call(inputs)
A:transformers.models.t5.modeling_tf_t5.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.t5.modeling_tf_t5.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.t5.modeling_tf_t5.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.t5.modeling_tf_t5.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.t5.modeling_tf_t5.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, name='shared')
A:transformers.models.t5.modeling_tf_t5.encoder_config->copy.deepcopy(config)
A:transformers.models.t5.modeling_tf_t5.self.encoder->TFT5MainLayer(encoder_config, embed_tokens, name='encoder')
A:transformers.models.t5.modeling_tf_t5.decoder_config->copy.deepcopy(config)
A:transformers.models.t5.modeling_tf_t5.self.decoder->TFT5MainLayer(decoder_config, embed_tokens, name='decoder')
A:transformers.models.t5.modeling_tf_t5.inputs['encoder_outputs']->TFBaseModelOutput(last_hidden_state=last_hidden_state, hidden_states=hidden_states, attentions=attentions)
A:transformers.models.t5.modeling_tf_t5.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=hidden_states, encoder_attention_mask=inputs['attention_mask'], inputs_embeds=inputs['decoder_inputs_embeds'], head_mask=inputs['decoder_head_mask'], past_key_values=inputs['past_key_values'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.t5.modeling_tf_t5.self.lm_head->tensorflow.keras.layers.Dense(shape_list(value)[0], use_bias=False, name='lm_head')
A:transformers.models.t5.modeling_tf_t5.transposed_value->tensorflow.transpose(value)
A:transformers.models.t5.modeling_tf_t5.inputs['decoder_input_ids']->self._shift_right(inputs['labels'])
A:transformers.models.t5.modeling_tf_t5.logits->self.lm_head(sequence_output)
A:transformers.models.t5.modeling_tf_t5.encoder_outputs->self.encoder(input_ids, attention_mask=inputs['attention_mask'], encoder_hidden_states=None, encoder_attention_mask=None, inputs_embeds=inputs['inputs_embeds'], head_mask=head_mask, past_key_values=None, use_cache=False, output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
transformers.TFT5EncoderModel(self,config,*inputs,**kwargs)
transformers.TFT5EncoderModel.call(self,input_ids,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFT5EncoderModel.get_encoder(self)
transformers.TFT5EncoderModel.serving_output(self,output)
transformers.TFT5ForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFT5ForConditionalGeneration._reorder_cache(self,past,beam_idx)->Tuple
transformers.TFT5ForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFT5ForConditionalGeneration.get_decoder(self)
transformers.TFT5ForConditionalGeneration.get_encoder(self)
transformers.TFT5ForConditionalGeneration.get_output_embeddings(self)
transformers.TFT5ForConditionalGeneration.prepare_inputs_for_generation(self,inputs,past,attention_mask,use_cache=None,**kwargs)
transformers.TFT5ForConditionalGeneration.serving_output(self,output)
transformers.TFT5ForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFT5Model(self,config,*inputs,**kwargs)
transformers.TFT5Model.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFT5Model.get_decoder(self)
transformers.TFT5Model.get_encoder(self)
transformers.TFT5Model.serving_output(self,output)
transformers.TFT5PreTrainedModel(TFPreTrainedModel)
transformers.TFT5PreTrainedModel._shift_right(self,input_ids)
transformers.TFT5PreTrainedModel.dummy_inputs(self)
transformers.TFT5PreTrainedModel.get_input_embeddings(self)
transformers.TFT5PreTrainedModel.serving(self,inputs)
transformers.TFT5PreTrainedModel.set_input_embeddings(self,value)
transformers.models.t5.modeling_tf_t5.TFT5Attention(self,config,has_relative_attention_bias=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Attention.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Attention._relative_position_bucket(relative_position,bidirectional=True,num_buckets=32,max_distance=128)
transformers.models.t5.modeling_tf_t5.TFT5Attention.build(self,input_shape)
transformers.models.t5.modeling_tf_t5.TFT5Attention.call(self,hidden_states,mask=None,key_value_states=None,position_bias=None,past_key_value=None,layer_head_mask=None,query_length=None,use_cache=False,training=False,output_attentions=False)
transformers.models.t5.modeling_tf_t5.TFT5Attention.compute_bias(self,query_length,key_length)
transformers.models.t5.modeling_tf_t5.TFT5Attention.prune_heads(self,heads)
transformers.models.t5.modeling_tf_t5.TFT5Block(self,config,has_relative_attention_bias=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Block.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Block.call(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,layer_head_mask=None,encoder_layer_head_mask=None,past_key_value=None,use_cache=False,output_attentions=False,training=False)
transformers.models.t5.modeling_tf_t5.TFT5DenseReluDense(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5DenseReluDense.__init__(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5DenseReluDense.call(self,hidden_states,training=False)
transformers.models.t5.modeling_tf_t5.TFT5EncoderModel(self,config,*inputs,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5EncoderModel.__init__(self,config,*inputs,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5EncoderModel.call(self,input_ids,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5EncoderModel.get_encoder(self)
transformers.models.t5.modeling_tf_t5.TFT5EncoderModel.serving_output(self,output)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration._reorder_cache(self,past,beam_idx)->Tuple
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.get_decoder(self)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.get_encoder(self)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.get_output_embeddings(self)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.prepare_inputs_for_generation(self,inputs,past,attention_mask,use_cache=None,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.serving_output(self,output)
transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.t5.modeling_tf_t5.TFT5GatedGeluDense(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5GatedGeluDense.__init__(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5GatedGeluDense.call(self,hidden_states,training=False)
transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention.__init__(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention.call(self,hidden_states,key_value_states,attention_mask=None,position_bias=None,layer_head_mask=None,past_key_value=None,query_length=None,use_cache=False,output_attentions=False,training=False)
transformers.models.t5.modeling_tf_t5.TFT5LayerFF(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerFF.__init__(self,config,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerFF.call(self,hidden_states,training=False)
transformers.models.t5.modeling_tf_t5.TFT5LayerNorm(self,epsilon=1e-06,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerNorm.__init__(self,epsilon=1e-06,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerNorm.build(self,input_shape)
transformers.models.t5.modeling_tf_t5.TFT5LayerNorm.call(self,hidden_states)
transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention(self,config,has_relative_attention_bias=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention.call(self,hidden_states,attention_mask=None,position_bias=None,layer_head_mask=None,past_key_value=None,use_cache=False,output_attentions=False,training=False)
transformers.models.t5.modeling_tf_t5.TFT5MainLayer(self,config,embed_tokens=None,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5MainLayer.__init__(self,config,embed_tokens=None,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5MainLayer._prune_heads(self,heads_to_prune)
transformers.models.t5.modeling_tf_t5.TFT5MainLayer.call(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,inputs_embeds=None,head_mask=None,encoder_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)->Tuple
transformers.models.t5.modeling_tf_t5.TFT5Model(self,config,*inputs,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Model.__init__(self,config,*inputs,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Model.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.t5.modeling_tf_t5.TFT5Model.get_decoder(self)
transformers.models.t5.modeling_tf_t5.TFT5Model.get_encoder(self)
transformers.models.t5.modeling_tf_t5.TFT5Model.serving_output(self,output)
transformers.models.t5.modeling_tf_t5.TFT5PreTrainedModel(TFPreTrainedModel)
transformers.models.t5.modeling_tf_t5.TFT5PreTrainedModel._shift_right(self,input_ids)
transformers.models.t5.modeling_tf_t5.TFT5PreTrainedModel.dummy_inputs(self)
transformers.models.t5.modeling_tf_t5.TFT5PreTrainedModel.get_input_embeddings(self)
transformers.models.t5.modeling_tf_t5.TFT5PreTrainedModel.serving(self,inputs)
transformers.models.t5.modeling_tf_t5.TFT5PreTrainedModel.set_input_embeddings(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/__init__.py----------------------------------------
A:transformers.models.t5.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/t5/modeling_flax_t5.py----------------------------------------
A:transformers.models.t5.modeling_flax_t5.logger->utils.logging.get_logger(__name__)
A:transformers.models.t5.modeling_flax_t5.shifted_input_ids->jax.numpy.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)
A:transformers.models.t5.modeling_flax_t5.self.weight->self.param('weight', self.weight_init, (self.hidden_size,))
A:transformers.models.t5.modeling_flax_t5.variance->jax.numpy.power(hidden_states.astype('f4'), 2).mean(axis=-1, keepdims=True)
A:transformers.models.t5.modeling_flax_t5.self.wi->flax.linen.Dense(self.config.d_ff, use_bias=False, kernel_init=jax.nn.initializers.normal(wi_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.wo->flax.linen.Dense(self.config.d_model, use_bias=False, kernel_init=jax.nn.initializers.normal(wo_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.dropout->flax.linen.Dropout(self.config.dropout_rate)
A:transformers.models.t5.modeling_flax_t5.hidden_states->self.dropout(hidden_states, deterministic=deterministic)
A:transformers.models.t5.modeling_flax_t5.self.wi_0->flax.linen.Dense(self.config.d_ff, use_bias=False, kernel_init=jax.nn.initializers.normal(wi_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.wi_1->flax.linen.Dense(self.config.d_ff, use_bias=False, kernel_init=jax.nn.initializers.normal(wi_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.hidden_gelu->self.gelu_act(self.wi_0(hidden_states))
A:transformers.models.t5.modeling_flax_t5.hidden_linear->self.wi_1(hidden_states)
A:transformers.models.t5.modeling_flax_t5.self.DenseReluDense->FlaxT5DenseGatedGeluDense(self.config, dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.layer_norm->FlaxT5LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon)
A:transformers.models.t5.modeling_flax_t5.forwarded_states->self.DenseReluDense(forwarded_states, deterministic=deterministic)
A:transformers.models.t5.modeling_flax_t5.self.q->flax.linen.Dense(self.inner_dim, use_bias=False, kernel_init=jax.nn.initializers.normal(d_model_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.k->flax.linen.Dense(self.inner_dim, use_bias=False, kernel_init=jax.nn.initializers.normal(d_model_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.v->flax.linen.Dense(self.inner_dim, use_bias=False, kernel_init=jax.nn.initializers.normal(d_model_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.o->flax.linen.Dense(self.d_model, use_bias=False, kernel_init=jax.nn.initializers.normal(inner_dim_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.relative_attention_bias->flax.linen.Embed(self.relative_attention_num_buckets, self.n_heads, embedding_init=jax.nn.initializers.normal(d_model_init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.relative_position->jax.numpy.abs(relative_position)
A:transformers.models.t5.modeling_flax_t5.relative_position_if_large->jax.numpy.clip(relative_position_if_large, a_max=num_buckets - 1)
A:transformers.models.t5.modeling_flax_t5.relative_position_bucket->self._relative_position_bucket(relative_position, bidirectional=not self.causal, num_buckets=self.relative_attention_num_buckets)
A:transformers.models.t5.modeling_flax_t5.values->self.relative_attention_bias(relative_position_bucket)
A:transformers.models.t5.modeling_flax_t5.is_initialized->self.has_variable('cache', 'cached_key')
A:transformers.models.t5.modeling_flax_t5.cached_key->self.variable('cache', 'cached_key', jnp.zeros, key.shape, key.dtype)
A:transformers.models.t5.modeling_flax_t5.cached_value->self.variable('cache', 'cached_value', jnp.zeros, value.shape, value.dtype)
A:transformers.models.t5.modeling_flax_t5.cache_index->self.variable('cache', 'cache_index', lambda : jnp.array(0, dtype=jnp.int32))
A:transformers.models.t5.modeling_flax_t5.key->jax.lax.dynamic_update_slice(cached_key.value, key, indices)
A:transformers.models.t5.modeling_flax_t5.value->jax.lax.dynamic_update_slice(cached_value.value, value, indices)
A:transformers.models.t5.modeling_flax_t5.pad_mask->jax.numpy.broadcast_to(jnp.arange(max_length) < cur_index + num_updated_cache_vectors, tuple(batch_dims) + (1, num_updated_cache_vectors, max_length))
A:transformers.models.t5.modeling_flax_t5.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.t5.modeling_flax_t5.position_bias->self._create_position_bias(key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift)
A:transformers.models.t5.modeling_flax_t5.query_states->self._split_heads(query_states)
A:transformers.models.t5.modeling_flax_t5.key_states->self._split_heads(key_states)
A:transformers.models.t5.modeling_flax_t5.value_states->self._split_heads(value_states)
A:transformers.models.t5.modeling_flax_t5.causal_attention_mask->jax.numpy.broadcast_to(causal_attention_mask, (batch_size,) + causal_attention_mask.shape[1:])
A:transformers.models.t5.modeling_flax_t5.(key_states, value_states, attention_attention_mask)->self._concatenate_to_cache(key_states, value_states, query_states, attention_mask)
A:transformers.models.t5.modeling_flax_t5.dropout_rng->self.make_rng('dropout')
A:transformers.models.t5.modeling_flax_t5.attn_weights->dot_product_attention_weights(query_states, key_states, bias=position_bias, dropout_rng=dropout_rng, dropout_rate=self.dropout, broadcast_dropout=True, deterministic=deterministic, dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.attn_output->self.o(attn_output)
A:transformers.models.t5.modeling_flax_t5.self.SelfAttention->FlaxT5Attention(self.config, has_relative_attention_bias=self.has_relative_attention_bias, causal=self.config.causal, dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.normed_hidden_states->self.layer_norm(hidden_states)
A:transformers.models.t5.modeling_flax_t5.attention_output->self.EncDecAttention(normed_hidden_states, attention_mask=attention_mask, key_value_states=key_value_states, position_bias=position_bias, output_attentions=output_attentions)
A:transformers.models.t5.modeling_flax_t5.self.EncDecAttention->FlaxT5Attention(self.config, has_relative_attention_bias=False, causal=False)
A:transformers.models.t5.modeling_flax_t5.self_attention_outputs->self.layer[0](hidden_states, attention_mask=attention_mask, position_bias=position_bias, output_attentions=output_attentions, deterministic=deterministic, init_cache=init_cache)
A:transformers.models.t5.modeling_flax_t5.cross_attention_outputs->self.layer[1](hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, position_bias=encoder_decoder_position_bias, output_attentions=output_attentions, deterministic=deterministic)
A:transformers.models.t5.modeling_flax_t5.self.layer->FlaxT5Block(self.config, has_relative_attention_bias=self.has_relative_attention_bias, dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.layer_outputs->layer_module(hidden_states, attention_mask=attention_mask, position_bias=position_bias, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, encoder_decoder_position_bias=encoder_decoder_position_bias, output_attentions=output_attentions, deterministic=deterministic, init_cache=init_cache)
A:transformers.models.t5.modeling_flax_t5.self.embed_tokens->flax.linen.Embed(self.config.vocab_size, self.config.d_model, embedding_init=jax.nn.initializers.normal(self.config.init_std, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.self.block->FlaxT5BlockCollection(self.config)
A:transformers.models.t5.modeling_flax_t5.self.final_layer_norm->FlaxT5LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon, dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.outputs->FlaxCausalLMOutputWithCrossAttentions(logits=lm_logits, hidden_states=decoder_outputs.hidden_states, attentions=decoder_outputs.attentions, cross_attentions=decoder_outputs.cross_attentions)
A:transformers.models.t5.modeling_flax_t5.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.t5.modeling_flax_t5.input_ids->jax.numpy.zeros(input_shape, dtype='i4')
A:transformers.models.t5.modeling_flax_t5.decoder_input_ids->jax.numpy.ones((batch_size, max_length), dtype='i4')
A:transformers.models.t5.modeling_flax_t5.decoder_attention_mask->jax.numpy.ones((batch_size, sequence_length))
A:transformers.models.t5.modeling_flax_t5.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.t5.modeling_flax_t5.decoder_module->self.module_class(config=config, dtype=dtype, **kwargs)._get_decoder_module()
A:transformers.models.t5.modeling_flax_t5.init_variables->self.module.init(jax.random.PRNGKey(0), decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], init_cache=True, method=_decoder_forward)
A:transformers.models.t5.modeling_flax_t5.encode_module->self.module_class(config=config, dtype=dtype, **kwargs)._get_encoder_module()
A:transformers.models.t5.modeling_flax_t5.encoder_attention_mask->jax.numpy.ones((batch_size, sequence_length))
A:transformers.models.t5.modeling_flax_t5.outputs['past_key_values']->unfreeze(past['cache'])
A:transformers.models.t5.modeling_flax_t5.self.shared->flax.linen.Embed(self.config.vocab_size, self.config.d_model, embedding_init=jax.nn.initializers.normal(self.config.initializer_factor, self.dtype))
A:transformers.models.t5.modeling_flax_t5.encoder_config->copy.deepcopy(self.config)
A:transformers.models.t5.modeling_flax_t5.self.encoder->FlaxT5Stack(encoder_config, self.shared)
A:transformers.models.t5.modeling_flax_t5.decoder_config->copy.deepcopy(self.config)
A:transformers.models.t5.modeling_flax_t5.self.decoder->FlaxT5Stack(decoder_config, self.shared)
A:transformers.models.t5.modeling_flax_t5.encoder_outputs->self.encoder(input_ids=input_ids, attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, deterministic=deterministic)
A:transformers.models.t5.modeling_flax_t5.decoder_outputs->decoder_module(decoder_input_ids, decoder_attention_mask, **kwargs)
A:transformers.models.t5.modeling_flax_t5.self.lm_head->flax.linen.Dense(self.config.vocab_size, use_bias=False, kernel_init=jax.nn.initializers.normal(self.config.initializer_factor, self.dtype), dtype=self.dtype)
A:transformers.models.t5.modeling_flax_t5.lm_logits->self.module_class(config=config, dtype=dtype, **kwargs).lm_head(sequence_output)
A:transformers.models.t5.modeling_flax_t5.past_key_values->self.init_cache(batch_size, max_length, encoder_outputs)
A:transformers.models.t5.modeling_flax_t5.extended_attention_mask->jax.lax.dynamic_update_slice(extended_attention_mask, decoder_attention_mask, (0, 0))
transformers.FlaxT5ForConditionalGeneration(FlaxT5PreTrainedModel)
transformers.FlaxT5ForConditionalGeneration.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.FlaxT5ForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,max_length,attention_mask:Optional[jnp.DeviceArray]=None,decoder_attention_mask:Optional[jnp.DeviceArray]=None,encoder_outputs=None,**kwargs)
transformers.FlaxT5ForConditionalGeneration.update_inputs_for_generation(self,model_outputs,model_kwargs)
transformers.FlaxT5ForConditionalGenerationModule(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,output_attentions=None,output_hidden_states=None,return_dict=None,deterministic:bool=True)
transformers.FlaxT5ForConditionalGenerationModule._get_decoder_module(self)
transformers.FlaxT5ForConditionalGenerationModule._get_encoder_module(self)
transformers.FlaxT5ForConditionalGenerationModule.setup(self)
transformers.FlaxT5Model(FlaxT5PreTrainedModel)
transformers.FlaxT5PreTrainedModel(self,config:T5Config,input_shape:Tuple[int]=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxT5PreTrainedModel.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.FlaxT5PreTrainedModel.encode(self,input_ids:jnp.ndarray,attention_mask:Optional[jnp.ndarray]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.FlaxT5PreTrainedModel.init_cache(self,batch_size,max_length,encoder_outputs)
transformers.FlaxT5PreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.t5.modeling_flax_t5.FlaxT5Attention(self,hidden_states,attention_mask=None,key_value_states=None,position_bias=None,use_cache=False,output_attentions=False,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention.__call__(self,hidden_states,attention_mask=None,key_value_states=None,position_bias=None,use_cache=False,output_attentions=False,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention._concatenate_to_cache(self,key,value,query,attention_mask)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention._create_position_bias(self,key_states,query_states,attention_mask,init_cache,seq_length,causal_attention_mask_shift)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention._merge_heads(self,hidden_states)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention._relative_position_bucket(relative_position,bidirectional=True,num_buckets=32,max_distance=128)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention._split_heads(self,hidden_states)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention.compute_bias(self,query_length,key_length)
transformers.models.t5.modeling_flax_t5.FlaxT5Attention.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5Block(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,output_attentions=False,return_dict=True,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5Block.__call__(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,output_attentions=False,return_dict=True,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5Block.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5BlockCollection(self,hidden_states=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions:bool=False,output_hidden_states:bool=False,deterministic:bool=True,init_cache:bool=False)
transformers.models.t5.modeling_flax_t5.FlaxT5BlockCollection.__call__(self,hidden_states=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions:bool=False,output_hidden_states:bool=False,deterministic:bool=True,init_cache:bool=False)
transformers.models.t5.modeling_flax_t5.FlaxT5BlockCollection.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5DenseGatedGeluDense(self,hidden_states,deterministic)
transformers.models.t5.modeling_flax_t5.FlaxT5DenseGatedGeluDense.__call__(self,hidden_states,deterministic)
transformers.models.t5.modeling_flax_t5.FlaxT5DenseGatedGeluDense.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5DenseReluDense(self,hidden_states,deterministic=True)
transformers.models.t5.modeling_flax_t5.FlaxT5DenseReluDense.__call__(self,hidden_states,deterministic=True)
transformers.models.t5.modeling_flax_t5.FlaxT5DenseReluDense.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGeneration(FlaxT5PreTrainedModel)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGeneration.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,max_length,attention_mask:Optional[jnp.DeviceArray]=None,decoder_attention_mask:Optional[jnp.DeviceArray]=None,encoder_outputs=None,**kwargs)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGeneration.update_inputs_for_generation(self,model_outputs,model_kwargs)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGenerationModule(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,output_attentions=None,output_hidden_states=None,return_dict=None,deterministic:bool=True)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGenerationModule.__call__(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,output_attentions=None,output_hidden_states=None,return_dict=None,deterministic:bool=True)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGenerationModule._get_decoder_module(self)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGenerationModule._get_encoder_module(self)
transformers.models.t5.modeling_flax_t5.FlaxT5ForConditionalGenerationModule.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerCollection(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,output_attentions=False,return_dict=True,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerCollection.__call__(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,output_attentions=False,return_dict=True,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerCollection.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerCrossAttention(self,hidden_states,key_value_states,attention_mask=None,position_bias=None,output_attentions=False,deterministic=True)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerCrossAttention.__call__(self,hidden_states,key_value_states,attention_mask=None,position_bias=None,output_attentions=False,deterministic=True)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerCrossAttention.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerFF(self,hidden_states,deterministic=True)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerFF.__call__(self,hidden_states,deterministic=True)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerFF.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerNorm(self,hidden_states)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerNorm.__call__(self,hidden_states)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerNorm.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerSelfAttention(self,hidden_states,attention_mask=None,position_bias=None,output_attentions=False,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerSelfAttention.__call__(self,hidden_states,attention_mask=None,position_bias=None,output_attentions=False,deterministic=True,init_cache=False)
transformers.models.t5.modeling_flax_t5.FlaxT5LayerSelfAttention.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5Model(FlaxT5PreTrainedModel)
transformers.models.t5.modeling_flax_t5.FlaxT5Module(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,output_attentions=None,output_hidden_states=None,return_dict=None,deterministic:bool=True)
transformers.models.t5.modeling_flax_t5.FlaxT5Module.__call__(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,output_attentions=None,output_hidden_states=None,return_dict=None,deterministic:bool=True)
transformers.models.t5.modeling_flax_t5.FlaxT5Module._get_decoder_module(self)
transformers.models.t5.modeling_flax_t5.FlaxT5Module._get_encoder_module(self)
transformers.models.t5.modeling_flax_t5.FlaxT5Module.setup(self)
transformers.models.t5.modeling_flax_t5.FlaxT5PreTrainedModel(self,config:T5Config,input_shape:Tuple[int]=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.t5.modeling_flax_t5.FlaxT5PreTrainedModel.__init__(self,config:T5Config,input_shape:Tuple[int]=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.t5.modeling_flax_t5.FlaxT5PreTrainedModel.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.models.t5.modeling_flax_t5.FlaxT5PreTrainedModel.encode(self,input_ids:jnp.ndarray,attention_mask:Optional[jnp.ndarray]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.models.t5.modeling_flax_t5.FlaxT5PreTrainedModel.init_cache(self,batch_size,max_length,encoder_outputs)
transformers.models.t5.modeling_flax_t5.FlaxT5PreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.t5.modeling_flax_t5.FlaxT5Stack(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True,init_cache:bool=False)
transformers.models.t5.modeling_flax_t5.FlaxT5Stack.__call__(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True,init_cache:bool=False)
transformers.models.t5.modeling_flax_t5.FlaxT5Stack.setup(self)
transformers.models.t5.modeling_flax_t5.shift_tokens_right(input_ids:jnp.ndarray,pad_token_id:int,decoder_start_token_id:int)->jnp.ndarray


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/configuration_albert.py----------------------------------------
transformers.AlbertConfig(self,vocab_size=30000,embedding_size=128,hidden_size=4096,num_hidden_layers=12,num_hidden_groups=1,num_attention_heads=64,intermediate_size=16384,inner_group_num=1,hidden_act='gelu_new',hidden_dropout_prob=0,attention_probs_dropout_prob=0,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,classifier_dropout_prob=0.1,position_embedding_type='absolute',pad_token_id=0,bos_token_id=2,eos_token_id=3,**kwargs)
transformers.models.albert.configuration_albert.AlbertConfig(self,vocab_size=30000,embedding_size=128,hidden_size=4096,num_hidden_layers=12,num_hidden_groups=1,num_attention_heads=64,intermediate_size=16384,inner_group_num=1,hidden_act='gelu_new',hidden_dropout_prob=0,attention_probs_dropout_prob=0,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,classifier_dropout_prob=0.1,position_embedding_type='absolute',pad_token_id=0,bos_token_id=2,eos_token_id=3,**kwargs)
transformers.models.albert.configuration_albert.AlbertConfig.__init__(self,vocab_size=30000,embedding_size=128,hidden_size=4096,num_hidden_layers=12,num_hidden_groups=1,num_attention_heads=64,intermediate_size=16384,inner_group_num=1,hidden_act='gelu_new',hidden_dropout_prob=0,attention_probs_dropout_prob=0,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,classifier_dropout_prob=0.1,position_embedding_type='absolute',pad_token_id=0,bos_token_id=2,eos_token_id=3,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/modeling_tf_albert.py----------------------------------------
A:transformers.models.albert.modeling_tf_albert.logger->utils.logging.get_logger(__name__)
A:transformers.models.albert.modeling_tf_albert.loss_fn->tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
A:transformers.models.albert.modeling_tf_albert.masked_lm_active_loss->tensorflow.not_equal(tf.reshape(tensor=labels['labels'], shape=(-1,)), -100)
A:transformers.models.albert.modeling_tf_albert.masked_lm_reduced_logits->tensorflow.boolean_mask(tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])), mask=masked_lm_active_loss)
A:transformers.models.albert.modeling_tf_albert.masked_lm_labels->tensorflow.boolean_mask(tensor=tf.reshape(tensor=labels['labels'], shape=(-1,)), mask=masked_lm_active_loss)
A:transformers.models.albert.modeling_tf_albert.sentence_order_active_loss->tensorflow.not_equal(tf.reshape(tensor=labels['sentence_order_label'], shape=(-1,)), -100)
A:transformers.models.albert.modeling_tf_albert.sentence_order_reduced_logits->tensorflow.boolean_mask(tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=sentence_order_active_loss)
A:transformers.models.albert.modeling_tf_albert.sentence_order_label->tensorflow.boolean_mask(tensor=tf.reshape(tensor=labels['sentence_order_label'], shape=(-1,)), mask=sentence_order_active_loss)
A:transformers.models.albert.modeling_tf_albert.masked_lm_loss->tensorflow.reduce_mean(input_tensor=masked_lm_loss, axis=0)
A:transformers.models.albert.modeling_tf_albert.sentence_order_loss->loss_fn(y_true=sentence_order_label, y_pred=sentence_order_reduced_logits)
A:transformers.models.albert.modeling_tf_albert.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.albert.modeling_tf_albert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.albert.modeling_tf_albert.self.dropout->tensorflow.keras.layers.Dropout(rate=config.hidden_dropout_prob)
A:transformers.models.albert.modeling_tf_albert.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.albert.modeling_tf_albert.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.albert.modeling_tf_albert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.albert.modeling_tf_albert.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.albert.modeling_tf_albert.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.albert.modeling_tf_albert.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.albert.modeling_tf_albert.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.albert.modeling_tf_albert.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.albert.modeling_tf_albert.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.albert.modeling_tf_albert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.albert.modeling_tf_albert.self.sqrt_att_head_size->math.sqrt(self.attention_head_size)
A:transformers.models.albert.modeling_tf_albert.self.query->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.albert.modeling_tf_albert.self.key->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.albert.modeling_tf_albert.self.value->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.albert.modeling_tf_albert.self.dense->tensorflow.keras.layers.Dense(config.embedding_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.models.albert.modeling_tf_albert.self.attention_dropout->tensorflow.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)
A:transformers.models.albert.modeling_tf_albert.self.output_dropout->tensorflow.keras.layers.Dropout(rate=config.hidden_dropout_prob)
A:transformers.models.albert.modeling_tf_albert.tensor->tensorflow.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.albert.modeling_tf_albert.mixed_query_layer->self.query(inputs=input_tensor)
A:transformers.models.albert.modeling_tf_albert.mixed_key_layer->self.key(inputs=input_tensor)
A:transformers.models.albert.modeling_tf_albert.mixed_value_layer->self.value(inputs=input_tensor)
A:transformers.models.albert.modeling_tf_albert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.albert.modeling_tf_albert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.albert.modeling_tf_albert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.albert.modeling_tf_albert.attention_scores->tensorflow.add(attention_scores, attention_mask)
A:transformers.models.albert.modeling_tf_albert.dk->tensorflow.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
A:transformers.models.albert.modeling_tf_albert.attention_probs->tensorflow.multiply(attention_probs, head_mask)
A:transformers.models.albert.modeling_tf_albert.context_layer->tensorflow.reshape(tensor=context_layer, shape=(batch_size, -1, self.all_head_size))
A:transformers.models.albert.modeling_tf_albert.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.decoder_bias)
A:transformers.models.albert.modeling_tf_albert.attention_output->self.LayerNorm(inputs=hidden_states + input_tensor)
A:transformers.models.albert.modeling_tf_albert.self.attention->TFAlbertAttention(config, name='attention')
A:transformers.models.albert.modeling_tf_albert.self.ffn->tensorflow.keras.layers.Dense(units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name='ffn')
A:transformers.models.albert.modeling_tf_albert.self.activation->get_tf_activation(config.hidden_act)
A:transformers.models.albert.modeling_tf_albert.self.ffn_output->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='ffn_output')
A:transformers.models.albert.modeling_tf_albert.self.full_layer_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='full_layer_layer_norm')
A:transformers.models.albert.modeling_tf_albert.attention_outputs->self.attention(input_tensor=hidden_states, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.albert.modeling_tf_albert.ffn_output->self.dropout(inputs=ffn_output, training=training)
A:transformers.models.albert.modeling_tf_albert.layer_output->albert_layer(hidden_states=hidden_states, attention_mask=attention_mask, head_mask=head_mask[layer_index], output_attentions=output_attentions, training=training)
A:transformers.models.albert.modeling_tf_albert.self.layers_per_group->int(config.num_hidden_layers / config.num_hidden_groups)
A:transformers.models.albert.modeling_tf_albert.self.embedding_hidden_mapping_in->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='embedding_hidden_mapping_in')
A:transformers.models.albert.modeling_tf_albert.group_idx->int(i / (self.num_hidden_layers / self.num_hidden_groups))
A:transformers.models.albert.modeling_tf_albert.layer_group_output->self.albert_layer_groups[group_idx](hidden_states=hidden_states, attention_mask=attention_mask, head_mask=head_mask[group_idx * self.layers_per_group:(group_idx + 1) * self.layers_per_group], output_attentions=output_attentions, output_hidden_states=output_hidden_states, training=training)
A:transformers.models.albert.modeling_tf_albert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.albert.modeling_tf_albert.self.decoder_bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='decoder/bias')
A:transformers.models.albert.modeling_tf_albert.self.embeddings->TFAlbertEmbeddings(config, name='embeddings')
A:transformers.models.albert.modeling_tf_albert.self.encoder->TFAlbertTransformer(config, name='encoder')
A:transformers.models.albert.modeling_tf_albert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.albert.modeling_tf_albert.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.albert.modeling_tf_albert.inputs['attention_mask']->tensorflow.fill(dims=input_shape, value=1)
A:transformers.models.albert.modeling_tf_albert.inputs['token_type_ids']->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.albert.modeling_tf_albert.embedding_output->self.embeddings(input_ids=inputs['input_ids'], position_ids=inputs['position_ids'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.albert.modeling_tf_albert.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.albert.modeling_tf_albert.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.albert.modeling_tf_albert.encoder_outputs->self.encoder(hidden_states=embedding_output, attention_mask=extended_attention_mask, head_mask=inputs['head_mask'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.self.albert->TFAlbertMainLayer(config, name='albert')
A:transformers.models.albert.modeling_tf_albert.outputs->self.albert(input_ids=flat_input_ids, attention_mask=flat_attention_mask, token_type_ids=flat_token_type_ids, position_ids=flat_position_ids, head_mask=inputs['head_mask'], inputs_embeds=flat_inputs_embeds, output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.self.predictions->TFAlbertMLMHead(config, input_embeddings=self.albert.embeddings, name='predictions')
A:transformers.models.albert.modeling_tf_albert.self.sop_classifier->TFAlbertSOPHead(config, name='sop_classifier')
A:transformers.models.albert.modeling_tf_albert.prediction_scores->self.predictions(hidden_states=sequence_output, training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.sop_scores->self.sop_classifier(pooled_output=pooled_output, training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.total_loss->self.compute_loss(labels=d_labels, logits=(prediction_scores, sop_scores))
A:transformers.models.albert.modeling_tf_albert.self.classifier->tensorflow.keras.layers.Dense(units=1, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.albert.modeling_tf_albert.dropout_pooled_output->self.dropout(inputs=pooled_output, training=training)
A:transformers.models.albert.modeling_tf_albert.logits->self.classifier(inputs=pooled_output)
A:transformers.models.albert.modeling_tf_albert.pooled_output->self.dropout(inputs=pooled_output, training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.sequence_output->self.dropout(inputs=sequence_output, training=inputs['training'])
A:transformers.models.albert.modeling_tf_albert.self.qa_outputs->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.albert.modeling_tf_albert.(start_logits, end_logits)->tensorflow.split(value=logits, num_or_size_splits=2, axis=-1)
A:transformers.models.albert.modeling_tf_albert.start_logits->tensorflow.squeeze(input=start_logits, axis=-1)
A:transformers.models.albert.modeling_tf_albert.end_logits->tensorflow.squeeze(input=end_logits, axis=-1)
A:transformers.models.albert.modeling_tf_albert.loss->self.compute_loss(labels=labels, logits=(start_logits, end_logits))
A:transformers.models.albert.modeling_tf_albert.reshaped_logits->tensorflow.reshape(tensor=logits, shape=(-1, num_choices))
A:transformers.models.albert.modeling_tf_albert.output->self.call(input_ids=inputs)
transformers.TFAlbertForMaskedLM(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.TFAlbertForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFAlbertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFAlbertForMultipleChoice(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertForMultipleChoice.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]
transformers.TFAlbertForMultipleChoice.dummy_inputs(self)
transformers.TFAlbertForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])->TFMultipleChoiceModelOutput
transformers.TFAlbertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFAlbertForPreTraining(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertForPreTraining.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,sentence_order_label:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFAlbertForPreTrainingOutput, Tuple[tf.Tensor]]
transformers.TFAlbertForPreTraining.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFAlbertForPreTraining.serving_output(self,output:TFAlbertForPreTrainingOutput)->TFAlbertForPreTrainingOutput
transformers.TFAlbertForPreTrainingOutput(ModelOutput)
transformers.TFAlbertForQuestionAnswering(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertForQuestionAnswering.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,start_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,end_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]
transformers.TFAlbertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFAlbertForSequenceClassification(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.TFAlbertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFAlbertForTokenClassification(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.TFAlbertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFAlbertMainLayer(self,config:AlbertConfig,add_pooling_layer:bool=True,**kwargs)
transformers.TFAlbertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFAlbertMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.TFAlbertMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.TFAlbertMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.TFAlbertModel(self,config:AlbertConfig,*inputs,**kwargs)
transformers.TFAlbertModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.TFAlbertModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.TFAlbertPreTrainedModel(TFPreTrainedModel)
transformers.models.albert.modeling_tf_albert.TFAlbertAttention(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertAttention.__init__(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertAttention.call(self,input_tensor:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.albert.modeling_tf_albert.TFAlbertAttention.transpose_for_scores(self,tensor:tf.Tensor,batch_size:int)->tf.Tensor
transformers.models.albert.modeling_tf_albert.TFAlbertEmbeddings(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertEmbeddings.__init__(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.albert.modeling_tf_albert.TFAlbertEmbeddings.call(self,input_ids:tf.Tensor=None,position_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.models.albert.modeling_tf_albert.TFAlbertForMaskedLM(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForMaskedLM.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.albert.modeling_tf_albert.TFAlbertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.albert.modeling_tf_albert.TFAlbertForMultipleChoice(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForMultipleChoice.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForMultipleChoice.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertForMultipleChoice.dummy_inputs(self)
transformers.models.albert.modeling_tf_albert.TFAlbertForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])->TFMultipleChoiceModelOutput
transformers.models.albert.modeling_tf_albert.TFAlbertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.albert.modeling_tf_albert.TFAlbertForPreTraining(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForPreTraining.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForPreTraining.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,sentence_order_label:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFAlbertForPreTrainingOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertForPreTraining.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.albert.modeling_tf_albert.TFAlbertForPreTraining.serving_output(self,output:TFAlbertForPreTrainingOutput)->TFAlbertForPreTrainingOutput
transformers.models.albert.modeling_tf_albert.TFAlbertForPreTrainingOutput(ModelOutput)
transformers.models.albert.modeling_tf_albert.TFAlbertForQuestionAnswering(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForQuestionAnswering.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForQuestionAnswering.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,start_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,end_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.albert.modeling_tf_albert.TFAlbertForSequenceClassification(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForSequenceClassification.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.albert.modeling_tf_albert.TFAlbertForTokenClassification(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForTokenClassification.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.albert.modeling_tf_albert.TFAlbertLayer(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertLayer.__init__(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.albert.modeling_tf_albert.TFAlbertLayerGroup(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertLayerGroup.__init__(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertLayerGroup.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead(self,config:AlbertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.__init__(self,config:AlbertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.build(self,input_shape:tf.TensorShape)
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.get_bias(self)->Dict[str, tf.Variable]
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.set_bias(self,value:tf.Variable)
transformers.models.albert.modeling_tf_albert.TFAlbertMLMHead.set_output_embeddings(self,value:tf.Variable)
transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer(self,config:AlbertConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer.__init__(self,config:AlbertConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer._prune_heads(self,heads_to_prune)
transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.albert.modeling_tf_albert.TFAlbertModel(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertModel.__init__(self,config:AlbertConfig,*inputs,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.albert.modeling_tf_albert.TFAlbertModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.albert.modeling_tf_albert.TFAlbertPreTrainedModel(TFPreTrainedModel)
transformers.models.albert.modeling_tf_albert.TFAlbertPreTrainingLoss
transformers.models.albert.modeling_tf_albert.TFAlbertPreTrainingLoss.compute_loss(self,labels:tf.Tensor,logits:tf.Tensor)->tf.Tensor
transformers.models.albert.modeling_tf_albert.TFAlbertSOPHead(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertSOPHead.__init__(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertSOPHead.call(self,pooled_output:tf.Tensor,training:bool)->tf.Tensor
transformers.models.albert.modeling_tf_albert.TFAlbertTransformer(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertTransformer.__init__(self,config:AlbertConfig,**kwargs)
transformers.models.albert.modeling_tf_albert.TFAlbertTransformer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,return_dict:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/modeling_albert.py----------------------------------------
A:transformers.models.albert.modeling_albert.logger->utils.logging.get_logger(__name__)
A:transformers.models.albert.modeling_albert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.albert.modeling_albert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.albert.modeling_albert.array->numpy.transpose(array)
A:transformers.models.albert.modeling_albert.name->name.split('/').split('/')
A:transformers.models.albert.modeling_albert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.albert.modeling_albert.pointer->getattr(pointer, 'weight')
A:transformers.models.albert.modeling_albert.num->int(scope_names[1])
A:transformers.models.albert.modeling_albert.pointer.data->torch.from_numpy(array)
A:transformers.models.albert.modeling_albert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.models.albert.modeling_albert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.embedding_size)
A:transformers.models.albert.modeling_albert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.models.albert.modeling_albert.self.LayerNorm->torch.nn.LayerNorm(config.embedding_size)
A:transformers.models.albert.modeling_albert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.albert.modeling_albert.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.albert.modeling_albert.input_shape->input_ids.size()
A:transformers.models.albert.modeling_albert.buffered_token_type_ids_expanded->buffered_token_type_ids.expand(batch_size, seq_length)
A:transformers.models.albert.modeling_albert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.albert.modeling_albert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.albert.modeling_albert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.albert.modeling_albert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.albert.modeling_albert.embeddings->self.dropout(embeddings)
A:transformers.models.albert.modeling_albert.self.query->prune_linear_layer(self.query, index)
A:transformers.models.albert.modeling_albert.self.key->prune_linear_layer(self.key, index)
A:transformers.models.albert.modeling_albert.self.value->prune_linear_layer(self.value, index)
A:transformers.models.albert.modeling_albert.self.attention_dropout->torch.nn.Dropout(config.attention_probs_dropout_prob)
A:transformers.models.albert.modeling_albert.self.output_dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.albert.modeling_albert.self.dense->torch.nn.Linear(config.hidden_size, config.embedding_size)
A:transformers.models.albert.modeling_albert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.albert.modeling_albert.self.distance_embedding->torch.nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
A:transformers.models.albert.modeling_albert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.albert.modeling_albert.(heads, index)->find_pruneable_heads_and_indices(heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads)
A:transformers.models.albert.modeling_albert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.albert.modeling_albert.mixed_key_layer->self.key(hidden_states)
A:transformers.models.albert.modeling_albert.mixed_value_layer->self.value(hidden_states)
A:transformers.models.albert.modeling_albert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.albert.modeling_albert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.models.albert.modeling_albert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.models.albert.modeling_albert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.albert.modeling_albert.position_ids_l->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
A:transformers.models.albert.modeling_albert.position_ids_r->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
A:transformers.models.albert.modeling_albert.positional_embedding->positional_embedding.to(dtype=query_layer.dtype).to(dtype=query_layer.dtype)
A:transformers.models.albert.modeling_albert.relative_position_scores->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.albert.modeling_albert.relative_position_scores_query->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.albert.modeling_albert.relative_position_scores_key->torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
A:transformers.models.albert.modeling_albert.attention_probs->self.attention_dropout(attention_probs)
A:transformers.models.albert.modeling_albert.context_layer->context_layer.permute(0, 2, 1, 3).contiguous().permute(0, 2, 1, 3).contiguous()
A:transformers.models.albert.modeling_albert.w->self.dense.weight.t().view(self.num_attention_heads, self.attention_head_size, self.hidden_size).to(context_layer.dtype)
A:transformers.models.albert.modeling_albert.b->self.dense.bias.to(context_layer.dtype)
A:transformers.models.albert.modeling_albert.projected_context_layer_dropout->self.output_dropout(projected_context_layer)
A:transformers.models.albert.modeling_albert.layernormed_context_layer->self.LayerNorm(hidden_states + projected_context_layer_dropout)
A:transformers.models.albert.modeling_albert.self.full_layer_layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.albert.modeling_albert.self.attention->AlbertAttention(config)
A:transformers.models.albert.modeling_albert.self.ffn->torch.nn.Linear(config.hidden_size, config.intermediate_size)
A:transformers.models.albert.modeling_albert.self.ffn_output->torch.nn.Linear(config.intermediate_size, config.hidden_size)
A:transformers.models.albert.modeling_albert.attention_output->self.attention(hidden_states, attention_mask, head_mask, output_attentions)
A:transformers.models.albert.modeling_albert.ffn_output->self.ffn_output(ffn_output)
A:transformers.models.albert.modeling_albert.hidden_states->self.decoder(hidden_states)
A:transformers.models.albert.modeling_albert.self.albert_layers->torch.nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])
A:transformers.models.albert.modeling_albert.layer_output->albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)
A:transformers.models.albert.modeling_albert.self.embedding_hidden_mapping_in->torch.nn.Linear(config.embedding_size, config.hidden_size)
A:transformers.models.albert.modeling_albert.self.albert_layer_groups->torch.nn.ModuleList([AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])
A:transformers.models.albert.modeling_albert.layers_per_group->int(self.config.num_hidden_layers / self.config.num_hidden_groups)
A:transformers.models.albert.modeling_albert.group_idx->int(layer / self.config.inner_group_num)
A:transformers.models.albert.modeling_albert.layer_group_output->self.albert_layer_groups[group_idx](hidden_states, attention_mask, head_mask[group_idx * layers_per_group:(group_idx + 1) * layers_per_group], output_attentions, output_hidden_states)
A:transformers.models.albert.modeling_albert.self.embeddings->AlbertEmbeddings(config)
A:transformers.models.albert.modeling_albert.self.encoder->AlbertTransformer(config)
A:transformers.models.albert.modeling_albert.self.pooler->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.albert.modeling_albert.self.pooler_activation->torch.nn.Tanh()
A:transformers.models.albert.modeling_albert.inner_group_idx->int(layer - group_idx * self.config.inner_group_num)
A:transformers.models.albert.modeling_albert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.albert.modeling_albert.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.albert.modeling_albert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.albert.modeling_albert.embedding_output->self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.albert.modeling_albert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.albert.modeling_albert.self.albert->AlbertModel(config)
A:transformers.models.albert.modeling_albert.self.predictions->AlbertMLMHead(config)
A:transformers.models.albert.modeling_albert.self.sop_classifier->AlbertSOPHead(config)
A:transformers.models.albert.modeling_albert.outputs->self.albert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.albert.modeling_albert.prediction_scores->self.predictions(sequence_outputs)
A:transformers.models.albert.modeling_albert.sop_scores->self.sop_classifier(pooled_output)
A:transformers.models.albert.modeling_albert.loss_fct->CrossEntropyLoss()
A:transformers.models.albert.modeling_albert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.albert.modeling_albert.sentence_order_loss->loss_fct(sop_scores.view(-1, 2), sentence_order_label.view(-1))
A:transformers.models.albert.modeling_albert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.albert.modeling_albert.self.decoder->torch.nn.Linear(config.embedding_size, config.vocab_size)
A:transformers.models.albert.modeling_albert.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.models.albert.modeling_albert.dropout_pooled_output->self.dropout(pooled_output)
A:transformers.models.albert.modeling_albert.logits->self.classifier(pooled_output)
A:transformers.models.albert.modeling_albert.pooled_output->self.dropout(pooled_output)
A:transformers.models.albert.modeling_albert.loss->loss_fct(reshaped_logits, labels)
A:transformers.models.albert.modeling_albert.sequence_output->self.dropout(sequence_output)
A:transformers.models.albert.modeling_albert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.albert.modeling_albert.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.models.albert.modeling_albert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.albert.modeling_albert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.albert.modeling_albert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.albert.modeling_albert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.albert.modeling_albert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.albert.modeling_albert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.albert.modeling_albert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.albert.modeling_albert.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.AlbertForMaskedLM(self,config)
transformers.AlbertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForMaskedLM.get_input_embeddings(self)
transformers.AlbertForMaskedLM.get_output_embeddings(self)
transformers.AlbertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.AlbertForMultipleChoice(self,config)
transformers.AlbertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForPreTraining(self,config)
transformers.AlbertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,sentence_order_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForPreTraining.get_input_embeddings(self)
transformers.AlbertForPreTraining.get_output_embeddings(self)
transformers.AlbertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.AlbertForPreTrainingOutput(ModelOutput)
transformers.AlbertForQuestionAnswering(self,config)
transformers.AlbertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForSequenceClassification(self,config)
transformers.AlbertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForTokenClassification(self,config)
transformers.AlbertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertModel(self,config,add_pooling_layer=True)
transformers.AlbertModel._prune_heads(self,heads_to_prune)
transformers.AlbertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertModel.get_input_embeddings(self)
transformers.AlbertModel.set_input_embeddings(self,value)
transformers.AlbertPreTrainedModel(PreTrainedModel)
transformers.AlbertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_albert(model,config,tf_checkpoint_path)
transformers.models.albert.modeling_albert.AlbertAttention(self,config)
transformers.models.albert.modeling_albert.AlbertAttention.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.albert.modeling_albert.AlbertAttention.prune_heads(self,heads)
transformers.models.albert.modeling_albert.AlbertAttention.transpose_for_scores(self,x)
transformers.models.albert.modeling_albert.AlbertEmbeddings(self,config)
transformers.models.albert.modeling_albert.AlbertEmbeddings.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.albert.modeling_albert.AlbertForMaskedLM(self,config)
transformers.models.albert.modeling_albert.AlbertForMaskedLM.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertForMaskedLM.get_input_embeddings(self)
transformers.models.albert.modeling_albert.AlbertForMaskedLM.get_output_embeddings(self)
transformers.models.albert.modeling_albert.AlbertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.albert.modeling_albert.AlbertForMultipleChoice(self,config)
transformers.models.albert.modeling_albert.AlbertForMultipleChoice.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertForPreTraining(self,config)
transformers.models.albert.modeling_albert.AlbertForPreTraining.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,sentence_order_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertForPreTraining.get_input_embeddings(self)
transformers.models.albert.modeling_albert.AlbertForPreTraining.get_output_embeddings(self)
transformers.models.albert.modeling_albert.AlbertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.models.albert.modeling_albert.AlbertForPreTrainingOutput(ModelOutput)
transformers.models.albert.modeling_albert.AlbertForQuestionAnswering(self,config)
transformers.models.albert.modeling_albert.AlbertForQuestionAnswering.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertForSequenceClassification(self,config)
transformers.models.albert.modeling_albert.AlbertForSequenceClassification.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertForTokenClassification(self,config)
transformers.models.albert.modeling_albert.AlbertForTokenClassification.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertLayer(self,config)
transformers.models.albert.modeling_albert.AlbertLayer.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertLayer.ff_chunk(self,attention_output)
transformers.models.albert.modeling_albert.AlbertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False)
transformers.models.albert.modeling_albert.AlbertLayerGroup(self,config)
transformers.models.albert.modeling_albert.AlbertLayerGroup.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertLayerGroup.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False)
transformers.models.albert.modeling_albert.AlbertMLMHead(self,config)
transformers.models.albert.modeling_albert.AlbertMLMHead.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertMLMHead.forward(self,hidden_states)
transformers.models.albert.modeling_albert.AlbertModel(self,config,add_pooling_layer=True)
transformers.models.albert.modeling_albert.AlbertModel.__init__(self,config,add_pooling_layer=True)
transformers.models.albert.modeling_albert.AlbertModel._prune_heads(self,heads_to_prune)
transformers.models.albert.modeling_albert.AlbertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.albert.modeling_albert.AlbertModel.get_input_embeddings(self)
transformers.models.albert.modeling_albert.AlbertModel.set_input_embeddings(self,value)
transformers.models.albert.modeling_albert.AlbertPreTrainedModel(PreTrainedModel)
transformers.models.albert.modeling_albert.AlbertPreTrainedModel._init_weights(self,module)
transformers.models.albert.modeling_albert.AlbertSOPHead(self,config)
transformers.models.albert.modeling_albert.AlbertSOPHead.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertSOPHead.forward(self,pooled_output)
transformers.models.albert.modeling_albert.AlbertTransformer(self,config)
transformers.models.albert.modeling_albert.AlbertTransformer.__init__(self,config)
transformers.models.albert.modeling_albert.AlbertTransformer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.albert.modeling_albert.load_tf_weights_in_albert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/tokenization_albert.py----------------------------------------
A:transformers.models.albert.tokenization_albert.logger->utils.logging.get_logger(__name__)
A:transformers.models.albert.tokenization_albert.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.albert.tokenization_albert.state->self.__dict__.copy()
A:transformers.models.albert.tokenization_albert.outputs->outputs.lower().lower()
A:transformers.models.albert.tokenization_albert.text->self.preprocess_text(text)
A:transformers.models.albert.tokenization_albert.pieces->self.sp_model.encode(text, out_type=str)
A:transformers.models.albert.tokenization_albert.cur_pieces->self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, ''))
A:transformers.models.albert.tokenization_albert.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.albert.tokenization_albert.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.AlbertTokenizer(self,vocab_file,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.AlbertTokenizer.__getstate__(self)
transformers.AlbertTokenizer.__setstate__(self,d)
transformers.AlbertTokenizer._convert_id_to_token(self,index)
transformers.AlbertTokenizer._convert_token_to_id(self,token)
transformers.AlbertTokenizer._tokenize(self,text:str)->List[str]
transformers.AlbertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.AlbertTokenizer.convert_tokens_to_string(self,tokens)
transformers.AlbertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.AlbertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.AlbertTokenizer.get_vocab(self)
transformers.AlbertTokenizer.preprocess_text(self,inputs)
transformers.AlbertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.AlbertTokenizer.vocab_size(self)
transformers.models.albert.tokenization_albert.AlbertTokenizer(self,vocab_file,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.albert.tokenization_albert.AlbertTokenizer.__getstate__(self)
transformers.models.albert.tokenization_albert.AlbertTokenizer.__init__(self,vocab_file,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.albert.tokenization_albert.AlbertTokenizer.__setstate__(self,d)
transformers.models.albert.tokenization_albert.AlbertTokenizer._convert_id_to_token(self,index)
transformers.models.albert.tokenization_albert.AlbertTokenizer._convert_token_to_id(self,token)
transformers.models.albert.tokenization_albert.AlbertTokenizer._tokenize(self,text:str)->List[str]
transformers.models.albert.tokenization_albert.AlbertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.albert.tokenization_albert.AlbertTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.albert.tokenization_albert.AlbertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.albert.tokenization_albert.AlbertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.albert.tokenization_albert.AlbertTokenizer.get_vocab(self)
transformers.models.albert.tokenization_albert.AlbertTokenizer.preprocess_text(self,inputs)
transformers.models.albert.tokenization_albert.AlbertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.albert.tokenization_albert.AlbertTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.albert.convert_albert_original_tf_checkpoint_to_pytorch.config->transformers.AlbertConfig.from_json_file(albert_config_file)
A:transformers.models.albert.convert_albert_original_tf_checkpoint_to_pytorch.model->AlbertForPreTraining(config)
A:transformers.models.albert.convert_albert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.albert.convert_albert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.albert.convert_albert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,albert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/tokenization_albert_fast.py----------------------------------------
A:transformers.models.albert.tokenization_albert_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.albert.tokenization_albert_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.AlbertTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',**kwargs)
transformers.AlbertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.AlbertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.AlbertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',**kwargs)
transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',**kwargs)
transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.albert.tokenization_albert_fast.AlbertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/albert/__init__.py----------------------------------------
A:transformers.models.albert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/squeezebert/configuration_squeezebert.py----------------------------------------
A:transformers.models.squeezebert.configuration_squeezebert.logger->utils.logging.get_logger(__name__)
transformers.SqueezeBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=768,q_groups=4,k_groups=4,v_groups=4,post_attention_groups=1,intermediate_groups=4,output_groups=4,**kwargs)
transformers.models.squeezebert.configuration_squeezebert.SqueezeBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=768,q_groups=4,k_groups=4,v_groups=4,post_attention_groups=1,intermediate_groups=4,output_groups=4,**kwargs)
transformers.models.squeezebert.configuration_squeezebert.SqueezeBertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=768,q_groups=4,k_groups=4,v_groups=4,post_attention_groups=1,intermediate_groups=4,output_groups=4,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/squeezebert/modeling_squeezebert.py----------------------------------------
A:transformers.models.squeezebert.modeling_squeezebert.logger->utils.logging.get_logger(__name__)
A:transformers.models.squeezebert.modeling_squeezebert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.models.squeezebert.modeling_squeezebert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.embedding_size)
A:transformers.models.squeezebert.modeling_squeezebert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.models.squeezebert.modeling_squeezebert.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.squeezebert.modeling_squeezebert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.squeezebert.modeling_squeezebert.input_shape->input_ids.size()
A:transformers.models.squeezebert.modeling_squeezebert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.squeezebert.modeling_squeezebert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.squeezebert.modeling_squeezebert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.squeezebert.modeling_squeezebert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.squeezebert.modeling_squeezebert.embeddings->self.dropout(embeddings)
A:transformers.models.squeezebert.modeling_squeezebert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.squeezebert.modeling_squeezebert.self.conv1d->torch.nn.Conv1d(in_channels=cin, out_channels=cout, kernel_size=1, groups=groups)
A:transformers.models.squeezebert.modeling_squeezebert.self.layernorm->SqueezeBertLayerNorm(cout)
A:transformers.models.squeezebert.modeling_squeezebert.output->self.conv1d(x)
A:transformers.models.squeezebert.modeling_squeezebert.self.attention_head_size->int(cin / config.num_attention_heads)
A:transformers.models.squeezebert.modeling_squeezebert.self.query->torch.nn.Conv1d(in_channels=cin, out_channels=cin, kernel_size=1, groups=q_groups)
A:transformers.models.squeezebert.modeling_squeezebert.self.key->torch.nn.Conv1d(in_channels=cin, out_channels=cin, kernel_size=1, groups=k_groups)
A:transformers.models.squeezebert.modeling_squeezebert.self.value->torch.nn.Conv1d(in_channels=cin, out_channels=cin, kernel_size=1, groups=v_groups)
A:transformers.models.squeezebert.modeling_squeezebert.self.softmax->torch.nn.Softmax(dim=-1)
A:transformers.models.squeezebert.modeling_squeezebert.self.matmul_qk->MatMulWrapper()
A:transformers.models.squeezebert.modeling_squeezebert.self.matmul_qkv->MatMulWrapper()
A:transformers.models.squeezebert.modeling_squeezebert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.squeezebert.modeling_squeezebert.mixed_key_layer->self.key(hidden_states)
A:transformers.models.squeezebert.modeling_squeezebert.mixed_value_layer->self.value(hidden_states)
A:transformers.models.squeezebert.modeling_squeezebert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.squeezebert.modeling_squeezebert.key_layer->self.transpose_key_for_scores(mixed_key_layer)
A:transformers.models.squeezebert.modeling_squeezebert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.models.squeezebert.modeling_squeezebert.attention_score->self.matmul_qk(query_layer, key_layer)
A:transformers.models.squeezebert.modeling_squeezebert.attention_probs->self.dropout(attention_probs)
A:transformers.models.squeezebert.modeling_squeezebert.context_layer->self.transpose_output(context_layer)
A:transformers.models.squeezebert.modeling_squeezebert.self.attention->SqueezeBertSelfAttention(config=config, cin=c0, q_groups=config.q_groups, k_groups=config.k_groups, v_groups=config.v_groups)
A:transformers.models.squeezebert.modeling_squeezebert.self.post_attention->ConvDropoutLayerNorm(cin=c0, cout=c1, groups=config.post_attention_groups, dropout_prob=config.hidden_dropout_prob)
A:transformers.models.squeezebert.modeling_squeezebert.self.intermediate->ConvActivation(cin=c1, cout=c2, groups=config.intermediate_groups, act=config.hidden_act)
A:transformers.models.squeezebert.modeling_squeezebert.self.output->ConvDropoutLayerNorm(cin=c2, cout=c3, groups=config.output_groups, dropout_prob=config.hidden_dropout_prob)
A:transformers.models.squeezebert.modeling_squeezebert.att->self.attention(hidden_states, attention_mask, output_attentions)
A:transformers.models.squeezebert.modeling_squeezebert.post_attention_output->self.post_attention(attention_output, hidden_states)
A:transformers.models.squeezebert.modeling_squeezebert.intermediate_output->self.intermediate(post_attention_output)
A:transformers.models.squeezebert.modeling_squeezebert.layer_output->layer.forward(hidden_states, attention_mask, output_attentions)
A:transformers.models.squeezebert.modeling_squeezebert.self.layers->torch.nn.ModuleList((SqueezeBertModule(config) for _ in range(config.num_hidden_layers)))
A:transformers.models.squeezebert.modeling_squeezebert.hidden_states->self.decoder(hidden_states)
A:transformers.models.squeezebert.modeling_squeezebert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.squeezebert.modeling_squeezebert.self.activation->torch.nn.Tanh()
A:transformers.models.squeezebert.modeling_squeezebert.pooled_output->self.dropout(pooled_output)
A:transformers.models.squeezebert.modeling_squeezebert.self.transform->SqueezeBertPredictionHeadTransform(config)
A:transformers.models.squeezebert.modeling_squeezebert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.squeezebert.modeling_squeezebert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.squeezebert.modeling_squeezebert.self.predictions->SqueezeBertLMPredictionHead(config)
A:transformers.models.squeezebert.modeling_squeezebert.prediction_scores->self.cls(sequence_output)
A:transformers.models.squeezebert.modeling_squeezebert.self.embeddings->SqueezeBertEmbeddings(config)
A:transformers.models.squeezebert.modeling_squeezebert.self.encoder->SqueezeBertEncoder(config)
A:transformers.models.squeezebert.modeling_squeezebert.self.pooler->SqueezeBertPooler(config)
A:transformers.models.squeezebert.modeling_squeezebert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.squeezebert.modeling_squeezebert.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, device)
A:transformers.models.squeezebert.modeling_squeezebert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.squeezebert.modeling_squeezebert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.squeezebert.modeling_squeezebert.encoder_outputs->self.encoder(hidden_states=embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.squeezebert.modeling_squeezebert.self.transformer->SqueezeBertModel(config)
A:transformers.models.squeezebert.modeling_squeezebert.self.cls->SqueezeBertOnlyMLMHead(config)
A:transformers.models.squeezebert.modeling_squeezebert.outputs->self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.squeezebert.modeling_squeezebert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.squeezebert.modeling_squeezebert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.squeezebert.modeling_squeezebert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.squeezebert.modeling_squeezebert.logits->self.qa_outputs(sequence_output)
A:transformers.models.squeezebert.modeling_squeezebert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.squeezebert.modeling_squeezebert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.squeezebert.modeling_squeezebert.sequence_output->self.dropout(sequence_output)
A:transformers.models.squeezebert.modeling_squeezebert.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.squeezebert.modeling_squeezebert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.squeezebert.modeling_squeezebert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.squeezebert.modeling_squeezebert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.squeezebert.modeling_squeezebert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.squeezebert.modeling_squeezebert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.squeezebert.modeling_squeezebert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.squeezebert.modeling_squeezebert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.squeezebert.modeling_squeezebert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.squeezebert.modeling_squeezebert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.squeezebert.modeling_squeezebert.end_loss->loss_fct(end_logits, end_positions)
transformers.SqueezeBertForMaskedLM(self,config)
transformers.SqueezeBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.SqueezeBertForMaskedLM.get_output_embeddings(self)
transformers.SqueezeBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.SqueezeBertForMultipleChoice(self,config)
transformers.SqueezeBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.SqueezeBertForQuestionAnswering(self,config)
transformers.SqueezeBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.SqueezeBertForSequenceClassification(self,config)
transformers.SqueezeBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.SqueezeBertForTokenClassification(self,config)
transformers.SqueezeBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.SqueezeBertModel(self,config)
transformers.SqueezeBertModel._prune_heads(self,heads_to_prune)
transformers.SqueezeBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.SqueezeBertModel.get_input_embeddings(self)
transformers.SqueezeBertModel.set_input_embeddings(self,new_embeddings)
transformers.SqueezeBertModule(self,config)
transformers.SqueezeBertModule.forward(self,hidden_states,attention_mask,output_attentions)
transformers.SqueezeBertPreTrainedModel(PreTrainedModel)
transformers.SqueezeBertPreTrainedModel._init_weights(self,module)
transformers.models.squeezebert.modeling_squeezebert.ConvActivation(self,cin,cout,groups,act)
transformers.models.squeezebert.modeling_squeezebert.ConvActivation.__init__(self,cin,cout,groups,act)
transformers.models.squeezebert.modeling_squeezebert.ConvActivation.forward(self,x)
transformers.models.squeezebert.modeling_squeezebert.ConvDropoutLayerNorm(self,cin,cout,groups,dropout_prob)
transformers.models.squeezebert.modeling_squeezebert.ConvDropoutLayerNorm.__init__(self,cin,cout,groups,dropout_prob)
transformers.models.squeezebert.modeling_squeezebert.ConvDropoutLayerNorm.forward(self,hidden_states,input_tensor)
transformers.models.squeezebert.modeling_squeezebert.MatMulWrapper(self)
transformers.models.squeezebert.modeling_squeezebert.MatMulWrapper.__init__(self)
transformers.models.squeezebert.modeling_squeezebert.MatMulWrapper.forward(self,mat1,mat2)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertEmbeddings(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertEmbeddings.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertEncoder(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertEncoder.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMaskedLM(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMaskedLM.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMaskedLM.get_output_embeddings(self)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMultipleChoice(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMultipleChoice.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForQuestionAnswering(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForQuestionAnswering.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForSequenceClassification(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForSequenceClassification.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForTokenClassification(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForTokenClassification.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertLMPredictionHead(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertLMPredictionHead.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertLMPredictionHead.forward(self,hidden_states)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertLayerNorm(self,hidden_size,eps=1e-12)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertLayerNorm.__init__(self,hidden_size,eps=1e-12)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertLayerNorm.forward(self,x)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModel(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModel.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModel._prune_heads(self,heads_to_prune)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModel.get_input_embeddings(self)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModel.set_input_embeddings(self,new_embeddings)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModule(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModule.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertModule.forward(self,hidden_states,attention_mask,output_attentions)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertOnlyMLMHead(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertOnlyMLMHead.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertOnlyMLMHead.forward(self,sequence_output)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPooler(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPooler.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPooler.forward(self,hidden_states)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPreTrainedModel(PreTrainedModel)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPreTrainedModel._init_weights(self,module)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPredictionHeadTransform(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPredictionHeadTransform.__init__(self,config)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertSelfAttention(self,config,cin,q_groups=1,k_groups=1,v_groups=1)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertSelfAttention.__init__(self,config,cin,q_groups=1,k_groups=1,v_groups=1)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertSelfAttention.forward(self,hidden_states,attention_mask,output_attentions)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertSelfAttention.transpose_for_scores(self,x)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertSelfAttention.transpose_key_for_scores(self,x)
transformers.models.squeezebert.modeling_squeezebert.SqueezeBertSelfAttention.transpose_output(self,x)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/squeezebert/tokenization_squeezebert.py----------------------------------------
A:transformers.models.squeezebert.tokenization_squeezebert.logger->utils.logging.get_logger(__name__)
transformers.SqueezeBertTokenizer(BertTokenizer)
transformers.models.squeezebert.tokenization_squeezebert.SqueezeBertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/squeezebert/__init__.py----------------------------------------
A:transformers.models.squeezebert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/squeezebert/tokenization_squeezebert_fast.py----------------------------------------
A:transformers.models.squeezebert.tokenization_squeezebert_fast.logger->utils.logging.get_logger(__name__)
transformers.SqueezeBertTokenizerFast(BertTokenizerFast)
transformers.models.squeezebert.tokenization_squeezebert_fast.SqueezeBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.lin_layer->torch.nn.Linear(vocab_size, emb_size, bias=False)
A:transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.mbart_config->transformers.MBartConfig.from_pretrained(hf_config_path, vocab_size=vocab_size)
A:transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.model->convert_fairseq_mbart_checkpoint_from_disk(args.fairseq_path, hf_config_path=args.hf_config, finetuned=args.finetuned, mbart_50=args.mbart_50)
A:transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.model.lm_head->make_linear_from_emb(model.model.shared)
A:transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.convert_fairseq_mbart_checkpoint_from_disk(checkpoint_path,hf_config_path='facebook/mbart-large-en-ro',finetuned=False,mbart_50=False)
transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.make_linear_from_emb(emb)
transformers.models.mbart.convert_mbart_original_checkpoint_to_pytorch.remove_ignore_keys_(state_dict)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/modeling_mbart.py----------------------------------------
A:transformers.models.mbart.modeling_mbart.logger->utils.logging.get_logger(__name__)
A:transformers.models.mbart.modeling_mbart.prev_output_tokens->input_ids.view(-1, input_shape[-1]).clone()
A:transformers.models.mbart.modeling_mbart.index_of_eos->(prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)
A:transformers.models.mbart.modeling_mbart.decoder_start_tokens->input_ids.view(-1, input_shape[-1]).clone().gather(1, index_of_eos).squeeze()
A:transformers.models.mbart.modeling_mbart.prev_output_tokens[:, 1:]->prev_output_tokens[:, :-1].clone()
A:transformers.models.mbart.modeling_mbart.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.mbart.modeling_mbart.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.mbart.modeling_mbart.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.mbart.modeling_mbart.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.mbart.modeling_mbart.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.mbart.modeling_mbart.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.mbart.modeling_mbart.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.mbart.modeling_mbart.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.mbart.modeling_mbart.self.out_proj->torch.nn.Linear(inner_dim, num_classes)
A:transformers.models.mbart.modeling_mbart.(bsz, tgt_len, embed_dim)->self.layer_norm(hidden_states).size()
A:transformers.models.mbart.modeling_mbart.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.mbart.modeling_mbart.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.mbart.modeling_mbart.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.mbart.modeling_mbart.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.mbart.modeling_mbart.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.mbart.modeling_mbart.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.mbart.modeling_mbart.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.mbart.modeling_mbart.attn_output->self.out_proj(attn_output)
A:transformers.models.mbart.modeling_mbart.self.self_attn->MBartAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.mbart.modeling_mbart.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.mbart.modeling_mbart.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.mbart.modeling_mbart.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.mbart.modeling_mbart.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.mbart.modeling_mbart.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.mbart.modeling_mbart.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.mbart.modeling_mbart.self.encoder_attn->MBartAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.mbart.modeling_mbart.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.mbart.modeling_mbart.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.mbart.modeling_mbart.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.mbart.modeling_mbart.self.dense->torch.nn.Linear(input_dim, inner_dim)
A:transformers.models.mbart.modeling_mbart.self.dropout->torch.nn.Dropout(p=pooler_dropout)
A:transformers.models.mbart.modeling_mbart.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.mbart.modeling_mbart.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.mbart.modeling_mbart.self.embed_positions->MBartLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model)
A:transformers.models.mbart.modeling_mbart.self.layers->torch.nn.ModuleList([MBartDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.mbart.modeling_mbart.self.layernorm_embedding->torch.nn.LayerNorm(config.d_model)
A:transformers.models.mbart.modeling_mbart.self.layer_norm->torch.nn.LayerNorm(config.d_model)
A:transformers.models.mbart.modeling_mbart.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.mbart.modeling_mbart.embed_pos->self.embed_positions(input_shape)
A:transformers.models.mbart.modeling_mbart.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.mbart.modeling_mbart.dropout_probability->random.uniform(0, 1)
A:transformers.models.mbart.modeling_mbart.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.mbart.modeling_mbart.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.mbart.modeling_mbart.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.mbart.modeling_mbart.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.mbart.modeling_mbart.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.mbart.modeling_mbart.self.encoder->MBartEncoder(config, self.shared)
A:transformers.models.mbart.modeling_mbart.self.decoder->MBartDecoder(config)
A:transformers.models.mbart.modeling_mbart.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id)
A:transformers.models.mbart.modeling_mbart.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.mbart.modeling_mbart.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mbart.modeling_mbart.self.model->MBartDecoderWrapper(config)
A:transformers.models.mbart.modeling_mbart.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.mbart.modeling_mbart.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.mbart.modeling_mbart.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.mbart.modeling_mbart.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.mbart.modeling_mbart.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mbart.modeling_mbart.loss_fct->CrossEntropyLoss()
A:transformers.models.mbart.modeling_mbart.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.mbart.modeling_mbart.self.classification_head->MBartClassificationHead(config.d_model, config.d_model, config.num_labels, config.classifier_dropout)
A:transformers.models.mbart.modeling_mbart.eos_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).eq(self.config.eos_token_id)
A:transformers.models.mbart.modeling_mbart.logits->self.lm_head(outputs[0])
A:transformers.models.mbart.modeling_mbart.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.mbart.modeling_mbart.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mbart.modeling_mbart.(start_logits, end_logits)->self.lm_head(outputs[0]).split(1, dim=-1)
A:transformers.models.mbart.modeling_mbart.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.mbart.modeling_mbart.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.mbart.modeling_mbart.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.mbart.modeling_mbart.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.mbart.modeling_mbart.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.mbart.modeling_mbart.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.mbart.modeling_mbart.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.mbart.modeling_mbart.config->copy.deepcopy(config)
transformers.MBartForCausalLM(self,config)
transformers.MBartForCausalLM._reorder_cache(past,beam_idx)
transformers.MBartForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MBartForCausalLM.get_decoder(self)
transformers.MBartForCausalLM.get_input_embeddings(self)
transformers.MBartForCausalLM.get_output_embeddings(self)
transformers.MBartForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.MBartForCausalLM.set_decoder(self,decoder)
transformers.MBartForCausalLM.set_input_embeddings(self,value)
transformers.MBartForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.MBartForConditionalGeneration(self,config:MBartConfig)
transformers.MBartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.MBartForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.MBartForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MBartForConditionalGeneration.get_decoder(self)
transformers.MBartForConditionalGeneration.get_encoder(self)
transformers.MBartForConditionalGeneration.get_output_embeddings(self)
transformers.MBartForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.MBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.MBartForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.MBartForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.MBartForQuestionAnswering(self,config)
transformers.MBartForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MBartForSequenceClassification(self,config:MBartConfig,**kwargs)
transformers.MBartForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MBartModel(self,config:MBartConfig)
transformers.MBartModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MBartModel.get_decoder(self)
transformers.MBartModel.get_encoder(self)
transformers.MBartModel.get_input_embeddings(self)
transformers.MBartModel.set_input_embeddings(self,value)
transformers.MBartPreTrainedModel(PreTrainedModel)
transformers.MBartPreTrainedModel._init_weights(self,module)
transformers.MBartPreTrainedModel.dummy_inputs(self)
transformers.models.mbart.modeling_mbart.MBartAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.mbart.modeling_mbart.MBartAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.mbart.modeling_mbart.MBartAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.mbart.modeling_mbart.MBartAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.mbart.modeling_mbart.MBartClassificationHead(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.mbart.modeling_mbart.MBartClassificationHead.__init__(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.mbart.modeling_mbart.MBartClassificationHead.forward(self,hidden_states:torch.Tensor)
transformers.models.mbart.modeling_mbart.MBartDecoder(self,config:MBartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.mbart.modeling_mbart.MBartDecoder.__init__(self,config:MBartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.mbart.modeling_mbart.MBartDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.mbart.modeling_mbart.MBartDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartDecoder.get_input_embeddings(self)
transformers.models.mbart.modeling_mbart.MBartDecoder.set_input_embeddings(self,value)
transformers.models.mbart.modeling_mbart.MBartDecoderLayer(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartDecoderLayer.__init__(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.mbart.modeling_mbart.MBartDecoderWrapper(self,config)
transformers.models.mbart.modeling_mbart.MBartDecoderWrapper.__init__(self,config)
transformers.models.mbart.modeling_mbart.MBartDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.mbart.modeling_mbart.MBartEncoder(self,config:MBartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.mbart.modeling_mbart.MBartEncoder.__init__(self,config:MBartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.mbart.modeling_mbart.MBartEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartEncoderLayer(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartEncoderLayer.__init__(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.mbart.modeling_mbart.MBartForCausalLM(self,config)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.__init__(self,config)
transformers.models.mbart.modeling_mbart.MBartForCausalLM._reorder_cache(past,beam_idx)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.get_decoder(self)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.get_input_embeddings(self)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.get_output_embeddings(self)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.set_decoder(self,decoder)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.set_input_embeddings(self,value)
transformers.models.mbart.modeling_mbart.MBartForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.__init__(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.get_decoder(self)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.get_encoder(self)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.get_output_embeddings(self)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.mbart.modeling_mbart.MBartForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.mbart.modeling_mbart.MBartForQuestionAnswering(self,config)
transformers.models.mbart.modeling_mbart.MBartForQuestionAnswering.__init__(self,config)
transformers.models.mbart.modeling_mbart.MBartForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartForSequenceClassification(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_mbart.MBartForSequenceClassification.__init__(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_mbart.MBartForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int)
transformers.models.mbart.modeling_mbart.MBartLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int)
transformers.models.mbart.modeling_mbart.MBartLearnedPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.mbart.modeling_mbart.MBartModel(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartModel.__init__(self,config:MBartConfig)
transformers.models.mbart.modeling_mbart.MBartModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mbart.modeling_mbart.MBartModel.get_decoder(self)
transformers.models.mbart.modeling_mbart.MBartModel.get_encoder(self)
transformers.models.mbart.modeling_mbart.MBartModel.get_input_embeddings(self)
transformers.models.mbart.modeling_mbart.MBartModel.set_input_embeddings(self,value)
transformers.models.mbart.modeling_mbart.MBartPreTrainedModel(PreTrainedModel)
transformers.models.mbart.modeling_mbart.MBartPreTrainedModel._init_weights(self,module)
transformers.models.mbart.modeling_mbart.MBartPreTrainedModel.dummy_inputs(self)
transformers.models.mbart.modeling_mbart._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.mbart.modeling_mbart._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.mbart.modeling_mbart.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/tokenization_mbart50_fast.py----------------------------------------
A:transformers.models.mbart.tokenization_mbart50_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.mbart.tokenization_mbart50_fast.self.cur_lang_code_id->self.convert_tokens_to_ids(tgt_lang)
A:transformers.models.mbart.tokenization_mbart50_fast.prefix_tokens_str->self.convert_ids_to_tokens(self.prefix_tokens)
A:transformers.models.mbart.tokenization_mbart50_fast.suffix_tokens_str->self.convert_ids_to_tokens(self.suffix_tokens)
A:transformers.models.mbart.tokenization_mbart50_fast.self._tokenizer.post_processor->tokenizers.processors.TemplateProcessing(single=prefix_tokens_str + ['$A'] + suffix_tokens_str, pair=prefix_tokens_str + ['$A', '$B'] + suffix_tokens_str, special_tokens=list(zip(prefix_tokens_str + suffix_tokens_str, self.prefix_tokens + self.suffix_tokens)))
A:transformers.models.mbart.tokenization_mbart50_fast.inputs->self(raw_inputs, add_special_tokens=True, return_tensors='pt', **extra_kwargs)
A:transformers.models.mbart.tokenization_mbart50_fast.tgt_lang_id->self.convert_tokens_to_ids(tgt_lang)
A:transformers.models.mbart.tokenization_mbart50_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.MBart50TokenizerFast(self,vocab_file,src_lang=None,tgt_lang=None,tokenizer_file=None,eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.MBart50TokenizerFast._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.MBart50TokenizerFast.as_target_tokenizer(self)
transformers.MBart50TokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MBart50TokenizerFast.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.MBart50TokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.MBart50TokenizerFast.set_src_lang_special_tokens(self,src_lang:str)->None
transformers.MBart50TokenizerFast.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.MBart50TokenizerFast.src_lang(self)->str
transformers.MBart50TokenizerFast.src_lang(self,new_src_lang:str)->None
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast(self,vocab_file,src_lang=None,tgt_lang=None,tokenizer_file=None,eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.__init__(self,vocab_file,src_lang=None,tgt_lang=None,tokenizer_file=None,eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.as_target_tokenizer(self)
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.set_src_lang_special_tokens(self,src_lang:str)->None
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.src_lang(self)->str
transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast.src_lang(self,new_src_lang:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/configuration_mbart.py----------------------------------------
A:transformers.models.mbart.configuration_mbart.logger->utils.logging.get_logger(__name__)
transformers.MBartConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.MBartConfig.hidden_size(self)->int
transformers.MBartConfig.num_attention_heads(self)->int
transformers.models.mbart.configuration_mbart.MBartConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.models.mbart.configuration_mbart.MBartConfig.__init__(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.models.mbart.configuration_mbart.MBartConfig.hidden_size(self)->int
transformers.models.mbart.configuration_mbart.MBartConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/tokenization_mbart_fast.py----------------------------------------
A:transformers.models.mbart.tokenization_mbart_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.mbart.tokenization_mbart_fast._additional_special_tokens->FAIRSEQ_LANGUAGE_CODES.copy()
A:transformers.models.mbart.tokenization_mbart_fast.self.cur_lang_code->self.convert_tokens_to_ids(lang)
A:transformers.models.mbart.tokenization_mbart_fast.inputs->self(raw_inputs, add_special_tokens=True, return_tensors='pt', **extra_kwargs)
A:transformers.models.mbart.tokenization_mbart_fast.tgt_lang_id->self.convert_tokens_to_ids(tgt_lang)
A:transformers.models.mbart.tokenization_mbart_fast.prefix_tokens_str->self.convert_ids_to_tokens(self.prefix_tokens)
A:transformers.models.mbart.tokenization_mbart_fast.suffix_tokens_str->self.convert_ids_to_tokens(self.suffix_tokens)
A:transformers.models.mbart.tokenization_mbart_fast.self._tokenizer.post_processor->tokenizers.processors.TemplateProcessing(single=prefix_tokens_str + ['$A'] + suffix_tokens_str, pair=prefix_tokens_str + ['$A', '$B'] + suffix_tokens_str, special_tokens=list(zip(prefix_tokens_str + suffix_tokens_str, self.prefix_tokens + self.suffix_tokens)))
transformers.MBartTokenizerFast(self,*args,tokenizer_file=None,src_lang=None,tgt_lang=None,additional_special_tokens=None,**kwargs)
transformers.MBartTokenizerFast._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.MBartTokenizerFast.as_target_tokenizer(self)
transformers.MBartTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MBartTokenizerFast.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.MBartTokenizerFast.set_src_lang_special_tokens(self,src_lang)->None
transformers.MBartTokenizerFast.set_tgt_lang_special_tokens(self,lang:str)->None
transformers.MBartTokenizerFast.src_lang(self)->str
transformers.MBartTokenizerFast.src_lang(self,new_src_lang:str)->None
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast(self,*args,tokenizer_file=None,src_lang=None,tgt_lang=None,additional_special_tokens=None,**kwargs)
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.__init__(self,*args,tokenizer_file=None,src_lang=None,tgt_lang=None,additional_special_tokens=None,**kwargs)
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.as_target_tokenizer(self)
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.set_src_lang_special_tokens(self,src_lang)->None
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.set_tgt_lang_special_tokens(self,lang:str)->None
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.src_lang(self)->str
transformers.models.mbart.tokenization_mbart_fast.MBartTokenizerFast.src_lang(self,new_src_lang:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/tokenization_mbart.py----------------------------------------
A:transformers.models.mbart.tokenization_mbart.logger->utils.logging.get_logger(__name__)
A:transformers.models.mbart.tokenization_mbart.self.sp_model_size->len(self.sp_model)
A:transformers.models.mbart.tokenization_mbart.self._additional_special_tokens->list(self.lang_code_to_id.keys())
A:transformers.models.mbart.tokenization_mbart.inputs->self(raw_inputs, add_special_tokens=True, return_tensors='pt', **extra_kwargs)
A:transformers.models.mbart.tokenization_mbart.tgt_lang_id->self.convert_tokens_to_ids(tgt_lang)
transformers.MBartTokenizer(self,*args,tokenizer_file=None,src_lang=None,tgt_lang=None,additional_special_tokens=None,**kwargs)
transformers.MBartTokenizer._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.MBartTokenizer.as_target_tokenizer(self)
transformers.MBartTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MBartTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.MBartTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.MBartTokenizer.set_src_lang_special_tokens(self,src_lang)->None
transformers.MBartTokenizer.set_tgt_lang_special_tokens(self,lang:str)->None
transformers.MBartTokenizer.src_lang(self)->str
transformers.MBartTokenizer.src_lang(self,new_src_lang:str)->None
transformers.MBartTokenizer.vocab_size(self)
transformers.models.mbart.tokenization_mbart.MBartTokenizer(self,*args,tokenizer_file=None,src_lang=None,tgt_lang=None,additional_special_tokens=None,**kwargs)
transformers.models.mbart.tokenization_mbart.MBartTokenizer.__init__(self,*args,tokenizer_file=None,src_lang=None,tgt_lang=None,additional_special_tokens=None,**kwargs)
transformers.models.mbart.tokenization_mbart.MBartTokenizer._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.models.mbart.tokenization_mbart.MBartTokenizer.as_target_tokenizer(self)
transformers.models.mbart.tokenization_mbart.MBartTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mbart.tokenization_mbart.MBartTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.mbart.tokenization_mbart.MBartTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.models.mbart.tokenization_mbart.MBartTokenizer.set_src_lang_special_tokens(self,src_lang)->None
transformers.models.mbart.tokenization_mbart.MBartTokenizer.set_tgt_lang_special_tokens(self,lang:str)->None
transformers.models.mbart.tokenization_mbart.MBartTokenizer.src_lang(self)->str
transformers.models.mbart.tokenization_mbart.MBartTokenizer.src_lang(self,new_src_lang:str)->None
transformers.models.mbart.tokenization_mbart.MBartTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/modeling_tf_mbart.py----------------------------------------
A:transformers.models.mbart.modeling_tf_mbart.logger->utils.logging.get_logger(__name__)
A:transformers.models.mbart.modeling_tf_mbart.input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.mbart.modeling_tf_mbart.language_id_index->tensorflow.stack([tf.range(shape_list(input_ids)[0]), language_id_index], axis=-1)
A:transformers.models.mbart.modeling_tf_mbart.languages_ids->tensorflow.gather_nd(input_ids, language_id_index)
A:transformers.models.mbart.modeling_tf_mbart.shifted_input_ids->tensorflow.concat([tf.expand_dims(languages_ids, axis=-1), input_ids[:, :-1]], axis=-1)
A:transformers.models.mbart.modeling_tf_mbart.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.mbart.modeling_tf_mbart.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.mbart.modeling_tf_mbart.one_cst->tensorflow.constant(1.0)
A:transformers.models.mbart.modeling_tf_mbart.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.mbart.modeling_tf_mbart.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.mbart.modeling_tf_mbart.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.mbart.modeling_tf_mbart.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.mbart.modeling_tf_mbart.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.mbart.modeling_tf_mbart.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.mbart.modeling_tf_mbart.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.mbart.modeling_tf_mbart.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.mbart.modeling_tf_mbart.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.mbart.modeling_tf_mbart.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.mbart.modeling_tf_mbart.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.mbart.modeling_tf_mbart.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.mbart.modeling_tf_mbart.attention_mask->_expand_mask(inputs['attention_mask'])
A:transformers.models.mbart.modeling_tf_mbart.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.mbart.modeling_tf_mbart.attn_output->self.out_proj(attn_output)
A:transformers.models.mbart.modeling_tf_mbart.self.self_attn->TFMBartAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.mbart.modeling_tf_mbart.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.mbart.modeling_tf_mbart.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.mbart.modeling_tf_mbart.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.mbart.modeling_tf_mbart.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.mbart.modeling_tf_mbart.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.mbart.modeling_tf_mbart.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.mbart.modeling_tf_mbart.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.mbart.modeling_tf_mbart.(hidden_states, self_attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.mbart.modeling_tf_mbart.self.encoder_attn->TFMBartAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.mbart.modeling_tf_mbart.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.mbart.modeling_tf_mbart.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.mbart.modeling_tf_mbart.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.mbart.modeling_tf_mbart.decoder_input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.mbart.modeling_tf_mbart.output->self.call(inputs)
A:transformers.models.mbart.modeling_tf_mbart.self.embed_positions->TFMBartLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.mbart.modeling_tf_mbart.self.layernorm_embedding->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layernorm_embedding')
A:transformers.models.mbart.modeling_tf_mbart.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layer_norm')
A:transformers.models.mbart.modeling_tf_mbart.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.mbart.modeling_tf_mbart.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.mbart.modeling_tf_mbart.embed_pos->self.embed_positions(input_shape)
A:transformers.models.mbart.modeling_tf_mbart.dropout_probability->random.uniform(0, 1)
A:transformers.models.mbart.modeling_tf_mbart.(hidden_states, attn)->encoder_layer(hidden_states, attention_mask, inputs['head_mask'][idx] if inputs['head_mask'] is not None else None)
A:transformers.models.mbart.modeling_tf_mbart.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.mbart.modeling_tf_mbart.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.mbart.modeling_tf_mbart.(hidden_states, layer_self_attn, layer_cross_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, cross_attn_layer_head_mask=inputs['cross_attn_head_mask'][idx] if inputs['cross_attn_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.mbart.modeling_tf_mbart.all_self_attns->list(all_self_attns)
A:transformers.models.mbart.modeling_tf_mbart.all_cross_attns->list(all_cross_attns)
A:transformers.models.mbart.modeling_tf_mbart.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='model.shared')
A:transformers.models.mbart.modeling_tf_mbart.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.mbart.modeling_tf_mbart.self.encoder->TFMBartEncoder(config, embed_tokens, name='encoder')
A:transformers.models.mbart.modeling_tf_mbart.self.decoder->TFMBartDecoder(config, embed_tokens, name='decoder')
A:transformers.models.mbart.modeling_tf_mbart.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id)
A:transformers.models.mbart.modeling_tf_mbart.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.mbart.modeling_tf_mbart.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.mbart.modeling_tf_mbart.self.model->TFMBartMainLayer(config, name='model')
A:transformers.models.mbart.modeling_tf_mbart.outputs->self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], encoder_outputs=inputs['encoder_outputs'], decoder_attention_mask=inputs['decoder_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.mbart.modeling_tf_mbart.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.mbart.modeling_tf_mbart.inputs['labels']->tensorflow.where(inputs['labels'] == self.config.pad_token_id, tf.fill(shape_list(inputs['labels']), -100), inputs['labels'])
A:transformers.models.mbart.modeling_tf_mbart.lm_logits->self.model.shared(outputs[0], mode='linear')
A:transformers.models.mbart.modeling_tf_mbart.encoder_outputs->TFBaseModelOutput(last_hidden_state=encoder_outputs)
transformers.TFMBartForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFMBartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.TFMBartForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMBartForConditionalGeneration.get_bias(self)
transformers.TFMBartForConditionalGeneration.get_decoder(self)
transformers.TFMBartForConditionalGeneration.get_encoder(self)
transformers.TFMBartForConditionalGeneration.get_output_embeddings(self)
transformers.TFMBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFMBartForConditionalGeneration.serving_output(self,output)
transformers.TFMBartForConditionalGeneration.set_bias(self,value)
transformers.TFMBartForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFMBartModel(self,config:MBartConfig,*inputs,**kwargs)
transformers.TFMBartModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMBartModel.get_decoder(self)
transformers.TFMBartModel.get_encoder(self)
transformers.TFMBartModel.serving_output(self,output)
transformers.TFMBartPreTrainedModel(TFPreTrainedModel)
transformers.TFMBartPreTrainedModel.dummy_inputs(self)
transformers.TFMBartPreTrainedModel.serving(self,inputs)
transformers.models.mbart.modeling_tf_mbart.TFMBartAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.mbart.modeling_tf_mbart.TFMBartAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoder(self,config:MBartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoder.__init__(self,config:MBartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoder.get_embed_tokens(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoderLayer(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoderLayer.__init__(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,cross_attn_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoder(self,config:MBartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoder.__init__(self,config:MBartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoder.get_embed_tokens(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoderLayer(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoderLayer.__init__(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,training=False)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.get_bias(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.get_decoder(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.get_encoder(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.get_output_embeddings(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.serving_output(self,output)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.set_bias(self,value)
transformers.models.mbart.modeling_tf_mbart.TFMBartForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.mbart.modeling_tf_mbart.TFMBartLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartLearnedPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.mbart.modeling_tf_mbart.TFMBartMainLayer(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartMainLayer.__init__(self,config:MBartConfig,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartMainLayer.get_input_embeddings(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.mbart.modeling_tf_mbart.TFMBartModel(self,config:MBartConfig,*inputs,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartModel.__init__(self,config:MBartConfig,*inputs,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mbart.modeling_tf_mbart.TFMBartModel.get_decoder(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartModel.get_encoder(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartModel.serving_output(self,output)
transformers.models.mbart.modeling_tf_mbart.TFMBartPreTrainedModel(TFPreTrainedModel)
transformers.models.mbart.modeling_tf_mbart.TFMBartPreTrainedModel.dummy_inputs(self)
transformers.models.mbart.modeling_tf_mbart.TFMBartPreTrainedModel.serving(self,inputs)
transformers.models.mbart.modeling_tf_mbart._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.mbart.modeling_tf_mbart._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.mbart.modeling_tf_mbart.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/__init__.py----------------------------------------
A:transformers.models.mbart.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mbart/tokenization_mbart50.py----------------------------------------
A:transformers.models.mbart.tokenization_mbart50.logger->utils.logging.get_logger(__name__)
A:transformers.models.mbart.tokenization_mbart50.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.mbart.tokenization_mbart50.self.sp_model_size->len(self.sp_model)
A:transformers.models.mbart.tokenization_mbart50.self._additional_special_tokens->list(self.lang_code_to_id.keys())
A:transformers.models.mbart.tokenization_mbart50.state->self.__dict__.copy()
A:transformers.models.mbart.tokenization_mbart50.spm_id->self.sp_model.PieceToId(token)
A:transformers.models.mbart.tokenization_mbart50.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.mbart.tokenization_mbart50.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.mbart.tokenization_mbart50.inputs->self(raw_inputs, add_special_tokens=True, return_tensors='pt', **extra_kwargs)
A:transformers.models.mbart.tokenization_mbart50.tgt_lang_id->self.convert_tokens_to_ids(tgt_lang)
transformers.MBart50Tokenizer(self,vocab_file,src_lang=None,tgt_lang=None,eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.MBart50Tokenizer.__getstate__(self)->Dict
transformers.MBart50Tokenizer.__setstate__(self,d:Dict)->None
transformers.MBart50Tokenizer._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.MBart50Tokenizer._convert_id_to_token(self,index:int)->str
transformers.MBart50Tokenizer._convert_token_to_id(self,token:str)->int
transformers.MBart50Tokenizer._tokenize(self,text:str)->List[str]
transformers.MBart50Tokenizer.as_target_tokenizer(self)
transformers.MBart50Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MBart50Tokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.MBart50Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.MBart50Tokenizer.get_vocab(self)->Dict
transformers.MBart50Tokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.MBart50Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.MBart50Tokenizer.set_src_lang_special_tokens(self,src_lang:str)->None
transformers.MBart50Tokenizer.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.MBart50Tokenizer.src_lang(self)->str
transformers.MBart50Tokenizer.src_lang(self,new_src_lang:str)->None
transformers.MBart50Tokenizer.vocab_size(self)->int
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer(self,vocab_file,src_lang=None,tgt_lang=None,eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.__getstate__(self)->Dict
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.__init__(self,vocab_file,src_lang=None,tgt_lang=None,eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.__setstate__(self,d:Dict)->None
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer._convert_id_to_token(self,index:int)->str
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer._convert_token_to_id(self,token:str)->int
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer._tokenize(self,text:str)->List[str]
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.as_target_tokenizer(self)
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.get_vocab(self)->Dict
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',**kwargs)->BatchEncoding
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.set_src_lang_special_tokens(self,src_lang:str)->None
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.src_lang(self)->str
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.src_lang(self,new_src_lang:str)->None
transformers.models.mbart.tokenization_mbart50.MBart50Tokenizer.vocab_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/led/modeling_tf_led.py----------------------------------------
A:transformers.models.led.modeling_tf_led.logger->utils.logging.get_logger(__name__)
A:transformers.models.led.modeling_tf_led.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.led.modeling_tf_led.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.led.modeling_tf_led.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.led.modeling_tf_led.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.led.modeling_tf_led.mask->tensorflow.cast(mask, dtype=attn_probs_from_global_key_trans.dtype)
A:transformers.models.led.modeling_tf_led.one_cst->tensorflow.constant(1.0)
A:transformers.models.led.modeling_tf_led.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.led.modeling_tf_led.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.led.modeling_tf_led.self.head_dim->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.led.modeling_tf_led.self.query->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.led.modeling_tf_led.self.key->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.led.modeling_tf_led.self.value->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.led.modeling_tf_led.self.query_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='query_global')
A:transformers.models.led.modeling_tf_led.self.key_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='key_global')
A:transformers.models.led.modeling_tf_led.self.value_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='value_global')
A:transformers.models.led.modeling_tf_led.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.led.modeling_tf_led.self.global_dropout->tensorflow.keras.layers.Dropout(config.attention_probs_dropout_prob)
A:transformers.models.led.modeling_tf_led.query_vectors->tensorflow.reshape(query_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.models.led.modeling_tf_led.key_vectors->tensorflow.reshape(key_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.models.led.modeling_tf_led.value_vectors->tensorflow.reshape(value_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.models.led.modeling_tf_led.(batch_size, seq_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.led.modeling_tf_led.attn_scores->tensorflow.concat((attn_probs_from_global_key, attn_scores), axis=-1)
A:transformers.models.led.modeling_tf_led.diagonal_mask->self._sliding_chunks_query_key_matmul(tf.ones(shape_list(attention_mask)), attention_mask, self.one_sided_attn_window_size)
A:transformers.models.led.modeling_tf_led.(max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero)->self._get_global_attn_indices(is_index_global_attn)
A:transformers.models.led.modeling_tf_led.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.led.modeling_tf_led.masked_index->tensorflow.cond(is_global_attn, lambda : tf.tile(is_index_masked[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + max_num_global_attn_indices + 1)), lambda : tf.tile(is_index_masked[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + 1)))
A:transformers.models.led.modeling_tf_led.attn_output->self.out_proj(attn_output)
A:transformers.models.led.modeling_tf_led.(attn_output, global_attn_probs)->tensorflow.cond(is_global_attn, lambda : self._compute_global_attn_output_from_hidden(attn_output=attn_output, hidden_states=hidden_states, max_num_global_attn_indices=max_num_global_attn_indices, layer_head_mask=layer_head_mask, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero, is_index_masked=is_index_masked, training=training), lambda : (attn_output, tf.zeros((batch_size, self.num_heads, max_num_global_attn_indices, seq_len))))
A:transformers.models.led.modeling_tf_led.masked_global_attn_index->tensorflow.cond(is_global_attn, lambda : tf.tile(is_index_global_attn[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + max_num_global_attn_indices + 1)), lambda : tf.tile(is_index_global_attn[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + 1)))
A:transformers.models.led.modeling_tf_led.(batch_size, seq_len, num_heads, head_dim)->shape_list(value)
A:transformers.models.led.modeling_tf_led.query->tensorflow.reshape(tf.transpose(query, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.models.led.modeling_tf_led.key->tensorflow.reshape(tf.transpose(key, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.models.led.modeling_tf_led.chunked_query->tensorflow.cast(chunked_query, dtype=chunked_key.dtype)
A:transformers.models.led.modeling_tf_led.chunked_key->self._chunk(key, window_overlap)
A:transformers.models.led.modeling_tf_led.chunked_attention_scores->tensorflow.einsum('bcxd,bcyd->bcxy', chunked_query, chunked_key)
A:transformers.models.led.modeling_tf_led.paddings->tensorflow.convert_to_tensor([[0, 0], [0, padding_len]])
A:transformers.models.led.modeling_tf_led.diagonal_chunked_attention_scores->self._pad_and_transpose_last_two_dims(chunked_attention_scores, paddings)
A:transformers.models.led.modeling_tf_led.diagonal_attn_scores_up_triang->tensorflow.concat([diagonal_chunked_attention_scores[:, :, :window_overlap, :window_overlap + 1], diagonal_chunked_attention_scores[:, -1:, window_overlap:, :window_overlap + 1]], axis=1)
A:transformers.models.led.modeling_tf_led.diagonal_attn_scores_low_triang->tensorflow.where(first_chunk_mask, diagonal_attn_scores_first_chunk, diagonal_attn_scores_low_triang)
A:transformers.models.led.modeling_tf_led.diagonal_attn_scores_first_chunk->tensorflow.concat([tf.roll(diagonal_chunked_attention_scores, shift=[1, window_overlap], axis=[2, 3])[:, :, :window_overlap, :window_overlap], tf.zeros((batch_size * num_heads, 1, window_overlap, window_overlap), dtype=diagonal_chunked_attention_scores.dtype)], axis=1)
A:transformers.models.led.modeling_tf_led.diagonal_attention_scores->self._mask_invalid_locations(diagonal_attention_scores, window_overlap)
A:transformers.models.led.modeling_tf_led.mask_2d_upper->tensorflow.reverse(tf.linalg.band_part(tf.ones(shape=(window_overlap, window_overlap + 1)), -1, 0), axis=[0])
A:transformers.models.led.modeling_tf_led.padding->tensorflow.convert_to_tensor([[0, shape_list(input_tensor)[1] - window_overlap], [0, shape_list(input_tensor)[3] - window_overlap - 1]])
A:transformers.models.led.modeling_tf_led.mask_2d->tensorflow.pad(mask_2d_upper, padding)
A:transformers.models.led.modeling_tf_led.mask_4d->tensorflow.tile(mask_2d[None, :, None, :], (shape_list(input_tensor)[0], 1, 1, 1))
A:transformers.models.led.modeling_tf_led.input_tensor->tensorflow.where(tf.math.greater(mask_4d, 0), inf_tensor, input_tensor)
A:transformers.models.led.modeling_tf_led.chunked_attn_probs->self._pad_and_diagonalize(chunked_attn_probs)
A:transformers.models.led.modeling_tf_led.value->tensorflow.reshape(tf.transpose(value, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.models.led.modeling_tf_led.padded_value->tensorflow.pad(value, paddings, constant_values=-1)
A:transformers.models.led.modeling_tf_led.chunked_value->tensorflow.reshape(chunked_value, (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim))
A:transformers.models.led.modeling_tf_led.context->tensorflow.transpose(tf.reshape(context, (batch_size, num_heads, seq_len, head_dim)), (0, 2, 1, 3))
A:transformers.models.led.modeling_tf_led.hidden_states_padded->tensorflow.reshape(hidden_states_padded, (batch_size, chunk_size, hidden_dim, seq_length))
A:transformers.models.led.modeling_tf_led.(batch_size, chunk_size, seq_length, hidden_dim)->shape_list(hidden_states_padded)
A:transformers.models.led.modeling_tf_led.(total_num_heads, num_chunks, window_overlap, hidden_dim)->shape_list(chunked_hidden_states)
A:transformers.models.led.modeling_tf_led.chunked_hidden_states->tensorflow.reshape(chunked_hidden_states, (batch_size, num_output_chunks, 2 * window_overlap, hidden_dim))
A:transformers.models.led.modeling_tf_led.(batch_size, seq_length, hidden_dim)->shape_list(hidden_states)
A:transformers.models.led.modeling_tf_led.hidden_states->self.dropout(hidden_states, training=inputs['training'])
A:transformers.models.led.modeling_tf_led.num_global_attn_indices->tensorflow.cast(num_global_attn_indices, dtype=tf.constant(1).dtype)
A:transformers.models.led.modeling_tf_led.max_num_global_attn_indices->tensorflow.reduce_max(num_global_attn_indices)
A:transformers.models.led.modeling_tf_led.is_index_global_attn_nonzero->tensorflow.where(is_index_global_attn)
A:transformers.models.led.modeling_tf_led.is_local_index_global_attn_nonzero->tensorflow.where(is_local_index_global_attn)
A:transformers.models.led.modeling_tf_led.is_local_index_no_global_attn_nonzero->tensorflow.where(tf.math.logical_not(is_local_index_global_attn))
A:transformers.models.led.modeling_tf_led.global_key_vectors->self.reshape_and_transpose(global_key_vectors, batch_size)
A:transformers.models.led.modeling_tf_led.key_vectors_only_global->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_key_vectors, shape=(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim))
A:transformers.models.led.modeling_tf_led.attn_probs_from_global_key->tensorflow.transpose(attn_probs_from_global_key_trans, (0, 2, 3, 1))
A:transformers.models.led.modeling_tf_led.attn_probs_from_global_key_trans->tensorflow.tensor_scatter_nd_update(attn_probs_from_global_key_trans, is_local_index_no_global_attn_nonzero, mask)
A:transformers.models.led.modeling_tf_led.global_value_vectors->self.reshape_and_transpose(global_value_vectors, batch_size)
A:transformers.models.led.modeling_tf_led.value_vectors_only_global->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_value_vectors, shape=(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim))
A:transformers.models.led.modeling_tf_led.attn_output_only_global->tensorflow.einsum('blhs,bshd->blhd', attn_probs_only_global, value_vectors_only_global)
A:transformers.models.led.modeling_tf_led.attn_output_without_global->self._sliding_chunks_matmul_attn_probs_value(attn_probs_without_global, value_vectors, self.one_sided_attn_window_size)
A:transformers.models.led.modeling_tf_led.global_attn_hidden_states->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_attn_hidden_states, shape=(batch_size, max_num_global_attn_indices, self.embed_dim))
A:transformers.models.led.modeling_tf_led.global_query_vectors_only_global->self.reshape_and_transpose(global_query_vectors_only_global, batch_size)
A:transformers.models.led.modeling_tf_led.global_attn_scores->tensorflow.reshape(global_attn_scores, (batch_size * self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.models.led.modeling_tf_led.global_attn_scores_trans->tensorflow.tensor_scatter_nd_update(global_attn_scores_trans, is_local_index_no_global_attn_nonzero, global_attn_mask)
A:transformers.models.led.modeling_tf_led.global_attn_mask->tensorflow.cast(global_attn_mask, dtype=global_attn_scores_trans.dtype)
A:transformers.models.led.modeling_tf_led.attn_mask->tensorflow.tile(is_index_masked[:, None, None, :], (1, shape_list(global_attn_scores)[1], 1, 1))
A:transformers.models.led.modeling_tf_led.global_attn_probs_float->tensorflow.reshape(global_attn_probs_float, (batch_size * self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.models.led.modeling_tf_led.global_attn_probs->tensorflow.reshape(global_attn_probs, (batch_size, self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.models.led.modeling_tf_led.global_attn_output->tensorflow.reshape(global_attn_output, (batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim))
A:transformers.models.led.modeling_tf_led.nonzero_global_attn_output->tensorflow.reshape(nonzero_global_attn_output, (shape_list(is_local_index_global_attn_nonzero)[0], -1))
A:transformers.models.led.modeling_tf_led.self.longformer_self_attn->TFLEDEncoderSelfAttention(config, layer_id=layer_id, name='longformer_self_attn')
A:transformers.models.led.modeling_tf_led.self.output_dense->tensorflow.keras.layers.Dense(config.d_model, use_bias=True, name='output')
A:transformers.models.led.modeling_tf_led.self_outputs->self.longformer_self_attn([hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn], training=training)
A:transformers.models.led.modeling_tf_led.attention_output->self.output_dense(self_outputs[0], training=training)
A:transformers.models.led.modeling_tf_led.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.led.modeling_tf_led.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.led.modeling_tf_led.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.led.modeling_tf_led.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.led.modeling_tf_led.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.led.modeling_tf_led.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.led.modeling_tf_led.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.led.modeling_tf_led.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.led.modeling_tf_led.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.led.modeling_tf_led.self.self_attn->TFLEDDecoderAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.led.modeling_tf_led.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.led.modeling_tf_led.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.led.modeling_tf_led.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.led.modeling_tf_led.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.led.modeling_tf_led.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.led.modeling_tf_led.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.led.modeling_tf_led.layer_outputs->encoder_layer(hidden_states=hidden_states, attention_mask=inputs['attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn)
A:transformers.models.led.modeling_tf_led.self.encoder_attn->TFLEDDecoderAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.led.modeling_tf_led.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.led.modeling_tf_led.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.led.modeling_tf_led.(hidden_states, _, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=encoder_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.led.modeling_tf_led.input_ids->tensorflow.pad(input_ids, paddings, constant_values=pad_token_id)
A:transformers.models.led.modeling_tf_led.attention_mask->tensorflow.pad(attention_mask, paddings, constant_values=False)
A:transformers.models.led.modeling_tf_led.global_attention_mask->tensorflow.convert_to_tensor([[0, 0, 0, 0, 1], [0, 0, 1, 0, 0]])
A:transformers.models.led.modeling_tf_led.output->self.call(inputs)
A:transformers.models.led.modeling_tf_led.self.embed_positions->TFLEDLearnedPositionalEmbedding(config.max_decoder_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.led.modeling_tf_led.self.layernorm_embedding->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layernorm_embedding')
A:transformers.models.led.modeling_tf_led.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, encoder_outputs=encoder_outputs, global_attention_mask=global_attention_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.led.modeling_tf_led.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.led.modeling_tf_led.inputs['inputs_embeds']->self.embed_tokens(inputs['input_ids'])
A:transformers.models.led.modeling_tf_led.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.led.modeling_tf_led.(padding_len, inputs['input_ids'], inputs['attention_mask'], inputs['inputs_embeds'])->self._pad_to_window_size(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], inputs_embeds=inputs['inputs_embeds'], pad_token_id=self.padding_idx)
A:transformers.models.led.modeling_tf_led.is_index_masked->tensorflow.math.less(tf.cast(inputs['attention_mask'], tf.int8), 1)
A:transformers.models.led.modeling_tf_led.is_index_global_attn->tensorflow.math.greater(tf.cast(inputs['attention_mask'], tf.int8), 1)
A:transformers.models.led.modeling_tf_led.is_global_attn->tensorflow.math.reduce_any(is_index_global_attn)
A:transformers.models.led.modeling_tf_led.embed_pos->self.embed_positions(input_shape)
A:transformers.models.led.modeling_tf_led.hidden_states_to_add->self.compute_hidden_states(hidden_states, padding_len)
A:transformers.models.led.modeling_tf_led.dropout_probability->random.uniform(0, 1)
A:transformers.models.led.modeling_tf_led.input_ids_padding->tensorflow.fill((batch_size, padding_len), pad_token_id)
A:transformers.models.led.modeling_tf_led.inputs_embeds_padding->self.embed_tokens(input_ids_padding)
A:transformers.models.led.modeling_tf_led.inputs_embeds->tensorflow.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda : inputs_embeds)
A:transformers.models.led.modeling_tf_led.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.led.modeling_tf_led.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.led.modeling_tf_led.(hidden_states, layer_self_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, encoder_layer_head_mask=inputs['encoder_head_mask'][idx] if inputs['encoder_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.led.modeling_tf_led.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='led.shared')
A:transformers.models.led.modeling_tf_led.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.led.modeling_tf_led.self.encoder->TFLEDEncoder(config, embed_tokens, name='encoder')
A:transformers.models.led.modeling_tf_led.self.decoder->TFLEDDecoder(config, embed_tokens, name='decoder')
A:transformers.models.led.modeling_tf_led.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.led.modeling_tf_led.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], encoder_head_mask=inputs['head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.led.modeling_tf_led.self.led->TFLEDMainLayer(config, name='led')
A:transformers.models.led.modeling_tf_led.outputs->self.led(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], decoder_attention_mask=inputs['decoder_attention_mask'], encoder_outputs=inputs['encoder_outputs'], global_attention_mask=inputs['global_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.led.modeling_tf_led.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.led.modeling_tf_led.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.led.modeling_tf_led.lm_logits->self.led.shared(outputs[0], mode='linear')
A:transformers.models.led.modeling_tf_led.encoder_outputs->TFLEDEncoderBaseModelOutput(last_hidden_state=encoder_outputs)
A:transformers.models.led.modeling_tf_led.loss_fn->tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
A:transformers.models.led.modeling_tf_led.melted_labels->tensorflow.reshape(labels, (-1,))
A:transformers.models.led.modeling_tf_led.active_loss->tensorflow.not_equal(melted_labels, self.config.pad_token_id)
A:transformers.models.led.modeling_tf_led.reduced_logits->tensorflow.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)
A:transformers.models.led.modeling_tf_led.labels->tensorflow.boolean_mask(melted_labels, active_loss)
transformers.TFLEDForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFLEDForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.TFLEDForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs:Optional[TFLEDEncoderBaseModelOutput]=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFLEDForConditionalGeneration.compute_loss(self,labels,logits)
transformers.TFLEDForConditionalGeneration.get_bias(self)
transformers.TFLEDForConditionalGeneration.get_decoder(self)
transformers.TFLEDForConditionalGeneration.get_encoder(self)
transformers.TFLEDForConditionalGeneration.get_output_embeddings(self)
transformers.TFLEDForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFLEDForConditionalGeneration.serving_output(self,output)
transformers.TFLEDForConditionalGeneration.set_bias(self,value)
transformers.TFLEDForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFLEDModel(self,config,*inputs,**kwargs)
transformers.TFLEDModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFLEDEncoderBaseModelOutput]]=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFLEDModel.get_decoder(self)
transformers.TFLEDModel.get_encoder(self)
transformers.TFLEDModel.serving_output(self,output)
transformers.TFLEDPreTrainedModel(TFPreTrainedModel)
transformers.TFLEDPreTrainedModel.dummy_inputs(self)
transformers.TFLEDPreTrainedModel.serving(self,inputs)
transformers.models.led.modeling_tf_led.TFLEDDecoder(self,config:LEDConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoder.__init__(self,config:LEDConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,encoder_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.led.modeling_tf_led.TFLEDDecoderAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoderAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoderAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.led.modeling_tf_led.TFLEDDecoderAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.led.modeling_tf_led.TFLEDDecoderLayer(self,config:LEDConfig,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoderLayer.__init__(self,config:LEDConfig,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,encoder_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.led.modeling_tf_led.TFLEDEncoder(self,config:LEDConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoder.__init__(self,config:LEDConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoder._pad_to_window_size(self,input_ids,attention_mask,inputs_embeds,pad_token_id)
transformers.models.led.modeling_tf_led.TFLEDEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,global_attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoder.compute_hidden_states(self,hidden_states,padding_len)
transformers.models.led.modeling_tf_led.TFLEDEncoder.get_embed_tokens(self)
transformers.models.led.modeling_tf_led.TFLEDEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.led.modeling_tf_led.TFLEDEncoderAttention(self,config,layer_id,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoderAttention.__init__(self,config,layer_id,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoderAttention.call(self,inputs,training=False)
transformers.models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput(ModelOutput)
transformers.models.led.modeling_tf_led.TFLEDEncoderLayer(self,config:LEDConfig,layer_id:int,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoderLayer.__init__(self,config:LEDConfig,layer_id:int,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,is_index_masked:tf.Tensor,is_index_global_attn:tf.Tensor,is_global_attn:bool,training=False)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention(self,config,layer_id,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention.__init__(self,config,layer_id,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._chunk(hidden_states,window_overlap)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._compute_global_attn_output_from_hidden(self,attn_output,hidden_states,max_num_global_attn_indices,layer_head_mask,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked,training)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._concat_with_global_key_attn_probs(self,attn_scores,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._mask_invalid_locations(input_tensor,window_overlap)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,paddings)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs,value,window_overlap)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention._sliding_chunks_query_key_matmul(self,query,key,window_overlap)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention.call(self,inputs,training=False)
transformers.models.led.modeling_tf_led.TFLEDEncoderSelfAttention.reshape_and_transpose(self,vector,batch_size)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs:Optional[TFLEDEncoderBaseModelOutput]=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.compute_loss(self,labels,logits)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.get_bias(self)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.get_decoder(self)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.get_encoder(self)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.get_output_embeddings(self)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.serving_output(self,output)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.set_bias(self,value)
transformers.models.led.modeling_tf_led.TFLEDForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.led.modeling_tf_led.TFLEDLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDLearnedPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.led.modeling_tf_led.TFLEDMainLayer(self,config:LEDConfig,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDMainLayer.__init__(self,config:LEDConfig,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFLEDEncoderBaseModelOutput]]=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDMainLayer.get_input_embeddings(self)
transformers.models.led.modeling_tf_led.TFLEDMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.led.modeling_tf_led.TFLEDModel(self,config,*inputs,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDModel.__init__(self,config,*inputs,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFLEDEncoderBaseModelOutput]]=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.led.modeling_tf_led.TFLEDModel.get_decoder(self)
transformers.models.led.modeling_tf_led.TFLEDModel.get_encoder(self)
transformers.models.led.modeling_tf_led.TFLEDModel.serving_output(self,output)
transformers.models.led.modeling_tf_led.TFLEDPreTrainedModel(TFPreTrainedModel)
transformers.models.led.modeling_tf_led.TFLEDPreTrainedModel.dummy_inputs(self)
transformers.models.led.modeling_tf_led.TFLEDPreTrainedModel.serving(self,inputs)
transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput(ModelOutput)
transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput(ModelOutput)
transformers.models.led.modeling_tf_led._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.led.modeling_tf_led._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.led.modeling_tf_led.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/led/tokenization_led.py----------------------------------------
A:transformers.models.led.tokenization_led.logger->utils.logging.get_logger(__name__)
transformers.LEDTokenizer(BartTokenizer)
transformers.models.led.tokenization_led.LEDTokenizer(BartTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/led/modeling_led.py----------------------------------------
A:transformers.models.led.modeling_led.logger->utils.logging.get_logger(__name__)
A:transformers.models.led.modeling_led.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.led.modeling_led.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.led.modeling_led.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.led.modeling_led.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.led.modeling_led.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.led.modeling_led.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.led.modeling_led.expanded_attention_mask->inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)
A:transformers.models.led.modeling_led.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.led.modeling_led.self.head_dim->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.led.modeling_led.self.query->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.led.modeling_led.self.key->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.led.modeling_led.self.value->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.led.modeling_led.self.query_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.led.modeling_led.self.key_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.led.modeling_led.self.value_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.led.modeling_led.hidden_states->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
A:transformers.models.led.modeling_led.query_vectors->query_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.led.modeling_led.key_vectors->key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.led.modeling_led.value_vectors->value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.led.modeling_led.(seq_len, batch_size, embed_dim)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.led.modeling_led.attn_scores->torch.cat((global_key_attn_scores, attn_scores), dim=-1)
A:transformers.models.led.modeling_led.float_mask->remove_from_windowed_attention_mask.type_as(query_vectors).masked_fill(remove_from_windowed_attention_mask, -10000.0)
A:transformers.models.led.modeling_led.diagonal_mask->self._sliding_chunks_query_key_matmul(float_mask.new_ones(size=float_mask.size()), float_mask, self.one_sided_attn_window_size)
A:transformers.models.led.modeling_led.(max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero)->self._get_global_attn_indices(is_index_global_attn)
A:transformers.models.led.modeling_led.global_key_attn_scores->self._concat_with_global_key_attn_probs(query_vectors=query_vectors, key_vectors=key_vectors, max_num_global_attn_indices=max_num_global_attn_indices, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero)
A:transformers.models.led.modeling_led.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.led.modeling_led.attn_output->self.out_proj(attn_output)
A:transformers.models.led.modeling_led.(global_attn_output, global_attn_probs)->self._compute_global_attn_output_from_hidden(hidden_states=hidden_states, max_num_global_attn_indices=max_num_global_attn_indices, layer_head_mask=layer_head_mask, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero, is_index_masked=is_index_masked)
A:transformers.models.led.modeling_led.attn_output[is_index_global_attn_nonzero[::-1]]->nonzero_global_attn_output.view(len(is_local_index_global_attn_nonzero[0]), -1)
A:transformers.models.led.modeling_led.hidden_states_padded->hidden_states_padded.view(*hidden_states_padded.size()[:-2], hidden_states_padded.size(-1), hidden_states_padded.size(-2)).view(*hidden_states_padded.size()[:-2], hidden_states_padded.size(-1), hidden_states_padded.size(-2))
A:transformers.models.led.modeling_led.(total_num_heads, num_chunks, window_overlap, hidden_dim)->chunked_hidden_states.view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim).size()
A:transformers.models.led.modeling_led.chunked_hidden_states->chunked_hidden_states.view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim).view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim)
A:transformers.models.led.modeling_led.chunk_size->list(hidden_states.size())
A:transformers.models.led.modeling_led.chunk_stride->list(hidden_states.stride())
A:transformers.models.led.modeling_led.beginning_mask_2d->input_tensor.new_ones(affected_seq_len, affected_seq_len + 1).tril().flip(dims=[0])
A:transformers.models.led.modeling_led.ending_mask->ending_mask.expand(ending_input.size()).expand(ending_input.size())
A:transformers.models.led.modeling_led.beginning_mask->beginning_mask.expand(beginning_input.size()).expand(beginning_input.size())
A:transformers.models.led.modeling_led.(batch_size, seq_len, num_heads, head_dim)->value.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).size()
A:transformers.models.led.modeling_led.query->self._chunk(query, window_overlap)
A:transformers.models.led.modeling_led.key->self._chunk(key, window_overlap)
A:transformers.models.led.modeling_led.diagonal_chunked_attention_scores->self._pad_and_transpose_last_two_dims(diagonal_chunked_attention_scores, padding=(0, 0, 0, 1))
A:transformers.models.led.modeling_led.diagonal_attention_scores->diagonal_attention_scores.view(batch_size, num_heads, seq_len, 2 * window_overlap + 1).transpose(2, 1).view(batch_size, num_heads, seq_len, 2 * window_overlap + 1).transpose(2, 1)
A:transformers.models.led.modeling_led.chunked_attn_probs->self._pad_and_diagonalize(chunked_attn_probs)
A:transformers.models.led.modeling_led.value->value.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)
A:transformers.models.led.modeling_led.padded_value->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1)
A:transformers.models.led.modeling_led.chunked_value_stride->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1).stride()
A:transformers.models.led.modeling_led.chunked_value->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1).as_strided(size=chunked_value_size, stride=chunked_value_stride)
A:transformers.models.led.modeling_led.context->torch.einsum('bcwd,bcdh->bcwh', (chunked_attn_probs, chunked_value))
A:transformers.models.led.modeling_led.num_global_attn_indices->is_index_global_attn.long().sum(dim=1)
A:transformers.models.led.modeling_led.max_num_global_attn_indices->is_index_global_attn.long().sum(dim=1).max()
A:transformers.models.led.modeling_led.is_index_global_attn_nonzero->is_index_global_attn.nonzero(as_tuple=True)
A:transformers.models.led.modeling_led.is_local_index_global_attn_nonzero->is_local_index_global_attn.nonzero(as_tuple=True)
A:transformers.models.led.modeling_led.is_local_index_no_global_attn_nonzero->(is_local_index_global_attn == 0).nonzero(as_tuple=True)
A:transformers.models.led.modeling_led.key_vectors_only_global->key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).new_zeros(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim)
A:transformers.models.led.modeling_led.attn_probs_from_global_key->torch.einsum('blhd,bshd->blhs', (query_vectors, key_vectors_only_global))
A:transformers.models.led.modeling_led.attn_probs_only_global->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training).narrow(-1, 0, max_num_global_attn_indices)
A:transformers.models.led.modeling_led.value_vectors_only_global->value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).new_zeros(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim)
A:transformers.models.led.modeling_led.attn_output_only_global->torch.matmul(attn_probs_only_global.transpose(1, 2), value_vectors_only_global.transpose(1, 2)).transpose(1, 2)
A:transformers.models.led.modeling_led.attn_probs_without_global->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training).narrow(-1, max_num_global_attn_indices, attn_probs.size(-1) - max_num_global_attn_indices).contiguous()
A:transformers.models.led.modeling_led.attn_output_without_global->self._sliding_chunks_matmul_attn_probs_value(attn_probs_without_global, value_vectors, self.one_sided_attn_window_size)
A:transformers.models.led.modeling_led.global_attn_hidden_states->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).new_zeros(max_num_global_attn_indices, batch_size, self.embed_dim)
A:transformers.models.led.modeling_led.global_query_vectors_only_global->global_query_vectors_only_global.contiguous().view(max_num_global_attn_indices, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(max_num_global_attn_indices, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.led.modeling_led.global_key_vectors->global_key_vectors.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.led.modeling_led.global_value_vectors->global_value_vectors.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.led.modeling_led.global_attn_scores->global_attn_scores.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.models.led.modeling_led.global_attn_probs_float->global_attn_probs_float.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.models.led.modeling_led.global_attn_probs->global_attn_probs.view(batch_size, self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size, self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.models.led.modeling_led.global_attn_output->global_attn_output.view(batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim).view(batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim)
A:transformers.models.led.modeling_led.self.longformer_self_attn->LEDEncoderSelfAttention(config, layer_id=layer_id)
A:transformers.models.led.modeling_led.self.output->torch.nn.Linear(config.d_model, config.d_model)
A:transformers.models.led.modeling_led.self_outputs->self.longformer_self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=output_attentions)
A:transformers.models.led.modeling_led.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.led.modeling_led.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.led.modeling_led.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.led.modeling_led.self.out_proj->torch.nn.Linear(inner_dim, num_classes)
A:transformers.models.led.modeling_led.(bsz, tgt_len, embed_dim)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.led.modeling_led.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.led.modeling_led.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.led.modeling_led.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.led.modeling_led.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.led.modeling_led.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.led.modeling_led.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.led.modeling_led.self.self_attn->LEDDecoderAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.led.modeling_led.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.led.modeling_led.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.led.modeling_led.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.led.modeling_led.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.led.modeling_led.attn_outputs->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=output_attentions)
A:transformers.models.led.modeling_led.self.encoder_attn->LEDDecoderAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.led.modeling_led.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.led.modeling_led.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.led.modeling_led.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.led.modeling_led.self.dense->torch.nn.Linear(input_dim, inner_dim)
A:transformers.models.led.modeling_led.self.dropout->torch.nn.Dropout(p=pooler_dropout)
A:transformers.models.led.modeling_led.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.led.modeling_led.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.led.modeling_led.self.embed_positions->LEDLearnedPositionalEmbedding(self.max_target_positions, config.d_model)
A:transformers.models.led.modeling_led.self.layers->torch.nn.ModuleList([LEDDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.led.modeling_led.self.layernorm_embedding->torch.nn.LayerNorm(config.d_model)
A:transformers.models.led.modeling_led.input_ids_padding->self.embed_tokens(input_ids).new_full((batch_size, padding_len), self.config.pad_token_id, dtype=torch.long)
A:transformers.models.led.modeling_led.inputs_embeds_padding->self.embed_tokens(input_ids_padding)
A:transformers.models.led.modeling_led.inputs_embeds->self.embed_tokens(input_ids)
A:transformers.models.led.modeling_led.attention_mask->self._merge_to_attention_mask(attention_mask, global_attention_mask)
A:transformers.models.led.modeling_led.(padding_len, input_ids, attention_mask, inputs_embeds)->self._pad_to_window_size(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, pad_token_id=self.config.pad_token_id)
A:transformers.models.led.modeling_led.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.led.modeling_led.is_global_attn->is_index_global_attn.flatten().any().item()
A:transformers.models.led.modeling_led.embed_pos->self.embed_positions(input_shape)
A:transformers.models.led.modeling_led.dropout_probability->random.uniform(0, 1)
A:transformers.models.led.modeling_led.layer_outputs->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.led.modeling_led.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.led.modeling_led.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.led.modeling_led.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.led.modeling_led.self.encoder->LEDEncoder(config, self.shared)
A:transformers.models.led.modeling_led.self.decoder->LEDDecoder(config, self.shared)
A:transformers.models.led.modeling_led.encoder_outputs->LEDEncoderBaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None, global_attentions=encoder_outputs[3] if len(encoder_outputs) > 3 else None)
A:transformers.models.led.modeling_led.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.led.modeling_led.self.led->LEDModel(config)
A:transformers.models.led.modeling_led.self.lm_head->torch.nn.Linear(config.d_model, self.led.shared.num_embeddings, bias=False)
A:transformers.models.led.modeling_led.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.led.modeling_led.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.led.modeling_led.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.led.modeling_led.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.led.modeling_led.outputs->self.led(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, global_attention_mask=global_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.led.modeling_led.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.led.modeling_led.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.led.modeling_led.self.classification_head->LEDClassificationHead(config.d_model, config.d_model, config.num_labels, config.classifier_dropout)
A:transformers.models.led.modeling_led.eos_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).eq(self.config.eos_token_id)
A:transformers.models.led.modeling_led.logits->self.qa_outputs(sequence_output)
A:transformers.models.led.modeling_led.loss->loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
A:transformers.models.led.modeling_led.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.led.modeling_led.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.led.modeling_led.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.led.modeling_led.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.led.modeling_led.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.led.modeling_led.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.led.modeling_led.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.led.modeling_led.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.led.modeling_led.end_loss->loss_fct(end_logits, end_positions)
transformers.LEDForConditionalGeneration(self,config:LEDConfig)
transformers.LEDForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.LEDForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.LEDForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LEDForConditionalGeneration.get_decoder(self)
transformers.LEDForConditionalGeneration.get_encoder(self)
transformers.LEDForConditionalGeneration.get_output_embeddings(self)
transformers.LEDForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.LEDForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.LEDForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.LEDForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.LEDForQuestionAnswering(self,config)
transformers.LEDForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LEDForSequenceClassification(self,config:LEDConfig,**kwargs)
transformers.LEDForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LEDModel(self,config:LEDConfig)
transformers.LEDModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LEDModel.get_decoder(self)
transformers.LEDModel.get_encoder(self)
transformers.LEDModel.get_input_embeddings(self)
transformers.LEDModel.set_input_embeddings(self,value)
transformers.LEDPreTrainedModel(PreTrainedModel)
transformers.LEDPreTrainedModel._init_weights(self,module)
transformers.LEDPreTrainedModel.dummy_inputs(self)
transformers.models.led.modeling_led.LEDClassificationHead(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.led.modeling_led.LEDClassificationHead.__init__(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.led.modeling_led.LEDClassificationHead.forward(self,hidden_states:torch.Tensor)
transformers.models.led.modeling_led.LEDDecoder(self,config:LEDConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.led.modeling_led.LEDDecoder.__init__(self,config:LEDConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.led.modeling_led.LEDDecoder.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.led.modeling_led.LEDDecoderAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.led.modeling_led.LEDDecoderAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.led.modeling_led.LEDDecoderAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.led.modeling_led.LEDDecoderAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.led.modeling_led.LEDDecoderLayer(self,config:LEDConfig)
transformers.models.led.modeling_led.LEDDecoderLayer.__init__(self,config:LEDConfig)
transformers.models.led.modeling_led.LEDDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.led.modeling_led.LEDEncoder(self,config:LEDConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.led.modeling_led.LEDEncoder.__init__(self,config:LEDConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.led.modeling_led.LEDEncoder._merge_to_attention_mask(self,attention_mask:torch.Tensor,global_attention_mask:torch.Tensor)
transformers.models.led.modeling_led.LEDEncoder._pad_to_window_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.models.led.modeling_led.LEDEncoder.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.led.modeling_led.LEDEncoderAttention(self,config,layer_id)
transformers.models.led.modeling_led.LEDEncoderAttention.__init__(self,config,layer_id)
transformers.models.led.modeling_led.LEDEncoderAttention.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,is_index_masked:Optional[torch.Tensor]=None,is_index_global_attn:Optional[torch.Tensor]=None,is_global_attn:Optional[bool]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.led.modeling_led.LEDEncoderBaseModelOutput(ModelOutput)
transformers.models.led.modeling_led.LEDEncoderLayer(self,config:LEDConfig,layer_id:int)
transformers.models.led.modeling_led.LEDEncoderLayer.__init__(self,config:LEDConfig,layer_id:int)
transformers.models.led.modeling_led.LEDEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=False)
transformers.models.led.modeling_led.LEDEncoderSelfAttention(self,config,layer_id)
transformers.models.led.modeling_led.LEDEncoderSelfAttention.__init__(self,config,layer_id)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._chunk(hidden_states,window_overlap)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._compute_global_attn_output_from_hidden(self,hidden_states,max_num_global_attn_indices,layer_head_mask,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._concat_with_global_key_attn_probs(self,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._mask_invalid_locations(input_tensor,affected_seq_len)->torch.Tensor
transformers.models.led.modeling_led.LEDEncoderSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,padding)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs:torch.Tensor,value:torch.Tensor,window_overlap:int)
transformers.models.led.modeling_led.LEDEncoderSelfAttention._sliding_chunks_query_key_matmul(self,query:torch.Tensor,key:torch.Tensor,window_overlap:int)
transformers.models.led.modeling_led.LEDEncoderSelfAttention.forward(self,hidden_states,attention_mask=None,layer_head_mask=None,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=False)
transformers.models.led.modeling_led.LEDForConditionalGeneration(self,config:LEDConfig)
transformers.models.led.modeling_led.LEDForConditionalGeneration.__init__(self,config:LEDConfig)
transformers.models.led.modeling_led.LEDForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.led.modeling_led.LEDForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.led.modeling_led.LEDForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.led.modeling_led.LEDForConditionalGeneration.get_decoder(self)
transformers.models.led.modeling_led.LEDForConditionalGeneration.get_encoder(self)
transformers.models.led.modeling_led.LEDForConditionalGeneration.get_output_embeddings(self)
transformers.models.led.modeling_led.LEDForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.led.modeling_led.LEDForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.led.modeling_led.LEDForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.led.modeling_led.LEDForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.led.modeling_led.LEDForQuestionAnswering(self,config)
transformers.models.led.modeling_led.LEDForQuestionAnswering.__init__(self,config)
transformers.models.led.modeling_led.LEDForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.led.modeling_led.LEDForSequenceClassification(self,config:LEDConfig,**kwargs)
transformers.models.led.modeling_led.LEDForSequenceClassification.__init__(self,config:LEDConfig,**kwargs)
transformers.models.led.modeling_led.LEDForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.led.modeling_led.LEDLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int)
transformers.models.led.modeling_led.LEDLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int)
transformers.models.led.modeling_led.LEDLearnedPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.led.modeling_led.LEDModel(self,config:LEDConfig)
transformers.models.led.modeling_led.LEDModel.__init__(self,config:LEDConfig)
transformers.models.led.modeling_led.LEDModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,global_attention_mask=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.led.modeling_led.LEDModel.get_decoder(self)
transformers.models.led.modeling_led.LEDModel.get_encoder(self)
transformers.models.led.modeling_led.LEDModel.get_input_embeddings(self)
transformers.models.led.modeling_led.LEDModel.set_input_embeddings(self,value)
transformers.models.led.modeling_led.LEDPreTrainedModel(PreTrainedModel)
transformers.models.led.modeling_led.LEDPreTrainedModel._init_weights(self,module)
transformers.models.led.modeling_led.LEDPreTrainedModel.dummy_inputs(self)
transformers.models.led.modeling_led.LEDSeq2SeqLMOutput(ModelOutput)
transformers.models.led.modeling_led.LEDSeq2SeqModelOutput(ModelOutput)
transformers.models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput(ModelOutput)
transformers.models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput(ModelOutput)
transformers.models.led.modeling_led._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.led.modeling_led._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.led.modeling_led.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/led/__init__.py----------------------------------------
A:transformers.models.led.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/led/configuration_led.py----------------------------------------
A:transformers.models.led.configuration_led.logger->utils.logging.get_logger(__name__)
transformers.LEDConfig(self,vocab_size=50265,max_encoder_position_embeddings=16384,max_decoder_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,pad_token_id=1,bos_token_id=0,eos_token_id=2,gradient_checkpointing=False,attention_window:Union[List[int],int]=512,**kwargs)
transformers.LEDConfig.attention_probs_dropout_prob(self)->float
transformers.LEDConfig.hidden_size(self)->int
transformers.LEDConfig.initializer_range(self)->float
transformers.LEDConfig.num_attention_heads(self)->int
transformers.models.led.configuration_led.LEDConfig(self,vocab_size=50265,max_encoder_position_embeddings=16384,max_decoder_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,pad_token_id=1,bos_token_id=0,eos_token_id=2,gradient_checkpointing=False,attention_window:Union[List[int],int]=512,**kwargs)
transformers.models.led.configuration_led.LEDConfig.__init__(self,vocab_size=50265,max_encoder_position_embeddings=16384,max_decoder_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,pad_token_id=1,bos_token_id=0,eos_token_id=2,gradient_checkpointing=False,attention_window:Union[List[int],int]=512,**kwargs)
transformers.models.led.configuration_led.LEDConfig.attention_probs_dropout_prob(self)->float
transformers.models.led.configuration_led.LEDConfig.hidden_size(self)->int
transformers.models.led.configuration_led.LEDConfig.initializer_range(self)->float
transformers.models.led.configuration_led.LEDConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/led/tokenization_led_fast.py----------------------------------------
A:transformers.models.led.tokenization_led_fast.logger->utils.logging.get_logger(__name__)
transformers.LEDTokenizerFast(BartTokenizerFast)
transformers.models.led.tokenization_led_fast.LEDTokenizerFast(BartTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm/configuration_xlm.py----------------------------------------
A:transformers.models.xlm.configuration_xlm.logger->utils.logging.get_logger(__name__)
transformers.XLMConfig(self,vocab_size=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,mask_token_id=0,lang_id=0,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.XLMConfig.hidden_size(self)
transformers.XLMConfig.n_words(self)
transformers.XLMConfig.n_words(self,value)
transformers.XLMConfig.num_attention_heads(self)
transformers.XLMConfig.num_hidden_layers(self)
transformers.models.xlm.configuration_xlm.XLMConfig(self,vocab_size=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,mask_token_id=0,lang_id=0,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.models.xlm.configuration_xlm.XLMConfig.__init__(self,vocab_size=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,mask_token_id=0,lang_id=0,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.models.xlm.configuration_xlm.XLMConfig.hidden_size(self)
transformers.models.xlm.configuration_xlm.XLMConfig.n_words(self)
transformers.models.xlm.configuration_xlm.XLMConfig.n_words(self,value)
transformers.models.xlm.configuration_xlm.XLMConfig.num_attention_heads(self)
transformers.models.xlm.configuration_xlm.XLMConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm/modeling_tf_xlm.py----------------------------------------
A:transformers.models.xlm.modeling_tf_xlm.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlm.modeling_tf_xlm.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.xlm.modeling_tf_xlm.out[:, 0::2]->tensorflow.constant(np.sin(position_enc[:, 0::2]))
A:transformers.models.xlm.modeling_tf_xlm.out[:, 1::2]->tensorflow.constant(np.cos(position_enc[:, 1::2]))
A:transformers.models.xlm.modeling_tf_xlm.alen->tensorflow.range(slen)
A:transformers.models.xlm.modeling_tf_xlm.mask->tensorflow.cast(mask, dtype=tensor.dtype)
A:transformers.models.xlm.modeling_tf_xlm.attn_mask->tensorflow.less_equal(tf.tile(tf.reshape(alen, (1, 1, slen)), (bs, slen, 1)), tf.reshape(alen, (1, slen, 1)))
A:transformers.models.xlm.modeling_tf_xlm.NEW_ID->itertools.count()
A:transformers.models.xlm.modeling_tf_xlm.self.layer_id->next(TFXLMMultiHeadAttention.NEW_ID)
A:transformers.models.xlm.modeling_tf_xlm.self.q_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='q_lin')
A:transformers.models.xlm.modeling_tf_xlm.self.k_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='k_lin')
A:transformers.models.xlm.modeling_tf_xlm.self.v_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='v_lin')
A:transformers.models.xlm.modeling_tf_xlm.self.out_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='out_lin')
A:transformers.models.xlm.modeling_tf_xlm.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.xlm.modeling_tf_xlm.self.pruned_heads->set()
A:transformers.models.xlm.modeling_tf_xlm.(bs, qlen, dim)->shape_list(input)
A:transformers.models.xlm.modeling_tf_xlm.q->tensorflow.multiply(q, tf.math.rsqrt(f_dim_per_head))
A:transformers.models.xlm.modeling_tf_xlm.k->tensorflow.cast(k, dtype=q.dtype)
A:transformers.models.xlm.modeling_tf_xlm.v->tensorflow.concat([v_, v], axis=2)
A:transformers.models.xlm.modeling_tf_xlm.f_dim_per_head->tensorflow.cast(dim_per_head, dtype=q.dtype)
A:transformers.models.xlm.modeling_tf_xlm.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.models.xlm.modeling_tf_xlm.weights->self.dropout(weights, training=training)
A:transformers.models.xlm.modeling_tf_xlm.context->unshape(context)
A:transformers.models.xlm.modeling_tf_xlm.self.lin1->tensorflow.keras.layers.Dense(dim_hidden, kernel_initializer=get_initializer(config.init_std), name='lin1')
A:transformers.models.xlm.modeling_tf_xlm.self.lin2->tensorflow.keras.layers.Dense(out_dim, kernel_initializer=get_initializer(config.init_std), name='lin2')
A:transformers.models.xlm.modeling_tf_xlm.x->self.dropout(x, training=training)
A:transformers.models.xlm.modeling_tf_xlm.self.attention_dropout->tensorflow.keras.layers.Dropout(config.attention_dropout)
A:transformers.models.xlm.modeling_tf_xlm.self.embeddings->TFSharedEmbeddings(self.n_words, self.dim, initializer_range=config.embed_init_std, name='embeddings')
A:transformers.models.xlm.modeling_tf_xlm.self.layer_norm_emb->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm_emb')
A:transformers.models.xlm.modeling_tf_xlm.pruned_heads->config.pruned_heads.copy().items()
A:transformers.models.xlm.modeling_tf_xlm.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.dim], initializer=get_initializer(self.embed_init_std))
A:transformers.models.xlm.modeling_tf_xlm.self.lang_embeddings->self.add_weight(name='embeddings', shape=[self.n_langs, self.dim], initializer=get_initializer(self.embed_init_std))
A:transformers.models.xlm.modeling_tf_xlm.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.xlm.modeling_tf_xlm.(bs, slen)->shape_list(inputs['input_ids'])
A:transformers.models.xlm.modeling_tf_xlm.inputs['lengths']->tensorflow.convert_to_tensor([slen] * bs)
A:transformers.models.xlm.modeling_tf_xlm.(mask, attn_mask)->get_masks(slen, inputs['lengths'], self.causal, padding_mask=inputs['attention_mask'])
A:transformers.models.xlm.modeling_tf_xlm.inputs['position_ids']->tensorflow.tile(inputs['position_ids'], (bs, 1))
A:transformers.models.xlm.modeling_tf_xlm.inputs['inputs_embeds']->self.embeddings(inputs['input_ids'])
A:transformers.models.xlm.modeling_tf_xlm.tensor->self.layer_norm2[i](tensor)
A:transformers.models.xlm.modeling_tf_xlm.attn_outputs->self.attentions[i](tensor, attn_mask, None, inputs['cache'], inputs['head_mask'][i], inputs['output_attentions'], training=inputs['training'])
A:transformers.models.xlm.modeling_tf_xlm.attn->self.dropout(attn, training=inputs['training'])
A:transformers.models.xlm.modeling_tf_xlm.inputs_list->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.models.xlm.modeling_tf_xlm.attns_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.models.xlm.modeling_tf_xlm.self.transformer->TFXLMMainLayer(config, name='transformer')
A:transformers.models.xlm.modeling_tf_xlm.outputs->self.pred_layer(output)
A:transformers.models.xlm.modeling_tf_xlm.self.bias->self.add_weight(shape=(self.n_words,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.xlm.modeling_tf_xlm.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.models.xlm.modeling_tf_xlm.self.pred_layer->TFXLMPredLayer(config, self.transformer.embeddings, name='pred_layer_._proj')
A:transformers.models.xlm.modeling_tf_xlm.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], langs=inputs['langs'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], lengths=inputs['lengths'], cache=inputs['cache'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.xlm.modeling_tf_xlm.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.init_std, name='sequence_summary')
A:transformers.models.xlm.modeling_tf_xlm.logits->self.qa_outputs(sequence_output)
A:transformers.models.xlm.modeling_tf_xlm.self.logits_proj->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='logits_proj')
A:transformers.models.xlm.modeling_tf_xlm.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.xlm.modeling_tf_xlm.output->self.call(input_ids=inputs)
A:transformers.models.xlm.modeling_tf_xlm.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.init_std), name='classifier')
A:transformers.models.xlm.modeling_tf_xlm.sequence_output->self.dropout(sequence_output, training=inputs['training'])
A:transformers.models.xlm.modeling_tf_xlm.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.init_std), name='qa_outputs')
A:transformers.models.xlm.modeling_tf_xlm.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.xlm.modeling_tf_xlm.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.xlm.modeling_tf_xlm.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.xlm.modeling_tf_xlm.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFXLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFXLMForMultipleChoice.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLMForMultipleChoice.dummy_inputs(self)
transformers.TFXLMForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.TFXLMForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFXLMForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFXLMForQuestionAnsweringSimple.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFXLMForQuestionAnsweringSimple.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFXLMForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFXLMForSequenceClassification.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLMForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFXLMForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFXLMForTokenClassification.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLMForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFXLMMainLayer(self,config,**kwargs)
transformers.TFXLMMainLayer._prune_heads(self,heads_to_prune)
transformers.TFXLMMainLayer.build(self,input_shape)
transformers.TFXLMMainLayer.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFXLMMainLayer.get_input_embeddings(self)
transformers.TFXLMMainLayer.set_input_embeddings(self,value)
transformers.TFXLMModel(self,config,*inputs,**kwargs)
transformers.TFXLMModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFXLMModel.serving_output(self,output)
transformers.TFXLMPreTrainedModel(TFPreTrainedModel)
transformers.TFXLMPreTrainedModel.dummy_inputs(self)
transformers.TFXLMWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFXLMWithLMHeadModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFXLMWithLMHeadModel.get_lm_head(self)
transformers.TFXLMWithLMHeadModel.get_prefix_bias_name(self)
transformers.TFXLMWithLMHeadModel.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.TFXLMWithLMHeadModel.serving_output(self,output)
transformers.TFXLMWithLMHeadModelOutput(ModelOutput)
transformers.models.xlm.modeling_tf_xlm.TFXLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForMultipleChoice.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForMultipleChoice.dummy_inputs(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.models.xlm.modeling_tf_xlm.TFXLMForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.xlm.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.xlm.modeling_tf_xlm.TFXLMForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForSequenceClassification.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.xlm.modeling_tf_xlm.TFXLMForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForTokenClassification.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer(self,config,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer.__init__(self,config,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer._prune_heads(self,heads_to_prune)
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer.build(self,input_shape)
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer.get_input_embeddings(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMMainLayer.set_input_embeddings(self,value)
transformers.models.xlm.modeling_tf_xlm.TFXLMModel(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMModel.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMModel.serving_output(self,output)
transformers.models.xlm.modeling_tf_xlm.TFXLMMultiHeadAttention(self,n_heads,dim,config,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMMultiHeadAttention.__init__(self,n_heads,dim,config,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMMultiHeadAttention.call(self,input,mask,kv,cache,head_mask,output_attentions,training=False)
transformers.models.xlm.modeling_tf_xlm.TFXLMMultiHeadAttention.prune_heads(self,heads)
transformers.models.xlm.modeling_tf_xlm.TFXLMPreTrainedModel(TFPreTrainedModel)
transformers.models.xlm.modeling_tf_xlm.TFXLMPreTrainedModel.dummy_inputs(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer(self,config,input_embeddings,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.__init__(self,config,input_embeddings,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.build(self,input_shape)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.call(self,hidden_states)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.get_bias(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.get_output_embeddings(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.set_bias(self,value)
transformers.models.xlm.modeling_tf_xlm.TFXLMPredLayer.set_output_embeddings(self,value)
transformers.models.xlm.modeling_tf_xlm.TFXLMTransformerFFN(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMTransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMTransformerFFN.call(self,input,training=False)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel.get_lm_head(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel.get_prefix_bias_name(self)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModel.serving_output(self,output)
transformers.models.xlm.modeling_tf_xlm.TFXLMWithLMHeadModelOutput(ModelOutput)
transformers.models.xlm.modeling_tf_xlm.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.models.xlm.modeling_tf_xlm.get_masks(slen,lengths,causal,padding_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm/tokenization_xlm.py----------------------------------------
A:transformers.models.xlm.tokenization_xlm.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlm.tokenization_xlm.pairs->get_pairs(word)
A:transformers.models.xlm.tokenization_xlm.text->lowercase_and_remove_accent(text)
A:transformers.models.xlm.tokenization_xlm.cat->unicodedata.category(char)
A:transformers.models.xlm.tokenization_xlm.self.cache_moses_punct_normalizer->dict()
A:transformers.models.xlm.tokenization_xlm.self.cache_moses_tokenizer->dict()
A:transformers.models.xlm.tokenization_xlm.self.lang_with_custom_tokenizer->set(['zh', 'th', 'ja'])
A:transformers.models.xlm.tokenization_xlm.self.encoder->json.load(vocab_handle)
A:transformers.models.xlm.tokenization_xlm.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.xlm.tokenization_xlm.punct_normalizer->sacremoses.MosesPunctNormalizer(lang=lang)
A:transformers.models.xlm.tokenization_xlm.moses_tokenizer->sacremoses.MosesTokenizer(lang=lang)
A:transformers.models.xlm.tokenization_xlm.self.ja_word_tokenizer->Mykytea.Mykytea(f"-model {os.path.expanduser('~')}/local/share/kytea/model.bin")
A:transformers.models.xlm.tokenization_xlm.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.xlm.tokenization_xlm.j->' '.join(word).index(first, i)
A:transformers.models.xlm.tokenization_xlm.new_word->tuple(new_word)
A:transformers.models.xlm.tokenization_xlm.word->' '.join(word)
A:transformers.models.xlm.tokenization_xlm.out_string->''.join(tokens).replace('</w>', ' ').strip()
A:transformers.models.xlm.tokenization_xlm.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.xlm.tokenization_xlm.merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
transformers.XLMTokenizer(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.XLMTokenizer._convert_id_to_token(self,index)
transformers.XLMTokenizer._convert_token_to_id(self,token)
transformers.XLMTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.XLMTokenizer.bpe(self,token)
transformers.XLMTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLMTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMTokenizer.do_lower_case(self)
transformers.XLMTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLMTokenizer.get_vocab(self)
transformers.XLMTokenizer.ja_tokenize(self,text)
transformers.XLMTokenizer.moses_pipeline(self,text,lang)
transformers.XLMTokenizer.moses_punct_norm(self,text,lang)
transformers.XLMTokenizer.moses_tokenize(self,text,lang)
transformers.XLMTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.XLMTokenizer.vocab_size(self)
transformers.models.xlm.tokenization_xlm.XLMTokenizer(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.models.xlm.tokenization_xlm.XLMTokenizer._convert_id_to_token(self,index)
transformers.models.xlm.tokenization_xlm.XLMTokenizer._convert_token_to_id(self,token)
transformers.models.xlm.tokenization_xlm.XLMTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.bpe(self,token)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm.tokenization_xlm.XLMTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm.tokenization_xlm.XLMTokenizer.do_lower_case(self)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.xlm.tokenization_xlm.XLMTokenizer.get_vocab(self)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.ja_tokenize(self,text)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_pipeline(self,text,lang)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_punct_norm(self,text,lang)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_tokenize(self,text,lang)
transformers.models.xlm.tokenization_xlm.XLMTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.xlm.tokenization_xlm.XLMTokenizer.vocab_size(self)
transformers.models.xlm.tokenization_xlm.get_pairs(word)
transformers.models.xlm.tokenization_xlm.lowercase_and_remove_accent(text)
transformers.models.xlm.tokenization_xlm.remove_non_printing_char(text)
transformers.models.xlm.tokenization_xlm.replace_unicode_punct(text)
transformers.models.xlm.tokenization_xlm.romanian_preprocessing(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch.chkpt->torch.load(xlm_checkpoint_path, map_location='cpu')
A:transformers.models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch.config->dict(((n, v) for (n, v) in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray))))
A:transformers.models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch.vocab->dict(((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for (s, i) in vocab.items()))
A:transformers.models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.xlm.convert_xlm_original_pytorch_checkpoint_to_pytorch.convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm/modeling_xlm.py----------------------------------------
A:transformers.models.xlm.modeling_xlm.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlm.modeling_xlm.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.xlm.modeling_xlm.out[:, 0::2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.models.xlm.modeling_xlm.out[:, 1::2]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.models.xlm.modeling_xlm.alen->torch.arange(slen, dtype=torch.long, device=lengths.device)
A:transformers.models.xlm.modeling_xlm.bs->torch.tensor([slen] * bs, device=device).size(0)
A:transformers.models.xlm.modeling_xlm.NEW_ID->itertools.count()
A:transformers.models.xlm.modeling_xlm.self.layer_id->next(MultiHeadAttention.NEW_ID)
A:transformers.models.xlm.modeling_xlm.self.q_lin->prune_linear_layer(self.q_lin, index)
A:transformers.models.xlm.modeling_xlm.self.k_lin->prune_linear_layer(self.k_lin, index)
A:transformers.models.xlm.modeling_xlm.self.v_lin->prune_linear_layer(self.v_lin, index)
A:transformers.models.xlm.modeling_xlm.self.out_lin->prune_linear_layer(self.out_lin, index, dim=1)
A:transformers.models.xlm.modeling_xlm.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.xlm.modeling_xlm.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)
A:transformers.models.xlm.modeling_xlm.(bs, qlen, dim)->input.size()
A:transformers.models.xlm.modeling_xlm.klen->kv.size(1)
A:transformers.models.xlm.modeling_xlm.q->shape(self.q_lin(input))
A:transformers.models.xlm.modeling_xlm.k->torch.cat([k_, k], dim=2)
A:transformers.models.xlm.modeling_xlm.v->torch.cat([v_, v], dim=2)
A:transformers.models.xlm.modeling_xlm.scores->self.proj.log_prob(x)
A:transformers.models.xlm.modeling_xlm.mask->(mask == 0).view(mask_reshape).expand_as(scores)
A:transformers.models.xlm.modeling_xlm.weights->torch.nn.functional.dropout(weights, p=self.dropout, training=self.training)
A:transformers.models.xlm.modeling_xlm.context->unshape(context)
A:transformers.models.xlm.modeling_xlm.self.lin1->torch.nn.Linear(in_dim, dim_hidden)
A:transformers.models.xlm.modeling_xlm.self.lin2->torch.nn.Linear(dim_hidden, out_dim)
A:transformers.models.xlm.modeling_xlm.x->torch.nn.functional.dropout(x, p=self.dropout, training=self.training)
A:transformers.models.xlm.modeling_xlm.inputs_list->torch.tensor([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.models.xlm.modeling_xlm.attns_list->torch.tensor([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.models.xlm.modeling_xlm.langs_list->torch.tensor([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.models.xlm.modeling_xlm.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, self.dim)
A:transformers.models.xlm.modeling_xlm.self.lang_embeddings->torch.nn.Embedding(self.n_langs, self.dim)
A:transformers.models.xlm.modeling_xlm.self.embeddings->torch.nn.Embedding(self.n_words, self.dim, padding_idx=self.pad_index)
A:transformers.models.xlm.modeling_xlm.self.layer_norm_emb->torch.nn.LayerNorm(self.dim, eps=config.layer_norm_eps)
A:transformers.models.xlm.modeling_xlm.self.attentions->torch.nn.ModuleList()
A:transformers.models.xlm.modeling_xlm.self.layer_norm1->torch.nn.ModuleList()
A:transformers.models.xlm.modeling_xlm.self.ffns->torch.nn.ModuleList()
A:transformers.models.xlm.modeling_xlm.self.layer_norm2->torch.nn.ModuleList()
A:transformers.models.xlm.modeling_xlm.pruned_heads->config.pruned_heads.copy().items()
A:transformers.models.xlm.modeling_xlm.(bs, slen)->torch.cat([input_ids, mask_token], dim=1).size()
A:transformers.models.xlm.modeling_xlm.lengths->torch.tensor([slen] * bs, device=device)
A:transformers.models.xlm.modeling_xlm.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.models.xlm.modeling_xlm.head_mask->self.get_head_mask(head_mask, self.config.n_layers)
A:transformers.models.xlm.modeling_xlm.inputs_embeds->self.embeddings(input_ids)
A:transformers.models.xlm.modeling_xlm.tensor->self.layer_norm2[i](tensor)
A:transformers.models.xlm.modeling_xlm.attn_outputs->self.attentions[i](tensor, attn_mask, cache=cache, head_mask=head_mask[i], output_attentions=output_attentions)
A:transformers.models.xlm.modeling_xlm.attn->torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)
A:transformers.models.xlm.modeling_xlm.self.proj->torch.nn.AdaptiveLogSoftmaxWithLoss(in_features=dim, n_classes=config.n_words, cutoffs=config.asm_cutoffs, div_value=config.asm_div_value, head_bias=True)
A:transformers.models.xlm.modeling_xlm.loss->loss_fct(reshaped_logits, labels)
A:transformers.models.xlm.modeling_xlm.(_, loss)->self.proj(x, y)
A:transformers.models.xlm.modeling_xlm.self.transformer->XLMModel(config)
A:transformers.models.xlm.modeling_xlm.self.pred_layer->XLMPredLayer(config)
A:transformers.models.xlm.modeling_xlm.mask_token->torch.full((effective_batch_size, 1), mask_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.models.xlm.modeling_xlm.input_ids->torch.cat([input_ids, mask_token], dim=1)
A:transformers.models.xlm.modeling_xlm.langs->torch.full_like(input_ids, lang_id)
A:transformers.models.xlm.modeling_xlm.transformer_outputs->self.transformer(input_ids=input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.xlm.modeling_xlm.outputs->self.transformer(input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.xlm.modeling_xlm.self.sequence_summary->SequenceSummary(config)
A:transformers.models.xlm.modeling_xlm.logits->self.logits_proj(logits)
A:transformers.models.xlm.modeling_xlm.loss_fct->CrossEntropyLoss()
A:transformers.models.xlm.modeling_xlm.self.qa_outputs->SQuADHead(config)
A:transformers.models.xlm.modeling_xlm.(start_logits, end_logits)->self.logits_proj(logits).split(1, dim=-1)
A:transformers.models.xlm.modeling_xlm.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.xlm.modeling_xlm.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.xlm.modeling_xlm.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.xlm.modeling_xlm.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.xlm.modeling_xlm.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.xlm.modeling_xlm.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.xlm.modeling_xlm.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.xlm.modeling_xlm.self.dropout->torch.nn.Dropout(config.dropout)
A:transformers.models.xlm.modeling_xlm.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.xlm.modeling_xlm.sequence_output->self.dropout(sequence_output)
A:transformers.models.xlm.modeling_xlm.active_logits->self.logits_proj(logits).view(-1, self.num_labels)
A:transformers.models.xlm.modeling_xlm.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.xlm.modeling_xlm.self.logits_proj->torch.nn.Linear(config.num_labels, 1)
A:transformers.models.xlm.modeling_xlm.reshaped_logits->self.logits_proj(logits).view(-1, num_choices)
transformers.XLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.XLMForMultipleChoice.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForQuestionAnswering(self,config)
transformers.XLMForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForQuestionAnsweringOutput(ModelOutput)
transformers.XLMForQuestionAnsweringSimple(self,config)
transformers.XLMForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForSequenceClassification(self,config)
transformers.XLMForSequenceClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForTokenClassification(self,config)
transformers.XLMForTokenClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMModel(self,config)
transformers.XLMModel._prune_heads(self,heads_to_prune)
transformers.XLMModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMModel.get_input_embeddings(self)
transformers.XLMModel.set_input_embeddings(self,new_embeddings)
transformers.XLMPreTrainedModel(self,*inputs,**kwargs)
transformers.XLMPreTrainedModel._init_weights(self,module)
transformers.XLMPreTrainedModel.dummy_inputs(self)
transformers.XLMWithLMHeadModel(self,config)
transformers.XLMWithLMHeadModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMWithLMHeadModel.get_output_embeddings(self)
transformers.XLMWithLMHeadModel.prepare_inputs_for_generation(self,input_ids,**kwargs)
transformers.XLMWithLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.xlm.modeling_xlm.MultiHeadAttention(self,n_heads,dim,config)
transformers.models.xlm.modeling_xlm.MultiHeadAttention.__init__(self,n_heads,dim,config)
transformers.models.xlm.modeling_xlm.MultiHeadAttention.forward(self,input,mask,kv=None,cache=None,head_mask=None,output_attentions=False)
transformers.models.xlm.modeling_xlm.MultiHeadAttention.prune_heads(self,heads)
transformers.models.xlm.modeling_xlm.TransformerFFN(self,in_dim,dim_hidden,out_dim,config)
transformers.models.xlm.modeling_xlm.TransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config)
transformers.models.xlm.modeling_xlm.TransformerFFN.ff_chunk(self,input)
transformers.models.xlm.modeling_xlm.TransformerFFN.forward(self,input)
transformers.models.xlm.modeling_xlm.XLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_xlm.XLMForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.xlm.modeling_xlm.XLMForMultipleChoice.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnswering(self,config)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnswering.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnsweringOutput(ModelOutput)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnsweringSimple(self,config)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnsweringSimple.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMForSequenceClassification(self,config)
transformers.models.xlm.modeling_xlm.XLMForSequenceClassification.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMForSequenceClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMForTokenClassification(self,config)
transformers.models.xlm.modeling_xlm.XLMForTokenClassification.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMForTokenClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMModel(self,config)
transformers.models.xlm.modeling_xlm.XLMModel.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMModel._prune_heads(self,heads_to_prune)
transformers.models.xlm.modeling_xlm.XLMModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMModel.get_input_embeddings(self)
transformers.models.xlm.modeling_xlm.XLMModel.set_input_embeddings(self,new_embeddings)
transformers.models.xlm.modeling_xlm.XLMPreTrainedModel(self,*inputs,**kwargs)
transformers.models.xlm.modeling_xlm.XLMPreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.models.xlm.modeling_xlm.XLMPreTrainedModel._init_weights(self,module)
transformers.models.xlm.modeling_xlm.XLMPreTrainedModel.dummy_inputs(self)
transformers.models.xlm.modeling_xlm.XLMPredLayer(self,config)
transformers.models.xlm.modeling_xlm.XLMPredLayer.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMPredLayer.forward(self,x,y=None)
transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel(self,config)
transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel.__init__(self,config)
transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel.get_output_embeddings(self)
transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel.prepare_inputs_for_generation(self,input_ids,**kwargs)
transformers.models.xlm.modeling_xlm.XLMWithLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.xlm.modeling_xlm.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.models.xlm.modeling_xlm.get_masks(slen,lengths,causal,padding_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm/__init__.py----------------------------------------
A:transformers.models.xlm.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/retribert/modeling_retribert.py----------------------------------------
A:transformers.models.retribert.modeling_retribert.logger->utils.logging.get_logger(__name__)
A:transformers.models.retribert.modeling_retribert.self.bert_query->BertModel(config)
A:transformers.models.retribert.modeling_retribert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.retribert.modeling_retribert.self.project_query->torch.nn.Linear(config.hidden_size, config.projection_dim, bias=False)
A:transformers.models.retribert.modeling_retribert.self.project_doc->torch.nn.Linear(config.hidden_size, config.projection_dim, bias=False)
A:transformers.models.retribert.modeling_retribert.self.ce_loss->torch.nn.CrossEntropyLoss(reduction='mean')
A:transformers.models.retribert.modeling_retribert.input_shape->input_ids.size()
A:transformers.models.retribert.modeling_retribert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.retribert.modeling_retribert.encoder_outputs->sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask)
A:transformers.models.retribert.modeling_retribert.pooled_output->torch.utils.checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)
A:transformers.models.retribert.modeling_retribert.embedding_output->sent_encoder.embeddings(input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None)
A:transformers.models.retribert.modeling_retribert.q_reps->self.embed_questions(input_ids_query, attention_mask_query, checkpoint_batch_size)
A:transformers.models.retribert.modeling_retribert.a_reps->self.embed_answers(input_ids_doc, attention_mask_doc, checkpoint_batch_size)
A:transformers.models.retribert.modeling_retribert.compare_scores->torch.mm(q_reps, a_reps.t())
A:transformers.models.retribert.modeling_retribert.loss_qa->self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))
A:transformers.models.retribert.modeling_retribert.loss_aq->self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))
transformers.RetriBertModel(self,config)
transformers.RetriBertModel.embed_answers(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.RetriBertModel.embed_questions(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.RetriBertModel.embed_sentences_checkpointed(self,input_ids,attention_mask,sent_encoder,checkpoint_batch_size=-1)
transformers.RetriBertModel.forward(self,input_ids_query,attention_mask_query,input_ids_doc,attention_mask_doc,checkpoint_batch_size=-1)
transformers.RetriBertPreTrainedModel(PreTrainedModel)
transformers.RetriBertPreTrainedModel._init_weights(self,module)
transformers.models.retribert.modeling_retribert.RetriBertModel(self,config)
transformers.models.retribert.modeling_retribert.RetriBertModel.__init__(self,config)
transformers.models.retribert.modeling_retribert.RetriBertModel.embed_answers(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.models.retribert.modeling_retribert.RetriBertModel.embed_questions(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.models.retribert.modeling_retribert.RetriBertModel.embed_sentences_checkpointed(self,input_ids,attention_mask,sent_encoder,checkpoint_batch_size=-1)
transformers.models.retribert.modeling_retribert.RetriBertModel.forward(self,input_ids_query,attention_mask_query,input_ids_doc,attention_mask_doc,checkpoint_batch_size=-1)
transformers.models.retribert.modeling_retribert.RetriBertPreTrainedModel(PreTrainedModel)
transformers.models.retribert.modeling_retribert.RetriBertPreTrainedModel._init_weights(self,module)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/retribert/tokenization_retribert.py----------------------------------------
A:transformers.models.retribert.tokenization_retribert.logger->utils.logging.get_logger(__name__)
transformers.RetriBertTokenizer(BertTokenizer)
transformers.models.retribert.tokenization_retribert.RetriBertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/retribert/configuration_retribert.py----------------------------------------
A:transformers.models.retribert.configuration_retribert.logger->utils.logging.get_logger(__name__)
transformers.RetriBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=8,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,share_encoders=True,projection_dim=128,pad_token_id=0,**kwargs)
transformers.models.retribert.configuration_retribert.RetriBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=8,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,share_encoders=True,projection_dim=128,pad_token_id=0,**kwargs)
transformers.models.retribert.configuration_retribert.RetriBertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=8,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,share_encoders=True,projection_dim=128,pad_token_id=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/retribert/__init__.py----------------------------------------
A:transformers.models.retribert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/retribert/tokenization_retribert_fast.py----------------------------------------
A:transformers.models.retribert.tokenization_retribert_fast.logger->utils.logging.get_logger(__name__)
transformers.RetriBertTokenizerFast(BertTokenizerFast)
transformers.models.retribert.tokenization_retribert_fast.RetriBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot_small/modeling_blenderbot_small.py----------------------------------------
A:transformers.models.blenderbot_small.modeling_blenderbot_small.logger->utils.logging.get_logger(__name__)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.blenderbot_small.modeling_blenderbot_small.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.blenderbot_small.modeling_blenderbot_small.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.blenderbot_small.modeling_blenderbot_small.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.(bsz, tgt_len, embed_dim)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.blenderbot_small.modeling_blenderbot_small.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.attn_output->self.out_proj(attn_output)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.self_attn->BlenderbotSmallAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.hidden_states->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.encoder_attn->BlenderbotSmallAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.embed_positions->BlenderbotSmallLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.layers->torch.nn.ModuleList([BlenderbotSmallDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.layernorm_embedding->torch.nn.LayerNorm(config.d_model)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.blenderbot_small.modeling_blenderbot_small.embed_pos->self.embed_positions(input_shape)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.dropout_probability->random.uniform(0, 1)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.blenderbot_small.modeling_blenderbot_small.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.blenderbot_small.modeling_blenderbot_small.inputs_embeds->self.layernorm_embedding(inputs_embeds)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.encoder->BlenderbotSmallEncoder(config, self.shared)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.decoder->BlenderbotSmallDecoder(config)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.model->BlenderbotSmallDecoderWrapper(config)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.loss_fct->CrossEntropyLoss()
A:transformers.models.blenderbot_small.modeling_blenderbot_small.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.blenderbot_small.modeling_blenderbot_small.config->copy.deepcopy(config)
A:transformers.models.blenderbot_small.modeling_blenderbot_small.logits->self.lm_head(outputs[0])
A:transformers.models.blenderbot_small.modeling_blenderbot_small.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
transformers.BlenderbotSmallForCausalLM(self,config)
transformers.BlenderbotSmallForCausalLM._reorder_cache(past,beam_idx)
transformers.BlenderbotSmallForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BlenderbotSmallForCausalLM.get_decoder(self)
transformers.BlenderbotSmallForCausalLM.get_input_embeddings(self)
transformers.BlenderbotSmallForCausalLM.get_output_embeddings(self)
transformers.BlenderbotSmallForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.BlenderbotSmallForCausalLM.set_decoder(self,decoder)
transformers.BlenderbotSmallForCausalLM.set_input_embeddings(self,value)
transformers.BlenderbotSmallForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.BlenderbotSmallForConditionalGeneration(self,config:BlenderbotSmallConfig)
transformers.BlenderbotSmallForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.BlenderbotSmallForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.BlenderbotSmallForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BlenderbotSmallForConditionalGeneration.get_decoder(self)
transformers.BlenderbotSmallForConditionalGeneration.get_encoder(self)
transformers.BlenderbotSmallForConditionalGeneration.get_output_embeddings(self)
transformers.BlenderbotSmallForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.BlenderbotSmallForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.BlenderbotSmallForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.BlenderbotSmallModel(self,config:BlenderbotSmallConfig)
transformers.BlenderbotSmallModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BlenderbotSmallModel.get_decoder(self)
transformers.BlenderbotSmallModel.get_encoder(self)
transformers.BlenderbotSmallModel.get_input_embeddings(self)
transformers.BlenderbotSmallModel.set_input_embeddings(self,value)
transformers.BlenderbotSmallPreTrainedModel(PreTrainedModel)
transformers.BlenderbotSmallPreTrainedModel._init_weights(self,module)
transformers.BlenderbotSmallPreTrainedModel.dummy_inputs(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoder(self,config:BlenderbotSmallConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoder.__init__(self,config:BlenderbotSmallConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoder.get_input_embeddings(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoder.set_input_embeddings(self,value)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoderLayer(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoderLayer.__init__(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoderWrapper(self,config)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoderWrapper.__init__(self,config)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallEncoder(self,config:BlenderbotSmallConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallEncoder.__init__(self,config:BlenderbotSmallConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallEncoderLayer(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallEncoderLayer.__init__(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM(self,config)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.__init__(self,config)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM._reorder_cache(past,beam_idx)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_decoder(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_input_embeddings(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_output_embeddings(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.set_decoder(self,decoder)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.set_input_embeddings(self,value)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.__init__(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_decoder(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_encoder(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_output_embeddings(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel.__init__(self,config:BlenderbotSmallConfig)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel.get_decoder(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel.get_encoder(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel.get_input_embeddings(self)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel.set_input_embeddings(self,value)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallPreTrainedModel(PreTrainedModel)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallPreTrainedModel._init_weights(self,module)
transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallPreTrainedModel.dummy_inputs(self)
transformers.models.blenderbot_small.modeling_blenderbot_small._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.blenderbot_small.modeling_blenderbot_small._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.blenderbot_small.modeling_blenderbot_small.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot_small/tokenization_blenderbot_small_fast.py----------------------------------------
A:transformers.models.blenderbot_small.tokenization_blenderbot_small_fast.logger->utils.logging.get_logger(__name__)
transformers.models.blenderbot_small.tokenization_blenderbot_small_fast.BlenderbotSmallTokenizerFast(self,vocab_file,merges_file,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.models.blenderbot_small.tokenization_blenderbot_small_fast.BlenderbotSmallTokenizerFast.__init__(self,vocab_file,merges_file,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.models.blenderbot_small.tokenization_blenderbot_small_fast.BlenderbotSmallTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.models.blenderbot_small.tokenization_blenderbot_small_fast.BlenderbotSmallTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot_small/configuration_blenderbot_small.py----------------------------------------
A:transformers.models.blenderbot_small.configuration_blenderbot_small.logger->utils.logging.get_logger(__name__)
transformers.BlenderbotSmallConfig(self,vocab_size=50265,max_position_embeddings=512,encoder_layers=8,encoder_ffn_dim=2048,encoder_attention_heads=16,decoder_layers=8,decoder_ffn_dim=2048,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=512,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=1,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.BlenderbotSmallConfig.hidden_size(self)->int
transformers.BlenderbotSmallConfig.num_attention_heads(self)->int
transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig(self,vocab_size=50265,max_position_embeddings=512,encoder_layers=8,encoder_ffn_dim=2048,encoder_attention_heads=16,decoder_layers=8,decoder_ffn_dim=2048,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=512,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=1,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig.__init__(self,vocab_size=50265,max_position_embeddings=512,encoder_layers=8,encoder_ffn_dim=2048,encoder_attention_heads=16,decoder_layers=8,decoder_ffn_dim=2048,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=512,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=1,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig.hidden_size(self)->int
transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot_small/modeling_tf_blenderbot_small.py----------------------------------------
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.logger->utils.logging.get_logger(__name__)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.one_cst->tensorflow.constant(1.0)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.attention_mask->_expand_mask(inputs['attention_mask'])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.attn_output->self.out_proj(attn_output)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.self_attn->TFBlenderbotSmallAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.(hidden_states, self_attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.hidden_states->self.dropout(hidden_states, training=inputs['training'])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.encoder_attn->TFBlenderbotSmallAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.decoder_input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.output->self.call(inputs)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.embed_positions->TFBlenderbotSmallLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.layernorm_embedding->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layernorm_embedding')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.embed_pos->self.embed_positions(input_shape)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.dropout_probability->random.uniform(0, 1)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.(hidden_states, attn)->encoder_layer(hidden_states, attention_mask, inputs['head_mask'][idx] if inputs['head_mask'] is not None else None)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.(hidden_states, layer_self_attn, layer_cross_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, cross_attn_layer_head_mask=inputs['cross_attn_head_mask'][idx] if inputs['cross_attn_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.all_self_attns->list(all_self_attns)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.all_cross_attns->list(all_cross_attns)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='model.shared')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.encoder->TFBlenderbotSmallEncoder(config, embed_tokens, name='encoder')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.decoder->TFBlenderbotSmallDecoder(config, embed_tokens, name='decoder')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.model->TFBlenderbotSmallMainLayer(config, name='model')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.outputs->self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], encoder_outputs=inputs['encoder_outputs'], decoder_attention_mask=inputs['decoder_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.inputs['labels']->tensorflow.where(inputs['labels'] == self.config.pad_token_id, tf.fill(shape_list(inputs['labels']), -100), inputs['labels'])
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.lm_logits->self.model.shared(outputs[0], mode='linear')
A:transformers.models.blenderbot_small.modeling_tf_blenderbot_small.encoder_outputs->TFBaseModelOutput(last_hidden_state=encoder_outputs)
transformers.TFBlenderbotSmallForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFBlenderbotSmallForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.TFBlenderbotSmallForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFBlenderbotSmallForConditionalGeneration.get_bias(self)
transformers.TFBlenderbotSmallForConditionalGeneration.get_decoder(self)
transformers.TFBlenderbotSmallForConditionalGeneration.get_encoder(self)
transformers.TFBlenderbotSmallForConditionalGeneration.get_output_embeddings(self)
transformers.TFBlenderbotSmallForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFBlenderbotSmallForConditionalGeneration.serving_output(self,output)
transformers.TFBlenderbotSmallForConditionalGeneration.set_bias(self,value)
transformers.TFBlenderbotSmallForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFBlenderbotSmallModel(self,config:BlenderbotSmallConfig,*inputs,**kwargs)
transformers.TFBlenderbotSmallModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFBlenderbotSmallModel.get_decoder(self)
transformers.TFBlenderbotSmallModel.get_encoder(self)
transformers.TFBlenderbotSmallModel.serving_output(self,output)
transformers.TFBlenderbotSmallPreTrainedModel(TFPreTrainedModel)
transformers.TFBlenderbotSmallPreTrainedModel.dummy_inputs(self)
transformers.TFBlenderbotSmallPreTrainedModel.serving(self,inputs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoder(self,config:BlenderbotSmallConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoder.__init__(self,config:BlenderbotSmallConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoder.get_embed_tokens(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoderLayer(self,config:BlenderbotSmallConfig,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoderLayer.__init__(self,config:BlenderbotSmallConfig,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,cross_attn_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoder(self,config:BlenderbotSmallConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoder.__init__(self,config:BlenderbotSmallConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoder.get_embed_tokens(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoderLayer(self,config:BlenderbotSmallConfig,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoderLayer.__init__(self,config:BlenderbotSmallConfig,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,training=False)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.get_bias(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.get_decoder(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.get_encoder(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.get_output_embeddings(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.serving_output(self,output)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.set_bias(self,value)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallLearnedPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallMainLayer(self,config:BlenderbotSmallConfig,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallMainLayer.__init__(self,config:BlenderbotSmallConfig,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallMainLayer.get_input_embeddings(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallModel(self,config:BlenderbotSmallConfig,*inputs,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallModel.__init__(self,config:BlenderbotSmallConfig,*inputs,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallModel.get_decoder(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallModel.get_encoder(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallModel.serving_output(self,output)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallPreTrainedModel(TFPreTrainedModel)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallPreTrainedModel.dummy_inputs(self)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.TFBlenderbotSmallPreTrainedModel.serving(self,inputs)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.blenderbot_small.modeling_tf_blenderbot_small.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot_small/tokenization_blenderbot_small.py----------------------------------------
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.logger->utils.logging.get_logger(__name__)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.pairs->get_pairs(word)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.self.encoder->json.load(vocab_handle)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.token->token.lower().lower()
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.tokens->token.lower().lower().split(' ')
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.word->'@@ '.join(word)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.j->'@@ '.join(word).index(first, i)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.new_word->tuple(new_word)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.words->regex.findall('\\S+\\n?', text)
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.out_string->' '.join(tokens).replace('@@ ', '').strip()
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.blenderbot_small.tokenization_blenderbot_small.merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
transformers.BlenderbotSmallTokenizer(self,vocab_file,merges_file,bos_token='__start__',eos_token='__end__',unk_token='__unk__',pad_token='__null__',**kwargs)
transformers.BlenderbotSmallTokenizer._convert_id_to_token(self,index:int)->str
transformers.BlenderbotSmallTokenizer._convert_token_to_id(self,token:str)->int
transformers.BlenderbotSmallTokenizer._tokenize(self,text:str)->List[str]
transformers.BlenderbotSmallTokenizer.bpe(self,token:str)->str
transformers.BlenderbotSmallTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.BlenderbotSmallTokenizer.get_vocab(self)->Dict
transformers.BlenderbotSmallTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.BlenderbotSmallTokenizer.vocab_size(self)->int
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer(self,vocab_file,merges_file,bos_token='__start__',eos_token='__end__',unk_token='__unk__',pad_token='__null__',**kwargs)
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer.__init__(self,vocab_file,merges_file,bos_token='__start__',eos_token='__end__',unk_token='__unk__',pad_token='__null__',**kwargs)
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer._convert_id_to_token(self,index:int)->str
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer._convert_token_to_id(self,token:str)->int
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer._tokenize(self,text:str)->List[str]
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer.bpe(self,token:str)->str
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer.get_vocab(self)->Dict
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.blenderbot_small.tokenization_blenderbot_small.BlenderbotSmallTokenizer.vocab_size(self)->int
transformers.models.blenderbot_small.tokenization_blenderbot_small.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/blenderbot_small/__init__.py----------------------------------------
A:transformers.models.blenderbot_small.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py----------------------------------------
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.k->k.replace(tf_name, hf_name).replace(tf_name, hf_name)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.cfg->BigBirdPegasusConfig(**config_update)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.torch_model->convert_bigbird_pegasus(tf_weights, config_update)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.state_dict->convert_bigbird_pegasus(tf_weights, config_update).state_dict()
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.new_k->rename_state_dict_key(k, patterns)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.mapping[new_k]->torch.from_numpy(v)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.mapping['model.decoder.embed_positions.weight']->mapping.pop('model.embed_positions.weight')
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.(missing, extra)->convert_bigbird_pegasus(tf_weights, config_update).load_state_dict(mapping, strict=False)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.init_vars->tensorflow.train.list_variables(path)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.skip_key->any([pat in name for pat in ignore_name])
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.array->tensorflow.train.load_variable(path, name)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.tf_weights->get_tf_weights_as_numpy(ckpt_path)
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.convert_bigbird_pegasus(tf_weights:dict,config_update:dict)->BigBirdPegasusForConditionalGeneration
transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.convert_bigbird_pegasus_ckpt_to_pytorch(ckpt_path:str,save_dir:str,config_update:dict)
transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.get_tf_weights_as_numpy(path)->Dict
transformers.models.bigbird_pegasus.convert_bigbird_pegasus_tf_to_pytorch.rename_state_dict_key(k,patterns)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bigbird_pegasus/modeling_bigbird_pegasus.py----------------------------------------
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.logger->utils.logging.get_logger(__name__)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.dropout->torch.nn.Dropout(p=pooler_dropout)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.mixed_query_layer->self.query(hidden_states)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.query_layer->self.transpose_for_scores(self.query(hidden_states))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attention_probs->torch.zeros(bsz, n_heads, from_seq_len, to_seq_len, dtype=torch.float, device=context_layer.device)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.context_layer->torch.transpose(context_layer, 1, 2)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(batch_size, seqlen, _)->self.layernorm_embedding(hidden_states).size()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(context_layer, attention_probs)->self.bigbird_block_sparse_attention(query_layer, key_layer, value_layer, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, self.num_attention_heads, self.num_random_blocks, self.attention_head_size, from_block_size, to_block_size, batch_size, from_seq_length, to_seq_length, seed=self.seed, plan_from_length=None, plan_num_rand_blocks=None, output_attentions=output_attentions)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(plan_from_length, plan_num_rand_blocks)->self._get_rand_attn_plan(from_seq_len, from_block_size, n_rand_blocks)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.rand_attn->numpy.zeros((from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.rand_mask->torch.einsum('blq,bhlk->bhlqk', from_blocked_mask[:, 1:-1], rand_mask)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.blocked_query_matrix->self.transpose_for_scores(self.query(hidden_states)).view(bsz, n_heads, from_seq_len // from_block_size, from_block_size, -1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.blocked_key_matrix->self.transpose_for_scores(self.key(hidden_states)).view(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.blocked_value_matrix->self.transpose_for_scores(self.value(hidden_states)).view(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.gathered_key->gathered_key.view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1).view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.gathered_value->gathered_value.view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1).view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.first_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, 0], key_layer, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.first_attn_weights->torch.nn.functional.softmax(first_product, dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.first_context_layer->self.torch_bmm_nd(first_attn_weights, value_layer, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_key_mat->torch.cat([blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, 1], blocked_key_matrix[:, :, 2], blocked_key_matrix[:, :, -1], gathered_key[:, :, 0]], dim=2)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_value_mat->torch.cat([blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, 1], blocked_value_matrix[:, :, 2], blocked_value_matrix[:, :, -1], gathered_value[:, :, 0]], dim=2)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, 1], second_key_mat, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_seq_pad->torch.cat([to_mask[:, :, :, :3 * to_block_size], to_mask[:, :, :, -to_block_size:], to_mask.new_ones([bsz, 1, 1, n_rand_blocks * to_block_size])], dim=3)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_rand_pad->torch.cat([rand_mask.new_ones([bsz, n_heads, from_block_size, 4 * to_block_size]), rand_mask[:, :, 0]], dim=3)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_attn_weights->torch.nn.functional.softmax(second_product, dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_context_layer->self.torch_bmm_nd(second_attn_weights, second_value_mat, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.exp_blocked_key_matrix->torch.cat([blocked_key_matrix[:, :, 1:-3], blocked_key_matrix[:, :, 2:-2], blocked_key_matrix[:, :, 3:-1]], dim=3)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.exp_blocked_value_matrix->torch.cat([blocked_value_matrix[:, :, 1:-3], blocked_value_matrix[:, :, 2:-2], blocked_value_matrix[:, :, 3:-1]], dim=3)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.inner_band_product->self.torch_bmm_nd_transpose(middle_query_matrix, exp_blocked_key_matrix, ndim=5)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.rand_band_product->self.torch_bmm_nd_transpose(middle_query_matrix, gathered_key[:, :, 1:-1], ndim=5)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.first_band_product->torch.einsum('bhlqd,bhkd->bhlqk', middle_query_matrix, blocked_key_matrix[:, :, 0])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.last_band_product->torch.einsum('bhlqd,bhkd->bhlqk', middle_query_matrix, blocked_key_matrix[:, :, -1])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.band_product->torch.cat([first_band_product, inner_band_product, rand_band_product, last_band_product], dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_key_mat->torch.cat([blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, -3], blocked_key_matrix[:, :, -2], blocked_key_matrix[:, :, -1], gathered_key[:, :, -1]], dim=2)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_value_mat->torch.cat([blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, -3], blocked_value_matrix[:, :, -2], blocked_value_matrix[:, :, -1], gathered_value[:, :, -1]], dim=2)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -2], second_last_key_mat, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_seq_pad->torch.cat([to_mask[:, :, :, :to_block_size], to_mask[:, :, :, -3 * to_block_size:], to_mask.new_ones([bsz, 1, 1, n_rand_blocks * to_block_size])], dim=3)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_rand_pad->torch.cat([rand_mask.new_ones([bsz, n_heads, from_block_size, 4 * to_block_size]), rand_mask[:, :, -1]], dim=3)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_attn_weights->torch.nn.functional.softmax(second_last_product, dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.second_last_context_layer->self.torch_bmm_nd(second_last_attn_weights, second_last_value_mat, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.last_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -1], key_layer, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.last_attn_weights->torch.nn.functional.softmax(last_product, dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.last_context_layer->self.torch_bmm_nd(last_attn_weights, value_layer, ndim=4)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_probs_view->torch.zeros(bsz, n_heads, from_seq_len, to_seq_len, dtype=torch.float, device=context_layer.device).view(bsz, n_heads, from_seq_len // from_block_size, from_block_size, to_seq_len // to_block_size, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_probs_view[p1, p2, 1, :, i2[0]]->right_slice.view(from_block_size, n_rand_blocks, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_probs_view[:, :, q_idx, :, q_idx:q_idx + 3, :]->right_slice.view(bsz, n_heads, from_block_size, 3, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attention_probs[:, :, 2 * from_block_size:-2 * from_block_size, :to_block_size]->attn_weights[:, :, :, :, :to_block_size].view(bsz, n_heads, -1, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attention_probs[:, :, 2 * from_block_size:-2 * from_block_size, -to_block_size:]->attn_weights[:, :, :, :, -to_block_size:].view(bsz, n_heads, -1, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_probs_view[p1, p2, q_idx + 1, :, i2[q_idx]]->right_slice.view(from_block_size, n_rand_blocks, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_probs_view[p1, p2, -2, :, i2[-1]]->right_slice.view(from_block_size, n_rand_blocks, to_block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.flattened_params->params.reshape(-1, params.shape[-2], params.shape[-1])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.out_flattened->params.reshape(-1, params.shape[-2], params.shape[-1]).index_select(0, flattened_indices)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.out->params.reshape(-1, params.shape[-2], params.shape[-1]).index_select(0, flattened_indices).reshape(params.shape[:2] + (num_indices_to_gather,) + params.shape[3:])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.middle_seq->numpy.arange(1, to_seq_length // to_block_size - 1, dtype=np.int32)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.max_plan_idx->plan_from_length.index(from_seq_length)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.rnd_r_cnt->int(np.sum(plan_num_rand_blocks[:plan_idx]))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.curr_r_cnt->int(np.sum(plan_num_rand_blocks[:plan_idx + 1]))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt]->self._get_single_block_row_attention(block_id=blk_rw_idx, to_start_block_id=to_start_block_id, to_end_block_id=plan_block_length[plan_idx], num_rand_blocks=plan_num_rand_blocks[plan_idx], window_block_left=window_block_left, window_block_right=window_block_right, global_block_left=global_block_left, global_block_right=global_block_right)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.to_block_list->numpy.arange(to_start_block_id, to_end_block_id, dtype=np.int32)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.perm_block->numpy.random.permutation(to_block_list)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.illegal_blocks->list(range(block_id - window_block_left, block_id + window_block_right + 1))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.self->BigBirdPegasusBlockSparseAttention(config, seed)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.output->torch.nn.Linear(config.hidden_size, config.hidden_size, bias=config.use_bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self_outputs->self.self(hidden_states, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attention_output->self.output(self_outputs[0])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.out_proj->torch.nn.Linear(inner_dim, num_classes)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(bsz, tgt_len, embed_dim)->self.layernorm_embedding(hidden_states).size()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attn_output->self.out_proj(attn_output)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.self_attn->BigBirdPegasusDecoderAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True, bias=config.use_bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.hidden_states->self.layernorm_embedding(hidden_states)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self_attention_outputs->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, head_mask=layer_head_mask, output_attentions=output_attentions, band_mask=band_mask, from_mask=from_mask, to_mask=to_mask, from_blocked_mask=from_blocked_mask, to_blocked_mask=to_blocked_mask)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.encoder_attn->BigBirdPegasusDecoderAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True, bias=config.use_bias)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.dense->torch.nn.Linear(input_dim, inner_dim)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.embed_positions->BigBirdPegasusLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.layers->torch.nn.ModuleList([BigBirdPegasusDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.layernorm_embedding->torch.nn.LayerNorm(config.d_model)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.embed_pos->self.embed_positions(input_shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(padding_len, hidden_states, attention_mask)->self._pad_to_block_size(hidden_states, attention_mask)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(blocked_encoder_mask, band_mask, from_mask, to_mask)->self.create_masks_for_block_sparse_attn(attention_mask, self.block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.dropout_probability->random.uniform(0, 1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(batch_size, seq_length)->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape).size()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.exp_blocked_to_pad->torch.cat([to_blocked_mask[:, 1:-3], to_blocked_mask[:, 2:-2], to_blocked_mask[:, 3:-1]], dim=2)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.band_mask->create_band_mask_from_inputs(blocked_encoder_mask, blocked_encoder_mask)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.blocked_encoder_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape).view(batch_size, seq_length // block_size, block_size)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.from_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape).view(batch_size, 1, seq_length, 1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.to_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape).view(batch_size, 1, 1, seq_length)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.inputs_embeds_padding->self.embed_tokens(input_ids_padding)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.encoder->BigBirdPegasusEncoder(config, self.shared)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.decoder->BigBirdPegasusDecoder(config)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.model->BigBirdPegasusDecoderWrapper(config)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.loss_fct->CrossEntropyLoss()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.classification_head->BigBirdPegasusClassificationHead(config.d_model, config.d_model, config.num_labels, config.classifier_dropout)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.eos_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).eq(self.config.eos_token_id)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.logits->self.lm_head(outputs[0])
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.(start_logits, end_logits)->self.lm_head(outputs[0]).split(1, dim=-1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.config->copy.deepcopy(config)
transformers.BigBirdPegasusForCausalLM(self,config)
transformers.BigBirdPegasusForCausalLM._reorder_cache(past,beam_idx)
transformers.BigBirdPegasusForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdPegasusForCausalLM.get_decoder(self)
transformers.BigBirdPegasusForCausalLM.get_input_embeddings(self)
transformers.BigBirdPegasusForCausalLM.get_output_embeddings(self)
transformers.BigBirdPegasusForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.BigBirdPegasusForCausalLM.set_decoder(self,decoder)
transformers.BigBirdPegasusForCausalLM.set_input_embeddings(self,value)
transformers.BigBirdPegasusForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.BigBirdPegasusForConditionalGeneration(self,config:BigBirdPegasusConfig)
transformers.BigBirdPegasusForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.BigBirdPegasusForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.BigBirdPegasusForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdPegasusForConditionalGeneration.get_decoder(self)
transformers.BigBirdPegasusForConditionalGeneration.get_encoder(self)
transformers.BigBirdPegasusForConditionalGeneration.get_output_embeddings(self)
transformers.BigBirdPegasusForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.BigBirdPegasusForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.BigBirdPegasusForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.BigBirdPegasusForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.BigBirdPegasusForQuestionAnswering(self,config)
transformers.BigBirdPegasusForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdPegasusForSequenceClassification(self,config:BigBirdPegasusConfig,**kwargs)
transformers.BigBirdPegasusForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdPegasusModel(self,config:BigBirdPegasusConfig)
transformers.BigBirdPegasusModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdPegasusModel.get_decoder(self)
transformers.BigBirdPegasusModel.get_encoder(self)
transformers.BigBirdPegasusModel.get_input_embeddings(self)
transformers.BigBirdPegasusModel.set_input_embeddings(self,value)
transformers.BigBirdPegasusPreTrainedModel(PreTrainedModel)
transformers.BigBirdPegasusPreTrainedModel._init_weights(self,module)
transformers.BigBirdPegasusPreTrainedModel.dummy_inputs(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention(self,config,seed=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.__init__(self,config,seed=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention._bigbird_block_rand_mask(from_seq_length,to_seq_length,from_block_size,to_block_size,num_rand_blocks,last_idx=-1)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention._bigbird_block_rand_mask_with_head(self,from_seq_length,to_seq_length,from_block_size,to_block_size,num_heads,plan_from_length,plan_num_rand_blocks,window_block_left=1,window_block_right=1,global_block_top=1,global_block_bottom=1,global_block_left=1,global_block_right=1)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention._create_rand_mask_from_inputs(from_blocked_mask,to_blocked_mask,rand_attn,num_attention_heads,num_rand_blocks,batch_size,from_seq_length,from_block_size)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention._get_rand_attn_plan(from_seq_length,from_block_size,num_rand_blocks)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention._get_single_block_row_attention(block_id,to_start_block_id,to_end_block_id,num_rand_blocks,window_block_left=1,window_block_right=1,global_block_left=1,global_block_right=1)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.bigbird_block_sparse_attention(self,query_layer,key_layer,value_layer,band_mask,from_mask,to_mask,from_blocked_mask,to_blocked_mask,n_heads,n_rand_blocks,attention_head_size,from_block_size,to_block_size,batch_size,from_seq_len,to_seq_len,seed,plan_from_length,plan_num_rand_blocks,output_attentions)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.forward(self,hidden_states,band_mask=None,from_mask=None,to_mask=None,from_blocked_mask=None,to_blocked_mask=None,output_attentions=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.torch_bmm_nd(inp_1,inp_2,ndim=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.torch_bmm_nd_transpose(inp_1,inp_2,ndim=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.torch_gather_b2(params,indices)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusBlockSparseAttention.transpose_for_scores(self,x)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusClassificationHead(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusClassificationHead.__init__(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusClassificationHead.forward(self,hidden_states:torch.Tensor)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoder(self,config:BigBirdPegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoder.__init__(self,config:BigBirdPegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoder.get_input_embeddings(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoder.set_input_embeddings(self,value)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderLayer(self,config:BigBirdPegasusConfig)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderLayer.__init__(self,config:BigBirdPegasusConfig)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderWrapper(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderWrapper.__init__(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoder(self,config:BigBirdPegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoder.__init__(self,config:BigBirdPegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoder._pad_to_block_size(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoder.create_masks_for_block_sparse_attn(attention_mask:torch.Tensor,block_size:int)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoder.set_attention_type(self,value:str)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderAttention(self,config,seed=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderAttention.__init__(self,config,seed=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,past_key_value=None,output_attentions=False,band_mask=None,from_mask=None,to_mask=None,from_blocked_mask=None,to_blocked_mask=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderAttention.set_attention_type(self,value:str)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderLayer(self,config:BigBirdPegasusConfig,seed=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderLayer.__init__(self,config:BigBirdPegasusConfig,seed=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,band_mask=None,from_mask=None,to_mask=None,from_blocked_mask=None,to_blocked_mask=None,output_attentions:bool=False)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusEncoderLayer.set_attention_type(self,value:str)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.__init__(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM._reorder_cache(past,beam_idx)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.get_decoder(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.get_input_embeddings(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.get_output_embeddings(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.set_decoder(self,decoder)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.set_input_embeddings(self,value)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration(self,config:BigBirdPegasusConfig)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.__init__(self,config:BigBirdPegasusConfig)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.get_decoder(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.get_encoder(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.get_output_embeddings(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForQuestionAnswering(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForQuestionAnswering.__init__(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForSequenceClassification(self,config:BigBirdPegasusConfig,**kwargs)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForSequenceClassification.__init__(self,config:BigBirdPegasusConfig,**kwargs)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusLearnedPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel(self,config:BigBirdPegasusConfig)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel.__init__(self,config:BigBirdPegasusConfig)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel.get_decoder(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel.get_encoder(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel.get_input_embeddings(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusModel.set_input_embeddings(self,value)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusPreTrainedModel(PreTrainedModel)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusPreTrainedModel._init_weights(self,module)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusPreTrainedModel.dummy_inputs(self)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusSelfAttention(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusSelfAttention.__init__(self,config)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.BigBirdPegasusSelfAttention.transpose_for_scores(self,x)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.bigbird_pegasus.modeling_bigbird_pegasus.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bigbird_pegasus/configuration_bigbird_pegasus.py----------------------------------------
A:transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.logger->utils.logging.get_logger(__name__)
transformers.BigBirdPegasusConfig(self,vocab_size=96103,max_position_embeddings=4096,encoder_layers=16,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=16,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu_new',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,scale_embedding=True,gradient_checkpointing=False,pad_token_id=0,bos_token_id=2,eos_token_id=1,attention_type='block_sparse',block_size=64,num_random_blocks=3,use_bias=False,**kwargs)
transformers.BigBirdPegasusConfig.attention_probs_dropout_prob(self)->float
transformers.BigBirdPegasusConfig.hidden_size(self)->int
transformers.BigBirdPegasusConfig.num_attention_heads(self)->int
transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig(self,vocab_size=96103,max_position_embeddings=4096,encoder_layers=16,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=16,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu_new',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,scale_embedding=True,gradient_checkpointing=False,pad_token_id=0,bos_token_id=2,eos_token_id=1,attention_type='block_sparse',block_size=64,num_random_blocks=3,use_bias=False,**kwargs)
transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig.__init__(self,vocab_size=96103,max_position_embeddings=4096,encoder_layers=16,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=16,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu_new',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,scale_embedding=True,gradient_checkpointing=False,pad_token_id=0,bos_token_id=2,eos_token_id=1,attention_type='block_sparse',block_size=64,num_random_blocks=3,use_bias=False,**kwargs)
transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig.attention_probs_dropout_prob(self)->float
transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig.hidden_size(self)->int
transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bigbird_pegasus/__init__.py----------------------------------------
A:transformers.models.bigbird_pegasus.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/detr/modeling_detr.py----------------------------------------
A:transformers.models.detr.modeling_detr.logger->utils.logging.get_logger(__name__)
A:transformers.models.detr.modeling_detr.weight->self.weight.reshape(1, -1, 1, 1)
A:transformers.models.detr.modeling_detr.bias->self.bias.reshape(1, -1, 1, 1)
A:transformers.models.detr.modeling_detr.running_var->self.running_var.reshape(1, -1, 1, 1)
A:transformers.models.detr.modeling_detr.running_mean->self.running_mean.reshape(1, -1, 1, 1)
A:transformers.models.detr.modeling_detr.target_attr->getattr(m, attr_str)
A:transformers.models.detr.modeling_detr.frozen->DetrFrozenBatchNorm2d(target_attr.num_features)
A:transformers.models.detr.modeling_detr.bn->getattr(m, attr_str)
A:transformers.models.detr.modeling_detr.backbone->DetrTimmConvEncoder(config.backbone, config.dilation)
A:transformers.models.detr.modeling_detr.self.intermediate_channel_sizes->self.model.feature_info.channels()
A:transformers.models.detr.modeling_detr.features->self.model(pixel_values)
A:transformers.models.detr.modeling_detr.out->self.conv_encoder(pixel_values, pixel_mask)
A:transformers.models.detr.modeling_detr.(bsz, src_len)->torch.ones((b, h, w), dtype=torch.bool, device=device).size()
A:transformers.models.detr.modeling_detr.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.detr.modeling_detr.y_embed->torch.ones((batch_size, height, width), device=device).cumsum(1, dtype=torch.float32)
A:transformers.models.detr.modeling_detr.x_embed->torch.ones((batch_size, height, width), device=device).cumsum(2, dtype=torch.float32)
A:transformers.models.detr.modeling_detr.dim_t->torch.arange(self.embedding_dim, dtype=torch.float32, device=pixel_values.device)
A:transformers.models.detr.modeling_detr.pos_x->torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
A:transformers.models.detr.modeling_detr.pos_y->torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)
A:transformers.models.detr.modeling_detr.pos->pos.repeat(pixel_values.shape[0], 1, 1, 1).repeat(pixel_values.shape[0], 1, 1, 1)
A:transformers.models.detr.modeling_detr.self.row_embeddings->torch.nn.Embedding(50, embedding_dim)
A:transformers.models.detr.modeling_detr.self.column_embeddings->torch.nn.Embedding(50, embedding_dim)
A:transformers.models.detr.modeling_detr.i->torch.arange(w, device=pixel_values.device)
A:transformers.models.detr.modeling_detr.j->torch.arange(h, device=pixel_values.device)
A:transformers.models.detr.modeling_detr.x_emb->self.column_embeddings(i)
A:transformers.models.detr.modeling_detr.y_emb->self.row_embeddings(j)
A:transformers.models.detr.modeling_detr.position_embedding->DetrLearnedPositionEmbedding(n_steps)
A:transformers.models.detr.modeling_detr.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.detr.modeling_detr.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.detr.modeling_detr.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.detr.modeling_detr.self.out_proj->torch.nn.Linear(inner_dim, num_classes)
A:transformers.models.detr.modeling_detr.(bsz, tgt_len, embed_dim)->self.layernorm(hidden_states).size()
A:transformers.models.detr.modeling_detr.hidden_states->self.layernorm(hidden_states)
A:transformers.models.detr.modeling_detr.key_value_states->self.with_pos_embed(key_value_states, key_value_position_embeddings)
A:transformers.models.detr.modeling_detr.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.detr.modeling_detr.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.detr.modeling_detr.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.detr.modeling_detr.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.detr.modeling_detr.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.detr.modeling_detr.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.detr.modeling_detr.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.detr.modeling_detr.attn_output->self.out_proj(attn_output)
A:transformers.models.detr.modeling_detr.self.self_attn->DetrAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.detr.modeling_detr.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.detr.modeling_detr.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.detr.modeling_detr.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.detr.modeling_detr.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.detr.modeling_detr.(hidden_states, attn_weights)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, position_embeddings=position_embeddings, output_attentions=output_attentions)
A:transformers.models.detr.modeling_detr.self.encoder_attn->DetrAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.detr.modeling_detr.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.detr.modeling_detr.(hidden_states, self_attn_weights)->self.self_attn(hidden_states=hidden_states, position_embeddings=query_position_embeddings, attention_mask=attention_mask, output_attentions=output_attentions)
A:transformers.models.detr.modeling_detr.(hidden_states, cross_attn_weights)->self.encoder_attn(hidden_states=hidden_states, position_embeddings=query_position_embeddings, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, key_value_position_embeddings=position_embeddings, output_attentions=output_attentions)
A:transformers.models.detr.modeling_detr.self.dense->torch.nn.Linear(input_dim, inner_dim)
A:transformers.models.detr.modeling_detr.self.dropout->torch.nn.Dropout(dropout)
A:transformers.models.detr.modeling_detr.self.layers->torch.nn.ModuleList((nn.Linear(n, k) for (n, k) in zip([input_dim] + h, h + [output_dim])))
A:transformers.models.detr.modeling_detr.attention_mask->_expand_mask(attention_mask, inputs_embeds.dtype)
A:transformers.models.detr.modeling_detr.dropout_probability->random.uniform(0, 1)
A:transformers.models.detr.modeling_detr.layer_outputs->decoder_layer(hidden_states, attention_mask=combined_attention_mask, position_embeddings=position_embeddings, query_position_embeddings=query_position_embeddings, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, output_attentions=output_attentions)
A:transformers.models.detr.modeling_detr.self.layernorm->torch.nn.LayerNorm(config.d_model)
A:transformers.models.detr.modeling_detr.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.detr.modeling_detr.intermediate->torch.stack(intermediate)
A:transformers.models.detr.modeling_detr.position_embeddings->position_embeddings_list[-1].flatten(2).permute(0, 2, 1)
A:transformers.models.detr.modeling_detr.self.backbone->DetrConvModel(backbone, position_embeddings)
A:transformers.models.detr.modeling_detr.self.input_projection->torch.nn.Conv2d(backbone.intermediate_channel_sizes[-1], config.d_model, kernel_size=1)
A:transformers.models.detr.modeling_detr.self.query_position_embeddings->torch.nn.Embedding(config.num_queries, config.d_model)
A:transformers.models.detr.modeling_detr.self.encoder->DetrEncoder(config)
A:transformers.models.detr.modeling_detr.self.decoder->DetrDecoder(config)
A:transformers.models.detr.modeling_detr.pixel_mask->torch.ones((batch_size, height, width), device=device)
A:transformers.models.detr.modeling_detr.(features, position_embeddings_list)->self.detr.model.backbone(pixel_values, pixel_mask=pixel_mask)
A:transformers.models.detr.modeling_detr.projected_feature_map->self.detr.model.input_projection(feature_map)
A:transformers.models.detr.modeling_detr.flattened_features->self.detr.model.input_projection(feature_map).flatten(2).permute(0, 2, 1)
A:transformers.models.detr.modeling_detr.flattened_mask->torch.ones((b, h, w), dtype=torch.bool, device=device).flatten(1)
A:transformers.models.detr.modeling_detr.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.detr.modeling_detr.query_position_embeddings->self.detr.model.query_position_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)
A:transformers.models.detr.modeling_detr.queries->torch.zeros_like(query_position_embeddings)
A:transformers.models.detr.modeling_detr.decoder_outputs->self.detr.model.decoder(inputs_embeds=queries, attention_mask=None, position_embeddings=position_embeddings, query_position_embeddings=query_position_embeddings, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=flattened_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.detr.modeling_detr.self.model->DetrModel(config)
A:transformers.models.detr.modeling_detr.self.class_labels_classifier->torch.nn.Linear(config.d_model, config.num_labels + 1)
A:transformers.models.detr.modeling_detr.self.bbox_predictor->DetrMLPPredictionHead(input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3)
A:transformers.models.detr.modeling_detr.outputs->self.model(pixel_values, pixel_mask=pixel_mask, decoder_attention_mask=decoder_attention_mask, encoder_outputs=encoder_outputs, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.detr.modeling_detr.logits->self.detr.class_labels_classifier(sequence_output)
A:transformers.models.detr.modeling_detr.pred_boxes->self.detr.bbox_predictor(sequence_output).sigmoid()
A:transformers.models.detr.modeling_detr.matcher->DetrHungarianMatcher(class_cost=self.config.class_cost, bbox_cost=self.config.bbox_cost, giou_cost=self.config.giou_cost)
A:transformers.models.detr.modeling_detr.criterion->DetrLoss(matcher=matcher, num_classes=self.config.num_labels, eos_coef=self.config.eos_coefficient, losses=losses)
A:transformers.models.detr.modeling_detr.outputs_class->self.class_labels_classifier(intermediate)
A:transformers.models.detr.modeling_detr.outputs_coord->self.bbox_predictor(intermediate).sigmoid()
A:transformers.models.detr.modeling_detr.auxiliary_outputs->self._set_aux_loss(outputs_class, outputs_coord)
A:transformers.models.detr.modeling_detr.loss_dict->criterion(outputs_loss, labels)
A:transformers.models.detr.modeling_detr.loss->sum((loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict))
A:transformers.models.detr.modeling_detr.self.detr->DetrForObjectDetection(config)
A:transformers.models.detr.modeling_detr.self.mask_head->DetrMaskHeadSmallConv(hidden_size + number_of_heads, intermediate_channel_sizes[::-1][-3:], hidden_size)
A:transformers.models.detr.modeling_detr.self.bbox_attention->DetrMHAttentionMap(hidden_size, hidden_size, number_of_heads, dropout=0.0, std=config.init_xavier_std)
A:transformers.models.detr.modeling_detr.memory->encoder_outputs[0].permute(0, 2, 1).view(batch_size, self.config.d_model, height, width)
A:transformers.models.detr.modeling_detr.mask->torch.ones((b, h, w), dtype=torch.bool, device=device)
A:transformers.models.detr.modeling_detr.bbox_mask->self.bbox_attention(sequence_output, memory, mask=~mask)
A:transformers.models.detr.modeling_detr.seg_masks->self.mask_head(projected_feature_map, bbox_mask, [features[2][0], features[1][0], features[0][0]])
A:transformers.models.detr.modeling_detr.pred_masks->self.mask_head(projected_feature_map, bbox_mask, [features[2][0], features[1][0], features[0][0]]).view(batch_size, self.detr.config.num_queries, seg_masks.shape[-2], seg_masks.shape[-1])
A:transformers.models.detr.modeling_detr.self.lay1->torch.nn.Conv2d(dim, dim, 3, padding=1)
A:transformers.models.detr.modeling_detr.self.gn1->torch.nn.GroupNorm(8, dim)
A:transformers.models.detr.modeling_detr.self.lay2->torch.nn.Conv2d(dim, inter_dims[1], 3, padding=1)
A:transformers.models.detr.modeling_detr.self.gn2->torch.nn.GroupNorm(8, inter_dims[1])
A:transformers.models.detr.modeling_detr.self.lay3->torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)
A:transformers.models.detr.modeling_detr.self.gn3->torch.nn.GroupNorm(8, inter_dims[2])
A:transformers.models.detr.modeling_detr.self.lay4->torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)
A:transformers.models.detr.modeling_detr.self.gn4->torch.nn.GroupNorm(8, inter_dims[3])
A:transformers.models.detr.modeling_detr.self.lay5->torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)
A:transformers.models.detr.modeling_detr.self.gn5->torch.nn.GroupNorm(8, inter_dims[4])
A:transformers.models.detr.modeling_detr.self.out_lay->torch.nn.Conv2d(inter_dims[4], 1, 3, padding=1)
A:transformers.models.detr.modeling_detr.self.adapter1->torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)
A:transformers.models.detr.modeling_detr.self.adapter2->torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)
A:transformers.models.detr.modeling_detr.self.adapter3->torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)
A:transformers.models.detr.modeling_detr.x->self.out_lay(x)
A:transformers.models.detr.modeling_detr.cur_fpn->_expand(cur_fpn, x.size(0) // cur_fpn.size(0))
A:transformers.models.detr.modeling_detr.self.q_linear->torch.nn.Linear(query_dim, hidden_dim, bias=bias)
A:transformers.models.detr.modeling_detr.self.k_linear->torch.nn.Linear(query_dim, hidden_dim, bias=bias)
A:transformers.models.detr.modeling_detr.q->self.q_linear(q)
A:transformers.models.detr.modeling_detr.k->torch.nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)
A:transformers.models.detr.modeling_detr.queries_per_head->self.q_linear(q).view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)
A:transformers.models.detr.modeling_detr.keys_per_head->torch.nn.functional.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias).view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])
A:transformers.models.detr.modeling_detr.weights->self.dropout(weights)
A:transformers.models.detr.modeling_detr.inputs->inputs.flatten(1).flatten(1)
A:transformers.models.detr.modeling_detr.prob->inputs.flatten(1).flatten(1).sigmoid()
A:transformers.models.detr.modeling_detr.ce_loss->torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
A:transformers.models.detr.modeling_detr.empty_weight->torch.ones(self.num_classes + 1)
A:transformers.models.detr.modeling_detr.idx->self._get_src_permutation_idx(indices)
A:transformers.models.detr.modeling_detr.target_classes_o->torch.cat([t['class_labels'][J] for (t, (_, J)) in zip(targets, indices)])
A:transformers.models.detr.modeling_detr.target_classes->torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device)
A:transformers.models.detr.modeling_detr.loss_ce->torch.nn.functional.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)
A:transformers.models.detr.modeling_detr.tgt_lengths->torch.as_tensor([len(v['class_labels']) for v in targets], device=device)
A:transformers.models.detr.modeling_detr.card_pred->(logits.argmax(-1) != logits.shape[-1] - 1).sum(1)
A:transformers.models.detr.modeling_detr.card_err->torch.nn.functional.l1_loss(card_pred.float(), tgt_lengths.float())
A:transformers.models.detr.modeling_detr.target_boxes->torch.cat([t['boxes'][i] for (t, (_, i)) in zip(targets, indices)], dim=0)
A:transformers.models.detr.modeling_detr.loss_bbox->torch.nn.functional.l1_loss(src_boxes, target_boxes, reduction='none')
A:transformers.models.detr.modeling_detr.src_idx->torch.cat([src for (src, _) in indices])
A:transformers.models.detr.modeling_detr.tgt_idx->torch.cat([tgt for (_, tgt) in indices])
A:transformers.models.detr.modeling_detr.(target_masks, valid)->nested_tensor_from_tensor_list(masks).decompose()
A:transformers.models.detr.modeling_detr.target_masks->target_masks.view(src_masks.shape).view(src_masks.shape)
A:transformers.models.detr.modeling_detr.src_masks->src_masks[:, 0].flatten(1)
A:transformers.models.detr.modeling_detr.batch_idx->torch.cat([torch.full_like(tgt, i) for (i, (_, tgt)) in enumerate(indices)])
A:transformers.models.detr.modeling_detr.indices->self.matcher(auxiliary_outputs, targets)
A:transformers.models.detr.modeling_detr.num_boxes->torch.clamp(num_boxes, min=1).item()
A:transformers.models.detr.modeling_detr.l_dict->self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)
A:transformers.models.detr.modeling_detr.out_prob->outputs['logits'].flatten(0, 1).softmax(-1)
A:transformers.models.detr.modeling_detr.out_bbox->outputs['pred_boxes'].flatten(0, 1)
A:transformers.models.detr.modeling_detr.tgt_ids->torch.cat([v['class_labels'] for v in targets])
A:transformers.models.detr.modeling_detr.tgt_bbox->torch.cat([v['boxes'] for v in targets])
A:transformers.models.detr.modeling_detr.bbox_cost->torch.cdist(out_bbox, tgt_bbox, p=1)
A:transformers.models.detr.modeling_detr.cost_matrix->cost_matrix.view(bs, num_queries, -1).cpu().view(bs, num_queries, -1).cpu()
A:transformers.models.detr.modeling_detr.boxes->_upcast(boxes)
A:transformers.models.detr.modeling_detr.area1->box_area(boxes1)
A:transformers.models.detr.modeling_detr.area2->box_area(boxes2)
A:transformers.models.detr.modeling_detr.lt->torch.min(boxes1[:, None, :2], boxes2[:, :2])
A:transformers.models.detr.modeling_detr.rb->torch.max(boxes1[:, None, 2:], boxes2[:, 2:])
A:transformers.models.detr.modeling_detr.wh->(rb - lt).clamp(min=0)
A:transformers.models.detr.modeling_detr.(iou, union)->box_iou(boxes1, boxes2)
A:transformers.models.detr.modeling_detr.maxes[index]->max(maxes[index], item)
A:transformers.models.detr.modeling_detr.cast_tensor->self.tensors.to(device)
A:transformers.models.detr.modeling_detr.cast_mask->torch.ones((b, h, w), dtype=torch.bool, device=device).to(device)
A:transformers.models.detr.modeling_detr.max_size->_max_by_axis([list(img.shape) for img in tensor_list])
A:transformers.models.detr.modeling_detr.tensor->torch.zeros(batch_shape, dtype=dtype, device=device)
transformers.DetrForObjectDetection(self,config:DetrConfig)
transformers.DetrForObjectDetection._set_aux_loss(self,outputs_class,outputs_coord)
transformers.DetrForObjectDetection.forward(self,pixel_values,pixel_mask=None,decoder_attention_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DetrForSegmentation(self,config:DetrConfig)
transformers.DetrForSegmentation.forward(self,pixel_values,pixel_mask=None,decoder_attention_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DetrModel(self,config:DetrConfig)
transformers.DetrModel.forward(self,pixel_values,pixel_mask=None,decoder_attention_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DetrModel.freeze_backbone(self)
transformers.DetrModel.get_decoder(self)
transformers.DetrModel.get_encoder(self)
transformers.DetrModel.unfreeze_backbone(self)
transformers.DetrModelOutput(Seq2SeqModelOutput)
transformers.DetrPreTrainedModel(PreTrainedModel)
transformers.DetrPreTrainedModel._init_weights(self,module)
transformers.models.detr.modeling_detr.DetrAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.detr.modeling_detr.DetrAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.detr.modeling_detr.DetrAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.detr.modeling_detr.DetrAttention.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,position_embeddings:Optional[torch.Tensor]=None,key_value_states:Optional[torch.Tensor]=None,key_value_position_embeddings:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.detr.modeling_detr.DetrAttention.with_pos_embed(self,tensor:torch.Tensor,position_embeddings:Optional[Tensor])
transformers.models.detr.modeling_detr.DetrClassificationHead(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.detr.modeling_detr.DetrClassificationHead.__init__(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.detr.modeling_detr.DetrClassificationHead.forward(self,hidden_states:torch.Tensor)
transformers.models.detr.modeling_detr.DetrConvModel(self,conv_encoder,position_embedding)
transformers.models.detr.modeling_detr.DetrConvModel.__init__(self,conv_encoder,position_embedding)
transformers.models.detr.modeling_detr.DetrConvModel.forward(self,pixel_values,pixel_mask)
transformers.models.detr.modeling_detr.DetrDecoder(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrDecoder.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrDecoder.forward(self,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,position_embeddings=None,query_position_embeddings=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.detr.modeling_detr.DetrDecoderLayer(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrDecoderLayer.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,position_embeddings:Optional[torch.Tensor]=None,query_position_embeddings:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,output_attentions:Optional[bool]=False)
transformers.models.detr.modeling_detr.DetrDecoderOutput(BaseModelOutputWithCrossAttentions)
transformers.models.detr.modeling_detr.DetrEncoder(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrEncoder.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrEncoder.forward(self,inputs_embeds=None,attention_mask=None,position_embeddings=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.detr.modeling_detr.DetrEncoderLayer(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrEncoderLayer.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,position_embeddings:torch.Tensor=None,output_attentions:bool=False)
transformers.models.detr.modeling_detr.DetrForObjectDetection(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrForObjectDetection.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrForObjectDetection._set_aux_loss(self,outputs_class,outputs_coord)
transformers.models.detr.modeling_detr.DetrForObjectDetection.forward(self,pixel_values,pixel_mask=None,decoder_attention_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.detr.modeling_detr.DetrForSegmentation(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrForSegmentation.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrForSegmentation.forward(self,pixel_values,pixel_mask=None,decoder_attention_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d(self,n)
transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d.__init__(self,n)
transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d._load_from_state_dict(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d.forward(self,x)
transformers.models.detr.modeling_detr.DetrHungarianMatcher(self,class_cost:float=1,bbox_cost:float=1,giou_cost:float=1)
transformers.models.detr.modeling_detr.DetrHungarianMatcher.__init__(self,class_cost:float=1,bbox_cost:float=1,giou_cost:float=1)
transformers.models.detr.modeling_detr.DetrHungarianMatcher.forward(self,outputs,targets)
transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding(self,embedding_dim=256)
transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding.__init__(self,embedding_dim=256)
transformers.models.detr.modeling_detr.DetrLearnedPositionEmbedding.forward(self,pixel_values,pixel_mask=None)
transformers.models.detr.modeling_detr.DetrLoss(self,matcher,num_classes,eos_coef,losses)
transformers.models.detr.modeling_detr.DetrLoss.__init__(self,matcher,num_classes,eos_coef,losses)
transformers.models.detr.modeling_detr.DetrLoss._get_src_permutation_idx(self,indices)
transformers.models.detr.modeling_detr.DetrLoss._get_tgt_permutation_idx(self,indices)
transformers.models.detr.modeling_detr.DetrLoss.forward(self,outputs,targets)
transformers.models.detr.modeling_detr.DetrLoss.get_loss(self,loss,outputs,targets,indices,num_boxes)
transformers.models.detr.modeling_detr.DetrLoss.loss_boxes(self,outputs,targets,indices,num_boxes)
transformers.models.detr.modeling_detr.DetrLoss.loss_cardinality(self,outputs,targets,indices,num_boxes)
transformers.models.detr.modeling_detr.DetrLoss.loss_labels(self,outputs,targets,indices,num_boxes)
transformers.models.detr.modeling_detr.DetrLoss.loss_masks(self,outputs,targets,indices,num_boxes)
transformers.models.detr.modeling_detr.DetrMHAttentionMap(self,query_dim,hidden_dim,num_heads,dropout=0.0,bias=True,std=None)
transformers.models.detr.modeling_detr.DetrMHAttentionMap.__init__(self,query_dim,hidden_dim,num_heads,dropout=0.0,bias=True,std=None)
transformers.models.detr.modeling_detr.DetrMHAttentionMap.forward(self,q,k,mask:Optional[Tensor]=None)
transformers.models.detr.modeling_detr.DetrMLPPredictionHead(self,input_dim,hidden_dim,output_dim,num_layers)
transformers.models.detr.modeling_detr.DetrMLPPredictionHead.__init__(self,input_dim,hidden_dim,output_dim,num_layers)
transformers.models.detr.modeling_detr.DetrMLPPredictionHead.forward(self,x)
transformers.models.detr.modeling_detr.DetrMaskHeadSmallConv(self,dim,fpn_dims,context_dim)
transformers.models.detr.modeling_detr.DetrMaskHeadSmallConv.__init__(self,dim,fpn_dims,context_dim)
transformers.models.detr.modeling_detr.DetrMaskHeadSmallConv.forward(self,x:Tensor,bbox_mask:Tensor,fpns:List[Tensor])
transformers.models.detr.modeling_detr.DetrModel(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrModel.__init__(self,config:DetrConfig)
transformers.models.detr.modeling_detr.DetrModel.forward(self,pixel_values,pixel_mask=None,decoder_attention_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.detr.modeling_detr.DetrModel.freeze_backbone(self)
transformers.models.detr.modeling_detr.DetrModel.get_decoder(self)
transformers.models.detr.modeling_detr.DetrModel.get_encoder(self)
transformers.models.detr.modeling_detr.DetrModel.unfreeze_backbone(self)
transformers.models.detr.modeling_detr.DetrModelOutput(Seq2SeqModelOutput)
transformers.models.detr.modeling_detr.DetrObjectDetectionOutput(ModelOutput)
transformers.models.detr.modeling_detr.DetrPreTrainedModel(PreTrainedModel)
transformers.models.detr.modeling_detr.DetrPreTrainedModel._init_weights(self,module)
transformers.models.detr.modeling_detr.DetrSegmentationOutput(ModelOutput)
transformers.models.detr.modeling_detr.DetrSinePositionEmbedding(self,embedding_dim=64,temperature=10000,normalize=False,scale=None)
transformers.models.detr.modeling_detr.DetrSinePositionEmbedding.__init__(self,embedding_dim=64,temperature=10000,normalize=False,scale=None)
transformers.models.detr.modeling_detr.DetrSinePositionEmbedding.forward(self,pixel_values,pixel_mask)
transformers.models.detr.modeling_detr.DetrTimmConvEncoder(self,name:str,dilation:bool)
transformers.models.detr.modeling_detr.DetrTimmConvEncoder.__init__(self,name:str,dilation:bool)
transformers.models.detr.modeling_detr.DetrTimmConvEncoder.forward(self,pixel_values:torch.Tensor,pixel_mask:torch.Tensor)
transformers.models.detr.modeling_detr.NestedTensor(self,tensors,mask:Optional[Tensor])
transformers.models.detr.modeling_detr.NestedTensor.__init__(self,tensors,mask:Optional[Tensor])
transformers.models.detr.modeling_detr.NestedTensor.__repr__(self)
transformers.models.detr.modeling_detr.NestedTensor.decompose(self)
transformers.models.detr.modeling_detr.NestedTensor.to(self,device)
transformers.models.detr.modeling_detr._expand(tensor,length:int)
transformers.models.detr.modeling_detr._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.detr.modeling_detr._max_by_axis(the_list)
transformers.models.detr.modeling_detr._upcast(t:Tensor)->Tensor
transformers.models.detr.modeling_detr.box_area(boxes:Tensor)->Tensor
transformers.models.detr.modeling_detr.box_iou(boxes1,boxes2)
transformers.models.detr.modeling_detr.build_position_encoding(config)
transformers.models.detr.modeling_detr.dice_loss(inputs,targets,num_boxes)
transformers.models.detr.modeling_detr.generalized_box_iou(boxes1,boxes2)
transformers.models.detr.modeling_detr.nested_tensor_from_tensor_list(tensor_list:List[Tensor])
transformers.models.detr.modeling_detr.replace_batch_norm(m,name='')
transformers.models.detr.modeling_detr.sigmoid_focal_loss(inputs,targets,num_boxes,alpha:float=0.25,gamma:float=2)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.val->rename_backbone_keys(state_dict).pop(key)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.new_state_dict->OrderedDict()
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.new_key->key.replace('backbone.0.body', 'backbone.conv_encoder.model')
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.in_proj_weight->rename_backbone_keys(state_dict).pop(f'{prefix}transformer.decoder.layers.{i}.self_attn.in_proj_weight')
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.in_proj_bias->rename_backbone_keys(state_dict).pop(f'{prefix}transformer.decoder.layers.{i}.self_attn.in_proj_bias')
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.in_proj_weight_cross_attn->rename_backbone_keys(state_dict).pop(f'{prefix}transformer.decoder.layers.{i}.multihead_attn.in_proj_weight')
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.in_proj_bias_cross_attn->rename_backbone_keys(state_dict).pop(f'{prefix}transformer.decoder.layers.{i}.multihead_attn.in_proj_bias')
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.im->PIL.Image.open(requests.get(url, stream=True).raw)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.config->DetrConfig()
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.feature_extractor->DetrFeatureExtractor(format=format)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.img->prepare_img()
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.encoding->feature_extractor(images=img, return_tensors='pt')
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.detr->torch.hub.load('facebookresearch/detr', model_name, pretrained=True).eval()
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.state_dict->rename_backbone_keys(state_dict)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.original_outputs->detr(pixel_values)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.outputs->model(pixel_values)
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.convert_detr_checkpoint(model_name,pytorch_dump_folder_path)
transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.prepare_img()
transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.read_in_q_k_v(state_dict,is_panoptic=False)
transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.rename_backbone_keys(state_dict)
transformers.models.detr.convert_detr_original_pytorch_checkpoint_to_pytorch.rename_key(state_dict,old,new)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/detr/configuration_detr.py----------------------------------------
A:transformers.models.detr.configuration_detr.logger->utils.logging.get_logger(__name__)
transformers.DetrConfig(self,num_queries=100,max_position_embeddings=1024,encoder_layers=6,encoder_ffn_dim=2048,encoder_attention_heads=8,decoder_layers=6,decoder_ffn_dim=2048,decoder_attention_heads=8,encoder_layerdrop=0.0,decoder_layerdrop=0.0,is_encoder_decoder=True,activation_function='relu',d_model=256,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,init_xavier_std=1.0,classifier_dropout=0.0,scale_embedding=False,auxiliary_loss=False,position_embedding_type='sine',backbone='resnet50',dilation=False,class_cost=1,bbox_cost=5,giou_cost=2,mask_loss_coefficient=1,dice_loss_coefficient=1,bbox_loss_coefficient=5,giou_loss_coefficient=2,eos_coefficient=0.1,**kwargs)
transformers.DetrConfig.hidden_size(self)->int
transformers.DetrConfig.num_attention_heads(self)->int
transformers.models.detr.configuration_detr.DetrConfig(self,num_queries=100,max_position_embeddings=1024,encoder_layers=6,encoder_ffn_dim=2048,encoder_attention_heads=8,decoder_layers=6,decoder_ffn_dim=2048,decoder_attention_heads=8,encoder_layerdrop=0.0,decoder_layerdrop=0.0,is_encoder_decoder=True,activation_function='relu',d_model=256,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,init_xavier_std=1.0,classifier_dropout=0.0,scale_embedding=False,auxiliary_loss=False,position_embedding_type='sine',backbone='resnet50',dilation=False,class_cost=1,bbox_cost=5,giou_cost=2,mask_loss_coefficient=1,dice_loss_coefficient=1,bbox_loss_coefficient=5,giou_loss_coefficient=2,eos_coefficient=0.1,**kwargs)
transformers.models.detr.configuration_detr.DetrConfig.__init__(self,num_queries=100,max_position_embeddings=1024,encoder_layers=6,encoder_ffn_dim=2048,encoder_attention_heads=8,decoder_layers=6,decoder_ffn_dim=2048,decoder_attention_heads=8,encoder_layerdrop=0.0,decoder_layerdrop=0.0,is_encoder_decoder=True,activation_function='relu',d_model=256,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,init_xavier_std=1.0,classifier_dropout=0.0,scale_embedding=False,auxiliary_loss=False,position_embedding_type='sine',backbone='resnet50',dilation=False,class_cost=1,bbox_cost=5,giou_cost=2,mask_loss_coefficient=1,dice_loss_coefficient=1,bbox_loss_coefficient=5,giou_loss_coefficient=2,eos_coefficient=0.1,**kwargs)
transformers.models.detr.configuration_detr.DetrConfig.hidden_size(self)->int
transformers.models.detr.configuration_detr.DetrConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/detr/__init__.py----------------------------------------
A:transformers.models.detr.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/detr/feature_extraction_detr.py----------------------------------------
A:transformers.models.detr.feature_extraction_detr.logger->utils.logging.get_logger(__name__)
A:transformers.models.detr.feature_extraction_detr.(x_c, y_c, w, h)->numpy.ma.array(x_mask, mask=~np.array(masks, dtype=bool)).unbind(-1)
A:transformers.models.detr.feature_extraction_detr.y->numpy.ma.array(y_mask, mask=~np.array(masks, dtype=bool))
A:transformers.models.detr.feature_extraction_detr.x->numpy.ma.array(x_mask, mask=~np.array(masks, dtype=bool))
A:transformers.models.detr.feature_extraction_detr.(y, x)->numpy.meshgrid(y, x, indexing='ij')
A:transformers.models.detr.feature_extraction_detr.x_max->x_mask.reshape(x_mask.shape[0], -1).max(-1)
A:transformers.models.detr.feature_extraction_detr.x_min->x_min.reshape(x_min.shape[0], -1).min(-1).reshape(x_min.shape[0], -1).min(-1)
A:transformers.models.detr.feature_extraction_detr.y_max->y_mask.reshape(x_mask.shape[0], -1).max(-1)
A:transformers.models.detr.feature_extraction_detr.y_min->y_min.reshape(y_min.shape[0], -1).min(-1).reshape(y_min.shape[0], -1).min(-1)
A:transformers.models.detr.feature_extraction_detr.color->color.astype(np.int32).astype(np.int32)
A:transformers.models.detr.feature_extraction_detr.id_map_copy->id_map.copy()
A:transformers.models.detr.feature_extraction_detr.rgb_shape->tuple(list(id_map.shape) + [3])
A:transformers.models.detr.feature_extraction_detr.rgb_map->numpy.zeros(rgb_shape, dtype=np.uint8)
A:transformers.models.detr.feature_extraction_detr.self.format->self._is_valid_format(format)
A:transformers.models.detr.feature_extraction_detr.(image, target)->self._normalize(image=image, mean=self.image_mean, std=self.image_std, target=target)
A:transformers.models.detr.feature_extraction_detr.rles->pycocotools.mask.frPyObjects(polygons, height, width)
A:transformers.models.detr.feature_extraction_detr.mask->numpy.zeros((h, w), dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.masks->torch.from_numpy(target['masks'][:, None]).float()
A:transformers.models.detr.feature_extraction_detr.image_id->numpy.asarray([image_id], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.boxes->center_to_corners_format(out_bbox)
A:transformers.models.detr.feature_extraction_detr.boxes[:, 0::2]->boxes[:, 0::2].clip(min=0, max=w).clip(min=0, max=w)
A:transformers.models.detr.feature_extraction_detr.boxes[:, 1::2]->boxes[:, 1::2].clip(min=0, max=h).clip(min=0, max=h)
A:transformers.models.detr.feature_extraction_detr.classes->numpy.asarray(classes, dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.keypoints->keypoints.reshape((-1, 3)).reshape((-1, 3))
A:transformers.models.detr.feature_extraction_detr.area->numpy.asarray([obj['area'] for obj in anno], dtype=np.float32)
A:transformers.models.detr.feature_extraction_detr.iscrowd->numpy.asarray([obj['iscrowd'] if 'iscrowd' in obj else 0 for obj in anno], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.target['orig_size']->numpy.asarray([int(h), int(w)], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.target['size']->numpy.asarray([h, w], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.ann_info->target.copy().copy()
A:transformers.models.detr.feature_extraction_detr.ids->numpy.array([ann['id'] for ann in ann_info['segments_info']])
A:transformers.models.detr.feature_extraction_detr.labels->numpy.asarray([ann['category_id'] for ann in ann_info['segments_info']], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.target['image_id']->numpy.asarray([ann_info['image_id'] if 'image_id' in ann_info else ann_info['id']], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.target['boxes']->masks_to_boxes(masks)
A:transformers.models.detr.feature_extraction_detr.target['iscrowd']->numpy.asarray([ann['iscrowd'] for ann in ann_info['segments_info']], dtype=np.int64)
A:transformers.models.detr.feature_extraction_detr.target['area']->numpy.asarray([ann['area'] for ann in ann_info['segments_info']], dtype=np.float32)
A:transformers.models.detr.feature_extraction_detr.image->self.to_pil_image(image)
A:transformers.models.detr.feature_extraction_detr.min_original_size->float(min((w, h)))
A:transformers.models.detr.feature_extraction_detr.max_original_size->float(max((w, h)))
A:transformers.models.detr.feature_extraction_detr.size->get_size(image.size, size, max_size)
A:transformers.models.detr.feature_extraction_detr.oh->int(size * h / w)
A:transformers.models.detr.feature_extraction_detr.ow->int(size * w / h)
A:transformers.models.detr.feature_extraction_detr.rescaled_image->self.resize(image, size=size)
A:transformers.models.detr.feature_extraction_detr.ratios->tuple((float(s) / float(s_orig) for (s, s_orig) in zip(rescaled_image.size, image.size)))
A:transformers.models.detr.feature_extraction_detr.target->target.copy().copy()
A:transformers.models.detr.feature_extraction_detr.target['masks']->interpolated_masks.numpy()
A:transformers.models.detr.feature_extraction_detr.is_batched->bool(isinstance(images, (list, tuple)) and (isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0])))
A:transformers.models.detr.feature_extraction_detr.max_size->self._max_by_axis([list(image.shape) for image in pixel_values_list])
A:transformers.models.detr.feature_extraction_detr.padded_image->numpy.zeros((c, h, w), dtype=np.float32)
A:transformers.models.detr.feature_extraction_detr.padded_image[:image.shape[0], :image.shape[1], :image.shape[2]]->numpy.copy(image)
A:transformers.models.detr.feature_extraction_detr.encoded_inputs->BatchFeature(data=data, tensor_type=return_tensors)
A:transformers.models.detr.feature_extraction_detr.tensor_type->TensorType(tensor_type)
A:transformers.models.detr.feature_extraction_detr.maxes[index]->max(maxes[index], item)
A:transformers.models.detr.feature_extraction_detr.prob->torch.nn.functional.softmax(out_logits, -1)
A:transformers.models.detr.feature_extraction_detr.(scores, labels)->cur_logits.softmax(-1).max(-1)
A:transformers.models.detr.feature_extraction_detr.(img_h, img_w)->target_sizes.unbind(1)
A:transformers.models.detr.feature_extraction_detr.scale_fct->torch.stack([img_w, img_h, img_w, img_h], dim=1)
A:transformers.models.detr.feature_extraction_detr.(max_h, max_w)->max_target_sizes.max(0)[0].tolist()
A:transformers.models.detr.feature_extraction_detr.outputs_masks->(outputs_masks.sigmoid() > threshold).cpu()
A:transformers.models.detr.feature_extraction_detr.results[i]['masks']->torch.nn.functional.interpolate(results[i]['masks'].float(), size=tuple(tt.tolist()), mode='nearest').byte()
A:transformers.models.detr.feature_extraction_detr.(cur_scores, cur_classes)->cur_logits.softmax(-1).max(-1)
A:transformers.models.detr.feature_extraction_detr.cur_masks->cur_masks.flatten(1).flatten(1)
A:transformers.models.detr.feature_extraction_detr.cur_boxes->center_to_corners_format(cur_boxes[keep])
A:transformers.models.detr.feature_extraction_detr.stuff_equiv_classes->defaultdict(lambda : [])
A:transformers.models.detr.feature_extraction_detr.m_id->torch.from_numpy(rgb_to_id(np_seg_img))
A:transformers.models.detr.feature_extraction_detr.(final_h, final_w)->to_tuple(target_size)
A:transformers.models.detr.feature_extraction_detr.seg_img->seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST).resize(size=(final_w, final_h), resample=Image.NEAREST)
A:transformers.models.detr.feature_extraction_detr.np_seg_img->np_seg_img.numpy().numpy()
A:transformers.models.detr.feature_extraction_detr.(area, seg_img)->get_ids_area(cur_masks, cur_scores)
A:transformers.models.detr.feature_extraction_detr.filtered_small->torch.as_tensor([area[i] <= 4 for (i, c) in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)
A:transformers.models.detr.feature_extraction_detr.cur_classes->torch.ones(1, dtype=torch.long, device=cur_classes.device)
A:transformers.models.detr.feature_extraction_detr.cat->cur_classes[i].item()
transformers.DetrFeatureExtractor(self,format='coco_detection',do_resize=True,size=800,max_size=1333,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.DetrFeatureExtractor._is_valid_format(self,format)
transformers.DetrFeatureExtractor._max_by_axis(self,the_list)
transformers.DetrFeatureExtractor._normalize(self,image,mean,std,target=None)
transformers.DetrFeatureExtractor._resize(self,image,size,target=None,max_size=None)
transformers.DetrFeatureExtractor.convert_coco_poly_to_mask(self,segmentations,height,width)
transformers.DetrFeatureExtractor.pad_and_create_pixel_mask(self,pixel_values_list:List['torch.Tensor'],return_tensors:Optional[Union[str,TensorType]]=None)
transformers.DetrFeatureExtractor.post_process(self,outputs,target_sizes)
transformers.DetrFeatureExtractor.post_process_panoptic(self,outputs,processed_sizes,target_sizes=None,is_thing_map=None,threshold=0.85)
transformers.DetrFeatureExtractor.post_process_segmentation(self,results,outputs,orig_target_sizes,max_target_sizes,threshold=0.5)
transformers.DetrFeatureExtractor.prepare(self,image,target,return_segmentation_masks=False,masks_path=None)
transformers.DetrFeatureExtractor.prepare_coco_detection(self,image,target,return_segmentation_masks=False)
transformers.DetrFeatureExtractor.prepare_coco_panoptic(self,image,target,masks_path,return_masks=True)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor(self,format='coco_detection',do_resize=True,size=800,max_size=1333,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.__init__(self,format='coco_detection',do_resize=True,size=800,max_size=1333,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor._is_valid_format(self,format)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor._max_by_axis(self,the_list)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor._normalize(self,image,mean,std,target=None)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor._resize(self,image,size,target=None,max_size=None)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.convert_coco_poly_to_mask(self,segmentations,height,width)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.pad_and_create_pixel_mask(self,pixel_values_list:List['torch.Tensor'],return_tensors:Optional[Union[str,TensorType]]=None)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.post_process(self,outputs,target_sizes)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.post_process_panoptic(self,outputs,processed_sizes,target_sizes=None,is_thing_map=None,threshold=0.85)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.post_process_segmentation(self,results,outputs,orig_target_sizes,max_target_sizes,threshold=0.5)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.prepare(self,image,target,return_segmentation_masks=False,masks_path=None)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.prepare_coco_detection(self,image,target,return_segmentation_masks=False)
transformers.models.detr.feature_extraction_detr.DetrFeatureExtractor.prepare_coco_panoptic(self,image,target,masks_path,return_masks=True)
transformers.models.detr.feature_extraction_detr.center_to_corners_format(x)
transformers.models.detr.feature_extraction_detr.corners_to_center_format(x)
transformers.models.detr.feature_extraction_detr.id_to_rgb(id_map)
transformers.models.detr.feature_extraction_detr.masks_to_boxes(masks)
transformers.models.detr.feature_extraction_detr.rgb_to_id(color)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/modeling_flax_clip.py----------------------------------------
A:transformers.models.clip.modeling_flax_clip.logger->utils.logging.get_logger(__name__)
A:transformers.models.clip.modeling_flax_clip.self.class_embedding->self.param('class_embedding', jax.nn.initializers.normal(stddev=0.02), (embed_dim,))
A:transformers.models.clip.modeling_flax_clip.self.patch_embedding->flax.linen.Conv(embed_dim, kernel_size=(patch_size, patch_size), strides=(patch_size, patch_size), padding='VALID', use_bias=False, dtype=self.dtype, kernel_init=jax.nn.initializers.normal())
A:transformers.models.clip.modeling_flax_clip.self.position_embedding->flax.linen.Embed(self.config.max_position_embeddings, embed_dim, embedding_init=jax.nn.initializers.normal())
A:transformers.models.clip.modeling_flax_clip.self.position_ids->jax.numpy.expand_dims(jnp.arange(0, self.config.max_position_embeddings, dtype='i4'), axis=(0, 1))
A:transformers.models.clip.modeling_flax_clip.patch_embeds->jax.numpy.reshape(patch_embeds, (batch_size, height * width, channels))
A:transformers.models.clip.modeling_flax_clip.class_embeds->jax.numpy.tile(class_embeds, (batch_size, 1, 1))
A:transformers.models.clip.modeling_flax_clip.embeddings->jax.numpy.concatenate([class_embeds, patch_embeds], axis=1)
A:transformers.models.clip.modeling_flax_clip.self.token_embedding->flax.linen.Embed(self.config.vocab_size, embed_dim, embedding_init=jax.nn.initializers.normal())
A:transformers.models.clip.modeling_flax_clip.input_embeds->self.token_embedding(input_ids.astype('i4'))
A:transformers.models.clip.modeling_flax_clip.position_embeds->self.position_embedding(position_ids.astype('i4'))
A:transformers.models.clip.modeling_flax_clip.self.k_proj->flax.linen.Dense(self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.01, dtype=self.dtype))
A:transformers.models.clip.modeling_flax_clip.self.v_proj->flax.linen.Dense(self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.01, dtype=self.dtype))
A:transformers.models.clip.modeling_flax_clip.self.q_proj->flax.linen.Dense(self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.01, dtype=self.dtype))
A:transformers.models.clip.modeling_flax_clip.self.out_proj->flax.linen.Dense(self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.01, dtype=self.dtype))
A:transformers.models.clip.modeling_flax_clip.self.causal->isinstance(self.config, CLIPTextConfig)
A:transformers.models.clip.modeling_flax_clip.self.causal_mask->make_causal_mask(jnp.ones((1, self.config.max_position_embeddings), dtype='i4'))
A:transformers.models.clip.modeling_flax_clip.query->self._split_heads(query)
A:transformers.models.clip.modeling_flax_clip.key->self._split_heads(key)
A:transformers.models.clip.modeling_flax_clip.value->self._split_heads(value)
A:transformers.models.clip.modeling_flax_clip.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.clip.modeling_flax_clip.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, -10000.0).astype(self.dtype))
A:transformers.models.clip.modeling_flax_clip.dropout_rng->self.make_rng('dropout')
A:transformers.models.clip.modeling_flax_clip.attn_weights->dot_product_attention_weights(query, key, bias=attention_bias, dropout_rng=dropout_rng, dropout_rate=self.dropout, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.clip.modeling_flax_clip.attn_output->self.out_proj(attn_output)
A:transformers.models.clip.modeling_flax_clip.self.fc1->flax.linen.Dense(self.config.intermediate_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.01, dtype=self.dtype))
A:transformers.models.clip.modeling_flax_clip.self.fc2->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.01, dtype=self.dtype))
A:transformers.models.clip.modeling_flax_clip.hidden_states->self.pre_layrnorm(hidden_states)
A:transformers.models.clip.modeling_flax_clip.self.self_attn->FlaxCLIPAttention(self.config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.layer_norm1->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.mlp->FlaxCLIPMLP(self.config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.layer_norm2->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.attn_outputs->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.clip.modeling_flax_clip.layer_outputs->layer(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.clip.modeling_flax_clip.self.layers->FlaxCLIPLayerCollection(self.config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.embeddings->FlaxCLIPVisionEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.encoder->FlaxCLIPEncoder(self.config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.final_layer_norm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.encoder_outputs->self.encoder(inputs_embeds=hidden_states, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.clip.modeling_flax_clip.last_hidden_state->self.final_layer_norm(last_hidden_state)
A:transformers.models.clip.modeling_flax_clip.self.pre_layrnorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.post_layernorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.pooled_output->self.post_layernorm(pooled_output)
A:transformers.models.clip.modeling_flax_clip.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.clip.modeling_flax_clip.input_ids->jax.numpy.zeros(input_shape[0], dtype='i4')
A:transformers.models.clip.modeling_flax_clip.position_ids->jax.numpy.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)
A:transformers.models.clip.modeling_flax_clip.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.clip.modeling_flax_clip.pixel_values->jax.numpy.transpose(pixel_values, (0, 2, 3, 1))
A:transformers.models.clip.modeling_flax_clip.text_outputs->self.text_model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.clip.modeling_flax_clip.text_features->self.module_class(config=config, dtype=dtype, **kwargs).text_projection(pooled_output)
A:transformers.models.clip.modeling_flax_clip.vision_outputs->self.vision_model(pixel_values=pixel_values, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.clip.modeling_flax_clip.image_features->self.module_class(config=config, dtype=dtype, **kwargs).visual_projection(pooled_output)
A:transformers.models.clip.modeling_flax_clip.self.text_model->FlaxCLIPTextTransformer(text_config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.vision_model->FlaxCLIPVisionTransformer(vision_config, dtype=self.dtype)
A:transformers.models.clip.modeling_flax_clip.self.visual_projection->flax.linen.Dense(self.projection_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.02, dtype=self.dtype), use_bias=False)
A:transformers.models.clip.modeling_flax_clip.self.text_projection->flax.linen.Dense(self.projection_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(0.02, dtype=self.dtype), use_bias=False)
A:transformers.models.clip.modeling_flax_clip.self.logit_scale->self.param('logit_scale', jax.nn.initializers.ones, [])
A:transformers.models.clip.modeling_flax_clip.image_embeds->self.visual_projection(image_embeds)
A:transformers.models.clip.modeling_flax_clip.text_embeds->self.text_projection(text_embeds)
A:transformers.models.clip.modeling_flax_clip.logit_scale->jax.numpy.exp(self.logit_scale)
transformers.FlaxCLIPModel(FlaxCLIPPreTrainedModel)
transformers.FlaxCLIPPreTrainedModel(self,config:CLIPConfig,input_shape:Optional[Tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxCLIPPreTrainedModel.get_image_features(self,pixel_values,dropout_rng:jax.random.PRNGKey=None,train=False)
transformers.FlaxCLIPPreTrainedModel.get_text_features(self,input_ids,attention_mask=None,position_ids=None,dropout_rng:jax.random.PRNGKey=None,train=False)
transformers.FlaxCLIPPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.FlaxCLIPTextModel(FlaxCLIPTextPreTrainedModel)
transformers.FlaxCLIPTextPreTrainedModel(self,config:CLIPTextConfig,input_shape=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxCLIPTextPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.FlaxCLIPVisionModel(FlaxCLIPVisionPreTrainedModel)
transformers.FlaxCLIPVisionPreTrainedModel(self,config:CLIPVisionConfig,input_shape:Optional[Tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxCLIPVisionPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.clip.modeling_flax_clip.FlaxCLIPAttention(self,hidden_states,attention_mask=None,deterministic:bool=True,output_attentions:bool=False)
transformers.models.clip.modeling_flax_clip.FlaxCLIPAttention.__call__(self,hidden_states,attention_mask=None,deterministic:bool=True,output_attentions:bool=False)
transformers.models.clip.modeling_flax_clip.FlaxCLIPAttention._merge_heads(self,hidden_states)
transformers.models.clip.modeling_flax_clip.FlaxCLIPAttention._split_heads(self,hidden_states)
transformers.models.clip.modeling_flax_clip.FlaxCLIPAttention.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPEncoder(self,inputs_embeds,attention_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPEncoder.__call__(self,inputs_embeds,attention_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPEncoder.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPEncoderLayer(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.clip.modeling_flax_clip.FlaxCLIPEncoderLayer.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.clip.modeling_flax_clip.FlaxCLIPEncoderLayer.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPLayerCollection(self,hidden_states,attention_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPLayerCollection.__call__(self,hidden_states,attention_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPLayerCollection.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPMLP(self,hidden_states)
transformers.models.clip.modeling_flax_clip.FlaxCLIPMLP.__call__(self,hidden_states)
transformers.models.clip.modeling_flax_clip.FlaxCLIPMLP.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPModel(FlaxCLIPPreTrainedModel)
transformers.models.clip.modeling_flax_clip.FlaxCLIPModule(self,input_ids=None,pixel_values=None,attention_mask=None,position_ids=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_flax_clip.FlaxCLIPModule.__call__(self,input_ids=None,pixel_values=None,attention_mask=None,position_ids=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_flax_clip.FlaxCLIPModule.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput(ModelOutput)
transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput.to_tuple(self)->Tuple[Any]
transformers.models.clip.modeling_flax_clip.FlaxCLIPPreTrainedModel(self,config:CLIPConfig,input_shape:Optional[Tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.clip.modeling_flax_clip.FlaxCLIPPreTrainedModel.__init__(self,config:CLIPConfig,input_shape:Optional[Tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.clip.modeling_flax_clip.FlaxCLIPPreTrainedModel.get_image_features(self,pixel_values,dropout_rng:jax.random.PRNGKey=None,train=False)
transformers.models.clip.modeling_flax_clip.FlaxCLIPPreTrainedModel.get_text_features(self,input_ids,attention_mask=None,position_ids=None,dropout_rng:jax.random.PRNGKey=None,train=False)
transformers.models.clip.modeling_flax_clip.FlaxCLIPPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextEmbeddings(self,input_ids,position_ids)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextEmbeddings.__call__(self,input_ids,position_ids)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextEmbeddings.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModel(FlaxCLIPTextPreTrainedModel)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModule(self,input_ids,attention_mask,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModule.__call__(self,input_ids,attention_mask,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModule.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextPreTrainedModel(self,config:CLIPTextConfig,input_shape=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextPreTrainedModel.__init__(self,config:CLIPTextConfig,input_shape=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextTransformer(self,input_ids,attention_mask,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextTransformer.__call__(self,input_ids,attention_mask,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPTextTransformer.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionEmbeddings(self,pixel_values)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionEmbeddings.__call__(self,pixel_values)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionEmbeddings.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionModel(FlaxCLIPVisionPreTrainedModel)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionModule(self,pixel_values,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionModule.__call__(self,pixel_values,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionModule.setup(self)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionPreTrainedModel(self,config:CLIPVisionConfig,input_shape:Optional[Tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionPreTrainedModel.__init__(self,config:CLIPVisionConfig,input_shape:Optional[Tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionTransformer(self,pixel_values=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionTransformer.__call__(self,pixel_values=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict:bool=True)
transformers.models.clip.modeling_flax_clip.FlaxCLIPVisionTransformer.setup(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/feature_extraction_clip.py----------------------------------------
A:transformers.models.clip.feature_extraction_clip.logger->utils.logging.get_logger(__name__)
A:transformers.models.clip.feature_extraction_clip.is_batched->bool(isinstance(images, (list, tuple)) and (isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0])))
A:transformers.models.clip.feature_extraction_clip.encoded_inputs->BatchFeature(data=data, tensor_type=return_tensors)
A:transformers.models.clip.feature_extraction_clip.image->self.to_pil_image(image)
A:transformers.models.clip.feature_extraction_clip.crop_top->int((image_height - crop_height + 1) * 0.5)
A:transformers.models.clip.feature_extraction_clip.crop_left->int((image_width - crop_width + 1) * 0.5)
transformers.CLIPFeatureExtractor(self,do_resize=True,size=224,resample=Image.BICUBIC,do_center_crop=True,crop_size=224,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.CLIPFeatureExtractor.center_crop(self,image,size)
transformers.CLIPFeatureExtractor.resize(self,image,size,resample=Image.BICUBIC)
transformers.models.clip.feature_extraction_clip.CLIPFeatureExtractor(self,do_resize=True,size=224,resample=Image.BICUBIC,do_center_crop=True,crop_size=224,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.clip.feature_extraction_clip.CLIPFeatureExtractor.__init__(self,do_resize=True,size=224,resample=Image.BICUBIC,do_center_crop=True,crop_size=224,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.clip.feature_extraction_clip.CLIPFeatureExtractor.center_crop(self,image,size)
transformers.models.clip.feature_extraction_clip.CLIPFeatureExtractor.resize(self,image,size,resample=Image.BICUBIC)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/tokenization_clip.py----------------------------------------
A:transformers.models.clip.tokenization_clip.logger->utils.logging.get_logger(__name__)
A:transformers.models.clip.tokenization_clip.pairs->get_pairs(word)
A:transformers.models.clip.tokenization_clip.text->bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors).replace('</w>', ' ')
A:transformers.models.clip.tokenization_clip.self.nlp->BasicTokenizer(do_lower_case=True)
A:transformers.models.clip.tokenization_clip.self.encoder->json.load(vocab_handle)
A:transformers.models.clip.tokenization_clip.self.byte_encoder->bytes_to_unicode()
A:transformers.models.clip.tokenization_clip.self.bpe_ranks->dict(zip(bpe_merges, range(len(bpe_merges))))
A:transformers.models.clip.tokenization_clip.self.pat->regex.compile("<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+", re.IGNORECASE)
A:transformers.models.clip.tokenization_clip.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.clip.tokenization_clip.j->' '.join(word).index(first, i)
A:transformers.models.clip.tokenization_clip.new_word->tuple(new_word)
A:transformers.models.clip.tokenization_clip.word->' '.join(word)
A:transformers.models.clip.tokenization_clip.token->''.join((self.byte_encoder[b] for b in token.encode('utf-8')))
A:transformers.models.clip.tokenization_clip.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.clip.tokenization_clip.merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
A:transformers.models.clip.tokenization_clip.add_prefix_space->kwargs.pop('add_prefix_space', self.add_prefix_space)
transformers.CLIPTokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|startoftext|>',eos_token='<|endoftext|>',pad_token='<|endoftext|>',add_prefix_space=False,do_lower_case=True,**kwargs)
transformers.CLIPTokenizer._convert_id_to_token(self,index)
transformers.CLIPTokenizer._convert_token_to_id(self,token)
transformers.CLIPTokenizer._tokenize(self,text)
transformers.CLIPTokenizer.bpe(self,token)
transformers.CLIPTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CLIPTokenizer.convert_tokens_to_string(self,tokens)
transformers.CLIPTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.CLIPTokenizer.get_vocab(self)
transformers.CLIPTokenizer.pad_token_id(self)->Optional[int]
transformers.CLIPTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.CLIPTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.CLIPTokenizer.vocab_size(self)
transformers.models.clip.tokenization_clip.CLIPTokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|startoftext|>',eos_token='<|endoftext|>',pad_token='<|endoftext|>',add_prefix_space=False,do_lower_case=True,**kwargs)
transformers.models.clip.tokenization_clip.CLIPTokenizer.__init__(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|startoftext|>',eos_token='<|endoftext|>',pad_token='<|endoftext|>',add_prefix_space=False,do_lower_case=True,**kwargs)
transformers.models.clip.tokenization_clip.CLIPTokenizer._convert_id_to_token(self,index)
transformers.models.clip.tokenization_clip.CLIPTokenizer._convert_token_to_id(self,token)
transformers.models.clip.tokenization_clip.CLIPTokenizer._tokenize(self,text)
transformers.models.clip.tokenization_clip.CLIPTokenizer.bpe(self,token)
transformers.models.clip.tokenization_clip.CLIPTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.clip.tokenization_clip.CLIPTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.clip.tokenization_clip.CLIPTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.clip.tokenization_clip.CLIPTokenizer.get_vocab(self)
transformers.models.clip.tokenization_clip.CLIPTokenizer.pad_token_id(self)->Optional[int]
transformers.models.clip.tokenization_clip.CLIPTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.models.clip.tokenization_clip.CLIPTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.clip.tokenization_clip.CLIPTokenizer.vocab_size(self)
transformers.models.clip.tokenization_clip.bytes_to_unicode()
transformers.models.clip.tokenization_clip.get_pairs(word)
transformers.models.clip.tokenization_clip.whitespace_clean(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/configuration_clip.py----------------------------------------
A:transformers.models.clip.configuration_clip.logger->utils.logging.get_logger(__name__)
A:transformers.models.clip.configuration_clip.self.text_config->CLIPTextConfig(**text_config_dict)
A:transformers.models.clip.configuration_clip.self.vision_config->CLIPVisionConfig(**vision_config_dict)
A:transformers.models.clip.configuration_clip.output->copy.deepcopy(self.__dict__)
A:transformers.models.clip.configuration_clip.output['text_config']->self.text_config.to_dict()
A:transformers.models.clip.configuration_clip.output['vision_config']->self.vision_config.to_dict()
transformers.CLIPConfig(self,text_config_dict=None,vision_config_dict=None,projection_dim=512,**kwargs)
transformers.CLIPConfig.from_text_vision_configs(cls,text_config:CLIPTextConfig,vision_config:CLIPVisionConfig,**kwargs)
transformers.CLIPConfig.to_dict(self)
transformers.CLIPTextConfig(self,vocab_size=49408,hidden_size=512,intermediate_size=2048,num_hidden_layers=12,num_attention_heads=8,max_position_embeddings=77,hidden_act='quick_gelu',layer_norm_eps=1e-05,dropout=0.0,attention_dropout=0.0,initializer_range=0.02,initializer_factor=1.0,pad_token_id=1,bos_token_id=0,eos_token_id=2,gradient_checkpointing=False,**kwargs)
transformers.CLIPVisionConfig(self,hidden_size=768,intermediate_size=3072,num_hidden_layers=12,num_attention_heads=12,image_size=224,patch_size=32,hidden_act='quick_gelu',layer_norm_eps=1e-05,dropout=0.0,attention_dropout=0.0,initializer_range=0.02,initializer_factor=1.0,gradient_checkpointing=False,**kwargs)
transformers.models.clip.configuration_clip.CLIPConfig(self,text_config_dict=None,vision_config_dict=None,projection_dim=512,**kwargs)
transformers.models.clip.configuration_clip.CLIPConfig.__init__(self,text_config_dict=None,vision_config_dict=None,projection_dim=512,**kwargs)
transformers.models.clip.configuration_clip.CLIPConfig.from_text_vision_configs(cls,text_config:CLIPTextConfig,vision_config:CLIPVisionConfig,**kwargs)
transformers.models.clip.configuration_clip.CLIPConfig.to_dict(self)
transformers.models.clip.configuration_clip.CLIPTextConfig(self,vocab_size=49408,hidden_size=512,intermediate_size=2048,num_hidden_layers=12,num_attention_heads=8,max_position_embeddings=77,hidden_act='quick_gelu',layer_norm_eps=1e-05,dropout=0.0,attention_dropout=0.0,initializer_range=0.02,initializer_factor=1.0,pad_token_id=1,bos_token_id=0,eos_token_id=2,gradient_checkpointing=False,**kwargs)
transformers.models.clip.configuration_clip.CLIPTextConfig.__init__(self,vocab_size=49408,hidden_size=512,intermediate_size=2048,num_hidden_layers=12,num_attention_heads=8,max_position_embeddings=77,hidden_act='quick_gelu',layer_norm_eps=1e-05,dropout=0.0,attention_dropout=0.0,initializer_range=0.02,initializer_factor=1.0,pad_token_id=1,bos_token_id=0,eos_token_id=2,gradient_checkpointing=False,**kwargs)
transformers.models.clip.configuration_clip.CLIPVisionConfig(self,hidden_size=768,intermediate_size=3072,num_hidden_layers=12,num_attention_heads=12,image_size=224,patch_size=32,hidden_act='quick_gelu',layer_norm_eps=1e-05,dropout=0.0,attention_dropout=0.0,initializer_range=0.02,initializer_factor=1.0,gradient_checkpointing=False,**kwargs)
transformers.models.clip.configuration_clip.CLIPVisionConfig.__init__(self,hidden_size=768,intermediate_size=3072,num_hidden_layers=12,num_attention_heads=12,image_size=224,patch_size=32,hidden_act='quick_gelu',layer_norm_eps=1e-05,dropout=0.0,attention_dropout=0.0,initializer_range=0.02,initializer_factor=1.0,gradient_checkpointing=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/convert_clip_original_pytorch_to_hf.py----------------------------------------
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.(q_proj, k_proj, v_proj)->pt_attn_layer.in_proj_weight.chunk(3, dim=0)
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.(q_proj_bias, k_proj_bias, v_proj_bias)->pt_attn_layer.in_proj_bias.chunk(3, dim=0)
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.config->CLIPConfig(projection_dim=512, text_config={}, vision_config={})
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.hf_model->CLIPModel(config).eval()
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.(pt_model, _)->load(checkpoint_path, jit=False)
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.pt_model->pt_model.eval().eval()
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.input_ids->torch.arange(0, 77).unsqueeze(0)
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.pixel_values->torch.randn(1, 3, 224, 224)
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.(pt_logits_per_image, pt_logits_per_text)->pt_model(pixel_values, input_ids)
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.parser->argparse.ArgumentParser()
A:transformers.models.clip.convert_clip_original_pytorch_to_hf.args->argparse.ArgumentParser().parse_args()
transformers.models.clip.convert_clip_original_pytorch_to_hf.convert_clip_checkpoint(checkpoint_path,pytorch_dump_folder_path,config_path=None)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_attn_layer(hf_attn_layer,pt_attn_layer)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_encoder(hf_encoder,pt_model)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_layer(hf_layer,pt_layer)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_layers(hf_layers,pt_layers)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_linear(hf_linear,pt_linear)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_mlp(hf_mlp,pt_mlp)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_text_model_and_projection(hf_model,pt_model)
transformers.models.clip.convert_clip_original_pytorch_to_hf.copy_vison_model_and_projection(hf_model,pt_model)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/tokenization_clip_fast.py----------------------------------------
A:transformers.models.clip.tokenization_clip_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.clip.tokenization_clip_fast.pre_tok_state->json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())
A:transformers.models.clip.tokenization_clip_fast.pre_tok_class->getattr(pre_tokenizers, pre_tok_state.pop('type'))
A:transformers.models.clip.tokenization_clip_fast.self.backend_tokenizer.pre_tokenizer->pre_tok_class(**pre_tok_state)
A:transformers.models.clip.tokenization_clip_fast.is_split_into_words->kwargs.get('is_split_into_words', False)
A:transformers.models.clip.tokenization_clip_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
transformers.CLIPTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<|endoftext|>',bos_token='<|startoftext|>',eos_token='<|endoftext|>',pad_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.CLIPTokenizerFast._batch_encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.CLIPTokenizerFast._encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.CLIPTokenizerFast.pad_token_id(self)->Optional[int]
transformers.CLIPTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<|endoftext|>',bos_token='<|startoftext|>',eos_token='<|endoftext|>',pad_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast.__init__(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<|endoftext|>',bos_token='<|startoftext|>',eos_token='<|endoftext|>',pad_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast._batch_encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast._encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast.pad_token_id(self)->Optional[int]
transformers.models.clip.tokenization_clip_fast.CLIPTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/__init__.py----------------------------------------
A:transformers.models.clip.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/processing_clip.py----------------------------------------
A:transformers.models.clip.processing_clip.feature_extractor->feature_extraction_clip.CLIPFeatureExtractor.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.models.clip.processing_clip.tokenizer->tokenization_clip.CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.models.clip.processing_clip.encoding->self.tokenizer(text, return_tensors=return_tensors, **kwargs)
A:transformers.models.clip.processing_clip.image_features->self.feature_extractor(images, return_tensors=return_tensors, **kwargs)
transformers.CLIPProcessor(self,feature_extractor,tokenizer)
transformers.CLIPProcessor.batch_decode(self,*args,**kwargs)
transformers.CLIPProcessor.decode(self,*args,**kwargs)
transformers.CLIPProcessor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.CLIPProcessor.save_pretrained(self,save_directory)
transformers.models.clip.processing_clip.CLIPProcessor(self,feature_extractor,tokenizer)
transformers.models.clip.processing_clip.CLIPProcessor.__init__(self,feature_extractor,tokenizer)
transformers.models.clip.processing_clip.CLIPProcessor.batch_decode(self,*args,**kwargs)
transformers.models.clip.processing_clip.CLIPProcessor.decode(self,*args,**kwargs)
transformers.models.clip.processing_clip.CLIPProcessor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.clip.processing_clip.CLIPProcessor.save_pretrained(self,save_directory)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/clip/modeling_clip.py----------------------------------------
A:transformers.models.clip.modeling_clip.logger->utils.logging.get_logger(__name__)
A:transformers.models.clip.modeling_clip.(bsz, src_len)->mask.unsqueeze(1).size()
A:transformers.models.clip.modeling_clip.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.clip.modeling_clip.neg_ce->torch.diag(nn.functional.log_softmax(logits, dim=dim))
A:transformers.models.clip.modeling_clip.caption_loss->contrastive_loss(similarity, dim=0)
A:transformers.models.clip.modeling_clip.image_loss->contrastive_loss(similarity, dim=1)
A:transformers.models.clip.modeling_clip.self.class_embedding->torch.nn.Parameter(torch.randn(self.embed_dim))
A:transformers.models.clip.modeling_clip.self.patch_embedding->torch.nn.Conv2d(in_channels=3, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size, bias=False)
A:transformers.models.clip.modeling_clip.self.position_embedding->torch.nn.Embedding(config.max_position_embeddings, embed_dim)
A:transformers.models.clip.modeling_clip.patch_embeds->patch_embeds.flatten(2).transpose(1, 2).flatten(2).transpose(1, 2)
A:transformers.models.clip.modeling_clip.class_embeds->self.class_embedding.expand(batch_size, 1, -1)
A:transformers.models.clip.modeling_clip.embeddings->torch.cat([class_embeds, patch_embeds], dim=1)
A:transformers.models.clip.modeling_clip.self.token_embedding->torch.nn.Embedding(config.vocab_size, embed_dim)
A:transformers.models.clip.modeling_clip.inputs_embeds->self.token_embedding(input_ids)
A:transformers.models.clip.modeling_clip.position_embeddings->self.position_embedding(position_ids)
A:transformers.models.clip.modeling_clip.self.k_proj->torch.nn.Linear(self.embed_dim, self.embed_dim)
A:transformers.models.clip.modeling_clip.self.v_proj->torch.nn.Linear(self.embed_dim, self.embed_dim)
A:transformers.models.clip.modeling_clip.self.q_proj->torch.nn.Linear(self.embed_dim, self.embed_dim)
A:transformers.models.clip.modeling_clip.self.out_proj->torch.nn.Linear(self.embed_dim, self.embed_dim)
A:transformers.models.clip.modeling_clip.(bsz, tgt_len, embed_dim)->self.pre_layrnorm(hidden_states).size()
A:transformers.models.clip.modeling_clip.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.clip.modeling_clip.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.clip.modeling_clip.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.clip.modeling_clip.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.clip.modeling_clip.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.clip.modeling_clip.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.clip.modeling_clip.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.clip.modeling_clip.attn_output->self.out_proj(attn_output)
A:transformers.models.clip.modeling_clip.self.fc1->torch.nn.Linear(config.hidden_size, config.intermediate_size)
A:transformers.models.clip.modeling_clip.self.fc2->torch.nn.Linear(config.intermediate_size, config.hidden_size)
A:transformers.models.clip.modeling_clip.hidden_states->self.pre_layrnorm(hidden_states)
A:transformers.models.clip.modeling_clip.self.self_attn->CLIPAttention(config)
A:transformers.models.clip.modeling_clip.self.layer_norm1->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.clip.modeling_clip.self.mlp->CLIPMLP(config)
A:transformers.models.clip.modeling_clip.self.layer_norm2->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.clip.modeling_clip.(hidden_states, attn_weights)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, causal_attention_mask=causal_attention_mask, output_attentions=output_attentions)
A:transformers.models.clip.modeling_clip.self.layers->torch.nn.ModuleList([CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.clip.modeling_clip.layer_outputs->encoder_layer(hidden_states, attention_mask, causal_attention_mask, output_attentions=output_attentions)
A:transformers.models.clip.modeling_clip.self.embeddings->CLIPVisionEmbeddings(config)
A:transformers.models.clip.modeling_clip.self.encoder->CLIPEncoder(config)
A:transformers.models.clip.modeling_clip.self.final_layer_norm->torch.nn.LayerNorm(embed_dim)
A:transformers.models.clip.modeling_clip.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.models.clip.modeling_clip.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.clip.modeling_clip.causal_attention_mask->self._build_causal_attention_mask(bsz, seq_len).to(hidden_states.device)
A:transformers.models.clip.modeling_clip.attention_mask->_expand_mask(attention_mask, hidden_states.dtype)
A:transformers.models.clip.modeling_clip.encoder_outputs->self.encoder(inputs_embeds=hidden_states, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.clip.modeling_clip.last_hidden_state->self.final_layer_norm(last_hidden_state)
A:transformers.models.clip.modeling_clip.mask->mask.unsqueeze(1).unsqueeze(1)
A:transformers.models.clip.modeling_clip.self.text_model->CLIPTextTransformer(text_config)
A:transformers.models.clip.modeling_clip.self.pre_layrnorm->torch.nn.LayerNorm(embed_dim)
A:transformers.models.clip.modeling_clip.self.post_layernorm->torch.nn.LayerNorm(embed_dim)
A:transformers.models.clip.modeling_clip.pooled_output->self.post_layernorm(pooled_output)
A:transformers.models.clip.modeling_clip.self.vision_model->CLIPVisionTransformer(vision_config)
A:transformers.models.clip.modeling_clip.self.visual_projection->torch.nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)
A:transformers.models.clip.modeling_clip.self.text_projection->torch.nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)
A:transformers.models.clip.modeling_clip.self.logit_scale->torch.nn.Parameter(torch.ones([]))
A:transformers.models.clip.modeling_clip.text_outputs->self.text_model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.clip.modeling_clip.text_features->self.text_projection(pooled_output)
A:transformers.models.clip.modeling_clip.vision_outputs->self.vision_model(pixel_values=pixel_values, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.clip.modeling_clip.image_features->self.visual_projection(pooled_output)
A:transformers.models.clip.modeling_clip.image_embeds->self.visual_projection(image_embeds)
A:transformers.models.clip.modeling_clip.text_embeds->self.text_projection(text_embeds)
A:transformers.models.clip.modeling_clip.logit_scale->self.logit_scale.exp()
A:transformers.models.clip.modeling_clip.loss->clip_loss(logits_per_text)
transformers.CLIPModel(self,config:CLIPConfig)
transformers.CLIPModel.forward(self,input_ids=None,pixel_values=None,attention_mask=None,position_ids=None,return_loss=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CLIPModel.get_image_features(self,pixel_values=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CLIPModel.get_text_features(self,input_ids=None,attention_mask=None,position_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CLIPPreTrainedModel(PreTrainedModel)
transformers.CLIPPreTrainedModel._init_weights(self,module)
transformers.CLIPTextModel(self,config:CLIPTextConfig)
transformers.CLIPTextModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CLIPTextModel.get_input_embeddings(self)->nn.Module
transformers.CLIPTextModel.set_input_embeddings(self,value)
transformers.CLIPVisionModel(self,config:CLIPVisionConfig)
transformers.CLIPVisionModel.forward(self,pixel_values=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CLIPVisionModel.get_input_embeddings(self)->nn.Module
transformers.models.clip.modeling_clip.CLIPAttention(self,config)
transformers.models.clip.modeling_clip.CLIPAttention.__init__(self,config)
transformers.models.clip.modeling_clip.CLIPAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.clip.modeling_clip.CLIPAttention.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,causal_attention_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.clip.modeling_clip.CLIPEncoder(self,config:CLIPConfig)
transformers.models.clip.modeling_clip.CLIPEncoder.__init__(self,config:CLIPConfig)
transformers.models.clip.modeling_clip.CLIPEncoder.forward(self,inputs_embeds,attention_mask=None,causal_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPEncoderLayer(self,config:CLIPConfig)
transformers.models.clip.modeling_clip.CLIPEncoderLayer.__init__(self,config:CLIPConfig)
transformers.models.clip.modeling_clip.CLIPEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,causal_attention_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.clip.modeling_clip.CLIPMLP(self,config)
transformers.models.clip.modeling_clip.CLIPMLP.__init__(self,config)
transformers.models.clip.modeling_clip.CLIPMLP.forward(self,hidden_states)
transformers.models.clip.modeling_clip.CLIPModel(self,config:CLIPConfig)
transformers.models.clip.modeling_clip.CLIPModel.__init__(self,config:CLIPConfig)
transformers.models.clip.modeling_clip.CLIPModel.forward(self,input_ids=None,pixel_values=None,attention_mask=None,position_ids=None,return_loss=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPModel.get_image_features(self,pixel_values=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPModel.get_text_features(self,input_ids=None,attention_mask=None,position_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPOutput(ModelOutput)
transformers.models.clip.modeling_clip.CLIPOutput.to_tuple(self)->Tuple[Any]
transformers.models.clip.modeling_clip.CLIPPreTrainedModel(PreTrainedModel)
transformers.models.clip.modeling_clip.CLIPPreTrainedModel._init_weights(self,module)
transformers.models.clip.modeling_clip.CLIPTextEmbeddings(self,config:CLIPTextConfig)
transformers.models.clip.modeling_clip.CLIPTextEmbeddings.__init__(self,config:CLIPTextConfig)
transformers.models.clip.modeling_clip.CLIPTextEmbeddings.forward(self,input_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.clip.modeling_clip.CLIPTextModel(self,config:CLIPTextConfig)
transformers.models.clip.modeling_clip.CLIPTextModel.__init__(self,config:CLIPTextConfig)
transformers.models.clip.modeling_clip.CLIPTextModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPTextModel.get_input_embeddings(self)->nn.Module
transformers.models.clip.modeling_clip.CLIPTextModel.set_input_embeddings(self,value)
transformers.models.clip.modeling_clip.CLIPTextTransformer(self,config:CLIPTextConfig)
transformers.models.clip.modeling_clip.CLIPTextTransformer.__init__(self,config:CLIPTextConfig)
transformers.models.clip.modeling_clip.CLIPTextTransformer._build_causal_attention_mask(self,bsz,seq_len)
transformers.models.clip.modeling_clip.CLIPTextTransformer.forward(self,input_ids=None,attention_mask=None,position_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPVisionEmbeddings(self,config:CLIPVisionConfig)
transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.__init__(self,config:CLIPVisionConfig)
transformers.models.clip.modeling_clip.CLIPVisionEmbeddings.forward(self,pixel_values)
transformers.models.clip.modeling_clip.CLIPVisionModel(self,config:CLIPVisionConfig)
transformers.models.clip.modeling_clip.CLIPVisionModel.__init__(self,config:CLIPVisionConfig)
transformers.models.clip.modeling_clip.CLIPVisionModel.forward(self,pixel_values=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip.CLIPVisionModel.get_input_embeddings(self)->nn.Module
transformers.models.clip.modeling_clip.CLIPVisionTransformer(self,config:CLIPVisionConfig)
transformers.models.clip.modeling_clip.CLIPVisionTransformer.__init__(self,config:CLIPVisionConfig)
transformers.models.clip.modeling_clip.CLIPVisionTransformer.forward(self,pixel_values=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.clip.modeling_clip._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.clip.modeling_clip.clip_loss(similarity:torch.Tensor)->torch.Tensor
transformers.models.clip.modeling_clip.contrastive_loss(logits:torch.Tensor,dim:int)->torch.Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/tokenization_dpr.py----------------------------------------
A:transformers.models.dpr.tokenization_dpr.logger->utils.logging.get_logger(__name__)
A:transformers.models.dpr.tokenization_dpr.DPRSpanPrediction->collections.namedtuple('DPRSpanPrediction', ['span_score', 'relevance_score', 'doc_id', 'start_index', 'end_index', 'text'])
A:transformers.models.dpr.tokenization_dpr.DPRReaderOutput->collections.namedtuple('DPRReaderOutput', ['start_logits', 'end_logits', 'relevance_logits'])
A:transformers.models.dpr.tokenization_dpr.n_passages->len(relevance_logits)
A:transformers.models.dpr.tokenization_dpr.sorted_docs->sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)
A:transformers.models.dpr.tokenization_dpr.sequence_ids->list(input_ids[doc_id])
A:transformers.models.dpr.tokenization_dpr.sequence_len->len(sequence_ids)
A:transformers.models.dpr.tokenization_dpr.best_spans->self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)
A:transformers.models.dpr.tokenization_dpr.scores->sorted(scores, key=lambda x: x[1], reverse=True)
transformers.DPRContextEncoderTokenizer(BertTokenizer)
transformers.DPRQuestionEncoderTokenizer(BertTokenizer)
transformers.DPRReaderTokenizer(CustomDPRReaderTokenizerMixin,BertTokenizer)
transformers.models.dpr.tokenization_dpr.CustomDPRReaderTokenizerMixin(self,questions,titles:Optional[str]=None,texts:Optional[str]=None,padding:Union[bool,str]=False,truncation:Union[bool,str]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_attention_mask:Optional[bool]=None,**kwargs)
transformers.models.dpr.tokenization_dpr.CustomDPRReaderTokenizerMixin.__call__(self,questions,titles:Optional[str]=None,texts:Optional[str]=None,padding:Union[bool,str]=False,truncation:Union[bool,str]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_attention_mask:Optional[bool]=None,**kwargs)
transformers.models.dpr.tokenization_dpr.CustomDPRReaderTokenizerMixin._get_best_spans(self,start_logits:List[int],end_logits:List[int],max_answer_length:int,top_spans:int)->List[DPRSpanPrediction]
transformers.models.dpr.tokenization_dpr.CustomDPRReaderTokenizerMixin.decode_best_spans(self,reader_input:BatchEncoding,reader_output:DPRReaderOutput,num_spans:int=16,max_answer_length:int=64,num_spans_per_passage:int=4)->List[DPRSpanPrediction]
transformers.models.dpr.tokenization_dpr.DPRContextEncoderTokenizer(BertTokenizer)
transformers.models.dpr.tokenization_dpr.DPRQuestionEncoderTokenizer(BertTokenizer)
transformers.models.dpr.tokenization_dpr.DPRReaderTokenizer(CustomDPRReaderTokenizerMixin,BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/modeling_dpr.py----------------------------------------
A:transformers.models.dpr.modeling_dpr.logger->utils.logging.get_logger(__name__)
A:transformers.models.dpr.modeling_dpr.self.bert_model->BertModel(config)
A:transformers.models.dpr.modeling_dpr.self.encode_proj->torch.nn.Linear(self.bert_model.config.hidden_size, config.projection_dim)
A:transformers.models.dpr.modeling_dpr.outputs->self.question_encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.dpr.modeling_dpr.pooled_output->self.encode_proj(pooled_output)
A:transformers.models.dpr.modeling_dpr.self.encoder->DPREncoder(config)
A:transformers.models.dpr.modeling_dpr.self.qa_outputs->torch.nn.Linear(self.encoder.embeddings_size, 2)
A:transformers.models.dpr.modeling_dpr.self.qa_classifier->torch.nn.Linear(self.encoder.embeddings_size, 1)
A:transformers.models.dpr.modeling_dpr.logits->self.qa_outputs(sequence_output)
A:transformers.models.dpr.modeling_dpr.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.dpr.modeling_dpr.start_logits->start_logits.view(n_passages, sequence_length).view(n_passages, sequence_length)
A:transformers.models.dpr.modeling_dpr.end_logits->end_logits.view(n_passages, sequence_length).view(n_passages, sequence_length)
A:transformers.models.dpr.modeling_dpr.relevance_logits->relevance_logits.view(n_passages).view(n_passages)
A:transformers.models.dpr.modeling_dpr.self.ctx_encoder->DPREncoder(config)
A:transformers.models.dpr.modeling_dpr.input_shape->input_ids.size()
A:transformers.models.dpr.modeling_dpr.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.dpr.modeling_dpr.self.question_encoder->DPREncoder(config)
A:transformers.models.dpr.modeling_dpr.self.span_predictor->DPRSpanPredictor(config)
A:transformers.models.dpr.modeling_dpr.attention_mask->torch.ones(input_shape, device=device)
transformers.DPRContextEncoder(self,config:DPRConfig)
transformers.DPRContextEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRContextEncoderOutput, Tuple[Tensor, ...]]
transformers.DPRContextEncoderOutput(ModelOutput)
transformers.DPRPretrainedContextEncoder(PreTrainedModel)
transformers.DPRPretrainedContextEncoder.init_weights(self)
transformers.DPRPretrainedQuestionEncoder(PreTrainedModel)
transformers.DPRPretrainedQuestionEncoder.init_weights(self)
transformers.DPRPretrainedReader(PreTrainedModel)
transformers.DPRPretrainedReader.init_weights(self)
transformers.DPRQuestionEncoder(self,config:DPRConfig)
transformers.DPRQuestionEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRQuestionEncoderOutput, Tuple[Tensor, ...]]
transformers.DPRQuestionEncoderOutput(ModelOutput)
transformers.DPRReader(self,config:DPRConfig)
transformers.DPRReader.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict=None)->Union[DPRReaderOutput, Tuple[Tensor, ...]]
transformers.DPRReaderOutput(ModelOutput)
transformers.models.dpr.modeling_dpr.DPRContextEncoder(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRContextEncoder.__init__(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRContextEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRContextEncoderOutput, Tuple[Tensor, ...]]
transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput(ModelOutput)
transformers.models.dpr.modeling_dpr.DPREncoder(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPREncoder.__init__(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPREncoder.embeddings_size(self)->int
transformers.models.dpr.modeling_dpr.DPREncoder.forward(self,input_ids:Tensor,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False)->Union[BaseModelOutputWithPooling, Tuple[Tensor, ...]]
transformers.models.dpr.modeling_dpr.DPREncoder.init_weights(self)
transformers.models.dpr.modeling_dpr.DPRPretrainedContextEncoder(PreTrainedModel)
transformers.models.dpr.modeling_dpr.DPRPretrainedContextEncoder.init_weights(self)
transformers.models.dpr.modeling_dpr.DPRPretrainedQuestionEncoder(PreTrainedModel)
transformers.models.dpr.modeling_dpr.DPRPretrainedQuestionEncoder.init_weights(self)
transformers.models.dpr.modeling_dpr.DPRPretrainedReader(PreTrainedModel)
transformers.models.dpr.modeling_dpr.DPRPretrainedReader.init_weights(self)
transformers.models.dpr.modeling_dpr.DPRQuestionEncoder(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRQuestionEncoder.__init__(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRQuestionEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRQuestionEncoderOutput, Tuple[Tensor, ...]]
transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput(ModelOutput)
transformers.models.dpr.modeling_dpr.DPRReader(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRReader.__init__(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRReader.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict=None)->Union[DPRReaderOutput, Tuple[Tensor, ...]]
transformers.models.dpr.modeling_dpr.DPRReaderOutput(ModelOutput)
transformers.models.dpr.modeling_dpr.DPRSpanPredictor(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRSpanPredictor.__init__(self,config:DPRConfig)
transformers.models.dpr.modeling_dpr.DPRSpanPredictor.forward(self,input_ids:Tensor,attention_mask:Tensor,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False)->Union[DPRReaderOutput, Tuple[Tensor, ...]]
transformers.models.dpr.modeling_dpr.DPRSpanPredictor.init_weights(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.CheckpointState->collections.namedtuple('CheckpointState', ['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.state_dict->torch.load(model_file, map_location=lambda s, l: default_restore_location(s, 'cpu'))
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.model->DPRState.from_type(comp_type, src_file=src_file).load_dpr_model()
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.saved_state->load_states_from_checkpoint(self.src_file)
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.dest_dir->Path(dest_dir)
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.dpr_state->DPRState.from_type(comp_type, src_file=src_file)
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.src_file->Path(args.src)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRContextEncoderState(DPRState)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRContextEncoderState.load_dpr_model(self)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRQuestionEncoderState(DPRState)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRQuestionEncoderState.load_dpr_model(self)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRReaderState(DPRState)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRReaderState.load_dpr_model(self)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRState(self,src_file:Path)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRState.__init__(self,src_file:Path)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRState.from_type(comp_type:str,*args,**kwargs)->'DPRState'
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.DPRState.load_dpr_model(self)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.convert(comp_type:str,src_file:Path,dest_dir:Path)
transformers.models.dpr.convert_dpr_original_checkpoint_to_pytorch.load_states_from_checkpoint(model_file:str)->CheckpointState


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/configuration_dpr.py----------------------------------------
A:transformers.models.dpr.configuration_dpr.logger->utils.logging.get_logger(__name__)
transformers.DPRConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',projection_dim:int=0,**kwargs)
transformers.models.dpr.configuration_dpr.DPRConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',projection_dim:int=0,**kwargs)
transformers.models.dpr.configuration_dpr.DPRConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',projection_dim:int=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/__init__.py----------------------------------------
A:transformers.models.dpr.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/tokenization_dpr_fast.py----------------------------------------
A:transformers.models.dpr.tokenization_dpr_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.dpr.tokenization_dpr_fast.DPRSpanPrediction->collections.namedtuple('DPRSpanPrediction', ['span_score', 'relevance_score', 'doc_id', 'start_index', 'end_index', 'text'])
A:transformers.models.dpr.tokenization_dpr_fast.DPRReaderOutput->collections.namedtuple('DPRReaderOutput', ['start_logits', 'end_logits', 'relevance_logits'])
A:transformers.models.dpr.tokenization_dpr_fast.n_passages->len(relevance_logits)
A:transformers.models.dpr.tokenization_dpr_fast.sorted_docs->sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)
A:transformers.models.dpr.tokenization_dpr_fast.sequence_ids->list(input_ids[doc_id])
A:transformers.models.dpr.tokenization_dpr_fast.sequence_len->len(sequence_ids)
A:transformers.models.dpr.tokenization_dpr_fast.best_spans->self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)
A:transformers.models.dpr.tokenization_dpr_fast.scores->sorted(scores, key=lambda x: x[1], reverse=True)
transformers.DPRContextEncoderTokenizerFast(BertTokenizerFast)
transformers.DPRQuestionEncoderTokenizerFast(BertTokenizerFast)
transformers.DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin,BertTokenizerFast)
transformers.models.dpr.tokenization_dpr_fast.CustomDPRReaderTokenizerMixin(self,questions,titles:Optional[str]=None,texts:Optional[str]=None,padding:Union[bool,str]=False,truncation:Union[bool,str]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_attention_mask:Optional[bool]=None,**kwargs)
transformers.models.dpr.tokenization_dpr_fast.CustomDPRReaderTokenizerMixin.__call__(self,questions,titles:Optional[str]=None,texts:Optional[str]=None,padding:Union[bool,str]=False,truncation:Union[bool,str]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_attention_mask:Optional[bool]=None,**kwargs)
transformers.models.dpr.tokenization_dpr_fast.CustomDPRReaderTokenizerMixin._get_best_spans(self,start_logits:List[int],end_logits:List[int],max_answer_length:int,top_spans:int)->List[DPRSpanPrediction]
transformers.models.dpr.tokenization_dpr_fast.CustomDPRReaderTokenizerMixin.decode_best_spans(self,reader_input:BatchEncoding,reader_output:DPRReaderOutput,num_spans:int=16,max_answer_length:int=64,num_spans_per_passage:int=4)->List[DPRSpanPrediction]
transformers.models.dpr.tokenization_dpr_fast.DPRContextEncoderTokenizerFast(BertTokenizerFast)
transformers.models.dpr.tokenization_dpr_fast.DPRQuestionEncoderTokenizerFast(BertTokenizerFast)
transformers.models.dpr.tokenization_dpr_fast.DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin,BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dpr/modeling_tf_dpr.py----------------------------------------
A:transformers.models.dpr.modeling_tf_dpr.logger->utils.logging.get_logger(__name__)
A:transformers.models.dpr.modeling_tf_dpr.self.bert_model->TFBertMainLayer(config, name='bert_model')
A:transformers.models.dpr.modeling_tf_dpr.self.encode_proj->tensorflow.keras.layers.Dense(config.projection_dim, kernel_initializer=get_initializer(config.initializer_range), name='encode_proj')
A:transformers.models.dpr.modeling_tf_dpr.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training, kwargs_call=kwargs)
A:transformers.models.dpr.modeling_tf_dpr.outputs->self.question_encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.dpr.modeling_tf_dpr.pooled_output->self.encode_proj(pooled_output)
A:transformers.models.dpr.modeling_tf_dpr.self.encoder->TFDPREncoderLayer(config)
A:transformers.models.dpr.modeling_tf_dpr.self.qa_outputs->tensorflow.keras.layers.Dense(2, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.dpr.modeling_tf_dpr.self.qa_classifier->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='qa_classifier')
A:transformers.models.dpr.modeling_tf_dpr.logits->self.qa_outputs(sequence_output)
A:transformers.models.dpr.modeling_tf_dpr.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.dpr.modeling_tf_dpr.start_logits->tensorflow.reshape(start_logits, [n_passages, sequence_length])
A:transformers.models.dpr.modeling_tf_dpr.end_logits->tensorflow.reshape(end_logits, [n_passages, sequence_length])
A:transformers.models.dpr.modeling_tf_dpr.relevance_logits->tensorflow.reshape(relevance_logits, [n_passages])
A:transformers.models.dpr.modeling_tf_dpr.output->self.call(inputs)
A:transformers.models.dpr.modeling_tf_dpr.self.ctx_encoder->TFDPREncoderLayer(config, name='ctx_encoder')
A:transformers.models.dpr.modeling_tf_dpr.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.dpr.modeling_tf_dpr.inputs['token_type_ids']->tensorflow.zeros(input_shape, dtype=tf.dtypes.int32)
A:transformers.models.dpr.modeling_tf_dpr.self.question_encoder->TFDPREncoderLayer(config, name='question_encoder')
A:transformers.models.dpr.modeling_tf_dpr.self.span_predictor->TFDPRSpanPredictorLayer(config, name='span_predictor')
A:transformers.models.dpr.modeling_tf_dpr.inputs['attention_mask']->tensorflow.ones(input_shape, dtype=tf.dtypes.int32)
transformers.TFDPRContextEncoder(self,config:DPRConfig,*args,**kwargs)
transformers.TFDPRContextEncoder.call(self,input_ids=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None,training:bool=False,**kwargs)->Union[TFDPRContextEncoderOutput, Tuple[tf.Tensor, ...]]
transformers.TFDPRContextEncoder.get_input_embeddings(self)
transformers.TFDPRContextEncoder.serving_output(self,output)
transformers.TFDPRContextEncoderOutput(ModelOutput)
transformers.TFDPRPretrainedContextEncoder(TFPreTrainedModel)
transformers.TFDPRPretrainedQuestionEncoder(TFPreTrainedModel)
transformers.TFDPRPretrainedReader(TFPreTrainedModel)
transformers.TFDPRPretrainedReader.serving(self,inputs)
transformers.TFDPRQuestionEncoder(self,config:DPRConfig,*args,**kwargs)
transformers.TFDPRQuestionEncoder.call(self,input_ids=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None,training:bool=False,**kwargs)->Union[TFDPRQuestionEncoderOutput, Tuple[tf.Tensor, ...]]
transformers.TFDPRQuestionEncoder.get_input_embeddings(self)
transformers.TFDPRQuestionEncoder.serving_output(self,output)
transformers.TFDPRQuestionEncoderOutput(ModelOutput)
transformers.TFDPRReader(self,config:DPRConfig,*args,**kwargs)
transformers.TFDPRReader.call(self,input_ids=None,attention_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict=None,training:bool=False,**kwargs)->Union[TFDPRReaderOutput, Tuple[tf.Tensor, ...]]
transformers.TFDPRReader.get_input_embeddings(self)
transformers.TFDPRReader.serving_output(self,output)
transformers.TFDPRReaderOutput(ModelOutput)
transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoder(self,config:DPRConfig,*args,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoder.__init__(self,config:DPRConfig,*args,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoder.call(self,input_ids=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None,training:bool=False,**kwargs)->Union[TFDPRContextEncoderOutput, Tuple[tf.Tensor, ...]]
transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoder.get_input_embeddings(self)
transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoder.serving_output(self,output)
transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput(ModelOutput)
transformers.models.dpr.modeling_tf_dpr.TFDPREncoder(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPREncoder.__init__(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPREncoder.call(self,input_ids:tf.Tensor=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False,training:bool=False,**kwargs)->Union[TFDPRReaderOutput, Tuple[tf.Tensor, ...]]
transformers.models.dpr.modeling_tf_dpr.TFDPREncoderLayer(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPREncoderLayer.__init__(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPREncoderLayer.call(self,input_ids:tf.Tensor=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict:bool=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor, ...]]
transformers.models.dpr.modeling_tf_dpr.TFDPREncoderLayer.embeddings_size(self)->int
transformers.models.dpr.modeling_tf_dpr.TFDPRPretrainedContextEncoder(TFPreTrainedModel)
transformers.models.dpr.modeling_tf_dpr.TFDPRPretrainedQuestionEncoder(TFPreTrainedModel)
transformers.models.dpr.modeling_tf_dpr.TFDPRPretrainedReader(TFPreTrainedModel)
transformers.models.dpr.modeling_tf_dpr.TFDPRPretrainedReader.serving(self,inputs)
transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoder(self,config:DPRConfig,*args,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoder.__init__(self,config:DPRConfig,*args,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoder.call(self,input_ids=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None,training:bool=False,**kwargs)->Union[TFDPRQuestionEncoderOutput, Tuple[tf.Tensor, ...]]
transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoder.get_input_embeddings(self)
transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoder.serving_output(self,output)
transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput(ModelOutput)
transformers.models.dpr.modeling_tf_dpr.TFDPRReader(self,config:DPRConfig,*args,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRReader.__init__(self,config:DPRConfig,*args,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRReader.call(self,input_ids=None,attention_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict=None,training:bool=False,**kwargs)->Union[TFDPRReaderOutput, Tuple[tf.Tensor, ...]]
transformers.models.dpr.modeling_tf_dpr.TFDPRReader.get_input_embeddings(self)
transformers.models.dpr.modeling_tf_dpr.TFDPRReader.serving_output(self,output)
transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput(ModelOutput)
transformers.models.dpr.modeling_tf_dpr.TFDPRSpanPredictor(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRSpanPredictor.__init__(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRSpanPredictor.call(self,input_ids:tf.Tensor=None,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False,training:bool=False,**kwargs)->Union[TFDPRReaderOutput, Tuple[tf.Tensor, ...]]
transformers.models.dpr.modeling_tf_dpr.TFDPRSpanPredictorLayer(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRSpanPredictorLayer.__init__(self,config:DPRConfig,**kwargs)
transformers.models.dpr.modeling_tf_dpr.TFDPRSpanPredictorLayer.call(self,input_ids:tf.Tensor=None,attention_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False,training:bool=False,**kwargs)->Union[TFDPRReaderOutput, Tuple[tf.Tensor, ...]]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/reformer/tokenization_reformer_fast.py----------------------------------------
A:transformers.models.reformer.tokenization_reformer_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.reformer.tokenization_reformer_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.ReformerTokenizerFast(self,vocab_file,tokenizer_file=None,eos_token='</s>',unk_token='<unk>',additional_special_tokens=[],**kwargs)
transformers.ReformerTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.reformer.tokenization_reformer_fast.ReformerTokenizerFast(self,vocab_file,tokenizer_file=None,eos_token='</s>',unk_token='<unk>',additional_special_tokens=[],**kwargs)
transformers.models.reformer.tokenization_reformer_fast.ReformerTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,eos_token='</s>',unk_token='<unk>',additional_special_tokens=[],**kwargs)
transformers.models.reformer.tokenization_reformer_fast.ReformerTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/reformer/tokenization_reformer.py----------------------------------------
A:transformers.models.reformer.tokenization_reformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.reformer.tokenization_reformer.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.reformer.tokenization_reformer.state->self.__dict__.copy()
A:transformers.models.reformer.tokenization_reformer.token->self.sp_model.IdToPiece(index)
A:transformers.models.reformer.tokenization_reformer.out_string->self.sp_model.decode_pieces(tokens)
A:transformers.models.reformer.tokenization_reformer.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.ReformerTokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',additional_special_tokens=[],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.ReformerTokenizer.__getstate__(self)
transformers.ReformerTokenizer.__setstate__(self,d)
transformers.ReformerTokenizer._convert_id_to_token(self,index)
transformers.ReformerTokenizer._convert_token_to_id(self,token)
transformers.ReformerTokenizer._tokenize(self,text:str)->List[str]
transformers.ReformerTokenizer.convert_tokens_to_string(self,tokens)
transformers.ReformerTokenizer.get_vocab(self)->Dict[str, int]
transformers.ReformerTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.ReformerTokenizer.vocab_size(self)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',additional_special_tokens=[],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.__getstate__(self)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.__init__(self,vocab_file,eos_token='</s>',unk_token='<unk>',additional_special_tokens=[],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.__setstate__(self,d)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer._convert_id_to_token(self,index)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer._convert_token_to_id(self,token)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer._tokenize(self,text:str)->List[str]
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.get_vocab(self)->Dict[str, int]
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.reformer.tokenization_reformer.ReformerTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.torch_layer.weight->torch.nn.Parameter(weight)
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.torch_layer.bias->torch.nn.Parameter(bias)
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.np_query_key->numpy.asarray(weights[0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.np_value->numpy.asarray(weights[2])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.np_dense->numpy.asarray(weights[3])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.np_query->numpy.asarray(weights[0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.np_key->numpy.asarray(weights[1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_1_weight->numpy.asarray(layer_norm_1[0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_1_bias->numpy.asarray(layer_norm_1[1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_2_weight->numpy.asarray(intermediate_weights[0][0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_2_bias->numpy.asarray(intermediate_weights[0][1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.inter_dense_weight->numpy.asarray(intermediate_weights[1][0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.inter_dense_bias->numpy.asarray(intermediate_weights[1][1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.out_dense_weight->numpy.asarray(intermediate_weights[4][0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.out_dense_bias->numpy.asarray(intermediate_weights[4][1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.word_embeddings->numpy.asarray(weights[1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.emb_weights->numpy.asarray(weights[3][emb_idx][0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.position_embeddings.weights[emb_idx]->torch.nn.Parameter(torch.tensor(emb_weights))
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_out_weight->numpy.asarray(weights[7][0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_out_bias->numpy.asarray(weights[7][1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.output_embed_weights->numpy.asarray(weights[9][0])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.output_embed_bias->numpy.asarray(weights[9][1])
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.config->transformers.ReformerConfig.from_json_file(config_file)
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.model->ReformerModelWithLMHead(config)
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.convert_trax_checkpoint_to_pytorch(trax_model_pkl_path,config_file,pytorch_dump_path)
transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.set_block_weights_in_torch(weights,torch_block,hidden_size)
transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.set_layer_weights_in_torch_local(weights,torch_layer,hidden_size)
transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.set_layer_weights_in_torch_lsh(weights,torch_layer,hidden_size)
transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.set_model_weights_in_torch(weights,torch_model,hidden_size)
transformers.models.reformer.convert_reformer_trax_checkpoint_to_pytorch.set_param(torch_layer,weight,bias=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/reformer/configuration_reformer.py----------------------------------------
A:transformers.models.reformer.configuration_reformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.reformer.configuration_reformer.self.num_hidden_layers->len(attn_layers)
A:transformers.models.reformer.configuration_reformer.self.axial_pos_shape->tuple(axial_pos_shape)
A:transformers.models.reformer.configuration_reformer.self.axial_pos_embds_dim->tuple(axial_pos_embds_dim)
transformers.ReformerConfig(self,attention_head_size=64,attn_layers=['local','lsh','local','lsh','local','lsh'],axial_norm_std=1.0,axial_pos_embds=True,axial_pos_shape=[64,64],axial_pos_embds_dim=[64,192],chunk_size_lm_head=0,eos_token_id=2,feed_forward_size=512,hash_seed=None,hidden_act='relu',hidden_dropout_prob=0.05,hidden_size=256,initializer_range=0.02,is_decoder=False,layer_norm_eps=1e-12,local_num_chunks_before=1,local_num_chunks_after=0,local_attention_probs_dropout_prob=0.05,local_attn_chunk_length=64,lsh_attn_chunk_length=64,lsh_attention_probs_dropout_prob=0.0,lsh_num_chunks_before=1,lsh_num_chunks_after=0,max_position_embeddings=4096,num_attention_heads=12,num_buckets=None,num_hashes=1,pad_token_id=0,vocab_size=320,tie_word_embeddings=False,use_cache=True,**kwargs)
transformers.models.reformer.configuration_reformer.ReformerConfig(self,attention_head_size=64,attn_layers=['local','lsh','local','lsh','local','lsh'],axial_norm_std=1.0,axial_pos_embds=True,axial_pos_shape=[64,64],axial_pos_embds_dim=[64,192],chunk_size_lm_head=0,eos_token_id=2,feed_forward_size=512,hash_seed=None,hidden_act='relu',hidden_dropout_prob=0.05,hidden_size=256,initializer_range=0.02,is_decoder=False,layer_norm_eps=1e-12,local_num_chunks_before=1,local_num_chunks_after=0,local_attention_probs_dropout_prob=0.05,local_attn_chunk_length=64,lsh_attn_chunk_length=64,lsh_attention_probs_dropout_prob=0.0,lsh_num_chunks_before=1,lsh_num_chunks_after=0,max_position_embeddings=4096,num_attention_heads=12,num_buckets=None,num_hashes=1,pad_token_id=0,vocab_size=320,tie_word_embeddings=False,use_cache=True,**kwargs)
transformers.models.reformer.configuration_reformer.ReformerConfig.__init__(self,attention_head_size=64,attn_layers=['local','lsh','local','lsh','local','lsh'],axial_norm_std=1.0,axial_pos_embds=True,axial_pos_shape=[64,64],axial_pos_embds_dim=[64,192],chunk_size_lm_head=0,eos_token_id=2,feed_forward_size=512,hash_seed=None,hidden_act='relu',hidden_dropout_prob=0.05,hidden_size=256,initializer_range=0.02,is_decoder=False,layer_norm_eps=1e-12,local_num_chunks_before=1,local_num_chunks_after=0,local_attention_probs_dropout_prob=0.05,local_attn_chunk_length=64,lsh_attn_chunk_length=64,lsh_attention_probs_dropout_prob=0.0,lsh_num_chunks_before=1,lsh_num_chunks_after=0,max_position_embeddings=4096,num_attention_heads=12,num_buckets=None,num_hashes=1,pad_token_id=0,vocab_size=320,tie_word_embeddings=False,use_cache=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/reformer/__init__.py----------------------------------------
A:transformers.models.reformer.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/reformer/modeling_reformer.py----------------------------------------
A:transformers.models.reformer.modeling_reformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.reformer.modeling_reformer.LSHSelfAttentionOutput->namedtuple('LSHSelfAttentionOutput', ['hidden_states', 'attention_probs', 'buckets'])
A:transformers.models.reformer.modeling_reformer.LocalSelfAttentionOutput->namedtuple('LocalSelfAttentionOutput', ['hidden_states', 'attention_probs'])
A:transformers.models.reformer.modeling_reformer.AttentionOutput->namedtuple('AttentionOutput', ['hidden_states', 'attention_probs', 'buckets'])
A:transformers.models.reformer.modeling_reformer.ReformerOutput->namedtuple('ReformerOutput', ['hidden_states', 'attn_output', 'attention_probs', 'buckets'])
A:transformers.models.reformer.modeling_reformer.ReformerBackwardOutput->namedtuple('ReformerBackwardOutput', ['attn_output', 'hidden_states', 'grad_attn_output', 'grad_hidden_states'])
A:transformers.models.reformer.modeling_reformer.ReformerEncoderOutput->namedtuple('ReformerEncoderOutput', ['hidden_states', 'all_hidden_states', 'all_attentions', 'past_buckets_states'])
A:transformers.models.reformer.modeling_reformer.scale_offset->scale_offset.expand(vector.shape).expand(vector.shape)
A:transformers.models.reformer.modeling_reformer.attn_types_set->set(attn_types)
A:transformers.models.reformer.modeling_reformer.self.least_common_mult_chunk_length->_get_least_common_mult_chunk_len(config)
A:transformers.models.reformer.modeling_reformer.self.weights->torch.nn.ParameterList()
A:transformers.models.reformer.modeling_reformer.weights->torch.cat(broadcasted_weights, dim=-1)
A:transformers.models.reformer.modeling_reformer.transposed_weights->torch.cat(broadcasted_weights, dim=-1).transpose(2, 1)
A:transformers.models.reformer.modeling_reformer.dropped_transposed_weights->torch.nn.functional.dropout2d(transposed_weights, p=self.dropout, training=self.training)
A:transformers.models.reformer.modeling_reformer.dropped_weights->torch.nn.functional.dropout2d(transposed_weights, p=self.dropout, training=self.training).transpose(2, 1)
A:transformers.models.reformer.modeling_reformer.position_encodings->torch.cat([torch.index_select(position_encodings[i], 0, position_ids[i]).unsqueeze(0) for i in range(batch_size)], dim=0)
A:transformers.models.reformer.modeling_reformer.max_position_id->torch.cat([position_ids, padded_position_ids], dim=-1).max().item()
A:transformers.models.reformer.modeling_reformer.self.embedding->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.reformer.modeling_reformer.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.reformer.modeling_reformer.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size)
A:transformers.models.reformer.modeling_reformer.input_shape->torch.cat([inputs_embeds, padded_inputs_embeds], dim=-2).size()
A:transformers.models.reformer.modeling_reformer.position_ids->torch.cat([position_ids, padded_position_ids], dim=-1)
A:transformers.models.reformer.modeling_reformer.inputs_embeds->torch.cat([inputs_embeds, padded_inputs_embeds], dim=-2)
A:transformers.models.reformer.modeling_reformer.embeddings->torch.nn.functional.dropout(inputs_embeds, p=self.dropout, training=self.training)
A:transformers.models.reformer.modeling_reformer.x->x.permute(0, 2, 1, 3).permute(0, 2, 1, 3)
A:transformers.models.reformer.modeling_reformer.self.query_key->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.models.reformer.modeling_reformer.self.value->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.models.reformer.modeling_reformer.query_vectors->self._split_seq_length_dim_to(query_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size)
A:transformers.models.reformer.modeling_reformer.(key_value_hidden_states, sorted_bucket_idx, buckets)->self._get_relevant_hid_states_and_buckets(query_vectors=query_vectors, attention_mask=attention_mask, num_hashes=num_hashes, hidden_states=hidden_states, past_states=past_states, past_buckets=past_buckets)
A:transformers.models.reformer.modeling_reformer.query_key_vectors->torch.einsum('balh,ahr->balr', hidden_states, per_head_query_key)
A:transformers.models.reformer.modeling_reformer.value_vectors->self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
A:transformers.models.reformer.modeling_reformer.key_value_hidden_states->torch.cat([key_value_hidden_states, hidden_states], dim=1)
A:transformers.models.reformer.modeling_reformer.buckets->torch.where(buckets_mask, buckets, torch.tensor(num_buckets - 1, dtype=torch.long, device=buckets.device))
A:transformers.models.reformer.modeling_reformer.(sorted_bucket_idx, undo_sorted_bucket_idx)->self._get_sorted_bucket_idx_and_undo_sorted_bucket_idx(sequence_length, buckets, num_hashes)
A:transformers.models.reformer.modeling_reformer.sorted_bucket_idx_per_hash->torch.arange(sequence_length, device=query_key_vectors.device).repeat(batch_size, self.num_attention_heads, 1)
A:transformers.models.reformer.modeling_reformer.key_vectors->self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
A:transformers.models.reformer.modeling_reformer.(out_vectors, logits, attention_probs)->self._attend(query_vectors=query_vectors, key_vectors=key_vectors, value_vectors=value_vectors, sorted_bucket_idx_per_hash=sorted_bucket_idx_per_hash, attention_mask=attention_mask, head_mask=head_mask, do_standard_self_attention=do_standard_self_attention, do_cached_attention=do_cached_attention)
A:transformers.models.reformer.modeling_reformer.(out_vectors, logits)->ReverseSort.apply(out_vectors, logits, sorted_bucket_idx, undo_sorted_bucket_idx)
A:transformers.models.reformer.modeling_reformer.out_vectors->self._merge_hidden_size_dims(out_vectors, self.num_attention_heads, self.attention_head_size)
A:transformers.models.reformer.modeling_reformer.logits->self.qa_outputs(sequence_output)
A:transformers.models.reformer.modeling_reformer.probs_vectors->torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
A:transformers.models.reformer.modeling_reformer.per_head_query_key->self.query_key.weight.reshape(self.num_attention_heads, self.attention_head_size, self.hidden_size).transpose(-2, -1)
A:transformers.models.reformer.modeling_reformer.per_head_value->self.value.weight.reshape(self.num_attention_heads, self.attention_head_size, self.hidden_size).transpose(-2, -1)
A:transformers.models.reformer.modeling_reformer.vectors->vectors.repeat(1, 1, num_hashes, 1).repeat(1, 1, num_hashes, 1)
A:transformers.models.reformer.modeling_reformer.random_rotations->torch.randn(rotations_shape, device=vectors.device, dtype=vectors.dtype)
A:transformers.models.reformer.modeling_reformer.rotated_vectors->torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
A:transformers.models.reformer.modeling_reformer.rotated_vectors_factor->torch.cat([rotated_vectors_factor, -rotated_vectors_factor], dim=-1)
A:transformers.models.reformer.modeling_reformer.buckets_mask->torch.cat([torch.ones(input_shape, device=device, dtype=torch.uint8), torch.zeros((input_shape[0], padding_length), device=device, dtype=torch.uint8)], dim=-1).to(torch.uint8)[:, None, None, :].expand(buckets.shape)
A:transformers.models.reformer.modeling_reformer.offsets->offsets.expand((batch_size, self.num_attention_heads) + offsets.shape[-2:]).expand((batch_size, self.num_attention_heads) + offsets.shape[-2:])
A:transformers.models.reformer.modeling_reformer.offset_buckets->(buckets + offsets).flatten(start_dim=2, end_dim=3)
A:transformers.models.reformer.modeling_reformer.sorted_bucket_idx->_stable_argsort(buckets, dim=-1)
A:transformers.models.reformer.modeling_reformer.indices->torch.arange(sequence_length, device=query_vectors.device).repeat(batch_size, self.num_attention_heads, 1)
A:transformers.models.reformer.modeling_reformer.undo_sorted_bucket_idx->_stable_argsort(buckets, dim=-1).new(*sorted_bucket_idx.size())
A:transformers.models.reformer.modeling_reformer.query_key_dots->torch.where(mask, query_key_dots, mask_value)
A:transformers.models.reformer.modeling_reformer.query_bucket_idx->self._split_seq_length_dim_to(sorted_bucket_idx_per_hash, -1, self.chunk_length, self.num_attention_heads)
A:transformers.models.reformer.modeling_reformer.key_value_bucket_idx->torch.arange(query_key_dots.shape[-1], dtype=torch.long, device=query_key_dots.device)[None, None, :].expand(query_bucket_idx.shape[:2] + (-1,))
A:transformers.models.reformer.modeling_reformer.self_mask_value->self.self_mask_value_float16.half()
A:transformers.models.reformer.modeling_reformer.mask_value->self.mask_value_float16.half()
A:transformers.models.reformer.modeling_reformer.mask->self._compute_attn_mask(query_indices, key_indices, attention_mask, query_key_dots.shape, do_standard_self_attention)
A:transformers.models.reformer.modeling_reformer.self_mask->torch.ne(query_bucket_idx.unsqueeze(-1), key_value_bucket_idx.unsqueeze(-2)).to(query_bucket_idx.device)
A:transformers.models.reformer.modeling_reformer.attention_probs->torch.nn.functional.dropout(attention_probs, p=self.dropout, training=self.training)
A:transformers.models.reformer.modeling_reformer.attention_mask->torch.cat([torch.ones(input_shape, device=device, dtype=torch.uint8), torch.zeros((input_shape[0], padding_length), device=device, dtype=torch.uint8)], dim=-1)
A:transformers.models.reformer.modeling_reformer.causal_mask->torch.ge(query_indices.unsqueeze(-1), key_indices.unsqueeze(-2)).to(query_indices.device)
A:transformers.models.reformer.modeling_reformer.hidden_states->self.out_proj(hidden_states)
A:transformers.models.reformer.modeling_reformer.query_buckets->self._hash_vectors(query_vectors, num_hashes, attention_mask, increase_num_buckets=increase_num_buckets)
A:transformers.models.reformer.modeling_reformer.concat_buckets->torch.cat([past_buckets, query_buckets.unsqueeze(-1)], dim=-1)
A:transformers.models.reformer.modeling_reformer.bucket_idx->_stable_argsort(concat_buckets, dim=-1)
A:transformers.models.reformer.modeling_reformer.relevant_bucket_idx->(bucket_idx == bucket_idx.shape[-1] - 1).nonzero()
A:transformers.models.reformer.modeling_reformer.relevant_bucket_idx_chunk->relevant_bucket_idx_chunk.reshape(batch_size, self.num_attention_heads, num_hashes, -1).reshape(batch_size, self.num_attention_heads, num_hashes, -1)
A:transformers.models.reformer.modeling_reformer.relevant_hidden_states->relevant_hidden_states.reshape(batch_size, self.num_attention_heads, -1, self.hidden_size).reshape(batch_size, self.num_attention_heads, -1, self.hidden_size)
A:transformers.models.reformer.modeling_reformer.expanded_start_indices->start_indices_chunk.unsqueeze(-1).expand(indices.shape[0], total_chunk_size)
A:transformers.models.reformer.modeling_reformer.variance->torch.mean(x ** 2, -1, keepdim=True)
A:transformers.models.reformer.modeling_reformer.expanded_idxs->idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
A:transformers.models.reformer.modeling_reformer.expanded_undo_sort_indices->_stable_argsort(buckets, dim=-1).new(*sorted_bucket_idx.size()).unsqueeze(-1).expand(out_vectors.shape)
A:transformers.models.reformer.modeling_reformer.expanded_sort_indices->_stable_argsort(buckets, dim=-1).unsqueeze(-1).expand(grad_out_vectors.shape)
A:transformers.models.reformer.modeling_reformer.grad_out_vectors->torch.gather(grad_out_vectors, 2, expanded_sort_indices)
A:transformers.models.reformer.modeling_reformer.grad_logits->torch.gather(grad_logits, 2, sorted_bucket_idx)
A:transformers.models.reformer.modeling_reformer.self.query->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.models.reformer.modeling_reformer.self.key->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.models.reformer.modeling_reformer.query_indices->self._split_seq_length_dim_to(indices, -1, self.chunk_length, self.num_attention_heads)
A:transformers.models.reformer.modeling_reformer.key_indices->self._look_adjacent(key_indices, self.num_chunks_before, self.num_chunks_after)
A:transformers.models.reformer.modeling_reformer.self.dense->torch.nn.Linear(2 * config.hidden_size, config.hidden_size)
A:transformers.models.reformer.modeling_reformer.self.layer_norm->torch.nn.LayerNorm(2 * config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.reformer.modeling_reformer.self.self_attention->LocalSelfAttention(config)
A:transformers.models.reformer.modeling_reformer.self.output->ReformerFeedForwardOutput(config)
A:transformers.models.reformer.modeling_reformer.self_attention_outputs->self.self_attention(hidden_states=hidden_states, head_mask=head_mask, attention_mask=attention_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states_layer, use_cache=use_cache, output_attentions=output_attentions, buckets=buckets)
A:transformers.models.reformer.modeling_reformer.past_buckets->torch.cat([past_buckets_states[self.layer_id][0], buckets], dim=-1)
A:transformers.models.reformer.modeling_reformer.past_states->torch.cat([past_buckets_states[self.layer_id][1], hidden_states], dim=1)
A:transformers.models.reformer.modeling_reformer.attention_output->self.output(self_attention_outputs.hidden_states)
A:transformers.models.reformer.modeling_reformer.self.attention->ReformerAttention(config, layer_id)
A:transformers.models.reformer.modeling_reformer.self.feed_forward->ChunkReformerFeedForward(config)
A:transformers.models.reformer.modeling_reformer.device_idx->torch.cuda.current_device()
A:transformers.models.reformer.modeling_reformer.self.attention_seed->int(torch.seed() % sys.maxsize)
A:transformers.models.reformer.modeling_reformer.self.feed_forward_seed->int(torch.seed() % sys.maxsize)
A:transformers.models.reformer.modeling_reformer.attn_outputs->self.attention(hidden_states=hidden_states, head_mask=head_mask, attention_mask=attention_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states, use_cache=use_cache, orig_sequence_length=orig_sequence_length, output_attentions=output_attentions)
A:transformers.models.reformer.modeling_reformer.res_hidden_states->self.feed_forward(next_attn_output)
A:transformers.models.reformer.modeling_reformer.(hidden_states, attn_output)->torch.chunk(hidden_states, 2, dim=-1)
A:transformers.models.reformer.modeling_reformer.layer_outputs->layer(prev_attn_output=attn_output, hidden_states=hidden_states, attention_mask=attention_mask, head_mask=layer_head_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states, use_cache=use_cache, orig_sequence_length=orig_sequence_length, output_attentions=output_attentions)
A:transformers.models.reformer.modeling_reformer.(grad_attn_output, grad_hidden_states)->torch.chunk(grad_hidden_states, 2, dim=-1)
A:transformers.models.reformer.modeling_reformer.output->layer.backward_pass(next_attn_output=output.attn_output, hidden_states=output.hidden_states, grad_attn_output=output.grad_attn_output, grad_hidden_states=output.grad_hidden_states, head_mask=head_mask[len(layers) - idx - 1], attention_mask=attention_mask, buckets=buckets)
A:transformers.models.reformer.modeling_reformer.grad_hidden_states->torch.cat([output.grad_attn_output, output.grad_hidden_states], dim=-1)
A:transformers.models.reformer.modeling_reformer.self.layers->torch.nn.ModuleList([ReformerLayer(config, i) for i in range(config.num_hidden_layers)])
A:transformers.models.reformer.modeling_reformer.self.decoder->torch.nn.Linear(2 * config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.reformer.modeling_reformer.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.reformer.modeling_reformer.input_ids->torch.cat([input_ids, padded_input_ids], dim=-1)
A:transformers.models.reformer.modeling_reformer.input_mask->torch.tensor(DUMMY_MASK)
A:transformers.models.reformer.modeling_reformer.self.embeddings->ReformerEmbeddings(config)
A:transformers.models.reformer.modeling_reformer.self.encoder->ReformerEncoder(config)
A:transformers.models.reformer.modeling_reformer.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers, is_attention_chunked=True)
A:transformers.models.reformer.modeling_reformer.least_common_mult_chunk_length->_get_least_common_mult_chunk_len(self.config)
A:transformers.models.reformer.modeling_reformer.min_chunk_length->_get_min_chunk_len(self.config)
A:transformers.models.reformer.modeling_reformer.(input_ids, inputs_embeds, attention_mask, position_ids, input_shape)->self._pad_to_mult_of_chunk_length(input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids=position_ids, input_shape=input_shape, padding_length=padding_length, padded_seq_length=least_common_mult_chunk_length, device=device)
A:transformers.models.reformer.modeling_reformer.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, start_idx_pos_encodings=start_idx_pos_encodings)
A:transformers.models.reformer.modeling_reformer.encoder_outputs->self.encoder(hidden_states=embedding_output, head_mask=head_mask, attention_mask=attention_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states, use_cache=use_cache, orig_sequence_length=orig_sequence_length, output_hidden_states=output_hidden_states, output_attentions=output_attentions)
A:transformers.models.reformer.modeling_reformer.padded_input_ids->torch.full((input_shape[0], padding_length), self.config.pad_token_id, device=device, dtype=torch.long)
A:transformers.models.reformer.modeling_reformer.pad_attention_mask->torch.zeros(input_shape[0], padding_length, device=device, dtype=attention_mask.dtype)
A:transformers.models.reformer.modeling_reformer.padded_position_ids->torch.cat([position_ids, padded_position_ids], dim=-1).unsqueeze(0).expand(input_shape[0], padding_length)
A:transformers.models.reformer.modeling_reformer.padded_inputs_embeds->self.embeddings(padded_input_ids, position_ids)
A:transformers.models.reformer.modeling_reformer.self.reformer->ReformerModel(config)
A:transformers.models.reformer.modeling_reformer.self.lm_head->ReformerOnlyLMHead(config)
A:transformers.models.reformer.modeling_reformer.reformer_outputs->self.reformer(input_ids, position_ids=position_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, num_hashes=num_hashes, use_cache=False, output_hidden_states=output_hidden_states, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.models.reformer.modeling_reformer.shift_logits->logits[..., :-1, :].contiguous()
A:transformers.models.reformer.modeling_reformer.shift_labels->labels[..., 1:].contiguous()
A:transformers.models.reformer.modeling_reformer.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.reformer.modeling_reformer.loss->loss_fct(logits, labels)
A:transformers.models.reformer.modeling_reformer.reord_buckets->layer_past[0].index_select(0, beam_idx)
A:transformers.models.reformer.modeling_reformer.reord_hidden_states->layer_past[1].index_select(0, beam_idx)
A:transformers.models.reformer.modeling_reformer.masked_lm_loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.reformer.modeling_reformer.self.classifier->ReformerClassificationHead(config)
A:transformers.models.reformer.modeling_reformer.outputs->self.reformer(input_ids, position_ids=position_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, num_hashes=num_hashes, output_hidden_states=output_hidden_states, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.models.reformer.modeling_reformer.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.reformer.modeling_reformer.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.reformer.modeling_reformer.self.qa_outputs->torch.nn.Linear(2 * config.hidden_size, config.num_labels)
A:transformers.models.reformer.modeling_reformer.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.reformer.modeling_reformer.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.reformer.modeling_reformer.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.reformer.modeling_reformer.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.reformer.modeling_reformer.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.reformer.modeling_reformer.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.reformer.modeling_reformer.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.reformer.modeling_reformer.end_loss->loss_fct(end_logits, end_positions)
transformers.ReformerAttention(self,config,layer_id=0)
transformers.ReformerAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False,buckets=None)
transformers.ReformerForMaskedLM(self,config)
transformers.ReformerForMaskedLM.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerForMaskedLM.get_output_embeddings(self)
transformers.ReformerForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.ReformerForQuestionAnswering(self,config)
transformers.ReformerForQuestionAnswering.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,start_positions=None,end_positions=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerForSequenceClassification(self,config)
transformers.ReformerForSequenceClassification.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerLayer(self,config,layer_id=0)
transformers.ReformerLayer._init_attention_seed(self)
transformers.ReformerLayer._init_feed_forward_seed(self)
transformers.ReformerLayer.backward_pass(self,next_attn_output,hidden_states,grad_attn_output,grad_hidden_states,attention_mask=None,head_mask=None,buckets=None)
transformers.ReformerLayer.forward(self,prev_attn_output,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False)
transformers.ReformerModel(self,config)
transformers.ReformerModel._pad_to_mult_of_chunk_length(self,input_ids,inputs_embeds=None,attention_mask=None,position_ids=None,input_shape=None,padding_length=None,padded_seq_length=None,device=None)
transformers.ReformerModel._prune_heads(self,heads_to_prune)
transformers.ReformerModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerModel.get_input_embeddings(self)
transformers.ReformerModel.set_input_embeddings(self,value)
transformers.ReformerModelOutput(ModelOutput)
transformers.ReformerModelWithLMHead(self,config)
transformers.ReformerModelWithLMHead._reorder_cache(self,past,beam_idx)
transformers.ReformerModelWithLMHead.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None,labels=None)
transformers.ReformerModelWithLMHead.get_output_embeddings(self)
transformers.ReformerModelWithLMHead.prepare_inputs_for_generation(self,input_ids,past=None,use_cache=None,num_hashes=None,**kwargs)
transformers.ReformerModelWithLMHead.set_output_embeddings(self,new_embeddings)
transformers.ReformerModelWithLMHeadOutput(ModelOutput)
transformers.ReformerPreTrainedModel(PreTrainedModel)
transformers.ReformerPreTrainedModel._init_weights(self,module)
transformers.ReformerPreTrainedModel.dummy_inputs(self)
transformers.models.reformer.modeling_reformer.AxialPositionEmbeddings(self,config)
transformers.models.reformer.modeling_reformer.AxialPositionEmbeddings.__init__(self,config)
transformers.models.reformer.modeling_reformer.AxialPositionEmbeddings.forward(self,position_ids)
transformers.models.reformer.modeling_reformer.ChunkReformerFeedForward(self,config)
transformers.models.reformer.modeling_reformer.ChunkReformerFeedForward.__init__(self,config)
transformers.models.reformer.modeling_reformer.ChunkReformerFeedForward.forward(self,attention_output)
transformers.models.reformer.modeling_reformer.ChunkReformerFeedForward.forward_chunk(self,hidden_states)
transformers.models.reformer.modeling_reformer.EfficientAttentionMixin
transformers.models.reformer.modeling_reformer.EfficientAttentionMixin._look_adjacent(self,vectors,num_chunks_before,num_chunks_after)
transformers.models.reformer.modeling_reformer.EfficientAttentionMixin._merge_hidden_size_dims(self,x,num_attn_heads,attn_head_size)
transformers.models.reformer.modeling_reformer.EfficientAttentionMixin._split_hidden_size_dim(self,x,num_attn_heads,attn_head_size)
transformers.models.reformer.modeling_reformer.EfficientAttentionMixin._split_seq_length_dim_to(self,vectors,dim_factor_1,dim_factor_2,num_attn_heads,attn_head_size=None)
transformers.models.reformer.modeling_reformer.LSHSelfAttention(self,config)
transformers.models.reformer.modeling_reformer.LSHSelfAttention.__init__(self,config)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._attend(self,query_vectors,key_vectors,value_vectors,sorted_bucket_idx_per_hash,attention_mask,head_mask,do_standard_self_attention,do_cached_attention)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._compute_attn_mask(self,query_indices,key_indices,attention_mask,query_key_dot_shape,do_standard_self_attention)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._expand_to_indices_in_relevant_chunk(self,indices,sequence_length)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._gather_by_expansion(self,vectors,idxs,num_hashes)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._get_relevant_hid_states_and_buckets(self,query_vectors,attention_mask,num_hashes,hidden_states,past_states,past_buckets)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._get_sorted_bucket_idx_and_undo_sorted_bucket_idx(self,sequence_length,buckets,num_hashes)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._hash_vectors(self,vectors,num_hashes,attention_mask,increase_num_buckets=False)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._len_and_dim_norm(self,vectors)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._len_norm(self,x,epsilon=1e-06)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._query_per_attn_head(self,hidden_states)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._set_num_buckets(self,sequence_length)
transformers.models.reformer.modeling_reformer.LSHSelfAttention._value_per_attn_head(self,hidden_states)
transformers.models.reformer.modeling_reformer.LSHSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,buckets=None,past_buckets_states=None,use_cache=False,output_attentions=False,**kwargs)
transformers.models.reformer.modeling_reformer.LocalSelfAttention(self,config)
transformers.models.reformer.modeling_reformer.LocalSelfAttention.__init__(self,config)
transformers.models.reformer.modeling_reformer.LocalSelfAttention._compute_attn_mask(self,query_indices,key_indices,attention_mask,query_key_dots_shape,do_standard_self_attention)
transformers.models.reformer.modeling_reformer.LocalSelfAttention._retrieve_relevant_hidden_states(previous_hidden_states,chunk_length,num_chunks_before)
transformers.models.reformer.modeling_reformer.LocalSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,past_buckets_states=None,use_cache=False,output_attentions=False,**kwargs)
transformers.models.reformer.modeling_reformer.PositionEmbeddings(self,config)
transformers.models.reformer.modeling_reformer.PositionEmbeddings.__init__(self,config)
transformers.models.reformer.modeling_reformer.PositionEmbeddings.forward(self,position_ids)
transformers.models.reformer.modeling_reformer.ReformerAttention(self,config,layer_id=0)
transformers.models.reformer.modeling_reformer.ReformerAttention.__init__(self,config,layer_id=0)
transformers.models.reformer.modeling_reformer.ReformerAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False,buckets=None)
transformers.models.reformer.modeling_reformer.ReformerClassificationHead(self,config)
transformers.models.reformer.modeling_reformer.ReformerClassificationHead.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerClassificationHead.forward(self,hidden_states,**kwargs)
transformers.models.reformer.modeling_reformer.ReformerEmbeddings(self,config)
transformers.models.reformer.modeling_reformer.ReformerEmbeddings.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerEmbeddings.forward(self,input_ids=None,position_ids=None,inputs_embeds=None,start_idx_pos_encodings=0)
transformers.models.reformer.modeling_reformer.ReformerEncoder(self,config)
transformers.models.reformer.modeling_reformer.ReformerEncoder.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_hidden_states=False,output_attentions=False)
transformers.models.reformer.modeling_reformer.ReformerFeedForwardDense(self,config)
transformers.models.reformer.modeling_reformer.ReformerFeedForwardDense.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerFeedForwardDense.forward(self,hidden_states)
transformers.models.reformer.modeling_reformer.ReformerFeedForwardOutput(self,config)
transformers.models.reformer.modeling_reformer.ReformerFeedForwardOutput.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerFeedForwardOutput.forward(self,hidden_states)
transformers.models.reformer.modeling_reformer.ReformerForMaskedLM(self,config)
transformers.models.reformer.modeling_reformer.ReformerForMaskedLM.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerForMaskedLM.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.models.reformer.modeling_reformer.ReformerForMaskedLM.get_output_embeddings(self)
transformers.models.reformer.modeling_reformer.ReformerForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.reformer.modeling_reformer.ReformerForQuestionAnswering(self,config)
transformers.models.reformer.modeling_reformer.ReformerForQuestionAnswering.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerForQuestionAnswering.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,start_positions=None,end_positions=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.models.reformer.modeling_reformer.ReformerForSequenceClassification(self,config)
transformers.models.reformer.modeling_reformer.ReformerForSequenceClassification.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerForSequenceClassification.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.models.reformer.modeling_reformer.ReformerLayer(self,config,layer_id=0)
transformers.models.reformer.modeling_reformer.ReformerLayer.__init__(self,config,layer_id=0)
transformers.models.reformer.modeling_reformer.ReformerLayer._init_attention_seed(self)
transformers.models.reformer.modeling_reformer.ReformerLayer._init_feed_forward_seed(self)
transformers.models.reformer.modeling_reformer.ReformerLayer.backward_pass(self,next_attn_output,hidden_states,grad_attn_output,grad_hidden_states,attention_mask=None,head_mask=None,buckets=None)
transformers.models.reformer.modeling_reformer.ReformerLayer.forward(self,prev_attn_output,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False)
transformers.models.reformer.modeling_reformer.ReformerModel(self,config)
transformers.models.reformer.modeling_reformer.ReformerModel.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerModel._pad_to_mult_of_chunk_length(self,input_ids,inputs_embeds=None,attention_mask=None,position_ids=None,input_shape=None,padding_length=None,padded_seq_length=None,device=None)
transformers.models.reformer.modeling_reformer.ReformerModel._prune_heads(self,heads_to_prune)
transformers.models.reformer.modeling_reformer.ReformerModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.models.reformer.modeling_reformer.ReformerModel.get_input_embeddings(self)
transformers.models.reformer.modeling_reformer.ReformerModel.set_input_embeddings(self,value)
transformers.models.reformer.modeling_reformer.ReformerModelOutput(ModelOutput)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead(self,config)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead._reorder_cache(self,past,beam_idx)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None,labels=None)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead.get_output_embeddings(self)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead.prepare_inputs_for_generation(self,input_ids,past=None,use_cache=None,num_hashes=None,**kwargs)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHead.set_output_embeddings(self,new_embeddings)
transformers.models.reformer.modeling_reformer.ReformerModelWithLMHeadOutput(ModelOutput)
transformers.models.reformer.modeling_reformer.ReformerOnlyLMHead(self,config)
transformers.models.reformer.modeling_reformer.ReformerOnlyLMHead.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerOnlyLMHead.forward(self,hidden_states)
transformers.models.reformer.modeling_reformer.ReformerOnlyLMHead.forward_chunk(self,hidden_states)
transformers.models.reformer.modeling_reformer.ReformerPreTrainedModel(PreTrainedModel)
transformers.models.reformer.modeling_reformer.ReformerPreTrainedModel._init_weights(self,module)
transformers.models.reformer.modeling_reformer.ReformerPreTrainedModel.dummy_inputs(self)
transformers.models.reformer.modeling_reformer.ReformerSelfOutput(self,config)
transformers.models.reformer.modeling_reformer.ReformerSelfOutput.__init__(self,config)
transformers.models.reformer.modeling_reformer.ReformerSelfOutput.forward(self,hidden_states)
transformers.models.reformer.modeling_reformer.ReverseSort(Function)
transformers.models.reformer.modeling_reformer.ReverseSort.backward(ctx,grad_out_vectors,grad_logits)
transformers.models.reformer.modeling_reformer.ReverseSort.forward(ctx,out_vectors,logits,sorted_bucket_idx,undo_sorted_bucket_idx)
transformers.models.reformer.modeling_reformer._ReversibleFunction(Function)
transformers.models.reformer.modeling_reformer._ReversibleFunction.backward(ctx,grad_hidden_states)
transformers.models.reformer.modeling_reformer._ReversibleFunction.forward(ctx,hidden_states,layers,attention_mask,head_mask,num_hashes,all_hidden_states,all_attentions,past_buckets_states,use_cache,orig_sequence_length,output_hidden_states,output_attentions)
transformers.models.reformer.modeling_reformer._get_least_common_mult_chunk_len(config)
transformers.models.reformer.modeling_reformer._get_min_chunk_len(config)
transformers.models.reformer.modeling_reformer._stable_argsort(vector,dim)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mt5/modeling_mt5.py----------------------------------------
A:transformers.models.mt5.modeling_mt5.logger->utils.logging.get_logger(__name__)
transformers.MT5EncoderModel(T5EncoderModel)
transformers.MT5ForConditionalGeneration(T5ForConditionalGeneration)
transformers.MT5Model(T5Model)
transformers.models.mt5.modeling_mt5.MT5EncoderModel(T5EncoderModel)
transformers.models.mt5.modeling_mt5.MT5ForConditionalGeneration(T5ForConditionalGeneration)
transformers.models.mt5.modeling_mt5.MT5Model(T5Model)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mt5/configuration_mt5.py----------------------------------------
A:transformers.models.mt5.configuration_mt5.logger->utils.logging.get_logger(__name__)
transformers.MT5Config(self,vocab_size=250112,d_model=512,d_kv=64,d_ff=1024,num_layers=8,num_decoder_layers=None,num_heads=6,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,feed_forward_proj='gated-gelu',is_encoder_decoder=True,use_cache=True,tokenizer_class='T5Tokenizer',tie_word_embeddings=False,pad_token_id=0,eos_token_id=1,decoder_start_token_id=0,**kwargs)
transformers.MT5Config.hidden_size(self)
transformers.MT5Config.num_attention_heads(self)
transformers.MT5Config.num_hidden_layers(self)
transformers.models.mt5.configuration_mt5.MT5Config(self,vocab_size=250112,d_model=512,d_kv=64,d_ff=1024,num_layers=8,num_decoder_layers=None,num_heads=6,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,feed_forward_proj='gated-gelu',is_encoder_decoder=True,use_cache=True,tokenizer_class='T5Tokenizer',tie_word_embeddings=False,pad_token_id=0,eos_token_id=1,decoder_start_token_id=0,**kwargs)
transformers.models.mt5.configuration_mt5.MT5Config.__init__(self,vocab_size=250112,d_model=512,d_kv=64,d_ff=1024,num_layers=8,num_decoder_layers=None,num_heads=6,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,feed_forward_proj='gated-gelu',is_encoder_decoder=True,use_cache=True,tokenizer_class='T5Tokenizer',tie_word_embeddings=False,pad_token_id=0,eos_token_id=1,decoder_start_token_id=0,**kwargs)
transformers.models.mt5.configuration_mt5.MT5Config.hidden_size(self)
transformers.models.mt5.configuration_mt5.MT5Config.num_attention_heads(self)
transformers.models.mt5.configuration_mt5.MT5Config.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mt5/__init__.py----------------------------------------
A:transformers.models.mt5.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mt5/modeling_tf_mt5.py----------------------------------------
A:transformers.models.mt5.modeling_tf_mt5.logger->utils.logging.get_logger(__name__)
transformers.TFMT5EncoderModel(TFT5EncoderModel)
transformers.TFMT5ForConditionalGeneration(TFT5ForConditionalGeneration)
transformers.TFMT5Model(TFT5Model)
transformers.models.mt5.modeling_tf_mt5.TFMT5EncoderModel(TFT5EncoderModel)
transformers.models.mt5.modeling_tf_mt5.TFMT5ForConditionalGeneration(TFT5ForConditionalGeneration)
transformers.models.mt5.modeling_tf_mt5.TFMT5Model(TFT5Model)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/modeling_flax_auto.py----------------------------------------
A:transformers.models.auto.modeling_flax_auto.logger->utils.logging.get_logger(__name__)
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaModel), (BertConfig, FlaxBertModel), (BigBirdConfig, FlaxBigBirdModel), (BartConfig, FlaxBartModel), (GPT2Config, FlaxGPT2Model), (ElectraConfig, FlaxElectraModel), (CLIPConfig, FlaxCLIPModel), (ViTConfig, FlaxViTModel), (T5Config, FlaxT5Model)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_PRETRAINING_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaForMaskedLM), (BertConfig, FlaxBertForPreTraining), (BigBirdConfig, FlaxBigBirdForPreTraining), (BartConfig, FlaxBartForConditionalGeneration), (ElectraConfig, FlaxElectraForPreTraining), (T5Config, FlaxT5ForConditionalGeneration)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_MASKED_LM_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaForMaskedLM), (BertConfig, FlaxBertForMaskedLM), (BigBirdConfig, FlaxBigBirdForMaskedLM), (BartConfig, FlaxBartForConditionalGeneration), (ElectraConfig, FlaxElectraForMaskedLM)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING->OrderedDict([(BartConfig, FlaxBartForConditionalGeneration)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING->OrderedDict([(ViTConfig, FlaxViTForImageClassification)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_CAUSAL_LM_MAPPING->OrderedDict([(GPT2Config, FlaxGPT2LMHeadModel)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaForSequenceClassification), (BertConfig, FlaxBertForSequenceClassification), (BigBirdConfig, FlaxBigBirdForSequenceClassification), (BartConfig, FlaxBartForSequenceClassification), (ElectraConfig, FlaxElectraForSequenceClassification)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaForQuestionAnswering), (BertConfig, FlaxBertForQuestionAnswering), (BigBirdConfig, FlaxBigBirdForQuestionAnswering), (BartConfig, FlaxBartForQuestionAnswering), (ElectraConfig, FlaxElectraForQuestionAnswering)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaForTokenClassification), (BertConfig, FlaxBertForTokenClassification), (BigBirdConfig, FlaxBigBirdForTokenClassification), (ElectraConfig, FlaxElectraForTokenClassification)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING->OrderedDict([(RobertaConfig, FlaxRobertaForMultipleChoice), (BertConfig, FlaxBertForMultipleChoice), (BigBirdConfig, FlaxBigBirdForMultipleChoice), (ElectraConfig, FlaxElectraForMultipleChoice)])
A:transformers.models.auto.modeling_flax_auto.FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING->OrderedDict([(BertConfig, FlaxBertForNextSentencePrediction)])
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModel->auto_class_factory('FlaxAutoModel', FLAX_MODEL_MAPPING)
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForImageClassification->auto_class_factory('FlaxAutoModelForImageClassification', FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING, head_doc='image classification modeling')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForCausalLM->auto_class_factory('FlaxAutoModelForCausalLM', FLAX_MODEL_FOR_CAUSAL_LM_MAPPING, head_doc='causal language modeling')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForPreTraining->auto_class_factory('FlaxAutoModelForPreTraining', FLAX_MODEL_FOR_PRETRAINING_MAPPING, head_doc='pretraining')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForMaskedLM->auto_class_factory('FlaxAutoModelForMaskedLM', FLAX_MODEL_FOR_MASKED_LM_MAPPING, head_doc='masked language modeling')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForSeq2SeqLM->auto_class_factory('FlaxAutoModelForSeq2SeqLM', FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING, head_doc='sequence-to-sequence language modeling')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForSequenceClassification->auto_class_factory('FlaxAutoModelForSequenceClassification', FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING, head_doc='sequence classification')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForQuestionAnswering->auto_class_factory('FlaxAutoModelForQuestionAnswering', FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING, head_doc='question answering')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForTokenClassification->auto_class_factory('FlaxAutoModelForTokenClassification', FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING, head_doc='token classification')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForMultipleChoice->auto_class_factory('AutoModelForMultipleChoice', FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING, head_doc='multiple choice')
A:transformers.models.auto.modeling_flax_auto.FlaxAutoModelForNextSentencePrediction->auto_class_factory('FlaxAutoModelForNextSentencePrediction', FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING, head_doc='next sentence prediction')


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/auto_factory.py----------------------------------------
A:transformers.models.auto.auto_factory.logger->utils.logging.get_logger(__name__)
A:transformers.models.auto.auto_factory.architectures->getattr(config, 'architectures', [])
A:transformers.models.auto.auto_factory.model_class->_get_model_class(config, cls._model_mapping)
A:transformers.models.auto.auto_factory.config->kwargs.pop('config', None)
A:transformers.models.auto.auto_factory.(config, kwargs)->configuration_auto.AutoConfig.from_pretrained(pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs)
A:transformers.models.auto.auto_factory.new_class->types.new_class(name, (_BaseAutoModelClass,))
A:transformers.models.auto.auto_factory.class_docstring->insert_head_doc(CLASS_DOCSTRING, head_doc=head_doc)
A:transformers.models.auto.auto_factory.new_class.__doc__->insert_head_doc(CLASS_DOCSTRING, head_doc=head_doc).replace('BaseAutoModelClass', name)
A:transformers.models.auto.auto_factory.from_config->replace_list_option_in_docstrings(model_mapping, use_model_types=False)(from_config)
A:transformers.models.auto.auto_factory.from_config_docstring->from_config_docstring.replace('checkpoint_placeholder', checkpoint_for_example).replace('checkpoint_placeholder', checkpoint_for_example)
A:transformers.models.auto.auto_factory.new_class.from_config->classmethod(from_config)
A:transformers.models.auto.auto_factory.from_pretrained->replace_list_option_in_docstrings(model_mapping)(from_pretrained)
A:transformers.models.auto.auto_factory.from_pretrained_docstring->from_pretrained_docstring.replace('shortcut_placeholder', shortcut).replace('shortcut_placeholder', shortcut)
A:transformers.models.auto.auto_factory.new_class.from_pretrained->classmethod(from_pretrained)
transformers.models.auto.auto_factory._BaseAutoModelClass(self,*args,**kwargs)
transformers.models.auto.auto_factory._BaseAutoModelClass.__init__(self,*args,**kwargs)
transformers.models.auto.auto_factory._BaseAutoModelClass.from_config(cls,config,**kwargs)
transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.models.auto.auto_factory._get_model_class(config,model_mapping)
transformers.models.auto.auto_factory.auto_class_factory(name,model_mapping,checkpoint_for_example='bert-base-cased',head_doc='')
transformers.models.auto.auto_factory.get_values(model_mapping)
transformers.models.auto.auto_factory.insert_head_doc(docstring,head_doc='')
transformers.models.auto.get_values(model_mapping)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/modeling_auto.py----------------------------------------
A:transformers.models.auto.modeling_auto.logger->utils.logging.get_logger(__name__)
A:transformers.models.auto.modeling_auto.MODEL_MAPPING->OrderedDict([(VisualBertConfig, VisualBertModel), (RoFormerConfig, RoFormerModel), (CLIPConfig, CLIPModel), (BigBirdPegasusConfig, BigBirdPegasusModel), (DeiTConfig, DeiTModel), (LukeConfig, LukeModel), (DetrConfig, DetrModel), (GPTNeoConfig, GPTNeoModel), (BigBirdConfig, BigBirdModel), (Speech2TextConfig, Speech2TextModel), (ViTConfig, ViTModel), (Wav2Vec2Config, Wav2Vec2Model), (HubertConfig, HubertModel), (M2M100Config, M2M100Model), (ConvBertConfig, ConvBertModel), (LEDConfig, LEDModel), (BlenderbotSmallConfig, BlenderbotSmallModel), (RetriBertConfig, RetriBertModel), (MT5Config, MT5Model), (T5Config, T5Model), (PegasusConfig, PegasusModel), (MarianConfig, MarianMTModel), (MBartConfig, MBartModel), (BlenderbotConfig, BlenderbotModel), (DistilBertConfig, DistilBertModel), (AlbertConfig, AlbertModel), (CamembertConfig, CamembertModel), (XLMRobertaConfig, XLMRobertaModel), (BartConfig, BartModel), (LongformerConfig, LongformerModel), (RobertaConfig, RobertaModel), (LayoutLMConfig, LayoutLMModel), (SqueezeBertConfig, SqueezeBertModel), (BertConfig, BertModel), (OpenAIGPTConfig, OpenAIGPTModel), (GPT2Config, GPT2Model), (MegatronBertConfig, MegatronBertModel), (MobileBertConfig, MobileBertModel), (TransfoXLConfig, TransfoXLModel), (XLNetConfig, XLNetModel), (FlaubertConfig, FlaubertModel), (FSMTConfig, FSMTModel), (XLMConfig, XLMModel), (CTRLConfig, CTRLModel), (ElectraConfig, ElectraModel), (ReformerConfig, ReformerModel), (FunnelConfig, (FunnelModel, FunnelBaseModel)), (LxmertConfig, LxmertModel), (BertGenerationConfig, BertGenerationEncoder), (DebertaConfig, DebertaModel), (DebertaV2Config, DebertaV2Model), (DPRConfig, DPRQuestionEncoder), (XLMProphetNetConfig, XLMProphetNetModel), (ProphetNetConfig, ProphetNetModel), (MPNetConfig, MPNetModel), (TapasConfig, TapasModel), (MarianConfig, MarianModel), (IBertConfig, IBertModel)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_PRETRAINING_MAPPING->OrderedDict([(VisualBertConfig, VisualBertForPreTraining), (LayoutLMConfig, LayoutLMForMaskedLM), (RetriBertConfig, RetriBertModel), (T5Config, T5ForConditionalGeneration), (DistilBertConfig, DistilBertForMaskedLM), (AlbertConfig, AlbertForPreTraining), (CamembertConfig, CamembertForMaskedLM), (XLMRobertaConfig, XLMRobertaForMaskedLM), (BartConfig, BartForConditionalGeneration), (FSMTConfig, FSMTForConditionalGeneration), (LongformerConfig, LongformerForMaskedLM), (RobertaConfig, RobertaForMaskedLM), (SqueezeBertConfig, SqueezeBertForMaskedLM), (BertConfig, BertForPreTraining), (BigBirdConfig, BigBirdForPreTraining), (OpenAIGPTConfig, OpenAIGPTLMHeadModel), (GPT2Config, GPT2LMHeadModel), (MegatronBertConfig, MegatronBertForPreTraining), (MobileBertConfig, MobileBertForPreTraining), (TransfoXLConfig, TransfoXLLMHeadModel), (XLNetConfig, XLNetLMHeadModel), (FlaubertConfig, FlaubertWithLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (CTRLConfig, CTRLLMHeadModel), (ElectraConfig, ElectraForPreTraining), (LxmertConfig, LxmertForPreTraining), (FunnelConfig, FunnelForPreTraining), (MPNetConfig, MPNetForMaskedLM), (TapasConfig, TapasForMaskedLM), (IBertConfig, IBertForMaskedLM), (DebertaConfig, DebertaForMaskedLM), (DebertaV2Config, DebertaV2ForMaskedLM), (Wav2Vec2Config, Wav2Vec2ForPreTraining)])
A:transformers.models.auto.modeling_auto.MODEL_WITH_LM_HEAD_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForMaskedLM), (BigBirdPegasusConfig, BigBirdPegasusForConditionalGeneration), (GPTNeoConfig, GPTNeoForCausalLM), (BigBirdConfig, BigBirdForMaskedLM), (Speech2TextConfig, Speech2TextForConditionalGeneration), (Wav2Vec2Config, Wav2Vec2ForMaskedLM), (M2M100Config, M2M100ForConditionalGeneration), (ConvBertConfig, ConvBertForMaskedLM), (LEDConfig, LEDForConditionalGeneration), (BlenderbotSmallConfig, BlenderbotSmallForConditionalGeneration), (LayoutLMConfig, LayoutLMForMaskedLM), (T5Config, T5ForConditionalGeneration), (DistilBertConfig, DistilBertForMaskedLM), (AlbertConfig, AlbertForMaskedLM), (CamembertConfig, CamembertForMaskedLM), (XLMRobertaConfig, XLMRobertaForMaskedLM), (MarianConfig, MarianMTModel), (FSMTConfig, FSMTForConditionalGeneration), (BartConfig, BartForConditionalGeneration), (LongformerConfig, LongformerForMaskedLM), (RobertaConfig, RobertaForMaskedLM), (SqueezeBertConfig, SqueezeBertForMaskedLM), (BertConfig, BertForMaskedLM), (OpenAIGPTConfig, OpenAIGPTLMHeadModel), (GPT2Config, GPT2LMHeadModel), (MegatronBertConfig, MegatronBertForMaskedLM), (MobileBertConfig, MobileBertForMaskedLM), (TransfoXLConfig, TransfoXLLMHeadModel), (XLNetConfig, XLNetLMHeadModel), (FlaubertConfig, FlaubertWithLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (CTRLConfig, CTRLLMHeadModel), (ElectraConfig, ElectraForMaskedLM), (EncoderDecoderConfig, EncoderDecoderModel), (ReformerConfig, ReformerModelWithLMHead), (FunnelConfig, FunnelForMaskedLM), (MPNetConfig, MPNetForMaskedLM), (TapasConfig, TapasForMaskedLM), (DebertaConfig, DebertaForMaskedLM), (DebertaV2Config, DebertaV2ForMaskedLM), (IBertConfig, IBertForMaskedLM), (MegatronBertConfig, MegatronBertForCausalLM)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_CAUSAL_LM_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForCausalLM), (BigBirdPegasusConfig, BigBirdPegasusForCausalLM), (GPTNeoConfig, GPTNeoForCausalLM), (BigBirdConfig, BigBirdForCausalLM), (CamembertConfig, CamembertForCausalLM), (XLMRobertaConfig, XLMRobertaForCausalLM), (RobertaConfig, RobertaForCausalLM), (BertConfig, BertLMHeadModel), (OpenAIGPTConfig, OpenAIGPTLMHeadModel), (GPT2Config, GPT2LMHeadModel), (TransfoXLConfig, TransfoXLLMHeadModel), (XLNetConfig, XLNetLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (CTRLConfig, CTRLLMHeadModel), (ReformerConfig, ReformerModelWithLMHead), (BertGenerationConfig, BertGenerationDecoder), (XLMProphetNetConfig, XLMProphetNetForCausalLM), (ProphetNetConfig, ProphetNetForCausalLM), (BartConfig, BartForCausalLM), (MBartConfig, MBartForCausalLM), (PegasusConfig, PegasusForCausalLM), (MarianConfig, MarianForCausalLM), (BlenderbotConfig, BlenderbotForCausalLM), (BlenderbotSmallConfig, BlenderbotSmallForCausalLM), (MegatronBertConfig, MegatronBertForCausalLM)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING->OrderedDict([(ViTConfig, ViTForImageClassification), (DeiTConfig, (DeiTForImageClassification, DeiTForImageClassificationWithTeacher))])
A:transformers.models.auto.modeling_auto.MODEL_FOR_MASKED_LM_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForMaskedLM), (BigBirdConfig, BigBirdForMaskedLM), (Wav2Vec2Config, Wav2Vec2ForMaskedLM), (ConvBertConfig, ConvBertForMaskedLM), (LayoutLMConfig, LayoutLMForMaskedLM), (DistilBertConfig, DistilBertForMaskedLM), (AlbertConfig, AlbertForMaskedLM), (BartConfig, BartForConditionalGeneration), (MBartConfig, MBartForConditionalGeneration), (CamembertConfig, CamembertForMaskedLM), (XLMRobertaConfig, XLMRobertaForMaskedLM), (LongformerConfig, LongformerForMaskedLM), (RobertaConfig, RobertaForMaskedLM), (SqueezeBertConfig, SqueezeBertForMaskedLM), (BertConfig, BertForMaskedLM), (MegatronBertConfig, MegatronBertForMaskedLM), (MobileBertConfig, MobileBertForMaskedLM), (FlaubertConfig, FlaubertWithLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (ElectraConfig, ElectraForMaskedLM), (ReformerConfig, ReformerForMaskedLM), (FunnelConfig, FunnelForMaskedLM), (MPNetConfig, MPNetForMaskedLM), (TapasConfig, TapasForMaskedLM), (DebertaConfig, DebertaForMaskedLM), (DebertaV2Config, DebertaV2ForMaskedLM), (IBertConfig, IBertForMaskedLM)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_OBJECT_DETECTION_MAPPING->OrderedDict([(DetrConfig, DetrForObjectDetection)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING->OrderedDict([(BigBirdPegasusConfig, BigBirdPegasusForConditionalGeneration), (M2M100Config, M2M100ForConditionalGeneration), (LEDConfig, LEDForConditionalGeneration), (BlenderbotSmallConfig, BlenderbotSmallForConditionalGeneration), (MT5Config, MT5ForConditionalGeneration), (T5Config, T5ForConditionalGeneration), (PegasusConfig, PegasusForConditionalGeneration), (MarianConfig, MarianMTModel), (MBartConfig, MBartForConditionalGeneration), (BlenderbotConfig, BlenderbotForConditionalGeneration), (BartConfig, BartForConditionalGeneration), (FSMTConfig, FSMTForConditionalGeneration), (EncoderDecoderConfig, EncoderDecoderModel), (XLMProphetNetConfig, XLMProphetNetForConditionalGeneration), (ProphetNetConfig, ProphetNetForConditionalGeneration)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForSequenceClassification), (BigBirdPegasusConfig, BigBirdPegasusForSequenceClassification), (BigBirdConfig, BigBirdForSequenceClassification), (ConvBertConfig, ConvBertForSequenceClassification), (LEDConfig, LEDForSequenceClassification), (DistilBertConfig, DistilBertForSequenceClassification), (AlbertConfig, AlbertForSequenceClassification), (CamembertConfig, CamembertForSequenceClassification), (XLMRobertaConfig, XLMRobertaForSequenceClassification), (MBartConfig, MBartForSequenceClassification), (BartConfig, BartForSequenceClassification), (LongformerConfig, LongformerForSequenceClassification), (RobertaConfig, RobertaForSequenceClassification), (SqueezeBertConfig, SqueezeBertForSequenceClassification), (LayoutLMConfig, LayoutLMForSequenceClassification), (BertConfig, BertForSequenceClassification), (XLNetConfig, XLNetForSequenceClassification), (MegatronBertConfig, MegatronBertForSequenceClassification), (MobileBertConfig, MobileBertForSequenceClassification), (FlaubertConfig, FlaubertForSequenceClassification), (XLMConfig, XLMForSequenceClassification), (ElectraConfig, ElectraForSequenceClassification), (FunnelConfig, FunnelForSequenceClassification), (DebertaConfig, DebertaForSequenceClassification), (DebertaV2Config, DebertaV2ForSequenceClassification), (GPT2Config, GPT2ForSequenceClassification), (GPTNeoConfig, GPTNeoForSequenceClassification), (OpenAIGPTConfig, OpenAIGPTForSequenceClassification), (ReformerConfig, ReformerForSequenceClassification), (CTRLConfig, CTRLForSequenceClassification), (TransfoXLConfig, TransfoXLForSequenceClassification), (MPNetConfig, MPNetForSequenceClassification), (TapasConfig, TapasForSequenceClassification), (IBertConfig, IBertForSequenceClassification)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_QUESTION_ANSWERING_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForQuestionAnswering), (BigBirdPegasusConfig, BigBirdPegasusForQuestionAnswering), (BigBirdConfig, BigBirdForQuestionAnswering), (ConvBertConfig, ConvBertForQuestionAnswering), (LEDConfig, LEDForQuestionAnswering), (DistilBertConfig, DistilBertForQuestionAnswering), (AlbertConfig, AlbertForQuestionAnswering), (CamembertConfig, CamembertForQuestionAnswering), (BartConfig, BartForQuestionAnswering), (MBartConfig, MBartForQuestionAnswering), (LongformerConfig, LongformerForQuestionAnswering), (XLMRobertaConfig, XLMRobertaForQuestionAnswering), (RobertaConfig, RobertaForQuestionAnswering), (SqueezeBertConfig, SqueezeBertForQuestionAnswering), (BertConfig, BertForQuestionAnswering), (XLNetConfig, XLNetForQuestionAnsweringSimple), (FlaubertConfig, FlaubertForQuestionAnsweringSimple), (MegatronBertConfig, MegatronBertForQuestionAnswering), (MobileBertConfig, MobileBertForQuestionAnswering), (XLMConfig, XLMForQuestionAnsweringSimple), (ElectraConfig, ElectraForQuestionAnswering), (ReformerConfig, ReformerForQuestionAnswering), (FunnelConfig, FunnelForQuestionAnswering), (LxmertConfig, LxmertForQuestionAnswering), (MPNetConfig, MPNetForQuestionAnswering), (DebertaConfig, DebertaForQuestionAnswering), (DebertaV2Config, DebertaV2ForQuestionAnswering), (IBertConfig, IBertForQuestionAnswering)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING->OrderedDict([(TapasConfig, TapasForQuestionAnswering)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForTokenClassification), (BigBirdConfig, BigBirdForTokenClassification), (ConvBertConfig, ConvBertForTokenClassification), (LayoutLMConfig, LayoutLMForTokenClassification), (DistilBertConfig, DistilBertForTokenClassification), (CamembertConfig, CamembertForTokenClassification), (FlaubertConfig, FlaubertForTokenClassification), (XLMConfig, XLMForTokenClassification), (XLMRobertaConfig, XLMRobertaForTokenClassification), (LongformerConfig, LongformerForTokenClassification), (RobertaConfig, RobertaForTokenClassification), (SqueezeBertConfig, SqueezeBertForTokenClassification), (BertConfig, BertForTokenClassification), (MegatronBertConfig, MegatronBertForTokenClassification), (MobileBertConfig, MobileBertForTokenClassification), (XLNetConfig, XLNetForTokenClassification), (AlbertConfig, AlbertForTokenClassification), (ElectraConfig, ElectraForTokenClassification), (FlaubertConfig, FlaubertForTokenClassification), (FunnelConfig, FunnelForTokenClassification), (MPNetConfig, MPNetForTokenClassification), (DebertaConfig, DebertaForTokenClassification), (DebertaV2Config, DebertaV2ForTokenClassification), (IBertConfig, IBertForTokenClassification)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_MULTIPLE_CHOICE_MAPPING->OrderedDict([(RoFormerConfig, RoFormerForMultipleChoice), (BigBirdConfig, BigBirdForMultipleChoice), (ConvBertConfig, ConvBertForMultipleChoice), (CamembertConfig, CamembertForMultipleChoice), (ElectraConfig, ElectraForMultipleChoice), (XLMRobertaConfig, XLMRobertaForMultipleChoice), (LongformerConfig, LongformerForMultipleChoice), (RobertaConfig, RobertaForMultipleChoice), (SqueezeBertConfig, SqueezeBertForMultipleChoice), (BertConfig, BertForMultipleChoice), (DistilBertConfig, DistilBertForMultipleChoice), (MegatronBertConfig, MegatronBertForMultipleChoice), (MobileBertConfig, MobileBertForMultipleChoice), (XLNetConfig, XLNetForMultipleChoice), (AlbertConfig, AlbertForMultipleChoice), (XLMConfig, XLMForMultipleChoice), (FlaubertConfig, FlaubertForMultipleChoice), (FunnelConfig, FunnelForMultipleChoice), (MPNetConfig, MPNetForMultipleChoice), (IBertConfig, IBertForMultipleChoice)])
A:transformers.models.auto.modeling_auto.MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING->OrderedDict([(BertConfig, BertForNextSentencePrediction), (MegatronBertConfig, MegatronBertForNextSentencePrediction), (MobileBertConfig, MobileBertForNextSentencePrediction)])
A:transformers.models.auto.modeling_auto.AutoModel->auto_class_factory('AutoModel', MODEL_MAPPING)
A:transformers.models.auto.modeling_auto.AutoModelForPreTraining->auto_class_factory('AutoModelForPreTraining', MODEL_FOR_PRETRAINING_MAPPING, head_doc='pretraining')
A:transformers.models.auto.modeling_auto._AutoModelWithLMHead->auto_class_factory('AutoModelWithLMHead', MODEL_WITH_LM_HEAD_MAPPING, head_doc='language modeling')
A:transformers.models.auto.modeling_auto.AutoModelForCausalLM->auto_class_factory('AutoModelForCausalLM', MODEL_FOR_CAUSAL_LM_MAPPING, head_doc='causal language modeling')
A:transformers.models.auto.modeling_auto.AutoModelForMaskedLM->auto_class_factory('AutoModelForMaskedLM', MODEL_FOR_MASKED_LM_MAPPING, head_doc='masked language modeling')
A:transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM->auto_class_factory('AutoModelForSeq2SeqLM', MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING, head_doc='sequence-to-sequence language modeling', checkpoint_for_example='t5-base')
A:transformers.models.auto.modeling_auto.AutoModelForSequenceClassification->auto_class_factory('AutoModelForSequenceClassification', MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING, head_doc='sequence classification')
A:transformers.models.auto.modeling_auto.AutoModelForQuestionAnswering->auto_class_factory('AutoModelForQuestionAnswering', MODEL_FOR_QUESTION_ANSWERING_MAPPING, head_doc='question answering')
A:transformers.models.auto.modeling_auto.AutoModelForTableQuestionAnswering->auto_class_factory('AutoModelForTableQuestionAnswering', MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING, head_doc='table question answering', checkpoint_for_example='google/tapas-base-finetuned-wtq')
A:transformers.models.auto.modeling_auto.AutoModelForTokenClassification->auto_class_factory('AutoModelForTokenClassification', MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING, head_doc='token classification')
A:transformers.models.auto.modeling_auto.AutoModelForMultipleChoice->auto_class_factory('AutoModelForMultipleChoice', MODEL_FOR_MULTIPLE_CHOICE_MAPPING, head_doc='multiple choice')
A:transformers.models.auto.modeling_auto.AutoModelForNextSentencePrediction->auto_class_factory('AutoModelForNextSentencePrediction', MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING, head_doc='next sentence prediction')
A:transformers.models.auto.modeling_auto.AutoModelForImageClassification->auto_class_factory('AutoModelForImageClassification', MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING, head_doc='image classification')
transformers.AutoModelWithLMHead(_AutoModelWithLMHead)
transformers.AutoModelWithLMHead.from_config(cls,config)
transformers.AutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.models.auto.modeling_auto.AutoModelWithLMHead(_AutoModelWithLMHead)
transformers.models.auto.modeling_auto.AutoModelWithLMHead.from_config(cls,config)
transformers.models.auto.modeling_auto.AutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/feature_extraction_auto.py----------------------------------------
A:transformers.models.auto.feature_extraction_auto.FEATURE_EXTRACTOR_MAPPING->OrderedDict([(DeiTConfig, DeiTFeatureExtractor), (Speech2TextConfig, Speech2TextFeatureExtractor), (ViTConfig, ViTFeatureExtractor), (Wav2Vec2Config, Wav2Vec2FeatureExtractor)])
A:transformers.models.auto.feature_extraction_auto.config->configuration_auto.AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.models.auto.feature_extraction_auto.is_feature_extraction_file->os.path.isfile(pretrained_model_name_or_path)
A:transformers.models.auto.feature_extraction_auto.(config_dict, _)->feature_extraction_utils.FeatureExtractionMixin.get_feature_extractor_dict(pretrained_model_name_or_path, **kwargs)
A:transformers.models.auto.feature_extraction_auto.feature_extractor_class->feature_extractor_class_from_name(config_dict['feature_extractor_type'])
transformers.AutoFeatureExtractor(self)
transformers.AutoFeatureExtractor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.auto.feature_extraction_auto.AutoFeatureExtractor(self)
transformers.models.auto.feature_extraction_auto.AutoFeatureExtractor.__init__(self)
transformers.models.auto.feature_extraction_auto.AutoFeatureExtractor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.auto.feature_extraction_auto.feature_extractor_class_from_name(class_name:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/configuration_auto.py----------------------------------------
A:transformers.models.auto.configuration_auto.ALL_PRETRAINED_CONFIG_ARCHIVE_MAP->dict(((key, value) for pretrained_map in [VISUAL_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, ROFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP, CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP, BIGBIRD_PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP, DEIT_PRETRAINED_CONFIG_ARCHIVE_MAP, LUKE_PRETRAINED_CONFIG_ARCHIVE_MAP, DETR_PRETRAINED_CONFIG_ARCHIVE_MAP, GPT_NEO_PRETRAINED_CONFIG_ARCHIVE_MAP, BIG_BIRD_PRETRAINED_CONFIG_ARCHIVE_MAP, MEGATRON_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, SPEECH_TO_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP, VIT_PRETRAINED_CONFIG_ARCHIVE_MAP, WAV_2_VEC_2_PRETRAINED_CONFIG_ARCHIVE_MAP, M2M_100_PRETRAINED_CONFIG_ARCHIVE_MAP, CONVBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, LED_PRETRAINED_CONFIG_ARCHIVE_MAP, BLENDERBOT_SMALL_PRETRAINED_CONFIG_ARCHIVE_MAP, BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, BART_PRETRAINED_CONFIG_ARCHIVE_MAP, BLENDERBOT_PRETRAINED_CONFIG_ARCHIVE_MAP, MBART_PRETRAINED_CONFIG_ARCHIVE_MAP, OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP, TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP, GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP, CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP, XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_PRETRAINED_CONFIG_ARCHIVE_MAP, ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, T5_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, FSMT_PRETRAINED_CONFIG_ARCHIVE_MAP, ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP, LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP, RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, FUNNEL_PRETRAINED_CONFIG_ARCHIVE_MAP, LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP, LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP, DPR_PRETRAINED_CONFIG_ARCHIVE_MAP, DEBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, DEBERTA_V2_PRETRAINED_CONFIG_ARCHIVE_MAP, SQUEEZEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP, PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP, MPNET_PRETRAINED_CONFIG_ARCHIVE_MAP, TAPAS_PRETRAINED_CONFIG_ARCHIVE_MAP, IBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, HUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP] for (key, value) in pretrained_map.items()))
A:transformers.models.auto.configuration_auto.CONFIG_MAPPING->OrderedDict([('visual_bert', VisualBertConfig), ('roformer', RoFormerConfig), ('clip', CLIPConfig), ('bigbird_pegasus', BigBirdPegasusConfig), ('deit', DeiTConfig), ('luke', LukeConfig), ('detr', DetrConfig), ('gpt_neo', GPTNeoConfig), ('big_bird', BigBirdConfig), ('speech_to_text', Speech2TextConfig), ('vit', ViTConfig), ('wav2vec2', Wav2Vec2Config), ('m2m_100', M2M100Config), ('convbert', ConvBertConfig), ('led', LEDConfig), ('blenderbot-small', BlenderbotSmallConfig), ('retribert', RetriBertConfig), ('ibert', IBertConfig), ('mt5', MT5Config), ('t5', T5Config), ('mobilebert', MobileBertConfig), ('distilbert', DistilBertConfig), ('albert', AlbertConfig), ('bert-generation', BertGenerationConfig), ('camembert', CamembertConfig), ('xlm-roberta', XLMRobertaConfig), ('pegasus', PegasusConfig), ('marian', MarianConfig), ('mbart', MBartConfig), ('megatron_bert', MegatronBertConfig), ('mpnet', MPNetConfig), ('bart', BartConfig), ('blenderbot', BlenderbotConfig), ('reformer', ReformerConfig), ('longformer', LongformerConfig), ('roberta', RobertaConfig), ('deberta-v2', DebertaV2Config), ('deberta', DebertaConfig), ('flaubert', FlaubertConfig), ('fsmt', FSMTConfig), ('squeezebert', SqueezeBertConfig), ('hubert', HubertConfig), ('bert', BertConfig), ('openai-gpt', OpenAIGPTConfig), ('gpt2', GPT2Config), ('transfo-xl', TransfoXLConfig), ('xlnet', XLNetConfig), ('xlm-prophetnet', XLMProphetNetConfig), ('prophetnet', ProphetNetConfig), ('xlm', XLMConfig), ('ctrl', CTRLConfig), ('electra', ElectraConfig), ('encoder-decoder', EncoderDecoderConfig), ('funnel', FunnelConfig), ('lxmert', LxmertConfig), ('dpr', DPRConfig), ('layoutlm', LayoutLMConfig), ('rag', RagConfig), ('tapas', TapasConfig)])
A:transformers.models.auto.configuration_auto.MODEL_NAMES_MAPPING->OrderedDict([('visual_bert', 'VisualBert'), ('roformer', 'RoFormer'), ('clip', 'CLIP'), ('bigbird_pegasus', 'BigBirdPegasus'), ('deit', 'DeiT'), ('luke', 'LUKE'), ('detr', 'DETR'), ('gpt_neo', 'GPT Neo'), ('big_bird', 'BigBird'), ('speech_to_text', 'Speech2Text'), ('vit', 'ViT'), ('wav2vec2', 'Wav2Vec2'), ('m2m_100', 'M2M100'), ('convbert', 'ConvBERT'), ('led', 'LED'), ('blenderbot-small', 'BlenderbotSmall'), ('retribert', 'RetriBERT'), ('ibert', 'I-BERT'), ('t5', 'T5'), ('mobilebert', 'MobileBERT'), ('distilbert', 'DistilBERT'), ('albert', 'ALBERT'), ('bert-generation', 'Bert Generation'), ('camembert', 'CamemBERT'), ('xlm-roberta', 'XLM-RoBERTa'), ('pegasus', 'Pegasus'), ('blenderbot', 'Blenderbot'), ('marian', 'Marian'), ('mbart', 'mBART'), ('megatron_bert', 'MegatronBert'), ('bart', 'BART'), ('reformer', 'Reformer'), ('longformer', 'Longformer'), ('roberta', 'RoBERTa'), ('flaubert', 'FlauBERT'), ('fsmt', 'FairSeq Machine-Translation'), ('squeezebert', 'SqueezeBERT'), ('bert', 'BERT'), ('openai-gpt', 'OpenAI GPT'), ('gpt2', 'OpenAI GPT-2'), ('transfo-xl', 'Transformer-XL'), ('xlnet', 'XLNet'), ('xlm', 'XLM'), ('ctrl', 'CTRL'), ('electra', 'ELECTRA'), ('encoder-decoder', 'Encoder decoder'), ('funnel', 'Funnel Transformer'), ('lxmert', 'LXMERT'), ('deberta-v2', 'DeBERTa-v2'), ('deberta', 'DeBERTa'), ('layoutlm', 'LayoutLM'), ('dpr', 'DPR'), ('rag', 'RAG'), ('xlm-prophetnet', 'XLMProphetNet'), ('prophetnet', 'ProphetNet'), ('mt5', 'mT5'), ('mpnet', 'MPNet'), ('tapas', 'TAPAS'), ('hubert', 'Hubert')])
A:transformers.models.auto.configuration_auto.lines->'\n'.join(lines).split('\n')
A:transformers.models.auto.configuration_auto.lines[i]->_list_model_options(indent, config_to_class=config_to_class, use_model_types=use_model_types)
A:transformers.models.auto.configuration_auto.docstrings->'\n'.join(lines)
A:transformers.models.auto.configuration_auto.(config_dict, _)->configuration_utils.PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
transformers.AutoConfig(self)
transformers.AutoConfig.for_model(cls,model_type:str,*args,**kwargs)
transformers.AutoConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.auto.configuration_auto.AutoConfig(self)
transformers.models.auto.configuration_auto.AutoConfig.__init__(self)
transformers.models.auto.configuration_auto.AutoConfig.for_model(cls,model_type:str,*args,**kwargs)
transformers.models.auto.configuration_auto.AutoConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.auto.configuration_auto._get_class_name(model_class)
transformers.models.auto.configuration_auto._list_model_options(indent,config_to_class=None,use_model_types=True)
transformers.models.auto.configuration_auto.replace_list_option_in_docstrings(config_to_class=None,use_model_types=True)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/__init__.py----------------------------------------
A:transformers.models.auto.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/modeling_tf_auto.py----------------------------------------
A:transformers.models.auto.modeling_tf_auto.logger->utils.logging.get_logger(__name__)
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerModel), (ConvBertConfig, TFConvBertModel), (LEDConfig, TFLEDModel), (LxmertConfig, TFLxmertModel), (MT5Config, TFMT5Model), (T5Config, TFT5Model), (DistilBertConfig, TFDistilBertModel), (AlbertConfig, TFAlbertModel), (BartConfig, TFBartModel), (CamembertConfig, TFCamembertModel), (XLMRobertaConfig, TFXLMRobertaModel), (LongformerConfig, TFLongformerModel), (RobertaConfig, TFRobertaModel), (LayoutLMConfig, TFLayoutLMModel), (BertConfig, TFBertModel), (OpenAIGPTConfig, TFOpenAIGPTModel), (GPT2Config, TFGPT2Model), (MobileBertConfig, TFMobileBertModel), (TransfoXLConfig, TFTransfoXLModel), (XLNetConfig, TFXLNetModel), (FlaubertConfig, TFFlaubertModel), (XLMConfig, TFXLMModel), (CTRLConfig, TFCTRLModel), (ElectraConfig, TFElectraModel), (FunnelConfig, (TFFunnelModel, TFFunnelBaseModel)), (DPRConfig, TFDPRQuestionEncoder), (MPNetConfig, TFMPNetModel), (BartConfig, TFBartModel), (MBartConfig, TFMBartModel), (MarianConfig, TFMarianModel), (PegasusConfig, TFPegasusModel), (BlenderbotConfig, TFBlenderbotModel), (BlenderbotSmallConfig, TFBlenderbotSmallModel), (Wav2Vec2Config, TFWav2Vec2Model)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_PRETRAINING_MAPPING->OrderedDict([(LxmertConfig, TFLxmertForPreTraining), (T5Config, TFT5ForConditionalGeneration), (DistilBertConfig, TFDistilBertForMaskedLM), (AlbertConfig, TFAlbertForPreTraining), (BartConfig, TFBartForConditionalGeneration), (CamembertConfig, TFCamembertForMaskedLM), (XLMRobertaConfig, TFXLMRobertaForMaskedLM), (RobertaConfig, TFRobertaForMaskedLM), (LayoutLMConfig, TFLayoutLMForMaskedLM), (BertConfig, TFBertForPreTraining), (OpenAIGPTConfig, TFOpenAIGPTLMHeadModel), (GPT2Config, TFGPT2LMHeadModel), (MobileBertConfig, TFMobileBertForPreTraining), (TransfoXLConfig, TFTransfoXLLMHeadModel), (XLNetConfig, TFXLNetLMHeadModel), (FlaubertConfig, TFFlaubertWithLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (CTRLConfig, TFCTRLLMHeadModel), (ElectraConfig, TFElectraForPreTraining), (FunnelConfig, TFFunnelForPreTraining), (MPNetConfig, TFMPNetForMaskedLM)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_WITH_LM_HEAD_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForMaskedLM), (ConvBertConfig, TFConvBertForMaskedLM), (LEDConfig, TFLEDForConditionalGeneration), (T5Config, TFT5ForConditionalGeneration), (DistilBertConfig, TFDistilBertForMaskedLM), (AlbertConfig, TFAlbertForMaskedLM), (MarianConfig, TFMarianMTModel), (BartConfig, TFBartForConditionalGeneration), (CamembertConfig, TFCamembertForMaskedLM), (XLMRobertaConfig, TFXLMRobertaForMaskedLM), (LongformerConfig, TFLongformerForMaskedLM), (RobertaConfig, TFRobertaForMaskedLM), (LayoutLMConfig, TFLayoutLMForMaskedLM), (BertConfig, TFBertForMaskedLM), (OpenAIGPTConfig, TFOpenAIGPTLMHeadModel), (GPT2Config, TFGPT2LMHeadModel), (MobileBertConfig, TFMobileBertForMaskedLM), (TransfoXLConfig, TFTransfoXLLMHeadModel), (XLNetConfig, TFXLNetLMHeadModel), (FlaubertConfig, TFFlaubertWithLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (CTRLConfig, TFCTRLLMHeadModel), (ElectraConfig, TFElectraForMaskedLM), (FunnelConfig, TFFunnelForMaskedLM), (MPNetConfig, TFMPNetForMaskedLM)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_CAUSAL_LM_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForCausalLM), (BertConfig, TFBertLMHeadModel), (OpenAIGPTConfig, TFOpenAIGPTLMHeadModel), (GPT2Config, TFGPT2LMHeadModel), (TransfoXLConfig, TFTransfoXLLMHeadModel), (XLNetConfig, TFXLNetLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (CTRLConfig, TFCTRLLMHeadModel)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_MASKED_LM_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForMaskedLM), (ConvBertConfig, TFConvBertForMaskedLM), (DistilBertConfig, TFDistilBertForMaskedLM), (AlbertConfig, TFAlbertForMaskedLM), (CamembertConfig, TFCamembertForMaskedLM), (XLMRobertaConfig, TFXLMRobertaForMaskedLM), (LongformerConfig, TFLongformerForMaskedLM), (RobertaConfig, TFRobertaForMaskedLM), (LayoutLMConfig, TFLayoutLMForMaskedLM), (BertConfig, TFBertForMaskedLM), (MobileBertConfig, TFMobileBertForMaskedLM), (FlaubertConfig, TFFlaubertWithLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (ElectraConfig, TFElectraForMaskedLM), (FunnelConfig, TFFunnelForMaskedLM), (MPNetConfig, TFMPNetForMaskedLM)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING->OrderedDict([(LEDConfig, TFLEDForConditionalGeneration), (MT5Config, TFMT5ForConditionalGeneration), (T5Config, TFT5ForConditionalGeneration), (MarianConfig, TFMarianMTModel), (MBartConfig, TFMBartForConditionalGeneration), (PegasusConfig, TFPegasusForConditionalGeneration), (BlenderbotConfig, TFBlenderbotForConditionalGeneration), (BlenderbotSmallConfig, TFBlenderbotSmallForConditionalGeneration), (BartConfig, TFBartForConditionalGeneration)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForSequenceClassification), (ConvBertConfig, TFConvBertForSequenceClassification), (DistilBertConfig, TFDistilBertForSequenceClassification), (AlbertConfig, TFAlbertForSequenceClassification), (CamembertConfig, TFCamembertForSequenceClassification), (XLMRobertaConfig, TFXLMRobertaForSequenceClassification), (LongformerConfig, TFLongformerForSequenceClassification), (RobertaConfig, TFRobertaForSequenceClassification), (LayoutLMConfig, TFLayoutLMForSequenceClassification), (BertConfig, TFBertForSequenceClassification), (XLNetConfig, TFXLNetForSequenceClassification), (MobileBertConfig, TFMobileBertForSequenceClassification), (FlaubertConfig, TFFlaubertForSequenceClassification), (XLMConfig, TFXLMForSequenceClassification), (ElectraConfig, TFElectraForSequenceClassification), (FunnelConfig, TFFunnelForSequenceClassification), (GPT2Config, TFGPT2ForSequenceClassification), (MPNetConfig, TFMPNetForSequenceClassification), (OpenAIGPTConfig, TFOpenAIGPTForSequenceClassification), (TransfoXLConfig, TFTransfoXLForSequenceClassification), (CTRLConfig, TFCTRLForSequenceClassification)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForQuestionAnswering), (ConvBertConfig, TFConvBertForQuestionAnswering), (DistilBertConfig, TFDistilBertForQuestionAnswering), (AlbertConfig, TFAlbertForQuestionAnswering), (CamembertConfig, TFCamembertForQuestionAnswering), (XLMRobertaConfig, TFXLMRobertaForQuestionAnswering), (LongformerConfig, TFLongformerForQuestionAnswering), (RobertaConfig, TFRobertaForQuestionAnswering), (BertConfig, TFBertForQuestionAnswering), (XLNetConfig, TFXLNetForQuestionAnsweringSimple), (MobileBertConfig, TFMobileBertForQuestionAnswering), (FlaubertConfig, TFFlaubertForQuestionAnsweringSimple), (XLMConfig, TFXLMForQuestionAnsweringSimple), (ElectraConfig, TFElectraForQuestionAnswering), (FunnelConfig, TFFunnelForQuestionAnswering), (MPNetConfig, TFMPNetForQuestionAnswering)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForTokenClassification), (ConvBertConfig, TFConvBertForTokenClassification), (DistilBertConfig, TFDistilBertForTokenClassification), (AlbertConfig, TFAlbertForTokenClassification), (CamembertConfig, TFCamembertForTokenClassification), (FlaubertConfig, TFFlaubertForTokenClassification), (XLMConfig, TFXLMForTokenClassification), (XLMRobertaConfig, TFXLMRobertaForTokenClassification), (LongformerConfig, TFLongformerForTokenClassification), (RobertaConfig, TFRobertaForTokenClassification), (LayoutLMConfig, TFLayoutLMForTokenClassification), (BertConfig, TFBertForTokenClassification), (MobileBertConfig, TFMobileBertForTokenClassification), (XLNetConfig, TFXLNetForTokenClassification), (ElectraConfig, TFElectraForTokenClassification), (FunnelConfig, TFFunnelForTokenClassification), (MPNetConfig, TFMPNetForTokenClassification)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING->OrderedDict([(RoFormerConfig, TFRoFormerForMultipleChoice), (ConvBertConfig, TFConvBertForMultipleChoice), (CamembertConfig, TFCamembertForMultipleChoice), (XLMConfig, TFXLMForMultipleChoice), (XLMRobertaConfig, TFXLMRobertaForMultipleChoice), (LongformerConfig, TFLongformerForMultipleChoice), (RobertaConfig, TFRobertaForMultipleChoice), (BertConfig, TFBertForMultipleChoice), (DistilBertConfig, TFDistilBertForMultipleChoice), (MobileBertConfig, TFMobileBertForMultipleChoice), (XLNetConfig, TFXLNetForMultipleChoice), (FlaubertConfig, TFFlaubertForMultipleChoice), (AlbertConfig, TFAlbertForMultipleChoice), (ElectraConfig, TFElectraForMultipleChoice), (FunnelConfig, TFFunnelForMultipleChoice), (MPNetConfig, TFMPNetForMultipleChoice)])
A:transformers.models.auto.modeling_tf_auto.TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING->OrderedDict([(BertConfig, TFBertForNextSentencePrediction), (MobileBertConfig, TFMobileBertForNextSentencePrediction)])
A:transformers.models.auto.modeling_tf_auto.TFAutoModel->auto_class_factory('TFAutoModel', TF_MODEL_MAPPING)
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForPreTraining->auto_class_factory('TFAutoModelForPreTraining', TF_MODEL_FOR_PRETRAINING_MAPPING, head_doc='pretraining')
A:transformers.models.auto.modeling_tf_auto._TFAutoModelWithLMHead->auto_class_factory('TFAutoModelWithLMHead', TF_MODEL_WITH_LM_HEAD_MAPPING, head_doc='language modeling')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM->auto_class_factory('TFAutoModelForCausalLM', TF_MODEL_FOR_CAUSAL_LM_MAPPING, head_doc='causal language modeling')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForMaskedLM->auto_class_factory('TFAutoModelForMaskedLM', TF_MODEL_FOR_MASKED_LM_MAPPING, head_doc='masked language modeling')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM->auto_class_factory('TFAutoModelForSeq2SeqLM', TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING, head_doc='sequence-to-sequence language modeling', checkpoint_for_example='t5-base')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification->auto_class_factory('TFAutoModelForSequenceClassification', TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING, head_doc='sequence classification')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForQuestionAnswering->auto_class_factory('TFAutoModelForQuestionAnswering', TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING, head_doc='question answering')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification->auto_class_factory('TFAutoModelForTokenClassification', TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING, head_doc='token classification')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForMultipleChoice->auto_class_factory('TFAutoModelForMultipleChoice', TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING, head_doc='multiple choice')
A:transformers.models.auto.modeling_tf_auto.TFAutoModelForNextSentencePrediction->auto_class_factory('TFAutoModelForNextSentencePrediction', TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING, head_doc='next sentence prediction')
transformers.TFAutoModelWithLMHead(_TFAutoModelWithLMHead)
transformers.TFAutoModelWithLMHead.from_config(cls,config)
transformers.TFAutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.models.auto.modeling_tf_auto.TFAutoModelWithLMHead(_TFAutoModelWithLMHead)
transformers.models.auto.modeling_tf_auto.TFAutoModelWithLMHead.from_config(cls,config)
transformers.models.auto.modeling_tf_auto.TFAutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/auto/tokenization_auto.py----------------------------------------
A:transformers.models.auto.tokenization_auto.logger->utils.logging.get_logger(__name__)
A:transformers.models.auto.tokenization_auto.TOKENIZER_MAPPING->OrderedDict([(RetriBertConfig, (RetriBertTokenizer, RetriBertTokenizerFast)), (RoFormerConfig, (RoFormerTokenizer, None)), (T5Config, (T5Tokenizer, T5TokenizerFast)), (MT5Config, (MT5Tokenizer, MT5TokenizerFast)), (MobileBertConfig, (MobileBertTokenizer, MobileBertTokenizerFast)), (DistilBertConfig, (DistilBertTokenizer, DistilBertTokenizerFast)), (AlbertConfig, (AlbertTokenizer, AlbertTokenizerFast)), (CamembertConfig, (CamembertTokenizer, CamembertTokenizerFast)), (PegasusConfig, (PegasusTokenizer, PegasusTokenizerFast)), (MBartConfig, (MBartTokenizer, MBartTokenizerFast)), (XLMRobertaConfig, (XLMRobertaTokenizer, XLMRobertaTokenizerFast)), (MarianConfig, (MarianTokenizer, None)), (BlenderbotSmallConfig, (BlenderbotSmallTokenizer, None)), (BlenderbotConfig, (BlenderbotTokenizer, None)), (BartConfig, (BartTokenizer, BartTokenizerFast)), (LongformerConfig, (LongformerTokenizer, LongformerTokenizerFast)), (RobertaConfig, (RobertaTokenizer, RobertaTokenizerFast)), (ReformerConfig, (ReformerTokenizer, ReformerTokenizerFast)), (ElectraConfig, (ElectraTokenizer, ElectraTokenizerFast)), (FunnelConfig, (FunnelTokenizer, FunnelTokenizerFast)), (LxmertConfig, (LxmertTokenizer, LxmertTokenizerFast)), (LayoutLMConfig, (LayoutLMTokenizer, LayoutLMTokenizerFast)), (DPRConfig, (DPRQuestionEncoderTokenizer, DPRQuestionEncoderTokenizerFast)), (SqueezeBertConfig, (SqueezeBertTokenizer, SqueezeBertTokenizerFast)), (BertConfig, (BertTokenizer, BertTokenizerFast)), (OpenAIGPTConfig, (OpenAIGPTTokenizer, OpenAIGPTTokenizerFast)), (GPT2Config, (GPT2Tokenizer, GPT2TokenizerFast)), (TransfoXLConfig, (TransfoXLTokenizer, None)), (XLNetConfig, (XLNetTokenizer, XLNetTokenizerFast)), (FlaubertConfig, (FlaubertTokenizer, None)), (XLMConfig, (XLMTokenizer, None)), (CTRLConfig, (CTRLTokenizer, None)), (FSMTConfig, (FSMTTokenizer, None)), (BertGenerationConfig, (BertGenerationTokenizer, None)), (DebertaConfig, (DebertaTokenizer, DebertaTokenizerFast)), (DebertaV2Config, (DebertaV2Tokenizer, None)), (RagConfig, (RagTokenizer, None)), (XLMProphetNetConfig, (XLMProphetNetTokenizer, None)), (Speech2TextConfig, (Speech2TextTokenizer, None)), (M2M100Config, (M2M100Tokenizer, None)), (ProphetNetConfig, (ProphetNetTokenizer, None)), (MPNetConfig, (MPNetTokenizer, MPNetTokenizerFast)), (TapasConfig, (TapasTokenizer, None)), (LEDConfig, (LEDTokenizer, LEDTokenizerFast)), (ConvBertConfig, (ConvBertTokenizer, ConvBertTokenizerFast)), (BigBirdConfig, (BigBirdTokenizer, BigBirdTokenizerFast)), (IBertConfig, (RobertaTokenizer, RobertaTokenizerFast)), (Wav2Vec2Config, (Wav2Vec2CTCTokenizer, None)), (HubertConfig, (Wav2Vec2CTCTokenizer, None)), (GPTNeoConfig, (GPT2Tokenizer, GPT2TokenizerFast)), (LukeConfig, (LukeTokenizer, None)), (BigBirdPegasusConfig, (PegasusTokenizer, PegasusTokenizerFast))])
A:transformers.models.auto.tokenization_auto.pretrained_model_name_or_path->str(pretrained_model_name_or_path)
A:transformers.models.auto.tokenization_auto.config_file->hf_bucket_url(pretrained_model_name_or_path, filename=TOKENIZER_CONFIG_FILE, revision=revision, mirror=None)
A:transformers.models.auto.tokenization_auto.resolved_config_file->cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token)
A:transformers.models.auto.tokenization_auto.config->configuration_auto.AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.models.auto.tokenization_auto.use_fast->kwargs.pop('use_fast', True)
A:transformers.models.auto.tokenization_auto.tokenizer_config->get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
A:transformers.models.auto.tokenization_auto.config_tokenizer_class->get_tokenizer_config(pretrained_model_name_or_path, **kwargs).get('tokenizer_class')
A:transformers.models.auto.tokenization_auto.tokenizer_class->tokenizer_class_from_name(tokenizer_class_candidate)
transformers.AutoTokenizer(self)
transformers.AutoTokenizer.from_pretrained(cls,pretrained_model_name_or_path,*inputs,**kwargs)
transformers.models.auto.tokenization_auto.AutoTokenizer(self)
transformers.models.auto.tokenization_auto.AutoTokenizer.__init__(self)
transformers.models.auto.tokenization_auto.AutoTokenizer.from_pretrained(cls,pretrained_model_name_or_path,*inputs,**kwargs)
transformers.models.auto.tokenization_auto.get_tokenizer_config(pretrained_model_name_or_path:Union[str,os.PathLike],cache_dir:Optional[Union[str,os.PathLike]]=None,force_download:bool=False,resume_download:bool=False,proxies:Optional[Dict[str,str]]=None,use_auth_token:Optional[Union[bool,str]]=None,revision:Optional[str]=None,local_files_only:bool=False,**kwargs)
transformers.models.auto.tokenization_auto.tokenizer_class_from_name(class_name:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.electra.convert_electra_original_tf_checkpoint_to_pytorch.config->transformers.ElectraConfig.from_json_file(config_file)
A:transformers.models.electra.convert_electra_original_tf_checkpoint_to_pytorch.model->ElectraForMaskedLM(config)
A:transformers.models.electra.convert_electra_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.electra.convert_electra_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.electra.convert_electra_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path,discriminator_or_generator)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/configuration_electra.py----------------------------------------
A:transformers.models.electra.configuration_electra.logger->utils.logging.get_logger(__name__)
transformers.ElectraConfig(self,vocab_size=30522,embedding_size=128,hidden_size=256,num_hidden_layers=12,num_attention_heads=4,intermediate_size=1024,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,summary_type='first',summary_use_proj=True,summary_activation='gelu',summary_last_dropout=0.1,pad_token_id=0,position_embedding_type='absolute',**kwargs)
transformers.models.electra.configuration_electra.ElectraConfig(self,vocab_size=30522,embedding_size=128,hidden_size=256,num_hidden_layers=12,num_attention_heads=4,intermediate_size=1024,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,summary_type='first',summary_use_proj=True,summary_activation='gelu',summary_last_dropout=0.1,pad_token_id=0,position_embedding_type='absolute',**kwargs)
transformers.models.electra.configuration_electra.ElectraConfig.__init__(self,vocab_size=30522,embedding_size=128,hidden_size=256,num_hidden_layers=12,num_attention_heads=4,intermediate_size=1024,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,summary_type='first',summary_use_proj=True,summary_activation='gelu',summary_last_dropout=0.1,pad_token_id=0,position_embedding_type='absolute',**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/modeling_flax_electra.py----------------------------------------
A:transformers.models.electra.modeling_flax_electra.logger->utils.logging.get_logger(__name__)
A:transformers.models.electra.modeling_flax_electra.self.word_embeddings->flax.linen.Embed(self.config.vocab_size, self.config.embedding_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.position_embeddings->flax.linen.Embed(self.config.max_position_embeddings, self.config.embedding_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.token_type_embeddings->flax.linen.Embed(self.config.type_vocab_size, self.config.embedding_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.LayerNorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.dropout->flax.linen.Dropout(self.config.hidden_dropout_prob)
A:transformers.models.electra.modeling_flax_electra.inputs_embeds->self.word_embeddings(input_ids.astype('i4'))
A:transformers.models.electra.modeling_flax_electra.position_embeds->self.position_embeddings(position_ids.astype('i4'))
A:transformers.models.electra.modeling_flax_electra.token_type_embeddings->self.token_type_embeddings(token_type_ids.astype('i4'))
A:transformers.models.electra.modeling_flax_electra.hidden_states->self.dropout(hidden_states, deterministic=deterministic)
A:transformers.models.electra.modeling_flax_electra.self.query->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.electra.modeling_flax_electra.self.key->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.electra.modeling_flax_electra.self.value->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.electra.modeling_flax_electra.query_states->self.query(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.electra.modeling_flax_electra.value_states->self.value(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.electra.modeling_flax_electra.key_states->self.key(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.electra.modeling_flax_electra.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.electra.modeling_flax_electra.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, -10000000000.0).astype(self.dtype))
A:transformers.models.electra.modeling_flax_electra.dropout_rng->self.make_rng('dropout')
A:transformers.models.electra.modeling_flax_electra.attn_weights->dot_product_attention_weights(query_states, key_states, bias=attention_bias, dropout_rng=dropout_rng, dropout_rate=self.config.attention_probs_dropout_prob, broadcast_dropout=True, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.electra.modeling_flax_electra.attn_output->attn_output.reshape(attn_output.shape[:2] + (-1,)).reshape(attn_output.shape[:2] + (-1,))
A:transformers.models.electra.modeling_flax_electra.self.dense->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.self->FlaxElectraSelfAttention(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.output->FlaxElectraOutput(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.attn_outputs->self.self(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.electra.modeling_flax_electra.self.attention->FlaxElectraAttention(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.intermediate->FlaxElectraIntermediate(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.attention_outputs->self.attention(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.electra.modeling_flax_electra.layer_outputs->layer(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.electra.modeling_flax_electra.self.layer->FlaxElectraLayerCollection(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.dense_prediction->flax.linen.Dense(1, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.electra.modeling_flax_electra.input_ids->jax.numpy.zeros(input_shape, dtype='i4')
A:transformers.models.electra.modeling_flax_electra.token_type_ids->jax.numpy.ones_like(input_ids)
A:transformers.models.electra.modeling_flax_electra.position_ids->jax.numpy.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)
A:transformers.models.electra.modeling_flax_electra.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.electra.modeling_flax_electra.self.embeddings->FlaxElectraEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.embeddings_project->flax.linen.Dense(self.config.hidden_size)
A:transformers.models.electra.modeling_flax_electra.self.encoder->FlaxElectraEncoder(self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.embeddings->self.embeddings_project(embeddings)
A:transformers.models.electra.modeling_flax_electra.bias->self.param('bias', self.bias_init, (self.embedding_size,))
A:transformers.models.electra.modeling_flax_electra.self.bias->jax.numpy.asarray(bias, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.y->jax.lax.dot_general(x, kernel, (((x.ndim - 1,), (0,)), ((), ())), precision=self.precision)
A:transformers.models.electra.modeling_flax_electra.self.electra->FlaxElectraModule(config=self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.generator_predictions->FlaxElectraGeneratorPredictions(config=self.config)
A:transformers.models.electra.modeling_flax_electra.self.generator_lm_head->flax.linen.Dense(self.config.vocab_size, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.outputs->self.electra(input_ids, attention_mask, token_type_ids, position_ids, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.electra.modeling_flax_electra.prediction_scores->self.generator_lm_head(prediction_scores)
A:transformers.models.electra.modeling_flax_electra.self.discriminator_predictions->FlaxElectraDiscriminatorPredictions(config=self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.logits->self.classifier(hidden_states, deterministic=deterministic)
A:transformers.models.electra.modeling_flax_electra.self.classifier->FlaxElectraClassificationHead(config=self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.self.summary->flax.linen.Dense(num_classes, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.activation_string->getattr(self.config, 'summary_activation', None)
A:transformers.models.electra.modeling_flax_electra.self.first_dropout->flax.linen.Dropout(self.config.summary_first_dropout)
A:transformers.models.electra.modeling_flax_electra.self.last_dropout->flax.linen.Dropout(self.config.summary_last_dropout)
A:transformers.models.electra.modeling_flax_electra.output->self.last_dropout(output, deterministic=deterministic)
A:transformers.models.electra.modeling_flax_electra.self.sequence_summary->FlaxElectraSequenceSummary(config=self.config, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.pooled_output->self.sequence_summary(hidden_states, deterministic=deterministic)
A:transformers.models.electra.modeling_flax_electra.reshaped_logits->self.classifier(hidden_states, deterministic=deterministic).reshape(-1, num_choices)
A:transformers.models.electra.modeling_flax_electra.self.qa_outputs->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.(start_logits, end_logits)->self.classifier(hidden_states, deterministic=deterministic).split(self.config.num_labels, axis=-1)
A:transformers.models.electra.modeling_flax_electra.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.models.electra.modeling_flax_electra.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.models.electra.modeling_flax_electra.self.out_proj->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.electra.modeling_flax_electra.x->self.out_proj(x)
transformers.FlaxElectraForMaskedLM(FlaxElectraPreTrainedModel)
transformers.FlaxElectraForMaskedLMModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxElectraForMaskedLMModule.setup(self)
transformers.FlaxElectraForMultipleChoice(FlaxElectraPreTrainedModel)
transformers.FlaxElectraForMultipleChoiceModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxElectraForMultipleChoiceModule.setup(self)
transformers.FlaxElectraForPreTraining(FlaxElectraPreTrainedModel)
transformers.FlaxElectraForPreTrainingModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxElectraForPreTrainingModule.setup(self)
transformers.FlaxElectraForPreTrainingOutput(ModelOutput)
transformers.FlaxElectraForQuestionAnswering(FlaxElectraPreTrainedModel)
transformers.FlaxElectraForQuestionAnsweringModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxElectraForQuestionAnsweringModule.setup(self)
transformers.FlaxElectraForSequenceClassification(FlaxElectraPreTrainedModel)
transformers.FlaxElectraForSequenceClassificationModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxElectraForSequenceClassificationModule.setup(self)
transformers.FlaxElectraForTokenClassification(FlaxElectraPreTrainedModel)
transformers.FlaxElectraForTokenClassificationModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxElectraForTokenClassificationModule.setup(self)
transformers.FlaxElectraModel(FlaxElectraPreTrainedModel)
transformers.FlaxElectraPreTrainedModel(self,config:ElectraConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxElectraPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.electra.modeling_flax_electra.FlaxElectraAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.electra.modeling_flax_electra.FlaxElectraAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.electra.modeling_flax_electra.FlaxElectraAttention.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraClassificationHead(self,hidden_states,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraClassificationHead.__call__(self,hidden_states,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraClassificationHead.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraDiscriminatorPredictions(self,hidden_states)
transformers.models.electra.modeling_flax_electra.FlaxElectraDiscriminatorPredictions.__call__(self,hidden_states)
transformers.models.electra.modeling_flax_electra.FlaxElectraDiscriminatorPredictions.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraEmbeddings(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraEmbeddings.__call__(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraEmbeddings.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraEncoder(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraEncoder.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraEncoder.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMaskedLM(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMaskedLMModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMaskedLMModule.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMaskedLMModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMultipleChoice(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMultipleChoiceModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMultipleChoiceModule.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForMultipleChoiceModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraForPreTraining(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraForPreTrainingModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForPreTrainingModule.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForPreTrainingModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraForPreTrainingOutput(ModelOutput)
transformers.models.electra.modeling_flax_electra.FlaxElectraForQuestionAnswering(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraForQuestionAnsweringModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForQuestionAnsweringModule.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForQuestionAnsweringModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraForSequenceClassification(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraForSequenceClassificationModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForSequenceClassificationModule.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForSequenceClassificationModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraForTokenClassification(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraForTokenClassificationModule(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForTokenClassificationModule.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraForTokenClassificationModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraGeneratorPredictions(self,hidden_states)
transformers.models.electra.modeling_flax_electra.FlaxElectraGeneratorPredictions.__call__(self,hidden_states)
transformers.models.electra.modeling_flax_electra.FlaxElectraGeneratorPredictions.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraIntermediate(self,hidden_states)
transformers.models.electra.modeling_flax_electra.FlaxElectraIntermediate.__call__(self,hidden_states)
transformers.models.electra.modeling_flax_electra.FlaxElectraIntermediate.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraLayer(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.electra.modeling_flax_electra.FlaxElectraLayer.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.electra.modeling_flax_electra.FlaxElectraLayer.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraLayerCollection(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraLayerCollection.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraLayerCollection.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraModel(FlaxElectraPreTrainedModel)
transformers.models.electra.modeling_flax_electra.FlaxElectraModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraModule.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraOutput(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraOutput.__call__(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraOutput.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraPreTrainedModel(self,config:ElectraConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.electra.modeling_flax_electra.FlaxElectraPreTrainedModel.__init__(self,config:ElectraConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.electra.modeling_flax_electra.FlaxElectraPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.electra.modeling_flax_electra.FlaxElectraSelfAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.electra.modeling_flax_electra.FlaxElectraSelfAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.electra.modeling_flax_electra.FlaxElectraSelfAttention.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraSelfOutput(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraSelfOutput.__call__(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraSelfOutput.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraSequenceSummary(self,hidden_states,cls_index=None,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraSequenceSummary.__call__(self,hidden_states,cls_index=None,deterministic:bool=True)
transformers.models.electra.modeling_flax_electra.FlaxElectraSequenceSummary.setup(self)
transformers.models.electra.modeling_flax_electra.FlaxElectraTiedDense(self,x,kernel)
transformers.models.electra.modeling_flax_electra.FlaxElectraTiedDense.__call__(self,x,kernel)
transformers.models.electra.modeling_flax_electra.FlaxElectraTiedDense.setup(self)
transformers.models.electra.modeling_flax_electra.identity(x,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/modeling_tf_electra.py----------------------------------------
A:transformers.models.electra.modeling_tf_electra.logger->utils.logging.get_logger(__name__)
A:transformers.models.electra.modeling_tf_electra.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.electra.modeling_tf_electra.self.sqrt_att_head_size->math.sqrt(self.attention_head_size)
A:transformers.models.electra.modeling_tf_electra.self.query->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.electra.modeling_tf_electra.self.key->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.electra.modeling_tf_electra.self.value->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.electra.modeling_tf_electra.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.electra.modeling_tf_electra.tensor->tensorflow.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.electra.modeling_tf_electra.mixed_query_layer->self.query(inputs=hidden_states)
A:transformers.models.electra.modeling_tf_electra.mixed_key_layer->self.key(inputs=hidden_states)
A:transformers.models.electra.modeling_tf_electra.mixed_value_layer->self.value(inputs=hidden_states)
A:transformers.models.electra.modeling_tf_electra.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.electra.modeling_tf_electra.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.electra.modeling_tf_electra.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.electra.modeling_tf_electra.attention_scores->tensorflow.add(attention_scores, attention_mask)
A:transformers.models.electra.modeling_tf_electra.dk->tensorflow.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
A:transformers.models.electra.modeling_tf_electra.attention_probs->tensorflow.multiply(attention_probs, head_mask)
A:transformers.models.electra.modeling_tf_electra.attention_output->self.dense_output(hidden_states=self_outputs[0], input_tensor=input_tensor, training=training)
A:transformers.models.electra.modeling_tf_electra.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.models.electra.modeling_tf_electra.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.electra.modeling_tf_electra.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.bias)
A:transformers.models.electra.modeling_tf_electra.self.self_attention->TFElectraSelfAttention(config, name='self')
A:transformers.models.electra.modeling_tf_electra.self.dense_output->TFElectraSelfOutput(config, name='output')
A:transformers.models.electra.modeling_tf_electra.self_outputs->self.self_attention(hidden_states=input_tensor, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.electra.modeling_tf_electra.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.electra.modeling_tf_electra.self.attention->TFElectraAttention(config, name='attention')
A:transformers.models.electra.modeling_tf_electra.self.intermediate->TFElectraIntermediate(config, name='intermediate')
A:transformers.models.electra.modeling_tf_electra.self.bert_output->TFElectraOutput(config, name='output')
A:transformers.models.electra.modeling_tf_electra.attention_outputs->self.attention(input_tensor=hidden_states, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.electra.modeling_tf_electra.intermediate_output->self.intermediate(hidden_states=attention_output)
A:transformers.models.electra.modeling_tf_electra.layer_output->self.bert_output(hidden_states=intermediate_output, input_tensor=attention_output, training=training)
A:transformers.models.electra.modeling_tf_electra.layer_outputs->layer_module(hidden_states=hidden_states, attention_mask=attention_mask, head_mask=head_mask[i], output_attentions=output_attentions, training=training)
A:transformers.models.electra.modeling_tf_electra.pooled_output->self.dense(inputs=first_token_tensor)
A:transformers.models.electra.modeling_tf_electra.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.electra.modeling_tf_electra.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.electra.modeling_tf_electra.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.electra.modeling_tf_electra.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.electra.modeling_tf_electra.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.electra.modeling_tf_electra.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.electra.modeling_tf_electra.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.electra.modeling_tf_electra.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.electra.modeling_tf_electra.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.electra.modeling_tf_electra.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.electra.modeling_tf_electra.self.dense_prediction->tensorflow.keras.layers.Dense(1, name='dense_prediction')
A:transformers.models.electra.modeling_tf_electra.logits->self.qa_outputs(discriminator_sequence_output)
A:transformers.models.electra.modeling_tf_electra.self.embeddings->TFElectraEmbeddings(config, name='embeddings')
A:transformers.models.electra.modeling_tf_electra.self.embeddings_project->tensorflow.keras.layers.Dense(config.hidden_size, name='embeddings_project')
A:transformers.models.electra.modeling_tf_electra.self.encoder->TFElectraEncoder(config, name='encoder')
A:transformers.models.electra.modeling_tf_electra.attention_mask->tensorflow.fill(input_shape, 1)
A:transformers.models.electra.modeling_tf_electra.extended_attention_mask->self.get_extended_attention_mask(inputs['attention_mask'], input_shape, hidden_states.dtype)
A:transformers.models.electra.modeling_tf_electra.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.electra.modeling_tf_electra.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.electra.modeling_tf_electra.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.electra.modeling_tf_electra.inputs['token_type_ids']->tensorflow.fill(input_shape, 0)
A:transformers.models.electra.modeling_tf_electra.inputs['head_mask']->self.get_head_mask(inputs['head_mask'])
A:transformers.models.electra.modeling_tf_electra.self.electra->TFElectraMainLayer(config, name='electra')
A:transformers.models.electra.modeling_tf_electra.outputs->self.electra(flat_input_ids, flat_attention_mask, flat_token_type_ids, flat_position_ids, inputs['head_mask'], flat_inputs_embeds, inputs['output_attentions'], inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.electra.modeling_tf_electra.self.discriminator_predictions->TFElectraDiscriminatorPredictions(config, name='discriminator_predictions')
A:transformers.models.electra.modeling_tf_electra.discriminator_hidden_states->self.electra(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.electra.modeling_tf_electra.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.electra.modeling_tf_electra.self.generator_predictions->TFElectraGeneratorPredictions(config, name='generator_predictions')
A:transformers.models.electra.modeling_tf_electra.self.activation->get_tf_activation(config.hidden_act)
A:transformers.models.electra.modeling_tf_electra.self.generator_lm_head->TFElectraMaskedLMHead(config, self.electra.embeddings, name='generator_lm_head')
A:transformers.models.electra.modeling_tf_electra.generator_hidden_states->self.electra(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.electra.modeling_tf_electra.prediction_scores->self.generator_lm_head(prediction_scores, training=inputs['training'])
A:transformers.models.electra.modeling_tf_electra.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.models.electra.modeling_tf_electra.x->self.out_proj(x)
A:transformers.models.electra.modeling_tf_electra.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.electra.modeling_tf_electra.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.initializer_range, name='sequence_summary')
A:transformers.models.electra.modeling_tf_electra.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.electra.modeling_tf_electra.output->self.call(input_ids=inputs)
A:transformers.models.electra.modeling_tf_electra.discriminator_sequence_output->self.dropout(discriminator_sequence_output)
A:transformers.models.electra.modeling_tf_electra.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.electra.modeling_tf_electra.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.electra.modeling_tf_electra.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.electra.modeling_tf_electra.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.electra.modeling_tf_electra.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFElectraForMaskedLM(self,config,**kwargs)
transformers.TFElectraForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFElectraForMaskedLM.get_lm_head(self)
transformers.TFElectraForMaskedLM.get_prefix_bias_name(self)
transformers.TFElectraForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFElectraForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFElectraForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFElectraForMultipleChoice.dummy_inputs(self)
transformers.TFElectraForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.TFElectraForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFElectraForPreTraining(self,config,**kwargs)
transformers.TFElectraForPreTraining.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFElectraForPreTraining.serving_output(self,output)
transformers.TFElectraForPreTrainingOutput(ModelOutput)
transformers.TFElectraForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFElectraForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFElectraForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFElectraForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFElectraForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFElectraForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFElectraForTokenClassification(self,config,**kwargs)
transformers.TFElectraForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFElectraForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFElectraModel(self,config,*inputs,**kwargs)
transformers.TFElectraModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFElectraModel.serving_output(self,output)
transformers.TFElectraPreTrainedModel(TFPreTrainedModel)
transformers.models.electra.modeling_tf_electra.TFElectraAttention(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraAttention.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraAttention.call(self,input_tensor:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.electra.modeling_tf_electra.TFElectraAttention.prune_heads(self,heads)
transformers.models.electra.modeling_tf_electra.TFElectraClassificationHead(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraClassificationHead.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraClassificationHead.call(self,inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraDiscriminatorPredictions(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraDiscriminatorPredictions.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraDiscriminatorPredictions.call(self,discriminator_hidden_states,training=False)
transformers.models.electra.modeling_tf_electra.TFElectraEmbeddings(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraEmbeddings.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.electra.modeling_tf_electra.TFElectraEmbeddings.call(self,input_ids:tf.Tensor=None,position_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.models.electra.modeling_tf_electra.TFElectraEncoder(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraEncoder.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraEncoder.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,return_dict:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.electra.modeling_tf_electra.TFElectraForMaskedLM(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForMaskedLM.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForMaskedLM.get_lm_head(self)
transformers.models.electra.modeling_tf_electra.TFElectraForMaskedLM.get_prefix_bias_name(self)
transformers.models.electra.modeling_tf_electra.TFElectraForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.electra.modeling_tf_electra.TFElectraForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForMultipleChoice.dummy_inputs(self)
transformers.models.electra.modeling_tf_electra.TFElectraForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.models.electra.modeling_tf_electra.TFElectraForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.electra.modeling_tf_electra.TFElectraForPreTraining(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForPreTraining.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForPreTraining.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForPreTraining.serving_output(self,output)
transformers.models.electra.modeling_tf_electra.TFElectraForPreTrainingOutput(ModelOutput)
transformers.models.electra.modeling_tf_electra.TFElectraForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.electra.modeling_tf_electra.TFElectraForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.electra.modeling_tf_electra.TFElectraForTokenClassification(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForTokenClassification.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.electra.modeling_tf_electra.TFElectraGeneratorPredictions(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraGeneratorPredictions.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraGeneratorPredictions.call(self,generator_hidden_states,training=False)
transformers.models.electra.modeling_tf_electra.TFElectraIntermediate(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraIntermediate.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.electra.modeling_tf_electra.TFElectraLayer(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraLayer.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer.__init__(self,config,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer._prune_heads(self,heads_to_prune)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer.get_extended_attention_mask(self,attention_mask,input_shape,dtype)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer.get_head_mask(self,head_mask)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer.get_input_embeddings(self)
transformers.models.electra.modeling_tf_electra.TFElectraMainLayer.set_input_embeddings(self,value)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead(self,config,input_embeddings,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.build(self,input_shape)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.call(self,hidden_states)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.get_bias(self)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.get_output_embeddings(self)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.set_bias(self,value)
transformers.models.electra.modeling_tf_electra.TFElectraMaskedLMHead.set_output_embeddings(self,value)
transformers.models.electra.modeling_tf_electra.TFElectraModel(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraModel.__init__(self,config,*inputs,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraModel.serving_output(self,output)
transformers.models.electra.modeling_tf_electra.TFElectraOutput(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraOutput.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.electra.modeling_tf_electra.TFElectraPooler(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraPooler.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraPooler.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.electra.modeling_tf_electra.TFElectraPreTrainedModel(TFPreTrainedModel)
transformers.models.electra.modeling_tf_electra.TFElectraSelfAttention(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraSelfAttention.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraSelfAttention.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.electra.modeling_tf_electra.TFElectraSelfAttention.transpose_for_scores(self,tensor:tf.Tensor,batch_size:int)->tf.Tensor
transformers.models.electra.modeling_tf_electra.TFElectraSelfOutput(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraSelfOutput.__init__(self,config:ElectraConfig,**kwargs)
transformers.models.electra.modeling_tf_electra.TFElectraSelfOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/modeling_electra.py----------------------------------------
A:transformers.models.electra.modeling_electra.logger->utils.logging.get_logger(__name__)
A:transformers.models.electra.modeling_electra.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.electra.modeling_electra.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.electra.modeling_electra.array->numpy.transpose(array)
A:transformers.models.electra.modeling_electra.name->name.split('/').split('/')
A:transformers.models.electra.modeling_electra.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.electra.modeling_electra.pointer->getattr(pointer, 'weight')
A:transformers.models.electra.modeling_electra.num->int(scope_names[1])
A:transformers.models.electra.modeling_electra.pointer.data->torch.from_numpy(array)
A:transformers.models.electra.modeling_electra.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.models.electra.modeling_electra.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.embedding_size)
A:transformers.models.electra.modeling_electra.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.models.electra.modeling_electra.self.LayerNorm->torch.nn.LayerNorm(config.embedding_size)
A:transformers.models.electra.modeling_electra.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.electra.modeling_electra.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.electra.modeling_electra.input_shape->input_ids.size()
A:transformers.models.electra.modeling_electra.buffered_token_type_ids_expanded->buffered_token_type_ids.expand(batch_size, seq_length)
A:transformers.models.electra.modeling_electra.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.electra.modeling_electra.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.electra.modeling_electra.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.electra.modeling_electra.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.electra.modeling_electra.embeddings->self.dropout(embeddings)
A:transformers.models.electra.modeling_electra.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.electra.modeling_electra.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.electra.modeling_electra.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.electra.modeling_electra.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.electra.modeling_electra.self.distance_embedding->torch.nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
A:transformers.models.electra.modeling_electra.x->self.out_proj(x)
A:transformers.models.electra.modeling_electra.mixed_query_layer->self.query(hidden_states)
A:transformers.models.electra.modeling_electra.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.electra.modeling_electra.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.electra.modeling_electra.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.electra.modeling_electra.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.electra.modeling_electra.position_ids_l->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
A:transformers.models.electra.modeling_electra.position_ids_r->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
A:transformers.models.electra.modeling_electra.positional_embedding->positional_embedding.to(dtype=query_layer.dtype).to(dtype=query_layer.dtype)
A:transformers.models.electra.modeling_electra.relative_position_scores->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.electra.modeling_electra.relative_position_scores_query->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.electra.modeling_electra.relative_position_scores_key->torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
A:transformers.models.electra.modeling_electra.attention_probs->self.dropout(attention_probs)
A:transformers.models.electra.modeling_electra.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.electra.modeling_electra.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.electra.modeling_electra.hidden_states->self.encoder(hidden_states, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.electra.modeling_electra.self.self->ElectraSelfAttention(config)
A:transformers.models.electra.modeling_electra.self.output->ElectraOutput(config)
A:transformers.models.electra.modeling_electra.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.electra.modeling_electra.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.electra.modeling_electra.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.electra.modeling_electra.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.electra.modeling_electra.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.electra.modeling_electra.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.electra.modeling_electra.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.electra.modeling_electra.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.electra.modeling_electra.self.attention->ElectraAttention(config)
A:transformers.models.electra.modeling_electra.self.crossattention->ElectraAttention(config)
A:transformers.models.electra.modeling_electra.self.intermediate->ElectraIntermediate(config)
A:transformers.models.electra.modeling_electra.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.electra.modeling_electra.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.electra.modeling_electra.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.electra.modeling_electra.intermediate_output->self.intermediate(attention_output)
A:transformers.models.electra.modeling_electra.self.layer->torch.nn.ModuleList([ElectraLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.electra.modeling_electra.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.electra.modeling_electra.self.dense_prediction->torch.nn.Linear(config.hidden_size, 1)
A:transformers.models.electra.modeling_electra.logits->self.classifier(pooled_output)
A:transformers.models.electra.modeling_electra.self.embeddings->ElectraEmbeddings(config)
A:transformers.models.electra.modeling_electra.self.embeddings_project->torch.nn.Linear(config.embedding_size, config.hidden_size)
A:transformers.models.electra.modeling_electra.self.encoder->ElectraEncoder(config)
A:transformers.models.electra.modeling_electra.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.electra.modeling_electra.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, device)
A:transformers.models.electra.modeling_electra.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.electra.modeling_electra.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.electra.modeling_electra.self.electra->ElectraModel(config)
A:transformers.models.electra.modeling_electra.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.models.electra.modeling_electra.discriminator_hidden_states->self.electra(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.electra.modeling_electra.loss_fct->CrossEntropyLoss()
A:transformers.models.electra.modeling_electra.loss->loss_fct(reshaped_logits, labels)
A:transformers.models.electra.modeling_electra.self.discriminator_predictions->ElectraDiscriminatorPredictions(config)
A:transformers.models.electra.modeling_electra.self.generator_predictions->ElectraGeneratorPredictions(config)
A:transformers.models.electra.modeling_electra.self.generator_lm_head->torch.nn.Linear(config.embedding_size, config.vocab_size)
A:transformers.models.electra.modeling_electra.generator_hidden_states->self.electra(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
A:transformers.models.electra.modeling_electra.prediction_scores->self.generator_lm_head(prediction_scores)
A:transformers.models.electra.modeling_electra.discriminator_sequence_output->self.dropout(discriminator_sequence_output)
A:transformers.models.electra.modeling_electra.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.electra.modeling_electra.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.models.electra.modeling_electra.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.electra.modeling_electra.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.electra.modeling_electra.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.electra.modeling_electra.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.electra.modeling_electra.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.electra.modeling_electra.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.electra.modeling_electra.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.electra.modeling_electra.self.sequence_summary->SequenceSummary(config)
A:transformers.models.electra.modeling_electra.pooled_output->self.sequence_summary(sequence_output)
A:transformers.models.electra.modeling_electra.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.ElectraForMaskedLM(self,config)
transformers.ElectraForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForMaskedLM.get_output_embeddings(self)
transformers.ElectraForMaskedLM.set_output_embeddings(self,word_embeddings)
transformers.ElectraForMultipleChoice(self,config)
transformers.ElectraForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForPreTraining(self,config)
transformers.ElectraForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForPreTrainingOutput(ModelOutput)
transformers.ElectraForQuestionAnswering(self,config)
transformers.ElectraForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForSequenceClassification(self,config)
transformers.ElectraForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForTokenClassification(self,config)
transformers.ElectraForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraModel(self,config)
transformers.ElectraModel._prune_heads(self,heads_to_prune)
transformers.ElectraModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraModel.get_input_embeddings(self)
transformers.ElectraModel.set_input_embeddings(self,value)
transformers.ElectraPreTrainedModel(PreTrainedModel)
transformers.ElectraPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_electra(model,config,tf_checkpoint_path,discriminator_or_generator='discriminator')
transformers.models.electra.modeling_electra.ElectraAttention(self,config)
transformers.models.electra.modeling_electra.ElectraAttention.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.electra.modeling_electra.ElectraAttention.prune_heads(self,heads)
transformers.models.electra.modeling_electra.ElectraClassificationHead(self,config)
transformers.models.electra.modeling_electra.ElectraClassificationHead.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraClassificationHead.forward(self,features,**kwargs)
transformers.models.electra.modeling_electra.ElectraDiscriminatorPredictions(self,config)
transformers.models.electra.modeling_electra.ElectraDiscriminatorPredictions.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraDiscriminatorPredictions.forward(self,discriminator_hidden_states)
transformers.models.electra.modeling_electra.ElectraEmbeddings(self,config)
transformers.models.electra.modeling_electra.ElectraEmbeddings.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.electra.modeling_electra.ElectraEncoder(self,config)
transformers.models.electra.modeling_electra.ElectraEncoder.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.electra.modeling_electra.ElectraForMaskedLM(self,config)
transformers.models.electra.modeling_electra.ElectraForMaskedLM.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraForMaskedLM.get_output_embeddings(self)
transformers.models.electra.modeling_electra.ElectraForMaskedLM.set_output_embeddings(self,word_embeddings)
transformers.models.electra.modeling_electra.ElectraForMultipleChoice(self,config)
transformers.models.electra.modeling_electra.ElectraForMultipleChoice.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraForPreTraining(self,config)
transformers.models.electra.modeling_electra.ElectraForPreTraining.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraForPreTrainingOutput(ModelOutput)
transformers.models.electra.modeling_electra.ElectraForQuestionAnswering(self,config)
transformers.models.electra.modeling_electra.ElectraForQuestionAnswering.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraForSequenceClassification(self,config)
transformers.models.electra.modeling_electra.ElectraForSequenceClassification.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraForTokenClassification(self,config)
transformers.models.electra.modeling_electra.ElectraForTokenClassification.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraGeneratorPredictions(self,config)
transformers.models.electra.modeling_electra.ElectraGeneratorPredictions.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraGeneratorPredictions.forward(self,generator_hidden_states)
transformers.models.electra.modeling_electra.ElectraIntermediate(self,config)
transformers.models.electra.modeling_electra.ElectraIntermediate.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraIntermediate.forward(self,hidden_states)
transformers.models.electra.modeling_electra.ElectraLayer(self,config)
transformers.models.electra.modeling_electra.ElectraLayer.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraLayer.feed_forward_chunk(self,attention_output)
transformers.models.electra.modeling_electra.ElectraLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.electra.modeling_electra.ElectraModel(self,config)
transformers.models.electra.modeling_electra.ElectraModel.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraModel._prune_heads(self,heads_to_prune)
transformers.models.electra.modeling_electra.ElectraModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.electra.modeling_electra.ElectraModel.get_input_embeddings(self)
transformers.models.electra.modeling_electra.ElectraModel.set_input_embeddings(self,value)
transformers.models.electra.modeling_electra.ElectraOutput(self,config)
transformers.models.electra.modeling_electra.ElectraOutput.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraOutput.forward(self,hidden_states,input_tensor)
transformers.models.electra.modeling_electra.ElectraPreTrainedModel(PreTrainedModel)
transformers.models.electra.modeling_electra.ElectraPreTrainedModel._init_weights(self,module)
transformers.models.electra.modeling_electra.ElectraSelfAttention(self,config)
transformers.models.electra.modeling_electra.ElectraSelfAttention.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.electra.modeling_electra.ElectraSelfAttention.transpose_for_scores(self,x)
transformers.models.electra.modeling_electra.ElectraSelfOutput(self,config)
transformers.models.electra.modeling_electra.ElectraSelfOutput.__init__(self,config)
transformers.models.electra.modeling_electra.ElectraSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.electra.modeling_electra.load_tf_weights_in_electra(model,config,tf_checkpoint_path,discriminator_or_generator='discriminator')


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/tokenization_electra.py----------------------------------------
transformers.ElectraTokenizer(BertTokenizer)
transformers.models.electra.tokenization_electra.ElectraTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/__init__.py----------------------------------------
A:transformers.models.electra.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/electra/tokenization_electra_fast.py----------------------------------------
transformers.ElectraTokenizerFast(BertTokenizerFast)
transformers.models.electra.tokenization_electra_fast.ElectraTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/m2m_100/configuration_m2m_100.py----------------------------------------
A:transformers.models.m2m_100.configuration_m2m_100.logger->utils.logging.get_logger(__name__)
transformers.M2M100Config(self,vocab_size=128112,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.05,decoder_layerdrop=0.05,use_cache=True,is_encoder_decoder=True,activation_function='relu',d_model=1024,dropout=0.1,attention_dropout=0.1,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,scale_embedding=True,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.M2M100Config.hidden_size(self)->int
transformers.M2M100Config.num_attention_heads(self)->int
transformers.models.m2m_100.configuration_m2m_100.M2M100Config(self,vocab_size=128112,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.05,decoder_layerdrop=0.05,use_cache=True,is_encoder_decoder=True,activation_function='relu',d_model=1024,dropout=0.1,attention_dropout=0.1,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,scale_embedding=True,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.m2m_100.configuration_m2m_100.M2M100Config.__init__(self,vocab_size=128112,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.05,decoder_layerdrop=0.05,use_cache=True,is_encoder_decoder=True,activation_function='relu',d_model=1024,dropout=0.1,attention_dropout=0.1,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,scale_embedding=True,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.m2m_100.configuration_m2m_100.M2M100Config.hidden_size(self)->int
transformers.models.m2m_100.configuration_m2m_100.M2M100Config.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/m2m_100/convert_m2m100_original_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.lin_layer->torch.nn.Linear(vocab_size, emb_size, bias=False)
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.m2m_100->torch.load(checkpoint_path, map_location='cpu')
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.config->M2M100Config(vocab_size=vocab_size, max_position_embeddings=1024, encoder_layers=args.encoder_layers, decoder_layers=args.decoder_layers, encoder_attention_heads=args.encoder_attention_heads, decoder_attention_heads=args.decoder_attention_heads, encoder_ffn_dim=args.encoder_ffn_embed_dim, decoder_ffn_dim=args.decoder_ffn_embed_dim, d_model=args.encoder_embed_dim, encoder_layerdrop=args.encoder_layerdrop, decoder_layerdrop=args.decoder_layerdrop, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, activation_function='relu')
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.model->convert_fairseq_m2m100_checkpoint_from_disk(args.fairseq_path)
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.model.lm_head->make_linear_from_emb(model.model.shared)
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.convert_fairseq_m2m100_checkpoint_from_disk(checkpoint_path)
transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.make_linear_from_emb(emb)
transformers.models.m2m_100.convert_m2m100_original_checkpoint_to_pytorch.remove_ignore_keys_(state_dict)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/m2m_100/modeling_m2m_100.py----------------------------------------
A:transformers.models.m2m_100.modeling_m2m_100.logger->utils.logging.get_logger(__name__)
A:transformers.models.m2m_100.modeling_m2m_100.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.m2m_100.modeling_m2m_100.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.m2m_100.modeling_m2m_100.mask->input_ids.view(-1, input_shape[-1]).ne(padding_idx).int()
A:transformers.models.m2m_100.modeling_m2m_100.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.m2m_100.modeling_m2m_100.(bsz, src_len)->input_ids.view(-1, input_shape[-1]).ne(padding_idx).int().size()
A:transformers.models.m2m_100.modeling_m2m_100.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.m2m_100.modeling_m2m_100.emb_weights->emb_weights.to(self.weights.device).to(self.weights.device)
A:transformers.models.m2m_100.modeling_m2m_100.self.weights->torch.nn.Parameter(emb_weights)
A:transformers.models.m2m_100.modeling_m2m_100.emb->torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
A:transformers.models.m2m_100.modeling_m2m_100.(bsz, seq_len)->input_ids.view(-1, input_shape[-1]).size()
A:transformers.models.m2m_100.modeling_m2m_100.position_ids->torch.arange(self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device)
A:transformers.models.m2m_100.modeling_m2m_100.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.m2m_100.modeling_m2m_100.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.m2m_100.modeling_m2m_100.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.m2m_100.modeling_m2m_100.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.m2m_100.modeling_m2m_100.(bsz, tgt_len, embed_dim)->self.layer_norm(hidden_states).size()
A:transformers.models.m2m_100.modeling_m2m_100.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.m2m_100.modeling_m2m_100.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.m2m_100.modeling_m2m_100.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.m2m_100.modeling_m2m_100.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.m2m_100.modeling_m2m_100.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.m2m_100.modeling_m2m_100.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.m2m_100.modeling_m2m_100.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.m2m_100.modeling_m2m_100.attn_output->self.out_proj(attn_output)
A:transformers.models.m2m_100.modeling_m2m_100.self.self_attn->M2M100Attention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.m2m_100.modeling_m2m_100.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.m2m_100.modeling_m2m_100.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.m2m_100.modeling_m2m_100.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.m2m_100.modeling_m2m_100.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.m2m_100.modeling_m2m_100.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.m2m_100.modeling_m2m_100.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.m2m_100.modeling_m2m_100.self.encoder_attn->M2M100Attention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.m2m_100.modeling_m2m_100.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.m2m_100.modeling_m2m_100.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.m2m_100.modeling_m2m_100.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.m2m_100.modeling_m2m_100.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.m2m_100.modeling_m2m_100.self.embed_positions->M2M100SinusoidalPositionalEmbedding(config.max_position_embeddings, config.d_model, self.padding_idx)
A:transformers.models.m2m_100.modeling_m2m_100.self.layers->torch.nn.ModuleList([M2M100DecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.m2m_100.modeling_m2m_100.self.layer_norm->torch.nn.LayerNorm(config.d_model)
A:transformers.models.m2m_100.modeling_m2m_100.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.models.m2m_100.modeling_m2m_100.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.m2m_100.modeling_m2m_100.embed_pos->self.embed_positions(input_ids, inputs_embeds)
A:transformers.models.m2m_100.modeling_m2m_100.attention_mask->_expand_mask(attention_mask, inputs_embeds.dtype)
A:transformers.models.m2m_100.modeling_m2m_100.dropout_probability->random.uniform(0, 1)
A:transformers.models.m2m_100.modeling_m2m_100.layer_outputs->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.m2m_100.modeling_m2m_100.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.m2m_100.modeling_m2m_100.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.m2m_100.modeling_m2m_100.positions->self.embed_positions(input_ids, inputs_embeds, past_key_values_length)
A:transformers.models.m2m_100.modeling_m2m_100.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.m2m_100.modeling_m2m_100.self.encoder->M2M100Encoder(config, self.shared)
A:transformers.models.m2m_100.modeling_m2m_100.self.decoder->M2M100Decoder(config, self.shared)
A:transformers.models.m2m_100.modeling_m2m_100.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.m2m_100.modeling_m2m_100.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.m2m_100.modeling_m2m_100.self.model->M2M100Model(config)
A:transformers.models.m2m_100.modeling_m2m_100.self.lm_head->torch.nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)
A:transformers.models.m2m_100.modeling_m2m_100.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.m2m_100.modeling_m2m_100.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.m2m_100.modeling_m2m_100.outputs->self.model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, encoder_outputs=encoder_outputs, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.m2m_100.modeling_m2m_100.lm_logits->self.lm_head(outputs[0])
A:transformers.models.m2m_100.modeling_m2m_100.loss_fct->CrossEntropyLoss()
A:transformers.models.m2m_100.modeling_m2m_100.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
transformers.M2M100ForConditionalGeneration(self,config:M2M100Config)
transformers.M2M100ForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.M2M100ForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.M2M100ForConditionalGeneration.get_decoder(self)
transformers.M2M100ForConditionalGeneration.get_encoder(self)
transformers.M2M100ForConditionalGeneration.get_output_embeddings(self)
transformers.M2M100ForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.M2M100ForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.M2M100ForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.M2M100Model(self,config:M2M100Config)
transformers.M2M100Model.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.M2M100Model.get_decoder(self)
transformers.M2M100Model.get_encoder(self)
transformers.M2M100Model.get_input_embeddings(self)
transformers.M2M100Model.set_input_embeddings(self,value)
transformers.M2M100PreTrainedModel(PreTrainedModel)
transformers.M2M100PreTrainedModel._init_weights(self,module)
transformers.models.m2m_100.modeling_m2m_100.M2M100Attention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.m2m_100.modeling_m2m_100.M2M100Attention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.m2m_100.modeling_m2m_100.M2M100Attention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.m2m_100.modeling_m2m_100.M2M100Attention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.m2m_100.modeling_m2m_100.M2M100Decoder(self,config:M2M100Config,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100Decoder.__init__(self,config:M2M100Config,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100Decoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100DecoderLayer(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100DecoderLayer.__init__(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100DecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.m2m_100.modeling_m2m_100.M2M100Encoder(self,config:M2M100Config,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100Encoder.__init__(self,config:M2M100Config,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100Encoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100EncoderLayer(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100EncoderLayer.__init__(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100EncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.__init__(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.get_decoder(self)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.get_encoder(self)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.get_output_embeddings(self)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.m2m_100.modeling_m2m_100.M2M100ForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model.__init__(self,config:M2M100Config)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model.get_decoder(self)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model.get_encoder(self)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model.get_input_embeddings(self)
transformers.models.m2m_100.modeling_m2m_100.M2M100Model.set_input_embeddings(self,value)
transformers.models.m2m_100.modeling_m2m_100.M2M100PreTrainedModel(PreTrainedModel)
transformers.models.m2m_100.modeling_m2m_100.M2M100PreTrainedModel._init_weights(self,module)
transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.forward(self,input_ids:torch.Tensor=None,inputs_embeds:torch.Tensor=None,past_key_values_length:int=0)
transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.get_embedding(num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.make_weights(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.m2m_100.modeling_m2m_100._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.m2m_100.modeling_m2m_100._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.m2m_100.modeling_m2m_100.create_position_ids_from_input_ids(input_ids,padding_idx,past_key_values_length=0)
transformers.models.m2m_100.modeling_m2m_100.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/m2m_100/tokenization_m2m_100.py----------------------------------------
A:transformers.models.m2m_100.tokenization_m2m_100.logger->utils.logging.get_logger(__name__)
A:transformers.models.m2m_100.tokenization_m2m_100.self.encoder->load_json(vocab_file)
A:transformers.models.m2m_100.tokenization_m2m_100.self.sp_model->load_spm(self.spm_file, self.sp_model_kwargs)
A:transformers.models.m2m_100.tokenization_m2m_100.self.encoder_size->len(self.encoder)
A:transformers.models.m2m_100.tokenization_m2m_100.self._additional_special_tokens->list(self.lang_token_to_id.keys())
A:transformers.models.m2m_100.tokenization_m2m_100.self.cur_lang_id->self.get_lang_id(self._src_lang)
A:transformers.models.m2m_100.tokenization_m2m_100.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.m2m_100.tokenization_m2m_100.vocab->self.encoder.copy()
A:transformers.models.m2m_100.tokenization_m2m_100.state->self.__dict__.copy()
A:transformers.models.m2m_100.tokenization_m2m_100.save_dir->Path(save_directory)
A:transformers.models.m2m_100.tokenization_m2m_100.inputs->self(raw_inputs, add_special_tokens=True, return_tensors='pt', **extra_kwargs)
A:transformers.models.m2m_100.tokenization_m2m_100.tgt_lang_id->self.get_lang_id(tgt_lang)
A:transformers.models.m2m_100.tokenization_m2m_100.lang_token->self.get_lang_token(lang)
A:transformers.models.m2m_100.tokenization_m2m_100.spm->sentencepiece.SentencePieceProcessor(**sp_model_kwargs)
transformers.M2M100Tokenizer(self,vocab_file,spm_file,src_lang=None,tgt_lang=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',pad_token='<pad>',unk_token='<unk>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.M2M100Tokenizer.__getstate__(self)->Dict
transformers.M2M100Tokenizer.__setstate__(self,d:Dict)->None
transformers.M2M100Tokenizer._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.M2M100Tokenizer._convert_id_to_token(self,index:int)->str
transformers.M2M100Tokenizer._convert_token_to_id(self,token)
transformers.M2M100Tokenizer._tokenize(self,text:str)->List[str]
transformers.M2M100Tokenizer.as_target_tokenizer(self)
transformers.M2M100Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.M2M100Tokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.M2M100Tokenizer.get_lang_id(self,lang:str)->int
transformers.M2M100Tokenizer.get_lang_token(self,lang:str)->str
transformers.M2M100Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.M2M100Tokenizer.get_vocab(self)->Dict
transformers.M2M100Tokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro',**kwargs)->BatchEncoding
transformers.M2M100Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.M2M100Tokenizer.set_src_lang_special_tokens(self,src_lang:str)->None
transformers.M2M100Tokenizer.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.M2M100Tokenizer.src_lang(self)->str
transformers.M2M100Tokenizer.src_lang(self,new_src_lang:str)->None
transformers.M2M100Tokenizer.vocab_size(self)->int
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer(self,vocab_file,spm_file,src_lang=None,tgt_lang=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',pad_token='<pad>',unk_token='<unk>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.__getstate__(self)->Dict
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.__init__(self,vocab_file,spm_file,src_lang=None,tgt_lang=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',pad_token='<pad>',unk_token='<unk>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.__setstate__(self,d:Dict)->None
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer._build_translation_inputs(self,raw_inputs,src_lang:Optional[str],tgt_lang:Optional[str],**extra_kwargs)
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer._convert_id_to_token(self,index:int)->str
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer._convert_token_to_id(self,token)
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer._tokenize(self,text:str)->List[str]
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.as_target_tokenizer(self)
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.get_lang_id(self,lang:str)->int
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.get_lang_token(self,lang:str)->str
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.get_vocab(self)->Dict
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro',**kwargs)->BatchEncoding
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.set_src_lang_special_tokens(self,src_lang:str)->None
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.src_lang(self)->str
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.src_lang(self,new_src_lang:str)->None
transformers.models.m2m_100.tokenization_m2m_100.M2M100Tokenizer.vocab_size(self)->int
transformers.models.m2m_100.tokenization_m2m_100.load_json(path:str)->Union[Dict, List]
transformers.models.m2m_100.tokenization_m2m_100.load_spm(path:str,sp_model_kwargs:Dict[str,Any])->sentencepiece.SentencePieceProcessor
transformers.models.m2m_100.tokenization_m2m_100.save_json(data,path:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/m2m_100/__init__.py----------------------------------------
A:transformers.models.m2m_100.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/herbert/tokenization_herbert_fast.py----------------------------------------
A:transformers.models.herbert.tokenization_herbert_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.herbert.tokenization_herbert_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
transformers.HerbertTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sep_token='</s>',**kwargs)
transformers.HerbertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.HerbertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.HerbertTokenizerFast.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.HerbertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.herbert.tokenization_herbert_fast.HerbertTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sep_token='</s>',**kwargs)
transformers.models.herbert.tokenization_herbert_fast.HerbertTokenizerFast.__init__(self,vocab_file,merges_file,tokenizer_file=None,cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sep_token='</s>',**kwargs)
transformers.models.herbert.tokenization_herbert_fast.HerbertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.herbert.tokenization_herbert_fast.HerbertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.herbert.tokenization_herbert_fast.HerbertTokenizerFast.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.herbert.tokenization_herbert_fast.HerbertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/herbert/tokenization_herbert.py----------------------------------------
A:transformers.models.herbert.tokenization_herbert.logger->utils.logging.get_logger(__name__)
A:transformers.models.herbert.tokenization_herbert.self.bert_pre_tokenizer->BasicTokenizer(do_lower_case=False, never_split=self.all_special_tokens, tokenize_chinese_chars=False, strip_accents=False)
A:transformers.models.herbert.tokenization_herbert.pre_tokens->self.bert_pre_tokenizer.tokenize(text)
transformers.HerbertTokenizer(self,vocab_file,merges_file,tokenizer_file=None,cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sep_token='</s>',do_lowercase_and_remove_accent=False,**kwargs)
transformers.HerbertTokenizer._tokenize(self,text)
transformers.models.herbert.tokenization_herbert.HerbertTokenizer(self,vocab_file,merges_file,tokenizer_file=None,cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sep_token='</s>',do_lowercase_and_remove_accent=False,**kwargs)
transformers.models.herbert.tokenization_herbert.HerbertTokenizer.__init__(self,vocab_file,merges_file,tokenizer_file=None,cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sep_token='</s>',do_lowercase_and_remove_accent=False,**kwargs)
transformers.models.herbert.tokenization_herbert.HerbertTokenizer._tokenize(self,text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/herbert/__init__.py----------------------------------------
A:transformers.models.herbert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/configuration_convbert.py----------------------------------------
A:transformers.models.convbert.configuration_convbert.logger->utils.logging.get_logger(__name__)
transformers.ConvBertConfig(self,vocab_size=30522,hidden_size=768,is_encoder_decoder=False,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=1,bos_token_id=0,eos_token_id=2,embedding_size=768,head_ratio=2,conv_kernel_size=9,num_groups=1,**kwargs)
transformers.models.convbert.configuration_convbert.ConvBertConfig(self,vocab_size=30522,hidden_size=768,is_encoder_decoder=False,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=1,bos_token_id=0,eos_token_id=2,embedding_size=768,head_ratio=2,conv_kernel_size=9,num_groups=1,**kwargs)
transformers.models.convbert.configuration_convbert.ConvBertConfig.__init__(self,vocab_size=30522,hidden_size=768,is_encoder_decoder=False,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=1,bos_token_id=0,eos_token_id=2,embedding_size=768,head_ratio=2,conv_kernel_size=9,num_groups=1,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/modeling_tf_convbert.py----------------------------------------
A:transformers.models.convbert.modeling_tf_convbert.logger->utils.logging.get_logger(__name__)
A:transformers.models.convbert.modeling_tf_convbert.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.convbert.modeling_tf_convbert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.convbert.modeling_tf_convbert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.convbert.modeling_tf_convbert.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.convbert.modeling_tf_convbert.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.convbert.modeling_tf_convbert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.convbert.modeling_tf_convbert.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.convbert.modeling_tf_convbert.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.convbert.modeling_tf_convbert.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.convbert.modeling_tf_convbert.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.convbert.modeling_tf_convbert.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.convbert.modeling_tf_convbert.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.convbert.modeling_tf_convbert.new_num_attention_heads->int(config.num_attention_heads / config.head_ratio)
A:transformers.models.convbert.modeling_tf_convbert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.convbert.modeling_tf_convbert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.convbert.modeling_tf_convbert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.convbert.modeling_tf_convbert.self.key_conv_attn_layer->tensorflow.keras.layers.SeparableConv1D(self.all_head_size, self.conv_kernel_size, padding='same', activation=None, depthwise_initializer=get_initializer(1 / self.conv_kernel_size), pointwise_initializer=get_initializer(config.initializer_range), name='key_conv_attn_layer')
A:transformers.models.convbert.modeling_tf_convbert.self.conv_kernel_layer->tensorflow.keras.layers.Dense(self.num_attention_heads * self.conv_kernel_size, activation=None, name='conv_kernel_layer', kernel_initializer=get_initializer(config.initializer_range))
A:transformers.models.convbert.modeling_tf_convbert.self.conv_out_layer->tensorflow.keras.layers.Dense(self.all_head_size, activation=None, name='conv_out_layer', kernel_initializer=get_initializer(config.initializer_range))
A:transformers.models.convbert.modeling_tf_convbert.x->self.out_proj(x)
A:transformers.models.convbert.modeling_tf_convbert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.convbert.modeling_tf_convbert.mixed_key_layer->self.key(hidden_states)
A:transformers.models.convbert.modeling_tf_convbert.mixed_value_layer->self.value(hidden_states)
A:transformers.models.convbert.modeling_tf_convbert.mixed_key_conv_attn_layer->self.key_conv_attn_layer(hidden_states)
A:transformers.models.convbert.modeling_tf_convbert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.convbert.modeling_tf_convbert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.convbert.modeling_tf_convbert.conv_attn_layer->tensorflow.multiply(mixed_key_conv_attn_layer, mixed_query_layer)
A:transformers.models.convbert.modeling_tf_convbert.conv_kernel_layer->tensorflow.nn.softmax(conv_kernel_layer, axis=1)
A:transformers.models.convbert.modeling_tf_convbert.paddings->tensorflow.constant([[0, 0], [int((self.conv_kernel_size - 1) / 2), int((self.conv_kernel_size - 1) / 2)], [0, 0]])
A:transformers.models.convbert.modeling_tf_convbert.conv_out_layer->tensorflow.reshape(conv_out_layer, [-1, self.all_head_size])
A:transformers.models.convbert.modeling_tf_convbert.unfold_conv_out_layer->tensorflow.stack([tf.slice(conv_out_layer, [0, i, 0], [batch_size, shape_list(mixed_query_layer)[1], self.all_head_size]) for i in range(self.conv_kernel_size)], axis=-1)
A:transformers.models.convbert.modeling_tf_convbert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.models.convbert.modeling_tf_convbert.dk->tensorflow.cast(shape_list(key_layer)[-1], attention_scores.dtype)
A:transformers.models.convbert.modeling_tf_convbert.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.models.convbert.modeling_tf_convbert.value_layer->tensorflow.transpose(value_layer, [0, 2, 1, 3])
A:transformers.models.convbert.modeling_tf_convbert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.head_ratio * self.all_head_size))
A:transformers.models.convbert.modeling_tf_convbert.conv_out->tensorflow.reshape(conv_out_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size])
A:transformers.models.convbert.modeling_tf_convbert.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.models.convbert.modeling_tf_convbert.hidden_states->self.LayerNorm(hidden_states)
A:transformers.models.convbert.modeling_tf_convbert.self.self_attention->TFConvBertSelfAttention(config, name='self')
A:transformers.models.convbert.modeling_tf_convbert.self.dense_output->TFConvBertSelfOutput(config, name='output')
A:transformers.models.convbert.modeling_tf_convbert.self_outputs->self.self_attention(input_tensor, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.convbert.modeling_tf_convbert.attention_output->self.dense_output(self_outputs[0], input_tensor, training=training)
A:transformers.models.convbert.modeling_tf_convbert.self.kernel->self.add_weight('kernel', shape=[self.group_out_dim, self.group_in_dim, self.num_groups], initializer=self.kernel_initializer, trainable=True)
A:transformers.models.convbert.modeling_tf_convbert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.convbert.modeling_tf_convbert.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.convbert.modeling_tf_convbert.self.attention->TFConvBertAttention(config, name='attention')
A:transformers.models.convbert.modeling_tf_convbert.self.intermediate->TFConvBertIntermediate(config, name='intermediate')
A:transformers.models.convbert.modeling_tf_convbert.self.bert_output->TFConvBertOutput(config, name='output')
A:transformers.models.convbert.modeling_tf_convbert.attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.convbert.modeling_tf_convbert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.convbert.modeling_tf_convbert.layer_output->self.bert_output(intermediate_output, attention_output, training=training)
A:transformers.models.convbert.modeling_tf_convbert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], output_attentions, training=training)
A:transformers.models.convbert.modeling_tf_convbert.self.transform_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.convbert.modeling_tf_convbert.self.embeddings->TFConvBertEmbeddings(config, name='embeddings')
A:transformers.models.convbert.modeling_tf_convbert.self.embeddings_project->tensorflow.keras.layers.Dense(config.hidden_size, name='embeddings_project')
A:transformers.models.convbert.modeling_tf_convbert.self.encoder->TFConvBertEncoder(config, name='encoder')
A:transformers.models.convbert.modeling_tf_convbert.attention_mask->tensorflow.fill(input_shape, 1)
A:transformers.models.convbert.modeling_tf_convbert.extended_attention_mask->self.get_extended_attention_mask(inputs['attention_mask'], input_shape, hidden_states.dtype)
A:transformers.models.convbert.modeling_tf_convbert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.convbert.modeling_tf_convbert.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.convbert.modeling_tf_convbert.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.convbert.modeling_tf_convbert.inputs['token_type_ids']->tensorflow.fill(input_shape, 0)
A:transformers.models.convbert.modeling_tf_convbert.inputs['head_mask']->self.get_head_mask(inputs['head_mask'])
A:transformers.models.convbert.modeling_tf_convbert.self.convbert->TFConvBertMainLayer(config, name='convbert')
A:transformers.models.convbert.modeling_tf_convbert.outputs->self.convbert(inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.convbert.modeling_tf_convbert.self.generator_predictions->TFConvBertGeneratorPredictions(config, name='generator_predictions')
A:transformers.models.convbert.modeling_tf_convbert.self.activation->get_tf_activation(config.hidden_act)
A:transformers.models.convbert.modeling_tf_convbert.self.generator_lm_head->TFConvBertMaskedLMHead(config, self.convbert.embeddings, name='generator_lm_head')
A:transformers.models.convbert.modeling_tf_convbert.generator_hidden_states->self.convbert(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.convbert.modeling_tf_convbert.prediction_scores->self.generator_lm_head(prediction_scores, training=inputs['training'])
A:transformers.models.convbert.modeling_tf_convbert.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.models.convbert.modeling_tf_convbert.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.convbert.modeling_tf_convbert.logits->self.qa_outputs(sequence_output)
A:transformers.models.convbert.modeling_tf_convbert.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.initializer_range, name='sequence_summary')
A:transformers.models.convbert.modeling_tf_convbert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.convbert.modeling_tf_convbert.output->self.call(inputs)
A:transformers.models.convbert.modeling_tf_convbert.sequence_output->self.dropout(sequence_output, training=inputs['training'])
A:transformers.models.convbert.modeling_tf_convbert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.convbert.modeling_tf_convbert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.convbert.modeling_tf_convbert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.convbert.modeling_tf_convbert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.convbert.modeling_tf_convbert.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFConvBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFConvBertForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFConvBertForMaskedLM.get_lm_head(self)
transformers.TFConvBertForMaskedLM.get_prefix_bias_name(self)
transformers.TFConvBertForMaskedLM.serving_output(self,output)
transformers.TFConvBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFConvBertForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFConvBertForMultipleChoice.dummy_inputs(self)
transformers.TFConvBertForMultipleChoice.serving(self,inputs)
transformers.TFConvBertForMultipleChoice.serving_output(self,output)
transformers.TFConvBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFConvBertForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFConvBertForQuestionAnswering.serving_output(self,output)
transformers.TFConvBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFConvBertForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFConvBertForSequenceClassification.serving_output(self,output)
transformers.TFConvBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFConvBertForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFConvBertForTokenClassification.serving_output(self,output)
transformers.TFConvBertLayer(self,config,**kwargs)
transformers.TFConvBertLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.TFConvBertModel(self,config,*inputs,**kwargs)
transformers.TFConvBertModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFConvBertModel.serving_output(self,output)
transformers.TFConvBertPreTrainedModel(TFPreTrainedModel)
transformers.models.convbert.modeling_tf_convbert.GroupedLinearLayer(self,input_size,output_size,num_groups,kernel_initializer,**kwargs)
transformers.models.convbert.modeling_tf_convbert.GroupedLinearLayer.__init__(self,input_size,output_size,num_groups,kernel_initializer,**kwargs)
transformers.models.convbert.modeling_tf_convbert.GroupedLinearLayer.build(self,input_shape)
transformers.models.convbert.modeling_tf_convbert.GroupedLinearLayer.call(self,hidden_states)
transformers.models.convbert.modeling_tf_convbert.TFConvBertAttention(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertAttention.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertAttention.call(self,input_tensor,attention_mask,head_mask,output_attentions,training=False)
transformers.models.convbert.modeling_tf_convbert.TFConvBertAttention.prune_heads(self,heads)
transformers.models.convbert.modeling_tf_convbert.TFConvBertClassificationHead(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertClassificationHead.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertClassificationHead.call(self,hidden_states,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertEmbeddings(self,config:ConvBertConfig,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertEmbeddings.__init__(self,config:ConvBertConfig,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.convbert.modeling_tf_convbert.TFConvBertEmbeddings.call(self,input_ids:tf.Tensor=None,position_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.models.convbert.modeling_tf_convbert.TFConvBertEncoder(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertEncoder.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertEncoder.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMaskedLM.get_lm_head(self)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMaskedLM.get_prefix_bias_name(self)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMaskedLM.serving_output(self,output)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMultipleChoice.dummy_inputs(self)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMultipleChoice.serving(self,inputs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForMultipleChoice.serving_output(self,output)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForQuestionAnswering.serving_output(self,output)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForSequenceClassification.serving_output(self,output)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertForTokenClassification.serving_output(self,output)
transformers.models.convbert.modeling_tf_convbert.TFConvBertGeneratorPredictions(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertGeneratorPredictions.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertGeneratorPredictions.call(self,generator_hidden_states,training=False)
transformers.models.convbert.modeling_tf_convbert.TFConvBertIntermediate(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertIntermediate.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertIntermediate.call(self,hidden_states)
transformers.models.convbert.modeling_tf_convbert.TFConvBertLayer(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertLayer.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer._prune_heads(self,heads_to_prune)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer.get_extended_attention_mask(self,attention_mask,input_shape,dtype)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer.get_head_mask(self,head_mask)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer.get_input_embeddings(self)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMainLayer.set_input_embeddings(self,value)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead(self,config,input_embeddings,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.build(self,input_shape)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.call(self,hidden_states)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.get_bias(self)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.get_output_embeddings(self)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.set_bias(self,value)
transformers.models.convbert.modeling_tf_convbert.TFConvBertMaskedLMHead.set_output_embeddings(self,value)
transformers.models.convbert.modeling_tf_convbert.TFConvBertModel(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertModel.__init__(self,config,*inputs,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertModel.serving_output(self,output)
transformers.models.convbert.modeling_tf_convbert.TFConvBertOutput(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertOutput.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertOutput.call(self,hidden_states,input_tensor,training=False)
transformers.models.convbert.modeling_tf_convbert.TFConvBertPreTrainedModel(TFPreTrainedModel)
transformers.models.convbert.modeling_tf_convbert.TFConvBertPredictionHeadTransform(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertPredictionHeadTransform.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertPredictionHeadTransform.call(self,hidden_states)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfAttention(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfAttention.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfAttention.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfAttention.transpose_for_scores(self,x,batch_size)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfOutput(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfOutput.__init__(self,config,**kwargs)
transformers.models.convbert.modeling_tf_convbert.TFConvBertSelfOutput.call(self,hidden_states,input_tensor,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py----------------------------------------
A:transformers.models.convbert.convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.conf->transformers.ConvBertConfig.from_json_file(convbert_config_file)
A:transformers.models.convbert.convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.model->load_tf_weights_in_convbert(model, conf, tf_checkpoint_path)
A:transformers.models.convbert.convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.tf_model->transformers.TFConvBertModel.from_pretrained(pytorch_dump_path, from_pt=True)
A:transformers.models.convbert.convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.parser->argparse.ArgumentParser()
A:transformers.models.convbert.convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.args->argparse.ArgumentParser().parse_args()
transformers.models.convbert.convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.convert_orig_tf1_checkpoint_to_pytorch(tf_checkpoint_path,convbert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/tokenization_convbert.py----------------------------------------
A:transformers.models.convbert.tokenization_convbert.logger->utils.logging.get_logger(__name__)
transformers.ConvBertTokenizer(BertTokenizer)
transformers.models.convbert.tokenization_convbert.ConvBertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/__init__.py----------------------------------------
A:transformers.models.convbert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/tokenization_convbert_fast.py----------------------------------------
A:transformers.models.convbert.tokenization_convbert_fast.logger->utils.logging.get_logger(__name__)
transformers.ConvBertTokenizerFast(BertTokenizerFast)
transformers.models.convbert.tokenization_convbert_fast.ConvBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/convbert/modeling_convbert.py----------------------------------------
A:transformers.models.convbert.modeling_convbert.logger->utils.logging.get_logger(__name__)
A:transformers.models.convbert.modeling_convbert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.convbert.modeling_convbert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.convbert.modeling_convbert.array->tensorflow.train.load_variable(tf_path, name)
A:transformers.models.convbert.modeling_convbert.retriever->attrgetter(param_name)
A:transformers.models.convbert.modeling_convbert.result->retriever(model)
A:transformers.models.convbert.modeling_convbert.value->value.unsqueeze(-1).unsqueeze(-1)
A:transformers.models.convbert.modeling_convbert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.models.convbert.modeling_convbert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.embedding_size)
A:transformers.models.convbert.modeling_convbert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.models.convbert.modeling_convbert.self.LayerNorm->torch.nn.LayerNorm(config.embedding_size)
A:transformers.models.convbert.modeling_convbert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.convbert.modeling_convbert.input_shape->input_ids.size()
A:transformers.models.convbert.modeling_convbert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.convbert.modeling_convbert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.convbert.modeling_convbert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.convbert.modeling_convbert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.convbert.modeling_convbert.embeddings->self.dropout(embeddings)
A:transformers.models.convbert.modeling_convbert.self.depthwise->torch.nn.Conv1d(input_filters, input_filters, kernel_size=kernel_size, groups=input_filters, padding=kernel_size // 2, bias=False)
A:transformers.models.convbert.modeling_convbert.self.pointwise->torch.nn.Conv1d(input_filters, output_filters, kernel_size=1, bias=False)
A:transformers.models.convbert.modeling_convbert.self.bias->torch.nn.Parameter(torch.empty(output_size))
A:transformers.models.convbert.modeling_convbert.x->self.out_proj(x)
A:transformers.models.convbert.modeling_convbert.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.convbert.modeling_convbert.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.convbert.modeling_convbert.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.convbert.modeling_convbert.self.key_conv_attn_layer->SeparableConv1D(config, config.hidden_size, self.all_head_size, self.conv_kernel_size)
A:transformers.models.convbert.modeling_convbert.self.conv_kernel_layer->torch.nn.Linear(self.all_head_size, self.num_attention_heads * self.conv_kernel_size)
A:transformers.models.convbert.modeling_convbert.self.conv_out_layer->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.convbert.modeling_convbert.self.unfold->torch.nn.Unfold(kernel_size=[self.conv_kernel_size, 1], padding=[int((self.conv_kernel_size - 1) / 2), 0])
A:transformers.models.convbert.modeling_convbert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.convbert.modeling_convbert.batch_size->self.LayerNorm(hidden_states).size(0)
A:transformers.models.convbert.modeling_convbert.mixed_key_layer->self.key(hidden_states)
A:transformers.models.convbert.modeling_convbert.mixed_value_layer->self.value(hidden_states)
A:transformers.models.convbert.modeling_convbert.mixed_key_conv_attn_layer->mixed_key_conv_attn_layer.transpose(1, 2).transpose(1, 2)
A:transformers.models.convbert.modeling_convbert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.convbert.modeling_convbert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.models.convbert.modeling_convbert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.models.convbert.modeling_convbert.conv_attn_layer->torch.multiply(mixed_key_conv_attn_layer, mixed_query_layer)
A:transformers.models.convbert.modeling_convbert.conv_kernel_layer->torch.softmax(conv_kernel_layer, dim=1)
A:transformers.models.convbert.modeling_convbert.conv_out_layer->torch.reshape(conv_out_layer, [-1, self.all_head_size])
A:transformers.models.convbert.modeling_convbert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.convbert.modeling_convbert.attention_probs->self.dropout(attention_probs)
A:transformers.models.convbert.modeling_convbert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.convbert.modeling_convbert.conv_out->torch.reshape(conv_out_layer, [batch_size, -1, self.num_attention_heads, self.attention_head_size])
A:transformers.models.convbert.modeling_convbert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.convbert.modeling_convbert.hidden_states->self.LayerNorm(hidden_states)
A:transformers.models.convbert.modeling_convbert.self.self->ConvBertSelfAttention(config)
A:transformers.models.convbert.modeling_convbert.self.output->ConvBertOutput(config)
A:transformers.models.convbert.modeling_convbert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.convbert.modeling_convbert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.convbert.modeling_convbert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.convbert.modeling_convbert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.convbert.modeling_convbert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.convbert.modeling_convbert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.convbert.modeling_convbert.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, output_attentions)
A:transformers.models.convbert.modeling_convbert.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.convbert.modeling_convbert.self.weight->torch.nn.Parameter(torch.empty(self.num_groups, self.group_in_dim, self.group_out_dim))
A:transformers.models.convbert.modeling_convbert.self.attention->ConvBertAttention(config)
A:transformers.models.convbert.modeling_convbert.self.crossattention->ConvBertAttention(config)
A:transformers.models.convbert.modeling_convbert.self.intermediate->ConvBertIntermediate(config)
A:transformers.models.convbert.modeling_convbert.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.models.convbert.modeling_convbert.cross_attention_outputs->self.crossattention(attention_output, encoder_attention_mask, head_mask, encoder_hidden_states, output_attentions)
A:transformers.models.convbert.modeling_convbert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.convbert.modeling_convbert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.convbert.modeling_convbert.self.layer->torch.nn.ModuleList([ConvBertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.convbert.modeling_convbert.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)
A:transformers.models.convbert.modeling_convbert.self.embeddings->ConvBertEmbeddings(config)
A:transformers.models.convbert.modeling_convbert.self.embeddings_project->torch.nn.Linear(config.embedding_size, config.hidden_size)
A:transformers.models.convbert.modeling_convbert.self.encoder->ConvBertEncoder(config)
A:transformers.models.convbert.modeling_convbert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.convbert.modeling_convbert.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, device)
A:transformers.models.convbert.modeling_convbert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.convbert.modeling_convbert.self.convbert->ConvBertModel(config)
A:transformers.models.convbert.modeling_convbert.self.generator_predictions->ConvBertGeneratorPredictions(config)
A:transformers.models.convbert.modeling_convbert.self.generator_lm_head->torch.nn.Linear(config.embedding_size, config.vocab_size)
A:transformers.models.convbert.modeling_convbert.generator_hidden_states->self.convbert(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
A:transformers.models.convbert.modeling_convbert.prediction_scores->self.generator_lm_head(prediction_scores)
A:transformers.models.convbert.modeling_convbert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.convbert.modeling_convbert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.convbert.modeling_convbert.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.convbert.modeling_convbert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.convbert.modeling_convbert.outputs->self.convbert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.convbert.modeling_convbert.logits->self.qa_outputs(sequence_output)
A:transformers.models.convbert.modeling_convbert.self.sequence_summary->SequenceSummary(config)
A:transformers.models.convbert.modeling_convbert.pooled_output->self.sequence_summary(sequence_output)
A:transformers.models.convbert.modeling_convbert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.convbert.modeling_convbert.sequence_output->self.dropout(sequence_output)
A:transformers.models.convbert.modeling_convbert.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.convbert.modeling_convbert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.convbert.modeling_convbert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.convbert.modeling_convbert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.convbert.modeling_convbert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.convbert.modeling_convbert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.convbert.modeling_convbert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.convbert.modeling_convbert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.convbert.modeling_convbert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.convbert.modeling_convbert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.convbert.modeling_convbert.end_loss->loss_fct(end_logits, end_positions)
transformers.ConvBertForMaskedLM(self,config)
transformers.ConvBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ConvBertForMaskedLM.get_output_embeddings(self)
transformers.ConvBertForMaskedLM.set_output_embeddings(self,word_embeddings)
transformers.ConvBertForMultipleChoice(self,config)
transformers.ConvBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ConvBertForQuestionAnswering(self,config)
transformers.ConvBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ConvBertForSequenceClassification(self,config)
transformers.ConvBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ConvBertForTokenClassification(self,config)
transformers.ConvBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ConvBertLayer(self,config)
transformers.ConvBertLayer.feed_forward_chunk(self,attention_output)
transformers.ConvBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False)
transformers.ConvBertModel(self,config)
transformers.ConvBertModel._prune_heads(self,heads_to_prune)
transformers.ConvBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ConvBertModel.get_input_embeddings(self)
transformers.ConvBertModel.set_input_embeddings(self,value)
transformers.ConvBertPreTrainedModel(PreTrainedModel)
transformers.ConvBertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_convbert(model,config,tf_checkpoint_path)
transformers.models.convbert.modeling_convbert.ConvBertAttention(self,config)
transformers.models.convbert.modeling_convbert.ConvBertAttention.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,output_attentions=False)
transformers.models.convbert.modeling_convbert.ConvBertAttention.prune_heads(self,heads)
transformers.models.convbert.modeling_convbert.ConvBertClassificationHead(self,config)
transformers.models.convbert.modeling_convbert.ConvBertClassificationHead.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertClassificationHead.forward(self,hidden_states,**kwargs)
transformers.models.convbert.modeling_convbert.ConvBertEmbeddings(self,config)
transformers.models.convbert.modeling_convbert.ConvBertEmbeddings.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.convbert.modeling_convbert.ConvBertEncoder(self,config)
transformers.models.convbert.modeling_convbert.ConvBertEncoder.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.convbert.modeling_convbert.ConvBertForMaskedLM(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForMaskedLM.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.convbert.modeling_convbert.ConvBertForMaskedLM.get_output_embeddings(self)
transformers.models.convbert.modeling_convbert.ConvBertForMaskedLM.set_output_embeddings(self,word_embeddings)
transformers.models.convbert.modeling_convbert.ConvBertForMultipleChoice(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForMultipleChoice.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.convbert.modeling_convbert.ConvBertForQuestionAnswering(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForQuestionAnswering.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.convbert.modeling_convbert.ConvBertForSequenceClassification(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForSequenceClassification.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.convbert.modeling_convbert.ConvBertForTokenClassification(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForTokenClassification.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.convbert.modeling_convbert.ConvBertGeneratorPredictions(self,config)
transformers.models.convbert.modeling_convbert.ConvBertGeneratorPredictions.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertGeneratorPredictions.forward(self,generator_hidden_states)
transformers.models.convbert.modeling_convbert.ConvBertIntermediate(self,config)
transformers.models.convbert.modeling_convbert.ConvBertIntermediate.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertIntermediate.forward(self,hidden_states)
transformers.models.convbert.modeling_convbert.ConvBertLayer(self,config)
transformers.models.convbert.modeling_convbert.ConvBertLayer.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertLayer.feed_forward_chunk(self,attention_output)
transformers.models.convbert.modeling_convbert.ConvBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False)
transformers.models.convbert.modeling_convbert.ConvBertModel(self,config)
transformers.models.convbert.modeling_convbert.ConvBertModel.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertModel._prune_heads(self,heads_to_prune)
transformers.models.convbert.modeling_convbert.ConvBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.convbert.modeling_convbert.ConvBertModel.get_input_embeddings(self)
transformers.models.convbert.modeling_convbert.ConvBertModel.set_input_embeddings(self,value)
transformers.models.convbert.modeling_convbert.ConvBertOutput(self,config)
transformers.models.convbert.modeling_convbert.ConvBertOutput.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertOutput.forward(self,hidden_states,input_tensor)
transformers.models.convbert.modeling_convbert.ConvBertPreTrainedModel(PreTrainedModel)
transformers.models.convbert.modeling_convbert.ConvBertPreTrainedModel._init_weights(self,module)
transformers.models.convbert.modeling_convbert.ConvBertPredictionHeadTransform(self,config)
transformers.models.convbert.modeling_convbert.ConvBertPredictionHeadTransform.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.convbert.modeling_convbert.ConvBertSelfAttention(self,config)
transformers.models.convbert.modeling_convbert.ConvBertSelfAttention.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,output_attentions=False)
transformers.models.convbert.modeling_convbert.ConvBertSelfAttention.transpose_for_scores(self,x)
transformers.models.convbert.modeling_convbert.ConvBertSelfOutput(self,config)
transformers.models.convbert.modeling_convbert.ConvBertSelfOutput.__init__(self,config)
transformers.models.convbert.modeling_convbert.ConvBertSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.convbert.modeling_convbert.GroupedLinearLayer(self,input_size,output_size,num_groups)
transformers.models.convbert.modeling_convbert.GroupedLinearLayer.__init__(self,input_size,output_size,num_groups)
transformers.models.convbert.modeling_convbert.GroupedLinearLayer.forward(self,hidden_states)
transformers.models.convbert.modeling_convbert.SeparableConv1D(self,config,input_filters,output_filters,kernel_size,**kwargs)
transformers.models.convbert.modeling_convbert.SeparableConv1D.__init__(self,config,input_filters,output_filters,kernel_size,**kwargs)
transformers.models.convbert.modeling_convbert.SeparableConv1D.forward(self,hidden_states)
transformers.models.convbert.modeling_convbert.load_tf_weights_in_convbert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/visual_bert/configuration_visual_bert.py----------------------------------------
A:transformers.models.visual_bert.configuration_visual_bert.logger->utils.logging.get_logger(__name__)
transformers.VisualBertConfig(self,vocab_size=30522,hidden_size=768,visual_embedding_dim=512,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,bypass_transformer=False,special_visual_initialize=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.visual_bert.configuration_visual_bert.VisualBertConfig(self,vocab_size=30522,hidden_size=768,visual_embedding_dim=512,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,bypass_transformer=False,special_visual_initialize=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.visual_bert.configuration_visual_bert.VisualBertConfig.__init__(self,vocab_size=30522,hidden_size=768,visual_embedding_dim=512,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,bypass_transformer=False,special_visual_initialize=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/visual_bert/convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.sd->torch.load(checkpoint_path, map_location='cpu')
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.new_d->OrderedDict()
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.new_d['visual_bert.embeddings.position_ids']->torch.arange(config.max_position_embeddings).expand((1, -1))
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.new_key->new_key.replace(name_pair[0], name_pair[1]).replace(name_pair[0], name_pair[1])
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.config->VisualBertConfig(**config_params)
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.state_dict->load_state_dict(checkpoint_path)
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.new_state_dict->get_new_dict(state_dict, config)
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.model->VisualBertForMultipleChoice(config)
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.convert_visual_bert_checkpoint(checkpoint_path,pytorch_dump_folder_path)
transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.get_new_dict(d,config,rename_keys_prefix=rename_keys_prefix)
transformers.models.visual_bert.convert_visual_bert_original_pytorch_checkpoint_to_pytorch.load_state_dict(checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/visual_bert/__init__.py----------------------------------------
A:transformers.models.visual_bert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/visual_bert/modeling_visual_bert.py----------------------------------------
A:transformers.models.visual_bert.modeling_visual_bert.logger->utils.logging.get_logger(__name__)
A:transformers.models.visual_bert.modeling_visual_bert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.visual_bert.modeling_visual_bert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.visual_bert.modeling_visual_bert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.visual_bert.modeling_visual_bert.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.visual_bert.modeling_visual_bert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.visual_bert.modeling_visual_bert.self.visual_token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.visual_bert.modeling_visual_bert.self.visual_position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.visual_bert.modeling_visual_bert.self.visual_token_type_embeddings.weight.data->torch.nn.Parameter(self.token_type_embeddings.weight.data.clone(), requires_grad=True)
A:transformers.models.visual_bert.modeling_visual_bert.self.visual_position_embeddings.weight.data->torch.nn.Parameter(self.position_embeddings.weight.data.clone(), requires_grad=True)
A:transformers.models.visual_bert.modeling_visual_bert.self.visual_projection->torch.nn.Linear(config.visual_embedding_dim, config.hidden_size)
A:transformers.models.visual_bert.modeling_visual_bert.input_shape->input_ids.size()
A:transformers.models.visual_bert.modeling_visual_bert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.visual_bert.modeling_visual_bert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)
A:transformers.models.visual_bert.modeling_visual_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.visual_bert.modeling_visual_bert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.visual_bert.modeling_visual_bert.visual_token_type_ids->torch.ones(visual_embeds.size()[:-1], dtype=torch.long, device=self.position_ids.device)
A:transformers.models.visual_bert.modeling_visual_bert.visual_embeds->self.visual_projection(visual_embeds)
A:transformers.models.visual_bert.modeling_visual_bert.visual_token_type_embeddings->self.visual_token_type_embeddings(visual_token_type_ids)
A:transformers.models.visual_bert.modeling_visual_bert.image_text_alignment_mask->image_text_alignment_mask.to(dtype=dtype).sum(2).to(dtype=dtype).sum(2)
A:transformers.models.visual_bert.modeling_visual_bert.visual_position_embeddings->self.visual_position_embeddings(visual_position_ids)
A:transformers.models.visual_bert.modeling_visual_bert.visual_position_ids->torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=visual_embeds.device)
A:transformers.models.visual_bert.modeling_visual_bert.embeddings->self.dropout(embeddings)
A:transformers.models.visual_bert.modeling_visual_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.visual_bert.modeling_visual_bert.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.visual_bert.modeling_visual_bert.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.visual_bert.modeling_visual_bert.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.visual_bert.modeling_visual_bert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.visual_bert.modeling_visual_bert.mixed_query_layer->self.query(query)
A:transformers.models.visual_bert.modeling_visual_bert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.models.visual_bert.modeling_visual_bert.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.visual_bert.modeling_visual_bert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.visual_bert.modeling_visual_bert.attention_scores->attention_scores.squeeze(1).squeeze(1)
A:transformers.models.visual_bert.modeling_visual_bert.attention_probs->self.dropout(attention_probs)
A:transformers.models.visual_bert.modeling_visual_bert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.visual_bert.modeling_visual_bert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.visual_bert.modeling_visual_bert.hidden_states->self.decoder(hidden_states)
A:transformers.models.visual_bert.modeling_visual_bert.self.self->VisualBertSelfAttention(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.output->VisualBertOutput(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.visual_bert.modeling_visual_bert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.visual_bert.modeling_visual_bert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.visual_bert.modeling_visual_bert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.visual_bert.modeling_visual_bert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.visual_bert.modeling_visual_bert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.visual_bert.modeling_visual_bert.self_outputs->self.self(hidden_states, attention_mask, head_mask, output_attentions)
A:transformers.models.visual_bert.modeling_visual_bert.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.visual_bert.modeling_visual_bert.self.attention->VisualBertRegionToPhraseAttention(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.intermediate->VisualBertIntermediate(config)
A:transformers.models.visual_bert.modeling_visual_bert.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.models.visual_bert.modeling_visual_bert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.visual_bert.modeling_visual_bert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.visual_bert.modeling_visual_bert.self.layer->torch.nn.ModuleList([VisualBertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.visual_bert.modeling_visual_bert.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)
A:transformers.models.visual_bert.modeling_visual_bert.self.activation->torch.nn.Tanh()
A:transformers.models.visual_bert.modeling_visual_bert.pooled_output->self.dropout(pooled_output)
A:transformers.models.visual_bert.modeling_visual_bert.self.transform->VisualBertPredictionHeadTransform(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.visual_bert.modeling_visual_bert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.visual_bert.modeling_visual_bert.self.predictions->VisualBertLMPredictionHead(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.models.visual_bert.modeling_visual_bert.prediction_scores->self.predictions(sequence_output)
A:transformers.models.visual_bert.modeling_visual_bert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.visual_bert.modeling_visual_bert.self.embeddings->VisualBertEmbeddings(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.encoder->VisualBertEncoder(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.additional_layer->VisualBertLayer(config)
A:transformers.models.visual_bert.modeling_visual_bert.attention_mask->attention_mask.unsqueeze(1).unsqueeze(2).unsqueeze(1).unsqueeze(2)
A:transformers.models.visual_bert.modeling_visual_bert.visual_attention_mask->torch.ones(visual_input_shape, device=device)
A:transformers.models.visual_bert.modeling_visual_bert.combined_attention_mask->torch.cat((attention_mask, visual_attention_mask), dim=-1)
A:transformers.models.visual_bert.modeling_visual_bert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.visual_bert.modeling_visual_bert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, visual_embeds=visual_embeds, visual_token_type_ids=visual_token_type_ids, image_text_alignment=image_text_alignment)
A:transformers.models.visual_bert.modeling_visual_bert.text_length->input_ids.size(1)
A:transformers.models.visual_bert.modeling_visual_bert.encoded_outputs->self.encoder(text_embedding_output, attention_mask=text_extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.visual_bert.modeling_visual_bert.concatenated_input->torch.cat((sequence_output, visual_embedding_output), dim=1)
A:transformers.models.visual_bert.modeling_visual_bert.sequence_output->self.additional_layer(concatenated_input, extended_attention_mask)
A:transformers.models.visual_bert.modeling_visual_bert.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.visual_bert.modeling_visual_bert.self.visual_bert->VisualBertModel(config)
A:transformers.models.visual_bert.modeling_visual_bert.self.cls->VisualBertPreTrainingHeads(config)
A:transformers.models.visual_bert.modeling_visual_bert.outputs->self.visual_bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids, image_text_alignment=image_text_alignment, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.visual_bert.modeling_visual_bert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.models.visual_bert.modeling_visual_bert.loss_fct->KLDivLoss(reduction='batchmean')
A:transformers.models.visual_bert.modeling_visual_bert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.visual_bert.modeling_visual_bert.sentence_image_loss->loss_fct(seq_relationship_score.view(-1, 2), sentence_image_labels.view(-1))
A:transformers.models.visual_bert.modeling_visual_bert.total_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.visual_bert.modeling_visual_bert.logits->self.attention(selected_positions, visual_features, visual_attention_mask)
A:transformers.models.visual_bert.modeling_visual_bert.reshaped_logits->self.attention(selected_positions, visual_features, visual_attention_mask).contiguous()
A:transformers.models.visual_bert.modeling_visual_bert.loss->loss_fct(scores, labels)
A:transformers.models.visual_bert.modeling_visual_bert.index_to_gather->index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1)).unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))
A:transformers.models.visual_bert.modeling_visual_bert.log_softmax->LogSoftmax(dim=-1)
A:transformers.models.visual_bert.modeling_visual_bert.mixed_key_layer->self.key(key)
A:transformers.models.visual_bert.modeling_visual_bert.region_to_phrase_position_mask->(region_to_phrase_position != -1).long()
A:transformers.models.visual_bert.modeling_visual_bert.expanded_region_to_phrase_positions->region_to_phrase_position.unsqueeze(2).expand(region_to_phrase_position.size(0), region_to_phrase_position.size(1), sequence_output.size(2))
A:transformers.models.visual_bert.modeling_visual_bert.selected_positions->self.additional_layer(concatenated_input, extended_attention_mask).gather(1, expanded_region_to_phrase_positions)
A:transformers.models.visual_bert.modeling_visual_bert.scores->log_softmax(logits)
A:transformers.models.visual_bert.modeling_visual_bert.labels->labels.contiguous().contiguous()
transformers.VisualBertForMultipleChoice(self,config)
transformers.VisualBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.VisualBertForPreTraining(self,config)
transformers.VisualBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,sentence_image_labels=None)
transformers.VisualBertForPreTraining.get_output_embeddings(self)
transformers.VisualBertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.VisualBertForPreTrainingOutput(ModelOutput)
transformers.VisualBertForQuestionAnswering(self,config)
transformers.VisualBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.VisualBertForRegionToPhraseAlignment(self,config)
transformers.VisualBertForRegionToPhraseAlignment.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,region_to_phrase_position=None,labels=None)
transformers.VisualBertForVisualReasoning(self,config)
transformers.VisualBertForVisualReasoning.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.VisualBertLayer(self,config)
transformers.VisualBertLayer.feed_forward_chunk(self,attention_output)
transformers.VisualBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.VisualBertModel(self,config,add_pooling_layer=True)
transformers.VisualBertModel._prune_heads(self,heads_to_prune)
transformers.VisualBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.VisualBertModel.get_input_embeddings(self)
transformers.VisualBertModel.set_input_embeddings(self,value)
transformers.VisualBertPreTrainedModel(PreTrainedModel)
transformers.VisualBertPreTrainedModel._init_weights(self,module)
transformers.models.visual_bert.modeling_visual_bert.VisualBertAttention(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertAttention.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.visual_bert.modeling_visual_bert.VisualBertAttention.prune_heads(self,heads)
transformers.models.visual_bert.modeling_visual_bert.VisualBertEmbeddings(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertEmbeddings.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,visual_embeds=None,visual_token_type_ids=None,image_text_alignment=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertEncoder(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertEncoder.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForMultipleChoice(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForMultipleChoice.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTraining(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTraining.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,sentence_image_labels=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTraining.get_output_embeddings(self)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput(ModelOutput)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForQuestionAnswering(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForQuestionAnswering.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForRegionToPhraseAlignment(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForRegionToPhraseAlignment.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForRegionToPhraseAlignment.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,region_to_phrase_position=None,labels=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForVisualReasoning(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForVisualReasoning.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertForVisualReasoning.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertIntermediate(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertIntermediate.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertIntermediate.forward(self,hidden_states)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLMPredictionHead(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLMPredictionHead.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLMPredictionHead.forward(self,hidden_states)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLayer(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLayer.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLayer.feed_forward_chunk(self,attention_output)
transformers.models.visual_bert.modeling_visual_bert.VisualBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.visual_bert.modeling_visual_bert.VisualBertModel(self,config,add_pooling_layer=True)
transformers.models.visual_bert.modeling_visual_bert.VisualBertModel.__init__(self,config,add_pooling_layer=True)
transformers.models.visual_bert.modeling_visual_bert.VisualBertModel._prune_heads(self,heads_to_prune)
transformers.models.visual_bert.modeling_visual_bert.VisualBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,visual_embeds=None,visual_attention_mask=None,visual_token_type_ids=None,image_text_alignment=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.visual_bert.modeling_visual_bert.VisualBertModel.get_input_embeddings(self)
transformers.models.visual_bert.modeling_visual_bert.VisualBertModel.set_input_embeddings(self,value)
transformers.models.visual_bert.modeling_visual_bert.VisualBertOutput(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertOutput.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertOutput.forward(self,hidden_states,input_tensor)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPooler(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPooler.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPooler.forward(self,hidden_states)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPreTrainedModel(PreTrainedModel)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPreTrainedModel._init_weights(self,module)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPreTrainingHeads(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPreTrainingHeads.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPredictionHeadTransform(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPredictionHeadTransform.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.visual_bert.modeling_visual_bert.VisualBertRegionToPhraseAttention(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertRegionToPhraseAttention.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertRegionToPhraseAttention.forward(self,query,key,attention_mask)
transformers.models.visual_bert.modeling_visual_bert.VisualBertRegionToPhraseAttention.transpose_for_scores(self,x)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfAttention(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfAttention.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfAttention.transpose_for_scores(self,x)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfOutput(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfOutput.__init__(self,config)
transformers.models.visual_bert.modeling_visual_bert.VisualBertSelfOutput.forward(self,hidden_states,input_tensor)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/modeling_transfo_xl_utilities.py----------------------------------------
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.self.cluster_weight->torch.nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.self.cluster_bias->torch.nn.Parameter(torch.zeros(self.n_clusters))
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.self.out_layers->torch.nn.ModuleList()
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.self.out_projs->torch.nn.ParameterList()
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.logit->self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.proj_hid->torch.nn.functional.linear(hidden, proj.t().contiguous())
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.hidden->hidden.view(-1, hidden.size(-1)).view(-1, hidden.size(-1))
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.labels->labels.view(-1).view(-1)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.out->hidden.view(-1, hidden.size(-1)).view(-1, hidden.size(-1)).new_empty((head_logit.size(0), self.n_token))
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.weight_i->torch.cat([weight_i, self.cluster_weight], dim=0)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.bias_i->torch.cat([bias_i, self.cluster_bias], dim=0)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.head_logit->self._compute_logit(hidden, head_weight, head_bias, head_proj)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.head_logprob->torch.nn.functional.log_softmax(head_logit, dim=1)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.indices_i->mask_i.nonzero().squeeze()
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.head_logprob_i->torch.nn.functional.log_softmax(head_logit, dim=1).index_select(0, indices_i)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.hidden_i->hidden.view(-1, hidden.size(-1)).view(-1, hidden.size(-1)).index_select(0, indices_i)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.logprob_i->torch.nn.functional.log_softmax(head_logit, dim=1).index_select(0, indices_i).gather(1, target_i[:, None]).squeeze(1)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.tail_logit_i->self._compute_logit(hidden, weight_i, bias_i, proj_i)
A:transformers.models.transfo_xl.modeling_transfo_xl_utilities.tail_logprob_i->torch.nn.functional.log_softmax(tail_logit_i, dim=1)
transformers.models.transfo_xl.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False)
transformers.models.transfo_xl.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False)
transformers.models.transfo_xl.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit(self,hidden,weight,bias,proj)
transformers.models.transfo_xl.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.forward(self,hidden,labels=None,keep_order=False)
transformers.models.transfo_xl.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.log_prob(self,hidden)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.corpus->pickle.load(fp, encoding='latin1')
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.config_path->os.path.abspath(transfo_xl_config_file)
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.config->transformers.TransfoXLConfig.from_json_file(transfo_xl_config_file)
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.model->load_tf_weights_in_transfo_xl(model, config, tf_path)
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.pytorch_config_dump_path->os.path.join(pytorch_dump_folder_path, CONFIG_NAME)
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.transfo_xl.convert_transfo_xl_original_tf_checkpoint_to_pytorch.convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,transfo_xl_config_file,pytorch_dump_folder_path,transfo_xl_dataset_file)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/configuration_transfo_xl.py----------------------------------------
A:transformers.models.transfo_xl.configuration_transfo_xl.logger->utils.logging.get_logger(__name__)
transformers.TransfoXLConfig(self,vocab_size=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,eos_token_id=0,**kwargs)
transformers.TransfoXLConfig.hidden_size(self)
transformers.TransfoXLConfig.max_position_embeddings(self)
transformers.TransfoXLConfig.n_token(self)
transformers.TransfoXLConfig.n_token(self,value)
transformers.TransfoXLConfig.num_attention_heads(self)
transformers.TransfoXLConfig.num_hidden_layers(self)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig(self,vocab_size=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,eos_token_id=0,**kwargs)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.__init__(self,vocab_size=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,eos_token_id=0,**kwargs)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.hidden_size(self)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.max_position_embeddings(self)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.n_token(self)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.n_token(self,value)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.num_attention_heads(self)
transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/modeling_tf_transfo_xl.py----------------------------------------
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.logger->utils.logging.get_logger(__name__)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.inv_freq->tensorflow.cast(self.inv_freq, dtype=pos_seq.dtype)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.sinusoid_inp->tensorflow.einsum('i,j->ij', pos_seq, self.inv_freq)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.pos_emb->self.drop(pos_emb, training=inputs['training'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.layer_1->tensorflow.keras.layers.Dense(d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name='CoreNet_._0')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.drop_1->tensorflow.keras.layers.Dropout(dropout)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.layer_2->tensorflow.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name='CoreNet_._3')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.drop_2->tensorflow.keras.layers.Dropout(dropout)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layer_norm')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.core_out->tensorflow.transpose(core_out, perm=(1, 0, 2))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.output->self.call(inputs)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.qkv_net->tensorflow.keras.layers.Dense(3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name='qkv_net')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.drop->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.dropatt->tensorflow.keras.layers.Dropout(dropatt)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.o_net->tensorflow.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name='o_net')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.r_net->tensorflow.keras.layers.Dense(self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name='r_net')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.r_r_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_r_bias')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.r_w_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_w_bias')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.x_size->shape_list(x)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.x->tensorflow.reshape(x, x_size)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.mems->tensorflow.cast(mems, dtype=w.dtype)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.cat->tensorflow.concat([mems[i], hids[i]], axis=0)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.w_heads->self.qkv_net(w)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.r_head_k->tensorflow.reshape(r_head_k, (rlen, self.n_head, self.d_head))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.(w_head_q, w_head_k, w_head_v)->tensorflow.split(w_heads, 3, axis=-1)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.w_head_q->tensorflow.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.w_head_k->tensorflow.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.w_head_v->tensorflow.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.AC->tensorflow.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.BD->self._rel_shift(BD)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_mask_t->tensorflow.cast(attn_mask_t, dtype=attn_score.dtype)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_prob->self.dropatt(attn_prob, training=training)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_vec->tensorflow.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_vec_sizes->shape_list(attn_vec)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_out->self.drop(attn_out, training=training)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.dec_attn->TFRelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, dropatt=dropatt, pre_lnorm=pre_lnorm, r_w_bias=r_w_bias, r_r_bias=r_r_bias, init_std=init_std, layer_norm_epsilon=layer_norm_epsilon, output_attentions=output_attentions, name='dec_attn')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.pos_ff->TFPositionwiseFF(d_model, d_inner, dropout, pre_lnorm=pre_lnorm, init_std=init_std, layer_norm_epsilon=layer_norm_epsilon, name='pos_ff')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_outputs->self.dec_attn(dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=training)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.ff_output->self.pos_ff(attn_outputs[0], training=training)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.weight->self.add_weight(shape=(self.vocab_size, self.emb_size), initializer=get_initializer(self.init_std), name='embeddings')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.inp_flat->tensorflow.reshape(inp, (-1,))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.emb_flat->tensorflow.cast(emb_flat, dtype=scatter.dtype)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.emb_i->tensorflow.einsum('id,de->ie', emb_i, self.emb_projs[i])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.mask_idx->tensorflow.where(mask_i)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.scatter->tensorflow.scatter_nd(mask_idx, emb_i, shape_list(emb_flat))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.embed->tensorflow.reshape(emb_flat, embed_shape)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.word_emb->TFAdaptiveEmbedding(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, init_std=config.init_std, name='word_emb')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.pos_emb->TFPositionalEmbedding(self.d_model, name='pos_emb')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.empty->tensorflow.zeros([self.mem_len, bsz, self.d_model])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.beg_idx->tensorflow.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.mems[i]->tensorflow.cast(mems[i], dtype=hids[i].dtype)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, mems=mems, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.inputs['input_ids']->tensorflow.transpose(inputs['input_ids'], perm=(1, 0))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.(qlen, bsz)->shape_list(inputs['input_ids'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.inputs['inputs_embeds']->tensorflow.transpose(inputs['inputs_embeds'], perm=(1, 0, 2))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.inputs['mems']->self.init_mems(bsz)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.word_emb->self.word_emb(inputs['input_ids'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_mask->tensorflow.ones([qlen, qlen])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.mask_u->tensorflow.linalg.band_part(attn_mask, 0, -1)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.mask_dia->tensorflow.linalg.band_part(attn_mask, 0, 0)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attn_mask_pad->tensorflow.zeros([qlen, mlen])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.dec_attn_mask->tensorflow.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.mask_l->tensorflow.linalg.band_part(attn_mask, -1, 0)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.pos_seq->tensorflow.minimum(pos_seq, self.clamp_len)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.layer_outputs->layer(core_out, pos_emb, dec_attn_mask, mems_i, inputs['head_mask'][i], inputs['output_attentions'], training=inputs['training'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.new_mems->self._update_mems(hids, inputs['mems'], mlen, qlen)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.hids->tuple((tf.transpose(t, perm=(1, 0, 2)) for t in hids))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.attentions->tuple((tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions))
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.transformer->TFTransfoXLMainLayer(config, name='transformer')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.outputs->self.transformer(input_ids=inputs['input_ids'], mems=inputs['mems'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.crit->TFAdaptiveSoftmaxMask(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, name='crit')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], mems=inputs['mems'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.softmax_output->self.crit(pred_hid, labels, training=inputs['training'])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.self.score->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.init_range), name='score', use_bias=False)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.logits->self.score(hidden_states)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.in_logits->tensorflow.gather(logits, sequence_lengths, batch_dims=1, axis=1)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl.loss->self.compute_loss(tf.reshape(inputs['labels'], [-1, 1]), tf.reshape(in_logits, [-1, self.num_labels]))
transformers.TFAdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.TFAdaptiveEmbedding.build(self,input_shape)
transformers.TFAdaptiveEmbedding.call(self,inp)
transformers.TFTransfoXLForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFTransfoXLForSequenceClassification.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFTransfoXLForSequenceClassification.get_output_embeddings(self)
transformers.TFTransfoXLForSequenceClassification.serving_output(self,output)
transformers.TFTransfoXLLMHeadModel(self,config)
transformers.TFTransfoXLLMHeadModel._resize_token_embeddings(self,new_num_tokens)
transformers.TFTransfoXLLMHeadModel.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFTransfoXLLMHeadModel.get_output_embeddings(self)
transformers.TFTransfoXLLMHeadModel.init_mems(self,bsz)
transformers.TFTransfoXLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**model_kwargs)
transformers.TFTransfoXLLMHeadModel.reset_memory_length(self,mem_len)
transformers.TFTransfoXLLMHeadModel.serving_output(self,output)
transformers.TFTransfoXLLMHeadModelOutput(ModelOutput)
transformers.TFTransfoXLMainLayer(self,config,**kwargs)
transformers.TFTransfoXLMainLayer._prune_heads(self,heads)
transformers.TFTransfoXLMainLayer._update_mems(self,hids,mems,mlen,qlen)
transformers.TFTransfoXLMainLayer.backward_compatible(self)
transformers.TFTransfoXLMainLayer.build(self,input_shape)
transformers.TFTransfoXLMainLayer.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFTransfoXLMainLayer.get_input_embeddings(self)
transformers.TFTransfoXLMainLayer.init_mems(self,bsz)
transformers.TFTransfoXLMainLayer.reset_memory_length(self,mem_len)
transformers.TFTransfoXLMainLayer.set_input_embeddings(self,value)
transformers.TFTransfoXLModel(self,config,*inputs,**kwargs)
transformers.TFTransfoXLModel.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFTransfoXLModel.serving_output(self,output)
transformers.TFTransfoXLModelOutput(ModelOutput)
transformers.TFTransfoXLPreTrainedModel(TFPreTrainedModel)
transformers.TFTransfoXLPreTrainedModel.serving(self,inputs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFAdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFAdaptiveEmbedding.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFAdaptiveEmbedding.build(self,input_shape)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFAdaptiveEmbedding.call(self,inp)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFPositionalEmbedding(self,demb,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFPositionalEmbedding.__init__(self,demb,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFPositionalEmbedding.call(self,pos_seq,bsz=None)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFPositionwiseFF(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFPositionwiseFF.__init__(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFPositionwiseFF.call(self,inp,training=False)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer(self,n_head,d_model,d_head,d_inner,dropout,dropatt=0.0,pre_lnorm=False,r_w_bias=None,r_r_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer.__init__(self,n_head,d_model,d_head,d_inner,dropout,dropatt=0.0,pre_lnorm=False,r_w_bias=None,r_r_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer.call(self,dec_inp,r,dec_attn_mask,mems,head_mask,output_attentions,training=False)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn(self,n_head,d_model,d_head,dropout,dropatt=0.0,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.__init__(self,n_head,d_model,d_head,dropout,dropatt=0.0,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn._rel_shift(self,x)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.build(self,input_shape)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.call(self,w,r,attn_mask,mems,head_mask,output_attentions,training=False)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoEmbeddings(self,vocab_size,emb_size,init_std,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoEmbeddings.__init__(self,vocab_size,emb_size,init_std,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoEmbeddings.build(self,input_shape)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoEmbeddings.call(self,inputs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLForSequenceClassification.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLForSequenceClassification.get_output_embeddings(self)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLForSequenceClassification.serving_output(self,output)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel(self,config)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.__init__(self,config)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel._resize_token_embeddings(self,new_num_tokens)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.get_output_embeddings(self)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.init_mems(self,bsz)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**model_kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.reset_memory_length(self,mem_len)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.serving_output(self,output)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput(ModelOutput)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer(self,config,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.__init__(self,config,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer._prune_heads(self,heads)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer._update_mems(self,hids,mems,mlen,qlen)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.backward_compatible(self)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.build(self,input_shape)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.get_input_embeddings(self)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.init_mems(self,bsz)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.reset_memory_length(self,mem_len)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLMainLayer.set_input_embeddings(self,value)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModel(self,config,*inputs,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModel.__init__(self,config,*inputs,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModel.call(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModel.serving_output(self,output)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput(ModelOutput)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLPreTrainedModel(TFPreTrainedModel)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLPreTrainedModel.serving(self,inputs)
transformers.models.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLSequenceClassifierOutputWithPast(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/modeling_tf_transfo_xl_utilities.py----------------------------------------
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.self.cluster_weight->self.add_weight(shape=(self.n_clusters, self.d_embed), initializer='zeros', trainable=True, name='cluster_weight')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.self.cluster_bias->self.add_weight(shape=(self.n_clusters,), initializer='zeros', trainable=True, name='cluster_bias')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.weight->self.add_weight(shape=(r_idx - l_idx, d_emb_i), initializer='zeros', trainable=True, name=f'out_layers_._{i}_._weight')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.bias->self.add_weight(shape=(r_idx - l_idx,), initializer='zeros', trainable=True, name=f'out_layers_._{i}_._bias')
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.y->tensorflow.einsum('ibd,ed->ibe', y, proj)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.lp_size->shape_list(logprob)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.r->tensorflow.range(lp_size[0])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.idx->tensorflow.stack([r, target], 1)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.output->self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.loss->tensorflow.reduce_mean(loss)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.out->tensorflow.concat(out, axis=-1)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.hidden_sizes->shape_list(hidden)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.mask_idx->tensorflow.where(mask)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.cur_W->tensorflow.concat([cur_W, self.cluster_weight], 0)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.cur_b->tensorflow.concat([cur_b, self.cluster_bias], 0)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.head_logit->self._logit(hidden, cur_W, cur_b, self.out_projs[0])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.head_logprob->tensorflow.nn.log_softmax(head_logit)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.cur_head_logprob->tensorflow.boolean_mask(head_logprob, mask)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.cur_logprob->self._gather_logprob(cur_tail_logprob, cur_target)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.tail_logit->self._logit(hidden, cur_W, cur_b, self.out_projs[i])
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.tail_logprob->tensorflow.nn.log_softmax(tail_logit)
A:transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.cur_tail_logprob->tensorflow.boolean_mask(tail_logprob, mask)
transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask(self,vocab_size,d_embed,d_proj,cutoffs,div_val=1,keep_order=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.__init__(self,vocab_size,d_embed,d_proj,cutoffs,div_val=1,keep_order=False,**kwargs)
transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask._gather_logprob(logprob,target)
transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask._logit(x,W,b,proj=None)
transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.build(self,input_shape)
transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.call(self,hidden,target,return_mean=True,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/__init__.py----------------------------------------
A:transformers.models.transfo_xl.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/modeling_transfo_xl.py----------------------------------------
A:transformers.models.transfo_xl.modeling_transfo_xl.logger->utils.logging.get_logger(__name__)
A:transformers.models.transfo_xl.modeling_transfo_xl.tf_to_pt_map->build_tf_to_pytorch_map(model, config)
A:transformers.models.transfo_xl.modeling_transfo_xl.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.transfo_xl.modeling_transfo_xl.array->numpy.transpose(array)
A:transformers.models.transfo_xl.modeling_transfo_xl.p_i.data->torch.from_numpy(arr_i)
A:transformers.models.transfo_xl.modeling_transfo_xl.pointer.data->torch.from_numpy(array)
A:transformers.models.transfo_xl.modeling_transfo_xl.sinusoid_inp->torch.ger(pos_seq, self.inv_freq)
A:transformers.models.transfo_xl.modeling_transfo_xl.pos_emb->self.drop(pos_emb)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.CoreNet->torch.nn.Sequential(nn.Linear(d_model, d_inner), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(d_inner, d_model), nn.Dropout(dropout))
A:transformers.models.transfo_xl.modeling_transfo_xl.self.layer_norm->torch.nn.LayerNorm(d_model, eps=layer_norm_epsilon)
A:transformers.models.transfo_xl.modeling_transfo_xl.core_out->core_out.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.models.transfo_xl.modeling_transfo_xl.output->self.layer_norm(inp + core_out)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.qkv_net->torch.nn.Linear(d_model, 3 * n_head * d_head, bias=False)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.drop->torch.nn.Dropout(config.dropout)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.dropatt->torch.nn.Dropout(dropatt)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.o_net->torch.nn.Linear(n_head * d_head, d_model, bias=False)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.r_r_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.models.transfo_xl.modeling_transfo_xl.self.r_w_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.models.transfo_xl.modeling_transfo_xl.self.r_net->torch.nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)
A:transformers.models.transfo_xl.modeling_transfo_xl.zero_pad->torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)
A:transformers.models.transfo_xl.modeling_transfo_xl.x_padded->x_padded.view(*x_padded_shape).view(*x_padded_shape)
A:transformers.models.transfo_xl.modeling_transfo_xl.x->x_padded[1:].view_as(x)
A:transformers.models.transfo_xl.modeling_transfo_xl.cat->torch.cat([mems[i], hids[i]], dim=0)
A:transformers.models.transfo_xl.modeling_transfo_xl.w_heads->self.qkv_net(w)
A:transformers.models.transfo_xl.modeling_transfo_xl.r_head_k->r_head_k.view(rlen, self.n_head, self.d_head).view(rlen, self.n_head, self.d_head)
A:transformers.models.transfo_xl.modeling_transfo_xl.(w_head_q, w_head_k, w_head_v)->torch.chunk(w_heads, 3, dim=-1)
A:transformers.models.transfo_xl.modeling_transfo_xl.klen->w_head_k.view(klen, bsz, self.n_head, self.d_head).size(0)
A:transformers.models.transfo_xl.modeling_transfo_xl.w_head_q->w_head_q.view(qlen, bsz, self.n_head, self.d_head).view(qlen, bsz, self.n_head, self.d_head)
A:transformers.models.transfo_xl.modeling_transfo_xl.w_head_k->w_head_k.view(klen, bsz, self.n_head, self.d_head).view(klen, bsz, self.n_head, self.d_head)
A:transformers.models.transfo_xl.modeling_transfo_xl.w_head_v->w_head_v.view(klen, bsz, self.n_head, self.d_head).view(klen, bsz, self.n_head, self.d_head)
A:transformers.models.transfo_xl.modeling_transfo_xl.AC->torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))
A:transformers.models.transfo_xl.modeling_transfo_xl.BD->self._rel_shift(BD)
A:transformers.models.transfo_xl.modeling_transfo_xl.attn_score->attn_score.float().masked_fill(attn_mask[:, :, :, None], -1e+30).type_as(attn_score).float().masked_fill(attn_mask[:, :, :, None], -1e+30).type_as(attn_score)
A:transformers.models.transfo_xl.modeling_transfo_xl.attn_prob->self.dropatt(attn_prob)
A:transformers.models.transfo_xl.modeling_transfo_xl.attn_vec->attn_vec.contiguous().view(attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head).contiguous().view(attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)
A:transformers.models.transfo_xl.modeling_transfo_xl.attn_out->self.drop(attn_out)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.dec_attn->RelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, layer_norm_epsilon=layer_norm_epsilon, **kwargs)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.pos_ff->PositionwiseFF(d_model, d_inner, dropout, pre_lnorm=kwargs.get('pre_lnorm'), layer_norm_epsilon=layer_norm_epsilon)
A:transformers.models.transfo_xl.modeling_transfo_xl.attn_outputs->self.dec_attn(dec_inp, r, attn_mask=dec_attn_mask, mems=mems, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.models.transfo_xl.modeling_transfo_xl.ff_output->self.pos_ff(attn_outputs[0])
A:transformers.models.transfo_xl.modeling_transfo_xl.self.emb_layers->torch.nn.ModuleList()
A:transformers.models.transfo_xl.modeling_transfo_xl.self.emb_projs->torch.nn.ParameterList()
A:transformers.models.transfo_xl.modeling_transfo_xl.embed->torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device).view(embed_shape)
A:transformers.models.transfo_xl.modeling_transfo_xl.param->next(self.parameters())
A:transformers.models.transfo_xl.modeling_transfo_xl.inp_flat->inp.view(-1)
A:transformers.models.transfo_xl.modeling_transfo_xl.emb_flat->torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device)
A:transformers.models.transfo_xl.modeling_transfo_xl.indices_i->mask_i.nonzero().squeeze()
A:transformers.models.transfo_xl.modeling_transfo_xl.emb_i->torch.nn.functional.linear(emb_i, self.emb_projs[i])
A:transformers.models.transfo_xl.modeling_transfo_xl.base_model->getattr(self, self.base_model_prefix, self)
A:transformers.models.transfo_xl.modeling_transfo_xl.(new_num_tokens_layer, layer)->self._get_new_num_tokens_layer(new_num_tokens, layer)
A:transformers.models.transfo_xl.modeling_transfo_xl.model_embeds->getattr(self, self.base_model_prefix, self)._resize_token_embeddings(new_num_tokens_layer, layer)
A:transformers.models.transfo_xl.modeling_transfo_xl.new_embedding_shapes->self._get_embedding_shapes()
A:transformers.models.transfo_xl.modeling_transfo_xl.embeddings->self.get_input_embeddings()
A:transformers.models.transfo_xl.modeling_transfo_xl.new_embeddings_layer->self._get_resized_embeddings(embeddings.emb_layers[layer], new_num_tokens)
A:transformers.models.transfo_xl.modeling_transfo_xl.embeddings.cutoffs[i]->sum(new_embedding_shapes[:i + 1])
A:transformers.models.transfo_xl.modeling_transfo_xl.self.word_emb->AdaptiveEmbedding(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.layers->torch.nn.ModuleList()
A:transformers.models.transfo_xl.modeling_transfo_xl.self.pos_emb->PositionalEmbedding(self.d_model)
A:transformers.models.transfo_xl.modeling_transfo_xl.empty->torch.zeros(self.mem_len, bsz, self.config.d_model, dtype=param.dtype, device=param.device)
A:transformers.models.transfo_xl.modeling_transfo_xl.beg_idx->max(0, end_idx - self.mem_len)
A:transformers.models.transfo_xl.modeling_transfo_xl.input_ids->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.models.transfo_xl.modeling_transfo_xl.(qlen, bsz)->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous().size()
A:transformers.models.transfo_xl.modeling_transfo_xl.inputs_embeds->inputs_embeds.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.models.transfo_xl.modeling_transfo_xl.mems->self.init_mems(bsz)
A:transformers.models.transfo_xl.modeling_transfo_xl.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.models.transfo_xl.modeling_transfo_xl.word_emb->self.word_emb(input_ids)
A:transformers.models.transfo_xl.modeling_transfo_xl.all_ones->self.word_emb(input_ids).new_ones((qlen, klen), dtype=torch.uint8)
A:transformers.models.transfo_xl.modeling_transfo_xl.pos_seq->torch.arange(klen - 1, -1, -1.0, device=word_emb.device, dtype=word_emb.dtype)
A:transformers.models.transfo_xl.modeling_transfo_xl.layer_outputs->layer(core_out, pos_emb, dec_attn_mask=dec_attn_mask, mems=mems_i, head_mask=head_mask[i], output_attentions=output_attentions)
A:transformers.models.transfo_xl.modeling_transfo_xl.new_mems->self._update_mems(hids, mems, mlen, qlen)
A:transformers.models.transfo_xl.modeling_transfo_xl.hids->tuple((t.transpose(0, 1).contiguous() for t in hids))
A:transformers.models.transfo_xl.modeling_transfo_xl.attentions->tuple((t.permute(2, 3, 0, 1).contiguous() for t in attentions))
A:transformers.models.transfo_xl.modeling_transfo_xl.self.transformer->TransfoXLModel(config)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.crit->ProjectedAdaptiveLogSoftmax(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.crit.out_projs[i]->torch.nn.Parameter(self.transformer.word_emb.emb_projs[i].clone())
A:transformers.models.transfo_xl.modeling_transfo_xl.transformer_outputs->self.transformer(input_ids, mems=mems, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.transfo_xl.modeling_transfo_xl.softmax_output->self.crit(pred_hid, labels)
A:transformers.models.transfo_xl.modeling_transfo_xl.inputs['input_ids']->input_ids[:, -1].unsqueeze(-1)
A:transformers.models.transfo_xl.modeling_transfo_xl.new_cutoffs->super()._resize_cutoffs(new_num_tokens, new_emb_size, new_embedding_shapes, layer)
A:transformers.models.transfo_xl.modeling_transfo_xl.self.score->torch.nn.Linear(config.d_embed, self.num_labels, bias=False)
A:transformers.models.transfo_xl.modeling_transfo_xl.logits->self.score(hidden_states)
A:transformers.models.transfo_xl.modeling_transfo_xl.loss_fct->CrossEntropyLoss()
A:transformers.models.transfo_xl.modeling_transfo_xl.loss->loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
transformers.AdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.AdaptiveEmbedding.forward(self,inp)
transformers.TransfoXLForSequenceClassification(self,config)
transformers.TransfoXLForSequenceClassification.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TransfoXLLMHeadModel(self,config)
transformers.TransfoXLLMHeadModel._reorder_cache(mems:List[torch.Tensor],beam_idx:torch.Tensor)->List[torch.Tensor]
transformers.TransfoXLLMHeadModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.TransfoXLLMHeadModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TransfoXLLMHeadModel.get_output_embeddings(self)
transformers.TransfoXLLMHeadModel.init_mems(self,bsz)
transformers.TransfoXLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,**model_kwargs)
transformers.TransfoXLLMHeadModel.reset_memory_length(self,mem_len)
transformers.TransfoXLLMHeadModel.tie_weights(self)
transformers.TransfoXLLMHeadModelOutput(ModelOutput)
transformers.TransfoXLLMHeadModelOutput.logits(self)
transformers.TransfoXLModel(self,config)
transformers.TransfoXLModel._prune_heads(self,heads)
transformers.TransfoXLModel._update_mems(self,hids,mems,mlen,qlen)
transformers.TransfoXLModel.backward_compatible(self)
transformers.TransfoXLModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TransfoXLModel.get_input_embeddings(self)
transformers.TransfoXLModel.init_mems(self,bsz)
transformers.TransfoXLModel.reset_memory_length(self,mem_len)
transformers.TransfoXLModel.set_input_embeddings(self,new_embeddings)
transformers.TransfoXLModelOutput(ModelOutput)
transformers.TransfoXLPreTrainedModel(PreTrainedModel)
transformers.TransfoXLPreTrainedModel._get_embedding_shapes(self)
transformers.TransfoXLPreTrainedModel._get_new_num_tokens_layer(self,new_num_tokens,layer)
transformers.TransfoXLPreTrainedModel._init_bias(self,bias)
transformers.TransfoXLPreTrainedModel._init_weight(self,weight)
transformers.TransfoXLPreTrainedModel._init_weights(self,m)
transformers.TransfoXLPreTrainedModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.TransfoXLPreTrainedModel._resize_token_embeddings(self,new_num_tokens,layer=-1)
transformers.TransfoXLPreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None,layer:Optional[int]=-1)
transformers.load_tf_weights_in_transfo_xl(model,config,tf_path)
transformers.models.transfo_xl.modeling_transfo_xl.AdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.models.transfo_xl.modeling_transfo_xl.AdaptiveEmbedding.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.models.transfo_xl.modeling_transfo_xl.AdaptiveEmbedding.forward(self,inp)
transformers.models.transfo_xl.modeling_transfo_xl.PositionalEmbedding(self,demb)
transformers.models.transfo_xl.modeling_transfo_xl.PositionalEmbedding.__init__(self,demb)
transformers.models.transfo_xl.modeling_transfo_xl.PositionalEmbedding.forward(self,pos_seq,bsz=None)
transformers.models.transfo_xl.modeling_transfo_xl.PositionwiseFF(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05)
transformers.models.transfo_xl.modeling_transfo_xl.PositionwiseFF.__init__(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05)
transformers.models.transfo_xl.modeling_transfo_xl.PositionwiseFF.forward(self,inp)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableDecoderLayer(self,n_head,d_model,d_head,d_inner,dropout,layer_norm_epsilon=1e-05,**kwargs)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableDecoderLayer.__init__(self,n_head,d_model,d_head,d_inner,dropout,layer_norm_epsilon=1e-05,**kwargs)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableDecoderLayer.forward(self,dec_inp,r,dec_attn_mask=None,mems=None,head_mask=None,output_attentions=False)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn(self,n_head,d_model,d_head,dropout,dropatt=0,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.__init__(self,n_head,d_model,d_head,dropout,dropatt=0,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn._rel_shift(self,x)
transformers.models.transfo_xl.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.forward(self,w,r,attn_mask=None,mems=None,head_mask=None,output_attentions=False)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLForSequenceClassification(self,config)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLForSequenceClassification.__init__(self,config)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLForSequenceClassification.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel(self,config)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.__init__(self,config)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel._reorder_cache(mems:List[torch.Tensor],beam_idx:torch.Tensor)->List[torch.Tensor]
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.get_output_embeddings(self)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.init_mems(self,bsz)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,**model_kwargs)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.reset_memory_length(self,mem_len)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModel.tie_weights(self)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput(ModelOutput)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput.logits(self)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel(self,config)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.__init__(self,config)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel._prune_heads(self,heads)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel._update_mems(self,hids,mems,mlen,qlen)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.backward_compatible(self)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.get_input_embeddings(self)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.init_mems(self,bsz)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.reset_memory_length(self,mem_len)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModel.set_input_embeddings(self,new_embeddings)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput(ModelOutput)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel(PreTrainedModel)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._get_embedding_shapes(self)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._get_new_num_tokens_layer(self,new_num_tokens,layer)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._init_bias(self,bias)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._init_weight(self,weight)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._init_weights(self,m)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel._resize_token_embeddings(self,new_num_tokens,layer=-1)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLPreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None,layer:Optional[int]=-1)
transformers.models.transfo_xl.modeling_transfo_xl.TransfoXLSequenceClassifierOutputWithPast(ModelOutput)
transformers.models.transfo_xl.modeling_transfo_xl.build_tf_to_pytorch_map(model,config)
transformers.models.transfo_xl.modeling_transfo_xl.load_tf_weights_in_transfo_xl(model,config,tf_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/transfo_xl/tokenization_transfo_xl.py----------------------------------------
A:transformers.models.transfo_xl.tokenization_transfo_xl.logger->utils.logging.get_logger(__name__)
A:transformers.models.transfo_xl.tokenization_transfo_xl.replaced->re.sub(reg, sub, text_array[i]).split()
A:transformers.models.transfo_xl.tokenization_transfo_xl.text->tokenize_numbers(text)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.counter->Counter()
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.punction_without_space_before_pattern->re.compile(f'[^\\s][{self.punctuation_symbols}]')
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.punctuation_with_space_around_pattern->self._compile_space_around_punctuation_pattern()
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.moses_punct_normalizer->sacremoses.MosesPunctNormalizer(language)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.moses_tokenizer->sacremoses.MosesTokenizer(language)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.moses_detokenizer->sacremoses.MosesDetokenizer(language)
A:transformers.models.transfo_xl.tokenization_transfo_xl.vocab_dict->torch.load(pretrained_vocab_file)
A:transformers.models.transfo_xl.tokenization_transfo_xl.symbols->self.moses_pipeline(line)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.sym2idx->OrderedDict()
A:transformers.models.transfo_xl.tokenization_transfo_xl.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['pretrained_vocab_file'])
A:transformers.models.transfo_xl.tokenization_transfo_xl.encoded->torch.cat(encoded)
A:transformers.models.transfo_xl.tokenization_transfo_xl.out_string->self.moses_detokenizer.detokenize(tokens)
A:transformers.models.transfo_xl.tokenization_transfo_xl.line->line.lower().lower()
A:transformers.models.transfo_xl.tokenization_transfo_xl.data->torch.LongTensor(self.bptt, self.bsz)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.data->torch.LongTensor(self.bptt, self.bsz).view(bsz, -1).t().contiguous().to(device)
A:transformers.models.transfo_xl.tokenization_transfo_xl.seq_len->min(bptt, self.data.size(0) - 1 - i)
A:transformers.models.transfo_xl.tokenization_transfo_xl.beg_idx->max(0, i - self.ext_len)
A:transformers.models.transfo_xl.tokenization_transfo_xl.data_out->torch.LongTensor(self.bptt, self.bsz).transpose(0, 1).contiguous().to(self.device)
A:transformers.models.transfo_xl.tokenization_transfo_xl.target_out->torch.LongTensor(self.bptt, self.bsz).transpose(0, 1).contiguous().to(self.device)
A:transformers.models.transfo_xl.tokenization_transfo_xl.bptt->min(max_len, max(min_len, int(np.random.normal(bptt, std))))
A:transformers.models.transfo_xl.tokenization_transfo_xl.(data, target, seq_len)->self.get_batch(i, bptt)
A:transformers.models.transfo_xl.tokenization_transfo_xl.target->torch.LongTensor(self.bptt, self.bsz)
A:transformers.models.transfo_xl.tokenization_transfo_xl.streams[i]->next(sent_stream)
A:transformers.models.transfo_xl.tokenization_transfo_xl.n_new->min(len(streams[i]) - 1, self.bptt - n_filled)
A:transformers.models.transfo_xl.tokenization_transfo_xl.n_retain->min(data.size(0), self.ext_len)
A:transformers.models.transfo_xl.tokenization_transfo_xl.sent_stream->self.get_sent_stream(path)
A:transformers.models.transfo_xl.tokenization_transfo_xl.sents->self.vocab.encode_file(path, add_double_eos=True)
A:transformers.models.transfo_xl.tokenization_transfo_xl.vocab->TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
A:transformers.models.transfo_xl.tokenization_transfo_xl.corpus_file->os.path.join(pretrained_model_name_or_path, CORPUS_NAME)
A:transformers.models.transfo_xl.tokenization_transfo_xl.resolved_corpus_file->cached_path(corpus_file, cache_dir=cache_dir)
A:transformers.models.transfo_xl.tokenization_transfo_xl.corpus->TransfoXLCorpus(datadir, dataset, **kwargs)
A:transformers.models.transfo_xl.tokenization_transfo_xl.corpus_dict->torch.load(resolved_corpus_file)
A:transformers.models.transfo_xl.tokenization_transfo_xl.corpus.train->torch.tensor(corpus.train, dtype=torch.long)
A:transformers.models.transfo_xl.tokenization_transfo_xl.corpus.valid->torch.tensor(corpus.valid, dtype=torch.long)
A:transformers.models.transfo_xl.tokenization_transfo_xl.corpus.test->torch.tensor(corpus.test, dtype=torch.long)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.vocab->TransfoXLTokenizer(*args, **kwargs)
A:transformers.models.transfo_xl.tokenization_transfo_xl.train_path_pattern->os.path.join(path, '1-billion-word-language-modeling-benchmark-r13output', 'training-monolingual.tokenized.shuffled', 'news.en-*')
A:transformers.models.transfo_xl.tokenization_transfo_xl.train_paths->glob.glob(train_path_pattern)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.train->self.vocab.encode_file(os.path.join(path, 'train.txt'), ordered=True, add_eos=False)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.valid->self.vocab.encode_file(os.path.join(path, 'valid.txt'), ordered=False, add_double_eos=True)
A:transformers.models.transfo_xl.tokenization_transfo_xl.self.test->self.vocab.encode_file(os.path.join(path, 'test.txt'), ordered=False, add_double_eos=True)
A:transformers.models.transfo_xl.tokenization_transfo_xl.data_iter->LMShuffledIterator(data, *args, **kwargs)
A:transformers.models.transfo_xl.tokenization_transfo_xl.fn->os.path.join(datadir, 'cache.pt')
A:transformers.models.transfo_xl.tokenization_transfo_xl.fn_pickle->os.path.join(datadir, 'cache.pkl')
A:transformers.models.transfo_xl.tokenization_transfo_xl.kwargs['vocab_file']->os.path.join(datadir, '1b_word_vocab.txt')
transformers.TransfoXLCorpus(self,*args,**kwargs)
transformers.TransfoXLCorpus.build_corpus(self,path,dataset)
transformers.TransfoXLCorpus.from_pretrained(cls,pretrained_model_name_or_path,cache_dir=None,*inputs,**kwargs)
transformers.TransfoXLCorpus.get_iterator(self,split,*args,**kwargs)
transformers.TransfoXLTokenizer(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file:str=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],language='en',**kwargs)
transformers.TransfoXLTokenizer._build_from_file(self,vocab_file)
transformers.TransfoXLTokenizer._compile_space_around_punctuation_pattern(self)
transformers.TransfoXLTokenizer._convert_id_to_token(self,idx)
transformers.TransfoXLTokenizer._convert_token_to_id(self,sym)
transformers.TransfoXLTokenizer._tokenize(self,line,add_eos=False,add_double_eos=False)
transformers.TransfoXLTokenizer.add_special(self,sym)
transformers.TransfoXLTokenizer.add_symbol(self,sym)
transformers.TransfoXLTokenizer.build_vocab(self)
transformers.TransfoXLTokenizer.convert_to_tensor(self,symbols)
transformers.TransfoXLTokenizer.convert_tokens_to_string(self,tokens)
transformers.TransfoXLTokenizer.count_file(self,path,verbose=False,add_eos=False)
transformers.TransfoXLTokenizer.count_sents(self,sents,verbose=False)
transformers.TransfoXLTokenizer.do_lower_case(self)
transformers.TransfoXLTokenizer.encode_file(self,path,ordered=False,verbose=False,add_eos=True,add_double_eos=False)
transformers.TransfoXLTokenizer.encode_sents(self,sents,ordered=False,verbose=False)
transformers.TransfoXLTokenizer.get_vocab(self)
transformers.TransfoXLTokenizer.moses_pipeline(self,text:str)->List[str]
transformers.TransfoXLTokenizer.moses_punct_norm(self,text)
transformers.TransfoXLTokenizer.moses_tokenize(self,text)
transformers.TransfoXLTokenizer.move_added_token(self,token:str,target_idx:int)
transformers.TransfoXLTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.TransfoXLTokenizer.vocab_size(self)
transformers.models.transfo_xl.tokenization_transfo_xl.LMMultiFileIterator(self,paths,vocab,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.models.transfo_xl.tokenization_transfo_xl.LMMultiFileIterator.__init__(self,paths,vocab,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.models.transfo_xl.tokenization_transfo_xl.LMMultiFileIterator.__iter__(self)
transformers.models.transfo_xl.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream(self,path)
transformers.models.transfo_xl.tokenization_transfo_xl.LMOrderedIterator(self,data,bsz,bptt,device='cpu',ext_len=None)
transformers.models.transfo_xl.tokenization_transfo_xl.LMOrderedIterator.__init__(self,data,bsz,bptt,device='cpu',ext_len=None)
transformers.models.transfo_xl.tokenization_transfo_xl.LMOrderedIterator.__iter__(self)
transformers.models.transfo_xl.tokenization_transfo_xl.LMOrderedIterator.get_batch(self,i,bptt=None)
transformers.models.transfo_xl.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter(self,start=0)
transformers.models.transfo_xl.tokenization_transfo_xl.LMOrderedIterator.get_varlen_iter(self,start=0,std=5,min_len=5,max_deviation=3)
transformers.models.transfo_xl.tokenization_transfo_xl.LMShuffledIterator(self,data,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.models.transfo_xl.tokenization_transfo_xl.LMShuffledIterator.__init__(self,data,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.models.transfo_xl.tokenization_transfo_xl.LMShuffledIterator.__iter__(self)
transformers.models.transfo_xl.tokenization_transfo_xl.LMShuffledIterator.get_sent_stream(self)
transformers.models.transfo_xl.tokenization_transfo_xl.LMShuffledIterator.stream_iterator(self,sent_stream)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLCorpus(self,*args,**kwargs)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLCorpus.__init__(self,*args,**kwargs)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLCorpus.build_corpus(self,path,dataset)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLCorpus.from_pretrained(cls,pretrained_model_name_or_path,cache_dir=None,*inputs,**kwargs)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLCorpus.get_iterator(self,split,*args,**kwargs)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file:str=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],language='en',**kwargs)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.__init__(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file:str=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],language='en',**kwargs)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file(self,vocab_file)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer._compile_space_around_punctuation_pattern(self)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer._convert_id_to_token(self,idx)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer._convert_token_to_id(self,sym)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer._tokenize(self,line,add_eos=False,add_double_eos=False)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.add_special(self,sym)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol(self,sym)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab(self)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor(self,symbols)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.count_file(self,path,verbose=False,add_eos=False)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.count_sents(self,sents,verbose=False)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.do_lower_case(self)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.encode_file(self,path,ordered=False,verbose=False,add_eos=True,add_double_eos=False)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.encode_sents(self,sents,ordered=False,verbose=False)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.get_vocab(self)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.moses_pipeline(self,text:str)->List[str]
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.moses_punct_norm(self,text)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.moses_tokenize(self,text)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.move_added_token(self,token:str,target_idx:int)
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.transfo_xl.tokenization_transfo_xl.TransfoXLTokenizer.vocab_size(self)
transformers.models.transfo_xl.tokenization_transfo_xl.detokenize_numbers(text:str)->str
transformers.models.transfo_xl.tokenization_transfo_xl.get_lm_corpus(datadir,dataset)
transformers.models.transfo_xl.tokenization_transfo_xl.tokenize_numbers(text_array:List[str])->List[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ctrl/configuration_ctrl.py----------------------------------------
A:transformers.models.ctrl.configuration_ctrl.logger->utils.logging.get_logger(__name__)
transformers.CTRLConfig(self,vocab_size=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,use_cache=True,**kwargs)
transformers.CTRLConfig.hidden_size(self)
transformers.CTRLConfig.max_position_embeddings(self)
transformers.CTRLConfig.num_attention_heads(self)
transformers.CTRLConfig.num_hidden_layers(self)
transformers.models.ctrl.configuration_ctrl.CTRLConfig(self,vocab_size=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,use_cache=True,**kwargs)
transformers.models.ctrl.configuration_ctrl.CTRLConfig.__init__(self,vocab_size=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,use_cache=True,**kwargs)
transformers.models.ctrl.configuration_ctrl.CTRLConfig.hidden_size(self)
transformers.models.ctrl.configuration_ctrl.CTRLConfig.max_position_embeddings(self)
transformers.models.ctrl.configuration_ctrl.CTRLConfig.num_attention_heads(self)
transformers.models.ctrl.configuration_ctrl.CTRLConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ctrl/modeling_ctrl.py----------------------------------------
A:transformers.models.ctrl.modeling_ctrl.logger->utils.logging.get_logger(__name__)
A:transformers.models.ctrl.modeling_ctrl.angle_rads->angle_defn(torch.arange(position, dtype=dtype).unsqueeze(1), torch.arange(d_model_size, dtype=dtype).unsqueeze(0), d_model_size)
A:transformers.models.ctrl.modeling_ctrl.sines->torch.sin(angle_rads[:, 0::2])
A:transformers.models.ctrl.modeling_ctrl.cosines->torch.cos(angle_rads[:, 1::2])
A:transformers.models.ctrl.modeling_ctrl.pos_encoding->torch.cat([sines, cosines], dim=-1)
A:transformers.models.ctrl.modeling_ctrl.matmul_qk->torch.matmul(q, k.permute(0, 1, 3, 2))
A:transformers.models.ctrl.modeling_ctrl.attention_weights->torch.softmax(scaled_attention_logits, dim=-1)
A:transformers.models.ctrl.modeling_ctrl.output->self.dense(original_size_attention)
A:transformers.models.ctrl.modeling_ctrl.self.depth->int(d_model_size / self.num_heads)
A:transformers.models.ctrl.modeling_ctrl.self.Wq->prune_linear_layer(self.Wq, index)
A:transformers.models.ctrl.modeling_ctrl.self.Wk->prune_linear_layer(self.Wk, index)
A:transformers.models.ctrl.modeling_ctrl.self.Wv->prune_linear_layer(self.Wv, index)
A:transformers.models.ctrl.modeling_ctrl.self.dense->prune_linear_layer(self.dense, index, dim=1)
A:transformers.models.ctrl.modeling_ctrl.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.ctrl.modeling_ctrl.(heads, index)->find_pruneable_heads_and_indices(heads, self.num_heads, attention_head_size, self.pruned_heads)
A:transformers.models.ctrl.modeling_ctrl.x->x.reshape(batch_size, -1, self.num_heads, self.depth).reshape(batch_size, -1, self.num_heads, self.depth)
A:transformers.models.ctrl.modeling_ctrl.q->self.split_into_heads(q, batch_size)
A:transformers.models.ctrl.modeling_ctrl.k->torch.cat((past_key, k), dim=-2)
A:transformers.models.ctrl.modeling_ctrl.v->torch.cat((past_value, v), dim=-2)
A:transformers.models.ctrl.modeling_ctrl.present->torch.stack((k, v))
A:transformers.models.ctrl.modeling_ctrl.scaled_attention->output[0].permute([0, 2, 1, 3])
A:transformers.models.ctrl.modeling_ctrl.original_size_attention->output[0].permute([0, 2, 1, 3]).reshape(batch_size, -1, self.d_model_size)
A:transformers.models.ctrl.modeling_ctrl.self.multi_head_attention->MultiHeadAttention(d_model_size, num_heads)
A:transformers.models.ctrl.modeling_ctrl.self.ffn->point_wise_feed_forward_network(d_model_size, dff)
A:transformers.models.ctrl.modeling_ctrl.self.layernorm1->torch.nn.LayerNorm(d_model_size, eps=1e-06)
A:transformers.models.ctrl.modeling_ctrl.self.layernorm2->torch.nn.LayerNorm(d_model_size, eps=1e-06)
A:transformers.models.ctrl.modeling_ctrl.self.dropout1->torch.nn.Dropout(rate)
A:transformers.models.ctrl.modeling_ctrl.self.dropout2->torch.nn.Dropout(rate)
A:transformers.models.ctrl.modeling_ctrl.normed->self.layernorm1(x)
A:transformers.models.ctrl.modeling_ctrl.attn_outputs->self.multi_head_attention(normed, normed, normed, mask, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.ctrl.modeling_ctrl.attn_output->self.dropout1(attn_output)
A:transformers.models.ctrl.modeling_ctrl.out2->self.layernorm2(out1)
A:transformers.models.ctrl.modeling_ctrl.ffn_output->self.dropout2(ffn_output)
A:transformers.models.ctrl.modeling_ctrl.self.pos_encoding->positional_encoding(config.n_positions, self.d_model_size, torch.float)
A:transformers.models.ctrl.modeling_ctrl.self.w->torch.nn.Embedding(config.vocab_size, config.n_embd)
A:transformers.models.ctrl.modeling_ctrl.self.dropout->torch.nn.Dropout(config.embd_pdrop)
A:transformers.models.ctrl.modeling_ctrl.self.h->torch.nn.ModuleList([EncoderLayer(config.n_embd, config.n_head, config.dff, config.resid_pdrop) for _ in range(config.n_layer)])
A:transformers.models.ctrl.modeling_ctrl.self.layernorm->torch.nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
A:transformers.models.ctrl.modeling_ctrl.input_shape->input_ids[:, -1].unsqueeze(-1).size()
A:transformers.models.ctrl.modeling_ctrl.input_ids->input_ids[:, -1].unsqueeze(-1)
A:transformers.models.ctrl.modeling_ctrl.past_key_values->tuple([None] * len(self.h))
A:transformers.models.ctrl.modeling_ctrl.past_length->past_key_values[0][0].size(-2)
A:transformers.models.ctrl.modeling_ctrl.position_ids->position_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.ctrl.modeling_ctrl.attention_mask->attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.ctrl.modeling_ctrl.head_mask->self.get_head_mask(head_mask, self.config.n_layer)
A:transformers.models.ctrl.modeling_ctrl.token_type_ids->token_type_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.ctrl.modeling_ctrl.token_type_embeds->self.w(token_type_ids)
A:transformers.models.ctrl.modeling_ctrl.inputs_embeds->self.w(input_ids)
A:transformers.models.ctrl.modeling_ctrl.mask->torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(device)
A:transformers.models.ctrl.modeling_ctrl.pos_embeds->self.pos_encoding[position_ids, :].to(device)
A:transformers.models.ctrl.modeling_ctrl.hidden_states->self.layernorm(hidden_states)
A:transformers.models.ctrl.modeling_ctrl.outputs->h(hidden_states, mask, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i], use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.ctrl.modeling_ctrl.self.transformer->CTRLModel(config)
A:transformers.models.ctrl.modeling_ctrl.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=True)
A:transformers.models.ctrl.modeling_ctrl.transformer_outputs->self.transformer(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.ctrl.modeling_ctrl.lm_logits->self.lm_head(hidden_states)
A:transformers.models.ctrl.modeling_ctrl.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.models.ctrl.modeling_ctrl.shift_labels->labels[..., 1:].contiguous()
A:transformers.models.ctrl.modeling_ctrl.loss_fct->CrossEntropyLoss()
A:transformers.models.ctrl.modeling_ctrl.loss->loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.ctrl.modeling_ctrl.self.classifier->torch.nn.Linear(config.n_embd, self.num_labels, bias=False)
A:transformers.models.ctrl.modeling_ctrl.logits->self.classifier(hidden_states)
transformers.CTRLForSequenceClassification(self,config)
transformers.CTRLForSequenceClassification.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CTRLLMHeadModel(self,config)
transformers.CTRLLMHeadModel._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.CTRLLMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CTRLLMHeadModel.get_output_embeddings(self)
transformers.CTRLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,use_cache=None,**kwargs)
transformers.CTRLLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.CTRLModel(self,config)
transformers.CTRLModel._prune_heads(self,heads_to_prune)
transformers.CTRLModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.CTRLModel.get_input_embeddings(self)
transformers.CTRLModel.set_input_embeddings(self,new_embeddings)
transformers.CTRLPreTrainedModel(PreTrainedModel)
transformers.CTRLPreTrainedModel._init_weights(self,module)
transformers.models.ctrl.modeling_ctrl.CTRLForSequenceClassification(self,config)
transformers.models.ctrl.modeling_ctrl.CTRLForSequenceClassification.__init__(self,config)
transformers.models.ctrl.modeling_ctrl.CTRLForSequenceClassification.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel(self,config)
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel.__init__(self,config)
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel.get_output_embeddings(self)
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,use_cache=None,**kwargs)
transformers.models.ctrl.modeling_ctrl.CTRLLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.ctrl.modeling_ctrl.CTRLModel(self,config)
transformers.models.ctrl.modeling_ctrl.CTRLModel.__init__(self,config)
transformers.models.ctrl.modeling_ctrl.CTRLModel._prune_heads(self,heads_to_prune)
transformers.models.ctrl.modeling_ctrl.CTRLModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ctrl.modeling_ctrl.CTRLModel.get_input_embeddings(self)
transformers.models.ctrl.modeling_ctrl.CTRLModel.set_input_embeddings(self,new_embeddings)
transformers.models.ctrl.modeling_ctrl.CTRLPreTrainedModel(PreTrainedModel)
transformers.models.ctrl.modeling_ctrl.CTRLPreTrainedModel._init_weights(self,module)
transformers.models.ctrl.modeling_ctrl.EncoderLayer(self,d_model_size,num_heads,dff,rate=0.1)
transformers.models.ctrl.modeling_ctrl.EncoderLayer.__init__(self,d_model_size,num_heads,dff,rate=0.1)
transformers.models.ctrl.modeling_ctrl.EncoderLayer.forward(self,x,mask,layer_past=None,attention_mask=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.models.ctrl.modeling_ctrl.MultiHeadAttention(self,d_model_size,num_heads)
transformers.models.ctrl.modeling_ctrl.MultiHeadAttention.__init__(self,d_model_size,num_heads)
transformers.models.ctrl.modeling_ctrl.MultiHeadAttention.forward(self,v,k,q,mask,layer_past=None,attention_mask=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.models.ctrl.modeling_ctrl.MultiHeadAttention.prune_heads(self,heads)
transformers.models.ctrl.modeling_ctrl.MultiHeadAttention.split_into_heads(self,x,batch_size)
transformers.models.ctrl.modeling_ctrl.angle_defn(pos,i,d_model_size)
transformers.models.ctrl.modeling_ctrl.point_wise_feed_forward_network(d_model_size,dff)
transformers.models.ctrl.modeling_ctrl.positional_encoding(position,d_model_size,dtype)
transformers.models.ctrl.modeling_ctrl.scaled_dot_product_attention(q,k,v,mask,attention_mask=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ctrl/tokenization_ctrl.py----------------------------------------
A:transformers.models.ctrl.tokenization_ctrl.logger->utils.logging.get_logger(__name__)
A:transformers.models.ctrl.tokenization_ctrl.pairs->get_pairs(word)
A:transformers.models.ctrl.tokenization_ctrl.self.encoder->json.load(vocab_handle)
A:transformers.models.ctrl.tokenization_ctrl.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.ctrl.tokenization_ctrl.word->'@@ '.join(word)
A:transformers.models.ctrl.tokenization_ctrl.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.ctrl.tokenization_ctrl.j->'@@ '.join(word).index(first, i)
A:transformers.models.ctrl.tokenization_ctrl.new_word->tuple(new_word)
A:transformers.models.ctrl.tokenization_ctrl.words->regex.findall('\\S+\\n?', text)
A:transformers.models.ctrl.tokenization_ctrl.out_string->' '.join(tokens).replace('@@ ', '').strip()
A:transformers.models.ctrl.tokenization_ctrl.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.ctrl.tokenization_ctrl.merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
transformers.CTRLTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.CTRLTokenizer._convert_id_to_token(self,index)
transformers.CTRLTokenizer._convert_token_to_id(self,token)
transformers.CTRLTokenizer._tokenize(self,text)
transformers.CTRLTokenizer.bpe(self,token)
transformers.CTRLTokenizer.convert_tokens_to_string(self,tokens)
transformers.CTRLTokenizer.get_vocab(self)
transformers.CTRLTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.CTRLTokenizer.vocab_size(self)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer._convert_id_to_token(self,index)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer._convert_token_to_id(self,token)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer._tokenize(self,text)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer.bpe(self,token)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer.get_vocab(self)
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.ctrl.tokenization_ctrl.CTRLTokenizer.vocab_size(self)
transformers.models.ctrl.tokenization_ctrl.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ctrl/__init__.py----------------------------------------
A:transformers.models.ctrl.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ctrl/modeling_tf_ctrl.py----------------------------------------
A:transformers.models.ctrl.modeling_tf_ctrl.logger->utils.logging.get_logger(__name__)
A:transformers.models.ctrl.modeling_tf_ctrl.angle_rads->angle_defn(np.arange(position)[:, np.newaxis], np.arange(d_model_size)[np.newaxis, :], d_model_size)
A:transformers.models.ctrl.modeling_tf_ctrl.sines->numpy.sin(angle_rads[:, 0::2])
A:transformers.models.ctrl.modeling_tf_ctrl.cosines->numpy.cos(angle_rads[:, 1::2])
A:transformers.models.ctrl.modeling_tf_ctrl.pos_encoding->tensorflow.convert_to_tensor(np.concatenate([sines, cosines], axis=-1))
A:transformers.models.ctrl.modeling_tf_ctrl.matmul_qk->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.models.ctrl.modeling_tf_ctrl.dk->tensorflow.cast(shape_list(k)[-1], dtype=matmul_qk.dtype)
A:transformers.models.ctrl.modeling_tf_ctrl.attention_mask->tensorflow.cast(attention_mask, dtype=scaled_attention_logits.dtype)
A:transformers.models.ctrl.modeling_tf_ctrl.attention_weights->tensorflow.nn.softmax(scaled_attention_logits, axis=-1)
A:transformers.models.ctrl.modeling_tf_ctrl.output->self.dense(original_size_attention)
A:transformers.models.ctrl.modeling_tf_ctrl.self.depth->int(d_model_size / self.num_heads)
A:transformers.models.ctrl.modeling_tf_ctrl.self.Wq->tensorflow.keras.layers.Dense(d_model_size, name='Wq')
A:transformers.models.ctrl.modeling_tf_ctrl.self.Wk->tensorflow.keras.layers.Dense(d_model_size, name='Wk')
A:transformers.models.ctrl.modeling_tf_ctrl.self.Wv->tensorflow.keras.layers.Dense(d_model_size, name='Wv')
A:transformers.models.ctrl.modeling_tf_ctrl.self.dense->tensorflow.keras.layers.Dense(d_model_size, name='dense')
A:transformers.models.ctrl.modeling_tf_ctrl.x->tensorflow.reshape(x, (batch_size, -1, self.num_heads, self.depth))
A:transformers.models.ctrl.modeling_tf_ctrl.q->self.split_into_heads(q, batch_size)
A:transformers.models.ctrl.modeling_tf_ctrl.k->tensorflow.concat((past_key, k), axis=-2)
A:transformers.models.ctrl.modeling_tf_ctrl.v->tensorflow.concat((past_value, v), axis=-2)
A:transformers.models.ctrl.modeling_tf_ctrl.(past_key, past_value)->tensorflow.unstack(layer_past, axis=0)
A:transformers.models.ctrl.modeling_tf_ctrl.present->tensorflow.stack((k, v), axis=0)
A:transformers.models.ctrl.modeling_tf_ctrl.scaled_attention->tensorflow.transpose(output[0], perm=[0, 2, 1, 3])
A:transformers.models.ctrl.modeling_tf_ctrl.original_size_attention->tensorflow.reshape(scaled_attention, (batch_size, -1, self.d_model_size))
A:transformers.models.ctrl.modeling_tf_ctrl.self.dense_0->tensorflow.keras.layers.Dense(dff, activation='relu', name='0')
A:transformers.models.ctrl.modeling_tf_ctrl.self.dense_2->tensorflow.keras.layers.Dense(d_model_size, name='2')
A:transformers.models.ctrl.modeling_tf_ctrl.dense_0_output->self.dense_0(inputs)
A:transformers.models.ctrl.modeling_tf_ctrl.dense_2_output->self.dense_2(dense_0_output)
A:transformers.models.ctrl.modeling_tf_ctrl.self.multi_head_attention->TFMultiHeadAttention(d_model_size, num_heads, output_attentions=self.output_attentions, name='multi_head_attention')
A:transformers.models.ctrl.modeling_tf_ctrl.self.ffn->TFPointWiseFeedForwardLayer(d_model_size, dff, name='ffn')
A:transformers.models.ctrl.modeling_tf_ctrl.self.layernorm1->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layernorm1')
A:transformers.models.ctrl.modeling_tf_ctrl.self.layernorm2->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layernorm2')
A:transformers.models.ctrl.modeling_tf_ctrl.self.dropout1->tensorflow.keras.layers.Dropout(rate)
A:transformers.models.ctrl.modeling_tf_ctrl.self.dropout2->tensorflow.keras.layers.Dropout(rate)
A:transformers.models.ctrl.modeling_tf_ctrl.normed->self.layernorm1(x)
A:transformers.models.ctrl.modeling_tf_ctrl.attn_outputs->self.multi_head_attention(normed, normed, normed, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training)
A:transformers.models.ctrl.modeling_tf_ctrl.attn_output->self.dropout1(attn_output, training=training)
A:transformers.models.ctrl.modeling_tf_ctrl.out2->self.layernorm2(out1)
A:transformers.models.ctrl.modeling_tf_ctrl.ffn_output->self.dropout2(ffn_output, training=training)
A:transformers.models.ctrl.modeling_tf_ctrl.self.pos_encoding->positional_encoding(config.n_positions, self.d_model_size)
A:transformers.models.ctrl.modeling_tf_ctrl.self.w->TFSharedEmbeddings(config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name='w')
A:transformers.models.ctrl.modeling_tf_ctrl.self.dropout->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.models.ctrl.modeling_tf_ctrl.self.layernorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='layernorm')
A:transformers.models.ctrl.modeling_tf_ctrl.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, past=past, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.ctrl.modeling_tf_ctrl.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.ctrl.modeling_tf_ctrl.inputs['input_ids']->tensorflow.reshape(inputs['input_ids'], [-1, input_shape[-1]])
A:transformers.models.ctrl.modeling_tf_ctrl.inputs['position_ids']->tensorflow.reshape(inputs['position_ids'], [-1, shape_list(inputs['position_ids'])[-1]])
A:transformers.models.ctrl.modeling_tf_ctrl.inputs['attention_mask']->tensorflow.multiply(tf.subtract(one_cst, inputs['attention_mask']), ten_thousand_cst)
A:transformers.models.ctrl.modeling_tf_ctrl.one_cst->tensorflow.constant(1.0)
A:transformers.models.ctrl.modeling_tf_ctrl.ten_thousand_cst->tensorflow.constant(-10000.0)
A:transformers.models.ctrl.modeling_tf_ctrl.inputs['token_type_ids']->tensorflow.reshape(inputs['token_type_ids'], [-1, shape_list(inputs['token_type_ids'])[-1]])
A:transformers.models.ctrl.modeling_tf_ctrl.token_type_embeds->tensorflow.constant(0.0)
A:transformers.models.ctrl.modeling_tf_ctrl.inputs['inputs_embeds']->self.w(inputs['input_ids'], mode='embedding')
A:transformers.models.ctrl.modeling_tf_ctrl.pos_embeds->tensorflow.cast(pos_embeds, dtype=token_type_embeds.dtype)
A:transformers.models.ctrl.modeling_tf_ctrl.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.models.ctrl.modeling_tf_ctrl.outputs->self.transformer(input_ids=inputs['input_ids'], past=inputs['past'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.ctrl.modeling_tf_ctrl.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.models.ctrl.modeling_tf_ctrl.self.transformer->TFCTRLMainLayer(config, name='transformer')
A:transformers.models.ctrl.modeling_tf_ctrl.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.ctrl.modeling_tf_ctrl.self.lm_head->TFCTRLLMHead(config, self.transformer.w, name='lm_head')
A:transformers.models.ctrl.modeling_tf_ctrl.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], past=inputs['past'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.ctrl.modeling_tf_ctrl.logits->self.classifier(hidden_states)
A:transformers.models.ctrl.modeling_tf_ctrl.loss->self.compute_loss(tf.reshape(inputs['labels'], [-1, 1]), tf.reshape(in_logits, [-1, self.num_labels]))
A:transformers.models.ctrl.modeling_tf_ctrl.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier', use_bias=False)
A:transformers.models.ctrl.modeling_tf_ctrl.in_logits->tensorflow.gather(logits, sequence_lengths, batch_dims=1, axis=1)
transformers.TFCTRLForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFCTRLForSequenceClassification.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFCTRLForSequenceClassification.get_output_embeddings(self)
transformers.TFCTRLForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFCTRLLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFCTRLLMHeadModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFCTRLLMHeadModel.get_lm_head(self)
transformers.TFCTRLLMHeadModel.get_prefix_bias_name(self)
transformers.TFCTRLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.TFCTRLLMHeadModel.serving_output(self,output)
transformers.TFCTRLModel(self,config,*inputs,**kwargs)
transformers.TFCTRLModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFCTRLModel.serving_output(self,output)
transformers.TFCTRLPreTrainedModel(TFPreTrainedModel)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLForSequenceClassification.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLForSequenceClassification.get_output_embeddings(self)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead(self,config,input_embeddings,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.build(self,input_shape)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.call(self,hidden_states)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.get_bias(self)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.get_output_embeddings(self)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.set_bias(self,value)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHead.set_output_embeddings(self,value)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel(self,config,*inputs,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel.get_lm_head(self)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel.get_prefix_bias_name(self)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLLMHeadModel.serving_output(self,output)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLMainLayer(self,config,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLMainLayer.__init__(self,config,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLMainLayer._prune_heads(self,heads_to_prune)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLMainLayer.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLMainLayer.get_input_embeddings(self)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLMainLayer.set_input_embeddings(self,value)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLModel(self,config,*inputs,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLModel.__init__(self,config,*inputs,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLModel.serving_output(self,output)
transformers.models.ctrl.modeling_tf_ctrl.TFCTRLPreTrainedModel(TFPreTrainedModel)
transformers.models.ctrl.modeling_tf_ctrl.TFEncoderLayer(self,d_model_size,num_heads,dff,rate=0.1,layer_norm_epsilon=1e-06,output_attentions=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFEncoderLayer.__init__(self,d_model_size,num_heads,dff,rate=0.1,layer_norm_epsilon=1e-06,output_attentions=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFEncoderLayer.call(self,x,mask,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.models.ctrl.modeling_tf_ctrl.TFMultiHeadAttention(self,d_model_size,num_heads,output_attentions=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFMultiHeadAttention.__init__(self,d_model_size,num_heads,output_attentions=False,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFMultiHeadAttention.call(self,v,k,q,mask,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.models.ctrl.modeling_tf_ctrl.TFMultiHeadAttention.split_into_heads(self,x,batch_size)
transformers.models.ctrl.modeling_tf_ctrl.TFPointWiseFeedForwardLayer(self,d_model_size,dff,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFPointWiseFeedForwardLayer.__init__(self,d_model_size,dff,**kwargs)
transformers.models.ctrl.modeling_tf_ctrl.TFPointWiseFeedForwardLayer.call(self,inputs,trainable=False)
transformers.models.ctrl.modeling_tf_ctrl.angle_defn(pos,i,d_model_size)
transformers.models.ctrl.modeling_tf_ctrl.positional_encoding(position,d_model_size)
transformers.models.ctrl.modeling_tf_ctrl.scaled_dot_product_attention(q,k,v,mask,attention_mask=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_roberta/tokenization_xlm_roberta.py----------------------------------------
A:transformers.models.xlm_roberta.tokenization_xlm_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlm_roberta.tokenization_xlm_roberta.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.xlm_roberta.tokenization_xlm_roberta.state->self.__dict__.copy()
A:transformers.models.xlm_roberta.tokenization_xlm_roberta.spm_id->self.sp_model.PieceToId(token)
A:transformers.models.xlm_roberta.tokenization_xlm_roberta.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.xlm_roberta.tokenization_xlm_roberta.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.XLMRobertaTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.XLMRobertaTokenizer.__getstate__(self)
transformers.XLMRobertaTokenizer.__setstate__(self,d)
transformers.XLMRobertaTokenizer._convert_id_to_token(self,index)
transformers.XLMRobertaTokenizer._convert_token_to_id(self,token)
transformers.XLMRobertaTokenizer._tokenize(self,text:str)->List[str]
transformers.XLMRobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMRobertaTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLMRobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMRobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLMRobertaTokenizer.get_vocab(self)
transformers.XLMRobertaTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.XLMRobertaTokenizer.vocab_size(self)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.__getstate__(self)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.__setstate__(self,d)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer._convert_id_to_token(self,index)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer._convert_token_to_id(self,token)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer._tokenize(self,text:str)->List[str]
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.get_vocab(self)
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.xlm_roberta.tokenization_xlm_roberta.XLMRobertaTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_roberta/modeling_tf_xlm_roberta.py----------------------------------------
A:transformers.models.xlm_roberta.modeling_tf_xlm_roberta.logger->utils.logging.get_logger(__name__)
transformers.TFXLMRobertaForMaskedLM(TFRobertaForMaskedLM)
transformers.TFXLMRobertaForMultipleChoice(TFRobertaForMultipleChoice)
transformers.TFXLMRobertaForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.TFXLMRobertaForSequenceClassification(TFRobertaForSequenceClassification)
transformers.TFXLMRobertaForTokenClassification(TFRobertaForTokenClassification)
transformers.TFXLMRobertaModel(TFRobertaModel)
transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaForMaskedLM(TFRobertaForMaskedLM)
transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaForMultipleChoice(TFRobertaForMultipleChoice)
transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaForSequenceClassification(TFRobertaForSequenceClassification)
transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaForTokenClassification(TFRobertaForTokenClassification)
transformers.models.xlm_roberta.modeling_tf_xlm_roberta.TFXLMRobertaModel(TFRobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_roberta/modeling_xlm_roberta.py----------------------------------------
A:transformers.models.xlm_roberta.modeling_xlm_roberta.logger->utils.logging.get_logger(__name__)
transformers.XLMRobertaForCausalLM(RobertaForCausalLM)
transformers.XLMRobertaForMaskedLM(RobertaForMaskedLM)
transformers.XLMRobertaForMultipleChoice(RobertaForMultipleChoice)
transformers.XLMRobertaForQuestionAnswering(RobertaForQuestionAnswering)
transformers.XLMRobertaForSequenceClassification(RobertaForSequenceClassification)
transformers.XLMRobertaForTokenClassification(RobertaForTokenClassification)
transformers.XLMRobertaModel(RobertaModel)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForCausalLM(RobertaForCausalLM)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForMaskedLM(RobertaForMaskedLM)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForMultipleChoice(RobertaForMultipleChoice)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForQuestionAnswering(RobertaForQuestionAnswering)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification(RobertaForSequenceClassification)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForTokenClassification(RobertaForTokenClassification)
transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaModel(RobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_roberta/configuration_xlm_roberta.py----------------------------------------
A:transformers.models.xlm_roberta.configuration_xlm_roberta.logger->utils.logging.get_logger(__name__)
transformers.XLMRobertaConfig(RobertaConfig)
transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig(RobertaConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_roberta/tokenization_xlm_roberta_fast.py----------------------------------------
A:transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.XLMRobertaTokenizerFast(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.XLMRobertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMRobertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMRobertaTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_roberta/__init__.py----------------------------------------
A:transformers.models.xlm_roberta.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch.config->transformers.GPT2Config.from_json_file(gpt2_config_file)
A:transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch.model->GPT2Model(config)
A:transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.gpt2.convert_gpt2_original_tf_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch(gpt2_checkpoint_path,gpt2_config_file,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/configuration_gpt2.py----------------------------------------
A:transformers.models.gpt2.configuration_gpt2.logger->utils.logging.get_logger(__name__)
transformers.GPT2Config(self,vocab_size=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,n_inner=None,activation_function='gelu_new',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,scale_attn_weights=True,gradient_checkpointing=False,use_cache=True,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.GPT2Config.hidden_size(self)
transformers.GPT2Config.max_position_embeddings(self)
transformers.GPT2Config.num_attention_heads(self)
transformers.GPT2Config.num_hidden_layers(self)
transformers.models.gpt2.configuration_gpt2.GPT2Config(self,vocab_size=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,n_inner=None,activation_function='gelu_new',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,scale_attn_weights=True,gradient_checkpointing=False,use_cache=True,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.models.gpt2.configuration_gpt2.GPT2Config.__init__(self,vocab_size=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,n_inner=None,activation_function='gelu_new',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,scale_attn_weights=True,gradient_checkpointing=False,use_cache=True,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.models.gpt2.configuration_gpt2.GPT2Config.hidden_size(self)
transformers.models.gpt2.configuration_gpt2.GPT2Config.max_position_embeddings(self)
transformers.models.gpt2.configuration_gpt2.GPT2Config.num_attention_heads(self)
transformers.models.gpt2.configuration_gpt2.GPT2Config.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/tokenization_gpt2.py----------------------------------------
A:transformers.models.gpt2.tokenization_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt2.tokenization_gpt2.pairs->get_pairs(word)
A:transformers.models.gpt2.tokenization_gpt2.self.encoder->json.load(vocab_handle)
A:transformers.models.gpt2.tokenization_gpt2.self.byte_encoder->bytes_to_unicode()
A:transformers.models.gpt2.tokenization_gpt2.self.bpe_ranks->dict(zip(bpe_merges, range(len(bpe_merges))))
A:transformers.models.gpt2.tokenization_gpt2.self.pat->regex.compile("'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+")
A:transformers.models.gpt2.tokenization_gpt2.word->' '.join(word)
A:transformers.models.gpt2.tokenization_gpt2.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.gpt2.tokenization_gpt2.j->' '.join(word).index(first, i)
A:transformers.models.gpt2.tokenization_gpt2.new_word->tuple(new_word)
A:transformers.models.gpt2.tokenization_gpt2.token->''.join((self.byte_encoder[b] for b in token.encode('utf-8')))
A:transformers.models.gpt2.tokenization_gpt2.text->bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)
A:transformers.models.gpt2.tokenization_gpt2.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.gpt2.tokenization_gpt2.merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
A:transformers.models.gpt2.tokenization_gpt2.add_prefix_space->kwargs.pop('add_prefix_space', self.add_prefix_space)
transformers.GPT2Tokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.GPT2Tokenizer._build_conversation_input_ids(self,conversation:'Conversation')->List[int]
transformers.GPT2Tokenizer._convert_id_to_token(self,index)
transformers.GPT2Tokenizer._convert_token_to_id(self,token)
transformers.GPT2Tokenizer._tokenize(self,text)
transformers.GPT2Tokenizer.bpe(self,token)
transformers.GPT2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.GPT2Tokenizer.get_vocab(self)
transformers.GPT2Tokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.GPT2Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.GPT2Tokenizer.vocab_size(self)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.__init__(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._build_conversation_input_ids(self,conversation:'Conversation')->List[int]
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token(self,index)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id(self,token)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize(self,text)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe(self,token)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_vocab(self)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.vocab_size(self)
transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode()
transformers.models.gpt2.tokenization_gpt2.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/tokenization_gpt2_fast.py----------------------------------------
A:transformers.models.gpt2.tokenization_gpt2_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt2.tokenization_gpt2_fast.pre_tok_state->json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())
A:transformers.models.gpt2.tokenization_gpt2_fast.pre_tok_class->getattr(pre_tokenizers, pre_tok_state.pop('type'))
A:transformers.models.gpt2.tokenization_gpt2_fast.self.backend_tokenizer.pre_tokenizer->pre_tok_class(**pre_tok_state)
A:transformers.models.gpt2.tokenization_gpt2_fast.is_split_into_words->kwargs.get('is_split_into_words', False)
A:transformers.models.gpt2.tokenization_gpt2_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
transformers.GPT2TokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.GPT2TokenizerFast._batch_encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.GPT2TokenizerFast._build_conversation_input_ids(self,conversation:'Conversation')->List[int]
transformers.GPT2TokenizerFast._encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.GPT2TokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast.__init__(self,vocab_file,merges_file,tokenizer_file=None,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast._batch_encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast._build_conversation_input_ids(self,conversation:'Conversation')->List[int]
transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast._encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/modeling_tf_gpt2.py----------------------------------------
A:transformers.models.gpt2.modeling_tf_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt2.modeling_tf_gpt2.self.c_attn->TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name='c_attn')
A:transformers.models.gpt2.modeling_tf_gpt2.self.c_proj->TFConv1D(nx, n_state, initializer_range=config.initializer_range, name='c_proj')
A:transformers.models.gpt2.modeling_tf_gpt2.self.attn_dropout->tensorflow.keras.layers.Dropout(config.attn_pdrop)
A:transformers.models.gpt2.modeling_tf_gpt2.self.resid_dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.models.gpt2.modeling_tf_gpt2.self.pruned_heads->set()
A:transformers.models.gpt2.modeling_tf_gpt2.j->tensorflow.range(ns)
A:transformers.models.gpt2.modeling_tf_gpt2.w->self.attn_dropout(w, training=training)
A:transformers.models.gpt2.modeling_tf_gpt2.dk->tensorflow.cast(shape_list(k)[-1], dtype=w.dtype)
A:transformers.models.gpt2.modeling_tf_gpt2.(_, _, nd, ns)->shape_list(w)
A:transformers.models.gpt2.modeling_tf_gpt2.b->tensorflow.reshape(b, [1, 1, nd, ns])
A:transformers.models.gpt2.modeling_tf_gpt2.attention_mask->tensorflow.cast(attention_mask, dtype=w.dtype)
A:transformers.models.gpt2.modeling_tf_gpt2.x->self.c_attn(x)
A:transformers.models.gpt2.modeling_tf_gpt2.x_shape->shape_list(x)
A:transformers.models.gpt2.modeling_tf_gpt2.(query, key, value)->tensorflow.split(x, 3, axis=2)
A:transformers.models.gpt2.modeling_tf_gpt2.query->self.split_heads(query)
A:transformers.models.gpt2.modeling_tf_gpt2.key->tensorflow.concat([past_key, key], axis=-2)
A:transformers.models.gpt2.modeling_tf_gpt2.value->tensorflow.concat([past_value, value], axis=-2)
A:transformers.models.gpt2.modeling_tf_gpt2.(past_key, past_value)->tensorflow.unstack(layer_past, axis=0)
A:transformers.models.gpt2.modeling_tf_gpt2.present->tensorflow.stack([key, value], axis=0)
A:transformers.models.gpt2.modeling_tf_gpt2.attn_outputs->self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.gpt2.modeling_tf_gpt2.a->self.ln_1(x)
A:transformers.models.gpt2.modeling_tf_gpt2.self.c_fc->TFConv1D(n_state, nx, initializer_range=config.initializer_range, name='c_fc')
A:transformers.models.gpt2.modeling_tf_gpt2.self.act->get_tf_activation('gelu')
A:transformers.models.gpt2.modeling_tf_gpt2.self.dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.models.gpt2.modeling_tf_gpt2.h->self.act(self.c_fc(x))
A:transformers.models.gpt2.modeling_tf_gpt2.h2->self.dropout(h2, training=training)
A:transformers.models.gpt2.modeling_tf_gpt2.self.ln_1->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_1')
A:transformers.models.gpt2.modeling_tf_gpt2.self.attn->TFAttention(nx, n_ctx, config, scale, name='attn')
A:transformers.models.gpt2.modeling_tf_gpt2.self.ln_2->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_2')
A:transformers.models.gpt2.modeling_tf_gpt2.self.mlp->TFMLP(inner_dim, config, name='mlp')
A:transformers.models.gpt2.modeling_tf_gpt2.output_attn->self.attn(a, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training)
A:transformers.models.gpt2.modeling_tf_gpt2.m->self.mlp(m, training=training)
A:transformers.models.gpt2.modeling_tf_gpt2.self.wte->TFSharedEmbeddings(config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name='wte')
A:transformers.models.gpt2.modeling_tf_gpt2.self.drop->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.models.gpt2.modeling_tf_gpt2.self.ln_f->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_f')
A:transformers.models.gpt2.modeling_tf_gpt2.self.wpe->self.add_weight(name='embeddings', shape=[self.n_positions, self.n_embd], initializer=get_initializer(self.initializer_range))
A:transformers.models.gpt2.modeling_tf_gpt2.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, past=past, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.gpt2.modeling_tf_gpt2.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.gpt2.modeling_tf_gpt2.inputs['input_ids']->tensorflow.reshape(inputs['input_ids'], [-1, input_shape[-1]])
A:transformers.models.gpt2.modeling_tf_gpt2.inputs['position_ids']->tensorflow.reshape(inputs['position_ids'], [-1, shape_list(inputs['position_ids'])[-1]])
A:transformers.models.gpt2.modeling_tf_gpt2.attention_mask_shape->shape_list(inputs['attention_mask'])
A:transformers.models.gpt2.modeling_tf_gpt2.inputs['attention_mask']->tensorflow.multiply(tf.subtract(one_cst, inputs['attention_mask']), tf.constant(-10000.0))
A:transformers.models.gpt2.modeling_tf_gpt2.one_cst->tensorflow.constant(1.0)
A:transformers.models.gpt2.modeling_tf_gpt2.inputs['inputs_embeds']->self.wte(inputs['input_ids'], mode='embedding')
A:transformers.models.gpt2.modeling_tf_gpt2.position_embeds->tensorflow.cast(position_embeds, dtype=inputs['inputs_embeds'].dtype)
A:transformers.models.gpt2.modeling_tf_gpt2.inputs['token_type_ids']->tensorflow.reshape(inputs['token_type_ids'], [-1, shape_list(inputs['token_type_ids'])[-1]])
A:transformers.models.gpt2.modeling_tf_gpt2.token_type_embeds->tensorflow.cast(token_type_embeds, dtype=inputs['inputs_embeds'].dtype)
A:transformers.models.gpt2.modeling_tf_gpt2.hidden_states->tensorflow.reshape(hidden_states, input_shapes + shape_list(hidden_states)[-1:])
A:transformers.models.gpt2.modeling_tf_gpt2.outputs->self.transformer(input_ids=inputs['input_ids'], past=inputs['past'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.gpt2.modeling_tf_gpt2.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.models.gpt2.modeling_tf_gpt2.output->self.call(inputs)
A:transformers.models.gpt2.modeling_tf_gpt2.self.transformer->TFGPT2MainLayer(config, name='transformer')
A:transformers.models.gpt2.modeling_tf_gpt2.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], past=inputs['past'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.gpt2.modeling_tf_gpt2.logits->self.score(hidden_states)
A:transformers.models.gpt2.modeling_tf_gpt2.loss->self.compute_loss(tf.reshape(inputs['labels'], [-1]), tf.reshape(in_logits, [-1, self.num_labels]))
A:transformers.models.gpt2.modeling_tf_gpt2.self.multiple_choice_head->TFSequenceSummary(config, initializer_range=config.initializer_range, name='multiple_choice_head')
A:transformers.models.gpt2.modeling_tf_gpt2.input_shapes->shape_list(inputs['input_ids'])
A:transformers.models.gpt2.modeling_tf_gpt2.lm_logits->self.transformer.wte(hidden_states, mode='linear')
A:transformers.models.gpt2.modeling_tf_gpt2.mc_logits->tensorflow.squeeze(mc_logits, axis=-1)
A:transformers.models.gpt2.modeling_tf_gpt2.self.score->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='score', use_bias=False)
A:transformers.models.gpt2.modeling_tf_gpt2.logits_shape->shape_list(logits)
A:transformers.models.gpt2.modeling_tf_gpt2.in_logits->tensorflow.gather(logits, sequence_lengths, batch_dims=1, axis=1)
transformers.TFGPT2DoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.TFGPT2DoubleHeadsModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFGPT2DoubleHeadsModel.serving(self,inputs)
transformers.TFGPT2DoubleHeadsModel.serving_output(self,output)
transformers.TFGPT2DoubleHeadsModelOutput(ModelOutput)
transformers.TFGPT2ForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFGPT2ForSequenceClassification.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFGPT2ForSequenceClassification.serving_output(self,output)
transformers.TFGPT2LMHeadModel(self,config,*inputs,**kwargs)
transformers.TFGPT2LMHeadModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFGPT2LMHeadModel.get_output_embeddings(self)
transformers.TFGPT2LMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.TFGPT2LMHeadModel.serving_output(self,output)
transformers.TFGPT2LMHeadModel.set_output_embeddings(self,value)
transformers.TFGPT2MainLayer(self,config,*inputs,**kwargs)
transformers.TFGPT2MainLayer._prune_heads(self,heads_to_prune)
transformers.TFGPT2MainLayer.build(self,input_shape)
transformers.TFGPT2MainLayer.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFGPT2MainLayer.get_input_embeddings(self)
transformers.TFGPT2MainLayer.set_input_embeddings(self,value)
transformers.TFGPT2Model(self,config,*inputs,**kwargs)
transformers.TFGPT2Model.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFGPT2Model.serving_output(self,output)
transformers.TFGPT2PreTrainedModel(TFPreTrainedModel)
transformers.TFGPT2PreTrainedModel.serving(self,inputs)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention.__init__(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention._attn(self,q,k,v,attention_mask,head_mask,output_attentions,training=False)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention.call(self,x,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention.causal_attention_mask(nd,ns,dtype)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention.merge_heads(self,x)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention.prune_heads(self,heads)
transformers.models.gpt2.modeling_tf_gpt2.TFAttention.split_heads(self,x)
transformers.models.gpt2.modeling_tf_gpt2.TFBlock(self,n_ctx,config,scale=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFBlock.__init__(self,n_ctx,config,scale=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFBlock.call(self,x,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.__init__(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.serving(self,inputs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.serving_output(self,output)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput(ModelOutput)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2ForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2ForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2ForSequenceClassification.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2ForSequenceClassification.serving_output(self,output)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.get_output_embeddings(self)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.serving_output(self,output)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.set_output_embeddings(self,value)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer.__init__(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer._prune_heads(self,heads_to_prune)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer.build(self,input_shape)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer.get_input_embeddings(self)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer.set_input_embeddings(self,value)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model.__init__(self,config,*inputs,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model.call(self,input_ids=None,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model.serving_output(self,output)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2PreTrainedModel(TFPreTrainedModel)
transformers.models.gpt2.modeling_tf_gpt2.TFGPT2PreTrainedModel.serving(self,inputs)
transformers.models.gpt2.modeling_tf_gpt2.TFMLP(self,n_state,config,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFMLP.__init__(self,n_state,config,**kwargs)
transformers.models.gpt2.modeling_tf_gpt2.TFMLP.call(self,x,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/__init__.py----------------------------------------
A:transformers.models.gpt2.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/modeling_flax_gpt2.py----------------------------------------
A:transformers.models.gpt2.modeling_flax_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt2.modeling_flax_gpt2.inputs->jax.numpy.asarray(inputs, self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.kernel->jax.numpy.asarray(kernel.transpose(), self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.y->jax.lax.dot_general(inputs, kernel, (((inputs.ndim - 1,), (0,)), ((), ())), precision=self.precision)
A:transformers.models.gpt2.modeling_flax_gpt2.bias->jax.numpy.asarray(bias, self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.c_attn->FlaxConv1D(features=3 * self.embed_dim, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.c_proj->FlaxConv1D(embed_dim, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.resid_dropout->flax.linen.Dropout(rate=config.resid_pdrop)
A:transformers.models.gpt2.modeling_flax_gpt2.self.causal_mask->make_causal_mask(jnp.ones((1, config.max_position_embeddings), dtype='bool'), dtype='bool')
A:transformers.models.gpt2.modeling_flax_gpt2.is_initialized->self.has_variable('cache', 'cached_key')
A:transformers.models.gpt2.modeling_flax_gpt2.cached_key->self.variable('cache', 'cached_key', jnp.zeros, key.shape, key.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.cached_value->self.variable('cache', 'cached_value', jnp.zeros, value.shape, value.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.cache_index->self.variable('cache', 'cache_index', lambda : jnp.array(0, dtype=jnp.int32))
A:transformers.models.gpt2.modeling_flax_gpt2.key->self._split_heads(key)
A:transformers.models.gpt2.modeling_flax_gpt2.value->self._split_heads(value)
A:transformers.models.gpt2.modeling_flax_gpt2.pad_mask->jax.numpy.broadcast_to(jnp.arange(max_length) < cur_index + num_updated_cache_vectors, tuple(batch_dims) + (1, num_updated_cache_vectors, max_length))
A:transformers.models.gpt2.modeling_flax_gpt2.attention_mask->jax.numpy.ones((batch_size, sequence_length))
A:transformers.models.gpt2.modeling_flax_gpt2.qkv_out->self.c_attn(hidden_states)
A:transformers.models.gpt2.modeling_flax_gpt2.(query, key, value)->jax.numpy.split(qkv_out, 3, axis=2)
A:transformers.models.gpt2.modeling_flax_gpt2.query->self._split_heads(query)
A:transformers.models.gpt2.modeling_flax_gpt2.causal_mask->jax.numpy.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])
A:transformers.models.gpt2.modeling_flax_gpt2.dropout_rng->self.make_rng('dropout')
A:transformers.models.gpt2.modeling_flax_gpt2.(key, value, attention_mask)->self._concatenate_to_cache(key, value, query, attention_mask)
A:transformers.models.gpt2.modeling_flax_gpt2.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, -10000.0).astype(self.dtype))
A:transformers.models.gpt2.modeling_flax_gpt2.attn_weights->dot_product_attention_weights(query, key, bias=attention_bias, dropout_rng=dropout_rng, dropout_rate=self.config.attn_pdrop, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.gpt2.modeling_flax_gpt2.attn_output->self.resid_dropout(attn_output, deterministic=deterministic)
A:transformers.models.gpt2.modeling_flax_gpt2.self.c_fc->FlaxConv1D(self.intermediate_size, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.dropout->flax.linen.Dropout(rate=self.config.embd_pdrop)
A:transformers.models.gpt2.modeling_flax_gpt2.hidden_states->self.ln_f(hidden_states)
A:transformers.models.gpt2.modeling_flax_gpt2.self.ln_1->flax.linen.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.attn->FlaxGPT2Attention(self.config, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.ln_2->flax.linen.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.mlp->FlaxGPT2MLP(self.config, inner_dim, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.outputs->self.transformer(input_ids, attention_mask, position_ids, deterministic=deterministic, init_cache=init_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.gpt2.modeling_flax_gpt2.feed_forward_hidden_states->self.mlp(hidden_states, deterministic=deterministic)
A:transformers.models.gpt2.modeling_flax_gpt2.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.gpt2.modeling_flax_gpt2.input_ids->jax.numpy.ones((batch_size, max_length))
A:transformers.models.gpt2.modeling_flax_gpt2.position_ids->jax.numpy.broadcast_to(jnp.arange(seq_length, dtype='i4')[None, :], (batch_size, seq_length))
A:transformers.models.gpt2.modeling_flax_gpt2.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.gpt2.modeling_flax_gpt2.init_variables->self.module.init(jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True)
A:transformers.models.gpt2.modeling_flax_gpt2.outputs['past_key_values']->unfreeze(past_key_values['cache'])
A:transformers.models.gpt2.modeling_flax_gpt2.layer_outputs->block(hidden_states, attention_mask, deterministic=deterministic, init_cache=init_cache, output_attentions=output_attentions)
A:transformers.models.gpt2.modeling_flax_gpt2.self.wte->flax.linen.Embed(self.config.vocab_size, self.embed_dim, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.wpe->flax.linen.Embed(self.config.max_position_embeddings, self.embed_dim, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.h->FlaxGPT2BlockCollection(self.config, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.ln_f->flax.linen.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.input_embeds->self.wte(input_ids.astype('i4'))
A:transformers.models.gpt2.modeling_flax_gpt2.position_embeds->self.wpe(position_ids.astype('i4'))
A:transformers.models.gpt2.modeling_flax_gpt2.self.transformer->FlaxGPT2Module(self.config, dtype=self.dtype)
A:transformers.models.gpt2.modeling_flax_gpt2.self.lm_head->flax.linen.Dense(self.config.vocab_size, use_bias=False, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range, dtype=self.dtype))
A:transformers.models.gpt2.modeling_flax_gpt2.lm_logits->self.lm_head(hidden_states)
A:transformers.models.gpt2.modeling_flax_gpt2.past_key_values->self.init_cache(batch_size, max_length)
A:transformers.models.gpt2.modeling_flax_gpt2.extended_attention_mask->jax.lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))
transformers.FlaxGPT2LMHeadModel(FlaxGPT2PreTrainedModel)
transformers.FlaxGPT2LMHeadModel.prepare_inputs_for_generation(self,input_ids,max_length,attention_mask:Optional[jnp.DeviceArray]=None)
transformers.FlaxGPT2LMHeadModel.update_inputs_for_generation(self,model_outputs,model_kwargs)
transformers.FlaxGPT2Model(FlaxGPT2PreTrainedModel)
transformers.FlaxGPT2PreTrainedModel(self,config:GPT2Config,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxGPT2PreTrainedModel.init_cache(self,batch_size,max_length)
transformers.FlaxGPT2PreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.gpt2.modeling_flax_gpt2.FlaxConv1D(self,inputs)
transformers.models.gpt2.modeling_flax_gpt2.FlaxConv1D.__call__(self,inputs)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Attention(self,hidden_states,attention_mask=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Attention.__call__(self,hidden_states,attention_mask=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Attention._concatenate_to_cache(self,key,value,query,attention_mask)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Attention._merge_heads(self,hidden_states)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Attention._split_heads(self,hidden_states)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Attention.setup(self)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Block(self,hidden_states,attention_mask=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Block.__call__(self,hidden_states,attention_mask=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Block.setup(self)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2BlockCollection(self,hidden_states,attention_mask=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2BlockCollection.__call__(self,hidden_states,attention_mask=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2BlockCollection.setup(self)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModel(FlaxGPT2PreTrainedModel)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModel.prepare_inputs_for_generation(self,input_ids,max_length,attention_mask:Optional[jnp.DeviceArray]=None)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModel.update_inputs_for_generation(self,model_outputs,model_kwargs)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModule(self,input_ids,attention_mask,position_ids,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModule.__call__(self,input_ids,attention_mask,position_ids,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModule.setup(self)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2MLP(self,hidden_states,deterministic:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2MLP.__call__(self,hidden_states,deterministic:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2MLP.setup(self)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Model(FlaxGPT2PreTrainedModel)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Module(self,input_ids,attention_mask,position_ids,deterministic=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Module.__call__(self,input_ids,attention_mask,position_ids,deterministic=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2Module.setup(self)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2PreTrainedModel(self,config:GPT2Config,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2PreTrainedModel.__init__(self,config:GPT2Config,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2PreTrainedModel.init_cache(self,batch_size,max_length)
transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2PreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt2/modeling_gpt2.py----------------------------------------
A:transformers.models.gpt2.modeling_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt2.modeling_gpt2.tf_path->os.path.abspath(gpt2_checkpoint_path)
A:transformers.models.gpt2.modeling_gpt2.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.gpt2.modeling_gpt2.array->tensorflow.train.load_variable(tf_path, name)
A:transformers.models.gpt2.modeling_gpt2.name->name.split('/').split('/')
A:transformers.models.gpt2.modeling_gpt2.scope_names->re.split('(\\d+)', m_name)
A:transformers.models.gpt2.modeling_gpt2.pointer->getattr(pointer, scope_names[0])
A:transformers.models.gpt2.modeling_gpt2.num->int(scope_names[1])
A:transformers.models.gpt2.modeling_gpt2.pointer.data->torch.from_numpy(array)
A:transformers.models.gpt2.modeling_gpt2.self.c_attn->prune_conv1d_layer(self.c_attn, index_attn, dim=1)
A:transformers.models.gpt2.modeling_gpt2.self.q_attn->Conv1D(self.embed_dim, self.embed_dim)
A:transformers.models.gpt2.modeling_gpt2.self.c_proj->Conv1D(embed_dim, intermediate_size)
A:transformers.models.gpt2.modeling_gpt2.self.attn_dropout->torch.nn.Dropout(config.attn_pdrop)
A:transformers.models.gpt2.modeling_gpt2.self.resid_dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.models.gpt2.modeling_gpt2.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.gpt2.modeling_gpt2.(heads, index)->find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)
A:transformers.models.gpt2.modeling_gpt2.index_attn->torch.cat([index, index + self.split_size, index + 2 * self.split_size])
A:transformers.models.gpt2.modeling_gpt2.attn_weights->self.attn_dropout(attn_weights)
A:transformers.models.gpt2.modeling_gpt2.causal_mask->self.bias[:, :, key_length - query_length:key_length, :key_length].bool()
A:transformers.models.gpt2.modeling_gpt2.attn_output->self.resid_dropout(attn_output)
A:transformers.models.gpt2.modeling_gpt2.tensor->tensor.permute(0, 2, 1, 3).contiguous().permute(0, 2, 1, 3).contiguous()
A:transformers.models.gpt2.modeling_gpt2.query->self._split_heads(query, self.num_heads, self.head_dim)
A:transformers.models.gpt2.modeling_gpt2.(key, value)->self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)
A:transformers.models.gpt2.modeling_gpt2.(query, key, value)->self.c_attn(hidden_states).split(self.split_size, dim=2)
A:transformers.models.gpt2.modeling_gpt2.key->torch.cat((past_key, key), dim=-2)
A:transformers.models.gpt2.modeling_gpt2.value->torch.cat((past_value, value), dim=-2)
A:transformers.models.gpt2.modeling_gpt2.(attn_output, attn_weights)->self._attn(query, key, value, attention_mask, head_mask)
A:transformers.models.gpt2.modeling_gpt2.self.c_fc->Conv1D(intermediate_size, embed_dim)
A:transformers.models.gpt2.modeling_gpt2.self.dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.models.gpt2.modeling_gpt2.hidden_states->hidden_states.to(self.lm_head.weight.device).to(self.lm_head.weight.device)
A:transformers.models.gpt2.modeling_gpt2.self.ln_1->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.models.gpt2.modeling_gpt2.self.attn->GPT2Attention(config)
A:transformers.models.gpt2.modeling_gpt2.self.ln_2->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.models.gpt2.modeling_gpt2.self.crossattention->GPT2Attention(config, is_cross_attention=True)
A:transformers.models.gpt2.modeling_gpt2.self.ln_cross_attn->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.models.gpt2.modeling_gpt2.self.mlp->GPT2MLP(inner_dim, config)
A:transformers.models.gpt2.modeling_gpt2.attn_outputs->self.attn(hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.gpt2.modeling_gpt2.cross_attn_outputs->self.crossattention(hidden_states, attention_mask=attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, output_attentions=output_attentions)
A:transformers.models.gpt2.modeling_gpt2.feed_forward_hidden_states->self.mlp(hidden_states)
A:transformers.models.gpt2.modeling_gpt2.self.wte->self.wte.to('cpu')
A:transformers.models.gpt2.modeling_gpt2.self.wpe->self.wpe.to('cpu')
A:transformers.models.gpt2.modeling_gpt2.self.drop->torch.nn.Dropout(config.embd_pdrop)
A:transformers.models.gpt2.modeling_gpt2.self.h->torch.nn.ModuleList([GPT2Block(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.gpt2.modeling_gpt2.self.ln_f->self.ln_f.to('cpu')
A:transformers.models.gpt2.modeling_gpt2.self.h[block]->self.h[block].to(cuda_device)
A:transformers.models.gpt2.modeling_gpt2.self.h[index]->self.h[index].to('cpu')
A:transformers.models.gpt2.modeling_gpt2.input_shape->input_ids[:, -1].unsqueeze(-1).size()
A:transformers.models.gpt2.modeling_gpt2.input_ids->input_ids[:, -1].unsqueeze(-1)
A:transformers.models.gpt2.modeling_gpt2.token_type_ids->token_type_ids[:, -1].unsqueeze(-1)
A:transformers.models.gpt2.modeling_gpt2.position_ids->position_ids[:, -1].unsqueeze(-1)
A:transformers.models.gpt2.modeling_gpt2.past_key_values->tuple([None] * len(self.h))
A:transformers.models.gpt2.modeling_gpt2.past_length->past_key_values[0][0].size(-2)
A:transformers.models.gpt2.modeling_gpt2.attention_mask->kwargs.get('attention_mask', None)
A:transformers.models.gpt2.modeling_gpt2.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.gpt2.modeling_gpt2.encoder_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.gpt2.modeling_gpt2.head_mask->head_mask.to(hidden_states.device).to(hidden_states.device)
A:transformers.models.gpt2.modeling_gpt2.inputs_embeds->self.wte(input_ids)
A:transformers.models.gpt2.modeling_gpt2.position_embeds->self.wpe(position_ids)
A:transformers.models.gpt2.modeling_gpt2.token_type_embeds->self.wte(token_type_ids)
A:transformers.models.gpt2.modeling_gpt2.layer_past->tuple((past_state.to(hidden_states.device) for past_state in layer_past))
A:transformers.models.gpt2.modeling_gpt2.outputs->block(hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i], encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.gpt2.modeling_gpt2.self.transformer->GPT2Model(config)
A:transformers.models.gpt2.modeling_gpt2.self.lm_head->self.lm_head.to('cpu')
A:transformers.models.gpt2.modeling_gpt2.transformer_outputs->self.transformer(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.gpt2.modeling_gpt2.lm_logits->self.lm_head(hidden_states)
A:transformers.models.gpt2.modeling_gpt2.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.models.gpt2.modeling_gpt2.shift_labels->labels[..., 1:].contiguous()
A:transformers.models.gpt2.modeling_gpt2.loss_fct->CrossEntropyLoss()
A:transformers.models.gpt2.modeling_gpt2.loss->loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.gpt2.modeling_gpt2.self.multiple_choice_head->self.multiple_choice_head.to('cpu')
A:transformers.models.gpt2.modeling_gpt2.mc_logits->self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)
A:transformers.models.gpt2.modeling_gpt2.mc_loss->loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))
A:transformers.models.gpt2.modeling_gpt2.lm_loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
A:transformers.models.gpt2.modeling_gpt2.self.score->torch.nn.Linear(config.n_embd, self.num_labels, bias=False)
A:transformers.models.gpt2.modeling_gpt2.logits->self.score(hidden_states)
transformers.GPT2DoubleHeadsModel(self,config)
transformers.GPT2DoubleHeadsModel._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.GPT2DoubleHeadsModel.deparallelize(self)
transformers.GPT2DoubleHeadsModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.GPT2DoubleHeadsModel.get_output_embeddings(self)
transformers.GPT2DoubleHeadsModel.parallelize(self,device_map=None)
transformers.GPT2DoubleHeadsModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.GPT2DoubleHeadsModel.set_output_embeddings(self,new_embeddings)
transformers.GPT2DoubleHeadsModelOutput(ModelOutput)
transformers.GPT2ForSequenceClassification(self,config)
transformers.GPT2ForSequenceClassification.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.GPT2LMHeadModel(self,config)
transformers.GPT2LMHeadModel._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.GPT2LMHeadModel.deparallelize(self)
transformers.GPT2LMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.GPT2LMHeadModel.get_output_embeddings(self)
transformers.GPT2LMHeadModel.parallelize(self,device_map=None)
transformers.GPT2LMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.GPT2LMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.GPT2Model(self,config)
transformers.GPT2Model._prune_heads(self,heads_to_prune)
transformers.GPT2Model.deparallelize(self)
transformers.GPT2Model.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.GPT2Model.get_input_embeddings(self)
transformers.GPT2Model.parallelize(self,device_map=None)
transformers.GPT2Model.set_input_embeddings(self,new_embeddings)
transformers.GPT2PreTrainedModel(self,*inputs,**kwargs)
transformers.GPT2PreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_gpt2(model,config,gpt2_checkpoint_path)
transformers.models.gpt2.modeling_gpt2.GPT2Attention(self,config,is_cross_attention=False)
transformers.models.gpt2.modeling_gpt2.GPT2Attention.__init__(self,config,is_cross_attention=False)
transformers.models.gpt2.modeling_gpt2.GPT2Attention._attn(self,query,key,value,attention_mask=None,head_mask=None)
transformers.models.gpt2.modeling_gpt2.GPT2Attention._merge_heads(self,tensor,num_heads,attn_head_size)
transformers.models.gpt2.modeling_gpt2.GPT2Attention._split_heads(self,tensor,num_heads,attn_head_size)
transformers.models.gpt2.modeling_gpt2.GPT2Attention.forward(self,hidden_states,layer_past=None,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=False,output_attentions=False)
transformers.models.gpt2.modeling_gpt2.GPT2Attention.prune_heads(self,heads)
transformers.models.gpt2.modeling_gpt2.GPT2Block(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2Block.__init__(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2Block.forward(self,hidden_states,layer_past=None,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=False,output_attentions=False)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.__init__(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.deparallelize(self)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.get_output_embeddings(self)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.parallelize(self,device_map=None)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModel.set_output_embeddings(self,new_embeddings)
transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput(ModelOutput)
transformers.models.gpt2.modeling_gpt2.GPT2ForSequenceClassification(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2ForSequenceClassification.__init__(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2ForSequenceClassification.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.__init__(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.deparallelize(self)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.get_output_embeddings(self)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.parallelize(self,device_map=None)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.gpt2.modeling_gpt2.GPT2MLP(self,intermediate_size,config)
transformers.models.gpt2.modeling_gpt2.GPT2MLP.__init__(self,intermediate_size,config)
transformers.models.gpt2.modeling_gpt2.GPT2MLP.forward(self,hidden_states)
transformers.models.gpt2.modeling_gpt2.GPT2Model(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2Model.__init__(self,config)
transformers.models.gpt2.modeling_gpt2.GPT2Model._prune_heads(self,heads_to_prune)
transformers.models.gpt2.modeling_gpt2.GPT2Model.deparallelize(self)
transformers.models.gpt2.modeling_gpt2.GPT2Model.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.gpt2.modeling_gpt2.GPT2Model.get_input_embeddings(self)
transformers.models.gpt2.modeling_gpt2.GPT2Model.parallelize(self,device_map=None)
transformers.models.gpt2.modeling_gpt2.GPT2Model.set_input_embeddings(self,new_embeddings)
transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel(self,*inputs,**kwargs)
transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel._init_weights(self,module)
transformers.models.gpt2.modeling_gpt2.load_tf_weights_in_gpt2(model,config,gpt2_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mmbt/configuration_mmbt.py----------------------------------------
A:transformers.models.mmbt.configuration_mmbt.logger->utils.logging.get_logger(__name__)
transformers.MMBTConfig(self,config,num_labels=None,modal_hidden_size=2048)
transformers.models.mmbt.configuration_mmbt.MMBTConfig(self,config,num_labels=None,modal_hidden_size=2048)
transformers.models.mmbt.configuration_mmbt.MMBTConfig.__init__(self,config,num_labels=None,modal_hidden_size=2048)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mmbt/modeling_mmbt.py----------------------------------------
A:transformers.models.mmbt.modeling_mmbt.logger->utils.logging.get_logger(__name__)
A:transformers.models.mmbt.modeling_mmbt.self.proj_embeddings->torch.nn.Linear(config.modal_hidden_size, config.hidden_size)
A:transformers.models.mmbt.modeling_mmbt.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.mmbt.modeling_mmbt.token_embeddings->torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1)
A:transformers.models.mmbt.modeling_mmbt.seq_length->torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1).size(1)
A:transformers.models.mmbt.modeling_mmbt.start_token_embeds->self.word_embeddings(start_token)
A:transformers.models.mmbt.modeling_mmbt.end_token_embeds->self.word_embeddings(end_token)
A:transformers.models.mmbt.modeling_mmbt.position_ids->position_ids.unsqueeze(0).expand(input_modal.size(0), seq_length).unsqueeze(0).expand(input_modal.size(0), seq_length)
A:transformers.models.mmbt.modeling_mmbt.token_type_ids->torch.ones(input_txt_shape, dtype=torch.long, device=device)
A:transformers.models.mmbt.modeling_mmbt.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.mmbt.modeling_mmbt.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.mmbt.modeling_mmbt.embeddings->self.dropout(embeddings)
A:transformers.models.mmbt.modeling_mmbt.self.modal_encoder->ModalEmbeddings(config, encoder, transformer.embeddings)
A:transformers.models.mmbt.modeling_mmbt.input_txt_shape->input_ids.size()
A:transformers.models.mmbt.modeling_mmbt.modal_embeddings->self.modal_encoder(input_modal, start_token=modal_start_tokens, end_token=modal_end_tokens, position_ids=modal_position_ids, token_type_ids=modal_token_type_ids)
A:transformers.models.mmbt.modeling_mmbt.txt_embeddings->self.transformer.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.mmbt.modeling_mmbt.embedding_output->torch.cat([modal_embeddings, txt_embeddings], 1)
A:transformers.models.mmbt.modeling_mmbt.attention_mask->torch.cat([torch.ones(input_modal_shape, device=device, dtype=torch.long), attention_mask], dim=1)
A:transformers.models.mmbt.modeling_mmbt.encoder_attention_mask->torch.cat([torch.ones(input_modal_shape, device=device), encoder_attention_mask], dim=1)
A:transformers.models.mmbt.modeling_mmbt.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, self.device)
A:transformers.models.mmbt.modeling_mmbt.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.mmbt.modeling_mmbt.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.mmbt.modeling_mmbt.encoder_outputs->self.transformer.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mmbt.modeling_mmbt.pooled_output->self.dropout(pooled_output)
A:transformers.models.mmbt.modeling_mmbt.self.mmbt->MMBTModel(config, transformer, encoder)
A:transformers.models.mmbt.modeling_mmbt.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mmbt.modeling_mmbt.outputs->self.mmbt(input_modal=input_modal, input_ids=input_ids, modal_start_tokens=modal_start_tokens, modal_end_tokens=modal_end_tokens, attention_mask=attention_mask, token_type_ids=token_type_ids, modal_token_type_ids=modal_token_type_ids, position_ids=position_ids, modal_position_ids=modal_position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, return_dict=return_dict)
A:transformers.models.mmbt.modeling_mmbt.logits->self.classifier(pooled_output)
A:transformers.models.mmbt.modeling_mmbt.loss_fct->CrossEntropyLoss()
A:transformers.models.mmbt.modeling_mmbt.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
transformers.MMBTForClassification(self,config,transformer,encoder)
transformers.MMBTForClassification.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,return_dict=None)
transformers.MMBTModel(self,config,transformer,encoder)
transformers.MMBTModel.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MMBTModel.get_input_embeddings(self)
transformers.MMBTModel.set_input_embeddings(self,value)
transformers.ModalEmbeddings(self,config,encoder,embeddings)
transformers.ModalEmbeddings.forward(self,input_modal,start_token=None,end_token=None,position_ids=None,token_type_ids=None)
transformers.models.mmbt.modeling_mmbt.MMBTForClassification(self,config,transformer,encoder)
transformers.models.mmbt.modeling_mmbt.MMBTForClassification.__init__(self,config,transformer,encoder)
transformers.models.mmbt.modeling_mmbt.MMBTForClassification.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,return_dict=None)
transformers.models.mmbt.modeling_mmbt.MMBTModel(self,config,transformer,encoder)
transformers.models.mmbt.modeling_mmbt.MMBTModel.__init__(self,config,transformer,encoder)
transformers.models.mmbt.modeling_mmbt.MMBTModel.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mmbt.modeling_mmbt.MMBTModel.get_input_embeddings(self)
transformers.models.mmbt.modeling_mmbt.MMBTModel.set_input_embeddings(self,value)
transformers.models.mmbt.modeling_mmbt.ModalEmbeddings(self,config,encoder,embeddings)
transformers.models.mmbt.modeling_mmbt.ModalEmbeddings.__init__(self,config,encoder,embeddings)
transformers.models.mmbt.modeling_mmbt.ModalEmbeddings.forward(self,input_modal,start_token=None,end_token=None,position_ids=None,token_type_ids=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mmbt/__init__.py----------------------------------------
A:transformers.models.mmbt.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/modeling_tf_funnel.py----------------------------------------
A:transformers.models.funnel.modeling_tf_funnel.logger->utils.logging.get_logger(__name__)
A:transformers.models.funnel.modeling_tf_funnel.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.funnel.modeling_tf_funnel.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_tf_funnel.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.funnel.modeling_tf_funnel.inputs_embeds->tensorflow.gather(self.weight, input_ids)
A:transformers.models.funnel.modeling_tf_funnel.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.funnel.modeling_tf_funnel.self.sin_dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_tf_funnel.self.cos_dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_tf_funnel.position_embeds->self.stride_pool(position_embeds, 0)
A:transformers.models.funnel.modeling_tf_funnel.token_type_mat->tensorflow.tile(token_type_mat[:, None], [1, shape_list(q_head)[2], 1, 1])
A:transformers.models.funnel.modeling_tf_funnel.cls_ids->tensorflow.equal(token_type_ids, tf.constant([self.cls_token_type_id], dtype=token_type_ids.dtype))
A:transformers.models.funnel.modeling_tf_funnel.cls_mat->tensorflow.logical_or(tf.expand_dims(cls_ids, -1), tf.expand_dims(cls_ids, -2))
A:transformers.models.funnel.modeling_tf_funnel.pos_seq->tensorflow.range(0, seq_len, 1.0)
A:transformers.models.funnel.modeling_tf_funnel.freq_seq->tensorflow.range(0, self.d_model // 2, 1.0)
A:transformers.models.funnel.modeling_tf_funnel.sinusoid->tensorflow.einsum('i,d->id', rel_pos_id, inv_freq)
A:transformers.models.funnel.modeling_tf_funnel.sin_embed->self.sin_dropout(tf.sin(sinusoid), training=training)
A:transformers.models.funnel.modeling_tf_funnel.sin_embed_d->self.sin_dropout(sin_embed, training=training)
A:transformers.models.funnel.modeling_tf_funnel.cos_embed->self.cos_dropout(tf.cos(sinusoid), training=training)
A:transformers.models.funnel.modeling_tf_funnel.cos_embed_d->self.cos_dropout(cos_embed, training=training)
A:transformers.models.funnel.modeling_tf_funnel.phi->tensorflow.concat([sin_embed_d, sin_embed_d], axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.psi->tensorflow.concat([cos_embed, sin_embed], axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.pi->tensorflow.concat([cos_embed_d, cos_embed_d], axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.omega->tensorflow.concat([-sin_embed, cos_embed], axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.rel_pos_id->tensorflow.range(-seq_len * 2, seq_len * 2, 1.0)
A:transformers.models.funnel.modeling_tf_funnel.pos_embed->tensorflow.concat([sin_embed, cos_embed], axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.pos->tensorflow.range(0, seq_len)
A:transformers.models.funnel.modeling_tf_funnel.position_embeds_pooling->tensorflow.gather(pos_embed, rel_pos, axis=0)
A:transformers.models.funnel.modeling_tf_funnel.pooled_pos->self.stride_pool_pos(pos, block_index)
A:transformers.models.funnel.modeling_tf_funnel.rel_pos->tensorflow.cast(rel_pos, dtype=zero_offset.dtype)
A:transformers.models.funnel.modeling_tf_funnel.position_embeds_no_pooling->tensorflow.gather(pos_embed, rel_pos, axis=0)
A:transformers.models.funnel.modeling_tf_funnel.cls_pos->tensorflow.constant([-2 ** block_index + 1], dtype=pos_id.dtype)
A:transformers.models.funnel.modeling_tf_funnel.tensor->tensorflow.nn.max_pool1d(tensor, stride, strides=stride, data_format='NWC', padding='SAME')
A:transformers.models.funnel.modeling_tf_funnel.ndim->len(shape_list(tensor))
A:transformers.models.funnel.modeling_tf_funnel.cls_mask->self.stride_pool(cls_mask, 1)
A:transformers.models.funnel.modeling_tf_funnel.output->self.call(input_ids=inputs)
A:transformers.models.funnel.modeling_tf_funnel.attention_mask->tensorflow.cast(attention_mask, dtype=attn_score.dtype)
A:transformers.models.funnel.modeling_tf_funnel.(batch_size, n_head, seq_len, max_rel_len)->shape_list(positional_attn)
A:transformers.models.funnel.modeling_tf_funnel.positional_attn->self.relative_positional_attention(position_embeds, q_head, context_len, cls_mask)
A:transformers.models.funnel.modeling_tf_funnel.self.hidden_dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_tf_funnel.self.attention_dropout->tensorflow.keras.layers.Dropout(config.attention_dropout)
A:transformers.models.funnel.modeling_tf_funnel.initializer->get_initializer(config.initializer_range)
A:transformers.models.funnel.modeling_tf_funnel.self.q_head->tensorflow.keras.layers.Dense(n_head * d_head, use_bias=False, kernel_initializer=initializer, name='q_head')
A:transformers.models.funnel.modeling_tf_funnel.self.k_head->tensorflow.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name='k_head')
A:transformers.models.funnel.modeling_tf_funnel.self.v_head->tensorflow.keras.layers.Dense(n_head * d_head, kernel_initializer=initializer, name='v_head')
A:transformers.models.funnel.modeling_tf_funnel.self.post_proj->tensorflow.keras.layers.Dense(d_model, kernel_initializer=initializer, name='post_proj')
A:transformers.models.funnel.modeling_tf_funnel.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.funnel.modeling_tf_funnel.self.r_w_bias->self.add_weight(shape=(n_head, d_head), initializer=initializer, trainable=True, name='r_w_bias')
A:transformers.models.funnel.modeling_tf_funnel.self.r_r_bias->self.add_weight(shape=(n_head, d_head), initializer=initializer, trainable=True, name='r_r_bias')
A:transformers.models.funnel.modeling_tf_funnel.self.r_kernel->self.add_weight(shape=(d_model, n_head, d_head), initializer=initializer, trainable=True, name='r_kernel')
A:transformers.models.funnel.modeling_tf_funnel.self.r_s_bias->self.add_weight(shape=(n_head, d_head), initializer=initializer, trainable=True, name='r_s_bias')
A:transformers.models.funnel.modeling_tf_funnel.self.seg_embed->self.add_weight(shape=(2, n_head, d_head), initializer=initializer, trainable=True, name='seg_embed')
A:transformers.models.funnel.modeling_tf_funnel.q_r_attention->tensorflow.einsum('binh,dnh->bind', q_head + u, w_r)
A:transformers.models.funnel.modeling_tf_funnel.r_head->tensorflow.einsum('td,dnh->tnh', r, w_r)
A:transformers.models.funnel.modeling_tf_funnel.(batch_size, seq_len, context_len)->shape_list(token_type_mat)
A:transformers.models.funnel.modeling_tf_funnel.token_type_bias->tensorflow.einsum('bind,snd->bnis', q_head + r_s_bias, self.seg_embed)
A:transformers.models.funnel.modeling_tf_funnel.(diff_token_type, same_token_type)->tensorflow.split(token_type_bias, 2, axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.token_type_attn->self.relative_token_type_attention(token_type_mat, q_head, cls_mask)
A:transformers.models.funnel.modeling_tf_funnel.(batch_size, seq_len, _)->shape_list(query)
A:transformers.models.funnel.modeling_tf_funnel.q_head->tensorflow.reshape(self.q_head(query), [batch_size, seq_len, n_head, d_head])
A:transformers.models.funnel.modeling_tf_funnel.k_head->tensorflow.reshape(self.k_head(key), [batch_size, context_len, n_head, d_head])
A:transformers.models.funnel.modeling_tf_funnel.v_head->tensorflow.reshape(self.v_head(value), [batch_size, context_len, n_head, d_head])
A:transformers.models.funnel.modeling_tf_funnel.content_score->tensorflow.einsum('bind,bjnd->bnij', q_head + r_w_bias, k_head)
A:transformers.models.funnel.modeling_tf_funnel.attn_prob->self.attention_dropout(attn_prob, training=training)
A:transformers.models.funnel.modeling_tf_funnel.attn_vec->tensorflow.einsum('bnij,bjnd->bind', attn_prob, v_head)
A:transformers.models.funnel.modeling_tf_funnel.attn_out->self.hidden_dropout(attn_out, training=training)
A:transformers.models.funnel.modeling_tf_funnel.self.linear_1->tensorflow.keras.layers.Dense(config.d_inner, kernel_initializer=initializer, name='linear_1')
A:transformers.models.funnel.modeling_tf_funnel.self.activation_function->get_tf_activation(config.hidden_act)
A:transformers.models.funnel.modeling_tf_funnel.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.funnel.modeling_tf_funnel.self.linear_2->tensorflow.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name='linear_2')
A:transformers.models.funnel.modeling_tf_funnel.h->self.dropout(h, training=training)
A:transformers.models.funnel.modeling_tf_funnel.self.attention->TFFunnelRelMultiheadAttention(config, block_index, name='attention')
A:transformers.models.funnel.modeling_tf_funnel.self.ffn->TFFunnelPositionwiseFFN(config, name='ffn')
A:transformers.models.funnel.modeling_tf_funnel.attn->self.attention(query, key, value, attention_inputs, output_attentions=output_attentions, training=training)
A:transformers.models.funnel.modeling_tf_funnel.self.attention_structure->TFFunnelAttentionStructure(config)
A:transformers.models.funnel.modeling_tf_funnel.attention_inputs->self.attention_structure.init_attention_inputs(hidden, attention_mask=attention_mask, token_type_ids=token_type_ids, training=training)
A:transformers.models.funnel.modeling_tf_funnel.pooled_hidden->tensorflow.zeros(shape_list(hidden))
A:transformers.models.funnel.modeling_tf_funnel.(pooled_hidden, attention_inputs)->self.attention_structure.pre_attention_pooling(hidden, attention_inputs)
A:transformers.models.funnel.modeling_tf_funnel.layer_output->layer(hidden, hidden, hidden, attention_inputs, output_attentions=output_attentions, training=training)
A:transformers.models.funnel.modeling_tf_funnel.upsampled_hidden->upsample(final_hidden, stride=self.stride, target_len=shape_list(first_block_hidden)[1], separate_cls=self.separate_cls, truncate_seq=self.truncate_seq)
A:transformers.models.funnel.modeling_tf_funnel.self.embeddings->TFFunnelEmbeddings(config, name='embeddings')
A:transformers.models.funnel.modeling_tf_funnel.self.encoder->TFFunnelEncoder(config, name='encoder')
A:transformers.models.funnel.modeling_tf_funnel.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.funnel.modeling_tf_funnel.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.funnel.modeling_tf_funnel.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.funnel.modeling_tf_funnel.inputs['token_type_ids']->tensorflow.fill(input_shape, 0)
A:transformers.models.funnel.modeling_tf_funnel.inputs['inputs_embeds']->self.embeddings(inputs['input_ids'], training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.encoder_outputs->self.encoder(inputs['inputs_embeds'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], output_attentions=inputs['output_attentions'], output_hidden_states=True, return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.self.decoder->TFFunnelDecoder(config, name='decoder')
A:transformers.models.funnel.modeling_tf_funnel.decoder_outputs->self.decoder(final_hidden=encoder_outputs[0], first_block_hidden=encoder_outputs[1][self.block_sizes[0]], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.self.dense->tensorflow.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name='dense')
A:transformers.models.funnel.modeling_tf_funnel.self.dense_prediction->tensorflow.keras.layers.Dense(1, kernel_initializer=initializer, name='dense_prediction')
A:transformers.models.funnel.modeling_tf_funnel.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.bias)
A:transformers.models.funnel.modeling_tf_funnel.logits->self.qa_outputs(sequence_output)
A:transformers.models.funnel.modeling_tf_funnel.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.funnel.modeling_tf_funnel.self.linear_hidden->tensorflow.keras.layers.Dense(config.d_model, kernel_initializer=initializer, name='linear_hidden')
A:transformers.models.funnel.modeling_tf_funnel.self.linear_out->tensorflow.keras.layers.Dense(n_labels, kernel_initializer=initializer, name='linear_out')
A:transformers.models.funnel.modeling_tf_funnel.hidden->self.dropout(hidden, training=training)
A:transformers.models.funnel.modeling_tf_funnel.self.funnel->TFFunnelMainLayer(config, name='funnel')
A:transformers.models.funnel.modeling_tf_funnel.self.discriminator_predictions->TFFunnelDiscriminatorPredictions(config, name='discriminator_predictions')
A:transformers.models.funnel.modeling_tf_funnel.discriminator_hidden_states->self.funnel(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['inputs_embeds'], inputs['output_attentions'], inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.self.lm_head->TFFunnelMaskedLMHead(config, self.funnel.embeddings, name='lm_head')
A:transformers.models.funnel.modeling_tf_funnel.outputs->self.funnel(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['inputs_embeds'], inputs['output_attentions'], inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.prediction_scores->self.lm_head(sequence_output, training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.funnel.modeling_tf_funnel.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.funnel.modeling_tf_funnel.sequence_output->self.dropout(sequence_output, training=inputs['training'])
A:transformers.models.funnel.modeling_tf_funnel.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.funnel.modeling_tf_funnel.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.funnel.modeling_tf_funnel.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFFunnelBaseModel(self,config,*inputs,**kwargs)
transformers.TFFunnelBaseModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFFunnelBaseModel.serving_output(self,output)
transformers.TFFunnelForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFFunnelForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFFunnelForMaskedLM.get_lm_head(self)
transformers.TFFunnelForMaskedLM.get_prefix_bias_name(self)
transformers.TFFunnelForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFFunnelForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFFunnelForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFFunnelForMultipleChoice.dummy_inputs(self)
transformers.TFFunnelForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.TFFunnelForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFFunnelForPreTraining(self,config,**kwargs)
transformers.TFFunnelForPreTraining.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFFunnelForPreTraining.serving_output(self,output)
transformers.TFFunnelForPreTrainingOutput(ModelOutput)
transformers.TFFunnelForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFFunnelForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFFunnelForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFFunnelForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFFunnelForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFFunnelForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFFunnelForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFFunnelForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFFunnelForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFFunnelModel(self,config,*inputs,**kwargs)
transformers.TFFunnelModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFFunnelModel.serving_output(self,output)
transformers.TFFunnelPreTrainedModel(TFPreTrainedModel)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure(self,config)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.__init__(self,config)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.get_position_embeds(self,seq_len,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.init_attention_inputs(self,inputs_embeds,attention_mask=None,token_type_ids=None,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.pool_tensor(self,tensor,mode='mean',stride=2)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.post_attention_pooling(self,attention_inputs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.pre_attention_pooling(self,output,attention_inputs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.relative_pos(self,pos,stride,pooled_pos=None,shift=1)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.stride_pool(self,tensor,axis)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.stride_pool_pos(self,pos_id,block_index)
transformers.models.funnel.modeling_tf_funnel.TFFunnelAttentionStructure.token_type_ids_to_mat(self,token_type_ids)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseLayer(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseLayer.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseLayer._prune_heads(self,heads_to_prune)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseLayer.get_input_embeddings(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseLayer.set_input_embeddings(self,value)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseModel(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseModel.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelBaseModel.serving_output(self,output)
transformers.models.funnel.modeling_tf_funnel.TFFunnelClassificationHead(self,config,n_labels,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelClassificationHead.__init__(self,config,n_labels,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelClassificationHead.call(self,hidden,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelDecoder(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelDecoder.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelDecoder.call(self,final_hidden,first_block_hidden,attention_mask=None,token_type_ids=None,output_attentions=False,output_hidden_states=False,return_dict=True,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelDiscriminatorPredictions(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelDiscriminatorPredictions.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelDiscriminatorPredictions.call(self,discriminator_hidden_states)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEmbeddings(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEmbeddings.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEmbeddings.build(self,input_shape)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEmbeddings.call(self,input_ids=None,inputs_embeds=None,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEncoder(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEncoder.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelEncoder.call(self,inputs_embeds,attention_mask=None,token_type_ids=None,output_attentions=False,output_hidden_states=False,return_dict=True,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMaskedLM.get_lm_head(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMaskedLM.get_prefix_bias_name(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMultipleChoice.dummy_inputs(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.models.funnel.modeling_tf_funnel.TFFunnelForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTraining(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTraining.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTraining.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTraining.serving_output(self,output)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput(ModelOutput)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.funnel.modeling_tf_funnel.TFFunnelForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.funnel.modeling_tf_funnel.TFFunnelForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.funnel.modeling_tf_funnel.TFFunnelLayer(self,config,block_index,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelLayer.__init__(self,config,block_index,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelLayer.call(self,query,key,value,attention_inputs,output_attentions=False,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMainLayer(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMainLayer.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMainLayer._prune_heads(self,heads_to_prune)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMainLayer.get_input_embeddings(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMainLayer.set_input_embeddings(self,value)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead(self,config,input_embeddings,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.build(self,input_shape)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.call(self,hidden_states,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.get_bias(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.get_output_embeddings(self)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.set_bias(self,value)
transformers.models.funnel.modeling_tf_funnel.TFFunnelMaskedLMHead.set_output_embeddings(self,value)
transformers.models.funnel.modeling_tf_funnel.TFFunnelModel(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelModel.__init__(self,config,*inputs,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelModel.serving_output(self,output)
transformers.models.funnel.modeling_tf_funnel.TFFunnelPositionwiseFFN(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelPositionwiseFFN.__init__(self,config,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelPositionwiseFFN.call(self,hidden,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelPreTrainedModel(TFPreTrainedModel)
transformers.models.funnel.modeling_tf_funnel.TFFunnelRelMultiheadAttention(self,config,block_index,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelRelMultiheadAttention.__init__(self,config,block_index,**kwargs)
transformers.models.funnel.modeling_tf_funnel.TFFunnelRelMultiheadAttention.build(self,input_shape)
transformers.models.funnel.modeling_tf_funnel.TFFunnelRelMultiheadAttention.call(self,query,key,value,attention_inputs,output_attentions=False,training=False)
transformers.models.funnel.modeling_tf_funnel.TFFunnelRelMultiheadAttention.relative_positional_attention(self,position_embeds,q_head,context_len,cls_mask=None)
transformers.models.funnel.modeling_tf_funnel.TFFunnelRelMultiheadAttention.relative_token_type_attention(self,token_type_mat,q_head,cls_mask=None)
transformers.models.funnel.modeling_tf_funnel._relative_shift_gather(positional_attn,context_len,shift)
transformers.models.funnel.modeling_tf_funnel.upsample(x,stride,target_len,separate_cls=True,truncate_seq=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/tokenization_funnel_fast.py----------------------------------------
A:transformers.models.funnel.tokenization_funnel_fast.logger->utils.logging.get_logger(__name__)
transformers.FunnelTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',bos_token='<s>',eos_token='</s>',clean_text=True,tokenize_chinese_chars=True,strip_accents=None,wordpieces_prefix='##',**kwargs)
transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.funnel.tokenization_funnel_fast.FunnelTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',bos_token='<s>',eos_token='</s>',clean_text=True,tokenize_chinese_chars=True,strip_accents=None,wordpieces_prefix='##',**kwargs)
transformers.models.funnel.tokenization_funnel_fast.FunnelTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',bos_token='<s>',eos_token='</s>',clean_text=True,tokenize_chinese_chars=True,strip_accents=None,wordpieces_prefix='##',**kwargs)
transformers.models.funnel.tokenization_funnel_fast.FunnelTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/configuration_funnel.py----------------------------------------
A:transformers.models.funnel.configuration_funnel.logger->utils.logging.get_logger(__name__)
transformers.FunnelConfig(self,vocab_size=30522,block_sizes=[4,4,4],block_repeats=None,num_decoder_layers=2,d_model=768,n_head=12,d_head=64,d_inner=3072,hidden_act='gelu_new',hidden_dropout=0.1,attention_dropout=0.1,activation_dropout=0.0,max_position_embeddings=512,type_vocab_size=3,initializer_range=0.1,initializer_std=None,layer_norm_eps=1e-09,pooling_type='mean',attention_type='relative_shift',separate_cls=True,truncate_seq=True,pool_q_only=True,**kwargs)
transformers.FunnelConfig.hidden_size(self)
transformers.FunnelConfig.num_attention_heads(self)
transformers.FunnelConfig.num_blocks(self)
transformers.FunnelConfig.num_hidden_layers(self)
transformers.models.funnel.configuration_funnel.FunnelConfig(self,vocab_size=30522,block_sizes=[4,4,4],block_repeats=None,num_decoder_layers=2,d_model=768,n_head=12,d_head=64,d_inner=3072,hidden_act='gelu_new',hidden_dropout=0.1,attention_dropout=0.1,activation_dropout=0.0,max_position_embeddings=512,type_vocab_size=3,initializer_range=0.1,initializer_std=None,layer_norm_eps=1e-09,pooling_type='mean',attention_type='relative_shift',separate_cls=True,truncate_seq=True,pool_q_only=True,**kwargs)
transformers.models.funnel.configuration_funnel.FunnelConfig.__init__(self,vocab_size=30522,block_sizes=[4,4,4],block_repeats=None,num_decoder_layers=2,d_model=768,n_head=12,d_head=64,d_inner=3072,hidden_act='gelu_new',hidden_dropout=0.1,attention_dropout=0.1,activation_dropout=0.0,max_position_embeddings=512,type_vocab_size=3,initializer_range=0.1,initializer_std=None,layer_norm_eps=1e-09,pooling_type='mean',attention_type='relative_shift',separate_cls=True,truncate_seq=True,pool_q_only=True,**kwargs)
transformers.models.funnel.configuration_funnel.FunnelConfig.hidden_size(self)
transformers.models.funnel.configuration_funnel.FunnelConfig.num_attention_heads(self)
transformers.models.funnel.configuration_funnel.FunnelConfig.num_blocks(self)
transformers.models.funnel.configuration_funnel.FunnelConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.funnel.convert_funnel_original_tf_checkpoint_to_pytorch.config->transformers.FunnelConfig.from_json_file(config_file)
A:transformers.models.funnel.convert_funnel_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.funnel.convert_funnel_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.funnel.convert_funnel_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path,base_model)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/modeling_funnel.py----------------------------------------
A:transformers.models.funnel.modeling_funnel.logger->utils.logging.get_logger(__name__)
A:transformers.models.funnel.modeling_funnel.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.funnel.modeling_funnel.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.funnel.modeling_funnel.array->numpy.transpose(array)
A:transformers.models.funnel.modeling_funnel.name->name.split('/').split('/')
A:transformers.models.funnel.modeling_funnel.layer_index->int(re.search('layer_(\\d+)', m_name).groups()[0])
A:transformers.models.funnel.modeling_funnel.pointer->getattr(pointer, m_name)
A:transformers.models.funnel.modeling_funnel.pointer.data->torch.from_numpy(array)
A:transformers.models.funnel.modeling_funnel.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.funnel.modeling_funnel.self.layer_norm->torch.nn.LayerNorm(config.d_model, config.layer_norm_eps)
A:transformers.models.funnel.modeling_funnel.self.dropout->torch.nn.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_funnel.inputs_embeds->self.embeddings(input_ids)
A:transformers.models.funnel.modeling_funnel.embeddings->self.dropout(embeddings)
A:transformers.models.funnel.modeling_funnel.self.sin_dropout->torch.nn.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_funnel.self.cos_dropout->torch.nn.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_funnel.self.seq_lenseq_len->self.embeddings(input_ids).size(1)
A:transformers.models.funnel.modeling_funnel.position_embeds->self.stride_pool(position_embeds, 0)
A:transformers.models.funnel.modeling_funnel.pos_seq->torch.arange(0, seq_len, 1.0, dtype=dtype, device=device)
A:transformers.models.funnel.modeling_funnel.freq_seq->torch.arange(0, d_model // 2, 1.0, dtype=dtype, device=device)
A:transformers.models.funnel.modeling_funnel.sin_embed->self.sin_dropout(torch.sin(sinusoid))
A:transformers.models.funnel.modeling_funnel.sin_embed_d->self.sin_dropout(sin_embed)
A:transformers.models.funnel.modeling_funnel.cos_embed->self.cos_dropout(torch.cos(sinusoid))
A:transformers.models.funnel.modeling_funnel.cos_embed_d->self.cos_dropout(cos_embed)
A:transformers.models.funnel.modeling_funnel.phi->torch.cat([sin_embed_d, sin_embed_d], dim=-1)
A:transformers.models.funnel.modeling_funnel.psi->torch.cat([cos_embed, sin_embed], dim=-1)
A:transformers.models.funnel.modeling_funnel.pi->torch.cat([cos_embed_d, cos_embed_d], dim=-1)
A:transformers.models.funnel.modeling_funnel.omega->torch.cat([-sin_embed, cos_embed], dim=-1)
A:transformers.models.funnel.modeling_funnel.rel_pos_id->torch.arange(-seq_len * 2, seq_len * 2, 1.0, dtype=dtype, device=device)
A:transformers.models.funnel.modeling_funnel.pos_embed->torch.cat([sin_embed, cos_embed], dim=-1)
A:transformers.models.funnel.modeling_funnel.pos->torch.arange(0, seq_len, dtype=dtype, device=device)
A:transformers.models.funnel.modeling_funnel.pooled_pos->self.stride_pool_pos(pos, block_index)
A:transformers.models.funnel.modeling_funnel.rel_pos->rel_pos.expand(rel_pos.size(0), d_model).expand(rel_pos.size(0), d_model)
A:transformers.models.funnel.modeling_funnel.position_embeds_pooling->torch.gather(pos_embed, 0, rel_pos)
A:transformers.models.funnel.modeling_funnel.position_embeds_no_pooling->torch.gather(pos_embed, 0, rel_pos)
A:transformers.models.funnel.modeling_funnel.cls_pos->pos_id.new_tensor([-2 ** block_index + 1])
A:transformers.models.funnel.modeling_funnel.tensor->torch.nn.functional.max_pool2d(tensor, stride, stride=stride, ceil_mode=True)
A:transformers.models.funnel.modeling_funnel.token_type_mat->token_type_mat[:, None].expand([batch_size, q_head.shape[2], seq_len, context_len])
A:transformers.models.funnel.modeling_funnel.cls_mask->self.stride_pool(cls_mask, 1)
A:transformers.models.funnel.modeling_funnel.output->torch.cat([cls, output], dim=1)
A:transformers.models.funnel.modeling_funnel.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.funnel.modeling_funnel.positional_attn->self.relative_positional_attention(position_embeds, q_head, context_len, cls_mask)
A:transformers.models.funnel.modeling_funnel.self.hidden_dropout->torch.nn.Dropout(config.hidden_dropout)
A:transformers.models.funnel.modeling_funnel.self.attention_dropout->torch.nn.Dropout(config.attention_dropout)
A:transformers.models.funnel.modeling_funnel.self.q_head->torch.nn.Linear(d_model, n_head * d_head, bias=False)
A:transformers.models.funnel.modeling_funnel.self.k_head->torch.nn.Linear(d_model, n_head * d_head)
A:transformers.models.funnel.modeling_funnel.self.v_head->torch.nn.Linear(d_model, n_head * d_head)
A:transformers.models.funnel.modeling_funnel.self.r_w_bias->torch.nn.Parameter(torch.zeros([n_head, d_head]))
A:transformers.models.funnel.modeling_funnel.self.r_r_bias->torch.nn.Parameter(torch.zeros([n_head, d_head]))
A:transformers.models.funnel.modeling_funnel.self.r_kernel->torch.nn.Parameter(torch.zeros([d_model, n_head, d_head]))
A:transformers.models.funnel.modeling_funnel.self.r_s_bias->torch.nn.Parameter(torch.zeros([n_head, d_head]))
A:transformers.models.funnel.modeling_funnel.self.seg_embed->torch.nn.Parameter(torch.zeros([2, n_head, d_head]))
A:transformers.models.funnel.modeling_funnel.self.post_proj->torch.nn.Linear(n_head * d_head, d_model)
A:transformers.models.funnel.modeling_funnel.q_r_attention->torch.einsum('binh,dnh->bind', q_head + u, w_r)
A:transformers.models.funnel.modeling_funnel.r_head->torch.einsum('td,dnh->tnh', r, w_r)
A:transformers.models.funnel.modeling_funnel.token_type_bias->torch.einsum('bind,snd->bnis', q_head + r_s_bias, self.seg_embed)
A:transformers.models.funnel.modeling_funnel.(diff_token_type, same_token_type)->torch.split(token_type_bias, 1, dim=-1)
A:transformers.models.funnel.modeling_funnel.token_type_attn->self.relative_token_type_attention(token_type_mat, q_head, cls_mask)
A:transformers.models.funnel.modeling_funnel.q_head->self.q_head(query).view(batch_size, seq_len, n_head, d_head)
A:transformers.models.funnel.modeling_funnel.k_head->self.k_head(key).view(batch_size, context_len, n_head, d_head)
A:transformers.models.funnel.modeling_funnel.v_head->self.v_head(value).view(batch_size, context_len, n_head, d_head)
A:transformers.models.funnel.modeling_funnel.content_score->torch.einsum('bind,bjnd->bnij', q_head + r_w_bias, k_head)
A:transformers.models.funnel.modeling_funnel.attn_score->attn_score.float().float()
A:transformers.models.funnel.modeling_funnel.attn_prob->self.attention_dropout(attn_prob)
A:transformers.models.funnel.modeling_funnel.attn_vec->torch.einsum('bnij,bjnd->bind', attn_prob, v_head)
A:transformers.models.funnel.modeling_funnel.attn_out->self.hidden_dropout(attn_out)
A:transformers.models.funnel.modeling_funnel.self.linear_1->torch.nn.Linear(config.d_model, config.d_inner)
A:transformers.models.funnel.modeling_funnel.self.activation_dropout->torch.nn.Dropout(config.activation_dropout)
A:transformers.models.funnel.modeling_funnel.self.linear_2->torch.nn.Linear(config.d_inner, config.d_model)
A:transformers.models.funnel.modeling_funnel.h->self.dropout(h)
A:transformers.models.funnel.modeling_funnel.self.attention->FunnelRelMultiheadAttention(config, block_index)
A:transformers.models.funnel.modeling_funnel.self.ffn->FunnelPositionwiseFFN(config)
A:transformers.models.funnel.modeling_funnel.attn->self.attention(query, key, value, attention_inputs, output_attentions=output_attentions)
A:transformers.models.funnel.modeling_funnel.self.attention_structure->FunnelAttentionStructure(config)
A:transformers.models.funnel.modeling_funnel.self.blocks->torch.nn.ModuleList([nn.ModuleList([FunnelLayer(config, block_index) for _ in range(block_size)]) for (block_index, block_size) in enumerate(config.block_sizes)])
A:transformers.models.funnel.modeling_funnel.attention_inputs->self.attention_structure.init_attention_inputs(hidden, attention_mask=attention_mask, token_type_ids=token_type_ids)
A:transformers.models.funnel.modeling_funnel.(pooled_hidden, attention_inputs)->self.attention_structure.pre_attention_pooling(hidden, attention_inputs)
A:transformers.models.funnel.modeling_funnel.layer_output->layer(hidden, hidden, hidden, attention_inputs, output_attentions=output_attentions)
A:transformers.models.funnel.modeling_funnel.self.layers->torch.nn.ModuleList([FunnelLayer(config, 0) for _ in range(config.num_decoder_layers)])
A:transformers.models.funnel.modeling_funnel.upsampled_hidden->upsample(final_hidden, stride=2 ** (len(self.config.block_sizes) - 1), target_len=first_block_hidden.shape[1], separate_cls=self.config.separate_cls, truncate_seq=self.config.truncate_seq)
A:transformers.models.funnel.modeling_funnel.self.dense->torch.nn.Linear(config.d_model, config.d_model)
A:transformers.models.funnel.modeling_funnel.self.dense_prediction->torch.nn.Linear(config.d_model, 1)
A:transformers.models.funnel.modeling_funnel.hidden_states->ACT2FN[self.config.hidden_act](hidden_states)
A:transformers.models.funnel.modeling_funnel.logits->self.qa_outputs(last_hidden_state)
A:transformers.models.funnel.modeling_funnel.std->numpy.sqrt(1.0 / float(fan_in + fan_out))
A:transformers.models.funnel.modeling_funnel.self.linear_hidden->torch.nn.Linear(config.d_model, config.d_model)
A:transformers.models.funnel.modeling_funnel.self.linear_out->torch.nn.Linear(config.d_model, n_labels)
A:transformers.models.funnel.modeling_funnel.hidden->self.dropout(hidden)
A:transformers.models.funnel.modeling_funnel.self.embeddings->FunnelEmbeddings(config)
A:transformers.models.funnel.modeling_funnel.self.encoder->FunnelEncoder(config)
A:transformers.models.funnel.modeling_funnel.input_shape->input_ids.size()
A:transformers.models.funnel.modeling_funnel.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.funnel.modeling_funnel.encoder_outputs->self.encoder(inputs_embeds, attention_mask=attention_mask, token_type_ids=token_type_ids, output_attentions=output_attentions, output_hidden_states=True, return_dict=return_dict)
A:transformers.models.funnel.modeling_funnel.self.decoder->FunnelDecoder(config)
A:transformers.models.funnel.modeling_funnel.decoder_outputs->self.decoder(final_hidden=encoder_outputs[0], first_block_hidden=encoder_outputs[1][self.config.block_sizes[0]], attention_mask=attention_mask, token_type_ids=token_type_ids, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.funnel.modeling_funnel.self.funnel->FunnelModel(config)
A:transformers.models.funnel.modeling_funnel.self.discriminator_predictions->FunnelDiscriminatorPredictions(config)
A:transformers.models.funnel.modeling_funnel.discriminator_hidden_states->self.funnel(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.funnel.modeling_funnel.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.funnel.modeling_funnel.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.funnel.modeling_funnel.self.lm_head->torch.nn.Linear(config.d_model, config.vocab_size)
A:transformers.models.funnel.modeling_funnel.outputs->self.funnel(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.funnel.modeling_funnel.prediction_logits->self.lm_head(last_hidden_state)
A:transformers.models.funnel.modeling_funnel.masked_lm_loss->loss_fct(prediction_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.funnel.modeling_funnel.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.funnel.modeling_funnel.reshaped_logits->self.qa_outputs(last_hidden_state).view(-1, num_choices)
A:transformers.models.funnel.modeling_funnel.last_hidden_state->self.dropout(last_hidden_state)
A:transformers.models.funnel.modeling_funnel.active_logits->self.qa_outputs(last_hidden_state).view(-1, self.num_labels)
A:transformers.models.funnel.modeling_funnel.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.funnel.modeling_funnel.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.funnel.modeling_funnel.(start_logits, end_logits)->self.qa_outputs(last_hidden_state).split(1, dim=-1)
A:transformers.models.funnel.modeling_funnel.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.funnel.modeling_funnel.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.funnel.modeling_funnel.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.funnel.modeling_funnel.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.funnel.modeling_funnel.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.funnel.modeling_funnel.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.funnel.modeling_funnel.end_loss->loss_fct(end_logits, end_positions)
transformers.FunnelBaseModel(self,config)
transformers.FunnelBaseModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelBaseModel.get_input_embeddings(self)
transformers.FunnelBaseModel.set_input_embeddings(self,new_embeddings)
transformers.FunnelForMaskedLM(self,config)
transformers.FunnelForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelForMaskedLM.get_output_embeddings(self)
transformers.FunnelForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.FunnelForMultipleChoice(self,config)
transformers.FunnelForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelForPreTraining(self,config)
transformers.FunnelForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelForPreTrainingOutput(ModelOutput)
transformers.FunnelForQuestionAnswering(self,config)
transformers.FunnelForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelForSequenceClassification(self,config)
transformers.FunnelForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelForTokenClassification(self,config)
transformers.FunnelForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelModel(self,config)
transformers.FunnelModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FunnelModel.get_input_embeddings(self)
transformers.FunnelModel.set_input_embeddings(self,new_embeddings)
transformers.FunnelPreTrainedModel(PreTrainedModel)
transformers.FunnelPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_funnel(model,config,tf_checkpoint_path)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure(self,config)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.get_position_embeds(self,seq_len,dtype,device)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.init_attention_inputs(self,inputs_embeds,attention_mask=None,token_type_ids=None)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.pool_tensor(self,tensor,mode='mean',stride=2)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.post_attention_pooling(self,attention_inputs)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.pre_attention_pooling(self,output,attention_inputs)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.relative_pos(self,pos,stride,pooled_pos=None,shift=1)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.stride_pool(self,tensor,axis)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.stride_pool_pos(self,pos_id,block_index)
transformers.models.funnel.modeling_funnel.FunnelAttentionStructure.token_type_ids_to_mat(self,token_type_ids)
transformers.models.funnel.modeling_funnel.FunnelBaseModel(self,config)
transformers.models.funnel.modeling_funnel.FunnelBaseModel.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelBaseModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelBaseModel.get_input_embeddings(self)
transformers.models.funnel.modeling_funnel.FunnelBaseModel.set_input_embeddings(self,new_embeddings)
transformers.models.funnel.modeling_funnel.FunnelClassificationHead(self,config,n_labels)
transformers.models.funnel.modeling_funnel.FunnelClassificationHead.__init__(self,config,n_labels)
transformers.models.funnel.modeling_funnel.FunnelClassificationHead.forward(self,hidden)
transformers.models.funnel.modeling_funnel.FunnelDecoder(self,config)
transformers.models.funnel.modeling_funnel.FunnelDecoder.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelDecoder.forward(self,final_hidden,first_block_hidden,attention_mask=None,token_type_ids=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.funnel.modeling_funnel.FunnelDiscriminatorPredictions(self,config)
transformers.models.funnel.modeling_funnel.FunnelDiscriminatorPredictions.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelDiscriminatorPredictions.forward(self,discriminator_hidden_states)
transformers.models.funnel.modeling_funnel.FunnelEmbeddings(self,config)
transformers.models.funnel.modeling_funnel.FunnelEmbeddings.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelEmbeddings.forward(self,input_ids=None,inputs_embeds=None)
transformers.models.funnel.modeling_funnel.FunnelEncoder(self,config)
transformers.models.funnel.modeling_funnel.FunnelEncoder.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelEncoder.forward(self,inputs_embeds,attention_mask=None,token_type_ids=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.funnel.modeling_funnel.FunnelForMaskedLM(self,config)
transformers.models.funnel.modeling_funnel.FunnelForMaskedLM.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelForMaskedLM.get_output_embeddings(self)
transformers.models.funnel.modeling_funnel.FunnelForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.funnel.modeling_funnel.FunnelForMultipleChoice(self,config)
transformers.models.funnel.modeling_funnel.FunnelForMultipleChoice.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelForPreTraining(self,config)
transformers.models.funnel.modeling_funnel.FunnelForPreTraining.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput(ModelOutput)
transformers.models.funnel.modeling_funnel.FunnelForQuestionAnswering(self,config)
transformers.models.funnel.modeling_funnel.FunnelForQuestionAnswering.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelForSequenceClassification(self,config)
transformers.models.funnel.modeling_funnel.FunnelForSequenceClassification.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelForTokenClassification(self,config)
transformers.models.funnel.modeling_funnel.FunnelForTokenClassification.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelLayer(self,config,block_index)
transformers.models.funnel.modeling_funnel.FunnelLayer.__init__(self,config,block_index)
transformers.models.funnel.modeling_funnel.FunnelLayer.forward(self,query,key,value,attention_inputs,output_attentions=False)
transformers.models.funnel.modeling_funnel.FunnelModel(self,config)
transformers.models.funnel.modeling_funnel.FunnelModel.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.funnel.modeling_funnel.FunnelModel.get_input_embeddings(self)
transformers.models.funnel.modeling_funnel.FunnelModel.set_input_embeddings(self,new_embeddings)
transformers.models.funnel.modeling_funnel.FunnelPositionwiseFFN(self,config)
transformers.models.funnel.modeling_funnel.FunnelPositionwiseFFN.__init__(self,config)
transformers.models.funnel.modeling_funnel.FunnelPositionwiseFFN.forward(self,hidden)
transformers.models.funnel.modeling_funnel.FunnelPreTrainedModel(PreTrainedModel)
transformers.models.funnel.modeling_funnel.FunnelPreTrainedModel._init_weights(self,module)
transformers.models.funnel.modeling_funnel.FunnelRelMultiheadAttention(self,config,block_index)
transformers.models.funnel.modeling_funnel.FunnelRelMultiheadAttention.__init__(self,config,block_index)
transformers.models.funnel.modeling_funnel.FunnelRelMultiheadAttention.forward(self,query,key,value,attention_inputs,output_attentions=False)
transformers.models.funnel.modeling_funnel.FunnelRelMultiheadAttention.relative_positional_attention(self,position_embeds,q_head,context_len,cls_mask=None)
transformers.models.funnel.modeling_funnel.FunnelRelMultiheadAttention.relative_token_type_attention(self,token_type_mat,q_head,cls_mask=None)
transformers.models.funnel.modeling_funnel._relative_shift_gather(positional_attn,context_len,shift)
transformers.models.funnel.modeling_funnel.load_tf_weights_in_funnel(model,config,tf_checkpoint_path)
transformers.models.funnel.modeling_funnel.upsample(x,stride,target_len,separate_cls=True,truncate_seq=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/__init__.py----------------------------------------
A:transformers.models.funnel.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/funnel/tokenization_funnel.py----------------------------------------
A:transformers.models.funnel.tokenization_funnel.logger->utils.logging.get_logger(__name__)
transformers.FunnelTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',bos_token='<s>',eos_token='</s>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.FunnelTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.funnel.tokenization_funnel.FunnelTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',bos_token='<s>',eos_token='</s>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.funnel.tokenization_funnel.FunnelTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',bos_token='<s>',eos_token='</s>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.funnel.tokenization_funnel.FunnelTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta/configuration_deberta.py----------------------------------------
A:transformers.models.deberta.configuration_deberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.deberta.configuration_deberta.self.pooler_hidden_size->kwargs.get('pooler_hidden_size', hidden_size)
transformers.DebertaConfig(self,vocab_size=50265,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=0,initializer_range=0.02,layer_norm_eps=1e-07,relative_attention=False,max_relative_positions=-1,pad_token_id=0,position_biased_input=True,pos_att_type=None,pooler_dropout=0,pooler_hidden_act='gelu',**kwargs)
transformers.models.deberta.configuration_deberta.DebertaConfig(self,vocab_size=50265,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=0,initializer_range=0.02,layer_norm_eps=1e-07,relative_attention=False,max_relative_positions=-1,pad_token_id=0,position_biased_input=True,pos_att_type=None,pooler_dropout=0,pooler_hidden_act='gelu',**kwargs)
transformers.models.deberta.configuration_deberta.DebertaConfig.__init__(self,vocab_size=50265,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=0,initializer_range=0.02,layer_norm_eps=1e-07,relative_attention=False,max_relative_positions=-1,pad_token_id=0,position_biased_input=True,pos_att_type=None,pooler_dropout=0,pooler_hidden_act='gelu',**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta/tokenization_deberta_fast.py----------------------------------------
A:transformers.models.deberta.tokenization_deberta_fast.logger->utils.logging.get_logger(__name__)
transformers.DebertaTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,errors='replace',bos_token='[CLS]',eos_token='[SEP]',sep_token='[SEP]',cls_token='[CLS]',unk_token='[UNK]',pad_token='[PAD]',mask_token='[MASK]',add_prefix_space=False,**kwargs)
transformers.DebertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.DebertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.DebertaTokenizerFast.mask_token(self)->str
transformers.DebertaTokenizerFast.mask_token(self,value)
transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,errors='replace',bos_token='[CLS]',eos_token='[SEP]',sep_token='[SEP]',cls_token='[CLS]',unk_token='[UNK]',pad_token='[PAD]',mask_token='[MASK]',add_prefix_space=False,**kwargs)
transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast.__init__(self,vocab_file,merges_file,tokenizer_file=None,errors='replace',bos_token='[CLS]',eos_token='[SEP]',sep_token='[SEP]',cls_token='[CLS]',unk_token='[UNK]',pad_token='[PAD]',mask_token='[MASK]',add_prefix_space=False,**kwargs)
transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast.mask_token(self)->str
transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast.mask_token(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta/tokenization_deberta.py----------------------------------------
A:transformers.models.deberta.tokenization_deberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.deberta.tokenization_deberta.add_prefix_space->kwargs.pop('add_prefix_space', self.add_prefix_space)
transformers.DebertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='[CLS]',eos_token='[SEP]',sep_token='[SEP]',cls_token='[CLS]',unk_token='[UNK]',pad_token='[PAD]',mask_token='[MASK]',add_prefix_space=False,**kwargs)
transformers.DebertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.DebertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.DebertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.DebertaTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.models.deberta.tokenization_deberta.DebertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='[CLS]',eos_token='[SEP]',sep_token='[SEP]',cls_token='[CLS]',unk_token='[UNK]',pad_token='[PAD]',mask_token='[MASK]',add_prefix_space=False,**kwargs)
transformers.models.deberta.tokenization_deberta.DebertaTokenizer.__init__(self,vocab_file,merges_file,errors='replace',bos_token='[CLS]',eos_token='[SEP]',sep_token='[SEP]',cls_token='[CLS]',unk_token='[UNK]',pad_token='[PAD]',mask_token='[MASK]',add_prefix_space=False,**kwargs)
transformers.models.deberta.tokenization_deberta.DebertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.deberta.tokenization_deberta.DebertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.deberta.tokenization_deberta.DebertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.deberta.tokenization_deberta.DebertaTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta/__init__.py----------------------------------------
A:transformers.models.deberta.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta/modeling_deberta.py----------------------------------------
A:transformers.models.deberta.modeling_deberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.deberta.modeling_deberta.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.deberta.modeling_deberta.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.deberta.modeling_deberta.context_token->self.dropout(context_token)
A:transformers.models.deberta.modeling_deberta.pooled_output->self.dropout(pooled_output)
A:transformers.models.deberta.modeling_deberta.output->torch.softmax(output, self.dim)
A:transformers.models.deberta.modeling_deberta.inputGrad->_softmax_backward_data(grad_output, output, self.dim, output)
A:transformers.models.deberta.modeling_deberta.mask->mask.to(embeddings.dtype).to(embeddings.dtype)
A:transformers.models.deberta.modeling_deberta.(mask, dropout)->get_mask(input, local_ctx)
A:transformers.models.deberta.modeling_deberta.self.weight->torch.nn.Parameter(torch.ones(size))
A:transformers.models.deberta.modeling_deberta.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.deberta.modeling_deberta.hidden_states->self.decoder(hidden_states)
A:transformers.models.deberta.modeling_deberta.mean->self.decoder(hidden_states).mean(-1, keepdim=True)
A:transformers.models.deberta.modeling_deberta.variance->(hidden_states - mean).pow(2).mean(-1, keepdim=True)
A:transformers.models.deberta.modeling_deberta.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.deberta.modeling_deberta.self.self->DisentangledSelfAttention(config)
A:transformers.models.deberta.modeling_deberta.self.output->DebertaOutput(config)
A:transformers.models.deberta.modeling_deberta.self_output->self.self(hidden_states, attention_mask, return_att, query_states=query_states, relative_pos=relative_pos, rel_embeddings=rel_embeddings)
A:transformers.models.deberta.modeling_deberta.attention_output->self.attention(hidden_states, attention_mask, return_att=return_att, query_states=query_states, relative_pos=relative_pos, rel_embeddings=rel_embeddings)
A:transformers.models.deberta.modeling_deberta.self.attention->DebertaAttention(config)
A:transformers.models.deberta.modeling_deberta.self.intermediate->DebertaIntermediate(config)
A:transformers.models.deberta.modeling_deberta.intermediate_output->self.intermediate(attention_output)
A:transformers.models.deberta.modeling_deberta.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.deberta.modeling_deberta.self.layer->torch.nn.ModuleList([DebertaLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.deberta.modeling_deberta.self.relative_attention->getattr(config, 'relative_attention', False)
A:transformers.models.deberta.modeling_deberta.self.max_relative_positions->getattr(config, 'max_relative_positions', -1)
A:transformers.models.deberta.modeling_deberta.self.rel_embeddings->torch.nn.Embedding(self.max_relative_positions * 2, config.hidden_size)
A:transformers.models.deberta.modeling_deberta.extended_attention_mask->self.encoder.get_attention_mask(attention_mask).unsqueeze(1).unsqueeze(2)
A:transformers.models.deberta.modeling_deberta.attention_mask->self.encoder.get_attention_mask(attention_mask)
A:transformers.models.deberta.modeling_deberta.relative_pos->relative_pos.long().to(query_layer.device).long().to(query_layer.device)
A:transformers.models.deberta.modeling_deberta.rel_embeddings->self.encoder.get_rel_embedding()
A:transformers.models.deberta.modeling_deberta.q_ids->torch.arange(query_size, dtype=torch.long, device=device)
A:transformers.models.deberta.modeling_deberta.k_ids->torch.arange(key_size, dtype=torch.long, device=device)
A:transformers.models.deberta.modeling_deberta.rel_pos_ids->rel_pos_ids.unsqueeze(0).unsqueeze(0)
A:transformers.models.deberta.modeling_deberta.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.deberta.modeling_deberta.self.in_proj->torch.nn.Linear(config.hidden_size, self.all_head_size * 3, bias=False)
A:transformers.models.deberta.modeling_deberta.self.q_bias->torch.nn.Parameter(torch.zeros(self.all_head_size, dtype=torch.float))
A:transformers.models.deberta.modeling_deberta.self.v_bias->torch.nn.Parameter(torch.zeros(self.all_head_size, dtype=torch.float))
A:transformers.models.deberta.modeling_deberta.self.talking_head->getattr(config, 'talking_head', False)
A:transformers.models.deberta.modeling_deberta.self.head_logits_proj->torch.nn.Linear(config.num_attention_heads, config.num_attention_heads, bias=False)
A:transformers.models.deberta.modeling_deberta.self.head_weights_proj->torch.nn.Linear(config.num_attention_heads, config.num_attention_heads, bias=False)
A:transformers.models.deberta.modeling_deberta.self.pos_dropout->StableDropout(config.hidden_dropout_prob)
A:transformers.models.deberta.modeling_deberta.self.pos_proj->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=False)
A:transformers.models.deberta.modeling_deberta.self.pos_q_proj->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.deberta.modeling_deberta.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.deberta.modeling_deberta.qp->self.in_proj(hidden_states)
A:transformers.models.deberta.modeling_deberta.(query_layer, key_layer, value_layer)->self.transpose_for_scores(qp).chunk(3, dim=-1)
A:transformers.models.deberta.modeling_deberta.ws->self.in_proj.weight.chunk(self.num_attention_heads * 3, dim=0)
A:transformers.models.deberta.modeling_deberta.q->query_layer.size(-2)
A:transformers.models.deberta.modeling_deberta.scale->math.sqrt(query_layer.size(-1) * scale_factor)
A:transformers.models.deberta.modeling_deberta.attention_scores->self.head_logits_proj(attention_scores.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
A:transformers.models.deberta.modeling_deberta.rel_att->self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
A:transformers.models.deberta.modeling_deberta.attention_probs->self.head_weights_proj(attention_probs.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
A:transformers.models.deberta.modeling_deberta.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.deberta.modeling_deberta.att_span->min(max(query_layer.size(-2), key_layer.size(-2)), self.max_relative_positions)
A:transformers.models.deberta.modeling_deberta.pos_key_layer->self.transpose_for_scores(pos_key_layer)
A:transformers.models.deberta.modeling_deberta.pos_query_layer->self.transpose_for_scores(pos_query_layer)
A:transformers.models.deberta.modeling_deberta.c2p_att->torch.gather(c2p_att, dim=-1, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))
A:transformers.models.deberta.modeling_deberta.c2p_pos->torch.clamp(relative_pos + att_span, 0, att_span * 2 - 1)
A:transformers.models.deberta.modeling_deberta.r_pos->build_relative_position(key_layer.size(-2), key_layer.size(-2), query_layer.device)
A:transformers.models.deberta.modeling_deberta.p2c_pos->torch.clamp(-r_pos + att_span, 0, att_span * 2 - 1)
A:transformers.models.deberta.modeling_deberta.pos_index->relative_pos[:, :, :, 0].unsqueeze(-1)
A:transformers.models.deberta.modeling_deberta.p2c_att->torch.gather(p2c_att, dim=-2, index=pos_dynamic_expand(pos_index, p2c_att, key_layer))
A:transformers.models.deberta.modeling_deberta.pad_token_id->getattr(config, 'pad_token_id', 0)
A:transformers.models.deberta.modeling_deberta.self.embedding_size->getattr(config, 'embedding_size', config.hidden_size)
A:transformers.models.deberta.modeling_deberta.self.word_embeddings->torch.nn.Embedding(config.vocab_size, self.embedding_size, padding_idx=pad_token_id)
A:transformers.models.deberta.modeling_deberta.self.position_biased_input->getattr(config, 'position_biased_input', True)
A:transformers.models.deberta.modeling_deberta.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, self.embedding_size)
A:transformers.models.deberta.modeling_deberta.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, self.embedding_size)
A:transformers.models.deberta.modeling_deberta.self.embed_proj->torch.nn.Linear(self.embedding_size, config.hidden_size, bias=False)
A:transformers.models.deberta.modeling_deberta.input_shape->input_ids.size()
A:transformers.models.deberta.modeling_deberta.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.deberta.modeling_deberta.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.deberta.modeling_deberta.position_embeddings->torch.zeros_like(inputs_embeds)
A:transformers.models.deberta.modeling_deberta.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.deberta.modeling_deberta.embeddings->self.dropout(embeddings)
A:transformers.models.deberta.modeling_deberta.self_state->self.state_dict()
A:transformers.models.deberta.modeling_deberta.self.embeddings->DebertaEmbeddings(config)
A:transformers.models.deberta.modeling_deberta.self.encoder->DebertaEncoder(config)
A:transformers.models.deberta.modeling_deberta.embedding_output->self.embeddings(input_ids=input_ids, token_type_ids=token_type_ids, position_ids=position_ids, mask=attention_mask, inputs_embeds=inputs_embeds)
A:transformers.models.deberta.modeling_deberta.encoder_outputs->self.encoder(embedding_output, attention_mask, output_hidden_states=True, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.models.deberta.modeling_deberta.rel_pos->self.encoder.get_rel_pos(embedding_output)
A:transformers.models.deberta.modeling_deberta.query_states->layer(hidden_states, attention_mask, return_att=False, query_states=query_states, relative_pos=rel_pos, rel_embeddings=rel_embeddings)
A:transformers.models.deberta.modeling_deberta.self.deberta->DebertaModel(config)
A:transformers.models.deberta.modeling_deberta.self.cls->DebertaOnlyMLMHead(config)
A:transformers.models.deberta.modeling_deberta.outputs->self.deberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.deberta.modeling_deberta.prediction_scores->self.predictions(sequence_output)
A:transformers.models.deberta.modeling_deberta.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.deberta.modeling_deberta.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.deberta.modeling_deberta.self.transform->DebertaPredictionHeadTransform(config)
A:transformers.models.deberta.modeling_deberta.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.deberta.modeling_deberta.self.predictions->DebertaLMPredictionHead(config)
A:transformers.models.deberta.modeling_deberta.num_labels->getattr(config, 'num_labels', 2)
A:transformers.models.deberta.modeling_deberta.self.pooler->ContextPooler(config)
A:transformers.models.deberta.modeling_deberta.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.deberta.modeling_deberta.drop_out->getattr(config, 'cls_dropout', None)
A:transformers.models.deberta.modeling_deberta.logits->self.qa_outputs(sequence_output)
A:transformers.models.deberta.modeling_deberta.loss_fn->torch.nn.MSELoss()
A:transformers.models.deberta.modeling_deberta.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.deberta.modeling_deberta.label_index->(labels >= 0).nonzero()
A:transformers.models.deberta.modeling_deberta.labels->torch.gather(labels, 0, label_index.view(-1))
A:transformers.models.deberta.modeling_deberta.labeled_logits->torch.gather(logits, 0, label_index.expand(label_index.size(0), logits.size(1)))
A:transformers.models.deberta.modeling_deberta.log_softmax->torch.nn.LogSoftmax(-1)
A:transformers.models.deberta.modeling_deberta.sequence_output->self.dropout(sequence_output)
A:transformers.models.deberta.modeling_deberta.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.deberta.modeling_deberta.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.deberta.modeling_deberta.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.deberta.modeling_deberta.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.deberta.modeling_deberta.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.deberta.modeling_deberta.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.deberta.modeling_deberta.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.deberta.modeling_deberta.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.deberta.modeling_deberta.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.deberta.modeling_deberta.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.deberta.modeling_deberta.end_loss->loss_fct(end_logits, end_positions)
transformers.DebertaForMaskedLM(self,config)
transformers.DebertaForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaForMaskedLM.get_output_embeddings(self)
transformers.DebertaForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.DebertaForQuestionAnswering(self,config)
transformers.DebertaForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaForSequenceClassification(self,config)
transformers.DebertaForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaForSequenceClassification.get_input_embeddings(self)
transformers.DebertaForSequenceClassification.set_input_embeddings(self,new_embeddings)
transformers.DebertaForTokenClassification(self,config)
transformers.DebertaForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaModel(self,config)
transformers.DebertaModel._prune_heads(self,heads_to_prune)
transformers.DebertaModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaModel.get_input_embeddings(self)
transformers.DebertaModel.set_input_embeddings(self,new_embeddings)
transformers.DebertaPreTrainedModel(self,config)
transformers.DebertaPreTrainedModel._init_weights(self,module)
transformers.DebertaPreTrainedModel._pre_load_hook(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
transformers.models.deberta.modeling_deberta.ContextPooler(self,config)
transformers.models.deberta.modeling_deberta.ContextPooler.__init__(self,config)
transformers.models.deberta.modeling_deberta.ContextPooler.forward(self,hidden_states)
transformers.models.deberta.modeling_deberta.ContextPooler.output_dim(self)
transformers.models.deberta.modeling_deberta.DebertaAttention(self,config)
transformers.models.deberta.modeling_deberta.DebertaAttention.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaAttention.forward(self,hidden_states,attention_mask,return_att=False,query_states=None,relative_pos=None,rel_embeddings=None)
transformers.models.deberta.modeling_deberta.DebertaEmbeddings(self,config)
transformers.models.deberta.modeling_deberta.DebertaEmbeddings.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,mask=None,inputs_embeds=None)
transformers.models.deberta.modeling_deberta.DebertaEncoder(self,config)
transformers.models.deberta.modeling_deberta.DebertaEncoder.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaEncoder.forward(self,hidden_states,attention_mask,output_hidden_states=True,output_attentions=False,query_states=None,relative_pos=None,return_dict=True)
transformers.models.deberta.modeling_deberta.DebertaEncoder.get_attention_mask(self,attention_mask)
transformers.models.deberta.modeling_deberta.DebertaEncoder.get_rel_embedding(self)
transformers.models.deberta.modeling_deberta.DebertaEncoder.get_rel_pos(self,hidden_states,query_states=None,relative_pos=None)
transformers.models.deberta.modeling_deberta.DebertaForMaskedLM(self,config)
transformers.models.deberta.modeling_deberta.DebertaForMaskedLM.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta.modeling_deberta.DebertaForMaskedLM.get_output_embeddings(self)
transformers.models.deberta.modeling_deberta.DebertaForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.deberta.modeling_deberta.DebertaForQuestionAnswering(self,config)
transformers.models.deberta.modeling_deberta.DebertaForQuestionAnswering.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification(self,config)
transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification.get_input_embeddings(self)
transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification.set_input_embeddings(self,new_embeddings)
transformers.models.deberta.modeling_deberta.DebertaForTokenClassification(self,config)
transformers.models.deberta.modeling_deberta.DebertaForTokenClassification.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta.modeling_deberta.DebertaIntermediate(self,config)
transformers.models.deberta.modeling_deberta.DebertaIntermediate.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaIntermediate.forward(self,hidden_states)
transformers.models.deberta.modeling_deberta.DebertaLMPredictionHead(self,config)
transformers.models.deberta.modeling_deberta.DebertaLMPredictionHead.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaLMPredictionHead.forward(self,hidden_states)
transformers.models.deberta.modeling_deberta.DebertaLayer(self,config)
transformers.models.deberta.modeling_deberta.DebertaLayer.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaLayer.forward(self,hidden_states,attention_mask,return_att=False,query_states=None,relative_pos=None,rel_embeddings=None)
transformers.models.deberta.modeling_deberta.DebertaLayerNorm(self,size,eps=1e-12)
transformers.models.deberta.modeling_deberta.DebertaLayerNorm.__init__(self,size,eps=1e-12)
transformers.models.deberta.modeling_deberta.DebertaLayerNorm.forward(self,hidden_states)
transformers.models.deberta.modeling_deberta.DebertaModel(self,config)
transformers.models.deberta.modeling_deberta.DebertaModel.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaModel._prune_heads(self,heads_to_prune)
transformers.models.deberta.modeling_deberta.DebertaModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta.modeling_deberta.DebertaModel.get_input_embeddings(self)
transformers.models.deberta.modeling_deberta.DebertaModel.set_input_embeddings(self,new_embeddings)
transformers.models.deberta.modeling_deberta.DebertaOnlyMLMHead(self,config)
transformers.models.deberta.modeling_deberta.DebertaOnlyMLMHead.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaOnlyMLMHead.forward(self,sequence_output)
transformers.models.deberta.modeling_deberta.DebertaOutput(self,config)
transformers.models.deberta.modeling_deberta.DebertaOutput.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaOutput.forward(self,hidden_states,input_tensor)
transformers.models.deberta.modeling_deberta.DebertaPreTrainedModel(self,config)
transformers.models.deberta.modeling_deberta.DebertaPreTrainedModel.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaPreTrainedModel._init_weights(self,module)
transformers.models.deberta.modeling_deberta.DebertaPreTrainedModel._pre_load_hook(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
transformers.models.deberta.modeling_deberta.DebertaPredictionHeadTransform(self,config)
transformers.models.deberta.modeling_deberta.DebertaPredictionHeadTransform.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaPredictionHeadTransform.forward(self,hidden_states)
transformers.models.deberta.modeling_deberta.DebertaSelfOutput(self,config)
transformers.models.deberta.modeling_deberta.DebertaSelfOutput.__init__(self,config)
transformers.models.deberta.modeling_deberta.DebertaSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.deberta.modeling_deberta.DisentangledSelfAttention(self,config)
transformers.models.deberta.modeling_deberta.DisentangledSelfAttention.__init__(self,config)
transformers.models.deberta.modeling_deberta.DisentangledSelfAttention.disentangled_att_bias(self,query_layer,key_layer,relative_pos,rel_embeddings,scale_factor)
transformers.models.deberta.modeling_deberta.DisentangledSelfAttention.forward(self,hidden_states,attention_mask,return_att=False,query_states=None,relative_pos=None,rel_embeddings=None)
transformers.models.deberta.modeling_deberta.DisentangledSelfAttention.transpose_for_scores(self,x)
transformers.models.deberta.modeling_deberta.DropoutContext(self)
transformers.models.deberta.modeling_deberta.DropoutContext.__init__(self)
transformers.models.deberta.modeling_deberta.StableDropout(self,drop_prob)
transformers.models.deberta.modeling_deberta.StableDropout.__init__(self,drop_prob)
transformers.models.deberta.modeling_deberta.StableDropout.clear_context(self)
transformers.models.deberta.modeling_deberta.StableDropout.forward(self,x)
transformers.models.deberta.modeling_deberta.StableDropout.get_context(self)
transformers.models.deberta.modeling_deberta.StableDropout.init_context(self,reuse_mask=True,scale=1)
transformers.models.deberta.modeling_deberta.XDropout(torch.autograd.Function)
transformers.models.deberta.modeling_deberta.XDropout.backward(ctx,grad_output)
transformers.models.deberta.modeling_deberta.XDropout.forward(ctx,input,local_ctx)
transformers.models.deberta.modeling_deberta.XSoftmax(torch.autograd.Function)
transformers.models.deberta.modeling_deberta.XSoftmax.backward(self,grad_output)
transformers.models.deberta.modeling_deberta.XSoftmax.forward(self,input,mask,dim)
transformers.models.deberta.modeling_deberta.build_relative_position(query_size,key_size,device)
transformers.models.deberta.modeling_deberta.c2p_dynamic_expand(c2p_pos,query_layer,relative_pos)
transformers.models.deberta.modeling_deberta.get_mask(input,local_context)
transformers.models.deberta.modeling_deberta.p2c_dynamic_expand(c2p_pos,query_layer,key_layer)
transformers.models.deberta.modeling_deberta.pos_dynamic_expand(pos_index,p2c_att,key_layer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/prophetnet/configuration_prophetnet.py----------------------------------------
A:transformers.models.prophetnet.configuration_prophetnet.logger->utils.logging.get_logger(__name__)
transformers.ProphetNetConfig(self,activation_dropout=0.1,activation_function='gelu',vocab_size=30522,hidden_size=1024,encoder_ffn_dim=4096,num_encoder_layers=12,num_encoder_attention_heads=16,decoder_ffn_dim=4096,num_decoder_layers=12,num_decoder_attention_heads=16,attention_dropout=0.1,dropout=0.1,max_position_embeddings=512,init_std=0.02,is_encoder_decoder=True,add_cross_attention=True,decoder_start_token_id=0,ngram=2,num_buckets=32,relative_max_distance=128,disable_ngram_loss=False,gradient_checkpointing=False,eps=0.0,use_cache=True,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.ProphetNetConfig.num_attention_heads(self)->int
transformers.ProphetNetConfig.num_hidden_layers(self)->int
transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig(self,activation_dropout=0.1,activation_function='gelu',vocab_size=30522,hidden_size=1024,encoder_ffn_dim=4096,num_encoder_layers=12,num_encoder_attention_heads=16,decoder_ffn_dim=4096,num_decoder_layers=12,num_decoder_attention_heads=16,attention_dropout=0.1,dropout=0.1,max_position_embeddings=512,init_std=0.02,is_encoder_decoder=True,add_cross_attention=True,decoder_start_token_id=0,ngram=2,num_buckets=32,relative_max_distance=128,disable_ngram_loss=False,gradient_checkpointing=False,eps=0.0,use_cache=True,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig.__init__(self,activation_dropout=0.1,activation_function='gelu',vocab_size=30522,hidden_size=1024,encoder_ffn_dim=4096,num_encoder_layers=12,num_encoder_attention_heads=16,decoder_ffn_dim=4096,num_decoder_layers=12,num_decoder_attention_heads=16,attention_dropout=0.1,dropout=0.1,max_position_embeddings=512,init_std=0.02,is_encoder_decoder=True,add_cross_attention=True,decoder_start_token_id=0,ngram=2,num_buckets=32,relative_max_distance=128,disable_ngram_loss=False,gradient_checkpointing=False,eps=0.0,use_cache=True,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig.num_attention_heads(self)->int
transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig.num_hidden_layers(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/prophetnet/modeling_prophetnet.py----------------------------------------
A:transformers.models.prophetnet.modeling_prophetnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.prophetnet.modeling_prophetnet.right_block->left_block.detach().clone()
A:transformers.models.prophetnet.modeling_prophetnet.inv_relative_positions->torch.max(inv_relative_positions, torch.zeros_like(inv_relative_positions))
A:transformers.models.prophetnet.modeling_prophetnet.is_small->torch.lt(inv_relative_positions, max_exact)
A:transformers.models.prophetnet.modeling_prophetnet.val_if_large->torch.min(val_if_large, torch.ones_like(val_if_large) * (num_buckets - 1)).int()
A:transformers.models.prophetnet.modeling_prophetnet.main_stream_relative_positions->torch.arange(1, self.max_target_positions).to(position_ids.device).repeat(1, 1).unsqueeze(1).repeat(1, position_ids.size(-1), 1)
A:transformers.models.prophetnet.modeling_prophetnet.predicting_stream_relative_positions->predicting_stream_relative_positions.repeat(1, position_ids.size(-1), 1).repeat(1, position_ids.size(-1), 1)
A:transformers.models.prophetnet.modeling_prophetnet.main_relative_position_buckets->main_relative_position_buckets.repeat(1, self.num_attn_heads, 1).view(-1, main_relative_position_buckets.shape[-1]).long().repeat(1, self.num_attn_heads, 1).view(-1, main_relative_position_buckets.shape[-1]).long()
A:transformers.models.prophetnet.modeling_prophetnet.predict_relative_position_buckets->predict_relative_position_buckets.view(-1, predict_relative_position_buckets.size(-1)).long().view(-1, predict_relative_position_buckets.size(-1)).long()
A:transformers.models.prophetnet.modeling_prophetnet.shifted_input_ids->input_ids.new_zeros(input_ids.shape)
A:transformers.models.prophetnet.modeling_prophetnet.shifted_input_ids[..., 1:]->input_ids[..., :-1].clone()
A:transformers.models.prophetnet.modeling_prophetnet.attention_mask->input_ids.new_ones(input_ids.shape)
A:transformers.models.prophetnet.modeling_prophetnet.position_ids->torch.arange(1, self.max_target_positions).to(position_ids.device).repeat(1, 1)
A:transformers.models.prophetnet.modeling_prophetnet.self.key_proj->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.self.value_proj->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.self.query_proj->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.self.out_proj->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.(batch_size, tgt_len, hidden_size)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.prophetnet.modeling_prophetnet.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.prophetnet.modeling_prophetnet.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.prophetnet.modeling_prophetnet.query_states->query_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.prophetnet.modeling_prophetnet.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.prophetnet.modeling_prophetnet.attn_weights->attn_weights.view(batch_size * self.num_attn_heads, tgt_len, src_len).view(batch_size * self.num_attn_heads, tgt_len, src_len)
A:transformers.models.prophetnet.modeling_prophetnet.attn_weights_reshaped->attn_weights.view(batch_size * self.num_attn_heads, tgt_len, src_len).view(batch_size * self.num_attn_heads, tgt_len, src_len).view(batch_size, self.num_attn_heads, tgt_len, src_len)
A:transformers.models.prophetnet.modeling_prophetnet.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
A:transformers.models.prophetnet.modeling_prophetnet.attn_output->torch.nn.functional.dropout(attn_output, p=self.dropout, training=self.training)
A:transformers.models.prophetnet.modeling_prophetnet.self.intermediate->torch.nn.Linear(config.hidden_size, ffn_dim)
A:transformers.models.prophetnet.modeling_prophetnet.self.output->torch.nn.Linear(ffn_dim, config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.hidden_states->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
A:transformers.models.prophetnet.modeling_prophetnet.self.relative_pos_embeddings->torch.nn.Linear(config.hidden_size, self.num_buckets * self.num_attn_heads)
A:transformers.models.prophetnet.modeling_prophetnet.(batch_size, ngram_sequence_length, hidden_size)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.prophetnet.modeling_prophetnet.hidden_states_list->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).chunk(1 + self.ngram, dim=1)
A:transformers.models.prophetnet.modeling_prophetnet.query_states_list->query_states.view(*proj_shape).view(*proj_shape).chunk(1 + self.ngram, dim=1)
A:transformers.models.prophetnet.modeling_prophetnet.key_states_list->key_states.view(*proj_shape).view(*proj_shape).chunk(1 + self.ngram, dim=1)
A:transformers.models.prophetnet.modeling_prophetnet.value_states_list->value_states.view(*proj_shape).view(*proj_shape).chunk(1 + self.ngram, dim=1)
A:transformers.models.prophetnet.modeling_prophetnet.prev_main_key_states->past_key_value[0].view(batch_size * self.num_attn_heads, -1, self.head_dim)
A:transformers.models.prophetnet.modeling_prophetnet.main_key_states->torch.cat((prev_main_key_states, main_key_states), dim=1)
A:transformers.models.prophetnet.modeling_prophetnet.prev_main_value_states->past_key_value[1].view(batch_size * self.num_attn_heads, -1, self.head_dim)
A:transformers.models.prophetnet.modeling_prophetnet.main_value_states->torch.cat((prev_main_value_states, main_value_states), dim=1)
A:transformers.models.prophetnet.modeling_prophetnet.main_attn_weights->torch.bmm(main_query_states, main_key_states.transpose(1, 2))
A:transformers.models.prophetnet.modeling_prophetnet.main_relative_pos_embeddings->torch.gather(rel_pos_embeddings, dim=1, index=main_relative_position_buckets).view(attn_weights.shape[:2] + (-1,))
A:transformers.models.prophetnet.modeling_prophetnet.main_attn_probs->main_attn_probs.view(batch_size, self.num_attn_heads, sequence_length, -1).view(batch_size, self.num_attn_heads, sequence_length, -1)
A:transformers.models.prophetnet.modeling_prophetnet.main_attn_output->self.out_proj(main_attn_output)
A:transformers.models.prophetnet.modeling_prophetnet.predict_query_states->torch.cat(predict_query_states_list, 0).view(self.ngram, -1, sequence_length, self.head_dim)
A:transformers.models.prophetnet.modeling_prophetnet.predict_key_states->torch.cat([torch.cat([main_key_states, key], 1).unsqueeze(0) for key in predict_key_states_list], 0)
A:transformers.models.prophetnet.modeling_prophetnet.predict_hidden_states->torch.cat(hidden_states_predict_list, 0).view(self.ngram, sequence_length, batch_size, hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.predict_value_states->torch.cat([torch.cat([main_value_states, v_p], 1).unsqueeze(0) for v_p in predict_value_states_list], 0)
A:transformers.models.prophetnet.modeling_prophetnet.predict_attn_weights->torch.einsum('nbtc,nbsc->nbts', (predict_query_states, predict_key_states))
A:transformers.models.prophetnet.modeling_prophetnet.predict_relative_pos_embeddings->torch.gather(rel_pos_embeddings, dim=1, index=predict_relative_position_buckets).view(self.ngram, batch_size * self.num_attn_heads, sequence_length, -1)
A:transformers.models.prophetnet.modeling_prophetnet.predict_attn_probs->predict_attn_probs.view(self.ngram, batch_size, self.num_attn_heads, sequence_length, -1).transpose(0, 1).view(self.ngram, batch_size, self.num_attn_heads, sequence_length, -1).transpose(0, 1)
A:transformers.models.prophetnet.modeling_prophetnet.predict_attn_output->self.out_proj(predict_attn_output)
A:transformers.models.prophetnet.modeling_prophetnet.relative_positions->torch.arange(0, key_sequence_length).unsqueeze(0).unsqueeze(0).repeat(batch_size, sequence_length, 1).to(position_ids.device)
A:transformers.models.prophetnet.modeling_prophetnet.rel_pos_embeddings->rel_pos_embeddings.reshape(-1, rel_pos_embeddings.size(-1)).reshape(-1, rel_pos_embeddings.size(-1))
A:transformers.models.prophetnet.modeling_prophetnet.self.self_attn->ProphetNetNgramSelfAttention(config)
A:transformers.models.prophetnet.modeling_prophetnet.self.self_attn_layer_norm->LayerNorm(config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.self.feed_forward->ProphetNetFeedForward(config, config.decoder_ffn_dim)
A:transformers.models.prophetnet.modeling_prophetnet.self.feed_forward_layer_norm->LayerNorm(config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.(attention_output, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.prophetnet.modeling_prophetnet.feed_forward_output->self.feed_forward(hidden_states)
A:transformers.models.prophetnet.modeling_prophetnet.self.cross_attn->ProphetNetAttention(config, config.num_decoder_attention_heads)
A:transformers.models.prophetnet.modeling_prophetnet.self.cross_attn_layer_norm->LayerNorm(config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.(ngram_attention_output, self_attn_weights, self_attn_weights_ngram, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, extended_predict_attention_mask=extended_predict_attention_mask, main_relative_position_buckets=main_relative_position_buckets, predict_relative_position_buckets=predict_relative_position_buckets, position_ids=position_ids)
A:transformers.models.prophetnet.modeling_prophetnet.(attention_output, cross_attn_weights, cross_attn_present_key_value)->self.cross_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attn_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.prophetnet.modeling_prophetnet.self.position_embeddings->ProphetNetPositionalEmbeddings(config)
A:transformers.models.prophetnet.modeling_prophetnet.self.embeddings_layer_norm->LayerNorm(config.hidden_size)
A:transformers.models.prophetnet.modeling_prophetnet.self.layers->torch.nn.ModuleList([ProphetNetDecoderLayer(config) for _ in range(config.num_decoder_layers)])
A:transformers.models.prophetnet.modeling_prophetnet.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.prophetnet.modeling_prophetnet.extended_attention_mask->torch.cat([extended_attention_mask, torch.zeros_like(extended_attention_mask)], dim=-1)
A:transformers.models.prophetnet.modeling_prophetnet.(position_embeddings, position_ids)->self.position_embeddings(inputs_embeds.shape[:2], inputs_embeds.device)
A:transformers.models.prophetnet.modeling_prophetnet.layer_outputs->decoder_layer(hidden_states, attention_mask=extended_attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attn_mask=extended_encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, extended_predict_attention_mask=extended_predict_attention_mask, main_relative_position_buckets=main_relative_position_buckets, predict_relative_position_buckets=predict_relative_position_buckets, position_ids=position_ids, past_key_value=past_key_value, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.prophetnet.modeling_prophetnet.self.ngram_embeddings->torch.nn.Embedding(self.ngram, config.hidden_size, None)
A:transformers.models.prophetnet.modeling_prophetnet.(main_stream_pos_embed, position_ids)->self.position_embeddings((batch_size, sequence_length), device=inputs_embeds.device, past_key_values=past_key_values)
A:transformers.models.prophetnet.modeling_prophetnet.(main_relative_position_buckets, predict_relative_position_buckets)->self.compute_buffered_relative_buckets(position_ids)
A:transformers.models.prophetnet.modeling_prophetnet.predicting_stream_pos_embed->self.position_embeddings._forward(position_ids + 1)
A:transformers.models.prophetnet.modeling_prophetnet.extended_predict_attention_mask->self.prepare_predict_attention_mask(hidden_states, attention_mask)
A:transformers.models.prophetnet.modeling_prophetnet.extended_encoder_attention_mask->extended_encoder_attention_mask.to(inputs_embeds.dtype).to(inputs_embeds.dtype)
A:transformers.models.prophetnet.modeling_prophetnet.(main_relative_buckets, predict_relative_buckets)->compute_all_stream_relative_buckets(self.num_buckets, self.relative_max_distance, position_ids)
A:transformers.models.prophetnet.modeling_prophetnet.main_relative_buckets->main_relative_buckets[:, :sequence_length, :sequence_length].repeat(batch_size, 1, 1)
A:transformers.models.prophetnet.modeling_prophetnet.predict_relative_buckets->torch.cat([predict_relative_buckets[:, :sequence_length, :sequence_length], predict_relative_buckets[:, :sequence_length, self.max_target_positions:self.max_target_positions + sequence_length]], 2).repeat(batch_size, 1, 1)
A:transformers.models.prophetnet.modeling_prophetnet.causal_mask->torch.triu(causal_mask, 1)
A:transformers.models.prophetnet.modeling_prophetnet.extended_causal_mask->causal_mask[:seq_length, :seq_length][None, :, :].expand((batch_size,) + causal_mask.shape)
A:transformers.models.prophetnet.modeling_prophetnet.predict_causal_mask->torch.cat([predict_causal_mask[:, :seq_length, :seq_length], predict_causal_mask[:, :seq_length, self.max_target_positions:self.max_target_positions + seq_length]], dim=-1)
A:transformers.models.prophetnet.modeling_prophetnet.extended_predict_causal_mask->predict_causal_mask[:, None, :, :].expand(predict_causal_mask.shape[:1] + (batch_size,) + predict_causal_mask.shape[1:])
A:transformers.models.prophetnet.modeling_prophetnet.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.prophetnet.modeling_prophetnet.encoder_config->copy.deepcopy(config)
A:transformers.models.prophetnet.modeling_prophetnet.self.encoder->ProphetNetEncoder(encoder_config, self.word_embeddings)
A:transformers.models.prophetnet.modeling_prophetnet.decoder_config->copy.deepcopy(config)
A:transformers.models.prophetnet.modeling_prophetnet.self.decoder->ProphetNetDecoder(config)
A:transformers.models.prophetnet.modeling_prophetnet.encoder_outputs->self.encoder(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.prophetnet.modeling_prophetnet.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, use_cache=use_cache, return_dict=return_dict)
A:transformers.models.prophetnet.modeling_prophetnet.self.prophetnet->ProphetNetDecoderWrapper(config)
A:transformers.models.prophetnet.modeling_prophetnet.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.prophetnet.modeling_prophetnet.decoder_input_ids->self._shift_right(labels)
A:transformers.models.prophetnet.modeling_prophetnet.outputs->self.prophetnet.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.prophetnet.modeling_prophetnet.predicting_streams->outputs[1].view(batch_size, self.config.ngram, sequence_length, -1)
A:transformers.models.prophetnet.modeling_prophetnet.predict_logits->self.lm_head(predicting_streams)
A:transformers.models.prophetnet.modeling_prophetnet.logits->logits.contiguous().contiguous()
A:transformers.models.prophetnet.modeling_prophetnet.loss->torch.nn.functional.nll_loss(lprobs, expend_targets.view(-1), reduction='mean')
A:transformers.models.prophetnet.modeling_prophetnet.all_logits->tuple((v for v in [logits, logits_ngram] if v is not None))
A:transformers.models.prophetnet.modeling_prophetnet.expend_targets->labels.new_zeros(self.config.ngram, labels.size(0), labels.size(1)).fill_(ignore_index)
A:transformers.models.prophetnet.modeling_prophetnet.lprobs->torch.nn.functional.log_softmax(logits.view(-1, logits.size(-1)), dim=-1, dtype=torch.float32)
A:transformers.models.prophetnet.modeling_prophetnet.non_masked_tokens->labels.new_zeros(self.config.ngram, labels.size(0), labels.size(1)).fill_(ignore_index).ne(ignore_index).view(-1)
A:transformers.models.prophetnet.modeling_prophetnet.smooth_loss->smooth_loss.mean().mean()
A:transformers.models.prophetnet.modeling_prophetnet.config->copy.deepcopy(config)
transformers.ProphetNetDecoder(self,config:ProphetNetConfig,word_embeddings:nn.Embedding=None)
transformers.ProphetNetDecoder.compute_buffered_relative_buckets(self,position_ids)
transformers.ProphetNetDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ProphetNetDecoder.get_input_embeddings(self)
transformers.ProphetNetDecoder.prepare_attention_mask(self,hidden_states,attention_mask)
transformers.ProphetNetDecoder.prepare_predict_attention_mask(self,hidden_states,attention_mask)
transformers.ProphetNetDecoder.set_input_embeddings(self,value)
transformers.ProphetNetDecoderLMOutput(ModelOutput)
transformers.ProphetNetDecoderLayer(self,config:ProphetNetConfig)
transformers.ProphetNetDecoderLayer.forward(self,hidden_states,attention_mask=None,encoder_hidden_states=None,encoder_attn_mask=None,layer_head_mask=None,cross_attn_layer_head_mask=None,extended_predict_attention_mask=None,main_relative_position_buckets=None,predict_relative_position_buckets=None,position_ids=None,past_key_value=None,use_cache:bool=True,output_attentions:bool=False)
transformers.ProphetNetDecoderModelOutput(ModelOutput)
transformers.ProphetNetDecoderWrapper(self,config)
transformers.ProphetNetDecoderWrapper.forward(self,*args,**kwargs)
transformers.ProphetNetEncoder(self,config:ProphetNetConfig,word_embeddings:nn.Embedding=None)
transformers.ProphetNetEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ProphetNetEncoder.get_input_embeddings(self)
transformers.ProphetNetEncoder.set_input_embeddings(self,value)
transformers.ProphetNetEncoderLayer(self,config:ProphetNetConfig)
transformers.ProphetNetEncoderLayer.forward(self,hidden_states,attention_mask,layer_head_mask,output_attentions:bool=False)
transformers.ProphetNetForCausalLM(self,config)
transformers.ProphetNetForCausalLM._compute_loss(self,logits,labels,ignore_index=-100)
transformers.ProphetNetForCausalLM._reorder_cache(past,beam_idx)
transformers.ProphetNetForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ProphetNetForCausalLM.get_decoder(self)
transformers.ProphetNetForCausalLM.get_input_embeddings(self)
transformers.ProphetNetForCausalLM.get_output_embeddings(self)
transformers.ProphetNetForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,head_mask=None,use_cache=None,**kwargs)
transformers.ProphetNetForCausalLM.set_decoder(self,decoder)
transformers.ProphetNetForCausalLM.set_input_embeddings(self,value)
transformers.ProphetNetForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.ProphetNetForConditionalGeneration(self,config:ProphetNetConfig)
transformers.ProphetNetForConditionalGeneration._compute_loss(self,logits,labels,ignore_index=-100)
transformers.ProphetNetForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.ProphetNetForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ProphetNetForConditionalGeneration.get_decoder(self)
transformers.ProphetNetForConditionalGeneration.get_encoder(self)
transformers.ProphetNetForConditionalGeneration.get_input_embeddings(self)
transformers.ProphetNetForConditionalGeneration.get_output_embeddings(self)
transformers.ProphetNetForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.ProphetNetForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.ProphetNetForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.ProphetNetModel(self,config)
transformers.ProphetNetModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Tuple]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ProphetNetModel.get_decoder(self)
transformers.ProphetNetModel.get_encoder(self)
transformers.ProphetNetModel.get_input_embeddings(self)
transformers.ProphetNetModel.set_input_embeddings(self,value)
transformers.ProphetNetPreTrainedModel(PreTrainedModel)
transformers.ProphetNetPreTrainedModel._init_weights(self,module)
transformers.ProphetNetPreTrainedModel._shift_right(self,input_ids)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetAttention(self,config:ProphetNetConfig,num_attn_heads:int)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetAttention.__init__(self,config:ProphetNetConfig,num_attn_heads:int)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetAttention.forward(self,hidden_states,key_value_states:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,layer_head_mask:Optional[Tensor]=None,past_key_value:Optional[Tuple[Tensor]]=None,output_attentions:bool=False)->Tuple[Tensor, Optional[Tensor]]
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder(self,config:ProphetNetConfig,word_embeddings:nn.Embedding=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.__init__(self,config:ProphetNetConfig,word_embeddings:nn.Embedding=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.compute_buffered_relative_buckets(self,position_ids)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.get_input_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.prepare_attention_mask(self,hidden_states,attention_mask)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.prepare_predict_attention_mask(self,hidden_states,attention_mask)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoder.set_input_embeddings(self,value)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput(ModelOutput)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLayer(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLayer.__init__(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLayer.forward(self,hidden_states,attention_mask=None,encoder_hidden_states=None,encoder_attn_mask=None,layer_head_mask=None,cross_attn_layer_head_mask=None,extended_predict_attention_mask=None,main_relative_position_buckets=None,predict_relative_position_buckets=None,position_ids=None,past_key_value=None,use_cache:bool=True,output_attentions:bool=False)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput(ModelOutput)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderWrapper(self,config)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderWrapper.__init__(self,config)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoder(self,config:ProphetNetConfig,word_embeddings:nn.Embedding=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoder.__init__(self,config:ProphetNetConfig,word_embeddings:nn.Embedding=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoder.get_input_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoder.set_input_embeddings(self,value)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoderLayer(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoderLayer.__init__(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetEncoderLayer.forward(self,hidden_states,attention_mask,layer_head_mask,output_attentions:bool=False)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetFeedForward(self,config:ProphetNetConfig,ffn_dim:int)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetFeedForward.__init__(self,config:ProphetNetConfig,ffn_dim:int)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetFeedForward.forward(self,hidden_states)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM(self,config)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.__init__(self,config)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM._compute_loss(self,logits,labels,ignore_index=-100)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM._reorder_cache(past,beam_idx)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.get_decoder(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.get_input_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.get_output_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,head_mask=None,use_cache=None,**kwargs)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.set_decoder(self,decoder)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.set_input_embeddings(self,value)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.__init__(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration._compute_loss(self,logits,labels,ignore_index=-100)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.get_decoder(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.get_encoder(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.get_input_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.get_output_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel(self,config)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel.__init__(self,config)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Tuple]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel.get_decoder(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel.get_encoder(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel.get_input_embeddings(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetModel.set_input_embeddings(self,value)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention.__init__(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention._shape(self,tensor,seq_len,batch_size)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention.forward(self,hidden_states,past_key_value:Optional[Tuple[Tensor]]=None,attention_mask=None,layer_head_mask=None,extended_predict_attention_mask=None,main_relative_position_buckets=None,predict_relative_position_buckets=None,position_ids=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention.get_main_relative_pos_embeddings(self,hidden_states,attn_weights,position_ids,main_relative_position_buckets)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention.get_predict_relative_pos_embeddings(self,hidden_states,attn_weights,position_ids,predict_relative_position_buckets)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetNgramSelfAttention.prepare_for_onnx_export_(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPositionalEmbeddings(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPositionalEmbeddings.__init__(self,config:ProphetNetConfig)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPositionalEmbeddings._forward(self,position_ids)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPositionalEmbeddings.forward(self,inputs_shape,device,attention_mask=None,past_key_values=None,position_ids=None)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPreTrainedModel(PreTrainedModel)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPreTrainedModel._init_weights(self,module)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetPreTrainedModel._shift_right(self,input_ids)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput(ModelOutput)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_cross_attentions(self)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput(ModelOutput)
transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_cross_attentions(self)
transformers.models.prophetnet.modeling_prophetnet.compute_all_stream_relative_buckets(num_buckets,max_distance,position_ids)
transformers.models.prophetnet.modeling_prophetnet.compute_relative_buckets(num_buckets,max_distance,relative_positions,is_bidirectional=False)
transformers.models.prophetnet.modeling_prophetnet.ngram_attention_bias(sequence_length,ngram,device,dtype)
transformers.models.prophetnet.modeling_prophetnet.softmax(hidden_state,dim,onnx_trace=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/prophetnet/__init__.py----------------------------------------
A:transformers.models.prophetnet.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/prophetnet/tokenization_prophetnet.py----------------------------------------
A:transformers.models.prophetnet.tokenization_prophetnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.prophetnet.tokenization_prophetnet.vocab->collections.OrderedDict()
A:transformers.models.prophetnet.tokenization_prophetnet.tokens->reader.readlines()
A:transformers.models.prophetnet.tokenization_prophetnet.token->token.rstrip('\n').rstrip('\n')
A:transformers.models.prophetnet.tokenization_prophetnet.self.vocab->load_vocab(vocab_file)
A:transformers.models.prophetnet.tokenization_prophetnet.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.models.prophetnet.tokenization_prophetnet.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars, strip_accents=strip_accents)
A:transformers.models.prophetnet.tokenization_prophetnet.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.models.prophetnet.tokenization_prophetnet.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.models.prophetnet.tokenization_prophetnet.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.models.prophetnet.tokenization_prophetnet.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.ProphetNetTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',x_sep_token='[X_SEP]',pad_token='[PAD]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.ProphetNetTokenizer._convert_id_to_token(self,index)
transformers.ProphetNetTokenizer._convert_token_to_id(self,token)
transformers.ProphetNetTokenizer._tokenize(self,text)
transformers.ProphetNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.ProphetNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.ProphetNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.ProphetNetTokenizer.get_vocab(self)
transformers.ProphetNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.ProphetNetTokenizer.vocab_size(self)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',x_sep_token='[X_SEP]',pad_token='[PAD]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',x_sep_token='[X_SEP]',pad_token='[PAD]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer._convert_id_to_token(self,index)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer._convert_token_to_id(self,token)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer._tokenize(self,text)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.get_vocab(self)
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.prophetnet.tokenization_prophetnet.ProphetNetTokenizer.vocab_size(self)
transformers.models.prophetnet.tokenization_prophetnet.load_vocab(vocab_file)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.logger->transformers.logging.get_logger(__name__)
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.prophet_old->transformers_old.modeling_prophetnet.ProphetNetForConditionalGeneration.from_pretrained(prophetnet_checkpoint_path)
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.(prophet, loading_info)->transformers.ProphetNetForConditionalGeneration.from_pretrained(prophetnet_checkpoint_path, output_loading_info=True)
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.attributes->key.split('.')
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.param->getattr(model, attribute)
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.query_proj.weight->torch.nn.Parameter(old_model.in_proj_weight[:embed_dim, :])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.query_proj.bias->torch.nn.Parameter(old_model.in_proj_bias[:embed_dim])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.key_proj.weight->torch.nn.Parameter(old_model.in_proj_weight[embed_dim:2 * embed_dim, :])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.key_proj.bias->torch.nn.Parameter(old_model.in_proj_bias[embed_dim:2 * embed_dim])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.value_proj.weight->torch.nn.Parameter(old_model.in_proj_weight[2 * embed_dim:, :])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.value_proj.bias->torch.nn.Parameter(old_model.in_proj_bias[2 * embed_dim:])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model.position_embeddings.weight->torch.nn.Parameter(old_model.embed_positions.weight[:512, :])
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.model->getattr(model, attribute)
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.old_model->getattr(old_model, old_attribute)
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.prophetnet.convert_prophetnet_original_pytorch_checkpoint_to_pytorch.convert_prophetnet_checkpoint_to_pytorch(prophetnet_checkpoint_path:str,pytorch_dump_folder_path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/modeling_tf_mobilebert.py----------------------------------------
A:transformers.models.mobilebert.modeling_tf_mobilebert.logger->utils.logging.get_logger(__name__)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.dense->self.add_weight(shape=(self.config.hidden_size - self.config.embedding_size, self.vocab_size), initializer='zeros', trainable=True, name='dense/weight')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.mobilebert.modeling_tf_mobilebert.hidden_states->tensorflow.matmul(hidden_states, tf.concat([tf.transpose(self.decoder), self.dense], axis=0))
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.embedding_transformation->tensorflow.keras.layers.Dense(config.hidden_size, name='embedding_transformation')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.LayerNorm->NORM2FN['layer_norm'](config.hidden_size, epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.mobilebert.modeling_tf_mobilebert.inputs_embeds->self.embedding_transformation(inputs_embeds)
A:transformers.models.mobilebert.modeling_tf_mobilebert.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.mobilebert.modeling_tf_mobilebert.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.mobilebert.modeling_tf_mobilebert.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.mobilebert.modeling_tf_mobilebert.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.mobilebert.modeling_tf_mobilebert.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.attention_head_size->int(config.true_hidden_size / config.num_attention_heads)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.mobilebert.modeling_tf_mobilebert.x->tensorflow.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.mobilebert.modeling_tf_mobilebert.mixed_query_layer->self.query(query_tensor)
A:transformers.models.mobilebert.modeling_tf_mobilebert.mixed_key_layer->self.key(key_tensor)
A:transformers.models.mobilebert.modeling_tf_mobilebert.mixed_value_layer->self.value(value_tensor)
A:transformers.models.mobilebert.modeling_tf_mobilebert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.mobilebert.modeling_tf_mobilebert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.mobilebert.modeling_tf_mobilebert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.mobilebert.modeling_tf_mobilebert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.models.mobilebert.modeling_tf_mobilebert.dk->tensorflow.cast(shape_list(key_layer)[-1], dtype=attention_scores.dtype)
A:transformers.models.mobilebert.modeling_tf_mobilebert.attention_mask->tensorflow.cast(attention_mask, dtype=attention_scores.dtype)
A:transformers.models.mobilebert.modeling_tf_mobilebert.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.models.mobilebert.modeling_tf_mobilebert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.all_head_size))
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.self->TFMobileBertSelfAttention(config, name='self')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.mobilebert_output->TFMobileBertOutput(config, name='output')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self_outputs->self.self(query_tensor, key_tensor, value_tensor, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.mobilebert.modeling_tf_mobilebert.attention_output->ffn_module(attention_output)
A:transformers.models.mobilebert.modeling_tf_mobilebert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], output_attentions, training=training)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.bottleneck->TFBottleneck(config, name='bottleneck')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.bottleneck_input->TFBottleneckLayer(config, name='input')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.attention->TFMobileBertAttention(config, name='attention')
A:transformers.models.mobilebert.modeling_tf_mobilebert.bottlenecked_hidden_states->self.bottleneck_input(hidden_states)
A:transformers.models.mobilebert.modeling_tf_mobilebert.shared_attention_input->self.attention(hidden_states)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.intermediate->TFMobileBertIntermediate(config, name='intermediate')
A:transformers.models.mobilebert.modeling_tf_mobilebert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.mobilebert.modeling_tf_mobilebert.(query_tensor, key_tensor, value_tensor, layer_input)->self.bottleneck(hidden_states)
A:transformers.models.mobilebert.modeling_tf_mobilebert.attention_outputs->self.attention(query_tensor, key_tensor, value_tensor, layer_input, attention_mask, head_mask, output_attentions, training=training)
A:transformers.models.mobilebert.modeling_tf_mobilebert.layer_output->self.mobilebert_output(intermediate_output, attention_output, hidden_states, training=training)
A:transformers.models.mobilebert.modeling_tf_mobilebert.pooled_output->self.dropout(pooled_output, training=inputs['training'])
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.transform_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.transform->TFMobileBertPredictionHeadTransform(config, name='transform')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.decoder->self.add_weight(shape=(self.config.vocab_size, self.config.embedding_size), initializer='zeros', trainable=True, name='decoder/weight')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.predictions->TFMobileBertMLMHead(config, name='predictions___cls')
A:transformers.models.mobilebert.modeling_tf_mobilebert.prediction_scores->self.predictions(sequence_output, training=inputs['training'])
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.embeddings->TFMobileBertEmbeddings(config, name='embeddings')
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.encoder->TFMobileBertEncoder(config, name='encoder')
A:transformers.models.mobilebert.modeling_tf_mobilebert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.mobilebert.modeling_tf_mobilebert.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.mobilebert.modeling_tf_mobilebert.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.mobilebert.modeling_tf_mobilebert.inputs['token_type_ids']->tensorflow.fill(input_shape, 0)
A:transformers.models.mobilebert.modeling_tf_mobilebert.embedding_output->self.embeddings(inputs['input_ids'], inputs['position_ids'], inputs['token_type_ids'], inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.mobilebert.modeling_tf_mobilebert.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.mobilebert.modeling_tf_mobilebert.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.mobilebert.modeling_tf_mobilebert.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.mobilebert.modeling_tf_mobilebert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, inputs['head_mask'], inputs['output_attentions'], inputs['output_hidden_states'], inputs['return_dict'], training=inputs['training'])
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.mobilebert->TFMobileBertMainLayer(config, add_pooling_layer=False, name='mobilebert')
A:transformers.models.mobilebert.modeling_tf_mobilebert.outputs->self.mobilebert(inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=return_dict, training=inputs['training'])
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.seq_relationship->tensorflow.keras.layers.Dense(2, name='seq_relationship')
A:transformers.models.mobilebert.modeling_tf_mobilebert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.cls->TFMobileBertOnlyNSPHead(config, name='seq_relationship___cls')
A:transformers.models.mobilebert.modeling_tf_mobilebert.seq_relationship_scores->self.cls(pooled_output)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.mobilebert.modeling_tf_mobilebert.logits->self.classifier(sequence_output)
A:transformers.models.mobilebert.modeling_tf_mobilebert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.mobilebert.modeling_tf_mobilebert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.mobilebert.modeling_tf_mobilebert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.mobilebert.modeling_tf_mobilebert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.mobilebert.modeling_tf_mobilebert.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.models.mobilebert.modeling_tf_mobilebert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.mobilebert.modeling_tf_mobilebert.output->self.call(input_ids=inputs)
A:transformers.models.mobilebert.modeling_tf_mobilebert.sequence_output->self.dropout(sequence_output, training=inputs['training'])
transformers.TFMobileBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFMobileBertForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMobileBertForMaskedLM.get_lm_head(self)
transformers.TFMobileBertForMaskedLM.get_prefix_bias_name(self)
transformers.TFMobileBertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFMobileBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFMobileBertForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMobileBertForMultipleChoice.dummy_inputs(self)
transformers.TFMobileBertForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.TFMobileBertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFMobileBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.TFMobileBertForNextSentencePrediction.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,next_sentence_label=None,training=False,**kwargs)
transformers.TFMobileBertForNextSentencePrediction.serving_output(self,output:TFNextSentencePredictorOutput)->TFNextSentencePredictorOutput
transformers.TFMobileBertForPreTraining(self,config,*inputs,**kwargs)
transformers.TFMobileBertForPreTraining.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMobileBertForPreTraining.get_lm_head(self)
transformers.TFMobileBertForPreTraining.get_prefix_bias_name(self)
transformers.TFMobileBertForPreTraining.serving_output(self,output)
transformers.TFMobileBertForPreTrainingOutput(ModelOutput)
transformers.TFMobileBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFMobileBertForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFMobileBertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFMobileBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFMobileBertForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMobileBertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFMobileBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFMobileBertForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMobileBertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFMobileBertMainLayer(self,config,add_pooling_layer=True,**kwargs)
transformers.TFMobileBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFMobileBertMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMobileBertMainLayer.get_input_embeddings(self)
transformers.TFMobileBertMainLayer.set_input_embeddings(self,value)
transformers.TFMobileBertModel(self,config,*inputs,**kwargs)
transformers.TFMobileBertModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMobileBertModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.TFMobileBertPreTrainedModel(TFPreTrainedModel)
transformers.models.mobilebert.modeling_tf_mobilebert.TFBottleneck(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFBottleneck.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFBottleneck.call(self,hidden_states)
transformers.models.mobilebert.modeling_tf_mobilebert.TFBottleneckLayer(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFBottleneckLayer.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFBottleneckLayer.call(self,inputs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFFFNLayer(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFFFNLayer.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFFFNLayer.call(self,hidden_states)
transformers.models.mobilebert.modeling_tf_mobilebert.TFFFNOutput(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFFFNOutput.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFFFNOutput.call(self,hidden_states,residual_tensor)
transformers.models.mobilebert.modeling_tf_mobilebert.TFLayerNorm(self,feat_size,*args,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFLayerNorm.__init__(self,feat_size,*args,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertAttention(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertAttention.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertAttention.call(self,query_tensor,key_tensor,value_tensor,layer_input,attention_mask,head_mask,output_attentions,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertAttention.prune_heads(self,heads)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEmbeddings(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEmbeddings.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEmbeddings.build(self,input_shape)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEncoder(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEncoder.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertEncoder.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMaskedLM.get_lm_head(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMaskedLM.get_prefix_bias_name(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.dummy_inputs(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,next_sentence_label=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction.serving_output(self,output:TFNextSentencePredictorOutput)->TFNextSentencePredictorOutput
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTraining(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTraining.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTraining.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTraining.get_lm_head(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTraining.get_prefix_bias_name(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTraining.serving_output(self,output)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForPreTrainingOutput(ModelOutput)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertIntermediate(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertIntermediate.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertIntermediate.call(self,hidden_states)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.build(self,input_shape)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.call(self,hidden_states)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.get_bias(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.get_output_embeddings(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.set_bias(self,value)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.set_output_embeddings(self,value)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLayer(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLayer.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMLMHead(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMLMHead.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMLMHead.call(self,sequence_output)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMainLayer(self,config,add_pooling_layer=True,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMainLayer.__init__(self,config,add_pooling_layer=True,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMainLayer._prune_heads(self,heads_to_prune)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMainLayer.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMainLayer.get_input_embeddings(self)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertMainLayer.set_input_embeddings(self,value)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertModel(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertModel.__init__(self,config,*inputs,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertOnlyNSPHead(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertOnlyNSPHead.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertOnlyNSPHead.call(self,pooled_output)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertOutput(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertOutput.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertOutput.call(self,hidden_states,residual_tensor_1,residual_tensor_2,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPooler(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPooler.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPooler.call(self,hidden_states)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPreTrainedModel(TFPreTrainedModel)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPredictionHeadTransform(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPredictionHeadTransform.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertPredictionHeadTransform.call(self,hidden_states)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfAttention(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfAttention.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfAttention.call(self,query_tensor,key_tensor,value_tensor,attention_mask,head_mask,output_attentions,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfAttention.transpose_for_scores(self,x,batch_size)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfOutput(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfOutput.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFMobileBertSelfOutput.call(self,hidden_states,residual_tensor,training=False)
transformers.models.mobilebert.modeling_tf_mobilebert.TFNoNorm(self,feat_size,epsilon=None,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFNoNorm.__init__(self,feat_size,epsilon=None,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFNoNorm.build(self,input_shape)
transformers.models.mobilebert.modeling_tf_mobilebert.TFNoNorm.call(self,inputs:tf.Tensor)
transformers.models.mobilebert.modeling_tf_mobilebert.TFOutputBottleneck(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFOutputBottleneck.__init__(self,config,**kwargs)
transformers.models.mobilebert.modeling_tf_mobilebert.TFOutputBottleneck.call(self,hidden_states,residual_tensor,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.mobilebert.convert_mobilebert_original_tf_checkpoint_to_pytorch.config->transformers.MobileBertConfig.from_json_file(mobilebert_config_file)
A:transformers.models.mobilebert.convert_mobilebert_original_tf_checkpoint_to_pytorch.model->load_tf_weights_in_mobilebert(model, config, tf_checkpoint_path)
A:transformers.models.mobilebert.convert_mobilebert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.mobilebert.convert_mobilebert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.mobilebert.convert_mobilebert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,mobilebert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/tokenization_mobilebert_fast.py----------------------------------------
A:transformers.models.mobilebert.tokenization_mobilebert_fast.logger->utils.logging.get_logger(__name__)
transformers.MobileBertTokenizerFast(BertTokenizerFast)
transformers.models.mobilebert.tokenization_mobilebert_fast.MobileBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/__init__.py----------------------------------------
A:transformers.models.mobilebert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/modeling_mobilebert.py----------------------------------------
A:transformers.models.mobilebert.modeling_mobilebert.logger->utils.logging.get_logger(__name__)
A:transformers.models.mobilebert.modeling_mobilebert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.mobilebert.modeling_mobilebert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.mobilebert.modeling_mobilebert.array->numpy.transpose(array)
A:transformers.models.mobilebert.modeling_mobilebert.name->name.split('/').split('/')
A:transformers.models.mobilebert.modeling_mobilebert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.mobilebert.modeling_mobilebert.pointer->getattr(pointer, 'weight')
A:transformers.models.mobilebert.modeling_mobilebert.num->int(scope_names[1])
A:transformers.models.mobilebert.modeling_mobilebert.pointer.data->torch.from_numpy(array)
A:transformers.models.mobilebert.modeling_mobilebert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.mobilebert.modeling_mobilebert.self.weight->torch.nn.Parameter(torch.ones(feat_size))
A:transformers.models.mobilebert.modeling_mobilebert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.models.mobilebert.modeling_mobilebert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.mobilebert.modeling_mobilebert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.mobilebert.modeling_mobilebert.self.embedding_transformation->torch.nn.Linear(embedded_input_size, config.hidden_size)
A:transformers.models.mobilebert.modeling_mobilebert.self.LayerNorm->NORM2FN['layer_norm'](config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.mobilebert.modeling_mobilebert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.mobilebert.modeling_mobilebert.input_shape->input_ids.size()
A:transformers.models.mobilebert.modeling_mobilebert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.mobilebert.modeling_mobilebert.inputs_embeds->self.embedding_transformation(inputs_embeds)
A:transformers.models.mobilebert.modeling_mobilebert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.mobilebert.modeling_mobilebert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.mobilebert.modeling_mobilebert.embeddings->self.dropout(embeddings)
A:transformers.models.mobilebert.modeling_mobilebert.self.attention_head_size->int(config.true_hidden_size / config.num_attention_heads)
A:transformers.models.mobilebert.modeling_mobilebert.self.query->torch.nn.Linear(config.true_hidden_size, self.all_head_size)
A:transformers.models.mobilebert.modeling_mobilebert.self.key->torch.nn.Linear(config.true_hidden_size, self.all_head_size)
A:transformers.models.mobilebert.modeling_mobilebert.self.value->torch.nn.Linear(config.true_hidden_size if config.use_bottleneck_attention else config.hidden_size, self.all_head_size)
A:transformers.models.mobilebert.modeling_mobilebert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.mobilebert.modeling_mobilebert.mixed_query_layer->self.query(query_tensor)
A:transformers.models.mobilebert.modeling_mobilebert.mixed_key_layer->self.key(key_tensor)
A:transformers.models.mobilebert.modeling_mobilebert.mixed_value_layer->self.value(value_tensor)
A:transformers.models.mobilebert.modeling_mobilebert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.mobilebert.modeling_mobilebert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.models.mobilebert.modeling_mobilebert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.models.mobilebert.modeling_mobilebert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.mobilebert.modeling_mobilebert.attention_probs->self.dropout(attention_probs)
A:transformers.models.mobilebert.modeling_mobilebert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.mobilebert.modeling_mobilebert.self.dense->torch.nn.Linear(config.vocab_size, config.hidden_size - config.embedding_size, bias=False)
A:transformers.models.mobilebert.modeling_mobilebert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], output_attentions)
A:transformers.models.mobilebert.modeling_mobilebert.self.self->MobileBertSelfAttention(config)
A:transformers.models.mobilebert.modeling_mobilebert.self.output->MobileBertOutput(config)
A:transformers.models.mobilebert.modeling_mobilebert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.mobilebert.modeling_mobilebert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.mobilebert.modeling_mobilebert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.mobilebert.modeling_mobilebert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.mobilebert.modeling_mobilebert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.mobilebert.modeling_mobilebert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.mobilebert.modeling_mobilebert.self_outputs->self.self(query_tensor, key_tensor, value_tensor, attention_mask, head_mask, output_attentions)
A:transformers.models.mobilebert.modeling_mobilebert.attention_output->ffn_module(attention_output)
A:transformers.models.mobilebert.modeling_mobilebert.hidden_states->hidden_states.matmul(torch.cat([self.decoder.weight.t(), self.dense.weight], dim=0)).matmul(torch.cat([self.decoder.weight.t(), self.dense.weight], dim=0))
A:transformers.models.mobilebert.modeling_mobilebert.self.bottleneck->Bottleneck(config)
A:transformers.models.mobilebert.modeling_mobilebert.layer_output->self.output(intermediate_output, attention_output, hidden_states)
A:transformers.models.mobilebert.modeling_mobilebert.layer_input->self.LayerNorm(layer_input)
A:transformers.models.mobilebert.modeling_mobilebert.self.input->BottleneckLayer(config)
A:transformers.models.mobilebert.modeling_mobilebert.self.attention->MobileBertAttention(config)
A:transformers.models.mobilebert.modeling_mobilebert.bottlenecked_hidden_states->self.input(hidden_states)
A:transformers.models.mobilebert.modeling_mobilebert.shared_attention_input->self.attention(hidden_states)
A:transformers.models.mobilebert.modeling_mobilebert.self.intermediate->MobileBertIntermediate(config)
A:transformers.models.mobilebert.modeling_mobilebert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.mobilebert.modeling_mobilebert.self.ffn->torch.nn.ModuleList([FFNLayer(config) for _ in range(config.num_feedforward_networks - 1)])
A:transformers.models.mobilebert.modeling_mobilebert.(query_tensor, key_tensor, value_tensor, layer_input)->self.bottleneck(hidden_states)
A:transformers.models.mobilebert.modeling_mobilebert.self_attention_outputs->self.attention(query_tensor, key_tensor, value_tensor, layer_input, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.models.mobilebert.modeling_mobilebert.self.layer->torch.nn.ModuleList([MobileBertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.mobilebert.modeling_mobilebert.pooled_output->self.dropout(pooled_output)
A:transformers.models.mobilebert.modeling_mobilebert.self.transform->MobileBertPredictionHeadTransform(config)
A:transformers.models.mobilebert.modeling_mobilebert.self.decoder->torch.nn.Linear(config.embedding_size, config.vocab_size, bias=False)
A:transformers.models.mobilebert.modeling_mobilebert.self.predictions->MobileBertLMPredictionHead(config)
A:transformers.models.mobilebert.modeling_mobilebert.prediction_scores->self.cls(sequence_output)
A:transformers.models.mobilebert.modeling_mobilebert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.models.mobilebert.modeling_mobilebert.seq_relationship_score->self.cls(pooled_output)
A:transformers.models.mobilebert.modeling_mobilebert.self.embeddings->MobileBertEmbeddings(config)
A:transformers.models.mobilebert.modeling_mobilebert.self.encoder->MobileBertEncoder(config)
A:transformers.models.mobilebert.modeling_mobilebert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.mobilebert.modeling_mobilebert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.mobilebert.modeling_mobilebert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.mobilebert.modeling_mobilebert.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mobilebert.modeling_mobilebert.self.mobilebert->MobileBertModel(config, add_pooling_layer=False)
A:transformers.models.mobilebert.modeling_mobilebert.self.cls->MobileBertOnlyNSPHead(config)
A:transformers.models.mobilebert.modeling_mobilebert.self.cls.predictions.dense->self._get_resized_lm_head(self.cls.predictions.dense, new_num_tokens=new_num_tokens, transposed=True)
A:transformers.models.mobilebert.modeling_mobilebert.outputs->self.mobilebert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mobilebert.modeling_mobilebert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.models.mobilebert.modeling_mobilebert.loss_fct->CrossEntropyLoss()
A:transformers.models.mobilebert.modeling_mobilebert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.mobilebert.modeling_mobilebert.next_sentence_loss->loss_fct(seq_relationship_score.view(-1, 2), labels.view(-1))
A:transformers.models.mobilebert.modeling_mobilebert.labels->kwargs.pop('next_sentence_label')
A:transformers.models.mobilebert.modeling_mobilebert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mobilebert.modeling_mobilebert.logits->self.classifier(sequence_output)
A:transformers.models.mobilebert.modeling_mobilebert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.mobilebert.modeling_mobilebert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mobilebert.modeling_mobilebert.(start_logits, end_logits)->self.classifier(sequence_output).split(1, dim=-1)
A:transformers.models.mobilebert.modeling_mobilebert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.mobilebert.modeling_mobilebert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.mobilebert.modeling_mobilebert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.mobilebert.modeling_mobilebert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.mobilebert.modeling_mobilebert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.mobilebert.modeling_mobilebert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.mobilebert.modeling_mobilebert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.mobilebert.modeling_mobilebert.reshaped_logits->self.classifier(sequence_output).view(-1, num_choices)
A:transformers.models.mobilebert.modeling_mobilebert.sequence_output->self.dropout(sequence_output)
A:transformers.models.mobilebert.modeling_mobilebert.active_logits->self.classifier(sequence_output).view(-1, self.num_labels)
A:transformers.models.mobilebert.modeling_mobilebert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
transformers.MobileBertForMaskedLM(self,config)
transformers.MobileBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForMaskedLM.get_output_embeddings(self)
transformers.MobileBertForMaskedLM.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.MobileBertForMaskedLM.set_output_embeddings(self,new_embeddigs)
transformers.MobileBertForMultipleChoice(self,config)
transformers.MobileBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForNextSentencePrediction(self,config)
transformers.MobileBertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.MobileBertForPreTraining(self,config)
transformers.MobileBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForPreTraining.get_output_embeddings(self)
transformers.MobileBertForPreTraining.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.MobileBertForPreTraining.set_output_embeddings(self,new_embeddigs)
transformers.MobileBertForPreTrainingOutput(ModelOutput)
transformers.MobileBertForQuestionAnswering(self,config)
transformers.MobileBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForSequenceClassification(self,config)
transformers.MobileBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForTokenClassification(self,config)
transformers.MobileBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertLayer(self,config)
transformers.MobileBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=None)
transformers.MobileBertModel(self,config,add_pooling_layer=True)
transformers.MobileBertModel._prune_heads(self,heads_to_prune)
transformers.MobileBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.MobileBertModel.get_input_embeddings(self)
transformers.MobileBertModel.set_input_embeddings(self,value)
transformers.MobileBertPreTrainedModel(PreTrainedModel)
transformers.MobileBertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_mobilebert(model,config,tf_checkpoint_path)
transformers.models.mobilebert.modeling_mobilebert.Bottleneck(self,config)
transformers.models.mobilebert.modeling_mobilebert.Bottleneck.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.Bottleneck.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.BottleneckLayer(self,config)
transformers.models.mobilebert.modeling_mobilebert.BottleneckLayer.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.BottleneckLayer.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.FFNLayer(self,config)
transformers.models.mobilebert.modeling_mobilebert.FFNLayer.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.FFNLayer.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.FFNOutput(self,config)
transformers.models.mobilebert.modeling_mobilebert.FFNOutput.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.FFNOutput.forward(self,hidden_states,residual_tensor)
transformers.models.mobilebert.modeling_mobilebert.MobileBertAttention(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertAttention.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertAttention.forward(self,query_tensor,key_tensor,value_tensor,layer_input,attention_mask=None,head_mask=None,output_attentions=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertAttention.prune_heads(self,heads)
transformers.models.mobilebert.modeling_mobilebert.MobileBertEmbeddings(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertEmbeddings.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertEncoder(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertEncoder.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMaskedLM(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMaskedLM.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMaskedLM.get_output_embeddings(self)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMaskedLM.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMaskedLM.set_output_embeddings(self,new_embeddigs)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMultipleChoice(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMultipleChoice.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForNextSentencePrediction(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForNextSentencePrediction.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTraining(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTraining.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTraining.get_output_embeddings(self)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTraining.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->nn.Embedding
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTraining.set_output_embeddings(self,new_embeddigs)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForPreTrainingOutput(ModelOutput)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForQuestionAnswering(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForQuestionAnswering.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForSequenceClassification(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForSequenceClassification.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForTokenClassification(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForTokenClassification.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertIntermediate(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertIntermediate.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertIntermediate.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.MobileBertLMPredictionHead(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertLMPredictionHead.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertLMPredictionHead.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.MobileBertLayer(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertLayer.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertModel(self,config,add_pooling_layer=True)
transformers.models.mobilebert.modeling_mobilebert.MobileBertModel.__init__(self,config,add_pooling_layer=True)
transformers.models.mobilebert.modeling_mobilebert.MobileBertModel._prune_heads(self,heads_to_prune)
transformers.models.mobilebert.modeling_mobilebert.MobileBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertModel.get_input_embeddings(self)
transformers.models.mobilebert.modeling_mobilebert.MobileBertModel.set_input_embeddings(self,value)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOnlyMLMHead(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOnlyMLMHead.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOnlyMLMHead.forward(self,sequence_output)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOnlyNSPHead(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOnlyNSPHead.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOnlyNSPHead.forward(self,pooled_output)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOutput(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOutput.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertOutput.forward(self,intermediate_states,residual_tensor_1,residual_tensor_2)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPooler(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPooler.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPooler.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPreTrainedModel(PreTrainedModel)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPreTrainedModel._init_weights(self,module)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPreTrainingHeads(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPreTrainingHeads.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPredictionHeadTransform(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPredictionHeadTransform.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfAttention(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfAttention.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfAttention.forward(self,query_tensor,key_tensor,value_tensor,attention_mask=None,head_mask=None,output_attentions=None)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfAttention.transpose_for_scores(self,x)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfOutput(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfOutput.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.MobileBertSelfOutput.forward(self,hidden_states,residual_tensor)
transformers.models.mobilebert.modeling_mobilebert.NoNorm(self,feat_size,eps=None)
transformers.models.mobilebert.modeling_mobilebert.NoNorm.__init__(self,feat_size,eps=None)
transformers.models.mobilebert.modeling_mobilebert.NoNorm.forward(self,input_tensor)
transformers.models.mobilebert.modeling_mobilebert.OutputBottleneck(self,config)
transformers.models.mobilebert.modeling_mobilebert.OutputBottleneck.__init__(self,config)
transformers.models.mobilebert.modeling_mobilebert.OutputBottleneck.forward(self,hidden_states,residual_tensor)
transformers.models.mobilebert.modeling_mobilebert.load_tf_weights_in_mobilebert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/configuration_mobilebert.py----------------------------------------
A:transformers.models.mobilebert.configuration_mobilebert.logger->utils.logging.get_logger(__name__)
transformers.MobileBertConfig(self,vocab_size=30522,hidden_size=512,num_hidden_layers=24,num_attention_heads=4,intermediate_size=512,hidden_act='relu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=128,trigram_input=True,use_bottleneck=True,intra_bottleneck_size=128,use_bottleneck_attention=False,key_query_shared_bottleneck=True,num_feedforward_networks=4,normalization_type='no_norm',classifier_activation=True,**kwargs)
transformers.models.mobilebert.configuration_mobilebert.MobileBertConfig(self,vocab_size=30522,hidden_size=512,num_hidden_layers=24,num_attention_heads=4,intermediate_size=512,hidden_act='relu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=128,trigram_input=True,use_bottleneck=True,intra_bottleneck_size=128,use_bottleneck_attention=False,key_query_shared_bottleneck=True,num_feedforward_networks=4,normalization_type='no_norm',classifier_activation=True,**kwargs)
transformers.models.mobilebert.configuration_mobilebert.MobileBertConfig.__init__(self,vocab_size=30522,hidden_size=512,num_hidden_layers=24,num_attention_heads=4,intermediate_size=512,hidden_act='relu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=128,trigram_input=True,use_bottleneck=True,intra_bottleneck_size=128,use_bottleneck_attention=False,key_query_shared_bottleneck=True,num_feedforward_networks=4,normalization_type='no_norm',classifier_activation=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mobilebert/tokenization_mobilebert.py----------------------------------------
A:transformers.models.mobilebert.tokenization_mobilebert.logger->utils.logging.get_logger(__name__)
transformers.MobileBertTokenizer(BertTokenizer)
transformers.models.mobilebert.tokenization_mobilebert.MobileBertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mpnet/configuration_mpnet.py----------------------------------------
A:transformers.models.mpnet.configuration_mpnet.logger->utils.logging.get_logger(__name__)
transformers.MPNetConfig(self,vocab_size=30527,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,initializer_range=0.02,layer_norm_eps=1e-12,relative_attention_num_buckets=32,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.mpnet.configuration_mpnet.MPNetConfig(self,vocab_size=30527,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,initializer_range=0.02,layer_norm_eps=1e-12,relative_attention_num_buckets=32,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.mpnet.configuration_mpnet.MPNetConfig.__init__(self,vocab_size=30527,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,initializer_range=0.02,layer_norm_eps=1e-12,relative_attention_num_buckets=32,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mpnet/modeling_mpnet.py----------------------------------------
A:transformers.models.mpnet.modeling_mpnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.mpnet.modeling_mpnet.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.models.mpnet.modeling_mpnet.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.models.mpnet.modeling_mpnet.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.mpnet.modeling_mpnet.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.mpnet.modeling_mpnet.position_ids->torch.arange(self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device)
A:transformers.models.mpnet.modeling_mpnet.input_shape->input_ids.size()
A:transformers.models.mpnet.modeling_mpnet.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.mpnet.modeling_mpnet.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.mpnet.modeling_mpnet.embeddings->self.dropout(embeddings)
A:transformers.models.mpnet.modeling_mpnet.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.mpnet.modeling_mpnet.self.q->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.mpnet.modeling_mpnet.self.k->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.mpnet.modeling_mpnet.self.v->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.mpnet.modeling_mpnet.self.o->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.mpnet.modeling_mpnet.x->self.out_proj(x)
A:transformers.models.mpnet.modeling_mpnet.q->self.transpose_for_scores(q)
A:transformers.models.mpnet.modeling_mpnet.k->self.transpose_for_scores(k)
A:transformers.models.mpnet.modeling_mpnet.v->self.transpose_for_scores(v)
A:transformers.models.mpnet.modeling_mpnet.attention_scores->torch.matmul(q, k.transpose(-1, -2))
A:transformers.models.mpnet.modeling_mpnet.attention_probs->self.dropout(attention_probs)
A:transformers.models.mpnet.modeling_mpnet.c->c.view(*new_c_shape).view(*new_c_shape)
A:transformers.models.mpnet.modeling_mpnet.o->self.o(c)
A:transformers.models.mpnet.modeling_mpnet.self.attn->MPNetSelfAttention(config)
A:transformers.models.mpnet.modeling_mpnet.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.mpnet.modeling_mpnet.(heads, index)->find_pruneable_heads_and_indices(heads, self.attn.num_attention_heads, self.attn.attention_head_size, self.pruned_heads)
A:transformers.models.mpnet.modeling_mpnet.self.attn.q->prune_linear_layer(self.attn.q, index)
A:transformers.models.mpnet.modeling_mpnet.self.attn.k->prune_linear_layer(self.attn.k, index)
A:transformers.models.mpnet.modeling_mpnet.self.attn.v->prune_linear_layer(self.attn.v, index)
A:transformers.models.mpnet.modeling_mpnet.self.attn.o->prune_linear_layer(self.attn.o, index, dim=1)
A:transformers.models.mpnet.modeling_mpnet.self_outputs->self.attn(hidden_states, attention_mask, head_mask, position_bias, output_attentions=output_attentions)
A:transformers.models.mpnet.modeling_mpnet.attention_output->self.LayerNorm(self.dropout(self_outputs[0]) + hidden_states)
A:transformers.models.mpnet.modeling_mpnet.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.mpnet.modeling_mpnet.hidden_states->self.LayerNorm(hidden_states + input_tensor)
A:transformers.models.mpnet.modeling_mpnet.self.attention->MPNetAttention(config)
A:transformers.models.mpnet.modeling_mpnet.self.intermediate->MPNetIntermediate(config)
A:transformers.models.mpnet.modeling_mpnet.self.output->MPNetOutput(config)
A:transformers.models.mpnet.modeling_mpnet.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, position_bias=position_bias, output_attentions=output_attentions)
A:transformers.models.mpnet.modeling_mpnet.intermediate_output->self.intermediate(attention_output)
A:transformers.models.mpnet.modeling_mpnet.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.mpnet.modeling_mpnet.self.layer->torch.nn.ModuleList([MPNetLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.mpnet.modeling_mpnet.self.relative_attention_bias->torch.nn.Embedding(config.relative_attention_num_buckets, self.n_heads)
A:transformers.models.mpnet.modeling_mpnet.position_bias->self.compute_position_bias(hidden_states)
A:transformers.models.mpnet.modeling_mpnet.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], position_bias, output_attentions=output_attentions, **kwargs)
A:transformers.models.mpnet.modeling_mpnet.rp_bucket->rp_bucket.to(x.device).to(x.device)
A:transformers.models.mpnet.modeling_mpnet.values->values.expand((bsz, -1, qlen, klen)).contiguous().expand((bsz, -1, qlen, klen)).contiguous()
A:transformers.models.mpnet.modeling_mpnet.n->torch.abs(n)
A:transformers.models.mpnet.modeling_mpnet.val_if_large->torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))
A:transformers.models.mpnet.modeling_mpnet.self.activation->torch.nn.Tanh()
A:transformers.models.mpnet.modeling_mpnet.pooled_output->self.dropout(pooled_output)
A:transformers.models.mpnet.modeling_mpnet.self.embeddings->MPNetEmbeddings(config)
A:transformers.models.mpnet.modeling_mpnet.self.encoder->MPNetEncoder(config)
A:transformers.models.mpnet.modeling_mpnet.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.mpnet.modeling_mpnet.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.mpnet.modeling_mpnet.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)
A:transformers.models.mpnet.modeling_mpnet.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mpnet.modeling_mpnet.self.mpnet->MPNetModel(config, add_pooling_layer=False)
A:transformers.models.mpnet.modeling_mpnet.self.lm_head->MPNetLMHead(config)
A:transformers.models.mpnet.modeling_mpnet.outputs->self.mpnet(input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.mpnet.modeling_mpnet.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.mpnet.modeling_mpnet.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.mpnet.modeling_mpnet.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.mpnet.modeling_mpnet.self.layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.mpnet.modeling_mpnet.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.mpnet.modeling_mpnet.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.mpnet.modeling_mpnet.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mpnet.modeling_mpnet.logits->self.qa_outputs(sequence_output)
A:transformers.models.mpnet.modeling_mpnet.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.mpnet.modeling_mpnet.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.mpnet.modeling_mpnet.sequence_output->self.dropout(sequence_output)
A:transformers.models.mpnet.modeling_mpnet.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.mpnet.modeling_mpnet.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.mpnet.modeling_mpnet.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mpnet.modeling_mpnet.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.mpnet.modeling_mpnet.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.mpnet.modeling_mpnet.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.mpnet.modeling_mpnet.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.mpnet.modeling_mpnet.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.mpnet.modeling_mpnet.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.mpnet.modeling_mpnet.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.mpnet.modeling_mpnet.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.mpnet.modeling_mpnet.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.mpnet.modeling_mpnet.mask->input_ids.ne(padding_idx).int()
transformers.MPNetForMaskedLM(self,config)
transformers.MPNetForMaskedLM.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MPNetForMaskedLM.get_output_embeddings(self)
transformers.MPNetForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.MPNetForMultipleChoice(self,config)
transformers.MPNetForMultipleChoice.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MPNetForQuestionAnswering(self,config)
transformers.MPNetForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MPNetForSequenceClassification(self,config)
transformers.MPNetForSequenceClassification.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MPNetForTokenClassification(self,config)
transformers.MPNetForTokenClassification.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MPNetLayer(self,config)
transformers.MPNetLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,position_bias=None,output_attentions=False,**kwargs)
transformers.MPNetModel(self,config,add_pooling_layer=True)
transformers.MPNetModel._prune_heads(self,heads_to_prune)
transformers.MPNetModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.MPNetModel.get_input_embeddings(self)
transformers.MPNetModel.set_input_embeddings(self,value)
transformers.MPNetPreTrainedModel(PreTrainedModel)
transformers.MPNetPreTrainedModel._init_weights(self,module)
transformers.models.mpnet.modeling_mpnet.MPNetAttention(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetAttention.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,position_bias=None,output_attentions=False,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetAttention.prune_heads(self,heads)
transformers.models.mpnet.modeling_mpnet.MPNetClassificationHead(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetClassificationHead.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetClassificationHead.forward(self,features,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetEmbeddings(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetEmbeddings.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.models.mpnet.modeling_mpnet.MPNetEmbeddings.forward(self,input_ids=None,position_ids=None,inputs_embeds=None,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetEncoder(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetEncoder.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetEncoder.compute_position_bias(self,x,position_ids=None,num_buckets=32)
transformers.models.mpnet.modeling_mpnet.MPNetEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=False,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetEncoder.relative_position_bucket(relative_position,num_buckets=32,max_distance=128)
transformers.models.mpnet.modeling_mpnet.MPNetForMaskedLM(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForMaskedLM.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForMaskedLM.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mpnet.modeling_mpnet.MPNetForMaskedLM.get_output_embeddings(self)
transformers.models.mpnet.modeling_mpnet.MPNetForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.mpnet.modeling_mpnet.MPNetForMultipleChoice(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForMultipleChoice.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForMultipleChoice.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mpnet.modeling_mpnet.MPNetForQuestionAnswering(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForQuestionAnswering.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mpnet.modeling_mpnet.MPNetForSequenceClassification(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForSequenceClassification.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForSequenceClassification.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mpnet.modeling_mpnet.MPNetForTokenClassification(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForTokenClassification.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetForTokenClassification.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.mpnet.modeling_mpnet.MPNetIntermediate(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetIntermediate.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetIntermediate.forward(self,hidden_states)
transformers.models.mpnet.modeling_mpnet.MPNetLMHead(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetLMHead.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetLMHead.forward(self,features,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetLayer(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetLayer.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,position_bias=None,output_attentions=False,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetModel(self,config,add_pooling_layer=True)
transformers.models.mpnet.modeling_mpnet.MPNetModel.__init__(self,config,add_pooling_layer=True)
transformers.models.mpnet.modeling_mpnet.MPNetModel._prune_heads(self,heads_to_prune)
transformers.models.mpnet.modeling_mpnet.MPNetModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetModel.get_input_embeddings(self)
transformers.models.mpnet.modeling_mpnet.MPNetModel.set_input_embeddings(self,value)
transformers.models.mpnet.modeling_mpnet.MPNetOutput(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetOutput.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetOutput.forward(self,hidden_states,input_tensor)
transformers.models.mpnet.modeling_mpnet.MPNetPooler(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetPooler.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetPooler.forward(self,hidden_states)
transformers.models.mpnet.modeling_mpnet.MPNetPreTrainedModel(PreTrainedModel)
transformers.models.mpnet.modeling_mpnet.MPNetPreTrainedModel._init_weights(self,module)
transformers.models.mpnet.modeling_mpnet.MPNetSelfAttention(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetSelfAttention.__init__(self,config)
transformers.models.mpnet.modeling_mpnet.MPNetSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,position_bias=None,output_attentions=False,**kwargs)
transformers.models.mpnet.modeling_mpnet.MPNetSelfAttention.transpose_for_scores(self,x)
transformers.models.mpnet.modeling_mpnet.create_position_ids_from_input_ids(input_ids,padding_idx)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mpnet/tokenization_mpnet_fast.py----------------------------------------
A:transformers.models.mpnet.tokenization_mpnet_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.mpnet.tokenization_mpnet_fast.pre_tok_state->json.loads(self.backend_tokenizer.normalizer.__getstate__())
A:transformers.models.mpnet.tokenization_mpnet_fast.pre_tok_class->getattr(normalizers, pre_tok_state.pop('type'))
A:transformers.models.mpnet.tokenization_mpnet_fast.self.backend_tokenizer.normalizer->pre_tok_class(**pre_tok_state)
A:transformers.models.mpnet.tokenization_mpnet_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
transformers.MPNetTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='[UNK]',pad_token='<pad>',mask_token='<mask>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.MPNetTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.MPNetTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MPNetTokenizerFast.mask_token(self)->str
transformers.MPNetTokenizerFast.mask_token(self,value)
transformers.MPNetTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='[UNK]',pad_token='<pad>',mask_token='<mask>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,do_lower_case=True,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='[UNK]',pad_token='<pad>',mask_token='<mask>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast.mask_token(self)->str
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast.mask_token(self,value)
transformers.models.mpnet.tokenization_mpnet_fast.MPNetTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mpnet/modeling_tf_mpnet.py----------------------------------------
A:transformers.models.mpnet.modeling_tf_mpnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.mpnet.modeling_tf_mpnet.output->self.call(inputs)
A:transformers.models.mpnet.modeling_tf_mpnet.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.mpnet.modeling_tf_mpnet.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.mpnet.modeling_tf_mpnet.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.mpnet.modeling_tf_mpnet.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.mpnet.modeling_tf_mpnet.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.mpnet.modeling_tf_mpnet.mask->tensorflow.cast(tf.math.not_equal(input_ids, self.padding_idx), dtype=input_ids.dtype)
A:transformers.models.mpnet.modeling_tf_mpnet.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.mpnet.modeling_tf_mpnet.position_ids->tensorflow.tile(input=position_ids, multiples=(input_shape[0], 1))
A:transformers.models.mpnet.modeling_tf_mpnet.position_embeds->tensorflow.gather(params=self.position_embeddings, indices=position_ids)
A:transformers.models.mpnet.modeling_tf_mpnet.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.mpnet.modeling_tf_mpnet.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), activation='tanh', name='dense')
A:transformers.models.mpnet.modeling_tf_mpnet.pooled_output->self.dropout(pooled_output, training=inputs['training'])
A:transformers.models.mpnet.modeling_tf_mpnet.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.mpnet.modeling_tf_mpnet.self.q->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='q')
A:transformers.models.mpnet.modeling_tf_mpnet.self.k->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='k')
A:transformers.models.mpnet.modeling_tf_mpnet.self.v->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='v')
A:transformers.models.mpnet.modeling_tf_mpnet.self.o->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='o')
A:transformers.models.mpnet.modeling_tf_mpnet.x->self.out_proj(x)
A:transformers.models.mpnet.modeling_tf_mpnet.q->self.transpose_for_scores(q, batch_size)
A:transformers.models.mpnet.modeling_tf_mpnet.k->self.transpose_for_scores(k, batch_size)
A:transformers.models.mpnet.modeling_tf_mpnet.v->self.transpose_for_scores(v, batch_size)
A:transformers.models.mpnet.modeling_tf_mpnet.attention_scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.models.mpnet.modeling_tf_mpnet.dk->tensorflow.cast(shape_list(k)[-1], attention_scores.dtype)
A:transformers.models.mpnet.modeling_tf_mpnet.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.models.mpnet.modeling_tf_mpnet.c->tensorflow.reshape(c, (batch_size, -1, self.all_head_size))
A:transformers.models.mpnet.modeling_tf_mpnet.o->self.o(c)
A:transformers.models.mpnet.modeling_tf_mpnet.self.attn->TFMPNetSelfAttention(config, name='attn')
A:transformers.models.mpnet.modeling_tf_mpnet.self_outputs->self.attn(input_tensor, attention_mask, head_mask, output_attentions, position_bias=position_bias, training=training)
A:transformers.models.mpnet.modeling_tf_mpnet.attention_output->self.LayerNorm(self.dropout(self_outputs[0]) + input_tensor)
A:transformers.models.mpnet.modeling_tf_mpnet.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.mpnet.modeling_tf_mpnet.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.bias)
A:transformers.models.mpnet.modeling_tf_mpnet.self.attention->TFMPNetAttention(config, name='attention')
A:transformers.models.mpnet.modeling_tf_mpnet.self.intermediate->TFMPNetIntermediate(config, name='intermediate')
A:transformers.models.mpnet.modeling_tf_mpnet.self.out->TFMPNetOutput(config, name='output')
A:transformers.models.mpnet.modeling_tf_mpnet.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions, position_bias=position_bias, training=training)
A:transformers.models.mpnet.modeling_tf_mpnet.intermediate_output->self.intermediate(attention_output)
A:transformers.models.mpnet.modeling_tf_mpnet.layer_output->self.out(intermediate_output, attention_output, training=training)
A:transformers.models.mpnet.modeling_tf_mpnet.self.relative_attention_bias->self.add_weight(name='embeddings', shape=[self.relative_attention_num_buckets, self.n_heads], initializer=get_initializer(self.initializer_range))
A:transformers.models.mpnet.modeling_tf_mpnet.position_bias->self.compute_position_bias(hidden_states)
A:transformers.models.mpnet.modeling_tf_mpnet.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], output_attentions, position_bias=position_bias, training=training)
A:transformers.models.mpnet.modeling_tf_mpnet.n->tensorflow.math.abs(n)
A:transformers.models.mpnet.modeling_tf_mpnet.is_small->tensorflow.math.less(n, max_exact)
A:transformers.models.mpnet.modeling_tf_mpnet.val_if_large->tensorflow.math.minimum(val_if_large, num_buckets - 1)
A:transformers.models.mpnet.modeling_tf_mpnet.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.mpnet.modeling_tf_mpnet.rp_bucket->self._relative_position_bucket(relative_position, num_buckets=self.relative_attention_num_buckets)
A:transformers.models.mpnet.modeling_tf_mpnet.values->tensorflow.expand_dims(tf.transpose(values, [2, 0, 1]), axis=0)
A:transformers.models.mpnet.modeling_tf_mpnet.self.encoder->TFMPNetEncoder(config, name='encoder')
A:transformers.models.mpnet.modeling_tf_mpnet.self.pooler->TFMPNetPooler(config, name='pooler')
A:transformers.models.mpnet.modeling_tf_mpnet.self.embeddings->TFMPNetEmbeddings(config, name='embeddings')
A:transformers.models.mpnet.modeling_tf_mpnet.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.mpnet.modeling_tf_mpnet.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.mpnet.modeling_tf_mpnet.embedding_output->self.embeddings(inputs['input_ids'], inputs['position_ids'], inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.mpnet.modeling_tf_mpnet.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.mpnet.modeling_tf_mpnet.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.mpnet.modeling_tf_mpnet.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.mpnet.modeling_tf_mpnet.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, inputs['head_mask'], inputs['output_attentions'], inputs['output_hidden_states'], inputs['return_dict'], training=inputs['training'])
A:transformers.models.mpnet.modeling_tf_mpnet.self.mpnet->TFMPNetMainLayer(config, name='mpnet')
A:transformers.models.mpnet.modeling_tf_mpnet.outputs->self.mpnet(inputs['input_ids'], attention_mask=inputs['attention_mask'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.mpnet.modeling_tf_mpnet.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.mpnet.modeling_tf_mpnet.self.act->get_tf_activation('gelu')
A:transformers.models.mpnet.modeling_tf_mpnet.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.mpnet.modeling_tf_mpnet.self.lm_head->TFMPNetLMHead(config, self.mpnet.embeddings, name='lm_head')
A:transformers.models.mpnet.modeling_tf_mpnet.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.mpnet.modeling_tf_mpnet.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.models.mpnet.modeling_tf_mpnet.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.mpnet.modeling_tf_mpnet.logits->self.qa_outputs(sequence_output)
A:transformers.models.mpnet.modeling_tf_mpnet.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.mpnet.modeling_tf_mpnet.sequence_output->self.dropout(sequence_output, training=inputs['training'])
A:transformers.models.mpnet.modeling_tf_mpnet.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.mpnet.modeling_tf_mpnet.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.mpnet.modeling_tf_mpnet.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.mpnet.modeling_tf_mpnet.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.mpnet.modeling_tf_mpnet.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFMPNetForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFMPNetForMaskedLM.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMPNetForMaskedLM.get_lm_head(self)
transformers.TFMPNetForMaskedLM.get_prefix_bias_name(self)
transformers.TFMPNetForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFMPNetForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFMPNetForMultipleChoice.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMPNetForMultipleChoice.dummy_inputs(self)
transformers.TFMPNetForMultipleChoice.serving(self,inputs)
transformers.TFMPNetForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFMPNetForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFMPNetForQuestionAnswering.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFMPNetForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFMPNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFMPNetForSequenceClassification.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMPNetForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFMPNetForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFMPNetForTokenClassification.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMPNetForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFMPNetMainLayer(self,config,**kwargs)
transformers.TFMPNetMainLayer._prune_heads(self,heads_to_prune)
transformers.TFMPNetMainLayer.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMPNetMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.TFMPNetMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.TFMPNetModel(self,config,*inputs,**kwargs)
transformers.TFMPNetModel.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMPNetModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.TFMPNetPreTrainedModel(TFPreTrainedModel)
transformers.TFMPNetPreTrainedModel.serving(self,inputs)
transformers.models.mpnet.TFMPNetEmbeddings(self,config,**kwargs)
transformers.models.mpnet.TFMPNetEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.mpnet.TFMPNetEmbeddings.call(self,input_ids=None,position_ids=None,inputs_embeds=None,training=False)
transformers.models.mpnet.TFMPNetEmbeddings.create_position_ids_from_input_ids(self,input_ids)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetAttention(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetAttention.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetAttention.call(self,input_tensor,attention_mask,head_mask,output_attentions,position_bias=None,training=False)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetAttention.prune_heads(self,heads)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetClassificationHead(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetClassificationHead.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetClassificationHead.call(self,features,training=False)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEmbeddings(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEmbeddings.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEmbeddings.call(self,input_ids=None,position_ids=None,inputs_embeds=None,training=False)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEmbeddings.create_position_ids_from_input_ids(self,input_ids)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEncoder(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEncoder.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEncoder._relative_position_bucket(relative_position,num_buckets=32,max_distance=128)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEncoder.build(self,input_shape)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEncoder.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetEncoder.compute_position_bias(self,x,position_ids=None)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMaskedLM.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMaskedLM.get_lm_head(self)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMaskedLM.get_prefix_bias_name(self)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMultipleChoice.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMultipleChoice.dummy_inputs(self)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMultipleChoice.serving(self,inputs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForQuestionAnswering.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForSequenceClassification.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForTokenClassification.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetIntermediate(self,config:MPNetConfig,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetIntermediate.__init__(self,config:MPNetConfig,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead(self,config,input_embeddings,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.build(self,input_shape)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.call(self,hidden_states)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.get_bias(self)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.get_output_embeddings(self)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.set_bias(self,value)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLMHead.set_output_embeddings(self,value)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLayer(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLayer.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,position_bias=None,training=False)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetMainLayer(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetMainLayer.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetMainLayer._prune_heads(self,heads_to_prune)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetMainLayer.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetModel(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetModel.__init__(self,config,*inputs,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetModel.call(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetOutput(self,config:MPNetConfig,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetOutput.__init__(self,config:MPNetConfig,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetPooler(self,config:MPNetConfig,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetPooler.__init__(self,config:MPNetConfig,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetPooler.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetPreTrainedModel(TFPreTrainedModel)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetPreTrainedModel.serving(self,inputs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetSelfAttention(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetSelfAttention.__init__(self,config,**kwargs)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetSelfAttention.call(self,hidden_states,attention_mask,head_mask,output_attentions,position_bias=None,training=False)
transformers.models.mpnet.modeling_tf_mpnet.TFMPNetSelfAttention.transpose_for_scores(self,x,batch_size)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mpnet/tokenization_mpnet.py----------------------------------------
A:transformers.models.mpnet.tokenization_mpnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.mpnet.tokenization_mpnet.vocab->collections.OrderedDict()
A:transformers.models.mpnet.tokenization_mpnet.tokens->unicodedata.normalize('NFD', text).split()
A:transformers.models.mpnet.tokenization_mpnet.token->self._run_strip_accents(token)
A:transformers.models.mpnet.tokenization_mpnet.text->unicodedata.normalize('NFD', text)
A:transformers.models.mpnet.tokenization_mpnet.self.vocab->load_vocab(vocab_file)
A:transformers.models.mpnet.tokenization_mpnet.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.models.mpnet.tokenization_mpnet.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars, strip_accents=strip_accents)
A:transformers.models.mpnet.tokenization_mpnet.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.models.mpnet.tokenization_mpnet.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.models.mpnet.tokenization_mpnet.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.models.mpnet.tokenization_mpnet.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.mpnet.tokenization_mpnet.self.never_split->set(never_split)
A:transformers.models.mpnet.tokenization_mpnet.orig_tokens->whitespace_tokenize(text)
A:transformers.models.mpnet.tokenization_mpnet.output_tokens->whitespace_tokenize(' '.join(split_tokens))
A:transformers.models.mpnet.tokenization_mpnet.cat->unicodedata.category(char)
A:transformers.models.mpnet.tokenization_mpnet.chars->list(token)
A:transformers.models.mpnet.tokenization_mpnet.cp->ord(char)
A:transformers.models.mpnet.tokenization_mpnet.end->len(chars)
A:transformers.models.mpnet.tokenization_mpnet.substr->''.join(chars[start:end])
transformers.MPNetTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='[UNK]',pad_token='<pad>',mask_token='<mask>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.MPNetTokenizer._convert_id_to_token(self,index)
transformers.MPNetTokenizer._convert_token_to_id(self,token)
transformers.MPNetTokenizer._tokenize(self,text)
transformers.MPNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MPNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.MPNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MPNetTokenizer.do_lower_case(self)
transformers.MPNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.MPNetTokenizer.get_vocab(self)
transformers.MPNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.MPNetTokenizer.vocab_size(self)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer.__init__(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer._clean_text(self,text)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer._is_chinese_char(self,cp)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer._run_strip_accents(self,text)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.models.mpnet.tokenization_mpnet.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='[UNK]',pad_token='<pad>',mask_token='<mask>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='[UNK]',pad_token='<pad>',mask_token='<mask>',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer._convert_id_to_token(self,index)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer._convert_token_to_id(self,token)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer._tokenize(self,text)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.do_lower_case(self)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.get_vocab(self)
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.mpnet.tokenization_mpnet.MPNetTokenizer.vocab_size(self)
transformers.models.mpnet.tokenization_mpnet.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.models.mpnet.tokenization_mpnet.WordpieceTokenizer.__init__(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.models.mpnet.tokenization_mpnet.WordpieceTokenizer.tokenize(self,text)
transformers.models.mpnet.tokenization_mpnet.load_vocab(vocab_file)
transformers.models.mpnet.tokenization_mpnet.whitespace_tokenize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/mpnet/__init__.py----------------------------------------
A:transformers.models.mpnet.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/camembert/modeling_tf_camembert.py----------------------------------------
A:transformers.models.camembert.modeling_tf_camembert.logger->utils.logging.get_logger(__name__)
transformers.TFCamembertForMaskedLM(TFRobertaForMaskedLM)
transformers.TFCamembertForMultipleChoice(TFRobertaForMultipleChoice)
transformers.TFCamembertForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.TFCamembertForSequenceClassification(TFRobertaForSequenceClassification)
transformers.TFCamembertForTokenClassification(TFRobertaForTokenClassification)
transformers.TFCamembertModel(TFRobertaModel)
transformers.models.camembert.modeling_tf_camembert.TFCamembertForMaskedLM(TFRobertaForMaskedLM)
transformers.models.camembert.modeling_tf_camembert.TFCamembertForMultipleChoice(TFRobertaForMultipleChoice)
transformers.models.camembert.modeling_tf_camembert.TFCamembertForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.models.camembert.modeling_tf_camembert.TFCamembertForSequenceClassification(TFRobertaForSequenceClassification)
transformers.models.camembert.modeling_tf_camembert.TFCamembertForTokenClassification(TFRobertaForTokenClassification)
transformers.models.camembert.modeling_tf_camembert.TFCamembertModel(TFRobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/camembert/modeling_camembert.py----------------------------------------
A:transformers.models.camembert.modeling_camembert.logger->utils.logging.get_logger(__name__)
transformers.CamembertForCausalLM(RobertaForCausalLM)
transformers.CamembertForMaskedLM(RobertaForMaskedLM)
transformers.CamembertForMultipleChoice(RobertaForMultipleChoice)
transformers.CamembertForQuestionAnswering(RobertaForQuestionAnswering)
transformers.CamembertForSequenceClassification(RobertaForSequenceClassification)
transformers.CamembertForTokenClassification(RobertaForTokenClassification)
transformers.CamembertModel(RobertaModel)
transformers.models.camembert.modeling_camembert.CamembertForCausalLM(RobertaForCausalLM)
transformers.models.camembert.modeling_camembert.CamembertForMaskedLM(RobertaForMaskedLM)
transformers.models.camembert.modeling_camembert.CamembertForMultipleChoice(RobertaForMultipleChoice)
transformers.models.camembert.modeling_camembert.CamembertForQuestionAnswering(RobertaForQuestionAnswering)
transformers.models.camembert.modeling_camembert.CamembertForSequenceClassification(RobertaForSequenceClassification)
transformers.models.camembert.modeling_camembert.CamembertForTokenClassification(RobertaForTokenClassification)
transformers.models.camembert.modeling_camembert.CamembertModel(RobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/camembert/configuration_camembert.py----------------------------------------
A:transformers.models.camembert.configuration_camembert.logger->utils.logging.get_logger(__name__)
transformers.CamembertConfig(RobertaConfig)
transformers.models.camembert.configuration_camembert.CamembertConfig(RobertaConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/camembert/tokenization_camembert_fast.py----------------------------------------
A:transformers.models.camembert.tokenization_camembert_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.camembert.tokenization_camembert_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.CamembertTokenizerFast(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],**kwargs)
transformers.CamembertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CamembertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.camembert.tokenization_camembert_fast.CamembertTokenizerFast(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],**kwargs)
transformers.models.camembert.tokenization_camembert_fast.CamembertTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],**kwargs)
transformers.models.camembert.tokenization_camembert_fast.CamembertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.camembert.tokenization_camembert_fast.CamembertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.camembert.tokenization_camembert_fast.CamembertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/camembert/tokenization_camembert.py----------------------------------------
A:transformers.models.camembert.tokenization_camembert.logger->utils.logging.get_logger(__name__)
A:transformers.models.camembert.tokenization_camembert.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.camembert.tokenization_camembert.self.fairseq_offset->len(self.fairseq_tokens_to_ids)
A:transformers.models.camembert.tokenization_camembert.state->self.__dict__.copy()
A:transformers.models.camembert.tokenization_camembert.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.camembert.tokenization_camembert.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.CamembertTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.CamembertTokenizer.__getstate__(self)
transformers.CamembertTokenizer.__setstate__(self,d)
transformers.CamembertTokenizer._convert_id_to_token(self,index)
transformers.CamembertTokenizer._convert_token_to_id(self,token)
transformers.CamembertTokenizer._tokenize(self,text:str)->List[str]
transformers.CamembertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CamembertTokenizer.convert_tokens_to_string(self,tokens)
transformers.CamembertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CamembertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.CamembertTokenizer.get_vocab(self)
transformers.CamembertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.CamembertTokenizer.vocab_size(self)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.__getstate__(self)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.__setstate__(self,d)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer._convert_id_to_token(self,index)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer._convert_token_to_id(self,token)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer._tokenize(self,text:str)->List[str]
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.get_vocab(self)
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.camembert.tokenization_camembert.CamembertTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/camembert/__init__.py----------------------------------------
A:transformers.models.camembert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ibert/configuration_ibert.py----------------------------------------
A:transformers.models.ibert.configuration_ibert.logger->utils.logging.get_logger(__name__)
transformers.IBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=1,bos_token_id=0,eos_token_id=2,position_embedding_type='absolute',quant_mode=False,force_dequant='none',**kwargs)
transformers.models.ibert.configuration_ibert.IBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=1,bos_token_id=0,eos_token_id=2,position_embedding_type='absolute',quant_mode=False,force_dequant='none',**kwargs)
transformers.models.ibert.configuration_ibert.IBertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=1,bos_token_id=0,eos_token_id=2,position_embedding_type='absolute',quant_mode=False,force_dequant='none',**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ibert/modeling_ibert.py----------------------------------------
A:transformers.models.ibert.modeling_ibert.logger->utils.logging.get_logger(__name__)
A:transformers.models.ibert.modeling_ibert.self.word_embeddings->QuantEmbedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id, weight_bit=self.embedding_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.token_type_embeddings->QuantEmbedding(config.type_vocab_size, config.hidden_size, weight_bit=self.embedding_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.ibert.modeling_ibert.self.position_embeddings->QuantEmbedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx, weight_bit=self.embedding_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.embeddings_act1->QuantAct(self.embedding_act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.embeddings_act2->QuantAct(self.embedding_act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.LayerNorm->IntLayerNorm(config.hidden_size, eps=config.layer_norm_eps, output_bit=self.ln_output_bit, quant_mode=self.quant_mode, force_dequant=config.force_dequant)
A:transformers.models.ibert.modeling_ibert.self.output_activation->QuantAct(self.act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.ibert.modeling_ibert.position_ids->torch.arange(self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device)
A:transformers.models.ibert.modeling_ibert.input_shape->input_ids.size()
A:transformers.models.ibert.modeling_ibert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.ibert.modeling_ibert.(inputs_embeds, inputs_embeds_scaling_factor)->self.word_embeddings(input_ids)
A:transformers.models.ibert.modeling_ibert.(token_type_embeddings, token_type_embeddings_scaling_factor)->self.token_type_embeddings(token_type_ids)
A:transformers.models.ibert.modeling_ibert.(embeddings, embeddings_scaling_factor)->self.output_activation(embeddings, embeddings_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(position_embeddings, position_embeddings_scaling_factor)->self.position_embeddings(position_ids)
A:transformers.models.ibert.modeling_ibert.embeddings->self.dropout(embeddings)
A:transformers.models.ibert.modeling_ibert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.ibert.modeling_ibert.self.query->QuantLinear(config.hidden_size, self.all_head_size, bias=True, weight_bit=self.weight_bit, bias_bit=self.bias_bit, quant_mode=self.quant_mode, per_channel=True)
A:transformers.models.ibert.modeling_ibert.self.key->QuantLinear(config.hidden_size, self.all_head_size, bias=True, weight_bit=self.weight_bit, bias_bit=self.bias_bit, quant_mode=self.quant_mode, per_channel=True)
A:transformers.models.ibert.modeling_ibert.self.value->QuantLinear(config.hidden_size, self.all_head_size, bias=True, weight_bit=self.weight_bit, bias_bit=self.bias_bit, quant_mode=self.quant_mode, per_channel=True)
A:transformers.models.ibert.modeling_ibert.self.query_activation->QuantAct(self.act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.key_activation->QuantAct(self.act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.value_activation->QuantAct(self.act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.softmax->IntSoftmax(self.act_bit, quant_mode=self.quant_mode, force_dequant=config.force_dequant)
A:transformers.models.ibert.modeling_ibert.x->self.decoder(x)
A:transformers.models.ibert.modeling_ibert.(mixed_query_layer, mixed_query_layer_scaling_factor)->self.query(hidden_states, hidden_states_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(mixed_key_layer, mixed_key_layer_scaling_factor)->self.key(hidden_states, hidden_states_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(mixed_value_layer, mixed_value_layer_scaling_factor)->self.value(hidden_states, hidden_states_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(query_layer, query_layer_scaling_factor)->self.query_activation(mixed_query_layer, mixed_query_layer_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(key_layer, key_layer_scaling_factor)->self.key_activation(mixed_key_layer, mixed_key_layer_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(value_layer, value_layer_scaling_factor)->self.value_activation(mixed_value_layer, mixed_value_layer_scaling_factor)
A:transformers.models.ibert.modeling_ibert.query_layer->self.transpose_for_scores(query_layer)
A:transformers.models.ibert.modeling_ibert.key_layer->self.transpose_for_scores(key_layer)
A:transformers.models.ibert.modeling_ibert.value_layer->self.transpose_for_scores(value_layer)
A:transformers.models.ibert.modeling_ibert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.ibert.modeling_ibert.scale->math.sqrt(self.attention_head_size)
A:transformers.models.ibert.modeling_ibert.(attention_probs, attention_probs_scaling_factor)->self.softmax(attention_scores, attention_scores_scaling_factor)
A:transformers.models.ibert.modeling_ibert.attention_probs->self.dropout(attention_probs)
A:transformers.models.ibert.modeling_ibert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.ibert.modeling_ibert.(context_layer, context_layer_scaling_factor)->self.output_activation(context_layer, context_layer_scaling_factor)
A:transformers.models.ibert.modeling_ibert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.ibert.modeling_ibert.self.ln_input_act->QuantAct(self.ln_input_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.(hidden_states, hidden_states_scaling_factor)->self.output_activation(hidden_states, hidden_states_scaling_factor)
A:transformers.models.ibert.modeling_ibert.hidden_states->self.out_proj(hidden_states)
A:transformers.models.ibert.modeling_ibert.self.self->IBertSelfAttention(config)
A:transformers.models.ibert.modeling_ibert.self.output->IBertOutput(config)
A:transformers.models.ibert.modeling_ibert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.ibert.modeling_ibert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.ibert.modeling_ibert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.ibert.modeling_ibert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.ibert.modeling_ibert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.ibert.modeling_ibert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.ibert.modeling_ibert.(self_outputs, self_outputs_scaling_factor)->self.self(hidden_states, hidden_states_scaling_factor, attention_mask, head_mask, output_attentions)
A:transformers.models.ibert.modeling_ibert.(attention_output, attention_output_scaling_factor)->self.pre_intermediate_act(attention_output, attention_output_scaling_factor)
A:transformers.models.ibert.modeling_ibert.self.intermediate_act_fn->IntGELU(quant_mode=self.quant_mode, force_dequant=config.force_dequant)
A:transformers.models.ibert.modeling_ibert.self.attention->IBertAttention(config)
A:transformers.models.ibert.modeling_ibert.self.intermediate->IBertIntermediate(config)
A:transformers.models.ibert.modeling_ibert.self.pre_intermediate_act->QuantAct(self.act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.self.pre_output_act->QuantAct(self.act_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.modeling_ibert.(self_attention_outputs, self_attention_outputs_scaling_factor)->self.attention(hidden_states, hidden_states_scaling_factor, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.models.ibert.modeling_ibert.(layer_output, layer_output_scaling_factor)->self.output(intermediate_output, intermediate_output_scaling_factor, attention_output, attention_output_scaling_factor)
A:transformers.models.ibert.modeling_ibert.(intermediate_output, intermediate_output_scaling_factor)->self.pre_output_act(intermediate_output, intermediate_output_scaling_factor)
A:transformers.models.ibert.modeling_ibert.self.layer->torch.nn.ModuleList([IBertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.ibert.modeling_ibert.layer_outputs->layer_module(hidden_states, hidden_states_scaling_factor, attention_mask, layer_head_mask, output_attentions)
A:transformers.models.ibert.modeling_ibert.self.activation->torch.nn.Tanh()
A:transformers.models.ibert.modeling_ibert.pooled_output->self.dropout(pooled_output)
A:transformers.models.ibert.modeling_ibert.self.embeddings->IBertEmbeddings(config)
A:transformers.models.ibert.modeling_ibert.self.encoder->IBertEncoder(config)
A:transformers.models.ibert.modeling_ibert.attention_mask->torch.ones((batch_size, seq_length), device=device)
A:transformers.models.ibert.modeling_ibert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.ibert.modeling_ibert.(embedding_output, embedding_output_scaling_factor)->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.ibert.modeling_ibert.encoder_outputs->self.encoder(embedding_output, embedding_output_scaling_factor, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.ibert.modeling_ibert.self.ibert->IBertModel(config, add_pooling_layer=False)
A:transformers.models.ibert.modeling_ibert.self.lm_head->IBertLMHead(config)
A:transformers.models.ibert.modeling_ibert.outputs->self.ibert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.ibert.modeling_ibert.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.ibert.modeling_ibert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.ibert.modeling_ibert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.ibert.modeling_ibert.self.layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.ibert.modeling_ibert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.ibert.modeling_ibert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.ibert.modeling_ibert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.ibert.modeling_ibert.logits->self.qa_outputs(sequence_output)
A:transformers.models.ibert.modeling_ibert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.ibert.modeling_ibert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.ibert.modeling_ibert.sequence_output->self.dropout(sequence_output)
A:transformers.models.ibert.modeling_ibert.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.ibert.modeling_ibert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.ibert.modeling_ibert.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.ibert.modeling_ibert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.ibert.modeling_ibert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.ibert.modeling_ibert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.ibert.modeling_ibert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.ibert.modeling_ibert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.ibert.modeling_ibert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.ibert.modeling_ibert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.ibert.modeling_ibert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.ibert.modeling_ibert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.ibert.modeling_ibert.mask->input_ids.ne(padding_idx).int()
transformers.IBertForMaskedLM(self,config)
transformers.IBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.IBertForMaskedLM.get_output_embeddings(self)
transformers.IBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.IBertForMultipleChoice(self,config)
transformers.IBertForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.IBertForQuestionAnswering(self,config)
transformers.IBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.IBertForSequenceClassification(self,config)
transformers.IBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.IBertForTokenClassification(self,config)
transformers.IBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.IBertModel(self,config,add_pooling_layer=True)
transformers.IBertModel._prune_heads(self,heads_to_prune)
transformers.IBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.IBertModel.get_input_embeddings(self)
transformers.IBertModel.set_input_embeddings(self,value)
transformers.IBertPreTrainedModel(PreTrainedModel)
transformers.IBertPreTrainedModel._init_weights(self,module)
transformers.IBertPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)
transformers.models.ibert.modeling_ibert.IBertAttention(self,config)
transformers.models.ibert.modeling_ibert.IBertAttention.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertAttention.forward(self,hidden_states,hidden_states_scaling_factor,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.ibert.modeling_ibert.IBertAttention.prune_heads(self,heads)
transformers.models.ibert.modeling_ibert.IBertClassificationHead(self,config)
transformers.models.ibert.modeling_ibert.IBertClassificationHead.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertClassificationHead.forward(self,features,**kwargs)
transformers.models.ibert.modeling_ibert.IBertEmbeddings(self,config)
transformers.models.ibert.modeling_ibert.IBertEmbeddings.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.models.ibert.modeling_ibert.IBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.ibert.modeling_ibert.IBertEncoder(self,config)
transformers.models.ibert.modeling_ibert.IBertEncoder.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertEncoder.forward(self,hidden_states,hidden_states_scaling_factor,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.ibert.modeling_ibert.IBertForMaskedLM(self,config)
transformers.models.ibert.modeling_ibert.IBertForMaskedLM.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ibert.modeling_ibert.IBertForMaskedLM.get_output_embeddings(self)
transformers.models.ibert.modeling_ibert.IBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.ibert.modeling_ibert.IBertForMultipleChoice(self,config)
transformers.models.ibert.modeling_ibert.IBertForMultipleChoice.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ibert.modeling_ibert.IBertForQuestionAnswering(self,config)
transformers.models.ibert.modeling_ibert.IBertForQuestionAnswering.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ibert.modeling_ibert.IBertForSequenceClassification(self,config)
transformers.models.ibert.modeling_ibert.IBertForSequenceClassification.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ibert.modeling_ibert.IBertForTokenClassification(self,config)
transformers.models.ibert.modeling_ibert.IBertForTokenClassification.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ibert.modeling_ibert.IBertIntermediate(self,config)
transformers.models.ibert.modeling_ibert.IBertIntermediate.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertIntermediate.forward(self,hidden_states,hidden_states_scaling_factor)
transformers.models.ibert.modeling_ibert.IBertLMHead(self,config)
transformers.models.ibert.modeling_ibert.IBertLMHead.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertLMHead.forward(self,features,**kwargs)
transformers.models.ibert.modeling_ibert.IBertLayer(self,config)
transformers.models.ibert.modeling_ibert.IBertLayer.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertLayer.feed_forward_chunk(self,attention_output,attention_output_scaling_factor)
transformers.models.ibert.modeling_ibert.IBertLayer.forward(self,hidden_states,hidden_states_scaling_factor,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.ibert.modeling_ibert.IBertModel(self,config,add_pooling_layer=True)
transformers.models.ibert.modeling_ibert.IBertModel.__init__(self,config,add_pooling_layer=True)
transformers.models.ibert.modeling_ibert.IBertModel._prune_heads(self,heads_to_prune)
transformers.models.ibert.modeling_ibert.IBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.ibert.modeling_ibert.IBertModel.get_input_embeddings(self)
transformers.models.ibert.modeling_ibert.IBertModel.set_input_embeddings(self,value)
transformers.models.ibert.modeling_ibert.IBertOutput(self,config)
transformers.models.ibert.modeling_ibert.IBertOutput.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertOutput.forward(self,hidden_states,hidden_states_scaling_factor,input_tensor,input_tensor_scaling_factor)
transformers.models.ibert.modeling_ibert.IBertPooler(self,config)
transformers.models.ibert.modeling_ibert.IBertPooler.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertPooler.forward(self,hidden_states)
transformers.models.ibert.modeling_ibert.IBertPreTrainedModel(PreTrainedModel)
transformers.models.ibert.modeling_ibert.IBertPreTrainedModel._init_weights(self,module)
transformers.models.ibert.modeling_ibert.IBertPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)
transformers.models.ibert.modeling_ibert.IBertSelfAttention(self,config)
transformers.models.ibert.modeling_ibert.IBertSelfAttention.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertSelfAttention.forward(self,hidden_states,hidden_states_scaling_factor,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.ibert.modeling_ibert.IBertSelfAttention.transpose_for_scores(self,x)
transformers.models.ibert.modeling_ibert.IBertSelfOutput(self,config)
transformers.models.ibert.modeling_ibert.IBertSelfOutput.__init__(self,config)
transformers.models.ibert.modeling_ibert.IBertSelfOutput.forward(self,hidden_states,hidden_states_scaling_factor,input_tensor,input_tensor_scaling_factor)
transformers.models.ibert.modeling_ibert.create_position_ids_from_input_ids(input_ids,padding_idx,past_key_values_length=0)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ibert/__init__.py----------------------------------------
A:transformers.models.ibert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/ibert/quant_modules.py----------------------------------------
A:transformers.models.ibert.quant_modules.logger->utils.logging.get_logger(__name__)
A:transformers.models.ibert.quant_modules.self.weight->torch.nn.Parameter(torch.zeros(normalized_shape))
A:transformers.models.ibert.quant_modules.w_transform->w.data.detach()
A:transformers.models.ibert.quant_modules.w_min->w.data.detach().min().expand(1)
A:transformers.models.ibert.quant_modules.w_max->w.data.detach().max().expand(1)
A:transformers.models.ibert.quant_modules.self.weight_scaling_factor->symmetric_linear_quantization_params(self.weight_bit, w_min, w_max, False)
A:transformers.models.ibert.quant_modules.self.weight_integer->self.weight_function(self.weight, self.weight_bit, self.percentile_mode, self.fc_scaling_factor)
A:transformers.models.ibert.quant_modules.emb_int->torch.nn.functional.embedding(x, self.weight_integer, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)
A:transformers.models.ibert.quant_modules.x_min->x_act.data.min()
A:transformers.models.ibert.quant_modules.x_max->x_act.data.max()
A:transformers.models.ibert.quant_modules.self.x_min->torch.min(self.x_min, x_min)
A:transformers.models.ibert.quant_modules.self.x_max->torch.max(self.x_max, x_max)
A:transformers.models.ibert.quant_modules.self.act_scaling_factor->symmetric_linear_quantization_params(self.activation_bit, x_min, x_max, per_channel=self.per_channel)
A:transformers.models.ibert.quant_modules.quant_act_int->FixedPointMul.apply(x, pre_act_scaling_factor, self.activation_bit, self.act_scaling_factor, identity, identity_scaling_factor)
A:transformers.models.ibert.quant_modules.correct_output_scale->self.act_scaling_factor.view(-1)
A:transformers.models.ibert.quant_modules.self.bias->torch.nn.Parameter(torch.zeros(normalized_shape))
A:transformers.models.ibert.quant_modules.s->super().__repr__()
A:transformers.models.ibert.quant_modules.(w_min, _)->torch.min(w_transform, dim=1, out=None)
A:transformers.models.ibert.quant_modules.(w_max, _)->torch.max(w_transform, dim=1, out=None)
A:transformers.models.ibert.quant_modules.self.fc_scaling_factor->symmetric_linear_quantization_params(self.weight_bit, w_min, w_max, self.per_channel)
A:transformers.models.ibert.quant_modules.self.bias_integer->self.weight_function(self.bias, self.bias_bit, False, bias_scaling_factor)
A:transformers.models.ibert.quant_modules.prev_act_scaling_factor->prev_act_scaling_factor.view(1, -1).view(1, -1)
A:transformers.models.ibert.quant_modules.self.activation_fn->torch.nn.GELU()
A:transformers.models.ibert.quant_modules.b_int->torch.floor(self.coef[1] / scaling_factor)
A:transformers.models.ibert.quant_modules.c_int->torch.floor(self.coef[2] / scaling_factor ** 2)
A:transformers.models.ibert.quant_modules.sign->torch.sign(x_int)
A:transformers.models.ibert.quant_modules.abs_int->torch.min(torch.abs(x_int), -b_int)
A:transformers.models.ibert.quant_modules.y_int->floor_ste.apply(y_int * factor / 2)
A:transformers.models.ibert.quant_modules.(sigmoid_int, sigmoid_scaling_factor)->self.int_erf(x_int, scaling_factor / self.k)
A:transformers.models.ibert.quant_modules.self.act->QuantAct(16, quant_mode=self.quant_mode)
A:transformers.models.ibert.quant_modules.x0_int->torch.floor(self.x0 / scaling_factor)
A:transformers.models.ibert.quant_modules.x_int->torch.max(x_int, self.const * x0_int)
A:transformers.models.ibert.quant_modules.q->floor_ste.apply(x_int / x0_int)
A:transformers.models.ibert.quant_modules.(exp_int, exp_scaling_factor)->self.int_exp(x_int, scaling_factor)
A:transformers.models.ibert.quant_modules.exp_int->floor_ste.apply(exp_int * factor / 2 ** (self.max_bit - self.output_bit))
A:transformers.models.ibert.quant_modules.(x_int_max, _)->torch.max(x_int, self.const * x0_int).max(dim=-1, keepdim=True)
A:transformers.models.ibert.quant_modules.(exp, exp_scaling_factor)->self.act(exp_int, exp_scaling_factor)
A:transformers.models.ibert.quant_modules.exp_int_sum->floor_ste.apply(exp_int * factor / 2 ** (self.max_bit - self.output_bit)).sum(dim=-1, keepdim=True)
A:transformers.models.ibert.quant_modules.factor->floor_ste.apply(2 ** 31 / std_int)
A:transformers.models.ibert.quant_modules.self.activation->QuantAct(self.output_bit, quant_mode=self.quant_mode)
A:transformers.models.ibert.quant_modules.var_int->self.overflow_fallback(y_int)
A:transformers.models.ibert.quant_modules.shift->torch.log2(torch.sqrt(var_int / 2 ** self.max_bit)).ceil().max()
A:transformers.models.ibert.quant_modules.self.shift->torch.max(self.shift, shift)
A:transformers.models.ibert.quant_modules.y_int_shifted->floor_ste.apply(y_int / 2 ** self.shift)
A:transformers.models.ibert.quant_modules.mean->x.mean(axis=2, keepdim=True)
A:transformers.models.ibert.quant_modules.var->torch.mean(y ** 2, axis=2, keepdim=True)
A:transformers.models.ibert.quant_modules.n->torch.tensor(x.shape[2], dtype=torch.float)
A:transformers.models.ibert.quant_modules.self.dim_sqrt->torch.sqrt(n).to(x.device)
A:transformers.models.ibert.quant_modules.mean_int->round_ste.apply(x_int.mean(axis=2, keepdim=True))
A:transformers.models.ibert.quant_modules.bias_int->floor_ste.apply(bias / scaling_factor)
A:transformers.models.ibert.quant_modules.lower_index->round(input_length * (1 - lower_percentile * 0.01))
A:transformers.models.ibert.quant_modules.upper_index->round(input_length * upper_percentile * 0.01)
A:transformers.models.ibert.quant_modules.lower_bound->lower_bound.item().item()
A:transformers.models.ibert.quant_modules.upper_bound->upper_bound.item().item()
A:transformers.models.ibert.quant_modules.scale->scale.view(-1).view(-1)
A:transformers.models.ibert.quant_modules.zero_point->torch.tensor(0.0).to(scale.device)
A:transformers.models.ibert.quant_modules.(scale, _)->torch.max(torch.stack([saturation_min.abs(), saturation_max.abs()], dim=1), dim=1)
A:transformers.models.ibert.quant_modules.new_quant_x->torch.clamp(new_quant_x, -n, n - 1)
A:transformers.models.ibert.quant_modules.shape_of_input->inputs.view(-1).size()
A:transformers.models.ibert.quant_modules.inputs->inputs.view(-1).view(-1)
A:transformers.models.ibert.quant_modules.(output_m, output_e)->numpy.frexp(inputs.cpu().numpy())
A:transformers.models.ibert.quant_modules.int_m_shifted->int(decimal.Decimal(m * 2 ** max_bit).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_UP))
A:transformers.models.ibert.quant_modules.output_m->numpy.array(tmp_m)
A:transformers.models.ibert.quant_modules.pre_act_scaling_factor->reshape(pre_act_scaling_factor)
A:transformers.models.ibert.quant_modules.identity_scaling_factor->reshape(identity_scaling_factor)
A:transformers.models.ibert.quant_modules.z_int->torch.round(pre_act / pre_act_scaling_factor)
A:transformers.models.ibert.quant_modules._A->reshape(identity_scaling_factor).type(torch.double)
A:transformers.models.ibert.quant_modules._B->z_scaling_factor.type(torch.float).type(torch.double)
A:transformers.models.ibert.quant_modules.new_scale->reshape(new_scale)
A:transformers.models.ibert.quant_modules.(m, e)->batch_frexp(new_scale)
A:transformers.models.ibert.quant_modules.output->torch.round(output / 2.0 ** e)
A:transformers.models.ibert.quant_modules.wx_int->torch.round(identity / identity_scaling_factor)
A:transformers.models.ibert.quant_modules.(m1, e1)->batch_frexp(new_scale)
A:transformers.models.ibert.quant_modules.output1->torch.round(output1 / 2.0 ** e1)
transformers.models.ibert.quant_modules.FixedPointMul(Function)
transformers.models.ibert.quant_modules.FixedPointMul.backward(ctx,grad_output)
transformers.models.ibert.quant_modules.FixedPointMul.forward(ctx,pre_act,pre_act_scaling_factor,bit_num,z_scaling_factor,identity=None,identity_scaling_factor=None)
transformers.models.ibert.quant_modules.IntGELU(self,quant_mode=True,force_dequant='none')
transformers.models.ibert.quant_modules.IntGELU.__init__(self,quant_mode=True,force_dequant='none')
transformers.models.ibert.quant_modules.IntGELU.forward(self,x,scaling_factor=None)
transformers.models.ibert.quant_modules.IntGELU.int_erf(self,x_int,scaling_factor)
transformers.models.ibert.quant_modules.IntLayerNorm(self,normalized_shape,eps,output_bit=8,quant_mode=False,force_dequant='none')
transformers.models.ibert.quant_modules.IntLayerNorm.__init__(self,normalized_shape,eps,output_bit=8,quant_mode=False,force_dequant='none')
transformers.models.ibert.quant_modules.IntLayerNorm.forward(self,x,scaling_factor=None)
transformers.models.ibert.quant_modules.IntLayerNorm.overflow_fallback(self,y_int)
transformers.models.ibert.quant_modules.IntLayerNorm.set_shift(self,y_int)
transformers.models.ibert.quant_modules.IntSoftmax(self,output_bit,quant_mode=False,force_dequant='none')
transformers.models.ibert.quant_modules.IntSoftmax.__init__(self,output_bit,quant_mode=False,force_dequant='none')
transformers.models.ibert.quant_modules.IntSoftmax.forward(self,x,scaling_factor)
transformers.models.ibert.quant_modules.IntSoftmax.int_exp(self,x_int,scaling_factor)
transformers.models.ibert.quant_modules.IntSoftmax.int_polynomial(self,x_int,scaling_factor)
transformers.models.ibert.quant_modules.QuantAct(self,activation_bit,act_range_momentum=0.95,per_channel=False,channel_len=None,quant_mode=False)
transformers.models.ibert.quant_modules.QuantAct.__init__(self,activation_bit,act_range_momentum=0.95,per_channel=False,channel_len=None,quant_mode=False)
transformers.models.ibert.quant_modules.QuantAct.__repr__(self)
transformers.models.ibert.quant_modules.QuantAct.forward(self,x,pre_act_scaling_factor=None,identity=None,identity_scaling_factor=None,specified_min=None,specified_max=None)
transformers.models.ibert.quant_modules.QuantEmbedding(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None,weight_bit=8,momentum=0.95,quant_mode=False)
transformers.models.ibert.quant_modules.QuantEmbedding.__init__(self,num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None,weight_bit=8,momentum=0.95,quant_mode=False)
transformers.models.ibert.quant_modules.QuantEmbedding.forward(self,x,positions=None,incremental_state=None)
transformers.models.ibert.quant_modules.QuantLinear(self,in_features,out_features,bias=True,weight_bit=8,bias_bit=32,per_channel=False,quant_mode=False)
transformers.models.ibert.quant_modules.QuantLinear.__init__(self,in_features,out_features,bias=True,weight_bit=8,bias_bit=32,per_channel=False,quant_mode=False)
transformers.models.ibert.quant_modules.QuantLinear.__repr__(self)
transformers.models.ibert.quant_modules.QuantLinear.forward(self,x,prev_act_scaling_factor=None)
transformers.models.ibert.quant_modules.SymmetricQuantFunction(Function)
transformers.models.ibert.quant_modules.SymmetricQuantFunction.backward(ctx,grad_output)
transformers.models.ibert.quant_modules.SymmetricQuantFunction.forward(ctx,x,k,percentile_mode,scale)
transformers.models.ibert.quant_modules.batch_frexp(inputs,max_bit=31)
transformers.models.ibert.quant_modules.floor_ste(Function)
transformers.models.ibert.quant_modules.floor_ste.backward(ctx,grad_output)
transformers.models.ibert.quant_modules.floor_ste.forward(ctx,x)
transformers.models.ibert.quant_modules.get_percentile_min_max(input,lower_percentile,upper_percentile,output_tensor=False)
transformers.models.ibert.quant_modules.linear_quantize(input,scale,zero_point,inplace=False)
transformers.models.ibert.quant_modules.round_ste(Function)
transformers.models.ibert.quant_modules.round_ste.backward(ctx,grad_output)
transformers.models.ibert.quant_modules.round_ste.forward(ctx,x)
transformers.models.ibert.quant_modules.symmetric_linear_quantization_params(num_bits,saturation_min,saturation_max,per_channel=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/tapas/configuration_tapas.py----------------------------------------
transformers.TapasConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=1024,type_vocab_sizes=[3,256,256,2,256,256,10],initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,positive_label_weight=10.0,num_aggregation_labels=0,aggregation_loss_weight=1.0,use_answer_as_supervision=None,answer_loss_importance=1.0,use_normalized_answer_loss=False,huber_loss_delta=None,temperature=1.0,aggregation_temperature=1.0,use_gumbel_for_cells=False,use_gumbel_for_aggregation=False,average_approximation_function='ratio',cell_selection_preference=None,answer_loss_cutoff=None,max_num_rows=64,max_num_columns=32,average_logits_per_cell=False,select_one_column=True,allow_empty_column_selection=False,init_cell_selection_weights_to_zero=False,reset_position_index_per_cell=True,disable_per_token_loss=False,aggregation_labels=None,no_aggregation_label_index=None,**kwargs)
transformers.models.tapas.configuration_tapas.TapasConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=1024,type_vocab_sizes=[3,256,256,2,256,256,10],initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,positive_label_weight=10.0,num_aggregation_labels=0,aggregation_loss_weight=1.0,use_answer_as_supervision=None,answer_loss_importance=1.0,use_normalized_answer_loss=False,huber_loss_delta=None,temperature=1.0,aggregation_temperature=1.0,use_gumbel_for_cells=False,use_gumbel_for_aggregation=False,average_approximation_function='ratio',cell_selection_preference=None,answer_loss_cutoff=None,max_num_rows=64,max_num_columns=32,average_logits_per_cell=False,select_one_column=True,allow_empty_column_selection=False,init_cell_selection_weights_to_zero=False,reset_position_index_per_cell=True,disable_per_token_loss=False,aggregation_labels=None,no_aggregation_label_index=None,**kwargs)
transformers.models.tapas.configuration_tapas.TapasConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=1024,type_vocab_sizes=[3,256,256,2,256,256,10],initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,positive_label_weight=10.0,num_aggregation_labels=0,aggregation_loss_weight=1.0,use_answer_as_supervision=None,answer_loss_importance=1.0,use_normalized_answer_loss=False,huber_loss_delta=None,temperature=1.0,aggregation_temperature=1.0,use_gumbel_for_cells=False,use_gumbel_for_aggregation=False,average_approximation_function='ratio',cell_selection_preference=None,answer_loss_cutoff=None,max_num_rows=64,max_num_columns=32,average_logits_per_cell=False,select_one_column=True,allow_empty_column_selection=False,init_cell_selection_weights_to_zero=False,reset_position_index_per_cell=True,disable_per_token_loss=False,aggregation_labels=None,no_aggregation_label_index=None,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/tapas/tokenization_tapas.py----------------------------------------
A:transformers.models.tapas.tokenization_tapas.logger->utils.logging.get_logger(__name__)
A:transformers.models.tapas.tokenization_tapas.TableValue->collections.namedtuple('TokenValue', ['token', 'column_id', 'row_id'])
A:transformers.models.tapas.tokenization_tapas.vocab->collections.OrderedDict()
A:transformers.models.tapas.tokenization_tapas.tokens->normalize_for_match(row[col_index].text).split()
A:transformers.models.tapas.tokenization_tapas.token->self._run_strip_accents(token)
A:transformers.models.tapas.tokenization_tapas.text->normalize_for_match(row[col_index].text)
A:transformers.models.tapas.tokenization_tapas.self.vocab->load_vocab(vocab_file)
A:transformers.models.tapas.tokenization_tapas.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.models.tapas.tokenization_tapas.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars, strip_accents=strip_accents)
A:transformers.models.tapas.tokenization_tapas.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.models.tapas.tokenization_tapas.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.models.tapas.tokenization_tapas.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.models.tapas.tokenization_tapas.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.tapas.tokenization_tapas.is_batched->isinstance(queries, (list, tuple))
A:transformers.models.tapas.tokenization_tapas.table_tokens->self._tokenize_table(table)
A:transformers.models.tapas.tokenization_tapas.query_tokens->self.tokenize(query)
A:transformers.models.tapas.tokenization_tapas.batch_outputs->BatchEncoding(encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis)
A:transformers.models.tapas.tokenization_tapas.outputs->self.prepare_for_model(raw_table, raw_query, tokenized_table=tokenized_table, query_tokens=query_tokens, answer_coordinates=answer_coords, answer_text=answer_txt, add_special_tokens=add_special_tokens, padding=PaddingStrategy.DO_NOT_PAD.value, truncation=truncation, max_length=max_length, pad_to_multiple_of=None, return_attention_mask=False, return_token_type_ids=return_token_type_ids, return_special_tokens_mask=return_special_tokens_mask, return_length=return_length, return_tensors=None, prepend_batch_axis=False, verbose=verbose, prev_answer_coordinates=answer_coordinates[index - 1] if index != 0 else None, prev_answer_text=answer_text[index - 1] if index != 0 else None)
A:transformers.models.tapas.tokenization_tapas.encoded_inputs->self.pad(encoded_inputs, max_length=max_length, padding=padding.value, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.models.tapas.tokenization_tapas.padding->PaddingStrategy(padding)
A:transformers.models.tapas.tokenization_tapas.truncation->TapasTruncationStrategy(truncation)
A:transformers.models.tapas.tokenization_tapas.num_rows->self._get_num_rows(raw_table, truncation != TapasTruncationStrategy.DO_NOT_TRUNCATE)
A:transformers.models.tapas.tokenization_tapas.num_columns->self._get_num_columns(raw_table)
A:transformers.models.tapas.tokenization_tapas.(_, _, num_tokens)->self._get_table_boundaries(tokenized_table)
A:transformers.models.tapas.tokenization_tapas.(num_rows, num_tokens)->self._get_truncated_table_rows(query_tokens, tokenized_table, num_rows, num_columns, max_length, truncation_strategy=truncation)
A:transformers.models.tapas.tokenization_tapas.table_data->list(self._get_table_values(tokenized_table, num_columns, num_rows, num_tokens))
A:transformers.models.tapas.tokenization_tapas.query_ids->self.convert_tokens_to_ids(query_tokens)
A:transformers.models.tapas.tokenization_tapas.table_ids->self.convert_tokens_to_ids(list(table_ids))
A:transformers.models.tapas.tokenization_tapas.input_ids->self.build_inputs_with_special_tokens(query_ids, table_ids)
A:transformers.models.tapas.tokenization_tapas.segment_ids->self.create_segment_token_type_ids_from_sequences(query_ids, table_data)
A:transformers.models.tapas.tokenization_tapas.column_ids->self.create_column_token_type_ids_from_sequences(query_ids, table_data)
A:transformers.models.tapas.tokenization_tapas.row_ids->self.create_row_token_type_ids_from_sequences(query_ids, table_data)
A:transformers.models.tapas.tokenization_tapas.prev_labels->self.get_answer_ids(column_ids, row_ids, table_data, prev_answer_text, prev_answer_coordinates)
A:transformers.models.tapas.tokenization_tapas.raw_table->add_numeric_table_values(raw_table)
A:transformers.models.tapas.tokenization_tapas.raw_query->add_numeric_values_to_question(raw_query)
A:transformers.models.tapas.tokenization_tapas.(column_ranks, inv_column_ranks)->self._get_numeric_column_ranks(column_ids, row_ids, raw_table)
A:transformers.models.tapas.tokenization_tapas.numeric_relations->self._get_numeric_relations(raw_query, column_ids, row_ids, raw_table)
A:transformers.models.tapas.tokenization_tapas.attention_mask->self.create_attention_mask_from_sequences(query_ids, table_data)
A:transformers.models.tapas.tokenization_tapas.labels->self.get_answer_ids(column_ids, row_ids, table_data, answer_text, answer_coordinates)
A:transformers.models.tapas.tokenization_tapas.numeric_values->self._get_numeric_values(raw_table, column_ids, row_ids)
A:transformers.models.tapas.tokenization_tapas.numeric_values_scale->self._get_numeric_values_scale(raw_table, column_ids, row_ids)
A:transformers.models.tapas.tokenization_tapas.encoded_inputs['special_tokens_mask']->self.get_special_tokens_mask(query_ids, table_ids)
A:transformers.models.tapas.tokenization_tapas.encoded_inputs['length']->len(encoded_inputs['input_ids'])
A:transformers.models.tapas.tokenization_tapas.truncation_strategy->TapasTruncationStrategy(truncation_strategy)
A:transformers.models.tapas.tokenization_tapas.num_tokens->self._get_max_num_tokens(query_tokens, tokenized_table, num_rows=num_rows, num_columns=num_columns, max_length=max_length)
A:transformers.models.tapas.tokenization_tapas.max_num_columns->min(self.max_column_id, max_num_columns)
A:transformers.models.tapas.tokenization_tapas.max_num_rows->min(self.max_row_id, max_num_rows)
A:transformers.models.tapas.tokenization_tapas.max_num_tokens->max(max_num_tokens, tc.token_index + 1)
A:transformers.models.tapas.tokenization_tapas.token_budget->self._get_token_budget(question_tokens, max_length)
A:transformers.models.tapas.tokenization_tapas.(_, _, max_num_tokens)->self._get_table_boundaries(tokenized_table)
A:transformers.models.tapas.tokenization_tapas.cost->self._get_table_cost(tokenized_table, num_columns, num_rows, num_tokens + 1)
A:transformers.models.tapas.tokenization_tapas.(tokens, segment_ids, column_ids, row_ids)->self._serialize_text(question_tokens)
A:transformers.models.tapas.tokenization_tapas.table_numeric_values->self._get_column_values(table, column_index)
A:transformers.models.tapas.tokenization_tapas.key_fn->get_numeric_sort_key_fn(table_numeric_values.values())
A:transformers.models.tapas.tokenization_tapas.table_numeric_values_inv->collections.defaultdict(list)
A:transformers.models.tapas.tokenization_tapas.unique_values->sorted(table_numeric_values_inv.keys())
A:transformers.models.tapas.tokenization_tapas.all_values->list(table_numeric_values.values())
A:transformers.models.tapas.tokenization_tapas.cell_indices_to_relations->collections.defaultdict(set)
A:transformers.models.tapas.tokenization_tapas.sort_key_fn->self._get_numeric_sort_key_fn(table_numeric_values, value)
A:transformers.models.tapas.tokenization_tapas.relation->get_numeric_relation(value, cell_value, sort_key_fn)
A:transformers.models.tapas.tokenization_tapas.num_indices->len(indices)
A:transformers.models.tapas.tokenization_tapas.numeric_values_scale[index]->float(num_indices)
A:transformers.models.tapas.tokenization_tapas.found_answers->set()
A:transformers.models.tapas.tokenization_tapas.all_answers->set()
A:transformers.models.tapas.tokenization_tapas.token_index->self._find_tokens(cell, answer_text)
A:transformers.models.tapas.tokenization_tapas.indexes->list(self._get_cell_token_indexes(column_ids, row_ids, column_id=coordinates.column_index, row_id=coordinates.row_index - 1))
A:transformers.models.tapas.tokenization_tapas.(answer_ids, missing_count)->self._get_all_answer_ids(column_ids, row_ids, answer_coordinates)
A:transformers.models.tapas.tokenization_tapas.max_length->len(encoded_inputs['input_ids'])
A:transformers.models.tapas.tokenization_tapas.coords_to_probs->collections.defaultdict(list)
A:transformers.models.tapas.tokenization_tapas.probabilities_example->probabilities[i].tolist()
A:transformers.models.tapas.tokenization_tapas.max_width->column_ids_example.max()
A:transformers.models.tapas.tokenization_tapas.max_height->row_ids_example.max()
A:transformers.models.tapas.tokenization_tapas.cell_coords_to_prob->self._get_mean_cell_probs(probabilities_example, segment_ids_example.tolist(), row_ids_example.tolist(), column_ids_example.tolist())
A:transformers.models.tapas.tokenization_tapas.cell_prob->self._get_mean_cell_probs(probabilities_example, segment_ids_example.tolist(), row_ids_example.tolist(), column_ids_example.tolist()).get((col, row), None)
A:transformers.models.tapas.tokenization_tapas.answer_coordinates->sorted(answer_coordinates)
A:transformers.models.tapas.tokenization_tapas.predicted_aggregation_indices->logits_agg.argmax(dim=-1)
A:transformers.models.tapas.tokenization_tapas.self.never_split->set(never_split)
A:transformers.models.tapas.tokenization_tapas.orig_tokens->whitespace_tokenize(text)
A:transformers.models.tapas.tokenization_tapas.output_tokens->whitespace_tokenize(' '.join(split_tokens))
A:transformers.models.tapas.tokenization_tapas.cat->unicodedata.category(char)
A:transformers.models.tapas.tokenization_tapas.chars->list(token)
A:transformers.models.tapas.tokenization_tapas.cp->ord(char)
A:transformers.models.tapas.tokenization_tapas.end->len(chars)
A:transformers.models.tapas.tokenization_tapas.substr->''.join(chars[start:end])
A:transformers.models.tapas.tokenization_tapas._DateMask->collections.namedtuple('_DateMask', ['year', 'month', 'day'])
A:transformers.models.tapas.tokenization_tapas._YEAR->_DateMask(True, False, False)
A:transformers.models.tapas.tokenization_tapas._YEAR_MONTH->_DateMask(True, True, False)
A:transformers.models.tapas.tokenization_tapas._YEAR_MONTH_DAY->_DateMask(True, True, True)
A:transformers.models.tapas.tokenization_tapas._MONTH->_DateMask(False, True, False)
A:transformers.models.tapas.tokenization_tapas._MONTH_DAY->_DateMask(False, True, True)
A:transformers.models.tapas.tokenization_tapas.regex->regex.replace(field, field_regex).replace(field, field_regex)
A:transformers.models.tapas.tokenization_tapas._PROCESSED_DATE_PATTERNS->_process_date_patterns()
A:transformers.models.tapas.tokenization_tapas._NUMBER_PATTERN->re.compile('((^|\\s)[+-])?((\\.\\d+)|(\\d+(,\\d\\d\\d)*(\\.\\d*)?))')
A:transformers.models.tapas.tokenization_tapas._INF->float('INF')
A:transformers.models.tapas.tokenization_tapas.new_date->Date()
A:transformers.models.tapas.tokenization_tapas.date->_parse_date(span_text)
A:transformers.models.tapas.tokenization_tapas.value->sort_key_fn(value)
A:transformers.models.tapas.tokenization_tapas.span_dict->collections.defaultdict(list)
A:transformers.models.tapas.tokenization_tapas.number->_parse_number(span_text)
A:transformers.models.tapas.tokenization_tapas.spans->sorted(span_dict.items(), key=lambda span_value: _get_span_length_key(span_value[0]), reverse=True)
A:transformers.models.tapas.tokenization_tapas.value_tuple[0]->float(date.year)
A:transformers.models.tapas.tokenization_tapas.value_tuple[1]->float(date.month)
A:transformers.models.tapas.tokenization_tapas.value_tuple[2]->float(date.day)
A:transformers.models.tapas.tokenization_tapas.value_types->_get_all_types(numeric_values)
A:transformers.models.tapas.tokenization_tapas.value_type->next(iter(value_types))
A:transformers.models.tapas.tokenization_tapas.valid_indexes->set(range(_DATE_TUPLE_SIZE))
A:transformers.models.tapas.tokenization_tapas.type_counts->collections.Counter()
A:transformers.models.tapas.tokenization_tapas.max_count->max(type_counts.values())
A:transformers.models.tapas.tokenization_tapas.valid_types->set()
A:transformers.models.tapas.tokenization_tapas.max_type->next(iter(valid_types))
A:transformers.models.tapas.tokenization_tapas.numeric_spans->parse_text(question)
A:transformers.models.tapas.tokenization_tapas.index_to_values[row_index]->list(_get_numeric_values(text))
A:transformers.models.tapas.tokenization_tapas.other_value->sort_key_fn(other_value)
A:transformers.models.tapas.tokenization_tapas.question->normalize_for_match(question)
A:transformers.models.tapas.tokenization_tapas.(cell, is_invalid)->filter_invalid_unicode(cell)
A:transformers.models.tapas.tokenization_tapas.(column, is_invalid)->filter_invalid_unicode(column)
A:transformers.models.tapas.tokenization_tapas.table->table.copy().copy()
A:transformers.models.tapas.tokenization_tapas.table.iloc[row_index, col_index]->Cell(text=cell)
A:transformers.models.tapas.tokenization_tapas.column_values->_consolidate_numeric_values(_get_column_values(table, col_index), min_consolidation_fraction=min_consolidation_fraction, debug_info=(debug_info, column))
transformers.TapasTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',empty_token='[EMPTY]',tokenize_chinese_chars=True,strip_accents=None,cell_trim_length:int=-1,max_column_id:int=None,max_row_id:int=None,strip_column_names:bool=False,update_answer_coordinates:bool=False,model_max_length:int=512,additional_special_tokens:Optional[List[str]]=None,**kwargs)
transformers.TapasTokenizer._batch_encode_plus(self,table,queries:Union[List[TextInput],List[PreTokenizedInput],List[EncodedInput]],answer_coordinates:Optional[List[List[Tuple]]]=None,answer_text:Optional[List[List[TextInput]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.TapasTokenizer._batch_prepare_for_model(self,raw_table:'pd.DataFrame',raw_queries:Union[List[TextInput],List[PreTokenizedInput],List[EncodedInput]],tokenized_table:Optional[TokenizedTable]=None,queries_tokens:Optional[List[List[str]]]=None,answer_coordinates:Optional[List[List[Tuple]]]=None,answer_text:Optional[List[List[TextInput]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=True,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.TapasTokenizer._convert_id_to_token(self,index)
transformers.TapasTokenizer._convert_token_to_id(self,token)
transformers.TapasTokenizer._encode_plus(self,table:'pd.DataFrame',query:Union[TextInput,PreTokenizedInput,EncodedInput],answer_coordinates:Optional[List[Tuple]]=None,answer_text:Optional[List[TextInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=True,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)
transformers.TapasTokenizer._find_answer_coordinates_from_answer_text(self,tokenized_table,answer_text)
transformers.TapasTokenizer._find_answer_ids_from_answer_texts(self,column_ids,row_ids,tokenized_table,answer_texts)
transformers.TapasTokenizer._find_tokens(self,text,segment)
transformers.TapasTokenizer._get_all_answer_ids(self,column_ids,row_ids,answer_coordinates)
transformers.TapasTokenizer._get_all_answer_ids_from_coordinates(self,column_ids,row_ids,answers_list)
transformers.TapasTokenizer._get_answer_ids(self,column_ids,row_ids,answer_coordinates)
transformers.TapasTokenizer._get_cell_token_indexes(self,column_ids,row_ids,column_id,row_id)
transformers.TapasTokenizer._get_cell_token_probs(self,probabilities,segment_ids,row_ids,column_ids)
transformers.TapasTokenizer._get_column_values(self,table,col_index)
transformers.TapasTokenizer._get_max_num_tokens(self,question_tokens,tokenized_table,num_columns,num_rows,max_length)
transformers.TapasTokenizer._get_mean_cell_probs(self,probabilities,segment_ids,row_ids,column_ids)
transformers.TapasTokenizer._get_num_columns(self,table)
transformers.TapasTokenizer._get_num_rows(self,table,drop_rows_to_fit)
transformers.TapasTokenizer._get_numeric_column_ranks(self,column_ids,row_ids,table)
transformers.TapasTokenizer._get_numeric_relations(self,question,column_ids,row_ids,table)
transformers.TapasTokenizer._get_numeric_sort_key_fn(self,table_numeric_values,value)
transformers.TapasTokenizer._get_numeric_values(self,table,column_ids,row_ids)
transformers.TapasTokenizer._get_numeric_values_scale(self,table,column_ids,row_ids)
transformers.TapasTokenizer._get_table_boundaries(self,table)
transformers.TapasTokenizer._get_table_cost(self,table,num_columns,num_rows,num_tokens)
transformers.TapasTokenizer._get_table_values(self,table,num_columns,num_rows,num_tokens)->Generator[TableValue, None, None]
transformers.TapasTokenizer._get_token_budget(self,question_tokens,max_length=None)
transformers.TapasTokenizer._get_truncated_table_rows(self,query_tokens:List[str],tokenized_table:TokenizedTable,num_rows:int,num_columns:int,max_length:int,truncation_strategy:Union[str,TapasTruncationStrategy])->Tuple[int, int]
transformers.TapasTokenizer._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.TapasTokenizer._pad_to_seq_length(self,inputs)
transformers.TapasTokenizer._question_encoding_cost(self,question_tokens)
transformers.TapasTokenizer._serialize(self,question_tokens,table,num_columns,num_rows,num_tokens)
transformers.TapasTokenizer._serialize_text(self,question_tokens)
transformers.TapasTokenizer._tokenize(self,text)
transformers.TapasTokenizer._tokenize_table(self,table=None)
transformers.TapasTokenizer.batch_encode_plus(self,table:'pd.DataFrame',queries:Optional[Union[List[TextInput],List[PreTokenizedInput],List[EncodedInput]]]=None,answer_coordinates:Optional[List[List[Tuple]]]=None,answer_text:Optional[List[List[TextInput]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.TapasTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.TapasTokenizer.convert_logits_to_predictions(self,data,logits,logits_agg=None,cell_classification_threshold=0.5)
transformers.TapasTokenizer.convert_tokens_to_string(self,tokens)
transformers.TapasTokenizer.create_attention_mask_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.TapasTokenizer.create_column_token_type_ids_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.TapasTokenizer.create_row_token_type_ids_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.TapasTokenizer.create_segment_token_type_ids_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.TapasTokenizer.do_lower_case(self)
transformers.TapasTokenizer.encode(self,table:'pd.DataFrame',query:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,**kwargs)->List[int]
transformers.TapasTokenizer.encode_plus(self,table:'pd.DataFrame',query:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,answer_coordinates:Optional[List[Tuple]]=None,answer_text:Optional[List[TextInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.TapasTokenizer.get_answer_ids(self,column_ids,row_ids,tokenized_table,answer_texts_question,answer_coordinates_question)
transformers.TapasTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.TapasTokenizer.get_vocab(self)
transformers.TapasTokenizer.prepare_for_model(self,raw_table:'pd.DataFrame',raw_query:Union[TextInput,PreTokenizedInput,EncodedInput],tokenized_table:Optional[TokenizedTable]=None,query_tokens:Optional[TokenizedTable]=None,answer_coordinates:Optional[List[Tuple]]=None,answer_text:Optional[List[TextInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=True,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.TapasTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.TapasTokenizer.vocab_size(self)
transformers.models.tapas.tokenization_tapas.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.models.tapas.tokenization_tapas.BasicTokenizer.__init__(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.models.tapas.tokenization_tapas.BasicTokenizer._clean_text(self,text)
transformers.models.tapas.tokenization_tapas.BasicTokenizer._is_chinese_char(self,cp)
transformers.models.tapas.tokenization_tapas.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.models.tapas.tokenization_tapas.BasicTokenizer._run_strip_accents(self,text)
transformers.models.tapas.tokenization_tapas.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.models.tapas.tokenization_tapas.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.models.tapas.tokenization_tapas.Cell
transformers.models.tapas.tokenization_tapas.Date
transformers.models.tapas.tokenization_tapas.NumericValue
transformers.models.tapas.tokenization_tapas.NumericValueSpan
transformers.models.tapas.tokenization_tapas.Question
transformers.models.tapas.tokenization_tapas.Relation(enum.Enum)
transformers.models.tapas.tokenization_tapas.SerializedExample
transformers.models.tapas.tokenization_tapas.TapasTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',empty_token='[EMPTY]',tokenize_chinese_chars=True,strip_accents=None,cell_trim_length:int=-1,max_column_id:int=None,max_row_id:int=None,strip_column_names:bool=False,update_answer_coordinates:bool=False,model_max_length:int=512,additional_special_tokens:Optional[List[str]]=None,**kwargs)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',empty_token='[EMPTY]',tokenize_chinese_chars=True,strip_accents=None,cell_trim_length:int=-1,max_column_id:int=None,max_row_id:int=None,strip_column_names:bool=False,update_answer_coordinates:bool=False,model_max_length:int=512,additional_special_tokens:Optional[List[str]]=None,**kwargs)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._batch_encode_plus(self,table,queries:Union[List[TextInput],List[PreTokenizedInput],List[EncodedInput]],answer_coordinates:Optional[List[List[Tuple]]]=None,answer_text:Optional[List[List[TextInput]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.tapas.tokenization_tapas.TapasTokenizer._batch_prepare_for_model(self,raw_table:'pd.DataFrame',raw_queries:Union[List[TextInput],List[PreTokenizedInput],List[EncodedInput]],tokenized_table:Optional[TokenizedTable]=None,queries_tokens:Optional[List[List[str]]]=None,answer_coordinates:Optional[List[List[Tuple]]]=None,answer_text:Optional[List[List[TextInput]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=True,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.models.tapas.tokenization_tapas.TapasTokenizer._convert_id_to_token(self,index)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._convert_token_to_id(self,token)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._encode_plus(self,table:'pd.DataFrame',query:Union[TextInput,PreTokenizedInput,EncodedInput],answer_coordinates:Optional[List[Tuple]]=None,answer_text:Optional[List[TextInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=True,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._find_answer_coordinates_from_answer_text(self,tokenized_table,answer_text)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._find_answer_ids_from_answer_texts(self,column_ids,row_ids,tokenized_table,answer_texts)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._find_tokens(self,text,segment)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_all_answer_ids(self,column_ids,row_ids,answer_coordinates)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_all_answer_ids_from_coordinates(self,column_ids,row_ids,answers_list)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_answer_ids(self,column_ids,row_ids,answer_coordinates)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_cell_token_indexes(self,column_ids,row_ids,column_id,row_id)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_cell_token_probs(self,probabilities,segment_ids,row_ids,column_ids)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_column_values(self,table,col_index)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_max_num_tokens(self,question_tokens,tokenized_table,num_columns,num_rows,max_length)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_mean_cell_probs(self,probabilities,segment_ids,row_ids,column_ids)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_num_columns(self,table)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_num_rows(self,table,drop_rows_to_fit)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_numeric_column_ranks(self,column_ids,row_ids,table)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_numeric_relations(self,question,column_ids,row_ids,table)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_numeric_sort_key_fn(self,table_numeric_values,value)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_numeric_values(self,table,column_ids,row_ids)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_numeric_values_scale(self,table,column_ids,row_ids)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_table_boundaries(self,table)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_table_cost(self,table,num_columns,num_rows,num_tokens)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_table_values(self,table,num_columns,num_rows,num_tokens)->Generator[TableValue, None, None]
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_token_budget(self,question_tokens,max_length=None)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._get_truncated_table_rows(self,query_tokens:List[str],tokenized_table:TokenizedTable,num_rows:int,num_columns:int,max_length:int,truncation_strategy:Union[str,TapasTruncationStrategy])->Tuple[int, int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.models.tapas.tokenization_tapas.TapasTokenizer._pad_to_seq_length(self,inputs)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._question_encoding_cost(self,question_tokens)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._serialize(self,question_tokens,table,num_columns,num_rows,num_tokens)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._serialize_text(self,question_tokens)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._tokenize(self,text)
transformers.models.tapas.tokenization_tapas.TapasTokenizer._tokenize_table(self,table=None)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.batch_encode_plus(self,table:'pd.DataFrame',queries:Optional[Union[List[TextInput],List[PreTokenizedInput],List[EncodedInput]]]=None,answer_coordinates:Optional[List[List[Tuple]]]=None,answer_text:Optional[List[List[TextInput]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.tapas.tokenization_tapas.TapasTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.convert_logits_to_predictions(self,data,logits,logits_agg=None,cell_classification_threshold=0.5)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.create_attention_mask_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.create_column_token_type_ids_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.create_row_token_type_ids_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.create_segment_token_type_ids_from_sequences(self,query_ids:List[int],table_values:List[TableValue])->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.do_lower_case(self)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.encode(self,table:'pd.DataFrame',query:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,**kwargs)->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.encode_plus(self,table:'pd.DataFrame',query:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,answer_coordinates:Optional[List[Tuple]]=None,answer_text:Optional[List[TextInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.tapas.tokenization_tapas.TapasTokenizer.get_answer_ids(self,column_ids,row_ids,tokenized_table,answer_texts_question,answer_coordinates_question)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.get_vocab(self)
transformers.models.tapas.tokenization_tapas.TapasTokenizer.prepare_for_model(self,raw_table:'pd.DataFrame',raw_query:Union[TextInput,PreTokenizedInput,EncodedInput],tokenized_table:Optional[TokenizedTable]=None,query_tokens:Optional[TokenizedTable]=None,answer_coordinates:Optional[List[Tuple]]=None,answer_text:Optional[List[TextInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TapasTruncationStrategy]=False,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=True,return_attention_mask:Optional[bool]=True,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.models.tapas.tokenization_tapas.TapasTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.tapas.tokenization_tapas.TapasTokenizer.vocab_size(self)
transformers.models.tapas.tokenization_tapas.TapasTruncationStrategy(ExplicitEnum)
transformers.models.tapas.tokenization_tapas.TokenCoordinates
transformers.models.tapas.tokenization_tapas.TokenizedTable
transformers.models.tapas.tokenization_tapas.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.models.tapas.tokenization_tapas.WordpieceTokenizer.__init__(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.models.tapas.tokenization_tapas.WordpieceTokenizer.tokenize(self,text)
transformers.models.tapas.tokenization_tapas._consolidate_numeric_values(row_index_to_values,min_consolidation_fraction,debug_info)
transformers.models.tapas.tokenization_tapas._get_all_types(numeric_values)
transformers.models.tapas.tokenization_tapas._get_column_values(table,col_index)
transformers.models.tapas.tokenization_tapas._get_numeric_value_from_date(date,mask)
transformers.models.tapas.tokenization_tapas._get_numeric_value_from_float(value)
transformers.models.tapas.tokenization_tapas._get_numeric_values(text)
transformers.models.tapas.tokenization_tapas._get_span_length_key(span)
transformers.models.tapas.tokenization_tapas._get_value_as_primitive_value(numeric_value)
transformers.models.tapas.tokenization_tapas._get_value_type(numeric_value)
transformers.models.tapas.tokenization_tapas._is_inner_wordpiece(token:Text)
transformers.models.tapas.tokenization_tapas._parse_date(text)
transformers.models.tapas.tokenization_tapas._parse_number(text)
transformers.models.tapas.tokenization_tapas._process_date_pattern(dp)
transformers.models.tapas.tokenization_tapas._process_date_patterns()
transformers.models.tapas.tokenization_tapas.add_numeric_table_values(table,min_consolidation_fraction=0.7,debug_info=None)
transformers.models.tapas.tokenization_tapas.add_numeric_values_to_question(question)
transformers.models.tapas.tokenization_tapas.filter_invalid_unicode(text)
transformers.models.tapas.tokenization_tapas.filter_invalid_unicode_from_table(table)
transformers.models.tapas.tokenization_tapas.format_text(text)
transformers.models.tapas.tokenization_tapas.get_all_spans(text,max_ngram_length)
transformers.models.tapas.tokenization_tapas.get_numeric_relation(value,other_value,sort_key_fn)
transformers.models.tapas.tokenization_tapas.get_numeric_sort_key_fn(numeric_values)
transformers.models.tapas.tokenization_tapas.load_vocab(vocab_file)
transformers.models.tapas.tokenization_tapas.normalize_for_match(text)
transformers.models.tapas.tokenization_tapas.parse_text(text)
transformers.models.tapas.tokenization_tapas.whitespace_tokenize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/tapas/modeling_tapas.py----------------------------------------
A:transformers.models.tapas.modeling_tapas.logger->utils.logging.get_logger(__name__)
A:transformers.models.tapas.modeling_tapas.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.tapas.modeling_tapas.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.tapas.modeling_tapas.array->numpy.array(array)
A:transformers.models.tapas.modeling_tapas.name->name.split('/').split('/')
A:transformers.models.tapas.modeling_tapas.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.tapas.modeling_tapas.pointer->getattr(pointer, 'weight')
A:transformers.models.tapas.modeling_tapas.num->int(scope_names[1])
A:transformers.models.tapas.modeling_tapas.pointer.data->torch.from_numpy(array)
A:transformers.models.tapas.modeling_tapas.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.tapas.modeling_tapas.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.tapas.modeling_tapas.self.number_of_token_type_embeddings->len(config.type_vocab_sizes)
A:transformers.models.tapas.modeling_tapas.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.tapas.modeling_tapas.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.tapas.modeling_tapas.input_shape->input_ids.size()
A:transformers.models.tapas.modeling_tapas.position_ids->torch.min(torch.as_tensor(self.config.max_position_embeddings - 1, device=device), position - first_position)
A:transformers.models.tapas.modeling_tapas.col_index->IndexMap(indices=torch.min(column_ids, torch.as_tensor(self.config.max_num_columns - 1, device=column_ids.device)), num_segments=self.config.max_num_columns, batch_dims=1)
A:transformers.models.tapas.modeling_tapas.row_index->IndexMap(indices=torch.min(row_ids, torch.as_tensor(self.config.max_num_rows - 1, device=row_ids.device)), num_segments=self.config.max_num_rows, batch_dims=1)
A:transformers.models.tapas.modeling_tapas.full_index->ProductIndexMap(col_index, row_index)
A:transformers.models.tapas.modeling_tapas.first_position->gather(first_position_per_segment, full_index)
A:transformers.models.tapas.modeling_tapas.position->torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0)
A:transformers.models.tapas.modeling_tapas.token_type_ids->torch.zeros((*input_shape, len(self.config.type_vocab_sizes)), dtype=torch.long, device=device)
A:transformers.models.tapas.modeling_tapas.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.tapas.modeling_tapas.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.tapas.modeling_tapas.embeddings->self.dropout(embeddings)
A:transformers.models.tapas.modeling_tapas.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.tapas.modeling_tapas.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.tapas.modeling_tapas.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.tapas.modeling_tapas.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.tapas.modeling_tapas.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.tapas.modeling_tapas.mixed_query_layer->self.query(hidden_states)
A:transformers.models.tapas.modeling_tapas.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.tapas.modeling_tapas.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.tapas.modeling_tapas.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.tapas.modeling_tapas.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.tapas.modeling_tapas.attention_probs->self.dropout(attention_probs)
A:transformers.models.tapas.modeling_tapas.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.tapas.modeling_tapas.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.tapas.modeling_tapas.hidden_states->self.LayerNorm(hidden_states + input_tensor)
A:transformers.models.tapas.modeling_tapas.self.self->TapasSelfAttention(config)
A:transformers.models.tapas.modeling_tapas.self.output->TapasOutput(config)
A:transformers.models.tapas.modeling_tapas.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.tapas.modeling_tapas.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.tapas.modeling_tapas.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.tapas.modeling_tapas.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.tapas.modeling_tapas.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.tapas.modeling_tapas.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.tapas.modeling_tapas.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.tapas.modeling_tapas.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.tapas.modeling_tapas.self.attention->TapasAttention(config)
A:transformers.models.tapas.modeling_tapas.self.crossattention->TapasAttention(config)
A:transformers.models.tapas.modeling_tapas.self.intermediate->TapasIntermediate(config)
A:transformers.models.tapas.modeling_tapas.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.tapas.modeling_tapas.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.tapas.modeling_tapas.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.tapas.modeling_tapas.intermediate_output->self.intermediate(attention_output)
A:transformers.models.tapas.modeling_tapas.self.layer->torch.nn.ModuleList([TapasLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.tapas.modeling_tapas.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions)
A:transformers.models.tapas.modeling_tapas.self.activation->torch.nn.Tanh()
A:transformers.models.tapas.modeling_tapas.pooled_output->self.dropout(pooled_output)
A:transformers.models.tapas.modeling_tapas.self.embeddings->TapasEmbeddings(config)
A:transformers.models.tapas.modeling_tapas.self.encoder->TapasEncoder(config)
A:transformers.models.tapas.modeling_tapas.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.tapas.modeling_tapas.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.tapas.modeling_tapas.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.tapas.modeling_tapas.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.tapas.modeling_tapas.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.tapas.modeling_tapas.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.tapas.modeling_tapas.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.tapas.modeling_tapas.self.tapas->TapasModel(config)
A:transformers.models.tapas.modeling_tapas.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size)
A:transformers.models.tapas.modeling_tapas.outputs->self.tapas(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.tapas.modeling_tapas.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.tapas.modeling_tapas.loss_fct->CrossEntropyLoss()
A:transformers.models.tapas.modeling_tapas.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.tapas.modeling_tapas.self.output_weights->torch.nn.Parameter(torch.empty(config.hidden_size))
A:transformers.models.tapas.modeling_tapas.self.column_output_weights->torch.nn.Parameter(torch.empty(config.hidden_size))
A:transformers.models.tapas.modeling_tapas.self.output_bias->torch.nn.Parameter(torch.zeros([]))
A:transformers.models.tapas.modeling_tapas.self.column_output_bias->torch.nn.Parameter(torch.zeros([]))
A:transformers.models.tapas.modeling_tapas.self.aggregation_classifier->torch.nn.Linear(config.hidden_size, config.num_aggregation_labels)
A:transformers.models.tapas.modeling_tapas.sequence_output->self.dropout(sequence_output)
A:transformers.models.tapas.modeling_tapas.cell_index->ProductIndexMap(row_index, col_index)
A:transformers.models.tapas.modeling_tapas.table_mask->torch.where(row_ids > 0, torch.ones_like(row_ids), torch.zeros_like(row_ids))
A:transformers.models.tapas.modeling_tapas.input_mask_float->torch.ones(input_shape, device=device).float().to(device)
A:transformers.models.tapas.modeling_tapas.table_mask_float->torch.where(row_ids > 0, torch.ones_like(row_ids), torch.zeros_like(row_ids)).float().to(device)
A:transformers.models.tapas.modeling_tapas.(cell_mask, _)->reduce_mean(input_mask_float, cell_index)
A:transformers.models.tapas.modeling_tapas.logits->gather(new_logits_per_cell, cell_index)
A:transformers.models.tapas.modeling_tapas.column_logits->compute_column_logits(sequence_output, self.column_output_weights, self.column_output_bias, cell_index, cell_mask, self.config.allow_empty_column_selection)
A:transformers.models.tapas.modeling_tapas.logits_aggregation->aggregation_classifier(pooled_output)
A:transformers.models.tapas.modeling_tapas.aggregate_mask->aggregate_mask.detach().detach()
A:transformers.models.tapas.modeling_tapas.(logits_per_cell, _)->reduce_mean(token_logits, cell_index)
A:transformers.models.tapas.modeling_tapas.dist_per_token->torch.distributions.Bernoulli(logits=logits)
A:transformers.models.tapas.modeling_tapas.weight->torch.where(labels == 0, torch.ones_like(labels, dtype=torch.float32), self.config.positive_label_weight * torch.ones_like(labels, dtype=torch.float32))
A:transformers.models.tapas.modeling_tapas.(selection_loss_per_example, logits)->_single_column_cell_selection_loss(logits, column_logits, labels, cell_index, col_index, cell_mask)
A:transformers.models.tapas.modeling_tapas.per_example_additional_loss->_calculate_aggregation_loss(logits_aggregation, aggregate_mask, aggregation_labels, self.config.use_answer_as_supervision, self.config.num_aggregation_labels, self.config.aggregation_loss_weight)
A:transformers.models.tapas.modeling_tapas.aggregation_labels->torch.zeros(labels.shape[0], dtype=torch.long, device=labels.device)
A:transformers.models.tapas.modeling_tapas.(answer_loss, large_answer_loss_mask)->_calculate_regression_loss(float_answer, aggregate_mask, dist_per_token, numeric_values, numeric_values_scale, table_mask_float, logits_aggregation, self.config)
A:transformers.models.tapas.modeling_tapas.labels->torch.zeros_like(logits)
A:transformers.models.tapas.modeling_tapas.(_, logits)->_single_column_cell_selection_loss(logits, column_logits, labels, cell_index, col_index, cell_mask)
A:transformers.models.tapas.modeling_tapas.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.tapas.modeling_tapas.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.tapas.modeling_tapas.self.indices->torch.as_tensor(indices)
A:transformers.models.tapas.modeling_tapas.self.num_segments->torch.as_tensor(num_segments, device=indices.device)
A:transformers.models.tapas.modeling_tapas.indices->indices.repeat(multiples.tolist()).repeat(multiples.tolist())
A:transformers.models.tapas.modeling_tapas.batch_size->torch.prod(torch.tensor(list(index.batch_shape())))
A:transformers.models.tapas.modeling_tapas.offset->offset.unsqueeze(-1).unsqueeze(-1)
A:transformers.models.tapas.modeling_tapas.batch_shape->torch.as_tensor(batch_shape, dtype=torch.long)
A:transformers.models.tapas.modeling_tapas.num_segments->torch.as_tensor(num_segments)
A:transformers.models.tapas.modeling_tapas.new_tensor->torch.cat([torch.ones_like(batch_shape, dtype=torch.long, device=num_segments.device), num_segments.unsqueeze(dim=0)], dim=0)
A:transformers.models.tapas.modeling_tapas.multiples->torch.cat([batch_shape, torch.as_tensor([1])], dim=0)
A:transformers.models.tapas.modeling_tapas.flat_index->flatten(index)
A:transformers.models.tapas.modeling_tapas.flattened_shape->torch.cat([torch.as_tensor([-1], dtype=torch.long), torch.as_tensor(vector_shape, dtype=torch.long)], dim=0)
A:transformers.models.tapas.modeling_tapas.flat_values->values.reshape(flattened_shape.tolist())
A:transformers.models.tapas.modeling_tapas.segment_means->scatter(src=flat_values, index=flat_index.indices.long(), dim=0, dim_size=int(flat_index.num_segments), reduce=segment_reduce_fn)
A:transformers.models.tapas.modeling_tapas.new_shape->torch.cat([torch.as_tensor(index.batch_shape(), dtype=torch.long), torch.as_tensor([index.num_segments], dtype=torch.long), torch.as_tensor(vector_shape, dtype=torch.long)], dim=0)
A:transformers.models.tapas.modeling_tapas.output_values->scatter(src=flat_values, index=flat_index.indices.long(), dim=0, dim_size=int(flat_index.num_segments), reduce=segment_reduce_fn).view(new_shape.tolist())
A:transformers.models.tapas.modeling_tapas.output_index->range_index_map(index.batch_shape(), index.num_segments)
A:transformers.models.tapas.modeling_tapas.(cell_logits, cell_logits_index)->reduce_mean(token_logits, cell_index)
A:transformers.models.tapas.modeling_tapas.column_index->ProductIndexMap(row_index, col_index).project_inner(cell_logits_index)
A:transformers.models.tapas.modeling_tapas.(column_logits, out_index)->reduce_sum(cell_logits * cell_mask, column_index)
A:transformers.models.tapas.modeling_tapas.(cell_count, _)->reduce_sum(cell_mask, column_index)
A:transformers.models.tapas.modeling_tapas.is_padding->torch.logical_and(cell_count < 0.5, ~torch.eq(out_index.indices, 0))
A:transformers.models.tapas.modeling_tapas.(labels_per_column, _)->reduce_sum(torch.as_tensor(labels, dtype=torch.float32, device=labels.device), col_index)
A:transformers.models.tapas.modeling_tapas.column_label->torch.where(no_cell_selected.view(column_label.size()), torch.zeros_like(column_label), column_label)
A:transformers.models.tapas.modeling_tapas.no_cell_selected->torch.eq(torch.max(labels_per_column, dim=-1)[0], 0)
A:transformers.models.tapas.modeling_tapas.column_dist->torch.distributions.Categorical(logits=column_logits)
A:transformers.models.tapas.modeling_tapas.(labels_per_cell, labels_index)->reduce_max(torch.as_tensor(labels, dtype=torch.long, device=labels.device), cell_index)
A:transformers.models.tapas.modeling_tapas.column_mask->torch.as_tensor(torch.eq(column_id_for_cells, torch.unsqueeze(column_label, dim=-1)), dtype=torch.float32, device=cell_mask.device)
A:transformers.models.tapas.modeling_tapas.cell_dist->torch.distributions.Bernoulli(logits=logits_per_cell)
A:transformers.models.tapas.modeling_tapas.cell_log_prob->torch.distributions.Bernoulli(logits=logits_per_cell).log_prob(labels_per_cell.type(torch.float32))
A:transformers.models.tapas.modeling_tapas.selected_column_id->torch.as_tensor(torch.argmax(column_logits, dim=-1), dtype=torch.long, device=column_logits.device)
A:transformers.models.tapas.modeling_tapas.selected_column_mask->torch.where(torch.eq(column_id_for_cells, 0).view(selected_column_mask.size()), torch.zeros_like(selected_column_mask), selected_column_mask)
A:transformers.models.tapas.modeling_tapas.aggregate_mask_init->torch.logical_not(torch.isnan(answer)).type(torch.FloatTensor).to(answer.device)
A:transformers.models.tapas.modeling_tapas.dist_aggregation->torch.distributions.categorical.Categorical(logits=logits_aggregation)
A:transformers.models.tapas.modeling_tapas.aggregation_ops_total_mass->torch.sum(dist_aggregation.probs[:, 1:], dim=1)
A:transformers.models.tapas.modeling_tapas.target_aggregation->torch.zeros_like(aggregate_mask, dtype=torch.long)
A:transformers.models.tapas.modeling_tapas.one_hot_labels->torch.nn.functional.one_hot(target_aggregation, num_classes=num_aggregation_labels).type(torch.float32)
A:transformers.models.tapas.modeling_tapas.log_probs->torch.nn.functional.log_softmax(logits_aggregation, dim=-1)
A:transformers.models.tapas.modeling_tapas.per_example_aggregation_loss->_calculate_aggregation_loss_known(logits_aggregation, aggregate_mask, aggregation_labels, use_answer_as_supervision, num_aggregation_labels)
A:transformers.models.tapas.modeling_tapas.gumbel_dist->torch.distributions.RelaxedOneHotCategorical(config.aggregation_temperature, logits=logits_aggregation[:, 1:])
A:transformers.models.tapas.modeling_tapas.scaled_probability_per_cell->torch.distributions.RelaxedOneHotCategorical(config.aggregation_temperature, logits=logits_aggregation[:, 1:]).sample()
A:transformers.models.tapas.modeling_tapas.count_result->torch.sum(scaled_probability_per_cell, dim=1)
A:transformers.models.tapas.modeling_tapas.numeric_values_masked->torch.where(torch.isnan(numeric_values), torch.zeros_like(numeric_values), numeric_values)
A:transformers.models.tapas.modeling_tapas.sum_result->torch.sum(scaled_probability_per_cell * numeric_values_masked, dim=1)
A:transformers.models.tapas.modeling_tapas.average_result->torch.sum(numeric_values_masked * scaled_probability_per_cell * multiplier, dim=1)
A:transformers.models.tapas.modeling_tapas.aggregation_op_only_probs->torch.nn.functional.softmax(logits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1)
A:transformers.models.tapas.modeling_tapas.all_results->torch.cat([torch.unsqueeze(sum_result, dim=1), torch.unsqueeze(average_result, dim=1), torch.unsqueeze(count_result, dim=1)], dim=1)
A:transformers.models.tapas.modeling_tapas.expected_result->_calculate_expected_result(dist_per_cell, numeric_values, numeric_values_scale, input_mask_float, logits_aggregation, config)
A:transformers.models.tapas.modeling_tapas.errors->torch.abs(input - target)
A:transformers.models.tapas.modeling_tapas.answer_masked->torch.where(torch.isnan(answer), torch.zeros_like(answer), answer)
A:transformers.models.tapas.modeling_tapas.normalizer->(torch.max(torch.abs(expected_result), torch.abs(answer_masked)) + EPSILON_ZERO_DIVISION).detach()
A:transformers.models.tapas.modeling_tapas.per_example_answer_loss->huber_loss(expected_result * aggregate_mask, answer_masked * aggregate_mask, delta=config.huber_loss_delta)
A:transformers.models.tapas.modeling_tapas.large_answer_loss_mask->torch.where(per_example_answer_loss > config.answer_loss_cutoff, torch.zeros_like(per_example_answer_loss, dtype=torch.float32), torch.ones_like(per_example_answer_loss, dtype=torch.float32))
transformers.TapasForMaskedLM(self,config)
transformers.TapasForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.TapasForMaskedLM.get_output_embeddings(self)
transformers.TapasForMaskedLM.set_output_embeddings(self,word_embeddings)
transformers.TapasForQuestionAnswering(self,config:TapasConfig)
transformers.TapasForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,table_mask=None,labels=None,aggregation_labels=None,float_answer=None,numeric_values=None,numeric_values_scale=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TapasForSequenceClassification(self,config)
transformers.TapasForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TapasModel(self,config,add_pooling_layer=True)
transformers.TapasModel._prune_heads(self,heads_to_prune)
transformers.TapasModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TapasModel.get_input_embeddings(self)
transformers.TapasModel.set_input_embeddings(self,value)
transformers.TapasPreTrainedModel(PreTrainedModel)
transformers.TapasPreTrainedModel._init_weights(self,module)
transformers.models.tapas.modeling_tapas.AverageApproximationFunction(str,enum.Enum)
transformers.models.tapas.modeling_tapas.IndexMap(self,indices,num_segments,batch_dims=0)
transformers.models.tapas.modeling_tapas.IndexMap.__init__(self,indices,num_segments,batch_dims=0)
transformers.models.tapas.modeling_tapas.IndexMap.batch_shape(self)
transformers.models.tapas.modeling_tapas.ProductIndexMap(self,outer_index,inner_index)
transformers.models.tapas.modeling_tapas.ProductIndexMap.__init__(self,outer_index,inner_index)
transformers.models.tapas.modeling_tapas.ProductIndexMap.project_inner(self,index)
transformers.models.tapas.modeling_tapas.ProductIndexMap.project_outer(self,index)
transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput(ModelOutput)
transformers.models.tapas.modeling_tapas.TapasAttention(self,config)
transformers.models.tapas.modeling_tapas.TapasAttention.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.tapas.modeling_tapas.TapasAttention.prune_heads(self,heads)
transformers.models.tapas.modeling_tapas.TapasEmbeddings(self,config)
transformers.models.tapas.modeling_tapas.TapasEmbeddings.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.tapas.modeling_tapas.TapasEncoder(self,config)
transformers.models.tapas.modeling_tapas.TapasEncoder.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.tapas.modeling_tapas.TapasForMaskedLM(self,config)
transformers.models.tapas.modeling_tapas.TapasForMaskedLM.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.tapas.modeling_tapas.TapasForMaskedLM.get_output_embeddings(self)
transformers.models.tapas.modeling_tapas.TapasForMaskedLM.set_output_embeddings(self,word_embeddings)
transformers.models.tapas.modeling_tapas.TapasForQuestionAnswering(self,config:TapasConfig)
transformers.models.tapas.modeling_tapas.TapasForQuestionAnswering.__init__(self,config:TapasConfig)
transformers.models.tapas.modeling_tapas.TapasForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,table_mask=None,labels=None,aggregation_labels=None,float_answer=None,numeric_values=None,numeric_values_scale=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.tapas.modeling_tapas.TapasForSequenceClassification(self,config)
transformers.models.tapas.modeling_tapas.TapasForSequenceClassification.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.tapas.modeling_tapas.TapasIntermediate(self,config)
transformers.models.tapas.modeling_tapas.TapasIntermediate.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasIntermediate.forward(self,hidden_states)
transformers.models.tapas.modeling_tapas.TapasLayer(self,config)
transformers.models.tapas.modeling_tapas.TapasLayer.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasLayer.feed_forward_chunk(self,attention_output)
transformers.models.tapas.modeling_tapas.TapasLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.tapas.modeling_tapas.TapasModel(self,config,add_pooling_layer=True)
transformers.models.tapas.modeling_tapas.TapasModel.__init__(self,config,add_pooling_layer=True)
transformers.models.tapas.modeling_tapas.TapasModel._prune_heads(self,heads_to_prune)
transformers.models.tapas.modeling_tapas.TapasModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.tapas.modeling_tapas.TapasModel.get_input_embeddings(self)
transformers.models.tapas.modeling_tapas.TapasModel.set_input_embeddings(self,value)
transformers.models.tapas.modeling_tapas.TapasOutput(self,config)
transformers.models.tapas.modeling_tapas.TapasOutput.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasOutput.forward(self,hidden_states,input_tensor)
transformers.models.tapas.modeling_tapas.TapasPooler(self,config)
transformers.models.tapas.modeling_tapas.TapasPooler.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasPooler.forward(self,hidden_states)
transformers.models.tapas.modeling_tapas.TapasPreTrainedModel(PreTrainedModel)
transformers.models.tapas.modeling_tapas.TapasPreTrainedModel._init_weights(self,module)
transformers.models.tapas.modeling_tapas.TapasSelfAttention(self,config)
transformers.models.tapas.modeling_tapas.TapasSelfAttention.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.tapas.modeling_tapas.TapasSelfAttention.transpose_for_scores(self,x)
transformers.models.tapas.modeling_tapas.TapasSelfOutput(self,config)
transformers.models.tapas.modeling_tapas.TapasSelfOutput.__init__(self,config)
transformers.models.tapas.modeling_tapas.TapasSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.tapas.modeling_tapas._calculate_aggregate_mask(answer,pooled_output,cell_selection_preference,labels,aggregation_classifier)
transformers.models.tapas.modeling_tapas._calculate_aggregation_loss(logits_aggregation,aggregate_mask,aggregation_labels,use_answer_as_supervision,num_aggregation_labels,aggregation_loss_weight)
transformers.models.tapas.modeling_tapas._calculate_aggregation_loss_known(logits_aggregation,aggregate_mask,aggregation_labels,use_answer_as_supervision,num_aggregation_labels)
transformers.models.tapas.modeling_tapas._calculate_aggregation_loss_unknown(logits_aggregation,aggregate_mask)
transformers.models.tapas.modeling_tapas._calculate_expected_result(dist_per_cell,numeric_values,numeric_values_scale,input_mask_float,logits_aggregation,config)
transformers.models.tapas.modeling_tapas._calculate_regression_loss(answer,aggregate_mask,dist_per_cell,numeric_values,numeric_values_scale,input_mask_float,logits_aggregation,config)
transformers.models.tapas.modeling_tapas._segment_reduce(values,index,segment_reduce_fn,name)
transformers.models.tapas.modeling_tapas._single_column_cell_selection_loss(token_logits,column_logits,labels,cell_index,col_index,cell_mask)
transformers.models.tapas.modeling_tapas.compute_column_logits(sequence_output,column_output_weights,column_output_bias,cell_index,cell_mask,allow_empty_column_selection)
transformers.models.tapas.modeling_tapas.compute_token_logits(sequence_output,temperature,output_weights,output_bias)
transformers.models.tapas.modeling_tapas.flatten(index,name='segmented_flatten')
transformers.models.tapas.modeling_tapas.gather(values,index,name='segmented_gather')
transformers.models.tapas.modeling_tapas.huber_loss(input,target,delta:float=1.0)
transformers.models.tapas.modeling_tapas.load_tf_weights_in_tapas(model,config,tf_checkpoint_path)
transformers.models.tapas.modeling_tapas.range_index_map(batch_shape,num_segments,name='range_index_map')
transformers.models.tapas.modeling_tapas.reduce_max(values,index,name='segmented_reduce_max')
transformers.models.tapas.modeling_tapas.reduce_mean(values,index,name='segmented_reduce_mean')
transformers.models.tapas.modeling_tapas.reduce_min(values,index,name='segmented_reduce_min')
transformers.models.tapas.modeling_tapas.reduce_sum(values,index,name='segmented_reduce_sum')


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.tapas.convert_tapas_original_tf_checkpoint_to_pytorch.config->transformers.TapasConfig.from_json_file(tapas_config_file)
A:transformers.models.tapas.convert_tapas_original_tf_checkpoint_to_pytorch.model->TapasModel(config=config)
A:transformers.models.tapas.convert_tapas_original_tf_checkpoint_to_pytorch.tokenizer->TapasTokenizer(vocab_file=dir_name + '\\vocab.txt', model_max_length=512)
A:transformers.models.tapas.convert_tapas_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.tapas.convert_tapas_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.tapas.convert_tapas_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(task,reset_position_index_per_cell,tf_checkpoint_path,tapas_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/tapas/__init__.py----------------------------------------
A:transformers.models.tapas.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta_v2/configuration_deberta_v2.py----------------------------------------
A:transformers.models.deberta_v2.configuration_deberta_v2.logger->utils.logging.get_logger(__name__)
A:transformers.models.deberta_v2.configuration_deberta_v2.self.pooler_hidden_size->kwargs.get('pooler_hidden_size', hidden_size)
transformers.DebertaV2Config(self,vocab_size=128100,hidden_size=1536,num_hidden_layers=24,num_attention_heads=24,intermediate_size=6144,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=0,initializer_range=0.02,layer_norm_eps=1e-07,relative_attention=False,max_relative_positions=-1,pad_token_id=0,position_biased_input=True,pos_att_type=None,pooler_dropout=0,pooler_hidden_act='gelu',**kwargs)
transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config(self,vocab_size=128100,hidden_size=1536,num_hidden_layers=24,num_attention_heads=24,intermediate_size=6144,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=0,initializer_range=0.02,layer_norm_eps=1e-07,relative_attention=False,max_relative_positions=-1,pad_token_id=0,position_biased_input=True,pos_att_type=None,pooler_dropout=0,pooler_hidden_act='gelu',**kwargs)
transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config.__init__(self,vocab_size=128100,hidden_size=1536,num_hidden_layers=24,num_attention_heads=24,intermediate_size=6144,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=0,initializer_range=0.02,layer_norm_eps=1e-07,relative_attention=False,max_relative_positions=-1,pad_token_id=0,position_biased_input=True,pos_att_type=None,pooler_dropout=0,pooler_hidden_act='gelu',**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta_v2/tokenization_deberta_v2.py----------------------------------------
A:transformers.models.deberta_v2.tokenization_deberta_v2.self._tokenizer->SPMTokenizer(vocab_file, split_by_punct=split_by_punct, sp_model_kwargs=self.sp_model_kwargs)
A:transformers.models.deberta_v2.tokenization_deberta_v2.vocab->self.vocab.copy()
A:transformers.models.deberta_v2.tokenization_deberta_v2.text->unicodedata.normalize('NFD', text)
A:transformers.models.deberta_v2.tokenization_deberta_v2.add_prefix_space->kwargs.pop('add_prefix_space', False)
A:transformers.models.deberta_v2.tokenization_deberta_v2.spm->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.deberta_v2.tokenization_deberta_v2.bpe_vocab_size->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs).GetPieceSize()
A:transformers.models.deberta_v2.tokenization_deberta_v2.state->self.__dict__.copy()
A:transformers.models.deberta_v2.tokenization_deberta_v2.self.spm->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.deberta_v2.tokenization_deberta_v2.pieces->self._encode_as_pieces(text)
A:transformers.models.deberta_v2.tokenization_deberta_v2.words->self._run_split_on_punc(text)
A:transformers.models.deberta_v2.tokenization_deberta_v2.word_start->b'\xe2\x96\x81'.decode('utf-8')
A:transformers.models.deberta_v2.tokenization_deberta_v2.w->p.replace(word_start, '')
A:transformers.models.deberta_v2.tokenization_deberta_v2.s->unicodedata.normalize('NFD', text).index(w, offset)
A:transformers.models.deberta_v2.tokenization_deberta_v2.pn->pieces[k].replace(word_start, '')
A:transformers.models.deberta_v2.tokenization_deberta_v2.cat->unicodedata.category(char)
A:transformers.models.deberta_v2.tokenization_deberta_v2.chars->list(text)
A:transformers.models.deberta_v2.tokenization_deberta_v2.full_path->os.path.join(path, filename)
A:transformers.models.deberta_v2.tokenization_deberta_v2.cp->ord(char)
transformers.DebertaV2Tokenizer(self,vocab_file,do_lower_case=False,split_by_punct=False,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.DebertaV2Tokenizer._convert_id_to_token(self,index)
transformers.DebertaV2Tokenizer._convert_token_to_id(self,token)
transformers.DebertaV2Tokenizer._tokenize(self,text:str)->List[str]
transformers.DebertaV2Tokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.DebertaV2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.DebertaV2Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.DebertaV2Tokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.DebertaV2Tokenizer.get_vocab(self)
transformers.DebertaV2Tokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.DebertaV2Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.DebertaV2Tokenizer.vocab(self)
transformers.DebertaV2Tokenizer.vocab_size(self)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer(self,vocab_file,do_lower_case=False,split_by_punct=False,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.__init__(self,vocab_file,do_lower_case=False,split_by_punct=False,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer._convert_id_to_token(self,index)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer._convert_token_to_id(self,token)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer._tokenize(self,text:str)->List[str]
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1=None)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1=None,already_has_special_tokens=False)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.get_vocab(self)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.vocab(self)
transformers.models.deberta_v2.tokenization_deberta_v2.DebertaV2Tokenizer.vocab_size(self)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer(self,vocab_file,split_by_punct=False,sp_model_kwargs:Optional[Dict[str,Any]]=None)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.__getstate__(self)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.__init__(self,vocab_file,split_by_punct=False,sp_model_kwargs:Optional[Dict[str,Any]]=None)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.__setstate__(self,d)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer._encode_as_pieces(self,text)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer._run_split_on_punc(self,text)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer._run_strip_accents(self,text)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.add_special_token(self,token)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.bos(self)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.convert_ids_to_tokens(self,ids)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.decode(self,tokens,start=-1,end=-1,raw_text=None)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.eos(self)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.id(self,sym)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.mask(self)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.pad(self)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.part_of_whole_word(self,token,is_bos=False)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.save_pretrained(self,path:str,filename_prefix:str=None)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.split_to_words(self,text)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.sym(self,id)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.tokenize(self,text)
transformers.models.deberta_v2.tokenization_deberta_v2.SPMTokenizer.unk(self)
transformers.models.deberta_v2.tokenization_deberta_v2._is_control(char)
transformers.models.deberta_v2.tokenization_deberta_v2._is_punctuation(char)
transformers.models.deberta_v2.tokenization_deberta_v2._is_whitespace(char)
transformers.models.deberta_v2.tokenization_deberta_v2.convert_to_unicode(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta_v2/__init__.py----------------------------------------
A:transformers.models.deberta_v2.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deberta_v2/modeling_deberta_v2.py----------------------------------------
A:transformers.models.deberta_v2.modeling_deberta_v2.logger->utils.logging.get_logger(__name__)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.deberta_v2.modeling_deberta_v2.context_token->self.dropout(context_token)
A:transformers.models.deberta_v2.modeling_deberta_v2.pooled_output->self.dropout(pooled_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.output->self.LayerNorm(layer_norm_input).to(layer_norm_input)
A:transformers.models.deberta_v2.modeling_deberta_v2.inputGrad->_softmax_backward_data(grad_output, output, self.dim, output)
A:transformers.models.deberta_v2.modeling_deberta_v2.mask->mask.to(embeddings.dtype).to(embeddings.dtype)
A:transformers.models.deberta_v2.modeling_deberta_v2.(mask, dropout)->get_mask(input, local_ctx)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.deberta_v2.modeling_deberta_v2.hidden_states->self.decoder(hidden_states)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.self->DisentangledSelfAttention(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.output->DebertaV2Output(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self_output->self.self(hidden_states, attention_mask, return_att, query_states=query_states, relative_pos=relative_pos, rel_embeddings=rel_embeddings)
A:transformers.models.deberta_v2.modeling_deberta_v2.attention_output->self.attention(hidden_states, attention_mask, return_att=return_att, query_states=query_states, relative_pos=relative_pos, rel_embeddings=rel_embeddings)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.attention->DebertaV2Attention(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.intermediate->DebertaV2Intermediate(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.intermediate_output->self.intermediate(attention_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.kernel_size->getattr(config, 'conv_kernel_size', 3)
A:transformers.models.deberta_v2.modeling_deberta_v2.groups->getattr(config, 'conv_groups', 1)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.conv_act->getattr(config, 'conv_act', 'tanh')
A:transformers.models.deberta_v2.modeling_deberta_v2.self.conv->torch.nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size, padding=(kernel_size - 1) // 2, groups=groups)
A:transformers.models.deberta_v2.modeling_deberta_v2.out->ACT2FN[self.conv_act](self.dropout(out))
A:transformers.models.deberta_v2.modeling_deberta_v2.rmask->(1 - input_mask).bool()
A:transformers.models.deberta_v2.modeling_deberta_v2.input_mask->(attention_mask.sum(-2) > 0).byte()
A:transformers.models.deberta_v2.modeling_deberta_v2.self.layer->torch.nn.ModuleList([DebertaV2Layer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.deberta_v2.modeling_deberta_v2.self.relative_attention->getattr(config, 'relative_attention', False)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.max_relative_positions->getattr(config, 'max_relative_positions', -1)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.position_buckets->getattr(config, 'position_buckets', -1)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.rel_embeddings->torch.nn.Embedding(pos_ebd_size, config.hidden_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.rel_embeddings->self.encoder.get_rel_embedding()
A:transformers.models.deberta_v2.modeling_deberta_v2.extended_attention_mask->self.encoder.get_attention_mask(attention_mask).unsqueeze(1).unsqueeze(2)
A:transformers.models.deberta_v2.modeling_deberta_v2.attention_mask->self.encoder.get_attention_mask(attention_mask)
A:transformers.models.deberta_v2.modeling_deberta_v2.relative_pos->relative_pos.long().to(query_layer.device).long().to(query_layer.device)
A:transformers.models.deberta_v2.modeling_deberta_v2.output_states->self.conv(hidden_states, output_states, input_mask)
A:transformers.models.deberta_v2.modeling_deberta_v2.sign->numpy.sign(relative_pos)
A:transformers.models.deberta_v2.modeling_deberta_v2.abs_pos->numpy.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, np.abs(relative_pos))
A:transformers.models.deberta_v2.modeling_deberta_v2.bucket_pos->numpy.where(abs_pos <= mid, relative_pos, log_pos * sign).astype(np.int)
A:transformers.models.deberta_v2.modeling_deberta_v2.q_ids->numpy.arange(0, query_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.k_ids->numpy.arange(0, key_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.rel_pos_ids->rel_pos_ids.unsqueeze(0).unsqueeze(0)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.attention_head_size->getattr(config, 'attention_head_size', _attention_head_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.query_proj->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=True)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.key_proj->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=True)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.value_proj->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=True)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.share_att_key->getattr(config, 'share_att_key', False)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.pos_dropout->StableDropout(config.hidden_dropout_prob)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.pos_key_proj->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=True)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.pos_query_proj->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.deberta_v2.modeling_deberta_v2.query_layer->self.transpose_for_scores(self.query_proj(query_states), self.num_attention_heads)
A:transformers.models.deberta_v2.modeling_deberta_v2.key_layer->self.transpose_for_scores(self.key_proj(hidden_states), self.num_attention_heads)
A:transformers.models.deberta_v2.modeling_deberta_v2.value_layer->self.transpose_for_scores(self.value_proj(hidden_states), self.num_attention_heads)
A:transformers.models.deberta_v2.modeling_deberta_v2.scale->math.sqrt(pos_query_layer.size(-1) * scale_factor)
A:transformers.models.deberta_v2.modeling_deberta_v2.rel_att->self.disentangled_attention_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
A:transformers.models.deberta_v2.modeling_deberta_v2.attention_scores->attention_scores.view(-1, self.num_attention_heads, attention_scores.size(-2), attention_scores.size(-1)).view(-1, self.num_attention_heads, attention_scores.size(-2), attention_scores.size(-1))
A:transformers.models.deberta_v2.modeling_deberta_v2.attention_probs->self.dropout(attention_probs)
A:transformers.models.deberta_v2.modeling_deberta_v2.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.deberta_v2.modeling_deberta_v2.q->self.transpose_for_scores(self.query_proj(query_states), self.num_attention_heads).size(-2)
A:transformers.models.deberta_v2.modeling_deberta_v2.pos_query_layer->self.transpose_for_scores(self.pos_query_proj(rel_embeddings), self.num_attention_heads).repeat(query_layer.size(0) // self.num_attention_heads, 1, 1)
A:transformers.models.deberta_v2.modeling_deberta_v2.pos_key_layer->self.transpose_for_scores(self.pos_key_proj(rel_embeddings), self.num_attention_heads).repeat(query_layer.size(0) // self.num_attention_heads, 1, 1)
A:transformers.models.deberta_v2.modeling_deberta_v2.c2p_att->torch.gather(c2p_att, dim=-1, index=c2p_pos.squeeze(0).expand([query_layer.size(0), query_layer.size(1), relative_pos.size(-1)]))
A:transformers.models.deberta_v2.modeling_deberta_v2.c2p_pos->torch.clamp(relative_pos + att_span, 0, att_span * 2 - 1)
A:transformers.models.deberta_v2.modeling_deberta_v2.r_pos->r_pos.unsqueeze(0).unsqueeze(0)
A:transformers.models.deberta_v2.modeling_deberta_v2.p2c_pos->torch.clamp(-r_pos + att_span, 0, att_span * 2 - 1)
A:transformers.models.deberta_v2.modeling_deberta_v2.pos_index->relative_pos[:, :, :, 0].unsqueeze(-1)
A:transformers.models.deberta_v2.modeling_deberta_v2.p2c_att->torch.gather(p2c_att, dim=-2, index=pos_index.expand(p2c_att.size()[:2] + (pos_index.size(-2), key_layer.size(-2))))
A:transformers.models.deberta_v2.modeling_deberta_v2.p2p_att->torch.gather(p2p_att, dim=-1, index=c2p_pos.expand([query_layer.size(0), query_layer.size(1), query_layer.size(2), relative_pos.size(-1)]))
A:transformers.models.deberta_v2.modeling_deberta_v2.pad_token_id->getattr(config, 'pad_token_id', 0)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.embedding_size->getattr(config, 'embedding_size', config.hidden_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.word_embeddings->torch.nn.Embedding(config.vocab_size, self.embedding_size, padding_idx=pad_token_id)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.position_biased_input->getattr(config, 'position_biased_input', True)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, self.embedding_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, self.embedding_size)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.embed_proj->torch.nn.Linear(self.embedding_size, config.hidden_size, bias=False)
A:transformers.models.deberta_v2.modeling_deberta_v2.input_shape->input_ids.size()
A:transformers.models.deberta_v2.modeling_deberta_v2.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.deberta_v2.modeling_deberta_v2.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.deberta_v2.modeling_deberta_v2.position_embeddings->torch.zeros_like(inputs_embeds)
A:transformers.models.deberta_v2.modeling_deberta_v2.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.deberta_v2.modeling_deberta_v2.embeddings->self.dropout(embeddings)
A:transformers.models.deberta_v2.modeling_deberta_v2.self_state->self.state_dict()
A:transformers.models.deberta_v2.modeling_deberta_v2.self.embeddings->DebertaV2Embeddings(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.encoder->DebertaV2Encoder(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.embedding_output->self.embeddings(input_ids=input_ids, token_type_ids=token_type_ids, position_ids=position_ids, mask=attention_mask, inputs_embeds=inputs_embeds)
A:transformers.models.deberta_v2.modeling_deberta_v2.encoder_outputs->self.encoder(embedding_output, attention_mask, output_hidden_states=True, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.models.deberta_v2.modeling_deberta_v2.rel_pos->self.encoder.get_rel_pos(embedding_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.query_states->layer(hidden_states, attention_mask, return_att=False, query_states=query_states, relative_pos=rel_pos, rel_embeddings=rel_embeddings)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.deberta->DebertaV2Model(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.cls->DebertaV2OnlyMLMHead(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.outputs->self.deberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.deberta_v2.modeling_deberta_v2.prediction_scores->self.predictions(sequence_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.deberta_v2.modeling_deberta_v2.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.deberta_v2.modeling_deberta_v2.self.transform->DebertaV2PredictionHeadTransform(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.deberta_v2.modeling_deberta_v2.self.predictions->DebertaV2LMPredictionHead(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.num_labels->getattr(config, 'num_labels', 2)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.pooler->ContextPooler(config)
A:transformers.models.deberta_v2.modeling_deberta_v2.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.deberta_v2.modeling_deberta_v2.drop_out->getattr(config, 'cls_dropout', None)
A:transformers.models.deberta_v2.modeling_deberta_v2.logits->self.qa_outputs(sequence_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.loss_fn->torch.nn.MSELoss()
A:transformers.models.deberta_v2.modeling_deberta_v2.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.deberta_v2.modeling_deberta_v2.label_index->(labels >= 0).nonzero()
A:transformers.models.deberta_v2.modeling_deberta_v2.labels->torch.gather(labels, 0, label_index.view(-1))
A:transformers.models.deberta_v2.modeling_deberta_v2.labeled_logits->torch.gather(logits, 0, label_index.expand(label_index.size(0), logits.size(1)))
A:transformers.models.deberta_v2.modeling_deberta_v2.log_softmax->torch.nn.LogSoftmax(-1)
A:transformers.models.deberta_v2.modeling_deberta_v2.sequence_output->self.dropout(sequence_output)
A:transformers.models.deberta_v2.modeling_deberta_v2.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.deberta_v2.modeling_deberta_v2.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.deberta_v2.modeling_deberta_v2.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.deberta_v2.modeling_deberta_v2.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.deberta_v2.modeling_deberta_v2.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.deberta_v2.modeling_deberta_v2.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.deberta_v2.modeling_deberta_v2.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.deberta_v2.modeling_deberta_v2.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.deberta_v2.modeling_deberta_v2.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.deberta_v2.modeling_deberta_v2.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.deberta_v2.modeling_deberta_v2.end_loss->loss_fct(end_logits, end_positions)
transformers.DebertaV2ForMaskedLM(self,config)
transformers.DebertaV2ForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaV2ForMaskedLM.get_output_embeddings(self)
transformers.DebertaV2ForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.DebertaV2ForQuestionAnswering(self,config)
transformers.DebertaV2ForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaV2ForSequenceClassification(self,config)
transformers.DebertaV2ForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaV2ForSequenceClassification.get_input_embeddings(self)
transformers.DebertaV2ForSequenceClassification.set_input_embeddings(self,new_embeddings)
transformers.DebertaV2ForTokenClassification(self,config)
transformers.DebertaV2ForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaV2Model(self,config)
transformers.DebertaV2Model._prune_heads(self,heads_to_prune)
transformers.DebertaV2Model.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DebertaV2Model.get_input_embeddings(self)
transformers.DebertaV2Model.set_input_embeddings(self,new_embeddings)
transformers.DebertaV2PreTrainedModel(self,config)
transformers.DebertaV2PreTrainedModel._init_weights(self,module)
transformers.DebertaV2PreTrainedModel._pre_load_hook(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
transformers.models.deberta_v2.modeling_deberta_v2.ContextPooler(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.ContextPooler.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.ContextPooler.forward(self,hidden_states)
transformers.models.deberta_v2.modeling_deberta_v2.ContextPooler.output_dim(self)
transformers.models.deberta_v2.modeling_deberta_v2.ConvLayer(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.ConvLayer.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.ConvLayer.forward(self,hidden_states,residual_states,input_mask)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Attention(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Attention.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Attention.forward(self,hidden_states,attention_mask,return_att=False,query_states=None,relative_pos=None,rel_embeddings=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Embeddings(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Embeddings.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Embeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,mask=None,inputs_embeds=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder.forward(self,hidden_states,attention_mask,output_hidden_states=True,output_attentions=False,query_states=None,relative_pos=None,return_dict=True)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder.get_attention_mask(self,attention_mask)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder.get_rel_embedding(self)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Encoder.get_rel_pos(self,hidden_states,query_states=None,relative_pos=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForMaskedLM(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForMaskedLM.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForMaskedLM.get_output_embeddings(self)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForQuestionAnswering(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForQuestionAnswering.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification.get_input_embeddings(self)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification.set_input_embeddings(self,new_embeddings)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForTokenClassification(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForTokenClassification.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Intermediate(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Intermediate.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Intermediate.forward(self,hidden_states)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2LMPredictionHead(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2LMPredictionHead.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2LMPredictionHead.forward(self,hidden_states)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Layer(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Layer.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Layer.forward(self,hidden_states,attention_mask,return_att=False,query_states=None,relative_pos=None,rel_embeddings=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Model(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Model.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Model._prune_heads(self,heads_to_prune)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Model.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Model.get_input_embeddings(self)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Model.set_input_embeddings(self,new_embeddings)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2OnlyMLMHead(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2OnlyMLMHead.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2OnlyMLMHead.forward(self,sequence_output)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Output(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Output.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2Output.forward(self,hidden_states,input_tensor)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PreTrainedModel(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PreTrainedModel.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PreTrainedModel._init_weights(self,module)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PreTrainedModel._pre_load_hook(self,state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PredictionHeadTransform(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PredictionHeadTransform.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2PredictionHeadTransform.forward(self,hidden_states)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2SelfOutput(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2SelfOutput.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2SelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.deberta_v2.modeling_deberta_v2.DisentangledSelfAttention(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DisentangledSelfAttention.__init__(self,config)
transformers.models.deberta_v2.modeling_deberta_v2.DisentangledSelfAttention.disentangled_attention_bias(self,query_layer,key_layer,relative_pos,rel_embeddings,scale_factor)
transformers.models.deberta_v2.modeling_deberta_v2.DisentangledSelfAttention.forward(self,hidden_states,attention_mask,return_att=False,query_states=None,relative_pos=None,rel_embeddings=None)
transformers.models.deberta_v2.modeling_deberta_v2.DisentangledSelfAttention.transpose_for_scores(self,x,attention_heads)
transformers.models.deberta_v2.modeling_deberta_v2.DropoutContext(self)
transformers.models.deberta_v2.modeling_deberta_v2.DropoutContext.__init__(self)
transformers.models.deberta_v2.modeling_deberta_v2.StableDropout(self,drop_prob)
transformers.models.deberta_v2.modeling_deberta_v2.StableDropout.__init__(self,drop_prob)
transformers.models.deberta_v2.modeling_deberta_v2.StableDropout.clear_context(self)
transformers.models.deberta_v2.modeling_deberta_v2.StableDropout.forward(self,x)
transformers.models.deberta_v2.modeling_deberta_v2.StableDropout.get_context(self)
transformers.models.deberta_v2.modeling_deberta_v2.StableDropout.init_context(self,reuse_mask=True,scale=1)
transformers.models.deberta_v2.modeling_deberta_v2.XDropout(torch.autograd.Function)
transformers.models.deberta_v2.modeling_deberta_v2.XDropout.backward(ctx,grad_output)
transformers.models.deberta_v2.modeling_deberta_v2.XDropout.forward(ctx,input,local_ctx)
transformers.models.deberta_v2.modeling_deberta_v2.XSoftmax(torch.autograd.Function)
transformers.models.deberta_v2.modeling_deberta_v2.XSoftmax.backward(self,grad_output)
transformers.models.deberta_v2.modeling_deberta_v2.XSoftmax.forward(self,input,mask,dim)
transformers.models.deberta_v2.modeling_deberta_v2.build_relative_position(query_size,key_size,bucket_size=-1,max_position=-1)
transformers.models.deberta_v2.modeling_deberta_v2.c2p_dynamic_expand(c2p_pos,query_layer,relative_pos)
transformers.models.deberta_v2.modeling_deberta_v2.get_mask(input,local_context)
transformers.models.deberta_v2.modeling_deberta_v2.make_log_bucket_position(relative_pos,bucket_size,max_position)
transformers.models.deberta_v2.modeling_deberta_v2.p2c_dynamic_expand(c2p_pos,query_layer,key_layer)
transformers.models.deberta_v2.modeling_deberta_v2.pos_dynamic_expand(pos_index,p2c_att,key_layer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/tokenization_utils.py----------------------------------------
A:transformers.models.roformer.tokenization_utils.self.normalizers->tokenizers.normalizers.BertNormalizer(clean_text=False, handle_chinese_chars=True, strip_accents=False, lowercase=False)
A:transformers.models.roformer.tokenization_utils.token_list->self.normalizers.normalize_str(token).split()
transformers.models.roformer.tokenization_utils.JiebaPreTokenizer(self,vocab)
transformers.models.roformer.tokenization_utils.JiebaPreTokenizer.__init__(self,vocab)
transformers.models.roformer.tokenization_utils.JiebaPreTokenizer.jieba_split(self,i:int,normalized_string:NormalizedString)->List[NormalizedString]
transformers.models.roformer.tokenization_utils.JiebaPreTokenizer.pre_tokenize(self,pretok:PreTokenizedString)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/configuration_roformer.py----------------------------------------
A:transformers.models.roformer.configuration_roformer.logger->utils.logging.get_logger(__name__)
transformers.RoFormerConfig(self,vocab_size=50000,embedding_size=768,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=1536,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,rotary_value=False,use_cache=True,**kwargs)
transformers.models.roformer.configuration_roformer.RoFormerConfig(self,vocab_size=50000,embedding_size=768,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=1536,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,rotary_value=False,use_cache=True,**kwargs)
transformers.models.roformer.configuration_roformer.RoFormerConfig.__init__(self,vocab_size=50000,embedding_size=768,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=1536,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,rotary_value=False,use_cache=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/tokenization_roformer_fast.py----------------------------------------
A:transformers.models.roformer.tokenization_roformer_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.roformer.tokenization_roformer_fast.pre_tok_state->json.loads(self.backend_tokenizer.normalizer.__getstate__())
A:transformers.models.roformer.tokenization_roformer_fast.pre_tok_class->getattr(normalizers, pre_tok_state.pop('type'))
A:transformers.models.roformer.tokenization_roformer_fast.self.backend_tokenizer.normalizer->pre_tok_class(**pre_tok_state)
A:transformers.models.roformer.tokenization_roformer_fast.state->self.__dict__.copy()
A:transformers.models.roformer.tokenization_roformer_fast.state['_tokenizer'].pre_tokenizer->BertPreTokenizer()
A:transformers.models.roformer.tokenization_roformer_fast.vocab->self.__dict__['_tokenizer'].get_vocab()
A:transformers.models.roformer.tokenization_roformer_fast.self.__dict__['_tokenizer'].pre_tokenizer->tokenizers.pre_tokenizers.PreTokenizer.custom(JiebaPreTokenizer(vocab))
A:transformers.models.roformer.tokenization_roformer_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
A:transformers.models.roformer.tokenization_roformer_fast.self.backend_tokenizer.pre_tokenizer->BertPreTokenizer()
transformers.RoFormerTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.RoFormerTokenizerFast.__getstate__(self)
transformers.RoFormerTokenizerFast.__setstate__(self,d)
transformers.RoFormerTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.RoFormerTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RoFormerTokenizerFast.save_pretrained(self,save_directory,legacy_format=None,filename_prefix=None,push_to_hub=False,**kwargs)
transformers.RoFormerTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.__getstate__(self)
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.__setstate__(self,d)
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.save_pretrained(self,save_directory,legacy_format=None,filename_prefix=None,push_to_hub=False,**kwargs)
transformers.models.roformer.tokenization_roformer_fast.RoFormerTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/tokenization_roformer.py----------------------------------------
A:transformers.models.roformer.tokenization_roformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.roformer.tokenization_roformer.self.vocab->load_vocab(vocab_file)
A:transformers.models.roformer.tokenization_roformer.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.models.roformer.tokenization_roformer.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars, strip_accents=strip_accents)
A:transformers.models.roformer.tokenization_roformer.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.models.roformer.tokenization_roformer.state->self.__dict__.copy()
A:transformers.models.roformer.tokenization_roformer.char_list->self._tokenize(wholword, use_jieba=False)
A:transformers.models.roformer.tokenization_roformer.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.models.roformer.tokenization_roformer.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.models.roformer.tokenization_roformer.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.RoFormerTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.RoFormerTokenizer.__getstate__(self)
transformers.RoFormerTokenizer.__setstate__(self,d)
transformers.RoFormerTokenizer._convert_id_to_token(self,index)
transformers.RoFormerTokenizer._convert_token_to_id(self,token)
transformers.RoFormerTokenizer._tokenize(self,text,use_jieba=True)
transformers.RoFormerTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RoFormerTokenizer.convert_tokens_to_string(self,tokens)
transformers.RoFormerTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RoFormerTokenizer.do_lower_case(self)
transformers.RoFormerTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.RoFormerTokenizer.get_vocab(self)
transformers.RoFormerTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.RoFormerTokenizer.vocab_size(self)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.__getstate__(self)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.__setstate__(self,d)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer._convert_id_to_token(self,index)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer._convert_token_to_id(self,token)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer._tokenize(self,text,use_jieba=True)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.do_lower_case(self)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.get_vocab(self)
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.roformer.tokenization_roformer.RoFormerTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/modeling_tf_roformer.py----------------------------------------
A:transformers.models.roformer.modeling_tf_roformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.roformer.modeling_tf_roformer.weight->tensorflow.cast(weight, dtype=self.weight.dtype)
A:transformers.models.roformer.modeling_tf_roformer.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.roformer.modeling_tf_roformer.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.roformer.modeling_tf_roformer.table->tensorflow.convert_to_tensor(table)
A:transformers.models.roformer.modeling_tf_roformer.table[:, 0:dim // 2]->numpy.sin(position_enc[:, 0::2])
A:transformers.models.roformer.modeling_tf_roformer.table[:, dim // 2:]->numpy.cos(position_enc[:, 1::2])
A:transformers.models.roformer.modeling_tf_roformer.positions->tensorflow.range(past_key_values_length, seq_len + past_key_values_length, delta=1, name='range')
A:transformers.models.roformer.modeling_tf_roformer.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.roformer.modeling_tf_roformer.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.roformer.modeling_tf_roformer.self.dropout->tensorflow.keras.layers.Dropout(rate=config.hidden_dropout_prob)
A:transformers.models.roformer.modeling_tf_roformer.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.roformer.modeling_tf_roformer.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.roformer.modeling_tf_roformer.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.roformer.modeling_tf_roformer.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.roformer.modeling_tf_roformer.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.roformer.modeling_tf_roformer.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.roformer.modeling_tf_roformer.self.sqrt_att_head_size->math.sqrt(self.attention_head_size)
A:transformers.models.roformer.modeling_tf_roformer.self.query->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.roformer.modeling_tf_roformer.self.key->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.roformer.modeling_tf_roformer.self.value->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.roformer.modeling_tf_roformer.tensor->tensorflow.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.roformer.modeling_tf_roformer.mixed_query_layer->self.query(inputs=hidden_states)
A:transformers.models.roformer.modeling_tf_roformer.mixed_key_layer->self.key(inputs=hidden_states)
A:transformers.models.roformer.modeling_tf_roformer.mixed_value_layer->self.value(inputs=hidden_states)
A:transformers.models.roformer.modeling_tf_roformer.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.roformer.modeling_tf_roformer.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.roformer.modeling_tf_roformer.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.roformer.modeling_tf_roformer.(query_layer, key_layer, value_layer)->self.apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer)
A:transformers.models.roformer.modeling_tf_roformer.(query_layer, key_layer)->self.apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer)
A:transformers.models.roformer.modeling_tf_roformer.attention_scores->tensorflow.add(attention_scores, attention_mask)
A:transformers.models.roformer.modeling_tf_roformer.dk->tensorflow.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
A:transformers.models.roformer.modeling_tf_roformer.attention_probs->tensorflow.multiply(attention_probs, head_mask)
A:transformers.models.roformer.modeling_tf_roformer.attention_output->self.dense_output(hidden_states=self_outputs[0], input_tensor=input_tensor, training=training)
A:transformers.models.roformer.modeling_tf_roformer.(sin, cos)->tensorflow.split(sinusoidal_pos, num_or_size_splits=2, axis=-1)
A:transformers.models.roformer.modeling_tf_roformer.sin_pos->tensorflow.repeat(sin, 2, axis=-1)
A:transformers.models.roformer.modeling_tf_roformer.cos_pos->tensorflow.repeat(cos, 2, axis=-1)
A:transformers.models.roformer.modeling_tf_roformer.rotate_half_query_layer->tensorflow.reshape(rotate_half_query_layer, shape_list(query_layer))
A:transformers.models.roformer.modeling_tf_roformer.rotate_half_key_layer->tensorflow.reshape(rotate_half_key_layer, shape_list(key_layer))
A:transformers.models.roformer.modeling_tf_roformer.rotate_half_value_layer->tensorflow.reshape(rotate_half_value_layer, shape_list(value_layer))
A:transformers.models.roformer.modeling_tf_roformer.self.dense->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.models.roformer.modeling_tf_roformer.hidden_states->self.out_proj(hidden_states)
A:transformers.models.roformer.modeling_tf_roformer.self.self_attention->TFRoFormerSelfAttention(config, name='self')
A:transformers.models.roformer.modeling_tf_roformer.self.dense_output->TFRoFormerSelfOutput(config, name='output')
A:transformers.models.roformer.modeling_tf_roformer.self_outputs->self.self_attention(hidden_states=input_tensor, attention_mask=attention_mask, sinusoidal_pos=sinusoidal_pos, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.roformer.modeling_tf_roformer.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.roformer.modeling_tf_roformer.self.attention->TFRoFormerAttention(config, name='attention')
A:transformers.models.roformer.modeling_tf_roformer.self.intermediate->TFRoFormerIntermediate(config, name='intermediate')
A:transformers.models.roformer.modeling_tf_roformer.self.roformer_output->TFRoFormerOutput(config, name='output')
A:transformers.models.roformer.modeling_tf_roformer.attention_outputs->self.attention(input_tensor=hidden_states, attention_mask=attention_mask, sinusoidal_pos=sinusoidal_pos, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.roformer.modeling_tf_roformer.intermediate_output->self.intermediate(hidden_states=attention_output)
A:transformers.models.roformer.modeling_tf_roformer.layer_output->self.roformer_output(hidden_states=intermediate_output, input_tensor=attention_output, training=training)
A:transformers.models.roformer.modeling_tf_roformer.self.embed_positions->TFRoFormerSinusoidalPositionalEmbedding(config.max_position_embeddings, config.hidden_size // config.num_attention_heads, name='embed_positions')
A:transformers.models.roformer.modeling_tf_roformer.layer_outputs->layer_module(hidden_states=hidden_states, attention_mask=attention_mask, sinusoidal_pos=sinusoidal_pos, head_mask=head_mask[i], output_attentions=output_attentions, training=training)
A:transformers.models.roformer.modeling_tf_roformer.self.transform_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.roformer.modeling_tf_roformer.self.transform->TFRoFormerPredictionHeadTransform(config, name='transform')
A:transformers.models.roformer.modeling_tf_roformer.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.roformer.modeling_tf_roformer.self.predictions->TFRoFormerLMPredictionHead(config, input_embeddings, name='predictions')
A:transformers.models.roformer.modeling_tf_roformer.prediction_scores->self.mlm(sequence_output=sequence_output, training=inputs['training'])
A:transformers.models.roformer.modeling_tf_roformer.self.embeddings->TFRoFormerEmbeddings(config, name='embeddings')
A:transformers.models.roformer.modeling_tf_roformer.self.embeddings_project->tensorflow.keras.layers.Dense(config.hidden_size, name='embeddings_project')
A:transformers.models.roformer.modeling_tf_roformer.self.encoder->TFRoFormerEncoder(config, name='encoder')
A:transformers.models.roformer.modeling_tf_roformer.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.roformer.modeling_tf_roformer.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.roformer.modeling_tf_roformer.inputs['attention_mask']->tensorflow.fill(dims=input_shape, value=1)
A:transformers.models.roformer.modeling_tf_roformer.inputs['token_type_ids']->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.roformer.modeling_tf_roformer.embedding_output->self.embeddings_project(embedding_output, training=inputs['training'])
A:transformers.models.roformer.modeling_tf_roformer.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.roformer.modeling_tf_roformer.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.roformer.modeling_tf_roformer.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.roformer.modeling_tf_roformer.encoder_outputs->self.encoder(hidden_states=embedding_output, attention_mask=extended_attention_mask, head_mask=inputs['head_mask'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.roformer.modeling_tf_roformer.self.roformer->TFRoFormerMainLayer(config, name='roformer')
A:transformers.models.roformer.modeling_tf_roformer.outputs->self.roformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.roformer.modeling_tf_roformer.self.mlm->TFRoFormerMLMHead(config, input_embeddings=self.roformer.embeddings, name='mlm___cls')
A:transformers.models.roformer.modeling_tf_roformer.logits->self.qa_outputs(inputs=sequence_output)
A:transformers.models.roformer.modeling_tf_roformer.loss->self.compute_loss(labels=labels, logits=(start_logits, end_logits))
A:transformers.models.roformer.modeling_tf_roformer.self.out_proj->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.models.roformer.modeling_tf_roformer.self.classifier_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.roformer.modeling_tf_roformer.self.classifier->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.roformer.modeling_tf_roformer.self.sequence_summary->TFSequenceSummary(config, config.initializer_range, name='sequence_summary')
A:transformers.models.roformer.modeling_tf_roformer.reshaped_logits->tensorflow.reshape(tensor=logits, shape=(-1, num_choices))
A:transformers.models.roformer.modeling_tf_roformer.output->self.call(input_ids=inputs)
A:transformers.models.roformer.modeling_tf_roformer.sequence_output->self.dropout(inputs=sequence_output, training=inputs['training'])
A:transformers.models.roformer.modeling_tf_roformer.self.qa_outputs->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.roformer.modeling_tf_roformer.(start_logits, end_logits)->tensorflow.split(value=logits, num_or_size_splits=2, axis=-1)
A:transformers.models.roformer.modeling_tf_roformer.start_logits->tensorflow.squeeze(input=start_logits, axis=-1)
A:transformers.models.roformer.modeling_tf_roformer.end_logits->tensorflow.squeeze(input=end_logits, axis=-1)
transformers.TFRoFormerForCausalLM(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerForCausalLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFCausalLMOutput, Tuple[tf.Tensor]]
transformers.TFRoFormerForCausalLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFRoFormerForCausalLM.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.TFRoFormerForMaskedLM(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.TFRoFormerForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFRoFormerForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFRoFormerForMultipleChoice(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerForMultipleChoice.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]
transformers.TFRoFormerForMultipleChoice.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.TFRoFormerForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])->TFMultipleChoiceModelOutput
transformers.TFRoFormerForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFRoFormerForQuestionAnswering(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerForQuestionAnswering.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,start_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,end_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]
transformers.TFRoFormerForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFRoFormerForSequenceClassification(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.TFRoFormerForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFRoFormerForTokenClassification(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.TFRoFormerForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFRoFormerLayer(self,config:RoFormerConfig,**kwargs)
transformers.TFRoFormerLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,sinusoidal_pos:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.TFRoFormerModel(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.TFRoFormerModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.TFRoFormerModel.serving_output(self,output:TFBaseModelOutput)->TFBaseModelOutput
transformers.TFRoFormerPreTrainedModel(TFPreTrainedModel)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerAttention(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerAttention.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerAttention.call(self,input_tensor:tf.Tensor,attention_mask:tf.Tensor,sinusoidal_pos:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerAttention.prune_heads(self,heads)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerClassificationHead(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerClassificationHead.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerClassificationHead.call(self,hidden_states:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEmbeddings(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEmbeddings.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEmbeddings.call(self,input_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEncoder(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEncoder.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerEncoder.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,return_dict:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForCausalLM(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForCausalLM.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForCausalLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFCausalLMOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForCausalLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForCausalLM.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMaskedLM(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMaskedLM.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMultipleChoice(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMultipleChoice.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMultipleChoice.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMultipleChoice.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])->TFMultipleChoiceModelOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForQuestionAnswering(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForQuestionAnswering.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForQuestionAnswering.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,start_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,end_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForSequenceClassification(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForSequenceClassification.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForTokenClassification(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForTokenClassification.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerIntermediate(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerIntermediate.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead(self,config:RoFormerConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.__init__(self,config:RoFormerConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.build(self,input_shape:tf.TensorShape)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.get_bias(self)->Dict[str, tf.Variable]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.set_bias(self,value:tf.Variable)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLMPredictionHead.set_output_embeddings(self,value:tf.Variable)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLayer(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLayer.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,sinusoidal_pos:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMLMHead(self,config:RoFormerConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMLMHead.__init__(self,config:RoFormerConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMLMHead.call(self,sequence_output:tf.Tensor)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMainLayer(self,config:RoFormerConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMainLayer.__init__(self,config:RoFormerConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMainLayer._prune_heads(self,heads_to_prune)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.roformer.modeling_tf_roformer.TFRoFormerMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerModel(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerModel.__init__(self,config:RoFormerConfig,*inputs,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerModel.serving_output(self,output:TFBaseModelOutput)->TFBaseModelOutput
transformers.models.roformer.modeling_tf_roformer.TFRoFormerOutput(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerOutput.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerPreTrainedModel(TFPreTrainedModel)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerPredictionHeadTransform(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerPredictionHeadTransform.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerPredictionHeadTransform.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfAttention(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfAttention.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfAttention.apply_rotary_position_embeddings(sinusoidal_pos,query_layer,key_layer,value_layer=None)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfAttention.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,sinusoidal_pos:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfAttention.transpose_for_scores(self,tensor:tf.Tensor,batch_size:int)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfOutput(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfOutput.__init__(self,config:RoFormerConfig,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSelfOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,**kwargs)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSinusoidalPositionalEmbedding._init_weight(n_pos:int,dim:int)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSinusoidalPositionalEmbedding.build(self,input_shape:tf.TensorShape)
transformers.models.roformer.modeling_tf_roformer.TFRoFormerSinusoidalPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/modeling_roformer.py----------------------------------------
A:transformers.models.roformer.modeling_roformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.roformer.modeling_roformer.self.weight->self._init_weight(self.weight)
A:transformers.models.roformer.modeling_roformer.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.roformer.modeling_roformer.out[:, 0:sentinel]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.models.roformer.modeling_roformer.out[:, sentinel:]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.models.roformer.modeling_roformer.positions->torch.arange(past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device)
A:transformers.models.roformer.modeling_roformer.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.roformer.modeling_roformer.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.roformer.modeling_roformer.array->numpy.transpose(array)
A:transformers.models.roformer.modeling_roformer.name->name.split('/').split('/')
A:transformers.models.roformer.modeling_roformer.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.roformer.modeling_roformer.pointer->getattr(pointer, 'weight')
A:transformers.models.roformer.modeling_roformer.num->int(scope_names[1])
A:transformers.models.roformer.modeling_roformer.pointer.data->torch.from_numpy(array)
A:transformers.models.roformer.modeling_roformer.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.models.roformer.modeling_roformer.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.models.roformer.modeling_roformer.self.LayerNorm->torch.nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)
A:transformers.models.roformer.modeling_roformer.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.roformer.modeling_roformer.input_shape->torch.cat([input_ids, dummy_token], dim=1).size()
A:transformers.models.roformer.modeling_roformer.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.roformer.modeling_roformer.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.roformer.modeling_roformer.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.roformer.modeling_roformer.embeddings->self.dropout(embeddings)
A:transformers.models.roformer.modeling_roformer.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.roformer.modeling_roformer.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.roformer.modeling_roformer.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.roformer.modeling_roformer.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.roformer.modeling_roformer.x->self.out_proj(x)
A:transformers.models.roformer.modeling_roformer.mixed_query_layer->self.query(hidden_states)
A:transformers.models.roformer.modeling_roformer.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.roformer.modeling_roformer.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.roformer.modeling_roformer.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.roformer.modeling_roformer.(query_layer, key_layer, value_layer)->self.apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer)
A:transformers.models.roformer.modeling_roformer.(query_layer, key_layer)->self.apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer)
A:transformers.models.roformer.modeling_roformer.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.roformer.modeling_roformer.attention_probs->self.dropout(attention_probs)
A:transformers.models.roformer.modeling_roformer.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.roformer.modeling_roformer.(sin, cos)->sinusoidal_pos.chunk(2, dim=-1)
A:transformers.models.roformer.modeling_roformer.sin_pos->torch.repeat_interleave(sin, 2, dim=-1)
A:transformers.models.roformer.modeling_roformer.cos_pos->torch.repeat_interleave(cos, 2, dim=-1)
A:transformers.models.roformer.modeling_roformer.rotate_half_query_layer->torch.stack([-query_layer[..., 1::2], query_layer[..., ::2]], dim=-1).reshape_as(query_layer)
A:transformers.models.roformer.modeling_roformer.rotate_half_key_layer->torch.stack([-key_layer[..., 1::2], key_layer[..., ::2]], dim=-1).reshape_as(key_layer)
A:transformers.models.roformer.modeling_roformer.rotate_half_value_layer->torch.stack([-value_layer[..., 1::2], value_layer[..., ::2]], dim=-1).reshape_as(value_layer)
A:transformers.models.roformer.modeling_roformer.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.roformer.modeling_roformer.hidden_states->self.decoder(hidden_states)
A:transformers.models.roformer.modeling_roformer.self.self->RoFormerSelfAttention(config)
A:transformers.models.roformer.modeling_roformer.self.output->RoFormerOutput(config)
A:transformers.models.roformer.modeling_roformer.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.roformer.modeling_roformer.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.roformer.modeling_roformer.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.roformer.modeling_roformer.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.roformer.modeling_roformer.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.roformer.modeling_roformer.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.roformer.modeling_roformer.self_outputs->self.self(hidden_states, attention_mask, sinusoidal_pos, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.roformer.modeling_roformer.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.roformer.modeling_roformer.self.attention->RoFormerAttention(config)
A:transformers.models.roformer.modeling_roformer.self.crossattention->RoFormerAttention(config)
A:transformers.models.roformer.modeling_roformer.self.intermediate->RoFormerIntermediate(config)
A:transformers.models.roformer.modeling_roformer.self_attention_outputs->self.attention(hidden_states, attention_mask, sinusoidal_pos, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.roformer.modeling_roformer.cross_attention_outputs->self.crossattention(attention_output, attention_mask, sinusoidal_pos, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.roformer.modeling_roformer.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.roformer.modeling_roformer.intermediate_output->self.intermediate(attention_output)
A:transformers.models.roformer.modeling_roformer.self.embed_positions->RoFormerSinusoidalPositionalEmbedding(config.max_position_embeddings, config.hidden_size // config.num_attention_heads)
A:transformers.models.roformer.modeling_roformer.self.layer->torch.nn.ModuleList([RoFormerLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.roformer.modeling_roformer.layer_outputs->layer_module(hidden_states, attention_mask, sinusoidal_pos, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.roformer.modeling_roformer.self.transform->RoFormerPredictionHeadTransform(config)
A:transformers.models.roformer.modeling_roformer.self.decoder->torch.nn.Linear(config.embedding_size, config.vocab_size, bias=False)
A:transformers.models.roformer.modeling_roformer.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.roformer.modeling_roformer.self.predictions->RoFormerLMPredictionHead(config)
A:transformers.models.roformer.modeling_roformer.prediction_scores->self.cls(sequence_output)
A:transformers.models.roformer.modeling_roformer.self.embeddings->RoFormerEmbeddings(config)
A:transformers.models.roformer.modeling_roformer.self.embeddings_project->torch.nn.Linear(config.embedding_size, config.hidden_size)
A:transformers.models.roformer.modeling_roformer.self.encoder->RoFormerEncoder(config)
A:transformers.models.roformer.modeling_roformer.attention_mask->torch.cat([input_ids, dummy_token], dim=1).new_ones(input_shape)
A:transformers.models.roformer.modeling_roformer.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.roformer.modeling_roformer.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.roformer.modeling_roformer.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.roformer.modeling_roformer.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.roformer.modeling_roformer.embedding_output->self.embeddings_project(embedding_output)
A:transformers.models.roformer.modeling_roformer.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.roformer.modeling_roformer.self.roformer->RoFormerModel(config)
A:transformers.models.roformer.modeling_roformer.self.cls->RoFormerOnlyMLMHead(config)
A:transformers.models.roformer.modeling_roformer.outputs->self.roformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.roformer.modeling_roformer.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.roformer.modeling_roformer.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.roformer.modeling_roformer.dummy_token->torch.full((effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.models.roformer.modeling_roformer.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.models.roformer.modeling_roformer.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.models.roformer.modeling_roformer.labels->labels[:, 1:].contiguous()
A:transformers.models.roformer.modeling_roformer.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.roformer.modeling_roformer.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.roformer.modeling_roformer.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.roformer.modeling_roformer.logits->self.qa_outputs(sequence_output)
A:transformers.models.roformer.modeling_roformer.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.roformer.modeling_roformer.self.sequence_summary->SequenceSummary(config)
A:transformers.models.roformer.modeling_roformer.pooled_output->self.sequence_summary(sequence_output)
A:transformers.models.roformer.modeling_roformer.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.roformer.modeling_roformer.sequence_output->self.dropout(sequence_output)
A:transformers.models.roformer.modeling_roformer.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.roformer.modeling_roformer.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.roformer.modeling_roformer.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.roformer.modeling_roformer.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.roformer.modeling_roformer.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.models.roformer.modeling_roformer.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.models.roformer.modeling_roformer.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.roformer.modeling_roformer.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.roformer.modeling_roformer.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.models.roformer.modeling_roformer.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.roformer.modeling_roformer.end_loss->loss_fct(end_logits, end_positions)
transformers.RoFormerForCausalLM(self,config)
transformers.RoFormerForCausalLM._reorder_cache(self,past,beam_idx)
transformers.RoFormerForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerForCausalLM.get_output_embeddings(self)
transformers.RoFormerForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.RoFormerForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.RoFormerForMaskedLM(self,config)
transformers.RoFormerForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerForMaskedLM.get_output_embeddings(self)
transformers.RoFormerForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.RoFormerForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.RoFormerForMultipleChoice(self,config)
transformers.RoFormerForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerForQuestionAnswering(self,config)
transformers.RoFormerForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerForSequenceClassification(self,config)
transformers.RoFormerForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerForTokenClassification(self,config)
transformers.RoFormerForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerLayer(self,config)
transformers.RoFormerLayer.feed_forward_chunk(self,attention_output)
transformers.RoFormerLayer.forward(self,hidden_states,attention_mask=None,sinusoidal_pos=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.RoFormerModel(self,config)
transformers.RoFormerModel._prune_heads(self,heads_to_prune)
transformers.RoFormerModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RoFormerModel.get_input_embeddings(self)
transformers.RoFormerModel.set_input_embeddings(self,value)
transformers.RoFormerPreTrainedModel(PreTrainedModel)
transformers.RoFormerPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_roformer(model,config,tf_checkpoint_path)
transformers.models.roformer.modeling_roformer.RoFormerAttention(self,config)
transformers.models.roformer.modeling_roformer.RoFormerAttention.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerAttention.forward(self,hidden_states,attention_mask=None,sinusoidal_pos=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.roformer.modeling_roformer.RoFormerAttention.prune_heads(self,heads)
transformers.models.roformer.modeling_roformer.RoFormerClassificationHead(self,config)
transformers.models.roformer.modeling_roformer.RoFormerClassificationHead.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerClassificationHead.forward(self,features,**kwargs)
transformers.models.roformer.modeling_roformer.RoFormerEmbeddings(self,config)
transformers.models.roformer.modeling_roformer.RoFormerEmbeddings.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerEmbeddings.forward(self,input_ids=None,token_type_ids=None,inputs_embeds=None)
transformers.models.roformer.modeling_roformer.RoFormerEncoder(self,config)
transformers.models.roformer.modeling_roformer.RoFormerEncoder.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM._reorder_cache(self,past,beam_idx)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM.get_output_embeddings(self)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.models.roformer.modeling_roformer.RoFormerForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.roformer.modeling_roformer.RoFormerForMaskedLM(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForMaskedLM.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerForMaskedLM.get_output_embeddings(self)
transformers.models.roformer.modeling_roformer.RoFormerForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.models.roformer.modeling_roformer.RoFormerForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.roformer.modeling_roformer.RoFormerForMultipleChoice(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForMultipleChoice.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerForQuestionAnswering(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForQuestionAnswering.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerForSequenceClassification(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForSequenceClassification.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerForTokenClassification(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForTokenClassification.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerIntermediate(self,config)
transformers.models.roformer.modeling_roformer.RoFormerIntermediate.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerIntermediate.forward(self,hidden_states)
transformers.models.roformer.modeling_roformer.RoFormerLMPredictionHead(self,config)
transformers.models.roformer.modeling_roformer.RoFormerLMPredictionHead.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerLMPredictionHead.forward(self,hidden_states)
transformers.models.roformer.modeling_roformer.RoFormerLayer(self,config)
transformers.models.roformer.modeling_roformer.RoFormerLayer.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerLayer.feed_forward_chunk(self,attention_output)
transformers.models.roformer.modeling_roformer.RoFormerLayer.forward(self,hidden_states,attention_mask=None,sinusoidal_pos=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.roformer.modeling_roformer.RoFormerModel(self,config)
transformers.models.roformer.modeling_roformer.RoFormerModel.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerModel._prune_heads(self,heads_to_prune)
transformers.models.roformer.modeling_roformer.RoFormerModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roformer.modeling_roformer.RoFormerModel.get_input_embeddings(self)
transformers.models.roformer.modeling_roformer.RoFormerModel.set_input_embeddings(self,value)
transformers.models.roformer.modeling_roformer.RoFormerOnlyMLMHead(self,config)
transformers.models.roformer.modeling_roformer.RoFormerOnlyMLMHead.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerOnlyMLMHead.forward(self,sequence_output)
transformers.models.roformer.modeling_roformer.RoFormerOutput(self,config)
transformers.models.roformer.modeling_roformer.RoFormerOutput.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerOutput.forward(self,hidden_states,input_tensor)
transformers.models.roformer.modeling_roformer.RoFormerPreTrainedModel(PreTrainedModel)
transformers.models.roformer.modeling_roformer.RoFormerPreTrainedModel._init_weights(self,module)
transformers.models.roformer.modeling_roformer.RoFormerPredictionHeadTransform(self,config)
transformers.models.roformer.modeling_roformer.RoFormerPredictionHeadTransform.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerPredictionHeadTransform.forward(self,hidden_states)
transformers.models.roformer.modeling_roformer.RoFormerSelfAttention(self,config)
transformers.models.roformer.modeling_roformer.RoFormerSelfAttention.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerSelfAttention.apply_rotary_position_embeddings(sinusoidal_pos,query_layer,key_layer,value_layer=None)
transformers.models.roformer.modeling_roformer.RoFormerSelfAttention.forward(self,hidden_states,attention_mask=None,sinusoidal_pos=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.roformer.modeling_roformer.RoFormerSelfAttention.transpose_for_scores(self,x)
transformers.models.roformer.modeling_roformer.RoFormerSelfOutput(self,config)
transformers.models.roformer.modeling_roformer.RoFormerSelfOutput.__init__(self,config)
transformers.models.roformer.modeling_roformer.RoFormerSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.roformer.modeling_roformer.RoFormerSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.roformer.modeling_roformer.RoFormerSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.roformer.modeling_roformer.RoFormerSinusoidalPositionalEmbedding._init_weight(out:nn.Parameter)
transformers.models.roformer.modeling_roformer.RoFormerSinusoidalPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.roformer.modeling_roformer.load_tf_weights_in_roformer(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/__init__.py----------------------------------------
A:transformers.models.roformer.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roformer/convert_roformer_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.roformer.convert_roformer_original_tf_checkpoint_to_pytorch.config->transformers.RoFormerConfig.from_json_file(bert_config_file)
A:transformers.models.roformer.convert_roformer_original_tf_checkpoint_to_pytorch.model->RoFormerForMaskedLM(config)
A:transformers.models.roformer.convert_roformer_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.roformer.convert_roformer_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.roformer.convert_roformer_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/rag/tokenization_rag.py----------------------------------------
A:transformers.models.rag.tokenization_rag.logger->utils.logging.get_logger(__name__)
A:transformers.models.rag.tokenization_rag.question_encoder_path->os.path.join(save_directory, 'question_encoder_tokenizer')
A:transformers.models.rag.tokenization_rag.generator_path->os.path.join(save_directory, 'generator_tokenizer')
A:transformers.models.rag.tokenization_rag.config->configuration_rag.RagConfig.from_pretrained(pretrained_model_name_or_path)
A:transformers.models.rag.tokenization_rag.question_encoder->auto.tokenization_auto.AutoTokenizer.from_pretrained(pretrained_model_name_or_path, config=config.question_encoder, subfolder='question_encoder_tokenizer')
A:transformers.models.rag.tokenization_rag.generator->auto.tokenization_auto.AutoTokenizer.from_pretrained(pretrained_model_name_or_path, config=config.generator, subfolder='generator_tokenizer')
A:transformers.models.rag.tokenization_rag.model_inputs->self(src_texts, add_special_tokens=True, return_tensors=return_tensors, max_length=max_length, padding=padding, truncation=truncation, **kwargs)
A:transformers.models.rag.tokenization_rag.labels->self(tgt_texts, add_special_tokens=True, return_tensors=return_tensors, padding=padding, max_length=max_target_length, truncation=truncation, **kwargs)
transformers.RagTokenizer(self,question_encoder,generator)
transformers.RagTokenizer.as_target_tokenizer(self)
transformers.RagTokenizer.batch_decode(self,*args,**kwargs)
transformers.RagTokenizer.decode(self,*args,**kwargs)
transformers.RagTokenizer.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.RagTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str=None,truncation:bool=True,**kwargs)->BatchEncoding
transformers.RagTokenizer.save_pretrained(self,save_directory)
transformers.models.rag.tokenization_rag.RagTokenizer(self,question_encoder,generator)
transformers.models.rag.tokenization_rag.RagTokenizer.__init__(self,question_encoder,generator)
transformers.models.rag.tokenization_rag.RagTokenizer.as_target_tokenizer(self)
transformers.models.rag.tokenization_rag.RagTokenizer.batch_decode(self,*args,**kwargs)
transformers.models.rag.tokenization_rag.RagTokenizer.decode(self,*args,**kwargs)
transformers.models.rag.tokenization_rag.RagTokenizer.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.rag.tokenization_rag.RagTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str=None,truncation:bool=True,**kwargs)->BatchEncoding
transformers.models.rag.tokenization_rag.RagTokenizer.save_pretrained(self,save_directory)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/rag/retrieval_rag.py----------------------------------------
A:transformers.models.rag.retrieval_rag.logger->utils.logging.get_logger(__name__)
A:transformers.models.rag.retrieval_rag.self.passages->self._load_passages()
A:transformers.models.rag.retrieval_rag.archive_file->os.path.join(index_path, filename)
A:transformers.models.rag.retrieval_rag.resolved_archive_file->cached_path(archive_file)
A:transformers.models.rag.retrieval_rag.passages_path->os.path.join(save_directory, 'hf_dataset')
A:transformers.models.rag.retrieval_rag.passages->pickle.load(passages_file)
A:transformers.models.rag.retrieval_rag.resolved_index_path->self._resolve_path(self.index_path, self.INDEX_FILENAME + '.index.dpr')
A:transformers.models.rag.retrieval_rag.self.index->faiss.read_index(resolved_index_path)
A:transformers.models.rag.retrieval_rag.resolved_meta_path->self._resolve_path(self.index_path, self.INDEX_FILENAME + '.index_meta.dpr')
A:transformers.models.rag.retrieval_rag.self.index_id_to_db_id->pickle.load(metadata_file)
A:transformers.models.rag.retrieval_rag.index->cls._build_index(config)
A:transformers.models.rag.retrieval_rag.aux_dim->numpy.zeros(len(question_hidden_states), dtype='float32').reshape(-1, 1)
A:transformers.models.rag.retrieval_rag.query_nhsw_vectors->numpy.hstack((question_hidden_states, aux_dim))
A:transformers.models.rag.retrieval_rag.(_, docs_ids)->self.index.search(query_nhsw_vectors, n_docs)
A:transformers.models.rag.retrieval_rag.(_, ids)->self.dataset.search_batch('embeddings', question_hidden_states, n_docs)
A:transformers.models.rag.retrieval_rag.vectors[i]->numpy.vstack([vectors[i], np.zeros((n_docs - len(vectors[i]), self.vector_size))])
A:transformers.models.rag.retrieval_rag.dataset->load_from_disk(dataset_path)
A:transformers.models.rag.retrieval_rag.self.dataset->load_dataset(self.dataset_name, with_embeddings=True, with_index=True, split=self.dataset_split, index_name=self.index_name, dummy=self.use_dummy_dataset)
A:transformers.models.rag.retrieval_rag.rag_tokenizer->RagTokenizer(question_encoder=self.question_encoder_tokenizer, generator=self.generator_tokenizer)
A:transformers.models.rag.retrieval_rag.index_path->os.path.join(save_directory, 'hf_dataset_index.faiss')
A:transformers.models.rag.retrieval_rag.faiss_index->self.index.dataset._indexes.pop('embeddings')
A:transformers.models.rag.retrieval_rag.out->(prefix + doc_title + self.config.title_sep + doc_text + self.config.doc_sep + input_string).replace('  ', ' ')
A:transformers.models.rag.retrieval_rag.contextualized_inputs->self.generator_tokenizer.batch_encode_plus(rag_input_strings, max_length=self.config.max_combined_length, return_tensors=return_tensors, padding='max_length', truncation=True)
A:transformers.models.rag.retrieval_rag.question_hidden_states_batched->self._chunk_tensor(question_hidden_states, self.batch_size)
A:transformers.models.rag.retrieval_rag.start_time->time.time()
A:transformers.models.rag.retrieval_rag.(ids, vectors)->self.index.get_top_docs(question_hidden_states, n_docs)
A:transformers.models.rag.retrieval_rag.(doc_ids, retrieved_doc_embeds)->self._main_retrieve(question_hidden_states, n_docs)
A:transformers.models.rag.retrieval_rag.(retrieved_doc_embeds, doc_ids, docs)->self.retrieve(question_hidden_states, n_docs)
A:transformers.models.rag.retrieval_rag.input_strings->self.question_encoder_tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)
A:transformers.models.rag.retrieval_rag.(context_input_ids, context_attention_mask)->self.postprocess_docs(docs, input_strings, prefix, n_docs, return_tensors=return_tensors)
A:transformers.models.rag.retrieval_rag.tokenized_docs->self.ctx_encoder_tokenizer(retrived_doc_title, retrived_doc_text, truncation=True, padding='longest', return_tensors=return_tensors)
transformers.RagRetriever(self,config,question_encoder_tokenizer,generator_tokenizer,index=None,init_retrieval=True)
transformers.RagRetriever._build_index(config)
transformers.RagRetriever._chunk_tensor(self,t:Iterable,chunk_size:int)->List[Iterable]
transformers.RagRetriever._main_retrieve(self,question_hidden_states:np.ndarray,n_docs:int)->Tuple[np.ndarray, np.ndarray]
transformers.RagRetriever.from_pretrained(cls,retriever_name_or_path,indexed_dataset=None,**kwargs)
transformers.RagRetriever.init_retrieval(self)
transformers.RagRetriever.postprocess_docs(self,docs,input_strings,prefix,n_docs,return_tensors=None)
transformers.RagRetriever.retrieve(self,question_hidden_states:np.ndarray,n_docs:int)->Tuple[np.ndarray, List[dict]]
transformers.RagRetriever.save_pretrained(self,save_directory)
transformers.RagRetriever.set_ctx_encoder_tokenizer(self,ctx_encoder_tokenizer:PreTrainedTokenizer)
transformers.models.rag.retrieval_rag.CanonicalHFIndex(self,vector_size:int,dataset_name:str='wiki_dpr',dataset_split:str='train',index_name:Optional[str]=None,index_path:Optional[str]=None,use_dummy_dataset=False)
transformers.models.rag.retrieval_rag.CanonicalHFIndex.__init__(self,vector_size:int,dataset_name:str='wiki_dpr',dataset_split:str='train',index_name:Optional[str]=None,index_path:Optional[str]=None,use_dummy_dataset=False)
transformers.models.rag.retrieval_rag.CanonicalHFIndex.init_index(self)
transformers.models.rag.retrieval_rag.CustomHFIndex(self,vector_size:int,dataset,index_path=None)
transformers.models.rag.retrieval_rag.CustomHFIndex.__init__(self,vector_size:int,dataset,index_path=None)
transformers.models.rag.retrieval_rag.CustomHFIndex.init_index(self)
transformers.models.rag.retrieval_rag.CustomHFIndex.load_from_disk(cls,vector_size,dataset_path,index_path)
transformers.models.rag.retrieval_rag.HFIndexBase(self,vector_size,dataset,index_initialized=False)
transformers.models.rag.retrieval_rag.HFIndexBase.__init__(self,vector_size,dataset,index_initialized=False)
transformers.models.rag.retrieval_rag.HFIndexBase._check_dataset_format(self,with_index:bool)
transformers.models.rag.retrieval_rag.HFIndexBase.get_doc_dicts(self,doc_ids:np.ndarray)->List[dict]
transformers.models.rag.retrieval_rag.HFIndexBase.get_top_docs(self,question_hidden_states:np.ndarray,n_docs=5)->Tuple[np.ndarray, np.ndarray]
transformers.models.rag.retrieval_rag.HFIndexBase.init_index(self)
transformers.models.rag.retrieval_rag.HFIndexBase.is_initialized(self)
transformers.models.rag.retrieval_rag.Index
transformers.models.rag.retrieval_rag.Index.get_doc_dicts(self,doc_ids:np.ndarray)->List[dict]
transformers.models.rag.retrieval_rag.Index.get_top_docs(self,question_hidden_states:np.ndarray,n_docs=5)->Tuple[np.ndarray, np.ndarray]
transformers.models.rag.retrieval_rag.Index.init_index(self)
transformers.models.rag.retrieval_rag.Index.is_initialized(self)
transformers.models.rag.retrieval_rag.LegacyIndex(self,vector_size,index_path)
transformers.models.rag.retrieval_rag.LegacyIndex.__init__(self,vector_size,index_path)
transformers.models.rag.retrieval_rag.LegacyIndex._deserialize_index(self)
transformers.models.rag.retrieval_rag.LegacyIndex._load_passages(self)
transformers.models.rag.retrieval_rag.LegacyIndex._resolve_path(self,index_path,filename)
transformers.models.rag.retrieval_rag.LegacyIndex.get_doc_dicts(self,doc_ids:np.array)
transformers.models.rag.retrieval_rag.LegacyIndex.get_top_docs(self,question_hidden_states:np.ndarray,n_docs=5)->Tuple[np.ndarray, np.ndarray]
transformers.models.rag.retrieval_rag.LegacyIndex.init_index(self)
transformers.models.rag.retrieval_rag.LegacyIndex.is_initialized(self)
transformers.models.rag.retrieval_rag.RagRetriever(self,config,question_encoder_tokenizer,generator_tokenizer,index=None,init_retrieval=True)
transformers.models.rag.retrieval_rag.RagRetriever.__init__(self,config,question_encoder_tokenizer,generator_tokenizer,index=None,init_retrieval=True)
transformers.models.rag.retrieval_rag.RagRetriever._build_index(config)
transformers.models.rag.retrieval_rag.RagRetriever._chunk_tensor(self,t:Iterable,chunk_size:int)->List[Iterable]
transformers.models.rag.retrieval_rag.RagRetriever._main_retrieve(self,question_hidden_states:np.ndarray,n_docs:int)->Tuple[np.ndarray, np.ndarray]
transformers.models.rag.retrieval_rag.RagRetriever.from_pretrained(cls,retriever_name_or_path,indexed_dataset=None,**kwargs)
transformers.models.rag.retrieval_rag.RagRetriever.init_retrieval(self)
transformers.models.rag.retrieval_rag.RagRetriever.postprocess_docs(self,docs,input_strings,prefix,n_docs,return_tensors=None)
transformers.models.rag.retrieval_rag.RagRetriever.retrieve(self,question_hidden_states:np.ndarray,n_docs:int)->Tuple[np.ndarray, List[dict]]
transformers.models.rag.retrieval_rag.RagRetriever.save_pretrained(self,save_directory)
transformers.models.rag.retrieval_rag.RagRetriever.set_ctx_encoder_tokenizer(self,ctx_encoder_tokenizer:PreTrainedTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/rag/__init__.py----------------------------------------
A:transformers.models.rag.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/rag/modeling_tf_rag.py----------------------------------------
A:transformers.models.rag.modeling_tf_rag.logger->utils.logging.get_logger(__name__)
A:transformers.models.rag.modeling_tf_rag.question_encoder->auto.modeling_tf_auto.TFAutoModel.from_config(config.question_encoder, name='question_encoder')
A:transformers.models.rag.modeling_tf_rag.question_encoder_config->auto.configuration_auto.AutoConfig.from_pretrained(question_encoder_pretrained_model_name_or_path)
A:transformers.models.rag.modeling_tf_rag.generator->auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM.from_config(config.generator, name='generator', load_weight_prefix=load_weight_prefix + '/generator')
A:transformers.models.rag.modeling_tf_rag.generator_config->auto.configuration_auto.AutoConfig.from_pretrained(generator_pretrained_model_name_or_path)
A:transformers.models.rag.modeling_tf_rag.config->configuration_rag.RagConfig.from_question_encoder_generator_configs(question_encoder.config, generator.config, **kwargs)
A:transformers.models.rag.modeling_tf_rag.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, doc_scores=doc_scores, context_input_ids=context_input_ids, context_attention_mask=context_attention_mask, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, output_retrieved=output_retrieved, n_docs=n_docs, exclude_bos_score=exclude_bos_score, labels=labels, reduce_loss=reduce_loss, training=training, return_dict=return_dict, kwargs_call=kwargs)
A:transformers.models.rag.modeling_tf_rag.question_enc_outputs->self.question_encoder(input_ids, attention_mask=attention_mask, return_dict=True, training=training)
A:transformers.models.rag.modeling_tf_rag.retriever_outputs->self.retriever(input_ids, question_encoder_last_hidden_state.numpy(), prefix=self.generator.config.prefix, n_docs=n_docs, return_tensors='tf')
A:transformers.models.rag.modeling_tf_rag.context_input_ids->tensorflow.cast(context_input_ids, tf.int32)
A:transformers.models.rag.modeling_tf_rag.context_attention_mask->extend_enc_output(context_attention_mask, num_beams=num_beams)
A:transformers.models.rag.modeling_tf_rag.retrieved_doc_embeds->tensorflow.cast(retrieved_doc_embeds, tf.float32)
A:transformers.models.rag.modeling_tf_rag.retrieved_doc_ids->tensorflow.cast(retrieved_doc_ids, tf.int32)
A:transformers.models.rag.modeling_tf_rag.doc_scores->tensorflow.repeat(doc_scores, num_beams, axis=0)
A:transformers.models.rag.modeling_tf_rag.decoder_input_ids->tensorflow.fill((batch_size * num_beams, 1), tf.cast(decoder_start_token_id, tf.int32))
A:transformers.models.rag.modeling_tf_rag.decoder_attention_mask->tensorflow.repeat(decoder_attention_mask, n_docs, axis=0)
A:transformers.models.rag.modeling_tf_rag.gen_outputs->self.generator(context_input_ids, attention_mask=context_attention_mask, encoder_outputs=encoder_outputs, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, return_dict=True, training=training)
A:transformers.models.rag.modeling_tf_rag.self.rag->TFRagModel(config=config, question_encoder=question_encoder, generator=generator, retriever=retriever, load_weight_prefix=self.load_weight_prefix, name='rag')
A:transformers.models.rag.modeling_tf_rag.encoder_outputs->encoder(input_ids=context_input_ids, attention_mask=context_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=True)
A:transformers.models.rag.modeling_tf_rag.shape->shape_list(input_)
A:transformers.models.rag.modeling_tf_rag.res->tensorflow.concat(tmp, axis=dim)
A:transformers.models.rag.modeling_tf_rag.hidden_states->tf_index_select(hidden_states, 0, new_order)
A:transformers.models.rag.modeling_tf_rag.seq_logprobs->tensorflow.reshape(seq_logprobs, (seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.shape[-1]))
A:transformers.models.rag.modeling_tf_rag.doc_logprobs->tensorflow.expand_dims(doc_logprobs, axis=-1)
A:transformers.models.rag.modeling_tf_rag.outputs->self(input_ids=None, context_input_ids=individual_input_ids, context_attention_mask=individual_attention_mask, doc_scores=individual_doc_scores, labels=output_sequences, exclude_bos_score=True)
A:transformers.models.rag.modeling_tf_rag.loss->self.get_nll(outputs.logits, outputs.doc_scores, inputs['labels'], reduce_loss=inputs['reduce_loss'], epsilon=self.config.label_smoothing, n_docs=inputs['n_docs'])
A:transformers.models.rag.modeling_tf_rag.logits->self.marginalize(logits, outputs.doc_scores, inputs['n_docs'])
A:transformers.models.rag.modeling_tf_rag.out->self.retriever(input_ids, question_hidden_states.numpy().astype(np.float32), prefix=self.generator.config.prefix, n_docs=n_docs, return_tensors='tf')
A:transformers.models.rag.modeling_tf_rag.encoder->self.rag.generator.get_encoder()
A:transformers.models.rag.modeling_tf_rag.tensor->tensorflow.broadcast_to(tensor, new_shape)
A:transformers.models.rag.modeling_tf_rag.encoder_outputs['last_hidden_state']->extend_enc_output(last_hidden_state, num_beams=num_beams)
A:transformers.models.rag.modeling_tf_rag.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.rag.modeling_tf_rag.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), start_token_id)
A:transformers.models.rag.modeling_tf_rag.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.cast(0, tf.int32))
A:transformers.models.rag.modeling_tf_rag.target->tensorflow.reshape(param, (-1, param.shape[-1]))
A:transformers.models.rag.modeling_tf_rag.rag_logprobs->tensorflow.concat([first_token_scores, second_token_scores + doc_logprobs, remainder], axis=2)
A:transformers.models.rag.modeling_tf_rag.loss_fn->tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)
A:transformers.models.rag.modeling_tf_rag.y_pred->tensorflow.math.log(y_pred)
A:transformers.models.rag.modeling_tf_rag.melted_labels->tensorflow.reshape(labels, (-1,))
A:transformers.models.rag.modeling_tf_rag.active_loss->tensorflow.not_equal(melted_labels, self.config.generator.pad_token_id)
A:transformers.models.rag.modeling_tf_rag.reduced_logits->tensorflow.boolean_mask(tf.reshape(logits, (-1, logits.shape[2])), active_loss)
A:transformers.models.rag.modeling_tf_rag.labels->tensorflow.boolean_mask(melted_labels, active_loss)
A:transformers.models.rag.modeling_tf_rag.nll_loss->tensorflow.reduce_sum(nll_loss)
A:transformers.models.rag.modeling_tf_rag.smooth_loss->tensorflow.reduce_sum(smooth_loss)
A:transformers.models.rag.modeling_tf_rag.equal_bos_token_id_all->tensorflow.reduce_all(tf.equal(target[:, 0], bos_token_id))
A:transformers.models.rag.modeling_tf_rag.pad_mask->tensorflow.equal(target, self.config.generator.pad_token_id)
A:transformers.models.rag.modeling_tf_rag.ll->tensorflow.math.reduce_logsumexp(ll, axis=1)
A:transformers.models.rag.modeling_tf_rag.smooth_obj->tensorflow.math.reduce_logsumexp(smooth_obj, axis=1)
A:transformers.models.rag.modeling_tf_rag.idx->tensorflow.stack([tf.range(tf.shape(id_tensor)[0]), id_tensor[:, 0]], axis=-1)
A:transformers.models.rag.modeling_tf_rag.result->gather2d(target, id_tensor)
A:transformers.models.rag.modeling_tf_rag.id_tensor->tensorflow.reshape(id_tensor, (-1, 1))
A:transformers.models.rag.modeling_tf_rag.(ll, smooth_obj)->_mask_pads(ll, smooth_obj)
A:transformers.models.rag.modeling_tf_rag.output_sequences->tensorflow.stack(list({str(k.numpy().tolist()): k for k in output_sequences}.values()))
A:transformers.models.rag.modeling_tf_rag.new_input_ids->tensorflow.tile(input_ids[index:index + 1], (num_candidates, 1))
A:transformers.models.rag.modeling_tf_rag.individual_input_ids->tensorflow.tile(generator_input_ids, (num_candidates, 1))
A:transformers.models.rag.modeling_tf_rag.individual_attention_mask->tensorflow.tile(individual_attention_mask, (num_candidates, 1))
A:transformers.models.rag.modeling_tf_rag.individual_doc_scores->tensorflow.tile(individual_doc_scores, (num_candidates, 1))
A:transformers.models.rag.modeling_tf_rag.output->tensorflow.convert_to_tensor(output)
transformers.TFRagModel(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,load_weight_prefix:Optional[str]=None,**kwargs)
transformers.TFRagModel.call(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None,return_dict=None,training=False,**kwargs)
transformers.TFRagModel.set_retriever(self,retriever:RagRetriever)
transformers.TFRagPreTrainedModel(TFPreTrainedModel)
transformers.TFRagPreTrainedModel.from_pretrained_question_encoder_generator(cls,question_encoder_pretrained_model_name_or_path:str=None,generator_pretrained_model_name_or_path:str=None,retriever:RagRetriever=None,*model_args,**kwargs)->TFPreTrainedModel
transformers.TFRagSequenceForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.TFRagSequenceForGeneration._cat_and_pad(tensors,pad_token_id)
transformers.TFRagSequenceForGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None,exclude_bos_score=None,labels=None,reduce_loss=None,return_dict=None,training=False,**kwargs)
transformers.TFRagSequenceForGeneration.generate(self,input_ids:Optional[tf.Tensor]=None,attention_mask:Optional[tf.Tensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,do_deduplication=None,num_return_sequences=None,num_beams=None,n_docs=None,**model_kwargs)
transformers.TFRagSequenceForGeneration.generator(self)
transformers.TFRagSequenceForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,exclude_bos_score=False,n_docs=None)
transformers.TFRagSequenceForGeneration.question_encoder(self)
transformers.TFRagSequenceForGeneration.retriever(self)
transformers.TFRagSequenceForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.TFRagTokenForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.TFRagTokenForGeneration._reorder_cache(past,beam_idx)
transformers.TFRagTokenForGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None,do_marginalize=None,labels=None,reduce_loss=None,return_dict=None,training=False,**kwargs)
transformers.TFRagTokenForGeneration.compute_loss(self,labels,y_pred,smooth_epsilon=0.0,from_logits=True,reduce_loss=False)
transformers.TFRagTokenForGeneration.generate(self,input_ids:Optional[tf.Tensor]=None,attention_mask:Optional[tf.Tensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,max_length=None,min_length=None,early_stopping=None,use_cache=None,num_beams=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,length_penalty=None,no_repeat_ngram_size=None,bad_words_ids=None,num_return_sequences=None,decoder_start_token_id=None,n_docs=None,output_scores=None,output_attentions=None,output_hidden_states=None,return_dict_in_generate=None,**model_kwargs)
transformers.TFRagTokenForGeneration.generator(self)
transformers.TFRagTokenForGeneration.get_input_embeddings(self)
transformers.TFRagTokenForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,n_docs=None)
transformers.TFRagTokenForGeneration.get_output_embeddings(self)
transformers.TFRagTokenForGeneration.marginalize(self,seq_logits,doc_scores,n_docs=None)
transformers.TFRagTokenForGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,use_cache,doc_scores,n_docs=None,**kwargs)->Dict
transformers.TFRagTokenForGeneration.question_encoder(self)
transformers.TFRagTokenForGeneration.retriever(self)
transformers.TFRagTokenForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.TFRagTokenForGeneration.shift_tokens_right(self,input_ids,start_token_id=None)
transformers.models.rag.modeling_tf_rag.TFRagModel(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,load_weight_prefix:Optional[str]=None,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagModel.__init__(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,load_weight_prefix:Optional[str]=None,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagModel.call(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None,return_dict=None,training=False,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagModel.set_retriever(self,retriever:RagRetriever)
transformers.models.rag.modeling_tf_rag.TFRagPreTrainedModel(TFPreTrainedModel)
transformers.models.rag.modeling_tf_rag.TFRagPreTrainedModel.from_pretrained_question_encoder_generator(cls,question_encoder_pretrained_model_name_or_path:str=None,generator_pretrained_model_name_or_path:str=None,retriever:RagRetriever=None,*model_args,**kwargs)->TFPreTrainedModel
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.__init__(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration._cat_and_pad(tensors,pad_token_id)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None,exclude_bos_score=None,labels=None,reduce_loss=None,return_dict=None,training=False,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.generate(self,input_ids:Optional[tf.Tensor]=None,attention_mask:Optional[tf.Tensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,do_deduplication=None,num_return_sequences=None,num_beams=None,n_docs=None,**model_kwargs)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.generator(self)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,exclude_bos_score=False,n_docs=None)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.question_encoder(self)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.retriever(self)
transformers.models.rag.modeling_tf_rag.TFRagSequenceForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.__init__(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[TFPreTrainedModel]=None,generator:Optional[TFPreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration._reorder_cache(past,beam_idx)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None,do_marginalize=None,labels=None,reduce_loss=None,return_dict=None,training=False,**kwargs)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.compute_loss(self,labels,y_pred,smooth_epsilon=0.0,from_logits=True,reduce_loss=False)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.generate(self,input_ids:Optional[tf.Tensor]=None,attention_mask:Optional[tf.Tensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,max_length=None,min_length=None,early_stopping=None,use_cache=None,num_beams=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,length_penalty=None,no_repeat_ngram_size=None,bad_words_ids=None,num_return_sequences=None,decoder_start_token_id=None,n_docs=None,output_scores=None,output_attentions=None,output_hidden_states=None,return_dict_in_generate=None,**model_kwargs)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.generator(self)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.get_input_embeddings(self)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,n_docs=None)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.get_output_embeddings(self)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.marginalize(self,seq_logits,doc_scores,n_docs=None)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,use_cache,doc_scores,n_docs=None,**kwargs)->Dict
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.question_encoder(self)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.retriever(self)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.models.rag.modeling_tf_rag.TFRagTokenForGeneration.shift_tokens_right(self,input_ids,start_token_id=None)
transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput(ModelOutput)
transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/rag/configuration_rag.py----------------------------------------
A:transformers.models.rag.configuration_rag.question_encoder_config->kwargs.pop('question_encoder')
A:transformers.models.rag.configuration_rag.question_encoder_model_type->kwargs.pop('question_encoder').pop('model_type')
A:transformers.models.rag.configuration_rag.decoder_config->kwargs.pop('generator')
A:transformers.models.rag.configuration_rag.decoder_model_type->kwargs.pop('generator').pop('model_type')
A:transformers.models.rag.configuration_rag.self.question_encoder->auto.configuration_auto.AutoConfig.for_model(question_encoder_model_type, **question_encoder_config)
A:transformers.models.rag.configuration_rag.self.generator->auto.configuration_auto.AutoConfig.for_model(decoder_model_type, **decoder_config)
A:transformers.models.rag.configuration_rag.self.forced_eos_token_id->getattr(self.generator, 'forced_eos_token_id', None)
A:transformers.models.rag.configuration_rag.output->copy.deepcopy(self.__dict__)
A:transformers.models.rag.configuration_rag.output['question_encoder']->self.question_encoder.to_dict()
A:transformers.models.rag.configuration_rag.output['generator']->self.generator.to_dict()
transformers.RagConfig(self,vocab_size=None,is_encoder_decoder=True,prefix=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,decoder_start_token_id=None,title_sep='/',doc_sep='//',n_docs=5,max_combined_length=300,retrieval_vector_size=768,retrieval_batch_size=8,dataset='wiki_dpr',dataset_split='train',index_name='compressed',index_path=None,passages_path=None,use_dummy_dataset=False,reduce_loss=False,label_smoothing=0.0,do_deduplication=True,exclude_bos_score=False,do_marginalize=False,output_retrieved=False,use_cache=True,forced_eos_token_id=None,**kwargs)
transformers.RagConfig.from_question_encoder_generator_configs(cls,question_encoder_config:PretrainedConfig,generator_config:PretrainedConfig,**kwargs)->PretrainedConfig
transformers.RagConfig.to_dict(self)
transformers.models.rag.configuration_rag.RagConfig(self,vocab_size=None,is_encoder_decoder=True,prefix=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,decoder_start_token_id=None,title_sep='/',doc_sep='//',n_docs=5,max_combined_length=300,retrieval_vector_size=768,retrieval_batch_size=8,dataset='wiki_dpr',dataset_split='train',index_name='compressed',index_path=None,passages_path=None,use_dummy_dataset=False,reduce_loss=False,label_smoothing=0.0,do_deduplication=True,exclude_bos_score=False,do_marginalize=False,output_retrieved=False,use_cache=True,forced_eos_token_id=None,**kwargs)
transformers.models.rag.configuration_rag.RagConfig.__init__(self,vocab_size=None,is_encoder_decoder=True,prefix=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,decoder_start_token_id=None,title_sep='/',doc_sep='//',n_docs=5,max_combined_length=300,retrieval_vector_size=768,retrieval_batch_size=8,dataset='wiki_dpr',dataset_split='train',index_name='compressed',index_path=None,passages_path=None,use_dummy_dataset=False,reduce_loss=False,label_smoothing=0.0,do_deduplication=True,exclude_bos_score=False,do_marginalize=False,output_retrieved=False,use_cache=True,forced_eos_token_id=None,**kwargs)
transformers.models.rag.configuration_rag.RagConfig.from_question_encoder_generator_configs(cls,question_encoder_config:PretrainedConfig,generator_config:PretrainedConfig,**kwargs)->PretrainedConfig
transformers.models.rag.configuration_rag.RagConfig.to_dict(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/rag/modeling_rag.py----------------------------------------
A:transformers.models.rag.modeling_rag.logger->utils.logging.get_logger(__name__)
A:transformers.models.rag.modeling_rag.question_encoder->auto.modeling_auto.AutoModel.from_config(config.question_encoder)
A:transformers.models.rag.modeling_rag.(question_encoder_config, kwargs_question_encoder)->auto.configuration_auto.AutoConfig.from_pretrained(question_encoder_pretrained_model_name_or_path, **kwargs_question_encoder, return_unused_kwargs=True)
A:transformers.models.rag.modeling_rag.generator->auto.modeling_auto.AutoModelForSeq2SeqLM.from_config(config.generator)
A:transformers.models.rag.modeling_rag.(generator_config, kwargs_generator)->auto.configuration_auto.AutoConfig.from_pretrained(generator_pretrained_model_name_or_path, **kwargs_generator, return_unused_kwargs=True)
A:transformers.models.rag.modeling_rag.config->configuration_rag.RagConfig.from_question_encoder_generator_configs(question_encoder.config, generator.config, **kwargs)
A:transformers.models.rag.modeling_rag.question_enc_outputs->self.question_encoder(input_ids, attention_mask=attention_mask, return_dict=True)
A:transformers.models.rag.modeling_rag.retriever_outputs->self.retriever(input_ids, question_encoder_last_hidden_state.cpu().detach().to(torch.float32).numpy(), prefix=self.generator.config.prefix, n_docs=n_docs, return_tensors='pt')
A:transformers.models.rag.modeling_rag.context_input_ids->context_input_ids.to(input_ids).to(input_ids)
A:transformers.models.rag.modeling_rag.context_attention_mask->extend_enc_output(context_attention_mask, num_beams=num_beams)
A:transformers.models.rag.modeling_rag.retrived_doc_input_ids->retrived_doc_input_ids.to(input_ids).to(input_ids)
A:transformers.models.rag.modeling_rag.retrived_doc_attention_mask->retrived_doc_attention_mask.to(input_ids).to(input_ids)
A:transformers.models.rag.modeling_rag.retrieved_doc_embeds->retrieved_doc_embeds.to(question_hidden_states).to(question_hidden_states)
A:transformers.models.rag.modeling_rag.doc_scores->doc_scores.repeat_interleave(num_beams, dim=0).repeat_interleave(num_beams, dim=0)
A:transformers.models.rag.modeling_rag.decoder_input_ids->decoder_input_ids.repeat_interleave(n_docs, dim=0).repeat_interleave(n_docs, dim=0)
A:transformers.models.rag.modeling_rag.decoder_attention_mask->decoder_attention_mask.repeat_interleave(n_docs, dim=0).repeat_interleave(n_docs, dim=0)
A:transformers.models.rag.modeling_rag.gen_outputs->self.generator(input_ids=context_input_ids, attention_mask=context_attention_mask, encoder_outputs=encoder_outputs, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, return_dict=True)
A:transformers.models.rag.modeling_rag.self.rag->RagModel(config=config, question_encoder=question_encoder, generator=generator, retriever=retriever)
A:transformers.models.rag.modeling_rag.outputs->self.rag(input_ids=input_ids, attention_mask=attention_mask, encoder_outputs=encoder_outputs, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, context_input_ids=context_input_ids, context_attention_mask=context_attention_mask, doc_scores=doc_scores, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, output_retrieved=output_retrieved, n_docs=n_docs)
A:transformers.models.rag.modeling_rag.loss->self.get_nll(outputs.logits, outputs.doc_scores, labels, reduce_loss=reduce_loss, epsilon=self.config.label_smoothing, n_docs=n_docs)
A:transformers.models.rag.modeling_rag.output_sequences->torch.stack(list({str(k.tolist()): k for k in output_sequences}.values()))
A:transformers.models.rag.modeling_rag.new_input_ids->input_ids[index:index + 1].repeat(num_candidates, 1)
A:transformers.models.rag.modeling_rag.individual_input_ids->generator_input_ids.repeat(num_candidates, 1)
A:transformers.models.rag.modeling_rag.individual_attention_mask->individual_attention_mask.repeat(num_candidates, 1).repeat(num_candidates, 1)
A:transformers.models.rag.modeling_rag.individual_doc_scores->individual_doc_scores.repeat(num_candidates, 1).repeat(num_candidates, 1)
A:transformers.models.rag.modeling_rag.target->target.unsqueeze(-1).unsqueeze(-1)
A:transformers.models.rag.modeling_rag.pad_mask->target.unsqueeze(-1).unsqueeze(-1).eq(self.config.generator.pad_token_id)
A:transformers.models.rag.modeling_rag.seq_logprobs->torch.nn.functional.log_softmax(seq_logits, dim=-1).view(seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1))
A:transformers.models.rag.modeling_rag.doc_logprobs->torch.log_softmax(doc_scores, dim=1)
A:transformers.models.rag.modeling_rag.rag_logprobs->self.marginalize(seq_logits, doc_scores, n_docs)
A:transformers.models.rag.modeling_rag.ll->ll.sum(1).sum(1)
A:transformers.models.rag.modeling_rag.smooth_obj->smooth_obj.sum(1).sum(1)
A:transformers.models.rag.modeling_rag.(ll, smooth_obj)->_mask_pads(ll, smooth_obj)
A:transformers.models.rag.modeling_rag.nll_loss->nll_loss.sum().sum()
A:transformers.models.rag.modeling_rag.smooth_loss->smooth_loss.sum().sum()
A:transformers.models.rag.modeling_rag.output->tensors[0].new(sum([t.shape[0] for t in tensors]), max([t.shape[1] for t in tensors])).fill_(pad_token_id)
A:transformers.models.rag.modeling_rag.hidden_states->hidden_states.index_select(0, new_order).index_select(0, new_order)
A:transformers.models.rag.modeling_rag.result->hidden_states.index_select(0, new_order).index_select(0, new_order).view(-1, *hidden_states.shape[2:])
A:transformers.models.rag.modeling_rag.logits->self.marginalize(logits, outputs.doc_scores, n_docs)
A:transformers.models.rag.modeling_rag.out->self.retriever(input_ids, question_hidden_states.cpu().detach().to(torch.float32).numpy(), prefix=self.generator.config.prefix, n_docs=n_docs, return_tensors='pt')
A:transformers.models.rag.modeling_rag.encoder->self.rag.generator.get_encoder()
A:transformers.models.rag.modeling_rag.encoder_outputs->encoder(input_ids=context_input_ids, attention_mask=context_attention_mask, return_dict=True)
A:transformers.models.rag.modeling_rag.input_ids->torch.full((batch_size * num_beams, 1), decoder_start_token_id, dtype=torch.long, device=next(self.parameters()).device)
A:transformers.models.rag.modeling_rag.tensor->tensor.expand((batch_size, num_beams, n_docs) + tensor.shape[3:]).expand((batch_size, num_beams, n_docs) + tensor.shape[3:])
A:transformers.models.rag.modeling_rag.encoder_outputs['last_hidden_state']->extend_enc_output(last_hidden_state, num_beams=num_beams)
A:transformers.models.rag.modeling_rag.pre_processor->self._get_logits_processor(repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size, encoder_input_ids=context_input_ids, bad_words_ids=bad_words_ids, min_length=min_length, max_length=max_length, eos_token_id=eos_token_id, forced_bos_token_id=forced_bos_token_id, forced_eos_token_id=forced_eos_token_id, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn, num_beams=num_beams, num_beam_groups=num_beam_groups, diversity_penalty=diversity_penalty, remove_invalid_values=remove_invalid_values)
A:transformers.models.rag.modeling_rag.beam_scorer->BeamSearchScorer(batch_size=batch_size, num_beams=num_beams, device=self.device, length_penalty=length_penalty, do_early_stopping=early_stopping, num_beam_hyps_to_keep=num_return_sequences)
A:transformers.models.rag.modeling_rag.shifted_input_ids->torch.full((batch_size * num_beams, 1), decoder_start_token_id, dtype=torch.long, device=next(self.parameters()).device).new_zeros(input_ids.shape)
A:transformers.models.rag.modeling_rag.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
transformers.RagModel(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.RagModel.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None)
transformers.RagPreTrainedModel(PreTrainedModel)
transformers.RagPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.RagPreTrainedModel.from_pretrained_question_encoder_generator(cls,question_encoder_pretrained_model_name_or_path:str=None,generator_pretrained_model_name_or_path:str=None,retriever:RagRetriever=None,**kwargs)->PreTrainedModel
transformers.RagSequenceForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.RagSequenceForGeneration._cat_and_pad(tensors,pad_token_id)
transformers.RagSequenceForGeneration.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,exclude_bos_score=None,reduce_loss=None,labels=None,n_docs=None,**kwargs)
transformers.RagSequenceForGeneration.generate(self,input_ids:Optional[torch.LongTensor]=None,attention_mask:Optional[torch.LongTensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,do_deduplication=None,num_return_sequences=None,num_beams=None,n_docs=None,**model_kwargs)
transformers.RagSequenceForGeneration.generator(self)
transformers.RagSequenceForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,exclude_bos_score=False,n_docs=None)
transformers.RagSequenceForGeneration.question_encoder(self)
transformers.RagSequenceForGeneration.retriever(self)
transformers.RagSequenceForGeneration.set_context_encoder_for_training(self,ctx_encoder:PreTrainedModel)
transformers.RagSequenceForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.RagTokenForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.RagTokenForGeneration._reorder_cache(past,beam_idx)
transformers.RagTokenForGeneration.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,do_marginalize=None,reduce_loss=None,labels=None,n_docs=None,**kwargs)
transformers.RagTokenForGeneration.generate(self,input_ids:Optional[torch.LongTensor]=None,attention_mask:Optional[torch.LongTensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,max_length=None,min_length=None,early_stopping=None,use_cache=None,num_beams=None,num_beam_groups=None,diversity_penalty=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,length_penalty=None,no_repeat_ngram_size=None,encoder_no_repeat_ngram_size=None,repetition_penalty=None,bad_words_ids=None,num_return_sequences=None,decoder_start_token_id=None,n_docs=None,prefix_allowed_tokens_fn:Callable[[int,torch.Tensor],List[int]]=None,forced_bos_token_id:Optional[int]=None,forced_eos_token_id:Optional[int]=None,remove_invalid_values:Optional[bool]=None,**model_kwargs)
transformers.RagTokenForGeneration.generator(self)
transformers.RagTokenForGeneration.get_input_embeddings(self)
transformers.RagTokenForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,n_docs=None)
transformers.RagTokenForGeneration.get_output_embeddings(self)
transformers.RagTokenForGeneration.marginalize(self,seq_logits,doc_scores,n_docs=None)
transformers.RagTokenForGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,use_cache=None,encoder_outputs=None,doc_scores=None,n_docs=None,**kwargs)
transformers.RagTokenForGeneration.question_encoder(self)
transformers.RagTokenForGeneration.retriever(self)
transformers.RagTokenForGeneration.set_context_encoder_for_training(self,ctx_encoder:PreTrainedModel)
transformers.RagTokenForGeneration.set_output_embeddings(self,new_embeddings)
transformers.RagTokenForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.RagTokenForGeneration.shift_tokens_right(self,input_ids,start_token_id=None)
transformers.models.rag.modeling_rag.RagModel(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_rag.RagModel.__init__(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_rag.RagModel.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,doc_scores=None,context_input_ids=None,context_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,n_docs=None)
transformers.models.rag.modeling_rag.RagPreTrainedModel(PreTrainedModel)
transformers.models.rag.modeling_rag.RagPreTrainedModel.from_pretrained(cls,*args,**kwargs)
transformers.models.rag.modeling_rag.RagPreTrainedModel.from_pretrained_question_encoder_generator(cls,question_encoder_pretrained_model_name_or_path:str=None,generator_pretrained_model_name_or_path:str=None,retriever:RagRetriever=None,**kwargs)->PreTrainedModel
transformers.models.rag.modeling_rag.RagSequenceForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.__init__(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_rag.RagSequenceForGeneration._cat_and_pad(tensors,pad_token_id)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,exclude_bos_score=None,reduce_loss=None,labels=None,n_docs=None,**kwargs)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.generate(self,input_ids:Optional[torch.LongTensor]=None,attention_mask:Optional[torch.LongTensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,do_deduplication=None,num_return_sequences=None,num_beams=None,n_docs=None,**model_kwargs)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.generator(self)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,exclude_bos_score=False,n_docs=None)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.question_encoder(self)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.retriever(self)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.set_context_encoder_for_training(self,ctx_encoder:PreTrainedModel)
transformers.models.rag.modeling_rag.RagSequenceForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.models.rag.modeling_rag.RagTokenForGeneration(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_rag.RagTokenForGeneration.__init__(self,config:Optional[PretrainedConfig]=None,question_encoder:Optional[PreTrainedModel]=None,generator:Optional[PreTrainedModel]=None,retriever:Optional=None,**kwargs)
transformers.models.rag.modeling_rag.RagTokenForGeneration._reorder_cache(past,beam_idx)
transformers.models.rag.modeling_rag.RagTokenForGeneration.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,use_cache=None,output_attentions=None,output_hidden_states=None,output_retrieved=None,do_marginalize=None,reduce_loss=None,labels=None,n_docs=None,**kwargs)
transformers.models.rag.modeling_rag.RagTokenForGeneration.generate(self,input_ids:Optional[torch.LongTensor]=None,attention_mask:Optional[torch.LongTensor]=None,context_input_ids=None,context_attention_mask=None,doc_scores=None,max_length=None,min_length=None,early_stopping=None,use_cache=None,num_beams=None,num_beam_groups=None,diversity_penalty=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,length_penalty=None,no_repeat_ngram_size=None,encoder_no_repeat_ngram_size=None,repetition_penalty=None,bad_words_ids=None,num_return_sequences=None,decoder_start_token_id=None,n_docs=None,prefix_allowed_tokens_fn:Callable[[int,torch.Tensor],List[int]]=None,forced_bos_token_id:Optional[int]=None,forced_eos_token_id:Optional[int]=None,remove_invalid_values:Optional[bool]=None,**model_kwargs)
transformers.models.rag.modeling_rag.RagTokenForGeneration.generator(self)
transformers.models.rag.modeling_rag.RagTokenForGeneration.get_input_embeddings(self)
transformers.models.rag.modeling_rag.RagTokenForGeneration.get_nll(self,seq_logits,doc_scores,target,reduce_loss=False,epsilon=0.0,n_docs=None)
transformers.models.rag.modeling_rag.RagTokenForGeneration.get_output_embeddings(self)
transformers.models.rag.modeling_rag.RagTokenForGeneration.marginalize(self,seq_logits,doc_scores,n_docs=None)
transformers.models.rag.modeling_rag.RagTokenForGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,use_cache=None,encoder_outputs=None,doc_scores=None,n_docs=None,**kwargs)
transformers.models.rag.modeling_rag.RagTokenForGeneration.question_encoder(self)
transformers.models.rag.modeling_rag.RagTokenForGeneration.retriever(self)
transformers.models.rag.modeling_rag.RagTokenForGeneration.set_context_encoder_for_training(self,ctx_encoder:PreTrainedModel)
transformers.models.rag.modeling_rag.RagTokenForGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.rag.modeling_rag.RagTokenForGeneration.set_retriever(self,retriever:RagRetriever)
transformers.models.rag.modeling_rag.RagTokenForGeneration.shift_tokens_right(self,input_ids,start_token_id=None)
transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput(ModelOutput)
transformers.models.rag.modeling_rag.RetrievAugLMOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/luke/modeling_luke.py----------------------------------------
A:transformers.models.luke.modeling_luke.logger->utils.logging.get_logger(__name__)
A:transformers.models.luke.modeling_luke.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.luke.modeling_luke.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.luke.modeling_luke.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.luke.modeling_luke.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.luke.modeling_luke.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.luke.modeling_luke.position_ids->torch.arange(self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device)
A:transformers.models.luke.modeling_luke.input_shape->input_ids.size()
A:transformers.models.luke.modeling_luke.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.luke.modeling_luke.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.luke.modeling_luke.position_embeddings->torch.sum(position_embeddings, dim=-2)
A:transformers.models.luke.modeling_luke.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.luke.modeling_luke.embeddings->self.dropout(embeddings)
A:transformers.models.luke.modeling_luke.self.entity_embeddings->LukeEntityEmbeddings(config)
A:transformers.models.luke.modeling_luke.self.entity_embedding_dense->torch.nn.Linear(config.entity_emb_size, config.hidden_size, bias=False)
A:transformers.models.luke.modeling_luke.entity_embeddings->self.entity_embedding_dense(entity_embeddings)
A:transformers.models.luke.modeling_luke.position_embedding_mask->(position_ids != -1).type_as(position_embeddings).unsqueeze(-1)
A:transformers.models.luke.modeling_luke.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.luke.modeling_luke.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.luke.modeling_luke.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.luke.modeling_luke.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.luke.modeling_luke.self.w2e_query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.luke.modeling_luke.self.e2w_query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.luke.modeling_luke.self.e2e_query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.luke.modeling_luke.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.luke.modeling_luke.word_size->word_hidden_states.size(1)
A:transformers.models.luke.modeling_luke.concat_hidden_states->torch.cat([word_hidden_states, entity_hidden_states], dim=1)
A:transformers.models.luke.modeling_luke.key_layer->self.transpose_for_scores(self.key(concat_hidden_states))
A:transformers.models.luke.modeling_luke.value_layer->self.transpose_for_scores(self.value(concat_hidden_states))
A:transformers.models.luke.modeling_luke.w2w_query_layer->self.transpose_for_scores(self.query(word_hidden_states))
A:transformers.models.luke.modeling_luke.w2e_query_layer->self.transpose_for_scores(self.w2e_query(word_hidden_states))
A:transformers.models.luke.modeling_luke.e2w_query_layer->self.transpose_for_scores(self.e2w_query(entity_hidden_states))
A:transformers.models.luke.modeling_luke.e2e_query_layer->self.transpose_for_scores(self.e2e_query(entity_hidden_states))
A:transformers.models.luke.modeling_luke.w2w_attention_scores->torch.matmul(w2w_query_layer, w2w_key_layer.transpose(-1, -2))
A:transformers.models.luke.modeling_luke.w2e_attention_scores->torch.matmul(w2e_query_layer, w2e_key_layer.transpose(-1, -2))
A:transformers.models.luke.modeling_luke.e2w_attention_scores->torch.matmul(e2w_query_layer, e2w_key_layer.transpose(-1, -2))
A:transformers.models.luke.modeling_luke.e2e_attention_scores->torch.matmul(e2e_query_layer, e2e_key_layer.transpose(-1, -2))
A:transformers.models.luke.modeling_luke.word_attention_scores->torch.cat([w2w_attention_scores, w2e_attention_scores], dim=3)
A:transformers.models.luke.modeling_luke.entity_attention_scores->torch.cat([e2w_attention_scores, e2e_attention_scores], dim=3)
A:transformers.models.luke.modeling_luke.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.luke.modeling_luke.query_layer->self.transpose_for_scores(self.query(concat_hidden_states))
A:transformers.models.luke.modeling_luke.attention_probs->self.dropout(attention_probs)
A:transformers.models.luke.modeling_luke.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.luke.modeling_luke.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.luke.modeling_luke.hidden_states->self.LayerNorm(hidden_states + input_tensor)
A:transformers.models.luke.modeling_luke.self.self->LukeSelfAttention(config)
A:transformers.models.luke.modeling_luke.self.output->LukeOutput(config)
A:transformers.models.luke.modeling_luke.self.pruned_heads->set()
A:transformers.models.luke.modeling_luke.self_outputs->self.self(word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions)
A:transformers.models.luke.modeling_luke.concat_self_outputs->torch.cat(self_outputs[:2], dim=1)
A:transformers.models.luke.modeling_luke.attention_output->self.output(concat_self_outputs, concat_hidden_states)
A:transformers.models.luke.modeling_luke.self.attention->LukeAttention(config)
A:transformers.models.luke.modeling_luke.self.intermediate->LukeIntermediate(config)
A:transformers.models.luke.modeling_luke.self_attention_outputs->self.attention(word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.models.luke.modeling_luke.concat_attention_output->torch.cat(self_attention_outputs[:2], dim=1)
A:transformers.models.luke.modeling_luke.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.luke.modeling_luke.intermediate_output->self.intermediate(attention_output)
A:transformers.models.luke.modeling_luke.self.layer->torch.nn.ModuleList([LukeLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.luke.modeling_luke.layer_outputs->layer_module(word_hidden_states, entity_hidden_states, attention_mask, layer_head_mask, output_attentions)
A:transformers.models.luke.modeling_luke.self.activation->torch.nn.Tanh()
A:transformers.models.luke.modeling_luke.pooled_output->self.activation(pooled_output)
A:transformers.models.luke.modeling_luke.self.embeddings->LukeEmbeddings(config)
A:transformers.models.luke.modeling_luke.self.encoder->LukeEncoder(config)
A:transformers.models.luke.modeling_luke.attention_mask->torch.cat([attention_mask, entity_attention_mask], dim=-1)
A:transformers.models.luke.modeling_luke.entity_seq_length->entity_ids.size(1)
A:transformers.models.luke.modeling_luke.entity_attention_mask->torch.ones((batch_size, entity_seq_length), device=device)
A:transformers.models.luke.modeling_luke.entity_token_type_ids->torch.zeros((batch_size, entity_seq_length), dtype=torch.long, device=device)
A:transformers.models.luke.modeling_luke.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.luke.modeling_luke.word_embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.luke.modeling_luke.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.luke.modeling_luke.entity_embedding_output->self.entity_embeddings(entity_ids, entity_position_ids, entity_token_type_ids)
A:transformers.models.luke.modeling_luke.encoder_outputs->self.encoder(word_embedding_output, entity_embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.luke.modeling_luke.mask->input_ids.ne(padding_idx).int()
A:transformers.models.luke.modeling_luke.self.luke->LukeModel(config)
A:transformers.models.luke.modeling_luke.self.classifier->torch.nn.Linear(config.hidden_size * 3, config.num_labels)
A:transformers.models.luke.modeling_luke.outputs->self.luke(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, entity_ids=entity_ids, entity_attention_mask=entity_attention_mask, entity_token_type_ids=entity_token_type_ids, entity_position_ids=entity_position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=True)
A:transformers.models.luke.modeling_luke.feature_vector->self.dropout(feature_vector)
A:transformers.models.luke.modeling_luke.logits->self.classifier(feature_vector)
A:transformers.models.luke.modeling_luke.loss->torch.nn.functional.binary_cross_entropy_with_logits(logits.view(-1), labels.view(-1).type_as(logits))
A:transformers.models.luke.modeling_luke.hidden_size->self.luke(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, entity_ids=entity_ids, entity_attention_mask=entity_attention_mask, entity_token_type_ids=entity_token_type_ids, entity_position_ids=entity_position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=True).last_hidden_state.size(-1)
A:transformers.models.luke.modeling_luke.entity_start_positions->entity_start_positions.unsqueeze(-1).expand(-1, -1, hidden_size).unsqueeze(-1).expand(-1, -1, hidden_size)
A:transformers.models.luke.modeling_luke.start_states->torch.gather(outputs.last_hidden_state, -2, entity_start_positions)
A:transformers.models.luke.modeling_luke.entity_end_positions->entity_end_positions.unsqueeze(-1).expand(-1, -1, hidden_size).unsqueeze(-1).expand(-1, -1, hidden_size)
A:transformers.models.luke.modeling_luke.end_states->torch.gather(outputs.last_hidden_state, -2, entity_end_positions)
transformers.LukeForEntityClassification(self,config)
transformers.LukeForEntityClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LukeForEntityPairClassification(self,config)
transformers.LukeForEntityPairClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LukeForEntitySpanClassification(self,config)
transformers.LukeForEntitySpanClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,entity_start_positions=None,entity_end_positions=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LukeModel(self,config,add_pooling_layer=True)
transformers.LukeModel._prune_heads(self,heads_to_prune)
transformers.LukeModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LukeModel.get_entity_embeddings(self)
transformers.LukeModel.get_extended_attention_mask(self,word_attention_mask:torch.LongTensor,entity_attention_mask:Optional[torch.LongTensor])
transformers.LukeModel.get_input_embeddings(self)
transformers.LukeModel.set_entity_embeddings(self,value)
transformers.LukeModel.set_input_embeddings(self,value)
transformers.LukePreTrainedModel(PreTrainedModel)
transformers.LukePreTrainedModel._init_weights(self,module:nn.Module)
transformers.models.luke.modeling_luke.BaseLukeModelOutput(BaseModelOutput)
transformers.models.luke.modeling_luke.BaseLukeModelOutputWithPooling(BaseModelOutputWithPooling)
transformers.models.luke.modeling_luke.EntityClassificationOutput(ModelOutput)
transformers.models.luke.modeling_luke.EntityPairClassificationOutput(ModelOutput)
transformers.models.luke.modeling_luke.EntitySpanClassificationOutput(ModelOutput)
transformers.models.luke.modeling_luke.LukeAttention(self,config)
transformers.models.luke.modeling_luke.LukeAttention.__init__(self,config)
transformers.models.luke.modeling_luke.LukeAttention.forward(self,word_hidden_states,entity_hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.luke.modeling_luke.LukeAttention.prune_heads(self,heads)
transformers.models.luke.modeling_luke.LukeEmbeddings(self,config)
transformers.models.luke.modeling_luke.LukeEmbeddings.__init__(self,config)
transformers.models.luke.modeling_luke.LukeEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.models.luke.modeling_luke.LukeEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.luke.modeling_luke.LukeEncoder(self,config)
transformers.models.luke.modeling_luke.LukeEncoder.__init__(self,config)
transformers.models.luke.modeling_luke.LukeEncoder.forward(self,word_hidden_states,entity_hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.luke.modeling_luke.LukeEntityEmbeddings(self,config:LukeConfig)
transformers.models.luke.modeling_luke.LukeEntityEmbeddings.__init__(self,config:LukeConfig)
transformers.models.luke.modeling_luke.LukeEntityEmbeddings.forward(self,entity_ids:torch.LongTensor,position_ids:torch.LongTensor,token_type_ids:torch.LongTensor=None)
transformers.models.luke.modeling_luke.LukeForEntityClassification(self,config)
transformers.models.luke.modeling_luke.LukeForEntityClassification.__init__(self,config)
transformers.models.luke.modeling_luke.LukeForEntityClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.luke.modeling_luke.LukeForEntityPairClassification(self,config)
transformers.models.luke.modeling_luke.LukeForEntityPairClassification.__init__(self,config)
transformers.models.luke.modeling_luke.LukeForEntityPairClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.luke.modeling_luke.LukeForEntitySpanClassification(self,config)
transformers.models.luke.modeling_luke.LukeForEntitySpanClassification.__init__(self,config)
transformers.models.luke.modeling_luke.LukeForEntitySpanClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,entity_start_positions=None,entity_end_positions=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.luke.modeling_luke.LukeIntermediate(self,config)
transformers.models.luke.modeling_luke.LukeIntermediate.__init__(self,config)
transformers.models.luke.modeling_luke.LukeIntermediate.forward(self,hidden_states)
transformers.models.luke.modeling_luke.LukeLayer(self,config)
transformers.models.luke.modeling_luke.LukeLayer.__init__(self,config)
transformers.models.luke.modeling_luke.LukeLayer.feed_forward_chunk(self,attention_output)
transformers.models.luke.modeling_luke.LukeLayer.forward(self,word_hidden_states,entity_hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.luke.modeling_luke.LukeModel(self,config,add_pooling_layer=True)
transformers.models.luke.modeling_luke.LukeModel.__init__(self,config,add_pooling_layer=True)
transformers.models.luke.modeling_luke.LukeModel._prune_heads(self,heads_to_prune)
transformers.models.luke.modeling_luke.LukeModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,entity_ids=None,entity_attention_mask=None,entity_token_type_ids=None,entity_position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.luke.modeling_luke.LukeModel.get_entity_embeddings(self)
transformers.models.luke.modeling_luke.LukeModel.get_extended_attention_mask(self,word_attention_mask:torch.LongTensor,entity_attention_mask:Optional[torch.LongTensor])
transformers.models.luke.modeling_luke.LukeModel.get_input_embeddings(self)
transformers.models.luke.modeling_luke.LukeModel.set_entity_embeddings(self,value)
transformers.models.luke.modeling_luke.LukeModel.set_input_embeddings(self,value)
transformers.models.luke.modeling_luke.LukeOutput(self,config)
transformers.models.luke.modeling_luke.LukeOutput.__init__(self,config)
transformers.models.luke.modeling_luke.LukeOutput.forward(self,hidden_states,input_tensor)
transformers.models.luke.modeling_luke.LukePooler(self,config)
transformers.models.luke.modeling_luke.LukePooler.__init__(self,config)
transformers.models.luke.modeling_luke.LukePooler.forward(self,hidden_states)
transformers.models.luke.modeling_luke.LukePreTrainedModel(PreTrainedModel)
transformers.models.luke.modeling_luke.LukePreTrainedModel._init_weights(self,module:nn.Module)
transformers.models.luke.modeling_luke.LukeSelfAttention(self,config)
transformers.models.luke.modeling_luke.LukeSelfAttention.__init__(self,config)
transformers.models.luke.modeling_luke.LukeSelfAttention.forward(self,word_hidden_states,entity_hidden_states,attention_mask=None,head_mask=None,output_attentions=False)
transformers.models.luke.modeling_luke.LukeSelfAttention.transpose_for_scores(self,x)
transformers.models.luke.modeling_luke.LukeSelfOutput(self,config)
transformers.models.luke.modeling_luke.LukeSelfOutput.__init__(self,config)
transformers.models.luke.modeling_luke.LukeSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.luke.modeling_luke.create_position_ids_from_input_ids(input_ids,padding_idx)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/luke/tokenization_luke.py----------------------------------------
A:transformers.models.luke.tokenization_luke.logger->utils.logging.get_logger(__name__)
A:transformers.models.luke.tokenization_luke.self.entity_vocab->json.load(entity_vocab_handle)
A:transformers.models.luke.tokenization_luke.is_valid_single_text->isinstance(text, str)
A:transformers.models.luke.tokenization_luke.is_valid_single_text_pair->isinstance(text_pair, str)
A:transformers.models.luke.tokenization_luke.is_batched->bool(isinstance(text, (list, tuple)))
A:transformers.models.luke.tokenization_luke.(padding_strategy, truncation_strategy, max_length, kwargs)->self._get_padding_truncation_strategies(padding=padding, truncation=truncation, max_length=max_length, pad_to_multiple_of=pad_to_multiple_of, verbose=verbose, **kwargs)
A:transformers.models.luke.tokenization_luke.(first_ids, second_ids, first_entity_ids, second_entity_ids, first_entity_token_spans, second_entity_token_spans)->self._create_input_sequence(text=text, text_pair=text_pair, entities=entities, entities_pair=entities_pair, entity_spans=entity_spans, entity_spans_pair=entity_spans_pair, **kwargs)
A:transformers.models.luke.tokenization_luke.batch_outputs->BatchEncoding(encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis)
A:transformers.models.luke.tokenization_luke.tokens->self.tokenize(text, **kwargs)
A:transformers.models.luke.tokenization_luke.split_char_positions->sorted(frozenset(itertools.chain(*entity_spans)))
A:transformers.models.luke.tokenization_luke.char_pos2token_pos[orig_split_char_position]->len(input_ids)
A:transformers.models.luke.tokenization_luke.first_ids->get_input_ids(text)
A:transformers.models.luke.tokenization_luke.(first_ids, first_entity_token_spans)->get_input_ids_and_entity_token_spans(text, entity_spans)
A:transformers.models.luke.tokenization_luke.second_ids->get_input_ids(text_pair)
A:transformers.models.luke.tokenization_luke.(second_ids, second_entity_token_spans)->get_input_ids_and_entity_token_spans(text_pair, entity_spans_pair)
A:transformers.models.luke.tokenization_luke.token_span_with_special_token_ids->reversed(token_span_with_special_token_ids)
A:transformers.models.luke.tokenization_luke.outputs->self._pad(inputs, max_length=max_length, max_entity_length=max_entity_length, padding_strategy=padding_strategy, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.models.luke.tokenization_luke.pair->bool(pair_ids is not None)
A:transformers.models.luke.tokenization_luke.len_ids->len(ids)
A:transformers.models.luke.tokenization_luke.(ids, pair_ids, overflowing_tokens)->self.truncate_sequences(ids, pair_ids=pair_ids, num_tokens_to_remove=total_len - max_length, truncation_strategy=truncation_strategy, stride=stride)
A:transformers.models.luke.tokenization_luke.sequence->self.build_inputs_with_special_tokens(ids, pair_ids)
A:transformers.models.luke.tokenization_luke.token_type_ids->self.create_token_type_ids_from_sequences(ids, pair_ids)
A:transformers.models.luke.tokenization_luke.pair_entity_token_offset->len(ids)
A:transformers.models.luke.tokenization_luke.encoded_inputs['special_tokens_mask']->self.get_special_tokens_mask(ids, pair_ids)
A:transformers.models.luke.tokenization_luke.(valid_entity_ids, valid_pair_entity_ids, overflowing_entities)->self.truncate_sequences(valid_entity_ids, pair_ids=valid_pair_entity_ids, num_tokens_to_remove=total_entity_len - max_entity_length, truncation_strategy=truncation_strategy, stride=stride)
A:transformers.models.luke.tokenization_luke.encoded_inputs['entity_ids']->list(final_entity_ids)
A:transformers.models.luke.tokenization_luke.encoded_inputs->self._pad(encoded_inputs, max_length=max_length, max_entity_length=max_entity_length, padding_strategy=padding_strategy, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.models.luke.tokenization_luke.encoded_inputs['length']->len(encoded_inputs['input_ids'])
A:transformers.models.luke.tokenization_luke.encoded_inputs[key]->to_py_obj(value)
A:transformers.models.luke.tokenization_luke.(padding_strategy, _, max_length, _)->self._get_padding_truncation_strategies(padding=padding, max_length=max_length, verbose=verbose)
A:transformers.models.luke.tokenization_luke.batch_size->len(required_input)
A:transformers.models.luke.tokenization_luke.max_length->len(encoded_inputs['input_ids'])
A:transformers.models.luke.tokenization_luke.inputs->dict(((k, v[i]) for (k, v) in encoded_inputs.items()))
A:transformers.models.luke.tokenization_luke.entities_provided->bool('entity_ids' in encoded_inputs)
A:transformers.models.luke.tokenization_luke.max_entity_length->len(encoded_inputs['entity_ids'])
A:transformers.models.luke.tokenization_luke.(vocab_file, merge_file)->super().save_vocabulary(save_directory, filename_prefix)
A:transformers.models.luke.tokenization_luke.entity_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['entity_vocab_file'])
transformers.LukeTokenizer(self,vocab_file,merges_file,entity_vocab_file,task=None,max_entity_length=32,max_mention_length=30,entity_token_1='<ent>',entity_token_2='<ent2>',**kwargs)
transformers.LukeTokenizer._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair]],batch_entity_spans_or_entity_spans_pairs:Optional[Union[List[EntitySpanInput],List[Tuple[EntitySpanInput,EntitySpanInput]]]]=None,batch_entities_or_entities_pairs:Optional[Union[List[EntityInput],List[Tuple[EntityInput,EntityInput]]]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.LukeTokenizer._batch_prepare_for_model(self,batch_ids_pairs:List[Tuple[List[int],None]],batch_entity_ids_pairs:List[Tuple[Optional[List[int]],Optional[List[int]]]],batch_entity_token_spans_pairs:List[Tuple[Optional[List[Tuple[int,int]]],Optional[List[Tuple[int,int]]]]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.LukeTokenizer._create_input_sequence(self,text:Union[TextInput],text_pair:Optional[Union[TextInput]]=None,entities:Optional[EntityInput]=None,entities_pair:Optional[EntityInput]=None,entity_spans:Optional[EntitySpanInput]=None,entity_spans_pair:Optional[EntitySpanInput]=None,**kwargs)->Tuple[list, list, list, list, list, list]
transformers.LukeTokenizer._encode_plus(self,text:Union[TextInput],text_pair:Optional[Union[TextInput]]=None,entity_spans:Optional[EntitySpanInput]=None,entity_spans_pair:Optional[EntitySpanInput]=None,entities:Optional[EntityInput]=None,entities_pair:Optional[EntityInput]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.LukeTokenizer._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,max_entity_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.LukeTokenizer.batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair]],batch_entity_spans_or_entity_spans_pairs:Optional[Union[List[EntitySpanInput],List[Tuple[EntitySpanInput,EntitySpanInput]]]]=None,batch_entities_or_entities_pairs:Optional[Union[List[EntityInput],List[Tuple[EntityInput,EntityInput]]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.LukeTokenizer.encode_plus(self,text:Union[TextInput],text_pair:Optional[Union[TextInput]]=None,entity_spans:Optional[EntitySpanInput]=None,entity_spans_pair:Optional[EntitySpanInput]=None,entities:Optional[EntityInput]=None,entities_pair:Optional[EntityInput]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.LukeTokenizer.pad(self,encoded_inputs:Union[BatchEncoding,List[BatchEncoding],Dict[str,EncodedInput],Dict[str,List[EncodedInput]],List[Dict[str,EncodedInput]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None,verbose:bool=True)->BatchEncoding
transformers.LukeTokenizer.prepare_for_model(self,ids:List[int],pair_ids:Optional[List[int]]=None,entity_ids:Optional[List[int]]=None,pair_entity_ids:Optional[List[int]]=None,entity_token_spans:Optional[List[Tuple[int,int]]]=None,pair_entity_token_spans:Optional[List[Tuple[int,int]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.LukeTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.luke.tokenization_luke.LukeTokenizer(self,vocab_file,merges_file,entity_vocab_file,task=None,max_entity_length=32,max_mention_length=30,entity_token_1='<ent>',entity_token_2='<ent2>',**kwargs)
transformers.models.luke.tokenization_luke.LukeTokenizer.__init__(self,vocab_file,merges_file,entity_vocab_file,task=None,max_entity_length=32,max_mention_length=30,entity_token_1='<ent>',entity_token_2='<ent2>',**kwargs)
transformers.models.luke.tokenization_luke.LukeTokenizer._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair]],batch_entity_spans_or_entity_spans_pairs:Optional[Union[List[EntitySpanInput],List[Tuple[EntitySpanInput,EntitySpanInput]]]]=None,batch_entities_or_entities_pairs:Optional[Union[List[EntityInput],List[Tuple[EntityInput,EntityInput]]]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer._batch_prepare_for_model(self,batch_ids_pairs:List[Tuple[List[int],None]],batch_entity_ids_pairs:List[Tuple[Optional[List[int]],Optional[List[int]]]],batch_entity_token_spans_pairs:List[Tuple[Optional[List[Tuple[int,int]]],Optional[List[Tuple[int,int]]]]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer._create_input_sequence(self,text:Union[TextInput],text_pair:Optional[Union[TextInput]]=None,entities:Optional[EntityInput]=None,entities_pair:Optional[EntityInput]=None,entity_spans:Optional[EntitySpanInput]=None,entity_spans_pair:Optional[EntitySpanInput]=None,**kwargs)->Tuple[list, list, list, list, list, list]
transformers.models.luke.tokenization_luke.LukeTokenizer._encode_plus(self,text:Union[TextInput],text_pair:Optional[Union[TextInput]]=None,entity_spans:Optional[EntitySpanInput]=None,entity_spans_pair:Optional[EntitySpanInput]=None,entities:Optional[EntityInput]=None,entities_pair:Optional[EntityInput]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,max_entity_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.models.luke.tokenization_luke.LukeTokenizer.batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair]],batch_entity_spans_or_entity_spans_pairs:Optional[Union[List[EntitySpanInput],List[Tuple[EntitySpanInput,EntitySpanInput]]]]=None,batch_entities_or_entities_pairs:Optional[Union[List[EntityInput],List[Tuple[EntityInput,EntityInput]]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer.encode_plus(self,text:Union[TextInput],text_pair:Optional[Union[TextInput]]=None,entity_spans:Optional[EntitySpanInput]=None,entity_spans_pair:Optional[EntitySpanInput]=None,entities:Optional[EntityInput]=None,entities_pair:Optional[EntityInput]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,is_split_into_words:Optional[bool]=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer.pad(self,encoded_inputs:Union[BatchEncoding,List[BatchEncoding],Dict[str,EncodedInput],Dict[str,List[EncodedInput]],List[Dict[str,EncodedInput]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None,verbose:bool=True)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer.prepare_for_model(self,ids:List[int],pair_ids:Optional[List[int]]=None,entity_ids:Optional[List[int]]=None,pair_entity_ids:Optional[List[int]]=None,entity_token_spans:Optional[List[Tuple[int,int]]]=None,pair_entity_token_spans:Optional[List[Tuple[int,int]]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,max_entity_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.models.luke.tokenization_luke.LukeTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/luke/configuration_luke.py----------------------------------------
A:transformers.models.luke.configuration_luke.logger->utils.logging.get_logger(__name__)
transformers.LukeConfig(self,vocab_size=50267,entity_vocab_size=500000,hidden_size=768,entity_emb_size=256,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,gradient_checkpointing=False,use_entity_aware_attention=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.luke.configuration_luke.LukeConfig(self,vocab_size=50267,entity_vocab_size=500000,hidden_size=768,entity_emb_size=256,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,gradient_checkpointing=False,use_entity_aware_attention=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.luke.configuration_luke.LukeConfig.__init__(self,vocab_size=50267,entity_vocab_size=500000,hidden_size=768,entity_emb_size=256,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,gradient_checkpointing=False,use_entity_aware_attention=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.metadata->json.load(metadata_file)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.config->LukeConfig(use_entity_aware_attention=True, **metadata['model_config'])
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.state_dict->torch.load(checkpoint_path, map_location='cpu')
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.entity_vocab->load_entity_vocab(entity_vocab_path)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.tokenizer->transformers.LukeTokenizer.from_pretrained(pytorch_dump_folder_path, task='entity_classification')
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.entity_token_1->AddedToken('<ent>', lstrip=False, rstrip=False)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.entity_token_2->AddedToken('<ent2>', lstrip=False, rstrip=False)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.ent_emb->word_emb[tokenizer.convert_tokens_to_ids(['@'])[0]].unsqueeze(0)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.ent2_emb->word_emb[tokenizer.convert_tokens_to_ids(['#'])[0]].unsqueeze(0)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.state_dict['embeddings.word_embeddings.weight']->torch.cat([word_emb, ent_emb, ent2_emb])
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.model->LukeModel(config=config).eval()
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.(missing_keys, unexpected_keys)->LukeModel(config=config).eval().load_state_dict(state_dict, strict=False)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.encoding->tokenizer(text, entity_spans=[span], add_prefix_space=True, return_tensors='pt')
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.outputs->model(**encoding)
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.expected_shape->torch.Size((1, 1, 768))
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.expected_slice->torch.tensor([[0.1457, 0.1044, 0.0174]])
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.(title, _)->line.rstrip().split('\t')
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.convert_luke_checkpoint(checkpoint_path,metadata_path,entity_vocab_path,pytorch_dump_folder_path,model_size)
transformers.models.luke.convert_luke_original_pytorch_checkpoint_to_pytorch.load_entity_vocab(entity_vocab_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/luke/__init__.py----------------------------------------
A:transformers.models.luke.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/encoder_decoder/configuration_encoder_decoder.py----------------------------------------
A:transformers.models.encoder_decoder.configuration_encoder_decoder.logger->utils.logging.get_logger(__name__)
A:transformers.models.encoder_decoder.configuration_encoder_decoder.encoder_config->kwargs.pop('encoder')
A:transformers.models.encoder_decoder.configuration_encoder_decoder.encoder_model_type->kwargs.pop('encoder').pop('model_type')
A:transformers.models.encoder_decoder.configuration_encoder_decoder.decoder_config->kwargs.pop('decoder')
A:transformers.models.encoder_decoder.configuration_encoder_decoder.decoder_model_type->kwargs.pop('decoder').pop('model_type')
A:transformers.models.encoder_decoder.configuration_encoder_decoder.self.encoder->auto.configuration_auto.AutoConfig.for_model(encoder_model_type, **encoder_config)
A:transformers.models.encoder_decoder.configuration_encoder_decoder.self.decoder->auto.configuration_auto.AutoConfig.for_model(decoder_model_type, **decoder_config)
A:transformers.models.encoder_decoder.configuration_encoder_decoder.output->copy.deepcopy(self.__dict__)
A:transformers.models.encoder_decoder.configuration_encoder_decoder.output['encoder']->self.encoder.to_dict()
A:transformers.models.encoder_decoder.configuration_encoder_decoder.output['decoder']->self.decoder.to_dict()
transformers.EncoderDecoderConfig(self,**kwargs)
transformers.EncoderDecoderConfig.from_encoder_decoder_configs(cls,encoder_config:PretrainedConfig,decoder_config:PretrainedConfig,**kwargs)->PretrainedConfig
transformers.EncoderDecoderConfig.to_dict(self)
transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig(self,**kwargs)
transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig.__init__(self,**kwargs)
transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig.from_encoder_decoder_configs(cls,encoder_config:PretrainedConfig,decoder_config:PretrainedConfig,**kwargs)->PretrainedConfig
transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig.to_dict(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/encoder_decoder/__init__.py----------------------------------------
A:transformers.models.encoder_decoder.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/encoder_decoder/modeling_encoder_decoder.py----------------------------------------
A:transformers.models.encoder_decoder.modeling_encoder_decoder.logger->utils.logging.get_logger(__name__)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.config->configuration_encoder_decoder.EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config, **kwargs)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.encoder->auto.modeling_auto.AutoModel.from_pretrained(encoder_pretrained_model_name_or_path, *model_args, **kwargs_encoder)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.decoder->auto.modeling_auto.AutoModelForCausalLM.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.encoder_config->auto.configuration_auto.AutoConfig.from_pretrained(encoder_pretrained_model_name_or_path)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.decoder_config->auto.configuration_auto.AutoConfig.from_pretrained(decoder_pretrained_model_name_or_path)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.encoder_outputs->self.encoder(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, **kwargs_encoder)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=attention_mask, inputs_embeds=decoder_inputs_embeds, labels=labels, output_attentions=output_attentions, output_hidden_states=output_hidden_states, use_cache=use_cache, past_key_values=past_key_values, return_dict=return_dict, **kwargs_decoder)
A:transformers.models.encoder_decoder.modeling_encoder_decoder.decoder_inputs->self.decoder.prepare_inputs_for_generation(input_ids, past=past)
transformers.EncoderDecoderModel(self,config:Optional[PretrainedConfig]=None,encoder:Optional[PreTrainedModel]=None,decoder:Optional[PreTrainedModel]=None)
transformers.EncoderDecoderModel._reorder_cache(self,past,beam_idx)
transformers.EncoderDecoderModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.EncoderDecoderModel.from_encoder_decoder_pretrained(cls,encoder_pretrained_model_name_or_path:str=None,decoder_pretrained_model_name_or_path:str=None,*model_args,**kwargs)->PreTrainedModel
transformers.EncoderDecoderModel.from_pretrained(cls,*args,**kwargs)
transformers.EncoderDecoderModel.get_decoder(self)
transformers.EncoderDecoderModel.get_encoder(self)
transformers.EncoderDecoderModel.get_input_embeddings(self)
transformers.EncoderDecoderModel.get_output_embeddings(self)
transformers.EncoderDecoderModel.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.EncoderDecoderModel.resize_token_embeddings(self,*args,**kwargs)
transformers.EncoderDecoderModel.set_output_embeddings(self,new_embeddings)
transformers.EncoderDecoderModel.tie_weights(self)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel(self,config:Optional[PretrainedConfig]=None,encoder:Optional[PreTrainedModel]=None,decoder:Optional[PreTrainedModel]=None)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.__init__(self,config:Optional[PretrainedConfig]=None,encoder:Optional[PreTrainedModel]=None,decoder:Optional[PreTrainedModel]=None)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel._reorder_cache(self,past,beam_idx)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.from_encoder_decoder_pretrained(cls,encoder_pretrained_model_name_or_path:str=None,decoder_pretrained_model_name_or_path:str=None,*model_args,**kwargs)->PreTrainedModel
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.from_pretrained(cls,*args,**kwargs)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.get_decoder(self)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.get_encoder(self)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.get_input_embeddings(self)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.get_output_embeddings(self)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.resize_token_embeddings(self,*args,**kwargs)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.set_output_embeddings(self,new_embeddings)
transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.tie_weights(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert_generation/tokenization_bert_generation.py----------------------------------------
A:transformers.models.bert_generation.tokenization_bert_generation.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert_generation.tokenization_bert_generation.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.bert_generation.tokenization_bert_generation.state->self.__dict__.copy()
A:transformers.models.bert_generation.tokenization_bert_generation.token->self.sp_model.IdToPiece(index)
A:transformers.models.bert_generation.tokenization_bert_generation.out_string->self.sp_model.decode_pieces(tokens)
A:transformers.models.bert_generation.tokenization_bert_generation.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.BertGenerationTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',sep_token='<::::>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.BertGenerationTokenizer.__getstate__(self)
transformers.BertGenerationTokenizer.__setstate__(self,d)
transformers.BertGenerationTokenizer._convert_id_to_token(self,index)
transformers.BertGenerationTokenizer._convert_token_to_id(self,token)
transformers.BertGenerationTokenizer._tokenize(self,text:str)->List[str]
transformers.BertGenerationTokenizer.convert_tokens_to_string(self,tokens)
transformers.BertGenerationTokenizer.get_vocab(self)
transformers.BertGenerationTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.BertGenerationTokenizer.vocab_size(self)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',sep_token='<::::>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.__getstate__(self)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',sep_token='<::::>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.__setstate__(self,d)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer._convert_id_to_token(self,index)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer._convert_token_to_id(self,token)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer._tokenize(self,text:str)->List[str]
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.get_vocab(self)
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.bert_generation.tokenization_bert_generation.BertGenerationTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert_generation/__init__.py----------------------------------------
A:transformers.models.bert_generation.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert_generation/modeling_bert_generation.py----------------------------------------
A:transformers.models.bert_generation.modeling_bert_generation.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert_generation.modeling_bert_generation.tf_model->tensorflow_hub.Module(tf_hub_path)
A:transformers.models.bert_generation.modeling_bert_generation.init->tensorflow.compat.v1.global_variables_initializer()
A:transformers.models.bert_generation.modeling_bert_generation.keep_track_variables->all_variables.copy()
A:transformers.models.bert_generation.modeling_bert_generation.model_pointer->getattr(model_pointer, sub_layer)
A:transformers.models.bert_generation.modeling_bert_generation.array->numpy.transpose(array)
A:transformers.models.bert_generation.modeling_bert_generation.model_pointer.data->torch.from_numpy(array.astype(np.float32))
A:transformers.models.bert_generation.modeling_bert_generation.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.bert_generation.modeling_bert_generation.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.bert_generation.modeling_bert_generation.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.bert_generation.modeling_bert_generation.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.bert_generation.modeling_bert_generation.input_shape->input_ids.size()
A:transformers.models.bert_generation.modeling_bert_generation.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.bert_generation.modeling_bert_generation.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.bert_generation.modeling_bert_generation.embeddings->self.dropout(embeddings)
A:transformers.models.bert_generation.modeling_bert_generation.self.embeddings->BertGenerationEmbeddings(config)
A:transformers.models.bert_generation.modeling_bert_generation.self.encoder->BertEncoder(config)
A:transformers.models.bert_generation.modeling_bert_generation.attention_mask->input_ids.new_ones(input_shape)
A:transformers.models.bert_generation.modeling_bert_generation.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.bert_generation.modeling_bert_generation.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.bert_generation.modeling_bert_generation.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.bert_generation.modeling_bert_generation.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.bert_generation.modeling_bert_generation.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, past_key_values_length=past_key_values_length)
A:transformers.models.bert_generation.modeling_bert_generation.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bert_generation.modeling_bert_generation.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.bert_generation.modeling_bert_generation.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.bert_generation.modeling_bert_generation.logits->self.decoder(hidden_states)
A:transformers.models.bert_generation.modeling_bert_generation.self.bert->BertGenerationEncoder(config)
A:transformers.models.bert_generation.modeling_bert_generation.self.lm_head->BertGenerationOnlyLMHead(config)
A:transformers.models.bert_generation.modeling_bert_generation.outputs->self.bert(input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bert_generation.modeling_bert_generation.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.bert_generation.modeling_bert_generation.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.models.bert_generation.modeling_bert_generation.labels->labels[:, 1:].contiguous()
A:transformers.models.bert_generation.modeling_bert_generation.loss_fct->CrossEntropyLoss()
A:transformers.models.bert_generation.modeling_bert_generation.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
transformers.BertGenerationDecoder(self,config)
transformers.BertGenerationDecoder._reorder_cache(self,past,beam_idx)
transformers.BertGenerationDecoder.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertGenerationDecoder.get_output_embeddings(self)
transformers.BertGenerationDecoder.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.BertGenerationDecoder.set_output_embeddings(self,new_embeddings)
transformers.BertGenerationEncoder(self,config)
transformers.BertGenerationEncoder._prune_heads(self,heads_to_prune)
transformers.BertGenerationEncoder.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertGenerationEncoder.get_input_embeddings(self)
transformers.BertGenerationEncoder.set_input_embeddings(self,value)
transformers.BertGenerationPreTrainedModel(PreTrainedModel)
transformers.BertGenerationPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_bert_generation(model,tf_hub_path,model_class,is_encoder_named_decoder=False,is_encoder=False)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder.__init__(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder._reorder_cache(self,past,beam_idx)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder.get_output_embeddings(self)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationDecoder.set_output_embeddings(self,new_embeddings)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEmbeddings(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEmbeddings.__init__(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEmbeddings.forward(self,input_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEncoder(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEncoder.__init__(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEncoder._prune_heads(self,heads_to_prune)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEncoder.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEncoder.get_input_embeddings(self)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationEncoder.set_input_embeddings(self,value)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationOnlyLMHead(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationOnlyLMHead.__init__(self,config)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationOnlyLMHead.forward(self,hidden_states)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationPreTrainedModel(PreTrainedModel)
transformers.models.bert_generation.modeling_bert_generation.BertGenerationPreTrainedModel._init_weights(self,module)
transformers.models.bert_generation.modeling_bert_generation.load_tf_weights_in_bert_generation(model,tf_hub_path,model_class,is_encoder_named_decoder=False,is_encoder=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert_generation/configuration_bert_generation.py----------------------------------------
transformers.BertGenerationConfig(self,vocab_size=50358,hidden_size=1024,num_hidden_layers=24,num_attention_heads=16,intermediate_size=4096,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,bos_token_id=2,eos_token_id=1,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)
transformers.models.bert_generation.configuration_bert_generation.BertGenerationConfig(self,vocab_size=50358,hidden_size=1024,num_hidden_layers=24,num_attention_heads=16,intermediate_size=4096,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,bos_token_id=2,eos_token_id=1,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)
transformers.models.bert_generation.configuration_bert_generation.BertGenerationConfig.__init__(self,vocab_size=50358,hidden_size=1024,num_hidden_layers=24,num_attention_heads=16,intermediate_size=4096,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,bos_token_id=2,eos_token_id=1,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/tokenization_bert_fast.py----------------------------------------
A:transformers.models.bert.tokenization_bert_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert.tokenization_bert_fast.pre_tok_state->json.loads(self.backend_tokenizer.normalizer.__getstate__())
A:transformers.models.bert.tokenization_bert_fast.pre_tok_class->getattr(normalizers, pre_tok_state.pop('type'))
A:transformers.models.bert.tokenization_bert_fast.self.backend_tokenizer.normalizer->pre_tok_class(**pre_tok_state)
A:transformers.models.bert.tokenization_bert_fast.files->self._tokenizer.model.save(save_directory, name=filename_prefix)
transformers.BertTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.BertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.BertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.bert.tokenization_bert_fast.BertTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/tokenization_bert.py----------------------------------------
A:transformers.models.bert.tokenization_bert.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert.tokenization_bert.vocab->collections.OrderedDict()
A:transformers.models.bert.tokenization_bert.tokens->unicodedata.normalize('NFD', text).split()
A:transformers.models.bert.tokenization_bert.token->self._run_strip_accents(token)
A:transformers.models.bert.tokenization_bert.text->unicodedata.normalize('NFD', text)
A:transformers.models.bert.tokenization_bert.self.vocab->load_vocab(vocab_file)
A:transformers.models.bert.tokenization_bert.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.models.bert.tokenization_bert.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars, strip_accents=strip_accents)
A:transformers.models.bert.tokenization_bert.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.models.bert.tokenization_bert.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.models.bert.tokenization_bert.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.models.bert.tokenization_bert.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.bert.tokenization_bert.self.never_split->set(never_split)
A:transformers.models.bert.tokenization_bert.orig_tokens->whitespace_tokenize(text)
A:transformers.models.bert.tokenization_bert.output_tokens->whitespace_tokenize(' '.join(split_tokens))
A:transformers.models.bert.tokenization_bert.cat->unicodedata.category(char)
A:transformers.models.bert.tokenization_bert.chars->list(token)
A:transformers.models.bert.tokenization_bert.cp->ord(char)
A:transformers.models.bert.tokenization_bert.end->len(chars)
A:transformers.models.bert.tokenization_bert.substr->''.join(chars[start:end])
transformers.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.BasicTokenizer._clean_text(self,text)
transformers.BasicTokenizer._is_chinese_char(self,cp)
transformers.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.BasicTokenizer._run_strip_accents(self,text)
transformers.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.BertTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.BertTokenizer._convert_id_to_token(self,index)
transformers.BertTokenizer._convert_token_to_id(self,token)
transformers.BertTokenizer._tokenize(self,text)
transformers.BertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertTokenizer.convert_tokens_to_string(self,tokens)
transformers.BertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertTokenizer.do_lower_case(self)
transformers.BertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.BertTokenizer.get_vocab(self)
transformers.BertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.BertTokenizer.vocab_size(self)
transformers.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.WordpieceTokenizer.tokenize(self,text)
transformers.models.bert.tokenization_bert.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.models.bert.tokenization_bert.BasicTokenizer.__init__(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.models.bert.tokenization_bert.BasicTokenizer._clean_text(self,text)
transformers.models.bert.tokenization_bert.BasicTokenizer._is_chinese_char(self,cp)
transformers.models.bert.tokenization_bert.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.models.bert.tokenization_bert.BasicTokenizer._run_strip_accents(self,text)
transformers.models.bert.tokenization_bert.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.models.bert.tokenization_bert.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.models.bert.tokenization_bert.BertTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.bert.tokenization_bert.BertTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.models.bert.tokenization_bert.BertTokenizer._convert_id_to_token(self,index)
transformers.models.bert.tokenization_bert.BertTokenizer._convert_token_to_id(self,token)
transformers.models.bert.tokenization_bert.BertTokenizer._tokenize(self,text)
transformers.models.bert.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.bert.tokenization_bert.BertTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.bert.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.bert.tokenization_bert.BertTokenizer.do_lower_case(self)
transformers.models.bert.tokenization_bert.BertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.bert.tokenization_bert.BertTokenizer.get_vocab(self)
transformers.models.bert.tokenization_bert.BertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.bert.tokenization_bert.BertTokenizer.vocab_size(self)
transformers.models.bert.tokenization_bert.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.models.bert.tokenization_bert.WordpieceTokenizer.__init__(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.models.bert.tokenization_bert.WordpieceTokenizer.tokenize(self,text)
transformers.models.bert.tokenization_bert.load_vocab(vocab_file)
transformers.models.bert.tokenization_bert.whitespace_tokenize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/configuration_bert.py----------------------------------------
A:transformers.models.bert.configuration_bert.logger->utils.logging.get_logger(__name__)
transformers.BertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)
transformers.models.bert.configuration_bert.BertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)
transformers.models.bert.configuration_bert.BertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/modeling_flax_bert.py----------------------------------------
A:transformers.models.bert.modeling_flax_bert.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert.modeling_flax_bert.self.word_embeddings->flax.linen.Embed(self.config.vocab_size, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.position_embeddings->flax.linen.Embed(self.config.max_position_embeddings, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.token_type_embeddings->flax.linen.Embed(self.config.type_vocab_size, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.LayerNorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.dropout->flax.linen.Dropout(rate=self.config.hidden_dropout_prob)
A:transformers.models.bert.modeling_flax_bert.inputs_embeds->self.word_embeddings(input_ids.astype('i4'))
A:transformers.models.bert.modeling_flax_bert.position_embeds->self.position_embeddings(position_ids.astype('i4'))
A:transformers.models.bert.modeling_flax_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids.astype('i4'))
A:transformers.models.bert.modeling_flax_bert.hidden_states->self.dropout(hidden_states, deterministic=deterministic)
A:transformers.models.bert.modeling_flax_bert.self.query->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.bert.modeling_flax_bert.self.key->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.bert.modeling_flax_bert.self.value->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.bert.modeling_flax_bert.query_states->self.query(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.bert.modeling_flax_bert.value_states->self.value(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.bert.modeling_flax_bert.key_states->self.key(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.bert.modeling_flax_bert.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.bert.modeling_flax_bert.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, -10000000000.0).astype(self.dtype))
A:transformers.models.bert.modeling_flax_bert.dropout_rng->self.make_rng('dropout')
A:transformers.models.bert.modeling_flax_bert.attn_weights->dot_product_attention_weights(query_states, key_states, bias=attention_bias, dropout_rng=dropout_rng, dropout_rate=self.config.attention_probs_dropout_prob, broadcast_dropout=True, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.bert.modeling_flax_bert.attn_output->attn_output.reshape(attn_output.shape[:2] + (-1,)).reshape(attn_output.shape[:2] + (-1,))
A:transformers.models.bert.modeling_flax_bert.self.dense->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.self->FlaxBertSelfAttention(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.output->FlaxBertOutput(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.attn_outputs->self.self(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.bert.modeling_flax_bert.self.attention->FlaxBertAttention(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.intermediate->FlaxBertIntermediate(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.attention_outputs->self.attention(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.bert.modeling_flax_bert.layer_outputs->layer(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.bert.modeling_flax_bert.self.layer->FlaxBertLayerCollection(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.cls_hidden_state->self.dense(cls_hidden_state)
A:transformers.models.bert.modeling_flax_bert.self.transform->FlaxBertPredictionHeadTransform(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.decoder->flax.linen.Dense(self.config.vocab_size, dtype=self.dtype, use_bias=False)
A:transformers.models.bert.modeling_flax_bert.self.bias->self.param('bias', self.bias_init, (self.config.vocab_size,))
A:transformers.models.bert.modeling_flax_bert.self.predictions->FlaxBertLMPredictionHead(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.seq_relationship->flax.linen.Dense(2, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.prediction_scores->self.predictions(hidden_states, shared_embedding=shared_embedding)
A:transformers.models.bert.modeling_flax_bert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.bert.modeling_flax_bert.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.bert.modeling_flax_bert.input_ids->jax.numpy.zeros(input_shape, dtype='i4')
A:transformers.models.bert.modeling_flax_bert.token_type_ids->jax.numpy.zeros_like(input_ids)
A:transformers.models.bert.modeling_flax_bert.position_ids->jax.numpy.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)
A:transformers.models.bert.modeling_flax_bert.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.bert.modeling_flax_bert.self.embeddings->FlaxBertEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.encoder->FlaxBertEncoder(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.self.pooler->FlaxBertPooler(self.config, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.outputs->self.bert(input_ids, attention_mask, token_type_ids, position_ids, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bert.modeling_flax_bert.self.bert->FlaxBertModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)
A:transformers.models.bert.modeling_flax_bert.self.cls->FlaxBertOnlyNSPHead(dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.(prediction_scores, seq_relationship_score)->self.cls(hidden_states, pooled_output, shared_embedding=shared_embedding)
A:transformers.models.bert.modeling_flax_bert.logits->self.qa_outputs(hidden_states)
A:transformers.models.bert.modeling_flax_bert.seq_relationship_scores->self.cls(pooled_output)
A:transformers.models.bert.modeling_flax_bert.self.classifier->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.pooled_output->self.dropout(pooled_output, deterministic=deterministic)
A:transformers.models.bert.modeling_flax_bert.reshaped_logits->self.qa_outputs(hidden_states).reshape(-1, num_choices)
A:transformers.models.bert.modeling_flax_bert.self.qa_outputs->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.bert.modeling_flax_bert.(start_logits, end_logits)->self.qa_outputs(hidden_states).split(self.config.num_labels, axis=-1)
A:transformers.models.bert.modeling_flax_bert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.models.bert.modeling_flax_bert.end_logits->end_logits.squeeze(-1).squeeze(-1)
transformers.FlaxBertForMaskedLM(FlaxBertPreTrainedModel)
transformers.FlaxBertForMaskedLMModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForMaskedLMModule.setup(self)
transformers.FlaxBertForMultipleChoice(FlaxBertPreTrainedModel)
transformers.FlaxBertForMultipleChoiceModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForMultipleChoiceModule.setup(self)
transformers.FlaxBertForNextSentencePrediction(FlaxBertPreTrainedModel)
transformers.FlaxBertForNextSentencePredictionModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForNextSentencePredictionModule.setup(self)
transformers.FlaxBertForPreTraining(FlaxBertPreTrainedModel)
transformers.FlaxBertForPreTrainingModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForPreTrainingModule.setup(self)
transformers.FlaxBertForPreTrainingOutput(ModelOutput)
transformers.FlaxBertForQuestionAnswering(FlaxBertPreTrainedModel)
transformers.FlaxBertForQuestionAnsweringModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForQuestionAnsweringModule.setup(self)
transformers.FlaxBertForSequenceClassification(FlaxBertPreTrainedModel)
transformers.FlaxBertForSequenceClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForSequenceClassificationModule.setup(self)
transformers.FlaxBertForTokenClassification(FlaxBertPreTrainedModel)
transformers.FlaxBertForTokenClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBertForTokenClassificationModule.setup(self)
transformers.FlaxBertModel(FlaxBertPreTrainedModel)
transformers.FlaxBertPreTrainedModel(self,config:BertConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxBertPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.bert.modeling_flax_bert.FlaxBertAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.bert.modeling_flax_bert.FlaxBertAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.bert.modeling_flax_bert.FlaxBertAttention.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertEmbeddings(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertEmbeddings.__call__(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertEmbeddings.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertEncoder(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertEncoder.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertEncoder.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForMaskedLM(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForMaskedLMModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForMaskedLMModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForMaskedLMModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForMultipleChoice(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForMultipleChoiceModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForMultipleChoiceModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForMultipleChoiceModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForNextSentencePrediction(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForNextSentencePredictionModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForNextSentencePredictionModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForNextSentencePredictionModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForPreTraining(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput(ModelOutput)
transformers.models.bert.modeling_flax_bert.FlaxBertForQuestionAnswering(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForQuestionAnsweringModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForQuestionAnsweringModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForQuestionAnsweringModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForSequenceClassification(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForSequenceClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForSequenceClassificationModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForSequenceClassificationModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertForTokenClassification(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertForTokenClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForTokenClassificationModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertForTokenClassificationModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertIntermediate(self,hidden_states)
transformers.models.bert.modeling_flax_bert.FlaxBertIntermediate.__call__(self,hidden_states)
transformers.models.bert.modeling_flax_bert.FlaxBertIntermediate.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertLMPredictionHead(self,hidden_states,shared_embedding=None)
transformers.models.bert.modeling_flax_bert.FlaxBertLMPredictionHead.__call__(self,hidden_states,shared_embedding=None)
transformers.models.bert.modeling_flax_bert.FlaxBertLMPredictionHead.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertLayer(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.bert.modeling_flax_bert.FlaxBertLayer.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.bert.modeling_flax_bert.FlaxBertLayer.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertLayerCollection(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertLayerCollection.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertLayerCollection.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertModel(FlaxBertPreTrainedModel)
transformers.models.bert.modeling_flax_bert.FlaxBertModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertModule.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertOnlyMLMHead(self,hidden_states,shared_embedding=None)
transformers.models.bert.modeling_flax_bert.FlaxBertOnlyMLMHead.__call__(self,hidden_states,shared_embedding=None)
transformers.models.bert.modeling_flax_bert.FlaxBertOnlyMLMHead.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertOnlyNSPHead(self,pooled_output)
transformers.models.bert.modeling_flax_bert.FlaxBertOnlyNSPHead.__call__(self,pooled_output)
transformers.models.bert.modeling_flax_bert.FlaxBertOnlyNSPHead.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertOutput(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertOutput.__call__(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertOutput.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertPooler(self,hidden_states)
transformers.models.bert.modeling_flax_bert.FlaxBertPooler.__call__(self,hidden_states)
transformers.models.bert.modeling_flax_bert.FlaxBertPooler.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainedModel(self,config:BertConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainedModel.__init__(self,config:BertConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainingHeads(self,hidden_states,pooled_output,shared_embedding=None)
transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainingHeads.__call__(self,hidden_states,pooled_output,shared_embedding=None)
transformers.models.bert.modeling_flax_bert.FlaxBertPreTrainingHeads.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertPredictionHeadTransform(self,hidden_states)
transformers.models.bert.modeling_flax_bert.FlaxBertPredictionHeadTransform.__call__(self,hidden_states)
transformers.models.bert.modeling_flax_bert.FlaxBertPredictionHeadTransform.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertSelfAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.bert.modeling_flax_bert.FlaxBertSelfAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.bert.modeling_flax_bert.FlaxBertSelfAttention.setup(self)
transformers.models.bert.modeling_flax_bert.FlaxBertSelfOutput(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertSelfOutput.__call__(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.bert.modeling_flax_bert.FlaxBertSelfOutput.setup(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.bert.convert_bert_original_tf_checkpoint_to_pytorch.config->transformers.BertConfig.from_json_file(bert_config_file)
A:transformers.models.bert.convert_bert_original_tf_checkpoint_to_pytorch.model->BertForPreTraining(config)
A:transformers.models.bert.convert_bert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.bert.convert_bert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.bert.convert_bert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py----------------------------------------
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.state_dict->transformers.BertModel.from_pretrained(pretrained_model_name_or_path=args.model_name, state_dict=torch.load(args.pytorch_model_path), cache_dir=args.cache_dir).state_dict()
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.name->name.replace(patt, repl).replace(patt, repl)
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.tf_dtype->tensorflow.dtypes.as_dtype(tensor.dtype)
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.tf_var->create_tf_var(tensor=torch_tensor, name=tf_name, session=session)
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.tf_name->to_tf_var_name(var_name)
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.torch_tensor->state_dict[var_name].numpy()
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.tf_weight->session.run(tf_var)
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.saver->tensorflow.train.Saver(tf.trainable_variables())
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.parser->argparse.ArgumentParser()
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.args->argparse.ArgumentParser().parse_args(raw_args)
A:transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.model->transformers.BertModel.from_pretrained(pretrained_model_name_or_path=args.model_name, state_dict=torch.load(args.pytorch_model_path), cache_dir=args.cache_dir)
transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.convert_pytorch_checkpoint_to_tf(model:BertModel,ckpt_dir:str,model_name:str)
transformers.models.bert.convert_bert_pytorch_checkpoint_to_original_tf.main(raw_args=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/modeling_bert.py----------------------------------------
A:transformers.models.bert.modeling_bert.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert.modeling_bert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.bert.modeling_bert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.bert.modeling_bert.array->numpy.transpose(array)
A:transformers.models.bert.modeling_bert.name->name.split('/').split('/')
A:transformers.models.bert.modeling_bert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.bert.modeling_bert.pointer->getattr(pointer, 'weight')
A:transformers.models.bert.modeling_bert.num->int(scope_names[1])
A:transformers.models.bert.modeling_bert.pointer.data->torch.from_numpy(array)
A:transformers.models.bert.modeling_bert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.bert.modeling_bert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.bert.modeling_bert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.bert.modeling_bert.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.bert.modeling_bert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.bert.modeling_bert.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.bert.modeling_bert.input_shape->torch.cat([input_ids, dummy_token], dim=1).size()
A:transformers.models.bert.modeling_bert.buffered_token_type_ids_expanded->buffered_token_type_ids.expand(batch_size, seq_length)
A:transformers.models.bert.modeling_bert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.bert.modeling_bert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.bert.modeling_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.bert.modeling_bert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.bert.modeling_bert.embeddings->self.dropout(embeddings)
A:transformers.models.bert.modeling_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.bert.modeling_bert.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.bert.modeling_bert.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.bert.modeling_bert.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.bert.modeling_bert.self.distance_embedding->torch.nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
A:transformers.models.bert.modeling_bert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.bert.modeling_bert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.bert.modeling_bert.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.bert.modeling_bert.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.bert.modeling_bert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.bert.modeling_bert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.bert.modeling_bert.position_ids_l->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
A:transformers.models.bert.modeling_bert.position_ids_r->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
A:transformers.models.bert.modeling_bert.positional_embedding->positional_embedding.to(dtype=query_layer.dtype).to(dtype=query_layer.dtype)
A:transformers.models.bert.modeling_bert.relative_position_scores->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.bert.modeling_bert.relative_position_scores_query->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.bert.modeling_bert.relative_position_scores_key->torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
A:transformers.models.bert.modeling_bert.attention_probs->self.dropout(attention_probs)
A:transformers.models.bert.modeling_bert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.bert.modeling_bert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.bert.modeling_bert.hidden_states->self.decoder(hidden_states)
A:transformers.models.bert.modeling_bert.self.self->BertSelfAttention(config)
A:transformers.models.bert.modeling_bert.self.output->BertOutput(config)
A:transformers.models.bert.modeling_bert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.bert.modeling_bert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.bert.modeling_bert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.bert.modeling_bert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.bert.modeling_bert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.bert.modeling_bert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.bert.modeling_bert.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.bert.modeling_bert.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.bert.modeling_bert.self.attention->BertAttention(config)
A:transformers.models.bert.modeling_bert.self.crossattention->BertAttention(config)
A:transformers.models.bert.modeling_bert.self.intermediate->BertIntermediate(config)
A:transformers.models.bert.modeling_bert.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.bert.modeling_bert.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.bert.modeling_bert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.bert.modeling_bert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.bert.modeling_bert.self.layer->torch.nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.bert.modeling_bert.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.bert.modeling_bert.self.activation->torch.nn.Tanh()
A:transformers.models.bert.modeling_bert.pooled_output->self.dropout(pooled_output)
A:transformers.models.bert.modeling_bert.self.transform->BertPredictionHeadTransform(config)
A:transformers.models.bert.modeling_bert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.bert.modeling_bert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.bert.modeling_bert.self.predictions->BertLMPredictionHead(config)
A:transformers.models.bert.modeling_bert.prediction_scores->self.cls(sequence_output)
A:transformers.models.bert.modeling_bert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.models.bert.modeling_bert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.bert.modeling_bert.self.embeddings->BertEmbeddings(config)
A:transformers.models.bert.modeling_bert.self.encoder->BertEncoder(config)
A:transformers.models.bert.modeling_bert.attention_mask->torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)
A:transformers.models.bert.modeling_bert.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.bert.modeling_bert.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.bert.modeling_bert.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.bert.modeling_bert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.bert.modeling_bert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, past_key_values_length=past_key_values_length)
A:transformers.models.bert.modeling_bert.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bert.modeling_bert.self.bert->BertModel(config, add_pooling_layer=False)
A:transformers.models.bert.modeling_bert.self.cls->BertOnlyNSPHead(config)
A:transformers.models.bert.modeling_bert.outputs->self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bert.modeling_bert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.models.bert.modeling_bert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.bert.modeling_bert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.bert.modeling_bert.next_sentence_loss->loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))
A:transformers.models.bert.modeling_bert.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.models.bert.modeling_bert.labels->kwargs.pop('next_sentence_label')
A:transformers.models.bert.modeling_bert.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.bert.modeling_bert.dummy_token->torch.full((effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.models.bert.modeling_bert.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.models.bert.modeling_bert.seq_relationship_scores->self.cls(pooled_output)
A:transformers.models.bert.modeling_bert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.bert.modeling_bert.logits->self.qa_outputs(sequence_output)
A:transformers.models.bert.modeling_bert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.bert.modeling_bert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.bert.modeling_bert.sequence_output->self.dropout(sequence_output)
A:transformers.models.bert.modeling_bert.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.bert.modeling_bert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.bert.modeling_bert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.bert.modeling_bert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.bert.modeling_bert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.bert.modeling_bert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.bert.modeling_bert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.bert.modeling_bert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.bert.modeling_bert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.bert.modeling_bert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.bert.modeling_bert.end_loss->loss_fct(end_logits, end_positions)
transformers.BertForMaskedLM(self,config)
transformers.BertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForMaskedLM.get_output_embeddings(self)
transformers.BertForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.BertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.BertForMultipleChoice(self,config)
transformers.BertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForNextSentencePrediction(self,config)
transformers.BertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.BertForPreTraining(self,config)
transformers.BertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForPreTraining.get_output_embeddings(self)
transformers.BertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.BertForPreTrainingOutput(ModelOutput)
transformers.BertForQuestionAnswering(self,config)
transformers.BertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForSequenceClassification(self,config)
transformers.BertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForTokenClassification(self,config)
transformers.BertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertLMHeadModel(self,config)
transformers.BertLMHeadModel._reorder_cache(self,past,beam_idx)
transformers.BertLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertLMHeadModel.get_output_embeddings(self)
transformers.BertLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.BertLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.BertLayer(self,config)
transformers.BertLayer.feed_forward_chunk(self,attention_output)
transformers.BertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.BertModel(self,config,add_pooling_layer=True)
transformers.BertModel._prune_heads(self,heads_to_prune)
transformers.BertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertModel.get_input_embeddings(self)
transformers.BertModel.set_input_embeddings(self,value)
transformers.BertPreTrainedModel(PreTrainedModel)
transformers.BertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_bert(model,config,tf_checkpoint_path)
transformers.models.bert.modeling_bert.BertAttention(self,config)
transformers.models.bert.modeling_bert.BertAttention.__init__(self,config)
transformers.models.bert.modeling_bert.BertAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.bert.modeling_bert.BertAttention.prune_heads(self,heads)
transformers.models.bert.modeling_bert.BertEmbeddings(self,config)
transformers.models.bert.modeling_bert.BertEmbeddings.__init__(self,config)
transformers.models.bert.modeling_bert.BertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.bert.modeling_bert.BertEncoder(self,config)
transformers.models.bert.modeling_bert.BertEncoder.__init__(self,config)
transformers.models.bert.modeling_bert.BertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.bert.modeling_bert.BertForMaskedLM(self,config)
transformers.models.bert.modeling_bert.BertForMaskedLM.__init__(self,config)
transformers.models.bert.modeling_bert.BertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertForMaskedLM.get_output_embeddings(self)
transformers.models.bert.modeling_bert.BertForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.models.bert.modeling_bert.BertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.bert.modeling_bert.BertForMultipleChoice(self,config)
transformers.models.bert.modeling_bert.BertForMultipleChoice.__init__(self,config)
transformers.models.bert.modeling_bert.BertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertForNextSentencePrediction(self,config)
transformers.models.bert.modeling_bert.BertForNextSentencePrediction.__init__(self,config)
transformers.models.bert.modeling_bert.BertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.bert.modeling_bert.BertForPreTraining(self,config)
transformers.models.bert.modeling_bert.BertForPreTraining.__init__(self,config)
transformers.models.bert.modeling_bert.BertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertForPreTraining.get_output_embeddings(self)
transformers.models.bert.modeling_bert.BertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.models.bert.modeling_bert.BertForPreTrainingOutput(ModelOutput)
transformers.models.bert.modeling_bert.BertForQuestionAnswering(self,config)
transformers.models.bert.modeling_bert.BertForQuestionAnswering.__init__(self,config)
transformers.models.bert.modeling_bert.BertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertForSequenceClassification(self,config)
transformers.models.bert.modeling_bert.BertForSequenceClassification.__init__(self,config)
transformers.models.bert.modeling_bert.BertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertForTokenClassification(self,config)
transformers.models.bert.modeling_bert.BertForTokenClassification.__init__(self,config)
transformers.models.bert.modeling_bert.BertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertIntermediate(self,config)
transformers.models.bert.modeling_bert.BertIntermediate.__init__(self,config)
transformers.models.bert.modeling_bert.BertIntermediate.forward(self,hidden_states)
transformers.models.bert.modeling_bert.BertLMHeadModel(self,config)
transformers.models.bert.modeling_bert.BertLMHeadModel.__init__(self,config)
transformers.models.bert.modeling_bert.BertLMHeadModel._reorder_cache(self,past,beam_idx)
transformers.models.bert.modeling_bert.BertLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertLMHeadModel.get_output_embeddings(self)
transformers.models.bert.modeling_bert.BertLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.models.bert.modeling_bert.BertLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.bert.modeling_bert.BertLMPredictionHead(self,config)
transformers.models.bert.modeling_bert.BertLMPredictionHead.__init__(self,config)
transformers.models.bert.modeling_bert.BertLMPredictionHead.forward(self,hidden_states)
transformers.models.bert.modeling_bert.BertLayer(self,config)
transformers.models.bert.modeling_bert.BertLayer.__init__(self,config)
transformers.models.bert.modeling_bert.BertLayer.feed_forward_chunk(self,attention_output)
transformers.models.bert.modeling_bert.BertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.bert.modeling_bert.BertModel(self,config,add_pooling_layer=True)
transformers.models.bert.modeling_bert.BertModel.__init__(self,config,add_pooling_layer=True)
transformers.models.bert.modeling_bert.BertModel._prune_heads(self,heads_to_prune)
transformers.models.bert.modeling_bert.BertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bert.modeling_bert.BertModel.get_input_embeddings(self)
transformers.models.bert.modeling_bert.BertModel.set_input_embeddings(self,value)
transformers.models.bert.modeling_bert.BertOnlyMLMHead(self,config)
transformers.models.bert.modeling_bert.BertOnlyMLMHead.__init__(self,config)
transformers.models.bert.modeling_bert.BertOnlyMLMHead.forward(self,sequence_output)
transformers.models.bert.modeling_bert.BertOnlyNSPHead(self,config)
transformers.models.bert.modeling_bert.BertOnlyNSPHead.__init__(self,config)
transformers.models.bert.modeling_bert.BertOnlyNSPHead.forward(self,pooled_output)
transformers.models.bert.modeling_bert.BertOutput(self,config)
transformers.models.bert.modeling_bert.BertOutput.__init__(self,config)
transformers.models.bert.modeling_bert.BertOutput.forward(self,hidden_states,input_tensor)
transformers.models.bert.modeling_bert.BertPooler(self,config)
transformers.models.bert.modeling_bert.BertPooler.__init__(self,config)
transformers.models.bert.modeling_bert.BertPooler.forward(self,hidden_states)
transformers.models.bert.modeling_bert.BertPreTrainedModel(PreTrainedModel)
transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights(self,module)
transformers.models.bert.modeling_bert.BertPreTrainingHeads(self,config)
transformers.models.bert.modeling_bert.BertPreTrainingHeads.__init__(self,config)
transformers.models.bert.modeling_bert.BertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.models.bert.modeling_bert.BertPredictionHeadTransform(self,config)
transformers.models.bert.modeling_bert.BertPredictionHeadTransform.__init__(self,config)
transformers.models.bert.modeling_bert.BertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.bert.modeling_bert.BertSelfAttention(self,config)
transformers.models.bert.modeling_bert.BertSelfAttention.__init__(self,config)
transformers.models.bert.modeling_bert.BertSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.bert.modeling_bert.BertSelfAttention.transpose_for_scores(self,x)
transformers.models.bert.modeling_bert.BertSelfOutput(self,config)
transformers.models.bert.modeling_bert.BertSelfOutput.__init__(self,config)
transformers.models.bert.modeling_bert.BertSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.bert.modeling_bert.load_tf_weights_in_bert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/__init__.py----------------------------------------
A:transformers.models.bert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/modeling_tf_bert.py----------------------------------------
A:transformers.models.bert.modeling_tf_bert.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert.modeling_tf_bert.loss_fn->tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
A:transformers.models.bert.modeling_tf_bert.masked_lm_active_loss->tensorflow.not_equal(tf.reshape(tensor=labels['labels'], shape=(-1,)), -100)
A:transformers.models.bert.modeling_tf_bert.masked_lm_reduced_logits->tensorflow.boolean_mask(tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])), mask=masked_lm_active_loss)
A:transformers.models.bert.modeling_tf_bert.masked_lm_labels->tensorflow.boolean_mask(tensor=tf.reshape(tensor=labels['labels'], shape=(-1,)), mask=masked_lm_active_loss)
A:transformers.models.bert.modeling_tf_bert.next_sentence_active_loss->tensorflow.not_equal(tf.reshape(tensor=labels['next_sentence_label'], shape=(-1,)), -100)
A:transformers.models.bert.modeling_tf_bert.next_sentence_reduced_logits->tensorflow.boolean_mask(tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss)
A:transformers.models.bert.modeling_tf_bert.next_sentence_label->tensorflow.boolean_mask(tensor=tf.reshape(tensor=labels['next_sentence_label'], shape=(-1,)), mask=next_sentence_active_loss)
A:transformers.models.bert.modeling_tf_bert.masked_lm_loss->tensorflow.reduce_mean(input_tensor=masked_lm_loss, axis=0)
A:transformers.models.bert.modeling_tf_bert.next_sentence_loss->loss_fn(y_true=next_sentence_label, y_pred=next_sentence_reduced_logits)
A:transformers.models.bert.modeling_tf_bert.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.bert.modeling_tf_bert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.bert.modeling_tf_bert.self.dropout->tensorflow.keras.layers.Dropout(rate=config.hidden_dropout_prob)
A:transformers.models.bert.modeling_tf_bert.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.bert.modeling_tf_bert.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.bert.modeling_tf_bert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.bert.modeling_tf_bert.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.bert.modeling_tf_bert.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.bert.modeling_tf_bert.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.bert.modeling_tf_bert.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.bert.modeling_tf_bert.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.bert.modeling_tf_bert.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.bert.modeling_tf_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.bert.modeling_tf_bert.self.sqrt_att_head_size->math.sqrt(self.attention_head_size)
A:transformers.models.bert.modeling_tf_bert.self.query->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.bert.modeling_tf_bert.self.key->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.bert.modeling_tf_bert.self.value->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.bert.modeling_tf_bert.tensor->tensorflow.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.bert.modeling_tf_bert.mixed_query_layer->self.query(inputs=hidden_states)
A:transformers.models.bert.modeling_tf_bert.mixed_key_layer->self.key(inputs=hidden_states)
A:transformers.models.bert.modeling_tf_bert.mixed_value_layer->self.value(inputs=hidden_states)
A:transformers.models.bert.modeling_tf_bert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.bert.modeling_tf_bert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.bert.modeling_tf_bert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.bert.modeling_tf_bert.attention_scores->tensorflow.add(attention_scores, attention_mask)
A:transformers.models.bert.modeling_tf_bert.dk->tensorflow.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
A:transformers.models.bert.modeling_tf_bert.attention_probs->tensorflow.multiply(attention_probs, head_mask)
A:transformers.models.bert.modeling_tf_bert.attention_output->self.dense_output(hidden_states=self_outputs[0], input_tensor=input_tensor, training=training)
A:transformers.models.bert.modeling_tf_bert.self.dense->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.models.bert.modeling_tf_bert.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.bias)
A:transformers.models.bert.modeling_tf_bert.self.self_attention->TFBertSelfAttention(config, name='self')
A:transformers.models.bert.modeling_tf_bert.self.dense_output->TFBertSelfOutput(config, name='output')
A:transformers.models.bert.modeling_tf_bert.self_outputs->self.self_attention(hidden_states=input_tensor, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.bert.modeling_tf_bert.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.bert.modeling_tf_bert.self.attention->TFBertAttention(config, name='attention')
A:transformers.models.bert.modeling_tf_bert.self.intermediate->TFBertIntermediate(config, name='intermediate')
A:transformers.models.bert.modeling_tf_bert.self.bert_output->TFBertOutput(config, name='output')
A:transformers.models.bert.modeling_tf_bert.attention_outputs->self.attention(input_tensor=hidden_states, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.bert.modeling_tf_bert.intermediate_output->self.intermediate(hidden_states=attention_output)
A:transformers.models.bert.modeling_tf_bert.layer_output->self.bert_output(hidden_states=intermediate_output, input_tensor=attention_output, training=training)
A:transformers.models.bert.modeling_tf_bert.layer_outputs->layer_module(hidden_states=hidden_states, attention_mask=attention_mask, head_mask=head_mask[i], output_attentions=output_attentions, training=training)
A:transformers.models.bert.modeling_tf_bert.pooled_output->self.dropout(inputs=pooled_output, training=inputs['training'])
A:transformers.models.bert.modeling_tf_bert.self.transform_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.bert.modeling_tf_bert.self.transform->TFBertPredictionHeadTransform(config, name='transform')
A:transformers.models.bert.modeling_tf_bert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.bert.modeling_tf_bert.self.predictions->TFBertLMPredictionHead(config, input_embeddings, name='predictions')
A:transformers.models.bert.modeling_tf_bert.prediction_scores->self.mlm(sequence_output=sequence_output, training=inputs['training'])
A:transformers.models.bert.modeling_tf_bert.self.seq_relationship->tensorflow.keras.layers.Dense(units=2, kernel_initializer=get_initializer(config.initializer_range), name='seq_relationship')
A:transformers.models.bert.modeling_tf_bert.seq_relationship_score->self.nsp(pooled_output=pooled_output)
A:transformers.models.bert.modeling_tf_bert.self.embeddings->TFBertEmbeddings(config, name='embeddings')
A:transformers.models.bert.modeling_tf_bert.self.encoder->TFBertEncoder(config, name='encoder')
A:transformers.models.bert.modeling_tf_bert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.bert.modeling_tf_bert.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.bert.modeling_tf_bert.inputs['attention_mask']->tensorflow.fill(dims=input_shape, value=1)
A:transformers.models.bert.modeling_tf_bert.inputs['token_type_ids']->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.bert.modeling_tf_bert.embedding_output->self.embeddings(input_ids=inputs['input_ids'], position_ids=inputs['position_ids'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.bert.modeling_tf_bert.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.bert.modeling_tf_bert.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.bert.modeling_tf_bert.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.bert.modeling_tf_bert.encoder_outputs->self.encoder(hidden_states=embedding_output, attention_mask=extended_attention_mask, head_mask=inputs['head_mask'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.bert.modeling_tf_bert.self.bert->TFBertMainLayer(config, add_pooling_layer=False, name='bert')
A:transformers.models.bert.modeling_tf_bert.outputs->self.bert(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.bert.modeling_tf_bert.self.nsp->TFBertNSPHead(config, name='nsp___cls')
A:transformers.models.bert.modeling_tf_bert.self.mlm->TFBertMLMHead(config, input_embeddings=self.bert.embeddings, name='mlm___cls')
A:transformers.models.bert.modeling_tf_bert.total_loss->self.compute_loss(labels=d_labels, logits=(prediction_scores, seq_relationship_score))
A:transformers.models.bert.modeling_tf_bert.logits->self.qa_outputs(inputs=sequence_output)
A:transformers.models.bert.modeling_tf_bert.loss->self.compute_loss(labels=labels, logits=(start_logits, end_logits))
A:transformers.models.bert.modeling_tf_bert.seq_relationship_scores->self.nsp(pooled_output=pooled_output)
A:transformers.models.bert.modeling_tf_bert.self.classifier->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.bert.modeling_tf_bert.reshaped_logits->tensorflow.reshape(tensor=logits, shape=(-1, num_choices))
A:transformers.models.bert.modeling_tf_bert.output->self.call(input_ids=inputs)
A:transformers.models.bert.modeling_tf_bert.sequence_output->self.dropout(inputs=sequence_output, training=inputs['training'])
A:transformers.models.bert.modeling_tf_bert.self.qa_outputs->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.bert.modeling_tf_bert.(start_logits, end_logits)->tensorflow.split(value=logits, num_or_size_splits=2, axis=-1)
A:transformers.models.bert.modeling_tf_bert.start_logits->tensorflow.squeeze(input=start_logits, axis=-1)
A:transformers.models.bert.modeling_tf_bert.end_logits->tensorflow.squeeze(input=end_logits, axis=-1)
transformers.TFBertEmbeddings(self,config:BertConfig,**kwargs)
transformers.TFBertEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.TFBertEmbeddings.call(self,input_ids:tf.Tensor=None,position_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.TFBertForMaskedLM(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.TFBertForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFBertForMaskedLM.get_prefix_bias_name(self)->str
transformers.TFBertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFBertForMultipleChoice(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForMultipleChoice.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]
transformers.TFBertForMultipleChoice.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.TFBertForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])->TFMultipleChoiceModelOutput
transformers.TFBertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFBertForNextSentencePrediction(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForNextSentencePrediction.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,next_sentence_label:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFNextSentencePredictorOutput, Tuple[tf.Tensor]]
transformers.TFBertForNextSentencePrediction.serving_output(self,output:TFNextSentencePredictorOutput)->TFNextSentencePredictorOutput
transformers.TFBertForPreTraining(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForPreTraining.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,next_sentence_label:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFBertForPreTrainingOutput, Tuple[tf.Tensor]]
transformers.TFBertForPreTraining.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFBertForPreTraining.get_prefix_bias_name(self)->str
transformers.TFBertForPreTraining.serving_output(self,output:TFBertForPreTrainingOutput)->TFBertForPreTrainingOutput
transformers.TFBertForPreTrainingOutput(ModelOutput)
transformers.TFBertForQuestionAnswering(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForQuestionAnswering.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,start_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,end_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]
transformers.TFBertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFBertForSequenceClassification(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.TFBertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFBertForTokenClassification(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.TFBertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFBertLMHeadModel(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertLMHeadModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFCausalLMOutput, Tuple[tf.Tensor]]
transformers.TFBertLMHeadModel.get_lm_head(self)->tf.keras.layers.Layer
transformers.TFBertLMHeadModel.get_prefix_bias_name(self)->str
transformers.TFBertLMHeadModel.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.TFBertMainLayer(self,config:BertConfig,add_pooling_layer:bool=True,**kwargs)
transformers.TFBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFBertMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.TFBertMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.TFBertMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.TFBertModel(self,config:BertConfig,*inputs,**kwargs)
transformers.TFBertModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.TFBertModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.TFBertPreTrainedModel(TFPreTrainedModel)
transformers.models.bert.modeling_tf_bert.TFBertAttention(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertAttention.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertAttention.call(self,input_tensor:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.bert.modeling_tf_bert.TFBertAttention.prune_heads(self,heads)
transformers.models.bert.modeling_tf_bert.TFBertEmbeddings(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call(self,input_ids:tf.Tensor=None,position_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertEncoder(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertEncoder.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertEncoder.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,return_dict:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.get_prefix_bias_name(self)->str
transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.serving(self,inputs:Dict[str,tf.Tensor])->TFMultipleChoiceModelOutput
transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.bert.modeling_tf_bert.TFBertForNextSentencePrediction(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForNextSentencePrediction.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForNextSentencePrediction.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,next_sentence_label:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFNextSentencePredictorOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForNextSentencePrediction.serving_output(self,output:TFNextSentencePredictorOutput)->TFNextSentencePredictorOutput
transformers.models.bert.modeling_tf_bert.TFBertForPreTraining(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForPreTraining.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForPreTraining.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,next_sentence_label:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFBertForPreTrainingOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForPreTraining.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.bert.modeling_tf_bert.TFBertForPreTraining.get_prefix_bias_name(self)->str
transformers.models.bert.modeling_tf_bert.TFBertForPreTraining.serving_output(self,output:TFBertForPreTrainingOutput)->TFBertForPreTrainingOutput
transformers.models.bert.modeling_tf_bert.TFBertForPreTrainingOutput(ModelOutput)
transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,start_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,end_positions:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.bert.modeling_tf_bert.TFBertIntermediate(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertIntermediate.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertLMHeadModel(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertLMHeadModel.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertLMHeadModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFCausalLMOutput, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertLMHeadModel.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.bert.modeling_tf_bert.TFBertLMHeadModel.get_prefix_bias_name(self)->str
transformers.models.bert.modeling_tf_bert.TFBertLMHeadModel.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead(self,config:BertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.__init__(self,config:BertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.build(self,input_shape:tf.TensorShape)
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.get_bias(self)->Dict[str, tf.Variable]
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.set_bias(self,value:tf.Variable)
transformers.models.bert.modeling_tf_bert.TFBertLMPredictionHead.set_output_embeddings(self,value:tf.Variable)
transformers.models.bert.modeling_tf_bert.TFBertLayer(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertLayer.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.bert.modeling_tf_bert.TFBertMLMHead(self,config:BertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertMLMHead.__init__(self,config:BertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertMLMHead.call(self,sequence_output:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertMainLayer(self,config:BertConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertMainLayer.__init__(self,config:BertConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertMainLayer._prune_heads(self,heads_to_prune)
transformers.models.bert.modeling_tf_bert.TFBertMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.bert.modeling_tf_bert.TFBertMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.bert.modeling_tf_bert.TFBertModel(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertModel.__init__(self,config:BertConfig,*inputs,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertModel.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.bert.modeling_tf_bert.TFBertModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.bert.modeling_tf_bert.TFBertNSPHead(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertNSPHead.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertNSPHead.call(self,pooled_output:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertOutput(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertOutput.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertPooler(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertPooler.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertPooler.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertPreTrainedModel(TFPreTrainedModel)
transformers.models.bert.modeling_tf_bert.TFBertPreTrainingLoss
transformers.models.bert.modeling_tf_bert.TFBertPreTrainingLoss.compute_loss(self,labels:tf.Tensor,logits:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertPredictionHeadTransform(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertPredictionHeadTransform.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertPredictionHeadTransform.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertSelfAttention(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertSelfAttention.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertSelfAttention.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.bert.modeling_tf_bert.TFBertSelfAttention.transpose_for_scores(self,tensor:tf.Tensor,batch_size:int)->tf.Tensor
transformers.models.bert.modeling_tf_bert.TFBertSelfOutput(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertSelfOutput.__init__(self,config:BertConfig,**kwargs)
transformers.models.bert.modeling_tf_bert.TFBertSelfOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.name->full_name.split('/')
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.array->array.transpose().transpose()
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.layer_num->int(m_name.split('-')[-1])
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.pointer->getattr(pointer, 'weight')
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.trace->'.'.join(trace)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.pointer.data->torch.from_numpy(array)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.config->transformers.BertConfig.from_json_file(config_path)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.model->BertModel(config)
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.convert_tf2_checkpoint_to_pytorch(tf_checkpoint_path,config_path,pytorch_dump_path)
transformers.models.bert.convert_bert_original_tf2_checkpoint_to_pytorch.load_tf2_weights_in_bert(model,tf_checkpoint_path,config)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/configuration_speech_to_text.py----------------------------------------
A:transformers.models.speech_to_text.configuration_speech_to_text.logger->utils.logging.get_logger(__name__)
A:transformers.models.speech_to_text.configuration_speech_to_text.self.conv_kernel_sizes->list(conv_kernel_sizes)
transformers.Speech2TextConfig(self,vocab_size=10000,encoder_layers=12,encoder_ffn_dim=2048,encoder_attention_heads=4,decoder_layers=6,decoder_ffn_dim=2048,decoder_attention_heads=4,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='relu',d_model=256,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,scale_embedding=True,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,max_source_positions=6000,max_target_positions=1024,num_conv_layers=2,conv_kernel_sizes=(5,5),conv_channels=1024,input_feat_per_channel=80,input_channels=1,**kwargs)
transformers.Speech2TextConfig.hidden_size(self)->int
transformers.Speech2TextConfig.num_attention_heads(self)->int
transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig(self,vocab_size=10000,encoder_layers=12,encoder_ffn_dim=2048,encoder_attention_heads=4,decoder_layers=6,decoder_ffn_dim=2048,decoder_attention_heads=4,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='relu',d_model=256,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,scale_embedding=True,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,max_source_positions=6000,max_target_positions=1024,num_conv_layers=2,conv_kernel_sizes=(5,5),conv_channels=1024,input_feat_per_channel=80,input_channels=1,**kwargs)
transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig.__init__(self,vocab_size=10000,encoder_layers=12,encoder_ffn_dim=2048,encoder_attention_heads=4,decoder_layers=6,decoder_ffn_dim=2048,decoder_attention_heads=4,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='relu',d_model=256,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,classifier_dropout=0.0,scale_embedding=True,gradient_checkpointing=False,pad_token_id=1,bos_token_id=0,eos_token_id=2,max_source_positions=6000,max_target_positions=1024,num_conv_layers=2,conv_kernel_sizes=(5,5),conv_channels=1024,input_feat_per_channel=80,input_channels=1,**kwargs)
transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig.hidden_size(self)->int
transformers.models.speech_to_text.configuration_speech_to_text.Speech2TextConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/convert_s2t_fairseq_to_tfms.py----------------------------------------
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.keys->list(s_dict.keys())
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.s_dict[key.replace('transformer_layers', 'layers')]->s_dict.pop(key)
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.s_dict[key.replace('subsample', 'conv')]->s_dict.pop(key)
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.lin_layer->torch.nn.Linear(vocab_size, emb_size, bias=False)
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.m2m_100->torch.load(checkpoint_path, map_location='cpu')
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.config->Speech2TextConfig(vocab_size=vocab_size, max_source_positions=args.max_source_positions, max_target_positions=args.max_target_positions, encoder_layers=args.encoder_layers, decoder_layers=args.decoder_layers, encoder_attention_heads=args.encoder_attention_heads, decoder_attention_heads=args.decoder_attention_heads, encoder_ffn_dim=args.encoder_ffn_embed_dim, decoder_ffn_dim=args.decoder_ffn_embed_dim, d_model=args.encoder_embed_dim, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, activation_function='relu', num_conv_layers=len(conv_kernel_sizes), conv_channels=args.conv_channels, conv_kernel_sizes=conv_kernel_sizes, input_feat_per_channel=args.input_feat_per_channel, input_channels=args.input_channels, tie_word_embeddings=tie_embeds, num_beams=5, max_length=200, use_cache=True, decoder_start_token_id=2, early_stopping=True)
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.model->Speech2TextForConditionalGeneration(config)
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.model.lm_head->make_linear_from_emb(model.model.decoder.embed_tokens)
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.parser->argparse.ArgumentParser()
A:transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.args->argparse.ArgumentParser().parse_args()
transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.convert_fairseq_s2t_checkpoint_to_tfms(checkpoint_path,pytorch_dump_folder_path)
transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.make_linear_from_emb(emb)
transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.remove_ignore_keys_(state_dict)
transformers.models.speech_to_text.convert_s2t_fairseq_to_tfms.rename_keys(s_dict)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/processing_speech_to_text.py----------------------------------------
A:transformers.models.speech_to_text.processing_speech_to_text.feature_extractor->feature_extraction_speech_to_text.Speech2TextFeatureExtractor.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.models.speech_to_text.processing_speech_to_text.tokenizer->tokenization_speech_to_text.Speech2TextTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)
transformers.Speech2TextProcessor(self,feature_extractor,tokenizer)
transformers.Speech2TextProcessor.as_target_processor(self)
transformers.Speech2TextProcessor.batch_decode(self,*args,**kwargs)
transformers.Speech2TextProcessor.decode(self,*args,**kwargs)
transformers.Speech2TextProcessor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.Speech2TextProcessor.save_pretrained(self,save_directory)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor(self,feature_extractor,tokenizer)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor.__init__(self,feature_extractor,tokenizer)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor.as_target_processor(self)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor.batch_decode(self,*args,**kwargs)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor.decode(self,*args,**kwargs)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.speech_to_text.processing_speech_to_text.Speech2TextProcessor.save_pretrained(self,save_directory)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/tokenization_speech_to_text.py----------------------------------------
A:transformers.models.speech_to_text.tokenization_speech_to_text.logger->utils.logging.get_logger(__name__)
A:transformers.models.speech_to_text.tokenization_speech_to_text.self.encoder->load_json(vocab_file)
A:transformers.models.speech_to_text.tokenization_speech_to_text.self.sp_model->load_spm(self.spm_file, self.sp_model_kwargs)
A:transformers.models.speech_to_text.tokenization_speech_to_text.out_string->out_string.upper().upper()
A:transformers.models.speech_to_text.tokenization_speech_to_text.vocab->self.encoder.copy()
A:transformers.models.speech_to_text.tokenization_speech_to_text.state->self.__dict__.copy()
A:transformers.models.speech_to_text.tokenization_speech_to_text.save_dir->Path(save_directory)
A:transformers.models.speech_to_text.tokenization_speech_to_text.spm->sentencepiece.SentencePieceProcessor(**sp_model_kwargs)
transformers.Speech2TextTokenizer(self,vocab_file,spm_file,bos_token='<s>',eos_token='</s>',pad_token='<pad>',unk_token='<unk>',do_upper_case=False,do_lower_case=False,tgt_lang=None,lang_codes=None,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.Speech2TextTokenizer.__getstate__(self)->Dict
transformers.Speech2TextTokenizer.__setstate__(self,d:Dict)->None
transformers.Speech2TextTokenizer._convert_id_to_token(self,index:int)->str
transformers.Speech2TextTokenizer._convert_token_to_id(self,token)
transformers.Speech2TextTokenizer._tokenize(self,text:str)->List[str]
transformers.Speech2TextTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.Speech2TextTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.Speech2TextTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.Speech2TextTokenizer.get_vocab(self)->Dict
transformers.Speech2TextTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.Speech2TextTokenizer.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.Speech2TextTokenizer.tgt_lang(self)->str
transformers.Speech2TextTokenizer.tgt_lang(self,new_tgt_lang)->None
transformers.Speech2TextTokenizer.vocab_size(self)->int
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer(self,vocab_file,spm_file,bos_token='<s>',eos_token='</s>',pad_token='<pad>',unk_token='<unk>',do_upper_case=False,do_lower_case=False,tgt_lang=None,lang_codes=None,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.__getstate__(self)->Dict
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.__init__(self,vocab_file,spm_file,bos_token='<s>',eos_token='</s>',pad_token='<pad>',unk_token='<unk>',do_upper_case=False,do_lower_case=False,tgt_lang=None,lang_codes=None,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.__setstate__(self,d:Dict)->None
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._convert_id_to_token(self,index:int)->str
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._convert_token_to_id(self,token)
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._tokenize(self,text:str)->List[str]
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.get_vocab(self)->Dict
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.set_tgt_lang_special_tokens(self,tgt_lang:str)->None
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.tgt_lang(self)->str
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.tgt_lang(self,new_tgt_lang)->None
transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.vocab_size(self)->int
transformers.models.speech_to_text.tokenization_speech_to_text.load_json(path:str)->Union[Dict, List]
transformers.models.speech_to_text.tokenization_speech_to_text.load_spm(path:str,sp_model_kwargs:Dict[str,Any])->sentencepiece.SentencePieceProcessor
transformers.models.speech_to_text.tokenization_speech_to_text.save_json(data,path:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/modeling_speech_to_text.py----------------------------------------
A:transformers.models.speech_to_text.modeling_speech_to_text.logger->utils.logging.get_logger(__name__)
A:transformers.models.speech_to_text.modeling_speech_to_text.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.speech_to_text.modeling_speech_to_text.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.speech_to_text.modeling_speech_to_text.mask->input_ids.view(-1, input_shape[-1]).ne(padding_idx).int()
A:transformers.models.speech_to_text.modeling_speech_to_text.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.speech_to_text.modeling_speech_to_text.(bsz, src_len)->input_ids.view(-1, input_shape[-1]).ne(padding_idx).int().size()
A:transformers.models.speech_to_text.modeling_speech_to_text.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.conv_layers->torch.nn.ModuleList((nn.Conv1d(self.in_channels if i == 0 else self.mid_channels // 2, self.mid_channels if i < self.num_layers - 1 else self.out_channels * 2, kernel_size=k, stride=2, padding=k // 2) for (i, k) in enumerate(self.kernel_sizes)))
A:transformers.models.speech_to_text.modeling_speech_to_text.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.speech_to_text.modeling_speech_to_text.emb_weights->emb_weights.to(self.weights.device).to(self.weights.device)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.weights->torch.nn.Parameter(emb_weights)
A:transformers.models.speech_to_text.modeling_speech_to_text.emb->torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
A:transformers.models.speech_to_text.modeling_speech_to_text.(bsz, seq_len)->input_ids.view(-1, input_shape[-1]).size()
A:transformers.models.speech_to_text.modeling_speech_to_text.position_ids->self.create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length).to(input_ids.device)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.speech_to_text.modeling_speech_to_text.(bsz, tgt_len, embed_dim)->self.layer_norm(hidden_states).size()
A:transformers.models.speech_to_text.modeling_speech_to_text.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.speech_to_text.modeling_speech_to_text.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.speech_to_text.modeling_speech_to_text.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.speech_to_text.modeling_speech_to_text.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.speech_to_text.modeling_speech_to_text.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.speech_to_text.modeling_speech_to_text.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.speech_to_text.modeling_speech_to_text.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.speech_to_text.modeling_speech_to_text.attn_output->self.out_proj(attn_output)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.self_attn->Speech2TextAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.speech_to_text.modeling_speech_to_text.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.encoder_attn->Speech2TextAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.speech_to_text.modeling_speech_to_text.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.speech_to_text.modeling_speech_to_text.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.speech_to_text.modeling_speech_to_text.subsampled_lengths->self._get_subsampled_output_lengths(attention_mask.sum(-1))
A:transformers.models.speech_to_text.modeling_speech_to_text.max_len->self._get_subsampled_output_lengths(attention_mask.sum(-1)).max().item()
A:transformers.models.speech_to_text.modeling_speech_to_text.attention_mask->self._prepare_decoder_attention_mask(attention_mask, input_shape, inputs_embeds, past_key_values_length)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.conv->Conv1dSubsampler(config)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.embed_positions->Speech2TextSinusoidalPositionalEmbedding(self.max_target_positions, config.d_model, self.padding_idx)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.layers->torch.nn.ModuleList([Speech2TextDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.speech_to_text.modeling_speech_to_text.self.layer_norm->torch.nn.LayerNorm(config.d_model)
A:transformers.models.speech_to_text.modeling_speech_to_text.inputs_embeds->self.conv(input_features)
A:transformers.models.speech_to_text.modeling_speech_to_text.padding_mask->self._prepare_decoder_attention_mask(attention_mask, input_shape, inputs_embeds, past_key_values_length).ne(1).long()
A:transformers.models.speech_to_text.modeling_speech_to_text.embed_pos->self.embed_positions(padding_mask)
A:transformers.models.speech_to_text.modeling_speech_to_text.dropout_probability->random.uniform(0, 1)
A:transformers.models.speech_to_text.modeling_speech_to_text.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.speech_to_text.modeling_speech_to_text.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.speech_to_text.modeling_speech_to_text.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.speech_to_text.modeling_speech_to_text.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.models.speech_to_text.modeling_speech_to_text.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.speech_to_text.modeling_speech_to_text.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.speech_to_text.modeling_speech_to_text.positions->self.embed_positions(input_ids, past_key_values_length=past_key_values_length)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.encoder->Speech2TextEncoder(config)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.decoder->Speech2TextDecoder(config)
A:transformers.models.speech_to_text.modeling_speech_to_text.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.speech_to_text.modeling_speech_to_text.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.model->Speech2TextModel(config)
A:transformers.models.speech_to_text.modeling_speech_to_text.self.lm_head->torch.nn.Linear(config.d_model, self.config.vocab_size, bias=False)
A:transformers.models.speech_to_text.modeling_speech_to_text.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.speech_to_text.modeling_speech_to_text.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.speech_to_text.modeling_speech_to_text.outputs->self.model(input_features, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, encoder_outputs=encoder_outputs, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.speech_to_text.modeling_speech_to_text.lm_logits->self.lm_head(outputs[0])
A:transformers.models.speech_to_text.modeling_speech_to_text.loss_fct->CrossEntropyLoss()
A:transformers.models.speech_to_text.modeling_speech_to_text.loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
transformers.Speech2TextForConditionalGeneration(self,config:Speech2TextConfig)
transformers.Speech2TextForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.Speech2TextForConditionalGeneration.forward(self,input_features=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.Speech2TextForConditionalGeneration.get_decoder(self)
transformers.Speech2TextForConditionalGeneration.get_encoder(self)
transformers.Speech2TextForConditionalGeneration.get_output_embeddings(self)
transformers.Speech2TextForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.Speech2TextForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.Speech2TextForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.Speech2TextModel(self,config:Speech2TextConfig)
transformers.Speech2TextModel.forward(self,input_features=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.Speech2TextModel.get_decoder(self)
transformers.Speech2TextModel.get_encoder(self)
transformers.Speech2TextModel.get_input_embeddings(self)
transformers.Speech2TextModel.set_input_embeddings(self,value)
transformers.Speech2TextPreTrainedModel(PreTrainedModel)
transformers.Speech2TextPreTrainedModel._get_subsampled_encoder_attn_mask(self,attention_mask)
transformers.Speech2TextPreTrainedModel._get_subsampled_output_lengths(self,input_lengths:torch.LongTensor)
transformers.Speech2TextPreTrainedModel._init_weights(self,module)
transformers.models.speech_to_text.modeling_speech_to_text.Conv1dSubsampler(self,config)
transformers.models.speech_to_text.modeling_speech_to_text.Conv1dSubsampler.__init__(self,config)
transformers.models.speech_to_text.modeling_speech_to_text.Conv1dSubsampler.forward(self,input_features)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoder(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoder.__init__(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoder.get_input_embeddings(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoder.set_input_embeddings(self,value)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoderLayer(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoderLayer.__init__(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextEncoder(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextEncoder.__init__(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextEncoder.forward(self,input_features,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextEncoderLayer(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextEncoderLayer.__init__(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.__init__(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.forward(self,input_features=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.get_decoder(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.get_encoder(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.get_output_embeddings(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel.__init__(self,config:Speech2TextConfig)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel.forward(self,input_features=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel.get_decoder(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel.get_encoder(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel.get_input_embeddings(self)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextModel.set_input_embeddings(self,value)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextPreTrainedModel(PreTrainedModel)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextPreTrainedModel._get_subsampled_encoder_attn_mask(self,attention_mask)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextPreTrainedModel._get_subsampled_output_lengths(self,input_lengths:torch.LongTensor)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextPreTrainedModel._init_weights(self,module)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding.create_position_ids_from_input_ids(self,input_ids:torch.Tensor,padding_idx:int,past_key_values_length:Optional[int]=0)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding.forward(self,input_ids:torch.Tensor,past_key_values_length:int=0)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding.get_embedding(num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.speech_to_text.modeling_speech_to_text.Speech2TextSinusoidalPositionalEmbedding.make_weights(self,num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.speech_to_text.modeling_speech_to_text._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.speech_to_text.modeling_speech_to_text._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.speech_to_text.modeling_speech_to_text.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/__init__.py----------------------------------------
A:transformers.models.speech_to_text.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/speech_to_text/feature_extraction_speech_to_text.py----------------------------------------
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.logger->utils.logging.get_logger(__name__)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.waveform->torch.from_numpy(waveform).unsqueeze(0)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.features->self.normalize(features)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.mean->numpy.divide(x, std).mean(axis=0)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.square_sums->(x ** 2).sum(axis=0)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.x->numpy.divide(x, std)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.std->numpy.sqrt(np.maximum(var, 1e-10))
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.is_batched->bool(isinstance(raw_speech, (list, tuple)) and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list))))
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.raw_speech->numpy.asarray(raw_speech)
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.encoded_inputs->BatchFeature({'input_features': features})
A:transformers.models.speech_to_text.feature_extraction_speech_to_text.padded_inputs->self.pad(encoded_inputs, padding=padding, max_length=max_length, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask, return_tensors=return_tensors, **kwargs)
transformers.Speech2TextFeatureExtractor(self,feature_size=80,sampling_rate=16000,num_mel_bins=80,padding_value=0.0,do_ceptral_normalize=True,normalize_means=True,normalize_vars=True,**kwargs)
transformers.Speech2TextFeatureExtractor._extract_fbank_features(self,waveform:np.ndarray)->np.ndarray
transformers.Speech2TextFeatureExtractor.normalize(self,input_values:List[np.ndarray])->List[np.ndarray]
transformers.Speech2TextFeatureExtractor.utterance_cmvn(x:np.ndarray,normalize_means:Optional[bool]=True,normalize_vars:Optional[bool]=True)->np.ndarray
transformers.models.speech_to_text.feature_extraction_speech_to_text.Speech2TextFeatureExtractor(self,feature_size=80,sampling_rate=16000,num_mel_bins=80,padding_value=0.0,do_ceptral_normalize=True,normalize_means=True,normalize_vars=True,**kwargs)
transformers.models.speech_to_text.feature_extraction_speech_to_text.Speech2TextFeatureExtractor.__init__(self,feature_size=80,sampling_rate=16000,num_mel_bins=80,padding_value=0.0,do_ceptral_normalize=True,normalize_means=True,normalize_vars=True,**kwargs)
transformers.models.speech_to_text.feature_extraction_speech_to_text.Speech2TextFeatureExtractor._extract_fbank_features(self,waveform:np.ndarray)->np.ndarray
transformers.models.speech_to_text.feature_extraction_speech_to_text.Speech2TextFeatureExtractor.normalize(self,input_values:List[np.ndarray])->List[np.ndarray]
transformers.models.speech_to_text.feature_extraction_speech_to_text.Speech2TextFeatureExtractor.utterance_cmvn(x:np.ndarray,normalize_means:Optional[bool]=True,normalize_vars:Optional[bool]=True)->np.ndarray


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/modeling_flax_roberta.py----------------------------------------
A:transformers.models.roberta.modeling_flax_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.roberta.modeling_flax_roberta.mask->mask.reshape((-1, mask.shape[-1])).reshape((-1, mask.shape[-1]))
A:transformers.models.roberta.modeling_flax_roberta.incremental_indices->incremental_indices.reshape(input_ids.shape).reshape(input_ids.shape)
A:transformers.models.roberta.modeling_flax_roberta.self.word_embeddings->flax.linen.Embed(self.config.vocab_size, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.position_embeddings->flax.linen.Embed(self.config.max_position_embeddings, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.token_type_embeddings->flax.linen.Embed(self.config.type_vocab_size, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.LayerNorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.dropout->flax.linen.Dropout(rate=self.config.hidden_dropout_prob)
A:transformers.models.roberta.modeling_flax_roberta.inputs_embeds->self.word_embeddings(input_ids.astype('i4'))
A:transformers.models.roberta.modeling_flax_roberta.position_embeds->self.position_embeddings(position_ids.astype('i4'))
A:transformers.models.roberta.modeling_flax_roberta.token_type_embeddings->self.token_type_embeddings(token_type_ids.astype('i4'))
A:transformers.models.roberta.modeling_flax_roberta.hidden_states->self.dropout(hidden_states, deterministic=deterministic)
A:transformers.models.roberta.modeling_flax_roberta.self.query->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.self.key->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.self.value->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.query_states->self.query(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.roberta.modeling_flax_roberta.value_states->self.value(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.roberta.modeling_flax_roberta.key_states->self.key(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.roberta.modeling_flax_roberta.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.roberta.modeling_flax_roberta.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, -10000000000.0).astype(self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.dropout_rng->self.make_rng('dropout')
A:transformers.models.roberta.modeling_flax_roberta.attn_weights->dot_product_attention_weights(query_states, key_states, bias=attention_bias, dropout_rng=dropout_rng, dropout_rate=self.config.attention_probs_dropout_prob, broadcast_dropout=True, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.roberta.modeling_flax_roberta.attn_output->attn_output.reshape(attn_output.shape[:2] + (-1,)).reshape(attn_output.shape[:2] + (-1,))
A:transformers.models.roberta.modeling_flax_roberta.self.dense->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.self.self->FlaxRobertaSelfAttention(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.output->FlaxRobertaOutput(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.attn_outputs->self.self(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.roberta.modeling_flax_roberta.self.attention->FlaxRobertaAttention(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.intermediate->FlaxRobertaIntermediate(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.attention_outputs->self.attention(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.roberta.modeling_flax_roberta.layer_outputs->layer(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.roberta.modeling_flax_roberta.self.layer->FlaxRobertaLayerCollection(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.cls_hidden_state->self.dense(cls_hidden_state)
A:transformers.models.roberta.modeling_flax_roberta.self.layer_norm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.decoder->flax.linen.Dense(self.config.vocab_size, dtype=self.dtype, use_bias=False, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.self.bias->self.param('bias', self.bias_init, (self.config.vocab_size,))
A:transformers.models.roberta.modeling_flax_roberta.self.out_proj->flax.linen.Dense(self.config.num_labels, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.roberta.modeling_flax_roberta.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.roberta.modeling_flax_roberta.input_ids->jax.numpy.zeros(input_shape, dtype='i4')
A:transformers.models.roberta.modeling_flax_roberta.token_type_ids->jax.numpy.zeros_like(input_ids)
A:transformers.models.roberta.modeling_flax_roberta.position_ids->create_position_ids_from_input_ids(input_ids, self.config.pad_token_id)
A:transformers.models.roberta.modeling_flax_roberta.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.roberta.modeling_flax_roberta.self.embeddings->FlaxRobertaEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.encoder->FlaxRobertaEncoder(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.self.pooler->FlaxRobertaPooler(self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.outputs->self.roberta(input_ids, attention_mask, token_type_ids, position_ids, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.roberta.modeling_flax_roberta.self.roberta->FlaxRobertaModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)
A:transformers.models.roberta.modeling_flax_roberta.self.lm_head->FlaxRobertaLMHead(config=self.config, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.logits->self.qa_outputs(hidden_states)
A:transformers.models.roberta.modeling_flax_roberta.self.classifier->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.pooled_output->self.dropout(pooled_output, deterministic=deterministic)
A:transformers.models.roberta.modeling_flax_roberta.reshaped_logits->self.qa_outputs(hidden_states).reshape(-1, num_choices)
A:transformers.models.roberta.modeling_flax_roberta.self.qa_outputs->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.roberta.modeling_flax_roberta.(start_logits, end_logits)->self.qa_outputs(hidden_states).split(self.config.num_labels, axis=-1)
A:transformers.models.roberta.modeling_flax_roberta.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.models.roberta.modeling_flax_roberta.end_logits->end_logits.squeeze(-1).squeeze(-1)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaAttention.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaClassificationHead(self,hidden_states,deterministic=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaClassificationHead.__call__(self,hidden_states,deterministic=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaClassificationHead.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaEmbeddings(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaEmbeddings.__call__(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaEmbeddings.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaEncoder(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaEncoder.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaEncoder.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMaskedLM(FlaxRobertaPreTrainedModel)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMaskedLMModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMaskedLMModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMaskedLMModule.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMultipleChoice(FlaxRobertaPreTrainedModel)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForMultipleChoiceModule.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForQuestionAnswering(FlaxRobertaPreTrainedModel)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForQuestionAnsweringModule.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForSequenceClassification(FlaxRobertaPreTrainedModel)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForSequenceClassificationModule.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForTokenClassification(FlaxRobertaPreTrainedModel)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForTokenClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaForTokenClassificationModule.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaIntermediate(self,hidden_states)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaIntermediate.__call__(self,hidden_states)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaIntermediate.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLMHead(self,hidden_states,shared_embedding=None)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLMHead.__call__(self,hidden_states,shared_embedding=None)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLMHead.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLayer(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLayer.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLayer.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLayerCollection(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLayerCollection.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaLayerCollection.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaModel(FlaxRobertaPreTrainedModel)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaModule.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaOutput(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaOutput.__call__(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaOutput.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaPooler(self,hidden_states)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaPooler.__call__(self,hidden_states)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaPooler.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel(self,config:RobertaConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.__init__(self,config:RobertaConfig,input_shape:Tuple=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaSelfAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaSelfAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaSelfAttention.setup(self)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaSelfOutput(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaSelfOutput.__call__(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.roberta.modeling_flax_roberta.FlaxRobertaSelfOutput.setup(self)
transformers.models.roberta.modeling_flax_roberta.create_position_ids_from_input_ids(input_ids,padding_idx)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/modeling_roberta.py----------------------------------------
A:transformers.models.roberta.modeling_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.roberta.modeling_roberta.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.roberta.modeling_roberta.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.models.roberta.modeling_roberta.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.roberta.modeling_roberta.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.roberta.modeling_roberta.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.roberta.modeling_roberta.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.roberta.modeling_roberta.position_ids->torch.arange(self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device)
A:transformers.models.roberta.modeling_roberta.input_shape->input_ids.size()
A:transformers.models.roberta.modeling_roberta.buffered_token_type_ids_expanded->buffered_token_type_ids.expand(batch_size, seq_length)
A:transformers.models.roberta.modeling_roberta.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.roberta.modeling_roberta.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.roberta.modeling_roberta.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.roberta.modeling_roberta.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.roberta.modeling_roberta.embeddings->self.dropout(embeddings)
A:transformers.models.roberta.modeling_roberta.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.roberta.modeling_roberta.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.roberta.modeling_roberta.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.roberta.modeling_roberta.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.roberta.modeling_roberta.self.distance_embedding->torch.nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
A:transformers.models.roberta.modeling_roberta.x->self.out_proj(x)
A:transformers.models.roberta.modeling_roberta.mixed_query_layer->self.query(hidden_states)
A:transformers.models.roberta.modeling_roberta.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.roberta.modeling_roberta.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.roberta.modeling_roberta.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.roberta.modeling_roberta.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.roberta.modeling_roberta.position_ids_l->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
A:transformers.models.roberta.modeling_roberta.position_ids_r->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
A:transformers.models.roberta.modeling_roberta.positional_embedding->positional_embedding.to(dtype=query_layer.dtype).to(dtype=query_layer.dtype)
A:transformers.models.roberta.modeling_roberta.relative_position_scores->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.roberta.modeling_roberta.relative_position_scores_query->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.roberta.modeling_roberta.relative_position_scores_key->torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
A:transformers.models.roberta.modeling_roberta.attention_probs->self.dropout(attention_probs)
A:transformers.models.roberta.modeling_roberta.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.roberta.modeling_roberta.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.roberta.modeling_roberta.hidden_states->self.LayerNorm(hidden_states + input_tensor)
A:transformers.models.roberta.modeling_roberta.self.self->RobertaSelfAttention(config)
A:transformers.models.roberta.modeling_roberta.self.output->RobertaOutput(config)
A:transformers.models.roberta.modeling_roberta.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.roberta.modeling_roberta.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.roberta.modeling_roberta.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.roberta.modeling_roberta.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.roberta.modeling_roberta.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.roberta.modeling_roberta.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.roberta.modeling_roberta.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.roberta.modeling_roberta.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.roberta.modeling_roberta.self.attention->RobertaAttention(config)
A:transformers.models.roberta.modeling_roberta.self.crossattention->RobertaAttention(config)
A:transformers.models.roberta.modeling_roberta.self.intermediate->RobertaIntermediate(config)
A:transformers.models.roberta.modeling_roberta.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.roberta.modeling_roberta.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.roberta.modeling_roberta.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.roberta.modeling_roberta.intermediate_output->self.intermediate(attention_output)
A:transformers.models.roberta.modeling_roberta.self.layer->torch.nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.roberta.modeling_roberta.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.roberta.modeling_roberta.self.activation->torch.nn.Tanh()
A:transformers.models.roberta.modeling_roberta.pooled_output->self.dropout(pooled_output)
A:transformers.models.roberta.modeling_roberta.self.embeddings->RobertaEmbeddings(config)
A:transformers.models.roberta.modeling_roberta.self.encoder->RobertaEncoder(config)
A:transformers.models.roberta.modeling_roberta.attention_mask->input_ids.new_ones(input_shape)
A:transformers.models.roberta.modeling_roberta.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.roberta.modeling_roberta.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.roberta.modeling_roberta.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.roberta.modeling_roberta.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.roberta.modeling_roberta.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, past_key_values_length=past_key_values_length)
A:transformers.models.roberta.modeling_roberta.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.roberta.modeling_roberta.self.roberta->RobertaModel(config, add_pooling_layer=False)
A:transformers.models.roberta.modeling_roberta.self.lm_head->RobertaLMHead(config)
A:transformers.models.roberta.modeling_roberta.outputs->self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.roberta.modeling_roberta.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.roberta.modeling_roberta.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.models.roberta.modeling_roberta.labels->labels[:, 1:].contiguous()
A:transformers.models.roberta.modeling_roberta.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.roberta.modeling_roberta.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.roberta.modeling_roberta.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.roberta.modeling_roberta.self.layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.roberta.modeling_roberta.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.roberta.modeling_roberta.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.roberta.modeling_roberta.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.roberta.modeling_roberta.logits->self.qa_outputs(sequence_output)
A:transformers.models.roberta.modeling_roberta.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.roberta.modeling_roberta.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.roberta.modeling_roberta.sequence_output->self.dropout(sequence_output)
A:transformers.models.roberta.modeling_roberta.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.roberta.modeling_roberta.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.roberta.modeling_roberta.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.roberta.modeling_roberta.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.roberta.modeling_roberta.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.roberta.modeling_roberta.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.roberta.modeling_roberta.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.roberta.modeling_roberta.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.roberta.modeling_roberta.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.roberta.modeling_roberta.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.roberta.modeling_roberta.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.roberta.modeling_roberta.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.roberta.modeling_roberta.mask->input_ids.ne(padding_idx).int()
transformers.RobertaForCausalLM(self,config)
transformers.RobertaForCausalLM._reorder_cache(self,past,beam_idx)
transformers.RobertaForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForCausalLM.get_output_embeddings(self)
transformers.RobertaForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.RobertaForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.RobertaForMaskedLM(self,config)
transformers.RobertaForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForMaskedLM.get_output_embeddings(self)
transformers.RobertaForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.RobertaForMultipleChoice(self,config)
transformers.RobertaForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForQuestionAnswering(self,config)
transformers.RobertaForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForSequenceClassification(self,config)
transformers.RobertaForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForTokenClassification(self,config)
transformers.RobertaForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaModel(self,config,add_pooling_layer=True)
transformers.RobertaModel._prune_heads(self,heads_to_prune)
transformers.RobertaModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaModel.get_input_embeddings(self)
transformers.RobertaModel.set_input_embeddings(self,value)
transformers.RobertaPreTrainedModel(PreTrainedModel)
transformers.RobertaPreTrainedModel._init_weights(self,module)
transformers.models.roberta.modeling_roberta.RobertaAttention(self,config)
transformers.models.roberta.modeling_roberta.RobertaAttention.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.roberta.modeling_roberta.RobertaAttention.prune_heads(self,heads)
transformers.models.roberta.modeling_roberta.RobertaClassificationHead(self,config)
transformers.models.roberta.modeling_roberta.RobertaClassificationHead.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaClassificationHead.forward(self,features,**kwargs)
transformers.models.roberta.modeling_roberta.RobertaEmbeddings(self,config)
transformers.models.roberta.modeling_roberta.RobertaEmbeddings.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.models.roberta.modeling_roberta.RobertaEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.roberta.modeling_roberta.RobertaEncoder(self,config)
transformers.models.roberta.modeling_roberta.RobertaEncoder.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM(self,config)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM._reorder_cache(self,past,beam_idx)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM.get_output_embeddings(self)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.models.roberta.modeling_roberta.RobertaForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.roberta.modeling_roberta.RobertaForMaskedLM(self,config)
transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.get_output_embeddings(self)
transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice(self,config)
transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering(self,config)
transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification(self,config)
transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaForTokenClassification(self,config)
transformers.models.roberta.modeling_roberta.RobertaForTokenClassification.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaIntermediate(self,config)
transformers.models.roberta.modeling_roberta.RobertaIntermediate.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaIntermediate.forward(self,hidden_states)
transformers.models.roberta.modeling_roberta.RobertaLMHead(self,config)
transformers.models.roberta.modeling_roberta.RobertaLMHead.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaLMHead.forward(self,features,**kwargs)
transformers.models.roberta.modeling_roberta.RobertaLayer(self,config)
transformers.models.roberta.modeling_roberta.RobertaLayer.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaLayer.feed_forward_chunk(self,attention_output)
transformers.models.roberta.modeling_roberta.RobertaLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.roberta.modeling_roberta.RobertaModel(self,config,add_pooling_layer=True)
transformers.models.roberta.modeling_roberta.RobertaModel.__init__(self,config,add_pooling_layer=True)
transformers.models.roberta.modeling_roberta.RobertaModel._prune_heads(self,heads_to_prune)
transformers.models.roberta.modeling_roberta.RobertaModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.roberta.modeling_roberta.RobertaModel.get_input_embeddings(self)
transformers.models.roberta.modeling_roberta.RobertaModel.set_input_embeddings(self,value)
transformers.models.roberta.modeling_roberta.RobertaOutput(self,config)
transformers.models.roberta.modeling_roberta.RobertaOutput.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaOutput.forward(self,hidden_states,input_tensor)
transformers.models.roberta.modeling_roberta.RobertaPooler(self,config)
transformers.models.roberta.modeling_roberta.RobertaPooler.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaPooler.forward(self,hidden_states)
transformers.models.roberta.modeling_roberta.RobertaPreTrainedModel(PreTrainedModel)
transformers.models.roberta.modeling_roberta.RobertaPreTrainedModel._init_weights(self,module)
transformers.models.roberta.modeling_roberta.RobertaSelfAttention(self,config)
transformers.models.roberta.modeling_roberta.RobertaSelfAttention.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.roberta.modeling_roberta.RobertaSelfAttention.transpose_for_scores(self,x)
transformers.models.roberta.modeling_roberta.RobertaSelfOutput(self,config)
transformers.models.roberta.modeling_roberta.RobertaSelfOutput.__init__(self,config)
transformers.models.roberta.modeling_roberta.RobertaSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids(input_ids,padding_idx,past_key_values_length=0)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/tokenization_roberta_fast.py----------------------------------------
A:transformers.models.roberta.tokenization_roberta_fast.logger->utils.logging.get_logger(__name__)
transformers.RobertaTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.RobertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.RobertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RobertaTokenizerFast.mask_token(self)->str
transformers.RobertaTokenizerFast.mask_token(self,value)
transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast(self,vocab_file,merges_file,tokenizer_file=None,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.__init__(self,vocab_file,merges_file,tokenizer_file=None,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.mask_token(self)->str
transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.mask_token(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/modeling_tf_roberta.py----------------------------------------
A:transformers.models.roberta.modeling_tf_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.roberta.modeling_tf_roberta.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.roberta.modeling_tf_roberta.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.roberta.modeling_tf_roberta.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.roberta.modeling_tf_roberta.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.roberta.modeling_tf_roberta.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.roberta.modeling_tf_roberta.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.roberta.modeling_tf_roberta.mask->tensorflow.cast(tf.math.not_equal(input_ids, self.padding_idx), dtype=input_ids.dtype)
A:transformers.models.roberta.modeling_tf_roberta.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.roberta.modeling_tf_roberta.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.roberta.modeling_tf_roberta.position_ids->tensorflow.tile(input=position_ids, multiples=(input_shape[0], 1))
A:transformers.models.roberta.modeling_tf_roberta.position_embeds->tensorflow.gather(params=self.position_embeddings, indices=position_ids)
A:transformers.models.roberta.modeling_tf_roberta.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.roberta.modeling_tf_roberta.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.roberta.modeling_tf_roberta.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), activation='tanh', name='dense')
A:transformers.models.roberta.modeling_tf_roberta.pooled_output->self.dropout(pooled_output, training=inputs['training'])
A:transformers.models.roberta.modeling_tf_roberta.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.roberta.modeling_tf_roberta.self.sqrt_att_head_size->math.sqrt(self.attention_head_size)
A:transformers.models.roberta.modeling_tf_roberta.self.query->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.roberta.modeling_tf_roberta.self.key->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.roberta.modeling_tf_roberta.self.value->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.roberta.modeling_tf_roberta.tensor->tensorflow.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.roberta.modeling_tf_roberta.mixed_query_layer->self.query(inputs=hidden_states)
A:transformers.models.roberta.modeling_tf_roberta.mixed_key_layer->self.key(inputs=hidden_states)
A:transformers.models.roberta.modeling_tf_roberta.mixed_value_layer->self.value(inputs=hidden_states)
A:transformers.models.roberta.modeling_tf_roberta.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.roberta.modeling_tf_roberta.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.roberta.modeling_tf_roberta.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.roberta.modeling_tf_roberta.attention_scores->tensorflow.add(attention_scores, attention_mask)
A:transformers.models.roberta.modeling_tf_roberta.dk->tensorflow.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
A:transformers.models.roberta.modeling_tf_roberta.attention_probs->tensorflow.multiply(attention_probs, head_mask)
A:transformers.models.roberta.modeling_tf_roberta.attention_output->self.dense_output(hidden_states=self_outputs[0], input_tensor=input_tensor, training=training)
A:transformers.models.roberta.modeling_tf_roberta.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.bias)
A:transformers.models.roberta.modeling_tf_roberta.self.self_attention->TFRobertaSelfAttention(config, name='self')
A:transformers.models.roberta.modeling_tf_roberta.self.dense_output->TFRobertaSelfOutput(config, name='output')
A:transformers.models.roberta.modeling_tf_roberta.self_outputs->self.self_attention(hidden_states=input_tensor, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.roberta.modeling_tf_roberta.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.roberta.modeling_tf_roberta.self.attention->TFRobertaAttention(config, name='attention')
A:transformers.models.roberta.modeling_tf_roberta.self.intermediate->TFRobertaIntermediate(config, name='intermediate')
A:transformers.models.roberta.modeling_tf_roberta.self.bert_output->TFRobertaOutput(config, name='output')
A:transformers.models.roberta.modeling_tf_roberta.attention_outputs->self.attention(input_tensor=hidden_states, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.roberta.modeling_tf_roberta.intermediate_output->self.intermediate(hidden_states=attention_output)
A:transformers.models.roberta.modeling_tf_roberta.layer_output->self.bert_output(hidden_states=intermediate_output, input_tensor=attention_output, training=training)
A:transformers.models.roberta.modeling_tf_roberta.layer_outputs->layer_module(hidden_states=hidden_states, attention_mask=attention_mask, head_mask=head_mask[i], output_attentions=output_attentions, training=training)
A:transformers.models.roberta.modeling_tf_roberta.self.encoder->TFRobertaEncoder(config, name='encoder')
A:transformers.models.roberta.modeling_tf_roberta.self.embeddings->TFRobertaEmbeddings(config, name='embeddings')
A:transformers.models.roberta.modeling_tf_roberta.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.roberta.modeling_tf_roberta.input_shape->shape_list(tensor=inputs['input_ids'])
A:transformers.models.roberta.modeling_tf_roberta.inputs['attention_mask']->tensorflow.fill(dims=input_shape, value=1)
A:transformers.models.roberta.modeling_tf_roberta.inputs['token_type_ids']->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.roberta.modeling_tf_roberta.embedding_output->self.embeddings(input_ids=inputs['input_ids'], position_ids=inputs['position_ids'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.roberta.modeling_tf_roberta.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.roberta.modeling_tf_roberta.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.roberta.modeling_tf_roberta.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.roberta.modeling_tf_roberta.encoder_outputs->self.encoder(hidden_states=embedding_output, attention_mask=extended_attention_mask, head_mask=inputs['head_mask'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.roberta.modeling_tf_roberta.output->self.call(inputs)
A:transformers.models.roberta.modeling_tf_roberta.self.roberta->TFRobertaMainLayer(config, add_pooling_layer=False, name='roberta')
A:transformers.models.roberta.modeling_tf_roberta.outputs->self.roberta(inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.roberta.modeling_tf_roberta.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.roberta.modeling_tf_roberta.self.act->get_tf_activation('gelu')
A:transformers.models.roberta.modeling_tf_roberta.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.roberta.modeling_tf_roberta.self.lm_head->TFRobertaLMHead(config, self.roberta.embeddings, name='lm_head')
A:transformers.models.roberta.modeling_tf_roberta.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.roberta.modeling_tf_roberta.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.models.roberta.modeling_tf_roberta.x->self.out_proj(x)
A:transformers.models.roberta.modeling_tf_roberta.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.roberta.modeling_tf_roberta.logits->self.qa_outputs(sequence_output)
A:transformers.models.roberta.modeling_tf_roberta.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.roberta.modeling_tf_roberta.sequence_output->self.dropout(sequence_output, training=inputs['training'])
A:transformers.models.roberta.modeling_tf_roberta.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.roberta.modeling_tf_roberta.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.roberta.modeling_tf_roberta.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.roberta.modeling_tf_roberta.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.roberta.modeling_tf_roberta.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFRobertaForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFRobertaForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFRobertaForMaskedLM.get_lm_head(self)
transformers.TFRobertaForMaskedLM.get_prefix_bias_name(self)
transformers.TFRobertaForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.TFRobertaForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFRobertaForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFRobertaForMultipleChoice.dummy_inputs(self)
transformers.TFRobertaForMultipleChoice.serving(self,inputs)
transformers.TFRobertaForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.TFRobertaForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFRobertaForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFRobertaForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.TFRobertaForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFRobertaForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFRobertaForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.TFRobertaForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFRobertaForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFRobertaForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.TFRobertaMainLayer(self,config,add_pooling_layer=True,**kwargs)
transformers.TFRobertaMainLayer._prune_heads(self,heads_to_prune)
transformers.TFRobertaMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.TFRobertaMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.TFRobertaMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.TFRobertaModel(self,config,*inputs,**kwargs)
transformers.TFRobertaModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFRobertaModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.TFRobertaPreTrainedModel(TFPreTrainedModel)
transformers.TFRobertaPreTrainedModel.serving(self,inputs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaAttention(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaAttention.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaAttention.call(self,input_tensor:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.roberta.modeling_tf_roberta.TFRobertaAttention.prune_heads(self,heads)
transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead(self,config,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead.__init__(self,config,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead.call(self,features,training=False)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings(self,config,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings.__init__(self,config,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,training=False)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings.create_position_ids_from_input_ids(self,input_ids)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEncoder(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEncoder.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaEncoder.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,return_dict:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.get_lm_head(self)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.get_prefix_bias_name(self)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.dummy_inputs(self)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.serving(self,inputs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForMultipleChoice.serving_output(self,output:TFMultipleChoiceModelOutput)->TFMultipleChoiceModelOutput
transformers.models.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForQuestionAnswering.serving_output(self,output:TFQuestionAnsweringModelOutput)->TFQuestionAnsweringModelOutput
transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.roberta.modeling_tf_roberta.TFRobertaForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForTokenClassification.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.roberta.modeling_tf_roberta.TFRobertaIntermediate(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaIntermediate.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead(self,config,input_embeddings,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.build(self,input_shape)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.call(self,hidden_states)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.get_bias(self)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.get_output_embeddings(self)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.set_bias(self,value)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLMHead.set_output_embeddings(self,value)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLayer(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLayer.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer(self,config,add_pooling_layer=True,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer.__init__(self,config,add_pooling_layer=True,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer._prune_heads(self,heads_to_prune)
transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.roberta.modeling_tf_roberta.TFRobertaModel(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaModel.__init__(self,config,*inputs,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaModel.call(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.roberta.modeling_tf_roberta.TFRobertaOutput(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaOutput.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.roberta.modeling_tf_roberta.TFRobertaPooler(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaPooler.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaPooler.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.roberta.modeling_tf_roberta.TFRobertaPreTrainedModel(TFPreTrainedModel)
transformers.models.roberta.modeling_tf_roberta.TFRobertaPreTrainedModel.serving(self,inputs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfAttention(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfAttention.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfAttention.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfAttention.transpose_for_scores(self,tensor:tf.Tensor,batch_size:int)->tf.Tensor
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfOutput(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfOutput.__init__(self,config:RobertaConfig,**kwargs)
transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/tokenization_roberta.py----------------------------------------
A:transformers.models.roberta.tokenization_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.models.roberta.tokenization_roberta.add_prefix_space->kwargs.pop('add_prefix_space', self.add_prefix_space)
transformers.RobertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.RobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.RobertaTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.models.roberta.tokenization_roberta.RobertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.models.roberta.tokenization_roberta.RobertaTokenizer.__init__(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.models.roberta.tokenization_roberta.RobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.roberta.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.roberta.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.roberta.tokenization_roberta.RobertaTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/configuration_roberta.py----------------------------------------
A:transformers.models.roberta.configuration_roberta.logger->utils.logging.get_logger(__name__)
transformers.RobertaConfig(self,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.roberta.configuration_roberta.RobertaConfig(self,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.models.roberta.configuration_roberta.RobertaConfig.__init__(self,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/__init__.py----------------------------------------
A:transformers.models.roberta.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.roberta->fairseq.models.roberta.RobertaModel.from_pretrained(roberta_checkpoint_path)
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.config->RobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.args.encoder_embed_dim, num_hidden_layers=roberta.args.encoder_layers, num_attention_heads=roberta.args.encoder_attention_heads, intermediate_size=roberta.args.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.model.roberta.embeddings.token_type_embeddings.weight.data->torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.their_output->fairseq.models.roberta.RobertaModel.from_pretrained(roberta_checkpoint_path).model.classification_heads['mnli'](roberta.extract_features(input_ids))
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.max_absolute_diff->torch.max(torch.abs(our_output - their_output)).item()
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.success->torch.allclose(our_output, their_output, atol=0.001)
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch.convert_roberta_checkpoint_to_pytorch(roberta_checkpoint_path:str,pytorch_dump_folder_path:str,classification_head:bool)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/convert_marian_to_pytorch.py----------------------------------------
A:transformers.models.marian.convert_marian_to_pytorch.stripped->remove_prefix(k, layer_prefix)
A:transformers.models.marian.convert_marian_to_pytorch.sd[converter[stripped]]->torch.tensor(v).squeeze()
A:transformers.models.marian.convert_marian_to_pytorch.sd->convert_encoder_layer(opus_state, layer_tag, converter)
A:transformers.models.marian.convert_marian_to_pytorch.api->HfApi()
A:transformers.models.marian.convert_marian_to_pytorch.model_list->HfApi().model_list()
A:transformers.models.marian.convert_marian_to_pytorch.embs_to_add->numpy.zeros((n_special_tokens, d_model))
A:transformers.models.marian.convert_marian_to_pytorch.new_embs->numpy.concatenate([wemb, embs_to_add])
A:transformers.models.marian.convert_marian_to_pytorch.bias_to_add->numpy.zeros((n_special_tokens, 1))
A:transformers.models.marian.convert_marian_to_pytorch.new_bias->numpy.concatenate((final_bias, bias_to_add), axis=1)
A:transformers.models.marian.convert_marian_to_pytorch.cfg_str->''.join([chr(x) for x in opus_dict[CONFIG_KEY]])
A:transformers.models.marian.convert_marian_to_pytorch.yaml_cfg->yaml.load(cfg_str[:-1], Loader=yaml.BaseLoader)
A:transformers.models.marian.convert_marian_to_pytorch.model_files->list(Path(dest_dir).glob('*.npz'))
A:transformers.models.marian.convert_marian_to_pytorch.x->x.replace(substr, grp_name).replace(substr, grp_name)
A:transformers.models.marian.convert_marian_to_pytorch.hf_model_name->remove_prefix(hf_model_name, ORG_NAME)
A:transformers.models.marian.convert_marian_to_pytorch.opus_w_prefix->remove_prefix(hf_model_name, ORG_NAME).replace('_', '+')
A:transformers.models.marian.convert_marian_to_pytorch.DEFAULT_MODEL_DIR->os.path.join(DEFAULT_REPO, 'models')
A:transformers.models.marian.convert_marian_to_pytorch.opus_readme_path->Path(repo_root).joinpath('models', opus_name, 'README.md')
A:transformers.models.marian.convert_marian_to_pytorch.content->'*'.join(splat)
A:transformers.models.marian.convert_marian_to_pytorch.items->'\n\n'.join([f'- {k}: {v}' for (k, v) in metadata.items()])
A:transformers.models.marian.convert_marian_to_pytorch.n_dash->p.name.count('-')
A:transformers.models.marian.convert_marian_to_pytorch.lns->Path(fname).open().readlines()
A:transformers.models.marian.convert_marian_to_pytorch.results[p.name]->_parse_readme(lns)
A:transformers.models.marian.convert_marian_to_pytorch.save_dir->Path('marian_ckpt')
A:transformers.models.marian.convert_marian_to_pytorch.dest_dir->Path(dest_dir)
A:transformers.models.marian.convert_marian_to_pytorch.pair_name->convert_opus_name_to_hf_name(k)
A:transformers.models.marian.convert_marian_to_pytorch.fname->wget.download(test_set_url, 'opus_test.txt')
A:transformers.models.marian.convert_marian_to_pytorch.src->lmap(str.strip, lns[::4])
A:transformers.models.marian.convert_marian_to_pytorch.gold->lmap(str.strip, lns[1::4])
A:transformers.models.marian.convert_marian_to_pytorch.mar_model->lmap(str.strip, lns[2::4])
A:transformers.models.marian.convert_marian_to_pytorch.ln->ln[1:].strip()
A:transformers.models.marian.convert_marian_to_pytorch.splat->ln[1:].strip().split(':')
A:transformers.models.marian.convert_marian_to_pytorch.dname->Path(dest_dir).name.split('-')
A:transformers.models.marian.convert_marian_to_pytorch.dct->dict(target_lang=dname[-1], source_lang='-'.join(dname[:-1]))
A:transformers.models.marian.convert_marian_to_pytorch.vocab->load_yaml(find_vocab_file(model_dir))
A:transformers.models.marian.convert_marian_to_pytorch.num_added->add_to_vocab_(vocab, ['<pad>'])
A:transformers.models.marian.convert_marian_to_pytorch.npz_path->find_model_file(source_dir)
A:transformers.models.marian.convert_marian_to_pytorch.self.state_dict->dict(self.state_dict)
A:transformers.models.marian.convert_marian_to_pytorch.cfg->load_config_from_state_dict(self.state_dict)
A:transformers.models.marian.convert_marian_to_pytorch.(self.wemb, self.final_bias)->add_emb_entries(self.state_dict['Wemb'], self.state_dict[BIAS_KEY], 1)
A:transformers.models.marian.convert_marian_to_pytorch.self.state_keys->list(self.state_dict.keys())
A:transformers.models.marian.convert_marian_to_pytorch.decoder_yml->cast_marian_config(load_yaml(source_dir / 'decoder.yml'))
A:transformers.models.marian.convert_marian_to_pytorch.self.hf_config->MarianConfig(vocab_size=cfg['vocab_size'], decoder_layers=cfg['dec-depth'], encoder_layers=cfg['enc-depth'], decoder_attention_heads=cfg['transformer-heads'], encoder_attention_heads=cfg['transformer-heads'], decoder_ffn_dim=cfg['transformer-dim-ffn'], encoder_ffn_dim=cfg['transformer-dim-ffn'], d_model=cfg['dim-emb'], activation_function=cfg['transformer-aan-activation'], pad_token_id=self.pad_token_id, eos_token_id=0, bos_token_id=0, max_position_embeddings=cfg['dim-emb'], scale_embedding=True, normalize_embedding='n' in cfg['transformer-preprocess'], static_position_embeddings=not cfg['transformer-train-position-embeddings'], dropout=0.1, num_beams=decoder_yml['beam-size'], decoder_start_token_id=self.pad_token_id, bad_words_ids=[[self.pad_token_id]], max_length=512)
A:transformers.models.marian.convert_marian_to_pytorch.self.encoder_l1->self.sub_keys('encoder_l1')
A:transformers.models.marian.convert_marian_to_pytorch.self.decoder_l1->self.sub_keys('decoder_l1')
A:transformers.models.marian.convert_marian_to_pytorch.self.decoder_l2->self.sub_keys('decoder_l2')
A:transformers.models.marian.convert_marian_to_pytorch.model->model.half().half()
A:transformers.models.marian.convert_marian_to_pytorch.wemb_tensor->torch.nn.Parameter(torch.FloatTensor(self.wemb))
A:transformers.models.marian.convert_marian_to_pytorch.bias_tensor->torch.nn.Parameter(torch.FloatTensor(self.final_bias))
A:transformers.models.marian.convert_marian_to_pytorch.wpos_tensor->torch.tensor(state_dict['Wpos'])
A:transformers.models.marian.convert_marian_to_pytorch.filename->wget.download(url)
A:transformers.models.marian.convert_marian_to_pytorch.tokenizer->transformers.MarianTokenizer.from_pretrained(str(source_dir))
A:transformers.models.marian.convert_marian_to_pytorch.opus_state->OpusState(source_dir)
A:transformers.models.marian.convert_marian_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.marian.convert_marian_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.models.marian.convert_marian_to_pytorch.source_dir->Path(args.src)
transformers.models.marian.convert_marian_to_pytorch.OpusState(self,source_dir)
transformers.models.marian.convert_marian_to_pytorch.OpusState.__init__(self,source_dir)
transformers.models.marian.convert_marian_to_pytorch.OpusState._check_layer_entries(self)
transformers.models.marian.convert_marian_to_pytorch.OpusState.extra_keys(self)
transformers.models.marian.convert_marian_to_pytorch.OpusState.load_marian_model(self)->MarianMTModel
transformers.models.marian.convert_marian_to_pytorch.OpusState.sub_keys(self,layer_prefix)
transformers.models.marian.convert_marian_to_pytorch._cast_yaml_str(v)
transformers.models.marian.convert_marian_to_pytorch._parse_readme(lns)
transformers.models.marian.convert_marian_to_pytorch.add_emb_entries(wemb,final_bias,n_special_tokens=1)
transformers.models.marian.convert_marian_to_pytorch.add_special_tokens_to_vocab(model_dir:Path)->None
transformers.models.marian.convert_marian_to_pytorch.add_to_vocab_(vocab:Dict[str,int],special_tokens:List[str])
transformers.models.marian.convert_marian_to_pytorch.cast_marian_config(raw_cfg:Dict[str,str])->Dict
transformers.models.marian.convert_marian_to_pytorch.check_equal(marian_cfg,k1,k2)
transformers.models.marian.convert_marian_to_pytorch.check_marian_cfg_assumptions(marian_cfg)
transformers.models.marian.convert_marian_to_pytorch.convert(source_dir:Path,dest_dir)
transformers.models.marian.convert_marian_to_pytorch.convert_all_sentencepiece_models(model_list=None,repo_path=None,dest_dir=Path('marian_converted'))
transformers.models.marian.convert_marian_to_pytorch.convert_encoder_layer(opus_dict,layer_prefix:str,converter:dict)
transformers.models.marian.convert_marian_to_pytorch.convert_hf_name_to_opus_name(hf_model_name)
transformers.models.marian.convert_marian_to_pytorch.convert_opus_name_to_hf_name(x)
transformers.models.marian.convert_marian_to_pytorch.convert_whole_dir(path=Path('marian_ckpt/'))
transformers.models.marian.convert_marian_to_pytorch.download_and_unzip(url,dest_dir)
transformers.models.marian.convert_marian_to_pytorch.fetch_test_set(test_set_url)
transformers.models.marian.convert_marian_to_pytorch.find_model_file(dest_dir)
transformers.models.marian.convert_marian_to_pytorch.find_pretrained_model(src_lang:str,tgt_lang:str)->List[str]
transformers.models.marian.convert_marian_to_pytorch.find_vocab_file(model_dir)
transformers.models.marian.convert_marian_to_pytorch.get_system_metadata(repo_root)
transformers.models.marian.convert_marian_to_pytorch.lmap(f,x)->List
transformers.models.marian.convert_marian_to_pytorch.load_config_from_state_dict(opus_dict)
transformers.models.marian.convert_marian_to_pytorch.load_layers_(layer_lst:nn.ModuleList,opus_state:dict,converter,is_decoder=False)
transformers.models.marian.convert_marian_to_pytorch.load_yaml(path)
transformers.models.marian.convert_marian_to_pytorch.make_registry(repo_path='Opus-MT-train/models')
transformers.models.marian.convert_marian_to_pytorch.remove_prefix(text:str,prefix:str)
transformers.models.marian.convert_marian_to_pytorch.remove_suffix(text:str,suffix:str)
transformers.models.marian.convert_marian_to_pytorch.save_json(content:Union[Dict,List],path:str)->None
transformers.models.marian.convert_marian_to_pytorch.save_tokenizer_config(dest_dir:Path)
transformers.models.marian.convert_marian_to_pytorch.unzip(zip_path:str,dest_dir:str)->None
transformers.models.marian.convert_marian_to_pytorch.write_model_card(hf_model_name:str,repo_root=DEFAULT_REPO,save_dir=Path('marian_converted'),dry_run=False,extra_metadata={})->str


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/modeling_tf_marian.py----------------------------------------
A:transformers.models.marian.modeling_tf_marian.logger->utils.logging.get_logger(__name__)
A:transformers.models.marian.modeling_tf_marian.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.marian.modeling_tf_marian.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.marian.modeling_tf_marian.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.marian.modeling_tf_marian.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.marian.modeling_tf_marian.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.marian.modeling_tf_marian.one_cst->tensorflow.constant(1.0)
A:transformers.models.marian.modeling_tf_marian.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.marian.modeling_tf_marian.weight->tensorflow.cast(weight, dtype=self.weight.dtype)
A:transformers.models.marian.modeling_tf_marian.self.weight->self.add_weight(name='embeddings', shape=[self.num_positions, self.embedding_dim])
A:transformers.models.marian.modeling_tf_marian.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.marian.modeling_tf_marian.position_enc[:, 0:dim // 2]->numpy.sin(position_enc[:, 0::2])
A:transformers.models.marian.modeling_tf_marian.position_enc[:, dim // 2:]->numpy.cos(position_enc[:, 1::2])
A:transformers.models.marian.modeling_tf_marian.table->tensorflow.convert_to_tensor(position_enc)
A:transformers.models.marian.modeling_tf_marian.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.marian.modeling_tf_marian.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.marian.modeling_tf_marian.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.marian.modeling_tf_marian.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.marian.modeling_tf_marian.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.marian.modeling_tf_marian.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.marian.modeling_tf_marian.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.marian.modeling_tf_marian.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.marian.modeling_tf_marian.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.marian.modeling_tf_marian.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.marian.modeling_tf_marian.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.marian.modeling_tf_marian.attention_mask->_expand_mask(inputs['attention_mask'])
A:transformers.models.marian.modeling_tf_marian.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.marian.modeling_tf_marian.attn_output->self.out_proj(attn_output)
A:transformers.models.marian.modeling_tf_marian.self.self_attn->TFMarianAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.marian.modeling_tf_marian.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.marian.modeling_tf_marian.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.marian.modeling_tf_marian.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.marian.modeling_tf_marian.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.marian.modeling_tf_marian.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.marian.modeling_tf_marian.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.marian.modeling_tf_marian.(hidden_states, self_attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.marian.modeling_tf_marian.hidden_states->self.dropout(hidden_states + positions, training=inputs['training'])
A:transformers.models.marian.modeling_tf_marian.self.encoder_attn->TFMarianAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.marian.modeling_tf_marian.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.marian.modeling_tf_marian.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.marian.modeling_tf_marian.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.marian.modeling_tf_marian.input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.marian.modeling_tf_marian.decoder_input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.marian.modeling_tf_marian.output->self.call(inputs)
A:transformers.models.marian.modeling_tf_marian.self.embed_positions->TFMarianSinusoidalPositionalEmbedding(config.max_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.marian.modeling_tf_marian.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.marian.modeling_tf_marian.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.marian.modeling_tf_marian.embed_pos->self.embed_positions(input_shape)
A:transformers.models.marian.modeling_tf_marian.dropout_probability->random.uniform(0, 1)
A:transformers.models.marian.modeling_tf_marian.(hidden_states, attn)->encoder_layer(hidden_states, attention_mask, inputs['head_mask'][idx] if inputs['head_mask'] is not None else None)
A:transformers.models.marian.modeling_tf_marian.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.marian.modeling_tf_marian.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.marian.modeling_tf_marian.(hidden_states, layer_self_attn, layer_cross_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, cross_attn_layer_head_mask=inputs['cross_attn_head_mask'][idx] if inputs['cross_attn_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.marian.modeling_tf_marian.all_self_attns->list(all_self_attns)
A:transformers.models.marian.modeling_tf_marian.all_cross_attns->list(all_cross_attns)
A:transformers.models.marian.modeling_tf_marian.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='model.shared')
A:transformers.models.marian.modeling_tf_marian.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.marian.modeling_tf_marian.self.encoder->TFMarianEncoder(config, embed_tokens, name='encoder')
A:transformers.models.marian.modeling_tf_marian.self.decoder->TFMarianDecoder(config, embed_tokens, name='decoder')
A:transformers.models.marian.modeling_tf_marian.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.marian.modeling_tf_marian.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.marian.modeling_tf_marian.self.model->TFMarianMainLayer(config, name='model')
A:transformers.models.marian.modeling_tf_marian.outputs->self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], encoder_outputs=inputs['encoder_outputs'], decoder_attention_mask=inputs['decoder_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.marian.modeling_tf_marian.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.marian.modeling_tf_marian.inputs['labels']->tensorflow.where(inputs['labels'] == self.config.pad_token_id, tf.fill(shape_list(inputs['labels']), -100), inputs['labels'])
A:transformers.models.marian.modeling_tf_marian.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.marian.modeling_tf_marian.lm_logits->self.model.shared(outputs[0], mode='linear')
A:transformers.models.marian.modeling_tf_marian.encoder_outputs->TFBaseModelOutput(last_hidden_state=encoder_outputs)
A:transformers.models.marian.modeling_tf_marian.vocab_range->tensorflow.constant(range(self.config.vocab_size))
A:transformers.models.marian.modeling_tf_marian.logits->tensorflow.where(vocab_range == self.config.pad_token_id, LARGE_NEGATIVE, logits)
transformers.TFMarianMTModel(self,config,*inputs,**kwargs)
transformers.TFMarianMTModel._reorder_cache(past,beam_idx)
transformers.TFMarianMTModel.adjust_logits_during_generation(self,logits,cur_len,max_length,forced_bos_token_id,forced_eos_token_id,**kwargs)
transformers.TFMarianMTModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFMarianMTModel.get_bias(self)
transformers.TFMarianMTModel.get_decoder(self)
transformers.TFMarianMTModel.get_encoder(self)
transformers.TFMarianMTModel.get_output_embeddings(self)
transformers.TFMarianMTModel.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFMarianMTModel.serving_output(self,output)
transformers.TFMarianMTModel.set_bias(self,value)
transformers.TFMarianMTModel.set_output_embeddings(self,value)
transformers.TFMarianModel(self,config:MarianConfig,*inputs,**kwargs)
transformers.TFMarianModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFMarianModel.get_decoder(self)
transformers.TFMarianModel.get_encoder(self)
transformers.TFMarianModel.serving_output(self,output)
transformers.TFMarianPreTrainedModel(TFPreTrainedModel)
transformers.TFMarianPreTrainedModel.dummy_inputs(self)
transformers.TFMarianPreTrainedModel.serving(self,inputs)
transformers.models.marian.modeling_tf_marian.TFMarianAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.marian.modeling_tf_marian.TFMarianAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.marian.modeling_tf_marian.TFMarianDecoder(self,config:MarianConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianDecoder.__init__(self,config:MarianConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianDecoder.get_embed_tokens(self)
transformers.models.marian.modeling_tf_marian.TFMarianDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.marian.modeling_tf_marian.TFMarianDecoderLayer(self,config:MarianConfig,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianDecoderLayer.__init__(self,config:MarianConfig,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,cross_attn_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.marian.modeling_tf_marian.TFMarianEncoder(self,config:MarianConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianEncoder.__init__(self,config:MarianConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianEncoder.get_embed_tokens(self)
transformers.models.marian.modeling_tf_marian.TFMarianEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.marian.modeling_tf_marian.TFMarianEncoderLayer(self,config:MarianConfig,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianEncoderLayer.__init__(self,config:MarianConfig,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,training=False)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel(self,config,*inputs,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.__init__(self,config,*inputs,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel._reorder_cache(past,beam_idx)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.adjust_logits_during_generation(self,logits,cur_len,max_length,forced_bos_token_id,forced_eos_token_id,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.get_bias(self)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.get_decoder(self)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.get_encoder(self)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.get_output_embeddings(self)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.serving_output(self,output)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.set_bias(self,value)
transformers.models.marian.modeling_tf_marian.TFMarianMTModel.set_output_embeddings(self,value)
transformers.models.marian.modeling_tf_marian.TFMarianMainLayer(self,config:MarianConfig,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMainLayer.__init__(self,config:MarianConfig,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianMainLayer.get_input_embeddings(self)
transformers.models.marian.modeling_tf_marian.TFMarianMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.marian.modeling_tf_marian.TFMarianModel(self,config:MarianConfig,*inputs,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianModel.__init__(self,config:MarianConfig,*inputs,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianModel.get_decoder(self)
transformers.models.marian.modeling_tf_marian.TFMarianModel.get_encoder(self)
transformers.models.marian.modeling_tf_marian.TFMarianModel.serving_output(self,output)
transformers.models.marian.modeling_tf_marian.TFMarianPreTrainedModel(TFPreTrainedModel)
transformers.models.marian.modeling_tf_marian.TFMarianPreTrainedModel.dummy_inputs(self)
transformers.models.marian.modeling_tf_marian.TFMarianPreTrainedModel.serving(self,inputs)
transformers.models.marian.modeling_tf_marian.TFMarianSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,**kwargs)
transformers.models.marian.modeling_tf_marian.TFMarianSinusoidalPositionalEmbedding._init_weight(n_pos:int,dim:int)
transformers.models.marian.modeling_tf_marian.TFMarianSinusoidalPositionalEmbedding.build(self,input_shape:tf.TensorShape)
transformers.models.marian.modeling_tf_marian.TFMarianSinusoidalPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.marian.modeling_tf_marian._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.marian.modeling_tf_marian._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.marian.modeling_tf_marian.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/tokenization_marian.py----------------------------------------
A:transformers.models.marian.tokenization_marian.language_code_re->re.compile('>>.+<<')
A:transformers.models.marian.tokenization_marian.self.encoder->load_json(vocab)
A:transformers.models.marian.tokenization_marian.self.spm_source->load_spm(source_spm, self.sp_model_kwargs)
A:transformers.models.marian.tokenization_marian.self.spm_target->load_spm(target_spm, self.sp_model_kwargs)
A:transformers.models.marian.tokenization_marian.match->self.language_code_re.match(text)
A:transformers.models.marian.tokenization_marian.(code, text)->self.remove_language_code(text)
A:transformers.models.marian.tokenization_marian.pieces->self.current_spm.encode(text, out_type=str)
A:transformers.models.marian.tokenization_marian.save_dir->Path(save_directory)
A:transformers.models.marian.tokenization_marian.vocab->self.encoder.copy()
A:transformers.models.marian.tokenization_marian.state->self.__dict__.copy()
A:transformers.models.marian.tokenization_marian.all_special_ids->set(self.all_special_ids)
A:transformers.models.marian.tokenization_marian.spm->sentencepiece.SentencePieceProcessor(**sp_model_kwargs)
transformers.MarianTokenizer(self,vocab,source_spm,target_spm,source_lang=None,target_lang=None,unk_token='<unk>',eos_token='</s>',pad_token='<pad>',model_max_length=512,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.MarianTokenizer.__getstate__(self)->Dict
transformers.MarianTokenizer.__setstate__(self,d:Dict)->None
transformers.MarianTokenizer._convert_id_to_token(self,index:int)->str
transformers.MarianTokenizer._convert_token_to_id(self,token)
transformers.MarianTokenizer._setup_normalizer(self)
transformers.MarianTokenizer._special_token_mask(self,seq)
transformers.MarianTokenizer._tokenize(self,text:str)->List[str]
transformers.MarianTokenizer.as_target_tokenizer(self)
transformers.MarianTokenizer.batch_decode(self,sequences,**kwargs)
transformers.MarianTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.MarianTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.MarianTokenizer.decode(self,token_ids,**kwargs)
transformers.MarianTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.MarianTokenizer.get_vocab(self)->Dict
transformers.MarianTokenizer.normalize(self,x:str)->str
transformers.MarianTokenizer.num_special_tokens_to_add(self,**unused)
transformers.MarianTokenizer.remove_language_code(self,text:str)
transformers.MarianTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.MarianTokenizer.vocab_size(self)->int
transformers.models.marian.tokenization_marian.MarianTokenizer(self,vocab,source_spm,target_spm,source_lang=None,target_lang=None,unk_token='<unk>',eos_token='</s>',pad_token='<pad>',model_max_length=512,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.marian.tokenization_marian.MarianTokenizer.__getstate__(self)->Dict
transformers.models.marian.tokenization_marian.MarianTokenizer.__init__(self,vocab,source_spm,target_spm,source_lang=None,target_lang=None,unk_token='<unk>',eos_token='</s>',pad_token='<pad>',model_max_length=512,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.marian.tokenization_marian.MarianTokenizer.__setstate__(self,d:Dict)->None
transformers.models.marian.tokenization_marian.MarianTokenizer._convert_id_to_token(self,index:int)->str
transformers.models.marian.tokenization_marian.MarianTokenizer._convert_token_to_id(self,token)
transformers.models.marian.tokenization_marian.MarianTokenizer._setup_normalizer(self)
transformers.models.marian.tokenization_marian.MarianTokenizer._special_token_mask(self,seq)
transformers.models.marian.tokenization_marian.MarianTokenizer._tokenize(self,text:str)->List[str]
transformers.models.marian.tokenization_marian.MarianTokenizer.as_target_tokenizer(self)
transformers.models.marian.tokenization_marian.MarianTokenizer.batch_decode(self,sequences,**kwargs)
transformers.models.marian.tokenization_marian.MarianTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.models.marian.tokenization_marian.MarianTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.models.marian.tokenization_marian.MarianTokenizer.decode(self,token_ids,**kwargs)
transformers.models.marian.tokenization_marian.MarianTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.marian.tokenization_marian.MarianTokenizer.get_vocab(self)->Dict
transformers.models.marian.tokenization_marian.MarianTokenizer.normalize(self,x:str)->str
transformers.models.marian.tokenization_marian.MarianTokenizer.num_special_tokens_to_add(self,**unused)
transformers.models.marian.tokenization_marian.MarianTokenizer.remove_language_code(self,text:str)
transformers.models.marian.tokenization_marian.MarianTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.marian.tokenization_marian.MarianTokenizer.vocab_size(self)->int
transformers.models.marian.tokenization_marian.load_json(path:str)->Union[Dict, List]
transformers.models.marian.tokenization_marian.load_spm(path:str,sp_model_kwargs:Dict[str,Any])->sentencepiece.SentencePieceProcessor
transformers.models.marian.tokenization_marian.save_json(data,path:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/modeling_marian.py----------------------------------------
A:transformers.models.marian.modeling_marian.logger->utils.logging.get_logger(__name__)
A:transformers.models.marian.modeling_marian.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.marian.modeling_marian.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.marian.modeling_marian.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.marian.modeling_marian.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.marian.modeling_marian.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.marian.modeling_marian.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.marian.modeling_marian.self.weight->self._init_weight(self.weight)
A:transformers.models.marian.modeling_marian.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.marian.modeling_marian.out[:, 0:sentinel]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.models.marian.modeling_marian.out[:, sentinel:]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.models.marian.modeling_marian.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.marian.modeling_marian.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.marian.modeling_marian.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.marian.modeling_marian.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.marian.modeling_marian.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.marian.modeling_marian.(bsz, tgt_len, embed_dim)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.marian.modeling_marian.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.marian.modeling_marian.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.marian.modeling_marian.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.marian.modeling_marian.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.marian.modeling_marian.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.marian.modeling_marian.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.marian.modeling_marian.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.marian.modeling_marian.attn_output->self.out_proj(attn_output)
A:transformers.models.marian.modeling_marian.self.self_attn->MarianAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.marian.modeling_marian.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.marian.modeling_marian.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.marian.modeling_marian.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.marian.modeling_marian.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.marian.modeling_marian.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.marian.modeling_marian.hidden_states->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
A:transformers.models.marian.modeling_marian.self.encoder_attn->MarianAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.marian.modeling_marian.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.marian.modeling_marian.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.marian.modeling_marian.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.marian.modeling_marian.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.marian.modeling_marian.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.marian.modeling_marian.self.embed_positions->MarianSinusoidalPositionalEmbedding(config.max_position_embeddings, config.d_model, self.padding_idx)
A:transformers.models.marian.modeling_marian.self.layers->torch.nn.ModuleList([MarianDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.marian.modeling_marian.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.marian.modeling_marian.embed_pos->self.embed_positions(input_shape)
A:transformers.models.marian.modeling_marian.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.marian.modeling_marian.dropout_probability->random.uniform(0, 1)
A:transformers.models.marian.modeling_marian.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.marian.modeling_marian.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.marian.modeling_marian.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.marian.modeling_marian.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.marian.modeling_marian.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.marian.modeling_marian.self.encoder->MarianEncoder(config, self.shared)
A:transformers.models.marian.modeling_marian.self.decoder->MarianDecoder(config)
A:transformers.models.marian.modeling_marian.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.marian.modeling_marian.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.marian.modeling_marian.self.model->MarianDecoderWrapper(config)
A:transformers.models.marian.modeling_marian.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.marian.modeling_marian.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.marian.modeling_marian.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.marian.modeling_marian.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.marian.modeling_marian.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.marian.modeling_marian.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.marian.modeling_marian.loss_fct->CrossEntropyLoss()
A:transformers.models.marian.modeling_marian.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.marian.modeling_marian.logits[:, self.config.pad_token_id]->float('-inf')
A:transformers.models.marian.modeling_marian.config->copy.deepcopy(config)
A:transformers.models.marian.modeling_marian.logits->self.lm_head(outputs[0])
A:transformers.models.marian.modeling_marian.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
transformers.MarianForCausalLM(self,config)
transformers.MarianForCausalLM._reorder_cache(past,beam_idx)
transformers.MarianForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MarianForCausalLM.get_decoder(self)
transformers.MarianForCausalLM.get_input_embeddings(self)
transformers.MarianForCausalLM.get_output_embeddings(self)
transformers.MarianForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.MarianForCausalLM.set_decoder(self,decoder)
transformers.MarianForCausalLM.set_input_embeddings(self,value)
transformers.MarianForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.MarianMTModel(self,config:MarianConfig)
transformers.MarianMTModel._reorder_cache(past,beam_idx)
transformers.MarianMTModel._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.MarianMTModel.adjust_logits_during_generation(self,logits,cur_len)
transformers.MarianMTModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MarianMTModel.get_decoder(self)
transformers.MarianMTModel.get_encoder(self)
transformers.MarianMTModel.get_output_embeddings(self)
transformers.MarianMTModel.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.MarianMTModel.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.MarianMTModel.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.MarianMTModel.set_output_embeddings(self,new_embeddings)
transformers.MarianModel(self,config:MarianConfig)
transformers.MarianModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MarianModel.get_decoder(self)
transformers.MarianModel.get_encoder(self)
transformers.MarianModel.get_input_embeddings(self)
transformers.MarianModel.set_input_embeddings(self,value)
transformers.models.marian.MarianPreTrainedModel(PreTrainedModel)
transformers.models.marian.MarianPreTrainedModel._init_weights(self,module)
transformers.models.marian.MarianPreTrainedModel.dummy_inputs(self)
transformers.models.marian.modeling_marian.MarianAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.marian.modeling_marian.MarianAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.marian.modeling_marian.MarianAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.marian.modeling_marian.MarianAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.marian.modeling_marian.MarianDecoder(self,config:MarianConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.marian.modeling_marian.MarianDecoder.__init__(self,config:MarianConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.marian.modeling_marian.MarianDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.marian.modeling_marian.MarianDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.marian.modeling_marian.MarianDecoder.get_input_embeddings(self)
transformers.models.marian.modeling_marian.MarianDecoder.set_input_embeddings(self,value)
transformers.models.marian.modeling_marian.MarianDecoderLayer(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianDecoderLayer.__init__(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.marian.modeling_marian.MarianDecoderWrapper(self,config)
transformers.models.marian.modeling_marian.MarianDecoderWrapper.__init__(self,config)
transformers.models.marian.modeling_marian.MarianDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.marian.modeling_marian.MarianEncoder(self,config:MarianConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.marian.modeling_marian.MarianEncoder.__init__(self,config:MarianConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.marian.modeling_marian.MarianEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.marian.modeling_marian.MarianEncoderLayer(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianEncoderLayer.__init__(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.marian.modeling_marian.MarianForCausalLM(self,config)
transformers.models.marian.modeling_marian.MarianForCausalLM.__init__(self,config)
transformers.models.marian.modeling_marian.MarianForCausalLM._reorder_cache(past,beam_idx)
transformers.models.marian.modeling_marian.MarianForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.marian.modeling_marian.MarianForCausalLM.get_decoder(self)
transformers.models.marian.modeling_marian.MarianForCausalLM.get_input_embeddings(self)
transformers.models.marian.modeling_marian.MarianForCausalLM.get_output_embeddings(self)
transformers.models.marian.modeling_marian.MarianForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.marian.modeling_marian.MarianForCausalLM.set_decoder(self,decoder)
transformers.models.marian.modeling_marian.MarianForCausalLM.set_input_embeddings(self,value)
transformers.models.marian.modeling_marian.MarianForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.marian.modeling_marian.MarianMTModel(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianMTModel.__init__(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianMTModel._reorder_cache(past,beam_idx)
transformers.models.marian.modeling_marian.MarianMTModel._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.marian.modeling_marian.MarianMTModel.adjust_logits_during_generation(self,logits,cur_len)
transformers.models.marian.modeling_marian.MarianMTModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.marian.modeling_marian.MarianMTModel.get_decoder(self)
transformers.models.marian.modeling_marian.MarianMTModel.get_encoder(self)
transformers.models.marian.modeling_marian.MarianMTModel.get_output_embeddings(self)
transformers.models.marian.modeling_marian.MarianMTModel.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.marian.modeling_marian.MarianMTModel.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.marian.modeling_marian.MarianMTModel.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.marian.modeling_marian.MarianMTModel.set_output_embeddings(self,new_embeddings)
transformers.models.marian.modeling_marian.MarianModel(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianModel.__init__(self,config:MarianConfig)
transformers.models.marian.modeling_marian.MarianModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.marian.modeling_marian.MarianModel.get_decoder(self)
transformers.models.marian.modeling_marian.MarianModel.get_encoder(self)
transformers.models.marian.modeling_marian.MarianModel.get_input_embeddings(self)
transformers.models.marian.modeling_marian.MarianModel.set_input_embeddings(self,value)
transformers.models.marian.modeling_marian.MarianPreTrainedModel(PreTrainedModel)
transformers.models.marian.modeling_marian.MarianPreTrainedModel._init_weights(self,module)
transformers.models.marian.modeling_marian.MarianPreTrainedModel.dummy_inputs(self)
transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding._init_weight(out:nn.Parameter)
transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.marian.modeling_marian._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.marian.modeling_marian._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.marian.modeling_marian.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/convert_marian_tatoeba_to_pytorch.py----------------------------------------
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.DEFAULT_MODEL_DIR->os.path.join(DEFAULT_REPO, 'models')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.reg->self.make_tatoeba_registry()
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.reg_df->reg_df.set_index('id').set_index('id')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.released['fname']->released['url_base'].apply(lambda x: remove_suffix(remove_prefix(x, 'https://object.pouta.csc.fi/Tatoeba-Challenge/opus'), '.zip'))
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.released['2m']->released.fname.str.startswith('2m')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.released['date']->pandas.to_datetime(released['fname'].apply(lambda x: remove_prefix(remove_prefix(x, '2m-'), '-')))
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.released['base_ext']->released.url_base.apply(lambda x: Path(x).name)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.reg_df['base_ext']->reg_df.set_index('id').set_index('id').url_model.apply(lambda x: Path(x).name)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.metadata_new->metadata_new.drop(DROP_COLS_BOTH, 1).drop(DROP_COLS_BOTH, 1)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.metadata_new['src_alpha2']->metadata_new.drop(DROP_COLS_BOTH, 1).drop(DROP_COLS_BOTH, 1).short_pair.apply(lambda x: x.split('-')[0])
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.metadata_new['tgt_alpha2']->metadata_new.drop(DROP_COLS_BOTH, 1).drop(DROP_COLS_BOTH, 1).short_pair.apply(lambda x: x.split('-')[1])
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.metadata_new['prefer_old']->metadata_new.drop(DROP_COLS_BOTH, 1).drop(DROP_COLS_BOTH, 1).long_pair.isin([])
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.self.metadata->self.metadata.set_index('short_pair')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.mapper->pandas.read_csv(LANG_CODE_PATH)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.self.iso_table->pandas.read_csv(ISO_PATH, sep='\t').rename(columns=lambda x: x.lower())
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.more_3_to_2->self.iso_table.set_index('id').part1.dropna().to_dict()
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.self.model_card_dir->Path(save_dir)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.converted_paths->convert_all_sentencepiece_models(entries_to_convert, dest_dir=self.model_card_dir)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.long_pair->remove_prefix(path.name, 'opus-mt-').split('-')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.new_p_src->self.get_two_letter_code(long_pair[0])
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.new_p_tgt->self.get_two_letter_code(long_pair[1])
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.new_path->path.parent.joinpath(hf_model_id)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.group->self.expand_group_to_two_letter_codes(code)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.(src, tgt)->remove_prefix(hf_model_id, 'opus-mt-').split('-')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.(src_tags, src_multilingual)->self.get_tags(src, r.src_name)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.(tgt_tags, tgt_multilingual)->self.get_tags(tgt, r.tgt_name)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.short_pair->remove_prefix(hf_model_id, 'opus-mt-')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.extra_metadata->self.metadata.loc[short_pair].drop('2m')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.(lang_tags, src_multilingual, tgt_multilingual)->self.resolve_lang_code(extra_metadata)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.opus_readme_path->Path(repo_root).joinpath('models', opus_name, 'README.md')
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.lang_tags->l2front_matter(lang_tags)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.content->'*'.join(splat)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.items->'\n\n'.join([f'- {k}: {v}' for (k, v) in metadata.items()])
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.lns->list(open(p / 'README.md').readlines())
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.results[p.name]->_parse_readme(lns)
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.models.marian.convert_marian_tatoeba_to_pytorch.resolver->TatoebaConverter(save_dir=args.save_dir)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter(self,save_dir='marian_converted')
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.__init__(self,save_dir='marian_converted')
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.convert_models(self,tatoeba_ids,dry_run=False)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.download_metadata(self)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.expand_group_to_two_letter_codes(self,grp_name)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.get_tags(self,code,ref_name)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.get_two_letter_code(self,three_letter_code)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.make_tatoeba_registry(repo_path=DEFAULT_MODEL_DIR)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.resolve_lang_code(self,r)->Tuple[List[str], str, str]
transformers.models.marian.convert_marian_tatoeba_to_pytorch.TatoebaConverter.write_model_card(self,hf_model_id:str,repo_root=DEFAULT_REPO,dry_run=False)->str
transformers.models.marian.convert_marian_tatoeba_to_pytorch.dedup(lst)
transformers.models.marian.convert_marian_tatoeba_to_pytorch.l2front_matter(langs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/configuration_marian.py----------------------------------------
A:transformers.models.marian.configuration_marian.logger->utils.logging.get_logger(__name__)
transformers.MarianConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=58100,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=58100,eos_token_id=0,forced_eos_token_id=0,**kwargs)
transformers.MarianConfig.hidden_size(self)->int
transformers.MarianConfig.num_attention_heads(self)->int
transformers.models.marian.configuration_marian.MarianConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=58100,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=58100,eos_token_id=0,forced_eos_token_id=0,**kwargs)
transformers.models.marian.configuration_marian.MarianConfig.__init__(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=58100,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=58100,eos_token_id=0,forced_eos_token_id=0,**kwargs)
transformers.models.marian.configuration_marian.MarianConfig.hidden_size(self)->int
transformers.models.marian.configuration_marian.MarianConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/marian/__init__.py----------------------------------------
A:transformers.models.marian.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/convert_pegasus_tf_to_pytorch.py----------------------------------------
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.k->k.replace(pegasus_name, hf_name).replace(pegasus_name, hf_name)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.cfg_kwargs->transformers.models.pegasus.configuration_pegasus.DEFAULTS.copy()
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.cfg->PegasusConfig(**cfg_kwargs)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.torch_model->convert_pegasus(tf_weights, cfg_updates)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.sd->convert_pegasus(tf_weights, cfg_updates).state_dict()
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.new_k->rename_state_dict_key(k)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.mapping[new_k]->torch.tensor(v, dtype=sd[new_k].dtype)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.mapping['shared.weight'][cfg.pad_token_id]->torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.(missing, extra)->convert_pegasus(tf_weights, cfg_updates).model.load_state_dict(mapping, strict=False)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.init_vars->tensorflow.train.list_variables(path)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.skip_key->any([pat in name for pat in ignore_name])
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.array->tensorflow.train.load_variable(path, name)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.tok->transformers.PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.tf_weights->get_tf_weights_as_numpy(ckpt_path)
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.models.pegasus.convert_pegasus_tf_to_pytorch.args.save_dir->os.path.join('pegasus', dataset)
transformers.models.pegasus.convert_pegasus_tf_to_pytorch.convert_pegasus(tf_weights:dict,cfg_updates:dict)->PegasusForConditionalGeneration
transformers.models.pegasus.convert_pegasus_tf_to_pytorch.convert_pegasus_ckpt_to_pytorch(ckpt_path:str,save_dir:str)
transformers.models.pegasus.convert_pegasus_tf_to_pytorch.get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000')->Dict
transformers.models.pegasus.convert_pegasus_tf_to_pytorch.rename_state_dict_key(k)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/configuration_pegasus.py----------------------------------------
A:transformers.models.pegasus.configuration_pegasus.logger->utils.logging.get_logger(__name__)
transformers.PegasusConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=0,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,eos_token_id=1,forced_eos_token_id=1,**kwargs)
transformers.PegasusConfig.hidden_size(self)->int
transformers.PegasusConfig.num_attention_heads(self)->int
transformers.models.pegasus.configuration_pegasus.PegasusConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=0,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,eos_token_id=1,forced_eos_token_id=1,**kwargs)
transformers.models.pegasus.configuration_pegasus.PegasusConfig.__init__(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,use_cache=True,is_encoder_decoder=True,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=0,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,pad_token_id=0,eos_token_id=1,forced_eos_token_id=1,**kwargs)
transformers.models.pegasus.configuration_pegasus.PegasusConfig.hidden_size(self)->int
transformers.models.pegasus.configuration_pegasus.PegasusConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/modeling_pegasus.py----------------------------------------
A:transformers.models.pegasus.modeling_pegasus.logger->utils.logging.get_logger(__name__)
A:transformers.models.pegasus.modeling_pegasus.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.pegasus.modeling_pegasus.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.pegasus.modeling_pegasus.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.pegasus.modeling_pegasus.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.pegasus.modeling_pegasus.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.pegasus.modeling_pegasus.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.pegasus.modeling_pegasus.self.weight->self._init_weight(self.weight)
A:transformers.models.pegasus.modeling_pegasus.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.pegasus.modeling_pegasus.out[:, 0:sentinel]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.models.pegasus.modeling_pegasus.out[:, sentinel:]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.models.pegasus.modeling_pegasus.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.pegasus.modeling_pegasus.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.pegasus.modeling_pegasus.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.pegasus.modeling_pegasus.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.pegasus.modeling_pegasus.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.pegasus.modeling_pegasus.(bsz, tgt_len, embed_dim)->self.layer_norm(hidden_states).size()
A:transformers.models.pegasus.modeling_pegasus.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.pegasus.modeling_pegasus.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.pegasus.modeling_pegasus.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.pegasus.modeling_pegasus.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.pegasus.modeling_pegasus.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.pegasus.modeling_pegasus.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.pegasus.modeling_pegasus.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.pegasus.modeling_pegasus.attn_output->self.out_proj(attn_output)
A:transformers.models.pegasus.modeling_pegasus.self.self_attn->PegasusAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.pegasus.modeling_pegasus.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.pegasus.modeling_pegasus.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.pegasus.modeling_pegasus.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.pegasus.modeling_pegasus.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.pegasus.modeling_pegasus.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.pegasus.modeling_pegasus.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.pegasus.modeling_pegasus.self.encoder_attn->PegasusAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.pegasus.modeling_pegasus.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.pegasus.modeling_pegasus.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.pegasus.modeling_pegasus.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.pegasus.modeling_pegasus.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.pegasus.modeling_pegasus.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.pegasus.modeling_pegasus.self.embed_positions->PegasusSinusoidalPositionalEmbedding(config.max_position_embeddings, config.d_model, self.padding_idx)
A:transformers.models.pegasus.modeling_pegasus.self.layers->torch.nn.ModuleList([PegasusDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.pegasus.modeling_pegasus.self.layer_norm->torch.nn.LayerNorm(config.d_model)
A:transformers.models.pegasus.modeling_pegasus.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.pegasus.modeling_pegasus.embed_pos->self.embed_positions(input_shape)
A:transformers.models.pegasus.modeling_pegasus.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.pegasus.modeling_pegasus.dropout_probability->random.uniform(0, 1)
A:transformers.models.pegasus.modeling_pegasus.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.pegasus.modeling_pegasus.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.pegasus.modeling_pegasus.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.pegasus.modeling_pegasus.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.pegasus.modeling_pegasus.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.pegasus.modeling_pegasus.self.encoder->PegasusEncoder(config, self.shared)
A:transformers.models.pegasus.modeling_pegasus.self.decoder->PegasusDecoder(config)
A:transformers.models.pegasus.modeling_pegasus.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.pegasus.modeling_pegasus.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.pegasus.modeling_pegasus.self.model->PegasusDecoderWrapper(config)
A:transformers.models.pegasus.modeling_pegasus.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.pegasus.modeling_pegasus.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.pegasus.modeling_pegasus.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.pegasus.modeling_pegasus.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.pegasus.modeling_pegasus.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.pegasus.modeling_pegasus.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.pegasus.modeling_pegasus.loss_fct->CrossEntropyLoss()
A:transformers.models.pegasus.modeling_pegasus.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.pegasus.modeling_pegasus.config->copy.deepcopy(config)
A:transformers.models.pegasus.modeling_pegasus.logits->self.lm_head(outputs[0])
A:transformers.models.pegasus.modeling_pegasus.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
transformers.PegasusForCausalLM(self,config)
transformers.PegasusForCausalLM._reorder_cache(past,beam_idx)
transformers.PegasusForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.PegasusForCausalLM.get_decoder(self)
transformers.PegasusForCausalLM.get_input_embeddings(self)
transformers.PegasusForCausalLM.get_output_embeddings(self)
transformers.PegasusForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.PegasusForCausalLM.set_decoder(self,decoder)
transformers.PegasusForCausalLM.set_input_embeddings(self,value)
transformers.PegasusForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.PegasusForConditionalGeneration(self,config:PegasusConfig)
transformers.PegasusForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.PegasusForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.PegasusForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.PegasusForConditionalGeneration.get_decoder(self)
transformers.PegasusForConditionalGeneration.get_encoder(self)
transformers.PegasusForConditionalGeneration.get_output_embeddings(self)
transformers.PegasusForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.PegasusForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.PegasusForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.PegasusForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.PegasusModel(self,config:PegasusConfig)
transformers.PegasusModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.PegasusModel.get_decoder(self)
transformers.PegasusModel.get_encoder(self)
transformers.PegasusModel.get_input_embeddings(self)
transformers.PegasusModel.set_input_embeddings(self,value)
transformers.PegasusPreTrainedModel(PreTrainedModel)
transformers.PegasusPreTrainedModel._init_weights(self,module)
transformers.PegasusPreTrainedModel.dummy_inputs(self)
transformers.models.pegasus.modeling_pegasus.PegasusAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.pegasus.modeling_pegasus.PegasusAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.pegasus.modeling_pegasus.PegasusAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.pegasus.modeling_pegasus.PegasusAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.pegasus.modeling_pegasus.PegasusDecoder(self,config:PegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.pegasus.modeling_pegasus.PegasusDecoder.__init__(self,config:PegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.pegasus.modeling_pegasus.PegasusDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.pegasus.modeling_pegasus.PegasusDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.pegasus.modeling_pegasus.PegasusDecoder.get_input_embeddings(self)
transformers.models.pegasus.modeling_pegasus.PegasusDecoder.set_input_embeddings(self,value)
transformers.models.pegasus.modeling_pegasus.PegasusDecoderLayer(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusDecoderLayer.__init__(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.pegasus.modeling_pegasus.PegasusDecoderWrapper(self,config)
transformers.models.pegasus.modeling_pegasus.PegasusDecoderWrapper.__init__(self,config)
transformers.models.pegasus.modeling_pegasus.PegasusDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.pegasus.modeling_pegasus.PegasusEncoder(self,config:PegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.pegasus.modeling_pegasus.PegasusEncoder.__init__(self,config:PegasusConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.pegasus.modeling_pegasus.PegasusEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.pegasus.modeling_pegasus.PegasusEncoderLayer(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusEncoderLayer.__init__(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM(self,config)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.__init__(self,config)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM._reorder_cache(past,beam_idx)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.get_decoder(self)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.get_input_embeddings(self)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.get_output_embeddings(self)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.set_decoder(self,decoder)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.set_input_embeddings(self,value)
transformers.models.pegasus.modeling_pegasus.PegasusForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.__init__(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.get_decoder(self)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.get_encoder(self)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.get_output_embeddings(self)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.pegasus.modeling_pegasus.PegasusForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.pegasus.modeling_pegasus.PegasusModel(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusModel.__init__(self,config:PegasusConfig)
transformers.models.pegasus.modeling_pegasus.PegasusModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.pegasus.modeling_pegasus.PegasusModel.get_decoder(self)
transformers.models.pegasus.modeling_pegasus.PegasusModel.get_encoder(self)
transformers.models.pegasus.modeling_pegasus.PegasusModel.get_input_embeddings(self)
transformers.models.pegasus.modeling_pegasus.PegasusModel.set_input_embeddings(self,value)
transformers.models.pegasus.modeling_pegasus.PegasusPreTrainedModel(PreTrainedModel)
transformers.models.pegasus.modeling_pegasus.PegasusPreTrainedModel._init_weights(self,module)
transformers.models.pegasus.modeling_pegasus.PegasusPreTrainedModel.dummy_inputs(self)
transformers.models.pegasus.modeling_pegasus.PegasusSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.pegasus.modeling_pegasus.PegasusSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,padding_idx:Optional[int]=None)
transformers.models.pegasus.modeling_pegasus.PegasusSinusoidalPositionalEmbedding._init_weight(out:nn.Parameter)
transformers.models.pegasus.modeling_pegasus.PegasusSinusoidalPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.pegasus.modeling_pegasus._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.pegasus.modeling_pegasus._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.pegasus.modeling_pegasus.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/__init__.py----------------------------------------
A:transformers.models.pegasus.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/tokenization_pegasus_fast.py----------------------------------------
A:transformers.models.pegasus.tokenization_pegasus_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.pegasus.tokenization_pegasus_fast.all_special_ids->set(self.all_special_ids)
A:transformers.models.pegasus.tokenization_pegasus_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.PegasusTokenizerFast(self,vocab_file,tokenizer_file=None,pad_token='<pad>',eos_token='</s>',unk_token='<unk>',mask_token='<mask_2>',mask_token_sent='<mask_1>',additional_special_tokens=None,offset=103,**kwargs)
transformers.PegasusTokenizerFast._special_token_mask(self,seq)
transformers.PegasusTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.PegasusTokenizerFast.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PegasusTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast(self,vocab_file,tokenizer_file=None,pad_token='<pad>',eos_token='</s>',unk_token='<unk>',mask_token='<mask_2>',mask_token_sent='<mask_1>',additional_special_tokens=None,offset=103,**kwargs)
transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,pad_token='<pad>',eos_token='</s>',unk_token='<unk>',mask_token='<mask_2>',mask_token_sent='<mask_1>',additional_special_tokens=None,offset=103,**kwargs)
transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast._special_token_mask(self,seq)
transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.pegasus.tokenization_pegasus_fast.PegasusTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/modeling_tf_pegasus.py----------------------------------------
A:transformers.models.pegasus.modeling_tf_pegasus.logger->utils.logging.get_logger(__name__)
A:transformers.models.pegasus.modeling_tf_pegasus.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.pegasus.modeling_tf_pegasus.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.pegasus.modeling_tf_pegasus.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.pegasus.modeling_tf_pegasus.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.pegasus.modeling_tf_pegasus.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.pegasus.modeling_tf_pegasus.one_cst->tensorflow.constant(1.0)
A:transformers.models.pegasus.modeling_tf_pegasus.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.pegasus.modeling_tf_pegasus.weight->tensorflow.cast(weight, dtype=self.weight.dtype)
A:transformers.models.pegasus.modeling_tf_pegasus.self.weight->self.add_weight(name='embeddings', shape=[self.num_positions, self.embedding_dim])
A:transformers.models.pegasus.modeling_tf_pegasus.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.models.pegasus.modeling_tf_pegasus.position_enc[:, 0:dim // 2]->numpy.sin(position_enc[:, 0::2])
A:transformers.models.pegasus.modeling_tf_pegasus.position_enc[:, dim // 2:]->numpy.cos(position_enc[:, 1::2])
A:transformers.models.pegasus.modeling_tf_pegasus.table->tensorflow.convert_to_tensor(position_enc)
A:transformers.models.pegasus.modeling_tf_pegasus.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.pegasus.modeling_tf_pegasus.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.pegasus.modeling_tf_pegasus.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.pegasus.modeling_tf_pegasus.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.pegasus.modeling_tf_pegasus.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.pegasus.modeling_tf_pegasus.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.pegasus.modeling_tf_pegasus.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.pegasus.modeling_tf_pegasus.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.pegasus.modeling_tf_pegasus.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.pegasus.modeling_tf_pegasus.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.pegasus.modeling_tf_pegasus.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.pegasus.modeling_tf_pegasus.attention_mask->_expand_mask(inputs['attention_mask'])
A:transformers.models.pegasus.modeling_tf_pegasus.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.pegasus.modeling_tf_pegasus.attn_output->self.out_proj(attn_output)
A:transformers.models.pegasus.modeling_tf_pegasus.self.self_attn->TFPegasusAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.pegasus.modeling_tf_pegasus.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.pegasus.modeling_tf_pegasus.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.pegasus.modeling_tf_pegasus.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.pegasus.modeling_tf_pegasus.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.pegasus.modeling_tf_pegasus.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.pegasus.modeling_tf_pegasus.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.pegasus.modeling_tf_pegasus.hidden_states->self.layer_norm(hidden_states)
A:transformers.models.pegasus.modeling_tf_pegasus.(hidden_states, self_attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.pegasus.modeling_tf_pegasus.self.encoder_attn->TFPegasusAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.pegasus.modeling_tf_pegasus.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.pegasus.modeling_tf_pegasus.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.pegasus.modeling_tf_pegasus.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.pegasus.modeling_tf_pegasus.input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.pegasus.modeling_tf_pegasus.decoder_input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.pegasus.modeling_tf_pegasus.output->self.call(inputs)
A:transformers.models.pegasus.modeling_tf_pegasus.self.embed_positions->TFPegasusSinusoidalPositionalEmbedding(config.max_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.pegasus.modeling_tf_pegasus.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layer_norm')
A:transformers.models.pegasus.modeling_tf_pegasus.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.pegasus.modeling_tf_pegasus.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.pegasus.modeling_tf_pegasus.embed_pos->self.embed_positions(input_shape)
A:transformers.models.pegasus.modeling_tf_pegasus.dropout_probability->random.uniform(0, 1)
A:transformers.models.pegasus.modeling_tf_pegasus.(hidden_states, attn)->encoder_layer(hidden_states, attention_mask, inputs['head_mask'][idx] if inputs['head_mask'] is not None else None)
A:transformers.models.pegasus.modeling_tf_pegasus.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.pegasus.modeling_tf_pegasus.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.pegasus.modeling_tf_pegasus.(hidden_states, layer_self_attn, layer_cross_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, cross_attn_layer_head_mask=inputs['cross_attn_head_mask'][idx] if inputs['cross_attn_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.pegasus.modeling_tf_pegasus.all_self_attns->list(all_self_attns)
A:transformers.models.pegasus.modeling_tf_pegasus.all_cross_attns->list(all_cross_attns)
A:transformers.models.pegasus.modeling_tf_pegasus.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='model.shared')
A:transformers.models.pegasus.modeling_tf_pegasus.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.pegasus.modeling_tf_pegasus.self.encoder->TFPegasusEncoder(config, embed_tokens, name='encoder')
A:transformers.models.pegasus.modeling_tf_pegasus.self.decoder->TFPegasusDecoder(config, embed_tokens, name='decoder')
A:transformers.models.pegasus.modeling_tf_pegasus.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.pegasus.modeling_tf_pegasus.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.pegasus.modeling_tf_pegasus.self.model->TFPegasusMainLayer(config, name='model')
A:transformers.models.pegasus.modeling_tf_pegasus.outputs->self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], encoder_outputs=inputs['encoder_outputs'], decoder_attention_mask=inputs['decoder_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.pegasus.modeling_tf_pegasus.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.pegasus.modeling_tf_pegasus.inputs['labels']->tensorflow.where(inputs['labels'] == self.config.pad_token_id, tf.fill(shape_list(inputs['labels']), -100), inputs['labels'])
A:transformers.models.pegasus.modeling_tf_pegasus.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.pegasus.modeling_tf_pegasus.lm_logits->self.model.shared(outputs[0], mode='linear')
A:transformers.models.pegasus.modeling_tf_pegasus.encoder_outputs->TFBaseModelOutput(last_hidden_state=encoder_outputs)
transformers.TFPegasusForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFPegasusForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.TFPegasusForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFPegasusForConditionalGeneration.get_bias(self)
transformers.TFPegasusForConditionalGeneration.get_decoder(self)
transformers.TFPegasusForConditionalGeneration.get_encoder(self)
transformers.TFPegasusForConditionalGeneration.get_output_embeddings(self)
transformers.TFPegasusForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFPegasusForConditionalGeneration.serving_output(self,output)
transformers.TFPegasusForConditionalGeneration.set_bias(self,value)
transformers.TFPegasusForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFPegasusModel(self,config:PegasusConfig,*inputs,**kwargs)
transformers.TFPegasusModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFPegasusModel.get_decoder(self)
transformers.TFPegasusModel.get_encoder(self)
transformers.TFPegasusModel.serving_output(self,output)
transformers.TFPegasusPreTrainedModel(TFPreTrainedModel)
transformers.TFPegasusPreTrainedModel.dummy_inputs(self)
transformers.TFPegasusPreTrainedModel.serving(self,inputs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoder(self,config:PegasusConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoder.__init__(self,config:PegasusConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoder.get_embed_tokens(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoderLayer(self,config:PegasusConfig,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoderLayer.__init__(self,config:PegasusConfig,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,cross_attn_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoder(self,config:PegasusConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoder.__init__(self,config:PegasusConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoder.get_embed_tokens(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoderLayer(self,config:PegasusConfig,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoderLayer.__init__(self,config:PegasusConfig,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,training=False)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.get_bias(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.get_decoder(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.get_encoder(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.get_output_embeddings(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.serving_output(self,output)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.set_bias(self,value)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusMainLayer(self,config:PegasusConfig,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusMainLayer.__init__(self,config:PegasusConfig,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusMainLayer.get_input_embeddings(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusModel(self,config:PegasusConfig,*inputs,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusModel.__init__(self,config:PegasusConfig,*inputs,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusModel.get_decoder(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusModel.get_encoder(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusModel.serving_output(self,output)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusPreTrainedModel(TFPreTrainedModel)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusPreTrainedModel.dummy_inputs(self)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusPreTrainedModel.serving(self,inputs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusSinusoidalPositionalEmbedding(self,num_positions:int,embedding_dim:int,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusSinusoidalPositionalEmbedding.__init__(self,num_positions:int,embedding_dim:int,**kwargs)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusSinusoidalPositionalEmbedding._init_weight(n_pos:int,dim:int)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusSinusoidalPositionalEmbedding.build(self,input_shape:tf.TensorShape)
transformers.models.pegasus.modeling_tf_pegasus.TFPegasusSinusoidalPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.pegasus.modeling_tf_pegasus._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.pegasus.modeling_tf_pegasus._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.pegasus.modeling_tf_pegasus.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/pegasus/tokenization_pegasus.py----------------------------------------
A:transformers.models.pegasus.tokenization_pegasus.logger->utils.logging.get_logger(__name__)
A:transformers.models.pegasus.tokenization_pegasus.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.pegasus.tokenization_pegasus.state->self.__dict__.copy()
A:transformers.models.pegasus.tokenization_pegasus.sp_id->self.sp_model.piece_to_id(token)
A:transformers.models.pegasus.tokenization_pegasus.token->self.sp_model.IdToPiece(index - self.offset)
A:transformers.models.pegasus.tokenization_pegasus.out_string->self.sp_model.decode_pieces(tokens)
A:transformers.models.pegasus.tokenization_pegasus.all_special_ids->set(self.all_special_ids)
A:transformers.models.pegasus.tokenization_pegasus.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.PegasusTokenizer(self,vocab_file,pad_token='<pad>',eos_token='</s>',unk_token='<unk>',mask_token='<mask_2>',mask_token_sent='<mask_1>',additional_special_tokens=None,offset=103,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.PegasusTokenizer.__getstate__(self)
transformers.PegasusTokenizer.__setstate__(self,d)
transformers.PegasusTokenizer._convert_id_to_token(self,index:int)->str
transformers.PegasusTokenizer._convert_token_to_id(self,token:str)->int
transformers.PegasusTokenizer._special_token_mask(self,seq)
transformers.PegasusTokenizer._tokenize(self,text:str)->List[str]
transformers.PegasusTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.PegasusTokenizer.convert_tokens_to_string(self,tokens)
transformers.PegasusTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PegasusTokenizer.get_vocab(self)->Dict[str, int]
transformers.PegasusTokenizer.num_special_tokens_to_add(self,pair=False)
transformers.PegasusTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.PegasusTokenizer.vocab_size(self)->int
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer(self,vocab_file,pad_token='<pad>',eos_token='</s>',unk_token='<unk>',mask_token='<mask_2>',mask_token_sent='<mask_1>',additional_special_tokens=None,offset=103,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.__getstate__(self)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.__init__(self,vocab_file,pad_token='<pad>',eos_token='</s>',unk_token='<unk>',mask_token='<mask_2>',mask_token_sent='<mask_1>',additional_special_tokens=None,offset=103,sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.__setstate__(self,d)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer._convert_id_to_token(self,index:int)->str
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer._convert_token_to_id(self,token:str)->int
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer._special_token_mask(self,seq)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer._tokenize(self,text:str)->List[str]
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.get_vocab(self)->Dict[str, int]
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.num_special_tokens_to_add(self,pair=False)
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.pegasus.tokenization_pegasus.PegasusTokenizer.vocab_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/barthez/tokenization_barthez.py----------------------------------------
A:transformers.models.barthez.tokenization_barthez.logger->utils.logging.get_logger(__name__)
A:transformers.models.barthez.tokenization_barthez.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.barthez.tokenization_barthez.spm_id->self.sp_model.PieceToId(token)
A:transformers.models.barthez.tokenization_barthez.state->self.__dict__.copy()
A:transformers.models.barthez.tokenization_barthez.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.barthez.tokenization_barthez.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.BarthezTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.BarthezTokenizer.__getstate__(self)
transformers.BarthezTokenizer.__setstate__(self,d)
transformers.BarthezTokenizer._convert_id_to_token(self,index)
transformers.BarthezTokenizer._convert_token_to_id(self,token)
transformers.BarthezTokenizer._tokenize(self,text:str)->List[str]
transformers.BarthezTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BarthezTokenizer.convert_tokens_to_string(self,tokens)
transformers.BarthezTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BarthezTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.BarthezTokenizer.get_vocab(self)
transformers.BarthezTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.BarthezTokenizer.vocab_size(self)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.__getstate__(self)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.__setstate__(self,d)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer._convert_id_to_token(self,index)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer._convert_token_to_id(self,token)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer._tokenize(self,text:str)->List[str]
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.get_vocab(self)
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.barthez.tokenization_barthez.BarthezTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/barthez/tokenization_barthez_fast.py----------------------------------------
A:transformers.models.barthez.tokenization_barthez_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.barthez.tokenization_barthez_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.BarthezTokenizerFast(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.BarthezTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BarthezTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BarthezTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.barthez.tokenization_barthez_fast.BarthezTokenizerFast(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.barthez.tokenization_barthez_fast.BarthezTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.barthez.tokenization_barthez_fast.BarthezTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.barthez.tokenization_barthez_fast.BarthezTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.barthez.tokenization_barthez_fast.BarthezTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/barthez/__init__.py----------------------------------------
A:transformers.models.barthez.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dialogpt/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.dialogpt.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.d->torch.load(checkpoint_path)
A:transformers.models.dialogpt.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.d[NEW_KEY]->torch.load(checkpoint_path).pop(OLD_KEY)
A:transformers.models.dialogpt.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.dialogpt.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.models.dialogpt.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.checkpoint_path->os.path.join(args.dialogpt_path, f'{MODEL}_ft.pkl')
transformers.models.dialogpt.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.convert_dialogpt_checkpoint(checkpoint_path:str,pytorch_dump_folder_path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.big_bird.convert_bigbird_original_tf_checkpoint_to_pytorch.config->transformers.BigBirdConfig.from_json_file(big_bird_config_file)
A:transformers.models.big_bird.convert_bigbird_original_tf_checkpoint_to_pytorch.model->BigBirdForPreTraining(config)
A:transformers.models.big_bird.convert_bigbird_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.big_bird.convert_bigbird_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.big_bird.convert_bigbird_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,big_bird_config_file,pytorch_dump_path,is_trivia_qa)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/configuration_big_bird.py----------------------------------------
A:transformers.models.big_bird.configuration_big_bird.logger->utils.logging.get_logger(__name__)
transformers.BigBirdConfig(self,vocab_size=50358,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu_new',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=4096,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,use_cache=True,is_encoder_decoder=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,sep_token_id=66,attention_type='block_sparse',use_bias=True,rescale_embeddings=False,block_size=64,num_random_blocks=3,gradient_checkpointing=False,**kwargs)
transformers.models.big_bird.configuration_big_bird.BigBirdConfig(self,vocab_size=50358,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu_new',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=4096,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,use_cache=True,is_encoder_decoder=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,sep_token_id=66,attention_type='block_sparse',use_bias=True,rescale_embeddings=False,block_size=64,num_random_blocks=3,gradient_checkpointing=False,**kwargs)
transformers.models.big_bird.configuration_big_bird.BigBirdConfig.__init__(self,vocab_size=50358,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu_new',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=4096,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,use_cache=True,is_encoder_decoder=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,sep_token_id=66,attention_type='block_sparse',use_bias=True,rescale_embeddings=False,block_size=64,num_random_blocks=3,gradient_checkpointing=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/tokenization_big_bird.py----------------------------------------
A:transformers.models.big_bird.tokenization_big_bird.logger->utils.logging.get_logger(__name__)
A:transformers.models.big_bird.tokenization_big_bird.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.big_bird.tokenization_big_bird.state->self.__dict__.copy()
A:transformers.models.big_bird.tokenization_big_bird.token->self.sp_model.IdToPiece(index)
A:transformers.models.big_bird.tokenization_big_bird.out_string->self.sp_model.decode_pieces(tokens)
A:transformers.models.big_bird.tokenization_big_bird.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.BigBirdTokenizer(self,vocab_file,unk_token='<unk>',bos_token='<s>',eos_token='</s>',pad_token='<pad>',sep_token='[SEP]',mask_token='[MASK]',cls_token='[CLS]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.BigBirdTokenizer.__getstate__(self)
transformers.BigBirdTokenizer.__setstate__(self,d)
transformers.BigBirdTokenizer._convert_id_to_token(self,index)
transformers.BigBirdTokenizer._convert_token_to_id(self,token)
transformers.BigBirdTokenizer._tokenize(self,text:str)->List[str]
transformers.BigBirdTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BigBirdTokenizer.convert_tokens_to_string(self,tokens)
transformers.BigBirdTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BigBirdTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.BigBirdTokenizer.get_vocab(self)
transformers.BigBirdTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.BigBirdTokenizer.vocab_size(self)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer(self,vocab_file,unk_token='<unk>',bos_token='<s>',eos_token='</s>',pad_token='<pad>',sep_token='[SEP]',mask_token='[MASK]',cls_token='[CLS]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.__getstate__(self)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.__init__(self,vocab_file,unk_token='<unk>',bos_token='<s>',eos_token='</s>',pad_token='<pad>',sep_token='[SEP]',mask_token='[MASK]',cls_token='[CLS]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.__setstate__(self,d)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer._convert_id_to_token(self,index)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer._convert_token_to_id(self,token)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer._tokenize(self,text:str)->List[str]
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.get_vocab(self)
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.big_bird.tokenization_big_bird.BigBirdTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/modeling_big_bird.py----------------------------------------
A:transformers.models.big_bird.modeling_big_bird.logger->utils.logging.get_logger(__name__)
A:transformers.models.big_bird.modeling_big_bird.array->array.reshape(pointer.shape).reshape(pointer.shape)
A:transformers.models.big_bird.modeling_big_bird.name->txt_name.split('/')
A:transformers.models.big_bird.modeling_big_bird.name_items->var.name.split('/')
A:transformers.models.big_bird.modeling_big_bird.layer_name_items->name_items[0].split('_')
A:transformers.models.big_bird.modeling_big_bird.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.big_bird.modeling_big_bird.pt_names->list(model.state_dict().keys())
A:transformers.models.big_bird.modeling_big_bird.(names, tf_weights)->load_tf_weights_bert(init_vars, tf_path)
A:transformers.models.big_bird.modeling_big_bird.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.big_bird.modeling_big_bird.pointer->getattr(pointer, 'weight')
A:transformers.models.big_bird.modeling_big_bird.num->int(scope_names[1])
A:transformers.models.big_bird.modeling_big_bird.pt_weight_name->'.'.join(pt_name)
A:transformers.models.big_bird.modeling_big_bird.pointer.data->torch.from_numpy(array)
A:transformers.models.big_bird.modeling_big_bird.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.big_bird.modeling_big_bird.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.big_bird.modeling_big_bird.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.big_bird.modeling_big_bird.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.big_bird.modeling_big_bird.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.big_bird.modeling_big_bird.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.big_bird.modeling_big_bird.input_shape->torch.cat([input_ids, dummy_token], dim=1).size()
A:transformers.models.big_bird.modeling_big_bird.buffered_token_type_ids_expanded->buffered_token_type_ids.expand(batch_size, seq_length)
A:transformers.models.big_bird.modeling_big_bird.token_type_ids->(~logits_mask).long()
A:transformers.models.big_bird.modeling_big_bird.inputs_embeds->torch.cat([inputs_embeds, inputs_embeds_padding], dim=-2)
A:transformers.models.big_bird.modeling_big_bird.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.big_bird.modeling_big_bird.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.big_bird.modeling_big_bird.embeddings->self.LayerNorm(embeddings)
A:transformers.models.big_bird.modeling_big_bird.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.big_bird.modeling_big_bird.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)
A:transformers.models.big_bird.modeling_big_bird.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)
A:transformers.models.big_bird.modeling_big_bird.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)
A:transformers.models.big_bird.modeling_big_bird.x->self.out_proj(x)
A:transformers.models.big_bird.modeling_big_bird.mixed_query_layer->self.query(hidden_states)
A:transformers.models.big_bird.modeling_big_bird.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.big_bird.modeling_big_bird.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.big_bird.modeling_big_bird.query_layer->self.transpose_for_scores(self.query(hidden_states))
A:transformers.models.big_bird.modeling_big_bird.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.big_bird.modeling_big_bird.attention_probs->torch.zeros(bsz, n_heads, from_seq_len, to_seq_len, dtype=torch.float, device=context_layer.device)
A:transformers.models.big_bird.modeling_big_bird.context_layer->torch.transpose(context_layer, 1, 2)
A:transformers.models.big_bird.modeling_big_bird.(batch_size, seqlen, _)->self.qa_outputs(hidden_states).size()
A:transformers.models.big_bird.modeling_big_bird.(context_layer, attention_probs)->self.bigbird_block_sparse_attention(query_layer, key_layer, value_layer, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, self.num_attention_heads, self.num_random_blocks, self.attention_head_size, from_block_size, to_block_size, batch_size, from_seq_length, to_seq_length, seed=self.seed, plan_from_length=None, plan_num_rand_blocks=None, output_attentions=output_attentions)
A:transformers.models.big_bird.modeling_big_bird.(plan_from_length, plan_num_rand_blocks)->self._get_rand_attn_plan(from_seq_len, from_block_size, n_rand_blocks)
A:transformers.models.big_bird.modeling_big_bird.rand_attn->numpy.zeros((from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)
A:transformers.models.big_bird.modeling_big_bird.rand_mask->torch.einsum('blq,bhlk->bhlqk', from_blocked_mask[:, 1:-1], rand_mask)
A:transformers.models.big_bird.modeling_big_bird.blocked_query_matrix->self.transpose_for_scores(self.query(hidden_states)).view(bsz, n_heads, from_seq_len // from_block_size, from_block_size, -1)
A:transformers.models.big_bird.modeling_big_bird.blocked_key_matrix->self.transpose_for_scores(self.key(hidden_states)).view(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)
A:transformers.models.big_bird.modeling_big_bird.blocked_value_matrix->self.transpose_for_scores(self.value(hidden_states)).view(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)
A:transformers.models.big_bird.modeling_big_bird.gathered_key->gathered_key.view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1).view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1)
A:transformers.models.big_bird.modeling_big_bird.gathered_value->gathered_value.view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1).view(bsz, n_heads, to_seq_len // to_block_size - 2, n_rand_blocks * to_block_size, -1)
A:transformers.models.big_bird.modeling_big_bird.first_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, 0], key_layer, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.first_attn_weights->torch.nn.functional.softmax(first_product, dim=-1)
A:transformers.models.big_bird.modeling_big_bird.first_context_layer->self.torch_bmm_nd(first_attn_weights, value_layer, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.second_key_mat->torch.cat([blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, 1], blocked_key_matrix[:, :, 2], blocked_key_matrix[:, :, -1], gathered_key[:, :, 0]], dim=2)
A:transformers.models.big_bird.modeling_big_bird.second_value_mat->torch.cat([blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, 1], blocked_value_matrix[:, :, 2], blocked_value_matrix[:, :, -1], gathered_value[:, :, 0]], dim=2)
A:transformers.models.big_bird.modeling_big_bird.second_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, 1], second_key_mat, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.second_seq_pad->torch.cat([to_mask[:, :, :, :3 * to_block_size], to_mask[:, :, :, -to_block_size:], to_mask.new_ones([bsz, 1, 1, n_rand_blocks * to_block_size])], dim=3)
A:transformers.models.big_bird.modeling_big_bird.second_rand_pad->torch.cat([rand_mask.new_ones([bsz, n_heads, from_block_size, 4 * to_block_size]), rand_mask[:, :, 0]], dim=3)
A:transformers.models.big_bird.modeling_big_bird.second_attn_weights->torch.nn.functional.softmax(second_product, dim=-1)
A:transformers.models.big_bird.modeling_big_bird.second_context_layer->self.torch_bmm_nd(second_attn_weights, second_value_mat, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.exp_blocked_key_matrix->torch.cat([blocked_key_matrix[:, :, 1:-3], blocked_key_matrix[:, :, 2:-2], blocked_key_matrix[:, :, 3:-1]], dim=3)
A:transformers.models.big_bird.modeling_big_bird.exp_blocked_value_matrix->torch.cat([blocked_value_matrix[:, :, 1:-3], blocked_value_matrix[:, :, 2:-2], blocked_value_matrix[:, :, 3:-1]], dim=3)
A:transformers.models.big_bird.modeling_big_bird.inner_band_product->self.torch_bmm_nd_transpose(middle_query_matrix, exp_blocked_key_matrix, ndim=5)
A:transformers.models.big_bird.modeling_big_bird.rand_band_product->self.torch_bmm_nd_transpose(middle_query_matrix, gathered_key[:, :, 1:-1], ndim=5)
A:transformers.models.big_bird.modeling_big_bird.first_band_product->torch.einsum('bhlqd,bhkd->bhlqk', middle_query_matrix, blocked_key_matrix[:, :, 0])
A:transformers.models.big_bird.modeling_big_bird.last_band_product->torch.einsum('bhlqd,bhkd->bhlqk', middle_query_matrix, blocked_key_matrix[:, :, -1])
A:transformers.models.big_bird.modeling_big_bird.band_product->torch.cat([first_band_product, inner_band_product, rand_band_product, last_band_product], dim=-1)
A:transformers.models.big_bird.modeling_big_bird.attn_weights->BigBirdBlockSparseAttention(self.config, self.seed)
A:transformers.models.big_bird.modeling_big_bird.second_last_key_mat->torch.cat([blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, -3], blocked_key_matrix[:, :, -2], blocked_key_matrix[:, :, -1], gathered_key[:, :, -1]], dim=2)
A:transformers.models.big_bird.modeling_big_bird.second_last_value_mat->torch.cat([blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, -3], blocked_value_matrix[:, :, -2], blocked_value_matrix[:, :, -1], gathered_value[:, :, -1]], dim=2)
A:transformers.models.big_bird.modeling_big_bird.second_last_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -2], second_last_key_mat, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.second_last_seq_pad->torch.cat([to_mask[:, :, :, :to_block_size], to_mask[:, :, :, -3 * to_block_size:], to_mask.new_ones([bsz, 1, 1, n_rand_blocks * to_block_size])], dim=3)
A:transformers.models.big_bird.modeling_big_bird.second_last_rand_pad->torch.cat([rand_mask.new_ones([bsz, n_heads, from_block_size, 4 * to_block_size]), rand_mask[:, :, -1]], dim=3)
A:transformers.models.big_bird.modeling_big_bird.second_last_attn_weights->torch.nn.functional.softmax(second_last_product, dim=-1)
A:transformers.models.big_bird.modeling_big_bird.second_last_context_layer->self.torch_bmm_nd(second_last_attn_weights, second_last_value_mat, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.last_product->self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -1], key_layer, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.last_attn_weights->torch.nn.functional.softmax(last_product, dim=-1)
A:transformers.models.big_bird.modeling_big_bird.last_context_layer->self.torch_bmm_nd(last_attn_weights, value_layer, ndim=4)
A:transformers.models.big_bird.modeling_big_bird.attn_probs_view->torch.zeros(bsz, n_heads, from_seq_len, to_seq_len, dtype=torch.float, device=context_layer.device).view(bsz, n_heads, from_seq_len // from_block_size, from_block_size, to_seq_len // to_block_size, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.attn_probs_view[p1, p2, 1, :, i2[0]]->right_slice.view(from_block_size, n_rand_blocks, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.attn_probs_view[:, :, q_idx, :, q_idx:q_idx + 3, :]->right_slice.view(bsz, n_heads, from_block_size, 3, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.attention_probs[:, :, 2 * from_block_size:-2 * from_block_size, :to_block_size]->attn_weights[:, :, :, :, :to_block_size].view(bsz, n_heads, -1, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.attention_probs[:, :, 2 * from_block_size:-2 * from_block_size, -to_block_size:]->attn_weights[:, :, :, :, -to_block_size:].view(bsz, n_heads, -1, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.attn_probs_view[p1, p2, q_idx + 1, :, i2[q_idx]]->right_slice.view(from_block_size, n_rand_blocks, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.attn_probs_view[p1, p2, -2, :, i2[-1]]->right_slice.view(from_block_size, n_rand_blocks, to_block_size)
A:transformers.models.big_bird.modeling_big_bird.flattened_params->params.reshape(-1, params.shape[-2], params.shape[-1])
A:transformers.models.big_bird.modeling_big_bird.out_flattened->params.reshape(-1, params.shape[-2], params.shape[-1]).index_select(0, flattened_indices)
A:transformers.models.big_bird.modeling_big_bird.out->params.reshape(-1, params.shape[-2], params.shape[-1]).index_select(0, flattened_indices).reshape(params.shape[:2] + (num_indices_to_gather,) + params.shape[3:])
A:transformers.models.big_bird.modeling_big_bird.middle_seq->numpy.arange(1, to_seq_length // to_block_size - 1, dtype=np.int32)
A:transformers.models.big_bird.modeling_big_bird.max_plan_idx->plan_from_length.index(from_seq_length)
A:transformers.models.big_bird.modeling_big_bird.rnd_r_cnt->int(np.sum(plan_num_rand_blocks[:plan_idx]))
A:transformers.models.big_bird.modeling_big_bird.curr_r_cnt->int(np.sum(plan_num_rand_blocks[:plan_idx + 1]))
A:transformers.models.big_bird.modeling_big_bird.rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt]->self._get_single_block_row_attention(block_id=blk_rw_idx, to_start_block_id=to_start_block_id, to_end_block_id=plan_block_length[plan_idx], num_rand_blocks=plan_num_rand_blocks[plan_idx], window_block_left=window_block_left, window_block_right=window_block_right, global_block_left=global_block_left, global_block_right=global_block_right)
A:transformers.models.big_bird.modeling_big_bird.to_block_list->numpy.arange(to_start_block_id, to_end_block_id, dtype=np.int32)
A:transformers.models.big_bird.modeling_big_bird.perm_block->numpy.random.permutation(to_block_list)
A:transformers.models.big_bird.modeling_big_bird.illegal_blocks->list(range(block_id - window_block_left, block_id + window_block_right + 1))
A:transformers.models.big_bird.modeling_big_bird.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.big_bird.modeling_big_bird.hidden_states->self.qa_outputs(hidden_states)
A:transformers.models.big_bird.modeling_big_bird.self.self->BigBirdBlockSparseAttention(config, seed)
A:transformers.models.big_bird.modeling_big_bird.self.output->BigBirdOutput(config)
A:transformers.models.big_bird.modeling_big_bird.self_outputs->self.self(hidden_states, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions)
A:transformers.models.big_bird.modeling_big_bird.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.big_bird.modeling_big_bird.self.attention->BigBirdAttention(config, seed=seed)
A:transformers.models.big_bird.modeling_big_bird.self.crossattention->BigBirdAttention(config)
A:transformers.models.big_bird.modeling_big_bird.self.intermediate->BigBirdIntermediate(config)
A:transformers.models.big_bird.modeling_big_bird.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_value=self_attn_past_key_value, output_attentions=output_attentions, band_mask=band_mask, from_mask=from_mask, to_mask=to_mask, from_blocked_mask=blocked_encoder_mask, to_blocked_mask=blocked_encoder_mask)
A:transformers.models.big_bird.modeling_big_bird.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.big_bird.modeling_big_bird.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.big_bird.modeling_big_bird.intermediate_output->self.intermediate(attention_output)
A:transformers.models.big_bird.modeling_big_bird.self.layer->torch.nn.ModuleList([BigBirdLayer(config, seed=layer_idx) for layer_idx in range(config.num_hidden_layers)])
A:transformers.models.big_bird.modeling_big_bird.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_value, output_attentions)
A:transformers.models.big_bird.modeling_big_bird.self.transform->BigBirdPredictionHeadTransform(config)
A:transformers.models.big_bird.modeling_big_bird.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.big_bird.modeling_big_bird.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.big_bird.modeling_big_bird.self.predictions->BigBirdLMPredictionHead(config)
A:transformers.models.big_bird.modeling_big_bird.prediction_scores->self.cls(sequence_output)
A:transformers.models.big_bird.modeling_big_bird.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.models.big_bird.modeling_big_bird.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.big_bird.modeling_big_bird.self.embeddings->BigBirdEmbeddings(config)
A:transformers.models.big_bird.modeling_big_bird.self.encoder->BigBirdEncoder(config)
A:transformers.models.big_bird.modeling_big_bird.self.pooler->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.big_bird.modeling_big_bird.self.activation->torch.nn.Tanh()
A:transformers.models.big_bird.modeling_big_bird.attention_mask->torch.cat([input_ids, dummy_token], dim=1).new_ones(input_shape)
A:transformers.models.big_bird.modeling_big_bird.(padding_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds)->self._pad_to_block_size(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, pad_token_id=self.config.pad_token_id)
A:transformers.models.big_bird.modeling_big_bird.(blocked_encoder_mask, band_mask, from_mask, to_mask)->self.create_masks_for_block_sparse_attn(attention_mask, self.block_size)
A:transformers.models.big_bird.modeling_big_bird.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.big_bird.modeling_big_bird.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.big_bird.modeling_big_bird.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.big_bird.modeling_big_bird.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.big_bird.modeling_big_bird.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, past_key_values_length=past_key_values_length)
A:transformers.models.big_bird.modeling_big_bird.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, band_mask=band_mask, from_mask=from_mask, to_mask=to_mask, blocked_encoder_mask=blocked_encoder_mask, return_dict=return_dict)
A:transformers.models.big_bird.modeling_big_bird.(batch_size, seq_length)->torch.cat([input_ids, dummy_token], dim=1).new_ones(input_shape).size()
A:transformers.models.big_bird.modeling_big_bird.exp_blocked_to_pad->torch.cat([to_blocked_mask[:, 1:-3], to_blocked_mask[:, 2:-2], to_blocked_mask[:, 3:-1]], dim=2)
A:transformers.models.big_bird.modeling_big_bird.band_mask->create_band_mask_from_inputs(blocked_encoder_mask, blocked_encoder_mask)
A:transformers.models.big_bird.modeling_big_bird.blocked_encoder_mask->torch.cat([input_ids, dummy_token], dim=1).new_ones(input_shape).view(batch_size, seq_length // block_size, block_size)
A:transformers.models.big_bird.modeling_big_bird.from_mask->torch.cat([input_ids, dummy_token], dim=1).new_ones(input_shape).view(batch_size, 1, seq_length, 1)
A:transformers.models.big_bird.modeling_big_bird.to_mask->torch.cat([input_ids, dummy_token], dim=1).new_ones(input_shape).view(batch_size, 1, 1, seq_length)
A:transformers.models.big_bird.modeling_big_bird.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.models.big_bird.modeling_big_bird.position_ids->torch.nn.functional.pad(position_ids, (0, padding_len), value=pad_token_id)
A:transformers.models.big_bird.modeling_big_bird.input_ids_padding->torch.cat([inputs_embeds, inputs_embeds_padding], dim=-2).new_full((batch_size, padding_len), self.config.pad_token_id, dtype=torch.long)
A:transformers.models.big_bird.modeling_big_bird.inputs_embeds_padding->self.embeddings(input_ids_padding)
A:transformers.models.big_bird.modeling_big_bird.self.bert->BigBirdModel(config, add_pooling_layer=add_pooling_layer)
A:transformers.models.big_bird.modeling_big_bird.self.cls->BigBirdOnlyMLMHead(config)
A:transformers.models.big_bird.modeling_big_bird.outputs->self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.big_bird.modeling_big_bird.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.models.big_bird.modeling_big_bird.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.big_bird.modeling_big_bird.total_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.big_bird.modeling_big_bird.next_sentence_loss->loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
A:transformers.models.big_bird.modeling_big_bird.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.big_bird.modeling_big_bird.dummy_token->torch.full((effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.models.big_bird.modeling_big_bird.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.models.big_bird.modeling_big_bird.labels->labels[:, 1:].contiguous()
A:transformers.models.big_bird.modeling_big_bird.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.big_bird.modeling_big_bird.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.big_bird.modeling_big_bird.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.big_bird.modeling_big_bird.logits->self.qa_classifier(sequence_output)
A:transformers.models.big_bird.modeling_big_bird.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.big_bird.modeling_big_bird.pooled_output->self.dropout(pooled_output)
A:transformers.models.big_bird.modeling_big_bird.reshaped_logits->self.qa_classifier(sequence_output).view(-1, num_choices)
A:transformers.models.big_bird.modeling_big_bird.sequence_output->self.dropout(sequence_output)
A:transformers.models.big_bird.modeling_big_bird.active_logits->self.qa_classifier(sequence_output).view(-1, self.num_labels)
A:transformers.models.big_bird.modeling_big_bird.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.big_bird.modeling_big_bird.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.big_bird.modeling_big_bird.self.qa_classifier->BigBirdForQuestionAnsweringHead(config)
A:transformers.models.big_bird.modeling_big_bird.logits_mask->self.prepare_question_mask(question_lengths, seqlen)
A:transformers.models.big_bird.modeling_big_bird.(start_logits, end_logits)->self.qa_classifier(sequence_output).split(1, dim=-1)
A:transformers.models.big_bird.modeling_big_bird.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.big_bird.modeling_big_bird.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.big_bird.modeling_big_bird.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.big_bird.modeling_big_bird.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.big_bird.modeling_big_bird.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.big_bird.modeling_big_bird.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.big_bird.modeling_big_bird.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.big_bird.modeling_big_bird.mask->torch.arange(0, maxlen).to(q_lengths.device)
transformers.BigBirdForCausalLM(self,config)
transformers.BigBirdForCausalLM._reorder_cache(self,past,beam_idx)
transformers.BigBirdForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdForCausalLM.get_output_embeddings(self)
transformers.BigBirdForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.BigBirdForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.BigBirdForMaskedLM(self,config)
transformers.BigBirdForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdForMaskedLM.get_output_embeddings(self)
transformers.BigBirdForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.BigBirdForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.BigBirdForMultipleChoice(self,config)
transformers.BigBirdForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdForPreTraining(self,config)
transformers.BigBirdForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdForPreTraining.get_output_embeddings(self)
transformers.BigBirdForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.BigBirdForPreTrainingOutput(ModelOutput)
transformers.BigBirdForQuestionAnswering(self,config,add_pooling_layer=False)
transformers.BigBirdForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,question_lengths=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdForQuestionAnswering.prepare_question_mask(q_lengths:torch.Tensor,maxlen:int)
transformers.BigBirdForQuestionAnsweringHead(self,config)
transformers.BigBirdForQuestionAnsweringHead.forward(self,encoder_output)
transformers.BigBirdForQuestionAnsweringModelOutput(ModelOutput)
transformers.BigBirdForSequenceClassification(self,config)
transformers.BigBirdForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdForTokenClassification(self,config)
transformers.BigBirdForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdLayer(self,config,seed=None)
transformers.BigBirdLayer.feed_forward_chunk(self,attention_output)
transformers.BigBirdLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,band_mask=None,from_mask=None,to_mask=None,blocked_encoder_mask=None,past_key_value=None,output_attentions=False)
transformers.BigBirdLayer.set_attention_type(self,value:str)
transformers.BigBirdModel(self,config,add_pooling_layer=True)
transformers.BigBirdModel._pad_to_block_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,token_type_ids:torch.Tensor,position_ids:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.BigBirdModel.create_masks_for_block_sparse_attn(attention_mask:torch.Tensor,block_size:int)
transformers.BigBirdModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BigBirdModel.get_input_embeddings(self)
transformers.BigBirdModel.set_attention_type(self,value:str)
transformers.BigBirdModel.set_input_embeddings(self,value)
transformers.BigBirdPreTrainedModel(PreTrainedModel)
transformers.BigBirdPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_big_bird(model,tf_checkpoint_path,is_trivia_qa=False)
transformers.models.big_bird.modeling_big_bird.BigBirdAttention(self,config,seed=None)
transformers.models.big_bird.modeling_big_bird.BigBirdAttention.__init__(self,config,seed=None)
transformers.models.big_bird.modeling_big_bird.BigBirdAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False,band_mask=None,from_mask=None,to_mask=None,from_blocked_mask=None,to_blocked_mask=None)
transformers.models.big_bird.modeling_big_bird.BigBirdAttention.set_attention_type(self,value:str)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention(self,config,seed=None)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.__init__(self,config,seed=None)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention._bigbird_block_rand_mask(from_seq_length,to_seq_length,from_block_size,to_block_size,num_rand_blocks,last_idx=-1)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention._bigbird_block_rand_mask_with_head(self,from_seq_length,to_seq_length,from_block_size,to_block_size,num_heads,plan_from_length,plan_num_rand_blocks,window_block_left=1,window_block_right=1,global_block_top=1,global_block_bottom=1,global_block_left=1,global_block_right=1)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention._create_rand_mask_from_inputs(from_blocked_mask,to_blocked_mask,rand_attn,num_attention_heads,num_rand_blocks,batch_size,from_seq_length,from_block_size)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention._get_rand_attn_plan(from_seq_length,from_block_size,num_rand_blocks)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention._get_single_block_row_attention(block_id,to_start_block_id,to_end_block_id,num_rand_blocks,window_block_left=1,window_block_right=1,global_block_left=1,global_block_right=1)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.bigbird_block_sparse_attention(self,query_layer,key_layer,value_layer,band_mask,from_mask,to_mask,from_blocked_mask,to_blocked_mask,n_heads,n_rand_blocks,attention_head_size,from_block_size,to_block_size,batch_size,from_seq_len,to_seq_len,seed,plan_from_length,plan_num_rand_blocks,output_attentions)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.forward(self,hidden_states,band_mask=None,from_mask=None,to_mask=None,from_blocked_mask=None,to_blocked_mask=None,output_attentions=None)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.torch_bmm_nd(inp_1,inp_2,ndim=None)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.torch_bmm_nd_transpose(inp_1,inp_2,ndim=None)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.torch_gather_b2(params,indices)
transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention.transpose_for_scores(self,x)
transformers.models.big_bird.modeling_big_bird.BigBirdClassificationHead(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdClassificationHead.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdClassificationHead.forward(self,features,**kwargs)
transformers.models.big_bird.modeling_big_bird.BigBirdEmbeddings(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdEmbeddings.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.big_bird.modeling_big_bird.BigBirdEncoder(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdEncoder.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,band_mask=None,from_mask=None,to_mask=None,blocked_encoder_mask=None,return_dict=True)
transformers.models.big_bird.modeling_big_bird.BigBirdEncoder.set_attention_type(self,value:str)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM._reorder_cache(self,past,beam_idx)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM.get_output_embeddings(self)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.models.big_bird.modeling_big_bird.BigBirdForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.big_bird.modeling_big_bird.BigBirdForMaskedLM(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForMaskedLM.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdForMaskedLM.get_output_embeddings(self)
transformers.models.big_bird.modeling_big_bird.BigBirdForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.models.big_bird.modeling_big_bird.BigBirdForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.big_bird.modeling_big_bird.BigBirdForMultipleChoice(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForMultipleChoice.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdForPreTraining(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForPreTraining.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdForPreTraining.get_output_embeddings(self)
transformers.models.big_bird.modeling_big_bird.BigBirdForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.models.big_bird.modeling_big_bird.BigBirdForPreTrainingOutput(ModelOutput)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnswering(self,config,add_pooling_layer=False)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnswering.__init__(self,config,add_pooling_layer=False)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,question_lengths=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnswering.prepare_question_mask(q_lengths:torch.Tensor,maxlen:int)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnsweringHead(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnsweringHead.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnsweringHead.forward(self,encoder_output)
transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnsweringModelOutput(ModelOutput)
transformers.models.big_bird.modeling_big_bird.BigBirdForSequenceClassification(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForSequenceClassification.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdForTokenClassification(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForTokenClassification.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdIntermediate(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdIntermediate.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdIntermediate.forward(self,hidden_states)
transformers.models.big_bird.modeling_big_bird.BigBirdLMPredictionHead(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdLMPredictionHead.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdLMPredictionHead.forward(self,hidden_states)
transformers.models.big_bird.modeling_big_bird.BigBirdLayer(self,config,seed=None)
transformers.models.big_bird.modeling_big_bird.BigBirdLayer.__init__(self,config,seed=None)
transformers.models.big_bird.modeling_big_bird.BigBirdLayer.feed_forward_chunk(self,attention_output)
transformers.models.big_bird.modeling_big_bird.BigBirdLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,band_mask=None,from_mask=None,to_mask=None,blocked_encoder_mask=None,past_key_value=None,output_attentions=False)
transformers.models.big_bird.modeling_big_bird.BigBirdLayer.set_attention_type(self,value:str)
transformers.models.big_bird.modeling_big_bird.BigBirdModel(self,config,add_pooling_layer=True)
transformers.models.big_bird.modeling_big_bird.BigBirdModel.__init__(self,config,add_pooling_layer=True)
transformers.models.big_bird.modeling_big_bird.BigBirdModel._pad_to_block_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,token_type_ids:torch.Tensor,position_ids:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.models.big_bird.modeling_big_bird.BigBirdModel.create_masks_for_block_sparse_attn(attention_mask:torch.Tensor,block_size:int)
transformers.models.big_bird.modeling_big_bird.BigBirdModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.big_bird.modeling_big_bird.BigBirdModel.get_input_embeddings(self)
transformers.models.big_bird.modeling_big_bird.BigBirdModel.set_attention_type(self,value:str)
transformers.models.big_bird.modeling_big_bird.BigBirdModel.set_input_embeddings(self,value)
transformers.models.big_bird.modeling_big_bird.BigBirdOnlyMLMHead(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdOnlyMLMHead.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdOnlyMLMHead.forward(self,sequence_output)
transformers.models.big_bird.modeling_big_bird.BigBirdOnlyNSPHead(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdOnlyNSPHead.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdOnlyNSPHead.forward(self,pooled_output)
transformers.models.big_bird.modeling_big_bird.BigBirdOutput(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdOutput.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdOutput.forward(self,hidden_states,input_tensor)
transformers.models.big_bird.modeling_big_bird.BigBirdPreTrainedModel(PreTrainedModel)
transformers.models.big_bird.modeling_big_bird.BigBirdPreTrainedModel._init_weights(self,module)
transformers.models.big_bird.modeling_big_bird.BigBirdPreTrainingHeads(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdPreTrainingHeads.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.models.big_bird.modeling_big_bird.BigBirdPredictionHeadTransform(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdPredictionHeadTransform.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdPredictionHeadTransform.forward(self,hidden_states)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfAttention(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfAttention.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfAttention.transpose_for_scores(self,x)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfOutput(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfOutput.__init__(self,config)
transformers.models.big_bird.modeling_big_bird.BigBirdSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.big_bird.modeling_big_bird.load_tf_weights_in_big_bird(model,tf_checkpoint_path,is_trivia_qa=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/tokenization_big_bird_fast.py----------------------------------------
A:transformers.models.big_bird.tokenization_big_bird_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.big_bird.tokenization_big_bird_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.BigBirdTokenizerFast(self,vocab_file,tokenizer_file=None,unk_token='<unk>',bos_token='<s>',eos_token='</s>',pad_token='<pad>',sep_token='[SEP]',mask_token='[MASK]',cls_token='[CLS]',**kwargs)
transformers.BigBirdTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BigBirdTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BigBirdTokenizerFast.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.BigBirdTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast(self,vocab_file,tokenizer_file=None,unk_token='<unk>',bos_token='<s>',eos_token='</s>',pad_token='<pad>',sep_token='[SEP]',mask_token='[MASK]',cls_token='[CLS]',**kwargs)
transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,unk_token='<unk>',bos_token='<s>',eos_token='</s>',pad_token='<pad>',sep_token='[SEP]',mask_token='[MASK]',cls_token='[CLS]',**kwargs)
transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.big_bird.tokenization_big_bird_fast.BigBirdTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/__init__.py----------------------------------------
A:transformers.models.big_bird.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/big_bird/modeling_flax_big_bird.py----------------------------------------
A:transformers.models.big_bird.modeling_flax_big_bird.logger->utils.logging.get_logger(__name__)
A:transformers.models.big_bird.modeling_flax_big_bird.self.word_embeddings->flax.linen.Embed(self.config.vocab_size, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.position_embeddings->flax.linen.Embed(self.config.max_position_embeddings, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.token_type_embeddings->flax.linen.Embed(self.config.type_vocab_size, self.config.hidden_size, embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range), dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.LayerNorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.dropout->flax.linen.Dropout(rate=self.config.hidden_dropout_prob)
A:transformers.models.big_bird.modeling_flax_big_bird.inputs_embeds->self.word_embeddings(input_ids.astype('i4'))
A:transformers.models.big_bird.modeling_flax_big_bird.position_embeds->self.position_embeddings(position_ids.astype('i4'))
A:transformers.models.big_bird.modeling_flax_big_bird.token_type_embeddings->self.token_type_embeddings(token_type_ids.astype('i4'))
A:transformers.models.big_bird.modeling_flax_big_bird.hidden_states->self.qa_outputs(hidden_states)
A:transformers.models.big_bird.modeling_flax_big_bird.self.query->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, use_bias=self.config.use_bias, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.big_bird.modeling_flax_big_bird.self.key->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, use_bias=self.config.use_bias, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.big_bird.modeling_flax_big_bird.self.value->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, use_bias=self.config.use_bias, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.big_bird.modeling_flax_big_bird.query_states->self.query(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.big_bird.modeling_flax_big_bird.value_states->self.value(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.big_bird.modeling_flax_big_bird.key_states->self.key(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.big_bird.modeling_flax_big_bird.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.big_bird.modeling_flax_big_bird.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, -10000000000.0).astype(self.dtype))
A:transformers.models.big_bird.modeling_flax_big_bird.dropout_rng->self.make_rng('dropout')
A:transformers.models.big_bird.modeling_flax_big_bird.attn_weights->jax.nn.softmax(band_product, axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.attn_output->attn_output.reshape(attn_output.shape[:2] + (-1,)).reshape(attn_output.shape[:2] + (-1,))
A:transformers.models.big_bird.modeling_flax_big_bird.x->self.out_proj(x)
A:transformers.models.big_bird.modeling_flax_big_bird.(blocked_encoder_mask, band_mask, from_mask, to_mask)->self.create_masks_for_block_sparse_attn(attention_mask, self.config.block_size)
A:transformers.models.big_bird.modeling_flax_big_bird.query_layer->self.transpose_for_scores(self.query(hidden_states), n_heads, head_size)
A:transformers.models.big_bird.modeling_flax_big_bird.key_layer->self.transpose_for_scores(self.key(hidden_states), n_heads, head_size)
A:transformers.models.big_bird.modeling_flax_big_bird.value_layer->self.transpose_for_scores(self.value(hidden_states), n_heads, head_size)
A:transformers.models.big_bird.modeling_flax_big_bird.(attn_output, attn_weights)->self.bigbird_block_sparse_attention(query_layer, key_layer, value_layer, band_mask, from_mask, to_mask, blocked_encoder_mask, blocked_encoder_mask, n_heads, head_size, plan_from_length=None, plan_num_rand_blocks=None, output_attentions=output_attentions)
A:transformers.models.big_bird.modeling_flax_big_bird.exp_blocked_to_pad->jax.numpy.concatenate([to_blocked_mask[:, 1:-3], to_blocked_mask[:, 2:-2], to_blocked_mask[:, 3:-1]], axis=2)
A:transformers.models.big_bird.modeling_flax_big_bird.band_mask->create_band_mask_from_inputs(blocked_encoder_mask, blocked_encoder_mask)
A:transformers.models.big_bird.modeling_flax_big_bird.blocked_encoder_mask->jax.numpy.ones_like(input_ids).reshape(batch_size, seq_length // block_size, block_size)
A:transformers.models.big_bird.modeling_flax_big_bird.from_mask->jax.numpy.ones_like(input_ids).reshape(batch_size, 1, seq_length, 1)
A:transformers.models.big_bird.modeling_flax_big_bird.to_mask->jax.numpy.ones_like(input_ids).reshape(batch_size, 1, 1, seq_length)
A:transformers.models.big_bird.modeling_flax_big_bird.(plan_from_length, plan_num_rand_blocks)->self._get_rand_attn_plan(from_seq_len, from_block_size, n_rand_blocks)
A:transformers.models.big_bird.modeling_flax_big_bird.rand_attn->numpy.zeros((from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)
A:transformers.models.big_bird.modeling_flax_big_bird.rand_mask->jax.numpy.einsum('blq,bhlk->bhlqk', from_blocked_mask[:, 1:-1], rand_mask)
A:transformers.models.big_bird.modeling_flax_big_bird.blocked_query_matrix->self.transpose_for_scores(self.query(hidden_states), n_heads, head_size).reshape(bsz, n_heads, from_seq_len // from_block_size, from_block_size, -1)
A:transformers.models.big_bird.modeling_flax_big_bird.blocked_key_matrix->self.transpose_for_scores(self.key(hidden_states), n_heads, head_size).reshape(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)
A:transformers.models.big_bird.modeling_flax_big_bird.blocked_value_matrix->self.transpose_for_scores(self.value(hidden_states), n_heads, head_size).reshape(bsz, n_heads, to_seq_len // to_block_size, to_block_size, -1)
A:transformers.models.big_bird.modeling_flax_big_bird.gathered_key->self.jax_gather(blocked_key_matrix, rand_attn, batch_dims=2).reshape(*shape)
A:transformers.models.big_bird.modeling_flax_big_bird.gathered_value->self.jax_gather(blocked_value_matrix, rand_attn, batch_dims=2).reshape(*shape)
A:transformers.models.big_bird.modeling_flax_big_bird.first_product->jax.numpy.einsum('bhqd,bhkd->bhqk', blocked_query_matrix[:, :, 0], key_layer)
A:transformers.models.big_bird.modeling_flax_big_bird.first_attn_weights->jax.nn.softmax(first_product, axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.first_context_layer->jax.numpy.expand_dims(first_context_layer, 2)
A:transformers.models.big_bird.modeling_flax_big_bird.second_key_mat->jax.numpy.concatenate([blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, 1], blocked_key_matrix[:, :, 2], blocked_key_matrix[:, :, -1], gathered_key[:, :, 0]], axis=2)
A:transformers.models.big_bird.modeling_flax_big_bird.second_value_mat->jax.numpy.concatenate([blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, 1], blocked_value_matrix[:, :, 2], blocked_value_matrix[:, :, -1], gathered_value[:, :, 0]], axis=2)
A:transformers.models.big_bird.modeling_flax_big_bird.second_product->jax.numpy.einsum('bhqd,bhkd->bhqk', blocked_query_matrix[:, :, 1], second_key_mat)
A:transformers.models.big_bird.modeling_flax_big_bird.second_seq_pad->jax.numpy.concatenate([to_mask[:, :, :, :3 * to_block_size], to_mask[:, :, :, -to_block_size:], jnp.ones([bsz, 1, 1, n_rand_blocks * to_block_size], dtype=to_mask.dtype)], axis=3)
A:transformers.models.big_bird.modeling_flax_big_bird.second_rand_pad->jax.numpy.concatenate([jnp.ones([bsz, n_heads, from_block_size, 4 * to_block_size], dtype=rand_mask.dtype), rand_mask[:, :, 0]], axis=3)
A:transformers.models.big_bird.modeling_flax_big_bird.second_attn_weights->jax.nn.softmax(second_product, axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.second_context_layer->jax.numpy.expand_dims(second_context_layer, 2)
A:transformers.models.big_bird.modeling_flax_big_bird.exp_blocked_key_matrix->jax.numpy.concatenate([blocked_key_matrix[:, :, 1:-3], blocked_key_matrix[:, :, 2:-2], blocked_key_matrix[:, :, 3:-1]], axis=3)
A:transformers.models.big_bird.modeling_flax_big_bird.exp_blocked_value_matrix->jax.numpy.concatenate([blocked_value_matrix[:, :, 1:-3], blocked_value_matrix[:, :, 2:-2], blocked_value_matrix[:, :, 3:-1]], axis=3)
A:transformers.models.big_bird.modeling_flax_big_bird.inner_band_product->jax.numpy.einsum('bhlqd,bhlkd->bhlqk', middle_query_matrix, exp_blocked_key_matrix)
A:transformers.models.big_bird.modeling_flax_big_bird.rand_band_product->jax.numpy.einsum('bhlqd,bhlkd->bhlqk', middle_query_matrix, gathered_key[:, :, 1:-1])
A:transformers.models.big_bird.modeling_flax_big_bird.first_band_product->jax.numpy.einsum('bhlqd,bhkd->bhlqk', middle_query_matrix, blocked_key_matrix[:, :, 0])
A:transformers.models.big_bird.modeling_flax_big_bird.last_band_product->jax.numpy.einsum('bhlqd,bhkd->bhlqk', middle_query_matrix, blocked_key_matrix[:, :, -1])
A:transformers.models.big_bird.modeling_flax_big_bird.band_product->jax.numpy.concatenate([first_band_product, inner_band_product, rand_band_product, last_band_product], axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.context_layer->jax.numpy.transpose(context_layer, axes=(0, 2, 1, 3)).reshape(bsz, from_seq_len, -1)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_key_mat->jax.numpy.concatenate([blocked_key_matrix[:, :, 0], blocked_key_matrix[:, :, -3], blocked_key_matrix[:, :, -2], blocked_key_matrix[:, :, -1], gathered_key[:, :, -1]], axis=2)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_value_mat->jax.numpy.concatenate([blocked_value_matrix[:, :, 0], blocked_value_matrix[:, :, -3], blocked_value_matrix[:, :, -2], blocked_value_matrix[:, :, -1], gathered_value[:, :, -1]], axis=2)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_product->jax.numpy.einsum('bhqd,bhkd->bhqk', blocked_query_matrix[:, :, -2], second_last_key_mat)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_seq_pad->jax.numpy.concatenate([to_mask[:, :, :, :to_block_size], to_mask[:, :, :, -3 * to_block_size:], jnp.ones([bsz, 1, 1, n_rand_blocks * to_block_size], dtype=to_mask.dtype)], axis=3)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_rand_pad->jax.numpy.concatenate([jnp.ones([bsz, n_heads, from_block_size, 4 * to_block_size], dtype=rand_mask.dtype), rand_mask[:, :, -1]], axis=3)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_attn_weights->jax.nn.softmax(second_last_product, axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.second_last_context_layer->jax.numpy.expand_dims(second_last_context_layer, 2)
A:transformers.models.big_bird.modeling_flax_big_bird.last_product->jax.numpy.einsum('bhqd,bhkd->bhqk', blocked_query_matrix[:, :, -1], key_layer)
A:transformers.models.big_bird.modeling_flax_big_bird.last_attn_weights->jax.nn.softmax(last_product, axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.last_context_layer->jax.numpy.expand_dims(last_context_layer, 2)
A:transformers.models.big_bird.modeling_flax_big_bird._jax_gather->jax.vmap(_jax_gather, in_axes=(0, 0))
A:transformers.models.big_bird.modeling_flax_big_bird.middle_seq->numpy.arange(1, to_seq_length // to_block_size - 1, dtype=np.int32)
A:transformers.models.big_bird.modeling_flax_big_bird.max_plan_idx->plan_from_length.index(from_seq_length)
A:transformers.models.big_bird.modeling_flax_big_bird.rnd_r_cnt->int(np.sum(plan_num_rand_blocks[:plan_idx]))
A:transformers.models.big_bird.modeling_flax_big_bird.curr_r_cnt->int(np.sum(plan_num_rand_blocks[:plan_idx + 1]))
A:transformers.models.big_bird.modeling_flax_big_bird.rand_attn[h][blk_rw_idx, rnd_r_cnt:curr_r_cnt]->self._get_single_block_row_attention(block_id=blk_rw_idx, to_start_block_id=to_start_block_id, to_end_block_id=plan_block_length[plan_idx], num_rand_blocks=plan_num_rand_blocks[plan_idx], window_block_left=window_block_left, window_block_right=window_block_right, global_block_left=global_block_left, global_block_right=global_block_right)
A:transformers.models.big_bird.modeling_flax_big_bird.to_block_list->numpy.arange(to_start_block_id, to_end_block_id, dtype=np.int32)
A:transformers.models.big_bird.modeling_flax_big_bird.perm_block->numpy.random.permutation(to_block_list)
A:transformers.models.big_bird.modeling_flax_big_bird.illegal_blocks->list(range(block_id - window_block_left, block_id + window_block_right + 1))
A:transformers.models.big_bird.modeling_flax_big_bird.self.dense->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.self->FlaxBigBirdBlockSparseAttention(self.config, block_sparse_seed=self.layer_id, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.output->FlaxBigBirdOutput(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.attn_outputs->self.self(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.big_bird.modeling_flax_big_bird.self.attention->FlaxBigBirdAttention(self.config, layer_id=self.layer_id, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.intermediate->FlaxBigBirdIntermediate(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.attention_outputs->self.attention(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.big_bird.modeling_flax_big_bird.layer_outputs->layer(hidden_states, attention_mask, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.big_bird.modeling_flax_big_bird.self.layer->FlaxBigBirdLayerCollection(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.transform->FlaxBigBirdPredictionHeadTransform(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.decoder->flax.linen.Dense(self.config.vocab_size, dtype=self.dtype, use_bias=False)
A:transformers.models.big_bird.modeling_flax_big_bird.self.bias->self.param('bias', self.bias_init, (self.config.vocab_size,))
A:transformers.models.big_bird.modeling_flax_big_bird.self.predictions->FlaxBigBirdLMPredictionHead(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.seq_relationship->flax.linen.Dense(2, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.prediction_scores->self.predictions(hidden_states, shared_embedding=shared_embedding)
A:transformers.models.big_bird.modeling_flax_big_bird.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.big_bird.modeling_flax_big_bird.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.big_bird.modeling_flax_big_bird.input_ids->jax.numpy.zeros(input_shape, dtype='i4')
A:transformers.models.big_bird.modeling_flax_big_bird.token_type_ids->jax.numpy.zeros_like(input_ids)
A:transformers.models.big_bird.modeling_flax_big_bird.position_ids->jax.numpy.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)
A:transformers.models.big_bird.modeling_flax_big_bird.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.big_bird.modeling_flax_big_bird.self.embeddings->FlaxBigBirdEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.encoder->FlaxBigBirdEncoder(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.pooler->flax.linen.Dense(self.config.hidden_size, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype), dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.outputs->self.bert(input_ids, attention_mask, token_type_ids, position_ids, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.big_bird.modeling_flax_big_bird.self.bert->FlaxBigBirdModule(self.config, dtype=self.dtype, add_pooling_layer=self.add_pooling_layer)
A:transformers.models.big_bird.modeling_flax_big_bird.self.cls->FlaxBigBirdOnlyMLMHead(config=self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.(prediction_scores, seq_relationship_score)->self.cls(hidden_states, pooled_output, shared_embedding=shared_embedding)
A:transformers.models.big_bird.modeling_flax_big_bird.logits->self.qa_classifier(hidden_states, deterministic=deterministic)
A:transformers.models.big_bird.modeling_flax_big_bird.self.out_proj->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.classifier->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.pooled_output->self.dropout(pooled_output, deterministic=deterministic)
A:transformers.models.big_bird.modeling_flax_big_bird.reshaped_logits->self.qa_classifier(hidden_states, deterministic=deterministic).reshape(-1, num_choices)
A:transformers.models.big_bird.modeling_flax_big_bird.self.qa_outputs->flax.linen.Dense(self.config.num_labels, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.self.qa_classifier->FlaxBigBirdForQuestionAnsweringHead(self.config, dtype=self.dtype)
A:transformers.models.big_bird.modeling_flax_big_bird.(start_logits, end_logits)->self.qa_classifier(hidden_states, deterministic=deterministic).split(self.config.num_labels, axis=-1)
A:transformers.models.big_bird.modeling_flax_big_bird.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.models.big_bird.modeling_flax_big_bird.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.models.big_bird.modeling_flax_big_bird.question_lengths->jax.numpy.expand_dims(question_lengths, axis=1)
A:transformers.models.big_bird.modeling_flax_big_bird.logits_mask->jax.numpy.expand_dims(logits_mask, axis=2)
A:transformers.models.big_bird.modeling_flax_big_bird.mask->jax.numpy.arange(0, maxlen)
transformers.FlaxBigBirdForMaskedLM(FlaxBigBirdPreTrainedModel)
transformers.FlaxBigBirdForMaskedLMModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBigBirdForMaskedLMModule.setup(self)
transformers.FlaxBigBirdForMultipleChoice(self,config:BigBirdConfig,input_shape:Optional[tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxBigBirdForMultipleChoiceModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBigBirdForMultipleChoiceModule.setup(self)
transformers.FlaxBigBirdForPreTraining(FlaxBigBirdPreTrainedModel)
transformers.FlaxBigBirdForPreTrainingModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBigBirdForPreTrainingModule.setup(self)
transformers.FlaxBigBirdForPreTrainingOutput(ModelOutput)
transformers.FlaxBigBirdForQuestionAnswering(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,question_lengths=None,params:dict=None,dropout_rng:jax.random.PRNGKey=None,train:bool=False,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None)
transformers.FlaxBigBirdForQuestionAnswering.prepare_question_mask(q_lengths,maxlen:int)
transformers.FlaxBigBirdForQuestionAnsweringHead(self,encoder_output,deterministic=True)
transformers.FlaxBigBirdForQuestionAnsweringHead.setup(self)
transformers.FlaxBigBirdForQuestionAnsweringModelOutput(ModelOutput)
transformers.FlaxBigBirdForQuestionAnsweringModule(self,input_ids,attention_mask,token_type_ids,position_ids,logits_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBigBirdForQuestionAnsweringModule.setup(self)
transformers.FlaxBigBirdForSequenceClassification(FlaxBigBirdPreTrainedModel)
transformers.FlaxBigBirdForSequenceClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBigBirdForSequenceClassificationModule.setup(self)
transformers.FlaxBigBirdForTokenClassification(FlaxBigBirdPreTrainedModel)
transformers.FlaxBigBirdForTokenClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.FlaxBigBirdForTokenClassificationModule.setup(self)
transformers.FlaxBigBirdModel(FlaxBigBirdPreTrainedModel)
transformers.FlaxBigBirdPreTrainedModel(self,config:BigBirdConfig,input_shape:Optional[tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxBigBirdPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdAttention(self,hidden_states,attention_mask=None,deterministic=True,output_attentions:bool=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdAttention.__call__(self,hidden_states,attention_mask=None,deterministic=True,output_attentions:bool=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdAttention.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention._bigbird_block_rand_mask(from_seq_length,to_seq_length,from_block_size,to_block_size,num_rand_blocks,last_idx=-1)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention._bigbird_block_rand_mask_with_head(self,from_seq_length,to_seq_length,from_block_size,to_block_size,num_heads,plan_from_length,plan_num_rand_blocks,window_block_left=1,window_block_right=1,global_block_top=1,global_block_bottom=1,global_block_left=1,global_block_right=1)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention._create_rand_mask_from_inputs(self,from_blocked_mask,to_blocked_mask,broadcasted_rand_attn,num_attention_heads,num_random_blocks,batch_size,from_seq_length,from_block_size)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention._get_rand_attn_plan(from_seq_length,from_block_size,num_rand_blocks)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention._get_single_block_row_attention(block_id,to_start_block_id,to_end_block_id,num_rand_blocks,window_block_left=1,window_block_right=1,global_block_left=1,global_block_right=1)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention.bigbird_block_sparse_attention(self,query_layer,key_layer,value_layer,band_mask,from_mask,to_mask,from_blocked_mask,to_blocked_mask,n_heads,head_size,plan_from_length=None,plan_num_rand_blocks=None,output_attentions=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention.create_masks_for_block_sparse_attn(attention_mask,block_size:int)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention.jax_gather(params,indices,batch_dims=2)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdBlockSparseAttention.transpose_for_scores(x,n_heads,head_size)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdClassificationHead(self,features,deterministic=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdClassificationHead.__call__(self,features,deterministic=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdClassificationHead.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdEmbeddings(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdEmbeddings.__call__(self,input_ids,token_type_ids,position_ids,attention_mask,deterministic:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdEmbeddings.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdEncoder(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdEncoder.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdEncoder.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMaskedLM(FlaxBigBirdPreTrainedModel)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMaskedLMModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMaskedLMModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMaskedLMModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMultipleChoice(self,config:BigBirdConfig,input_shape:Optional[tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMultipleChoice.__init__(self,config:BigBirdConfig,input_shape:Optional[tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMultipleChoiceModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMultipleChoiceModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForMultipleChoiceModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForPreTraining(FlaxBigBirdPreTrainedModel)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForPreTrainingModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForPreTrainingModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForPreTrainingModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForPreTrainingOutput(ModelOutput)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnswering(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,question_lengths=None,params:dict=None,dropout_rng:jax.random.PRNGKey=None,train:bool=False,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnswering.__call__(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,question_lengths=None,params:dict=None,dropout_rng:jax.random.PRNGKey=None,train:bool=False,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnswering.prepare_question_mask(q_lengths,maxlen:int)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringHead(self,encoder_output,deterministic=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringHead.__call__(self,encoder_output,deterministic=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringHead.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringModelOutput(ModelOutput)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringModule(self,input_ids,attention_mask,token_type_ids,position_ids,logits_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,logits_mask=None,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForSequenceClassification(FlaxBigBirdPreTrainedModel)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForSequenceClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForSequenceClassificationModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForSequenceClassificationModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForTokenClassification(FlaxBigBirdPreTrainedModel)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForTokenClassificationModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForTokenClassificationModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForTokenClassificationModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdIntermediate(self,hidden_states)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdIntermediate.__call__(self,hidden_states)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdIntermediate.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLMPredictionHead(self,hidden_states,shared_embedding=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLMPredictionHead.__call__(self,hidden_states,shared_embedding=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLMPredictionHead.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLayer(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLayer.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLayer.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLayerCollection(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLayerCollection.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdLayerCollection.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdModel(FlaxBigBirdPreTrainedModel)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdModule(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdModule.__call__(self,input_ids,attention_mask,token_type_ids,position_ids,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdModule.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdOnlyMLMHead(self,hidden_states,shared_embedding=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdOnlyMLMHead.__call__(self,hidden_states,shared_embedding=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdOnlyMLMHead.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdOutput(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdOutput.__call__(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdOutput.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPreTrainedModel(self,config:BigBirdConfig,input_shape:Optional[tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPreTrainedModel.__init__(self,config:BigBirdConfig,input_shape:Optional[tuple]=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPreTrainingHeads(self,hidden_states,pooled_output,shared_embedding=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPreTrainingHeads.__call__(self,hidden_states,pooled_output,shared_embedding=None)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPreTrainingHeads.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPredictionHeadTransform(self,hidden_states)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPredictionHeadTransform.__call__(self,hidden_states)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdPredictionHeadTransform.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdSelfAttention(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdSelfAttention.__call__(self,hidden_states,attention_mask,deterministic=True,output_attentions:bool=False)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdSelfAttention.setup(self)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdSelfOutput(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdSelfOutput.__call__(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdSelfOutput.setup(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert_japanese/tokenization_bert_japanese.py----------------------------------------
A:transformers.models.bert_japanese.tokenization_bert_japanese.logger->utils.logging.get_logger(__name__)
A:transformers.models.bert_japanese.tokenization_bert_japanese.self.vocab->load_vocab(vocab_file)
A:transformers.models.bert_japanese.tokenization_bert_japanese.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.models.bert_japanese.tokenization_bert_japanese.self.mecab_kwargs->copy.deepcopy(mecab_kwargs)
A:transformers.models.bert_japanese.tokenization_bert_japanese.self.word_tokenizer->MecabTokenizer(do_lower_case=self.do_lower_case, never_split=self.never_split, **self.mecab_kwargs or {})
A:transformers.models.bert_japanese.tokenization_bert_japanese.self.subword_tokenizer->CharacterTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.models.bert_japanese.tokenization_bert_japanese.state->dict(self.__dict__)
A:transformers.models.bert_japanese.tokenization_bert_japanese.tokens->self.word_tokenizer.tokenize(text, never_split=self.all_special_tokens)
A:transformers.models.bert_japanese.tokenization_bert_japanese.mecabrc->os.path.join(dic_dir, 'mecabrc')
A:transformers.models.bert_japanese.tokenization_bert_japanese.self.mecab->fugashi.GenericTagger(mecab_option)
A:transformers.models.bert_japanese.tokenization_bert_japanese.text->unicodedata.normalize('NFKC', text)
A:transformers.models.bert_japanese.tokenization_bert_japanese.token->token.lower().lower()
transformers.BertJapaneseTokenizer(self,vocab_file,do_lower_case=False,do_word_tokenize=True,do_subword_tokenize=True,word_tokenizer_type='basic',subword_tokenizer_type='wordpiece',never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',mecab_kwargs=None,**kwargs)
transformers.BertJapaneseTokenizer.__getstate__(self)
transformers.BertJapaneseTokenizer.__setstate__(self,state)
transformers.BertJapaneseTokenizer._tokenize(self,text)
transformers.BertJapaneseTokenizer.do_lower_case(self)
transformers.CharacterTokenizer(self,vocab,unk_token,normalize_text=True)
transformers.CharacterTokenizer.tokenize(self,text)
transformers.MecabTokenizer(self,do_lower_case=False,never_split=None,normalize_text=True,mecab_dic:Optional[str]='ipadic',mecab_option:Optional[str]=None)
transformers.MecabTokenizer.tokenize(self,text,never_split=None,**kwargs)
transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer(self,vocab_file,do_lower_case=False,do_word_tokenize=True,do_subword_tokenize=True,word_tokenizer_type='basic',subword_tokenizer_type='wordpiece',never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',mecab_kwargs=None,**kwargs)
transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer.__getstate__(self)
transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer.__init__(self,vocab_file,do_lower_case=False,do_word_tokenize=True,do_subword_tokenize=True,word_tokenizer_type='basic',subword_tokenizer_type='wordpiece',never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',mecab_kwargs=None,**kwargs)
transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer.__setstate__(self,state)
transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer._tokenize(self,text)
transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer.do_lower_case(self)
transformers.models.bert_japanese.tokenization_bert_japanese.CharacterTokenizer(self,vocab,unk_token,normalize_text=True)
transformers.models.bert_japanese.tokenization_bert_japanese.CharacterTokenizer.__init__(self,vocab,unk_token,normalize_text=True)
transformers.models.bert_japanese.tokenization_bert_japanese.CharacterTokenizer.tokenize(self,text)
transformers.models.bert_japanese.tokenization_bert_japanese.MecabTokenizer(self,do_lower_case=False,never_split=None,normalize_text=True,mecab_dic:Optional[str]='ipadic',mecab_option:Optional[str]=None)
transformers.models.bert_japanese.tokenization_bert_japanese.MecabTokenizer.__init__(self,do_lower_case=False,never_split=None,normalize_text=True,mecab_dic:Optional[str]='ipadic',mecab_option:Optional[str]=None)
transformers.models.bert_japanese.tokenization_bert_japanese.MecabTokenizer.tokenize(self,text,never_split=None,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bert_japanese/__init__.py----------------------------------------
A:transformers.models.bert_japanese.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deit/feature_extraction_deit.py----------------------------------------
A:transformers.models.deit.feature_extraction_deit.logger->utils.logging.get_logger(__name__)
A:transformers.models.deit.feature_extraction_deit.is_batched->bool(isinstance(images, (list, tuple)) and (isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0])))
A:transformers.models.deit.feature_extraction_deit.encoded_inputs->BatchFeature(data=data, tensor_type=return_tensors)
transformers.DeiTFeatureExtractor(self,do_resize=True,size=256,resample=Image.BICUBIC,do_center_crop=True,crop_size=224,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.deit.feature_extraction_deit.DeiTFeatureExtractor(self,do_resize=True,size=256,resample=Image.BICUBIC,do_center_crop=True,crop_size=224,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.deit.feature_extraction_deit.DeiTFeatureExtractor.__init__(self,do_resize=True,size=256,resample=Image.BICUBIC,do_center_crop=True,crop_size=224,do_normalize=True,image_mean=None,image_std=None,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deit/configuration_deit.py----------------------------------------
A:transformers.models.deit.configuration_deit.logger->utils.logging.get_logger(__name__)
transformers.DeiTConfig(self,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.0,initializer_range=0.02,layer_norm_eps=1e-12,is_encoder_decoder=False,image_size=224,patch_size=16,num_channels=3,**kwargs)
transformers.models.deit.configuration_deit.DeiTConfig(self,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.0,initializer_range=0.02,layer_norm_eps=1e-12,is_encoder_decoder=False,image_size=224,patch_size=16,num_channels=3,**kwargs)
transformers.models.deit.configuration_deit.DeiTConfig.__init__(self,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.0,initializer_range=0.02,layer_norm_eps=1e-12,is_encoder_decoder=False,image_size=224,patch_size=16,num_channels=3,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deit/convert_deit_timm_to_pytorch.py----------------------------------------
A:transformers.models.deit.convert_deit_timm_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.deit.convert_deit_timm_to_pytorch.in_proj_weight->timm_model.state_dict().pop(f'blocks.{i}.attn.qkv.weight')
A:transformers.models.deit.convert_deit_timm_to_pytorch.in_proj_bias->timm_model.state_dict().pop(f'blocks.{i}.attn.qkv.bias')
A:transformers.models.deit.convert_deit_timm_to_pytorch.val->dct.pop(old)
A:transformers.models.deit.convert_deit_timm_to_pytorch.im->PIL.Image.open(requests.get(url, stream=True).raw)
A:transformers.models.deit.convert_deit_timm_to_pytorch.config->DeiTConfig()
A:transformers.models.deit.convert_deit_timm_to_pytorch.config.patch_size->int(deit_name[-6:-4])
A:transformers.models.deit.convert_deit_timm_to_pytorch.config.image_size->int(deit_name[-3:])
A:transformers.models.deit.convert_deit_timm_to_pytorch.timm_model->timm.create_model(deit_name, pretrained=True)
A:transformers.models.deit.convert_deit_timm_to_pytorch.state_dict->timm.create_model(deit_name, pretrained=True).state_dict()
A:transformers.models.deit.convert_deit_timm_to_pytorch.rename_keys->create_rename_keys(config, base_model)
A:transformers.models.deit.convert_deit_timm_to_pytorch.model->DeiTForImageClassificationWithTeacher(config).eval()
A:transformers.models.deit.convert_deit_timm_to_pytorch.size->int(256 / 224 * config.image_size)
A:transformers.models.deit.convert_deit_timm_to_pytorch.feature_extractor->DeiTFeatureExtractor(size=size, crop_size=config.image_size)
A:transformers.models.deit.convert_deit_timm_to_pytorch.encoding->feature_extractor(images=prepare_img(), return_tensors='pt')
A:transformers.models.deit.convert_deit_timm_to_pytorch.outputs->model(pixel_values)
A:transformers.models.deit.convert_deit_timm_to_pytorch.timm_logits->timm_model(pixel_values)
A:transformers.models.deit.convert_deit_timm_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.deit.convert_deit_timm_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.deit.convert_deit_timm_to_pytorch.convert_deit_checkpoint(deit_name,pytorch_dump_folder_path)
transformers.models.deit.convert_deit_timm_to_pytorch.create_rename_keys(config,base_model=False)
transformers.models.deit.convert_deit_timm_to_pytorch.prepare_img()
transformers.models.deit.convert_deit_timm_to_pytorch.read_in_q_k_v(state_dict,config,base_model=False)
transformers.models.deit.convert_deit_timm_to_pytorch.rename_key(dct,old,new)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deit/__init__.py----------------------------------------
A:transformers.models.deit.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/deit/modeling_deit.py----------------------------------------
A:transformers.models.deit.modeling_deit.logger->utils.logging.get_logger(__name__)
A:transformers.models.deit.modeling_deit.self.cls_token->torch.nn.Parameter(torch.zeros(1, 1, config.hidden_size))
A:transformers.models.deit.modeling_deit.self.distillation_token->torch.nn.Parameter(torch.zeros(1, 1, config.hidden_size))
A:transformers.models.deit.modeling_deit.self.patch_embeddings->PatchEmbeddings(image_size=config.image_size, patch_size=config.patch_size, num_channels=config.num_channels, embed_dim=config.hidden_size)
A:transformers.models.deit.modeling_deit.self.position_embeddings->torch.nn.Parameter(torch.zeros(1, num_patches + 2, config.hidden_size))
A:transformers.models.deit.modeling_deit.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.deit.modeling_deit.embeddings->self.dropout(embeddings)
A:transformers.models.deit.modeling_deit.cls_tokens->self.cls_token.expand(batch_size, -1, -1)
A:transformers.models.deit.modeling_deit.distillation_tokens->self.distillation_token.expand(batch_size, -1, -1)
A:transformers.models.deit.modeling_deit.image_size->to_2tuple(image_size)
A:transformers.models.deit.modeling_deit.patch_size->to_2tuple(patch_size)
A:transformers.models.deit.modeling_deit.self.projection->torch.nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)
A:transformers.models.deit.modeling_deit.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.deit.modeling_deit.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.deit.modeling_deit.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.deit.modeling_deit.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.deit.modeling_deit.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.deit.modeling_deit.mixed_query_layer->self.query(hidden_states)
A:transformers.models.deit.modeling_deit.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.deit.modeling_deit.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.deit.modeling_deit.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.deit.modeling_deit.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.deit.modeling_deit.attention_probs->self.dropout(attention_probs)
A:transformers.models.deit.modeling_deit.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.deit.modeling_deit.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.deit.modeling_deit.hidden_states->self.dropout(hidden_states)
A:transformers.models.deit.modeling_deit.self.attention->DeiTAttention(config)
A:transformers.models.deit.modeling_deit.self.output->DeiTOutput(config)
A:transformers.models.deit.modeling_deit.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.deit.modeling_deit.(heads, index)->find_pruneable_heads_and_indices(heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads)
A:transformers.models.deit.modeling_deit.self.attention.query->prune_linear_layer(self.attention.query, index)
A:transformers.models.deit.modeling_deit.self.attention.key->prune_linear_layer(self.attention.key, index)
A:transformers.models.deit.modeling_deit.self.attention.value->prune_linear_layer(self.attention.value, index)
A:transformers.models.deit.modeling_deit.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.deit.modeling_deit.self_outputs->self.attention(hidden_states, head_mask, output_attentions)
A:transformers.models.deit.modeling_deit.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.deit.modeling_deit.self.intermediate->DeiTIntermediate(config)
A:transformers.models.deit.modeling_deit.self.layernorm_before->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.deit.modeling_deit.self.layernorm_after->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.deit.modeling_deit.self_attention_outputs->self.attention(self.layernorm_before(hidden_states), head_mask, output_attentions=output_attentions)
A:transformers.models.deit.modeling_deit.layer_output->self.output(intermediate_output)
A:transformers.models.deit.modeling_deit.intermediate_output->self.intermediate(attention_output)
A:transformers.models.deit.modeling_deit.self.layer->torch.nn.ModuleList([DeiTLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.deit.modeling_deit.layer_outputs->layer_module(hidden_states, layer_head_mask, output_attentions)
A:transformers.models.deit.modeling_deit.self.embeddings->DeiTEmbeddings(config)
A:transformers.models.deit.modeling_deit.self.encoder->DeiTEncoder(config)
A:transformers.models.deit.modeling_deit.self.layernorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.deit.modeling_deit.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.deit.modeling_deit.embedding_output->self.embeddings(pixel_values)
A:transformers.models.deit.modeling_deit.encoder_outputs->self.encoder(embedding_output, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.deit.modeling_deit.sequence_output->self.layernorm(sequence_output)
A:transformers.models.deit.modeling_deit.self.activation->torch.nn.Tanh()
A:transformers.models.deit.modeling_deit.pooled_output->self.activation(pooled_output)
A:transformers.models.deit.modeling_deit.self.deit->DeiTModel(config, add_pooling_layer=False)
A:transformers.models.deit.modeling_deit.outputs->self.deit(pixel_values, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.deit.modeling_deit.logits->self.classifier(sequence_output[:, 0, :])
A:transformers.models.deit.modeling_deit.loss_fct->CrossEntropyLoss()
A:transformers.models.deit.modeling_deit.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.deit.modeling_deit.cls_logits->self.cls_classifier(sequence_output[:, 0, :])
A:transformers.models.deit.modeling_deit.distillation_logits->self.distillation_classifier(sequence_output[:, 1, :])
transformers.DeiTForImageClassification(self,config)
transformers.DeiTForImageClassification.forward(self,pixel_values=None,head_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DeiTForImageClassificationWithTeacher(self,config)
transformers.DeiTForImageClassificationWithTeacher.forward(self,pixel_values=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DeiTForImageClassificationWithTeacherOutput(ModelOutput)
transformers.DeiTModel(self,config,add_pooling_layer=True)
transformers.DeiTModel._prune_heads(self,heads_to_prune)
transformers.DeiTModel.forward(self,pixel_values=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DeiTModel.get_input_embeddings(self)
transformers.DeiTPreTrainedModel(PreTrainedModel)
transformers.DeiTPreTrainedModel._init_weights(self,module)
transformers.models.deit.modeling_deit.DeiTAttention(self,config)
transformers.models.deit.modeling_deit.DeiTAttention.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTAttention.forward(self,hidden_states,head_mask=None,output_attentions=False)
transformers.models.deit.modeling_deit.DeiTAttention.prune_heads(self,heads)
transformers.models.deit.modeling_deit.DeiTEmbeddings(self,config)
transformers.models.deit.modeling_deit.DeiTEmbeddings.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTEmbeddings.forward(self,pixel_values)
transformers.models.deit.modeling_deit.DeiTEncoder(self,config)
transformers.models.deit.modeling_deit.DeiTEncoder.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTEncoder.forward(self,hidden_states,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.deit.modeling_deit.DeiTForImageClassification(self,config)
transformers.models.deit.modeling_deit.DeiTForImageClassification.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTForImageClassification.forward(self,pixel_values=None,head_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deit.modeling_deit.DeiTForImageClassificationWithTeacher(self,config)
transformers.models.deit.modeling_deit.DeiTForImageClassificationWithTeacher.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTForImageClassificationWithTeacher.forward(self,pixel_values=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deit.modeling_deit.DeiTForImageClassificationWithTeacherOutput(ModelOutput)
transformers.models.deit.modeling_deit.DeiTIntermediate(self,config)
transformers.models.deit.modeling_deit.DeiTIntermediate.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTIntermediate.forward(self,hidden_states)
transformers.models.deit.modeling_deit.DeiTLayer(self,config)
transformers.models.deit.modeling_deit.DeiTLayer.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTLayer.feed_forward_chunk(self,attention_output)
transformers.models.deit.modeling_deit.DeiTLayer.forward(self,hidden_states,head_mask=None,output_attentions=False)
transformers.models.deit.modeling_deit.DeiTModel(self,config,add_pooling_layer=True)
transformers.models.deit.modeling_deit.DeiTModel.__init__(self,config,add_pooling_layer=True)
transformers.models.deit.modeling_deit.DeiTModel._prune_heads(self,heads_to_prune)
transformers.models.deit.modeling_deit.DeiTModel.forward(self,pixel_values=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.deit.modeling_deit.DeiTModel.get_input_embeddings(self)
transformers.models.deit.modeling_deit.DeiTOutput(self,config)
transformers.models.deit.modeling_deit.DeiTOutput.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTOutput.forward(self,hidden_states,input_tensor)
transformers.models.deit.modeling_deit.DeiTPooler(self,config)
transformers.models.deit.modeling_deit.DeiTPooler.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTPooler.forward(self,hidden_states)
transformers.models.deit.modeling_deit.DeiTPreTrainedModel(PreTrainedModel)
transformers.models.deit.modeling_deit.DeiTPreTrainedModel._init_weights(self,module)
transformers.models.deit.modeling_deit.DeiTSelfAttention(self,config)
transformers.models.deit.modeling_deit.DeiTSelfAttention.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTSelfAttention.forward(self,hidden_states,head_mask=None,output_attentions=False)
transformers.models.deit.modeling_deit.DeiTSelfAttention.transpose_for_scores(self,x)
transformers.models.deit.modeling_deit.DeiTSelfOutput(self,config)
transformers.models.deit.modeling_deit.DeiTSelfOutput.__init__(self,config)
transformers.models.deit.modeling_deit.DeiTSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.deit.modeling_deit.PatchEmbeddings(self,image_size=224,patch_size=16,num_channels=3,embed_dim=768)
transformers.models.deit.modeling_deit.PatchEmbeddings.__init__(self,image_size=224,patch_size=16,num_channels=3,embed_dim=768)
transformers.models.deit.modeling_deit.PatchEmbeddings.forward(self,pixel_values)
transformers.models.deit.modeling_deit.to_2tuple(x)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/hubert/modeling_hubert.py----------------------------------------
A:transformers.models.hubert.modeling_hubert.logger->utils.logging.get_logger(__name__)
A:transformers.models.hubert.modeling_hubert.num_masked_spans->max(num_masked_spans, min_masks)
A:transformers.models.hubert.modeling_hubert.spec_aug_mask->spec_aug_mask.scatter(1, spec_aug_mask_idxs, True).scatter(1, spec_aug_mask_idxs, True)
A:transformers.models.hubert.modeling_hubert.uniform_dist->torch.ones((batch_size, sequence_length - (mask_length - 1)), device=device)
A:transformers.models.hubert.modeling_hubert.spec_aug_mask_idxs->spec_aug_mask_idxs.unsqueeze(dim=-1).expand((batch_size, num_masked_spans, mask_length)).reshape(batch_size, num_masked_spans * mask_length).unsqueeze(dim=-1).expand((batch_size, num_masked_spans, mask_length)).reshape(batch_size, num_masked_spans * mask_length)
A:transformers.models.hubert.modeling_hubert.offsets->torch.arange(mask_length, device=device)[None, None, :].expand((batch_size, num_masked_spans, mask_length)).reshape(batch_size, num_masked_spans * mask_length)
A:transformers.models.hubert.modeling_hubert.self.conv->torch.nn.utils.weight_norm(self.conv, name='weight', dim=2)
A:transformers.models.hubert.modeling_hubert.hidden_states->self.dropout(hidden_states)
A:transformers.models.hubert.modeling_hubert.self.layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.hubert.modeling_hubert.self.padding->HubertSamePadLayer(config.num_conv_pos_embeddings)
A:transformers.models.hubert.modeling_hubert.self.conv_layers->torch.nn.ModuleList(conv_layers)
A:transformers.models.hubert.modeling_hubert.self.projection->torch.nn.Linear(config.conv_dim[-1], config.hidden_size)
A:transformers.models.hubert.modeling_hubert.self.dropout->torch.nn.Dropout(config.final_dropout)
A:transformers.models.hubert.modeling_hubert.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.hubert.modeling_hubert.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.hubert.modeling_hubert.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.hubert.modeling_hubert.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.hubert.modeling_hubert.(bsz, tgt_len, embed_dim)->self.dropout(hidden_states).size()
A:transformers.models.hubert.modeling_hubert.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.hubert.modeling_hubert.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.hubert.modeling_hubert.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.hubert.modeling_hubert.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.hubert.modeling_hubert.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.hubert.modeling_hubert.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.hubert.modeling_hubert.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.hubert.modeling_hubert.attn_output->self.out_proj(attn_output)
A:transformers.models.hubert.modeling_hubert.self.intermediate_dropout->torch.nn.Dropout(config.activation_dropout)
A:transformers.models.hubert.modeling_hubert.self.intermediate_dense->torch.nn.Linear(config.hidden_size, config.intermediate_size)
A:transformers.models.hubert.modeling_hubert.self.output_dense->torch.nn.Linear(config.intermediate_size, config.hidden_size)
A:transformers.models.hubert.modeling_hubert.self.output_dropout->torch.nn.Dropout(config.hidden_dropout)
A:transformers.models.hubert.modeling_hubert.self.attention->HubertAttention(embed_dim=config.hidden_size, num_heads=config.num_attention_heads, dropout=config.attention_dropout, is_decoder=False)
A:transformers.models.hubert.modeling_hubert.self.feed_forward->HubertFeedForward(config)
A:transformers.models.hubert.modeling_hubert.self.final_layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.hubert.modeling_hubert.(hidden_states, attn_weights, _)->self.attention(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)
A:transformers.models.hubert.modeling_hubert.self.pos_conv_embed->HubertPositionalConvEmbedding(config)
A:transformers.models.hubert.modeling_hubert.self.layers->torch.nn.ModuleList([HubertEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.hubert.modeling_hubert.attention_mask->attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool().flip([-1]).cumsum(-1).flip([-1]).bool()
A:transformers.models.hubert.modeling_hubert.position_embeddings->self.pos_conv_embed(hidden_states)
A:transformers.models.hubert.modeling_hubert.deepspeed_zero3_is_enabled->is_deepspeed_zero3_enabled()
A:transformers.models.hubert.modeling_hubert.dropout_probability->numpy.random.uniform(0, 1)
A:transformers.models.hubert.modeling_hubert.layer_outputs->layer(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)
A:transformers.models.hubert.modeling_hubert.input_lengths->self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)
A:transformers.models.hubert.modeling_hubert.self.feature_extractor->HubertFeatureExtractor(config)
A:transformers.models.hubert.modeling_hubert.self.feature_projection->HubertFeatureProjection(config)
A:transformers.models.hubert.modeling_hubert.self.masked_spec_embed->torch.nn.Parameter(torch.FloatTensor(config.hidden_size).uniform_())
A:transformers.models.hubert.modeling_hubert.self.encoder->HubertEncoder(config)
A:transformers.models.hubert.modeling_hubert.hidden_states[mask_time_indices]->self.masked_spec_embed.to(hidden_states.dtype)
A:transformers.models.hubert.modeling_hubert.(batch_size, sequence_length, hidden_size)->self.dropout(hidden_states).size()
A:transformers.models.hubert.modeling_hubert.mask_time_indices->_compute_mask_indices((batch_size, sequence_length), mask_prob=self.config.mask_time_prob, mask_length=self.config.mask_time_length, device=hidden_states.device, min_masks=2)
A:transformers.models.hubert.modeling_hubert.mask_feature_indices->_compute_mask_indices((batch_size, hidden_size), mask_prob=self.config.mask_feature_prob, mask_length=self.config.mask_feature_length, device=hidden_states.device)
A:transformers.models.hubert.modeling_hubert.extract_features->extract_features.transpose(1, 2).transpose(1, 2)
A:transformers.models.hubert.modeling_hubert.output_lengths->self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)
A:transformers.models.hubert.modeling_hubert.encoder_outputs->self.encoder(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.hubert.modeling_hubert.self.hubert->HubertModel(config)
A:transformers.models.hubert.modeling_hubert.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size)
A:transformers.models.hubert.modeling_hubert.outputs->self.hubert(input_values, attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.hubert.modeling_hubert.logits->self.lm_head(hidden_states)
A:transformers.models.hubert.modeling_hubert.target_lengths->labels_mask.sum(-1)
A:transformers.models.hubert.modeling_hubert.flattened_targets->labels.masked_select(labels_mask)
A:transformers.models.hubert.modeling_hubert.log_probs->torch.nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)
A:transformers.models.hubert.modeling_hubert.loss->torch.nn.functional.ctc_loss(log_probs, flattened_targets, input_lengths, target_lengths, blank=self.config.pad_token_id, reduction=self.config.ctc_loss_reduction, zero_infinity=self.config.ctc_zero_infinity)
transformers.HubertForCTC(self,config)
transformers.HubertForCTC.forward(self,input_values,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.HubertForCTC.freeze_feature_extractor(self)
transformers.HubertModel(self,config:HubertConfig)
transformers.HubertModel._mask_hidden_states(self,hidden_states:torch.FloatTensor,mask_time_indices:Optional[torch.FloatTensor]=None)
transformers.HubertModel.forward(self,input_values,attention_mask=None,mask_time_indices=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.HubertPreTrainedModel(PreTrainedModel)
transformers.HubertPreTrainedModel._get_feat_extract_output_lengths(self,input_lengths:Union[torch.LongTensor,int])
transformers.HubertPreTrainedModel._init_weights(self,module)
transformers.models.hubert.modeling_hubert.HubertAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.hubert.modeling_hubert.HubertAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.hubert.modeling_hubert.HubertAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.hubert.modeling_hubert.HubertAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.hubert.modeling_hubert.HubertEncoder(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoder.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoder.forward(self,hidden_states,attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.hubert.modeling_hubert.HubertEncoderLayer(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoderLayer.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoderLayer.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.models.hubert.modeling_hubert.HubertEncoderLayerStableLayerNorm(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoderLayerStableLayerNorm.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoderLayerStableLayerNorm.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.models.hubert.modeling_hubert.HubertEncoderStableLayerNorm(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoderStableLayerNorm.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertEncoderStableLayerNorm.forward(self,hidden_states,attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.hubert.modeling_hubert.HubertFeatureExtractor(self,config)
transformers.models.hubert.modeling_hubert.HubertFeatureExtractor.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertFeatureExtractor._freeze_parameters(self)
transformers.models.hubert.modeling_hubert.HubertFeatureExtractor.forward(self,input_values)
transformers.models.hubert.modeling_hubert.HubertFeatureProjection(self,config)
transformers.models.hubert.modeling_hubert.HubertFeatureProjection.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertFeatureProjection.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert.HubertFeedForward(self,config)
transformers.models.hubert.modeling_hubert.HubertFeedForward.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertFeedForward.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert.HubertForCTC(self,config)
transformers.models.hubert.modeling_hubert.HubertForCTC.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertForCTC.forward(self,input_values,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.models.hubert.modeling_hubert.HubertForCTC.freeze_feature_extractor(self)
transformers.models.hubert.modeling_hubert.HubertGroupNormConvLayer(self,config,layer_id=0)
transformers.models.hubert.modeling_hubert.HubertGroupNormConvLayer.__init__(self,config,layer_id=0)
transformers.models.hubert.modeling_hubert.HubertGroupNormConvLayer.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert.HubertLayerNormConvLayer(self,config,layer_id=0)
transformers.models.hubert.modeling_hubert.HubertLayerNormConvLayer.__init__(self,config,layer_id=0)
transformers.models.hubert.modeling_hubert.HubertLayerNormConvLayer.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert.HubertModel(self,config:HubertConfig)
transformers.models.hubert.modeling_hubert.HubertModel.__init__(self,config:HubertConfig)
transformers.models.hubert.modeling_hubert.HubertModel._mask_hidden_states(self,hidden_states:torch.FloatTensor,mask_time_indices:Optional[torch.FloatTensor]=None)
transformers.models.hubert.modeling_hubert.HubertModel.forward(self,input_values,attention_mask=None,mask_time_indices=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.hubert.modeling_hubert.HubertNoLayerNormConvLayer(self,config,layer_id=0)
transformers.models.hubert.modeling_hubert.HubertNoLayerNormConvLayer.__init__(self,config,layer_id=0)
transformers.models.hubert.modeling_hubert.HubertNoLayerNormConvLayer.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert.HubertPositionalConvEmbedding(self,config)
transformers.models.hubert.modeling_hubert.HubertPositionalConvEmbedding.__init__(self,config)
transformers.models.hubert.modeling_hubert.HubertPositionalConvEmbedding.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert.HubertPreTrainedModel(PreTrainedModel)
transformers.models.hubert.modeling_hubert.HubertPreTrainedModel._get_feat_extract_output_lengths(self,input_lengths:Union[torch.LongTensor,int])
transformers.models.hubert.modeling_hubert.HubertPreTrainedModel._init_weights(self,module)
transformers.models.hubert.modeling_hubert.HubertSamePadLayer(self,num_conv_pos_embeddings)
transformers.models.hubert.modeling_hubert.HubertSamePadLayer.__init__(self,num_conv_pos_embeddings)
transformers.models.hubert.modeling_hubert.HubertSamePadLayer.forward(self,hidden_states)
transformers.models.hubert.modeling_hubert._compute_mask_indices(shape:Tuple[int,int],mask_prob:float,mask_length:int,device:torch.device,min_masks:int=0)->torch.tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/hubert/__init__.py----------------------------------------
A:transformers.models.hubert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/hubert/configuration_hubert.py----------------------------------------
A:transformers.models.hubert.configuration_hubert.logger->utils.logging.get_logger(__name__)
A:transformers.models.hubert.configuration_hubert.self.conv_dim->list(conv_dim)
A:transformers.models.hubert.configuration_hubert.self.conv_stride->list(conv_stride)
A:transformers.models.hubert.configuration_hubert.self.conv_kernel->list(conv_kernel)
A:transformers.models.hubert.configuration_hubert.self.num_feat_extract_layers->len(self.conv_dim)
transformers.HubertConfig(self,vocab_size=32,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout=0.1,activation_dropout=0.1,attention_dropout=0.1,feat_proj_dropout=0.1,final_dropout=0.1,layerdrop=0.1,initializer_range=0.02,layer_norm_eps=1e-05,feat_extract_norm='group',feat_extract_activation='gelu',conv_dim=(512,512,512,512,512,512,512),conv_stride=(5,2,2,2,2,2,2),conv_kernel=(10,3,3,3,3,2,2),conv_bias=False,num_conv_pos_embeddings=128,num_conv_pos_embedding_groups=16,do_stable_layer_norm=False,apply_spec_augment=True,mask_time_prob=0.05,mask_time_length=10,mask_feature_prob=0.0,mask_feature_length=10,ctc_loss_reduction='sum',ctc_zero_infinity=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.hubert.configuration_hubert.HubertConfig(self,vocab_size=32,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout=0.1,activation_dropout=0.1,attention_dropout=0.1,feat_proj_dropout=0.1,final_dropout=0.1,layerdrop=0.1,initializer_range=0.02,layer_norm_eps=1e-05,feat_extract_norm='group',feat_extract_activation='gelu',conv_dim=(512,512,512,512,512,512,512),conv_stride=(5,2,2,2,2,2,2),conv_kernel=(10,3,3,3,3,2,2),conv_bias=False,num_conv_pos_embeddings=128,num_conv_pos_embedding_groups=16,do_stable_layer_norm=False,apply_spec_augment=True,mask_time_prob=0.05,mask_time_length=10,mask_feature_prob=0.0,mask_feature_length=10,ctc_loss_reduction='sum',ctc_zero_infinity=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.hubert.configuration_hubert.HubertConfig.__init__(self,vocab_size=32,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout=0.1,activation_dropout=0.1,attention_dropout=0.1,feat_proj_dropout=0.1,final_dropout=0.1,layerdrop=0.1,initializer_range=0.02,layer_norm_eps=1e-05,feat_extract_norm='group',feat_extract_activation='gelu',conv_dim=(512,512,512,512,512,512,512),conv_stride=(5,2,2,2,2,2,2),conv_kernel=(10,3,3,3,3,2,2),conv_bias=False,num_conv_pos_embeddings=128,num_conv_pos_embedding_groups=16,do_stable_layer_norm=False,apply_spec_augment=True,mask_time_prob=0.05,mask_time_length=10,mask_feature_prob=0.0,mask_feature_length=10,ctc_loss_reduction='sum',ctc_zero_infinity=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.logger->transformers.logging.get_logger(__name__)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.hf_pointer->getattr(hf_pointer, attribute)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.fairseq_dict->fairseq_model.state_dict()
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.mapped_key->mapped_key.replace('*', layer_index).replace('*', layer_index)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.items->name.split('.')
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.layer_id->int(items[0])
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.type_id->int(items[1])
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.config->HubertConfig()
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.target_dict->fairseq.data.Dictionary.load(dict_path)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.config.vocab_size->len(target_dict.symbols)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.vocab_path->os.path.join(pytorch_dump_folder_path, 'vocab.json')
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.tokenizer->Wav2Vec2CTCTokenizer(vocab_path, unk_token=target_dict.unk_word, pad_token=target_dict.pad_word, bos_token=target_dict.bos_word, eos_token=target_dict.eos_word, word_delimiter_token='|', do_lower_case=False)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.feature_extractor->Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0, do_normalize=True, return_attention_mask=return_attention_mask)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.processor->Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.hf_wav2vec->HubertModel(config)
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.(model, _, _)->fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path])
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.model->model[0].eval()
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.convert_hubert_checkpoint(checkpoint_path,pytorch_dump_folder_path,config_path=None,dict_path=None,is_finetuned=True)
transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.load_conv_layer(full_name,value,feature_extractor,unused_weights,use_group_norm)
transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.recursively_load_weights(fairseq_model,hf_model,is_finetuned)
transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch.set_recursively(hf_pointer,key,value,full_name,weight_type)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/tokenization_lxmert.py----------------------------------------
transformers.LxmertTokenizer(BertTokenizer)
transformers.models.lxmert.tokenization_lxmert.LxmertTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/modeling_tf_lxmert.py----------------------------------------
A:transformers.models.lxmert.modeling_tf_lxmert.logger->utils.logging.get_logger(__name__)
A:transformers.models.lxmert.modeling_tf_lxmert.self.visn_fc->TFLxmertVisualFeatureEncoder(config, name='visn_fc')
A:transformers.models.lxmert.modeling_tf_lxmert.self.visn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='visn_layer_norm')
A:transformers.models.lxmert.modeling_tf_lxmert.self.box_fc->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='box_fc')
A:transformers.models.lxmert.modeling_tf_lxmert.self.box_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='box_layer_norm')
A:transformers.models.lxmert.modeling_tf_lxmert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.lxmert.modeling_tf_lxmert.x->tensorflow.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.lxmert.modeling_tf_lxmert.y->self.box_layer_norm(y)
A:transformers.models.lxmert.modeling_tf_lxmert.output->self.call(inputs)
A:transformers.models.lxmert.modeling_tf_lxmert.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.lxmert.modeling_tf_lxmert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.lxmert.modeling_tf_lxmert.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.lxmert.modeling_tf_lxmert.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.lxmert.modeling_tf_lxmert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(initializer_range=self.initializer_range))
A:transformers.models.lxmert.modeling_tf_lxmert.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.lxmert.modeling_tf_lxmert.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.lxmert.modeling_tf_lxmert.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.lxmert.modeling_tf_lxmert.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.lxmert.modeling_tf_lxmert.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.lxmert.modeling_tf_lxmert.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.lxmert.modeling_tf_lxmert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.lxmert.modeling_tf_lxmert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.lxmert.modeling_tf_lxmert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.lxmert.modeling_tf_lxmert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.lxmert.modeling_tf_lxmert.mixed_key_layer->self.key(context)
A:transformers.models.lxmert.modeling_tf_lxmert.mixed_value_layer->self.value(context)
A:transformers.models.lxmert.modeling_tf_lxmert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.lxmert.modeling_tf_lxmert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.lxmert.modeling_tf_lxmert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.lxmert.modeling_tf_lxmert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.models.lxmert.modeling_tf_lxmert.dk->tensorflow.cast(shape_list(key_layer)[-1], dtype=attention_scores.dtype)
A:transformers.models.lxmert.modeling_tf_lxmert.attention_mask->tensorflow.cast(attention_mask, dtype=attention_scores.dtype)
A:transformers.models.lxmert.modeling_tf_lxmert.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.all_head_size))
A:transformers.models.lxmert.modeling_tf_lxmert.self.dense->tensorflow.keras.layers.Dense(hid_dim * 2, kernel_initializer=get_initializer(config.initializer_range), name='logit_fc_._0')
A:transformers.models.lxmert.modeling_tf_lxmert.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.lxmert.modeling_tf_lxmert.hidden_states->self.transform(hidden_states)
A:transformers.models.lxmert.modeling_tf_lxmert.self.self->TFLxmertAttention(config, name='self')
A:transformers.models.lxmert.modeling_tf_lxmert.self.attention_output->TFLxmertAttentionOutput(config, name='output')
A:transformers.models.lxmert.modeling_tf_lxmert.self_output->self.self(input_tensor, input_tensor, attention_mask, output_attentions)
A:transformers.models.lxmert.modeling_tf_lxmert.attention_output->self.attention_output(output[0], input_tensor, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.self.att->TFLxmertAttention(config, name='att')
A:transformers.models.lxmert.modeling_tf_lxmert.self.attention->TFLxmertSelfAttentionLayer(config, name='attention')
A:transformers.models.lxmert.modeling_tf_lxmert.self.intermediate->TFLxmertIntermediate(config, name='intermediate')
A:transformers.models.lxmert.modeling_tf_lxmert.self.transformer_output->TFLxmertOutput(config, name='output')
A:transformers.models.lxmert.modeling_tf_lxmert.attention_outputs->self.attention(hidden_states, attention_mask, output_attentions, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.lxmert.modeling_tf_lxmert.layer_output->self.transformer_output(intermediate_output, attention_output, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.self.visual_attention->TFLxmertCrossAttentionLayer(config, name='visual_attention')
A:transformers.models.lxmert.modeling_tf_lxmert.self.lang_self_att->TFLxmertSelfAttentionLayer(config, name='lang_self_att')
A:transformers.models.lxmert.modeling_tf_lxmert.self.visn_self_att->TFLxmertSelfAttentionLayer(config, name='visn_self_att')
A:transformers.models.lxmert.modeling_tf_lxmert.self.lang_inter->TFLxmertIntermediate(config, name='lang_inter')
A:transformers.models.lxmert.modeling_tf_lxmert.self.lang_output->TFLxmertOutput(config, name='lang_output')
A:transformers.models.lxmert.modeling_tf_lxmert.self.visn_inter->TFLxmertIntermediate(config, name='visn_inter')
A:transformers.models.lxmert.modeling_tf_lxmert.self.visn_output->TFLxmertOutput(config, name='visn_output')
A:transformers.models.lxmert.modeling_tf_lxmert.lang_attention_lang_input->tensorflow.identity(lang_input)
A:transformers.models.lxmert.modeling_tf_lxmert.visn_attention_lang_input->tensorflow.identity(lang_input)
A:transformers.models.lxmert.modeling_tf_lxmert.lang_attention_visn_input->tensorflow.identity(visn_input)
A:transformers.models.lxmert.modeling_tf_lxmert.visn_attention_visn_input->tensorflow.identity(visn_input)
A:transformers.models.lxmert.modeling_tf_lxmert.lang_att_output->self.lang_self_att(lang_input, lang_attention_mask, output_attentions, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.visn_att_output->self.visn_self_att(visn_input, visn_attention_mask, output_attentions, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.lang_inter_output->self.lang_inter(lang_input)
A:transformers.models.lxmert.modeling_tf_lxmert.visn_inter_output->self.visn_inter(visn_input)
A:transformers.models.lxmert.modeling_tf_lxmert.lang_output->self.lang_output(lang_inter_output, lang_input, training)
A:transformers.models.lxmert.modeling_tf_lxmert.visn_output->self.visn_output(visn_inter_output, visn_input, training)
A:transformers.models.lxmert.modeling_tf_lxmert.(lang_att_output, visn_att_output)->self.self_att(lang_att_output[0], lang_attention_mask, visn_att_output[0], visn_attention_mask, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.(lang_output, visn_output)->self.output_fc(lang_att_output, visn_att_output, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.visual_feats->tensorflow.random.uniform((batch_size, num_visual_features, self.config.visual_feat_dim))
A:transformers.models.lxmert.modeling_tf_lxmert.l_outputs->layer_module(lang_feats, lang_attention_mask, output_attentions, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.v_outputs->layer_module(visual_feats, visual_attention_mask, output_attentions, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.x_outputs->layer_module(lang_feats, lang_attention_mask, visual_feats, visual_attention_mask, output_attentions, training=training)
A:transformers.models.lxmert.modeling_tf_lxmert.input_ids->tensorflow.constant([[3, 5, 6], [2, 3, 4]])
A:transformers.models.lxmert.modeling_tf_lxmert.visual_pos->tensorflow.random.uniform((batch_size, num_visual_features, 4))
A:transformers.models.lxmert.modeling_tf_lxmert.self.embeddings->TFLxmertEmbeddings(config, name='embeddings')
A:transformers.models.lxmert.modeling_tf_lxmert.self.encoder->TFLxmertEncoder(config, name='encoder')
A:transformers.models.lxmert.modeling_tf_lxmert.self.pooler->TFLxmertPooler(config, name='pooler')
A:transformers.models.lxmert.modeling_tf_lxmert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, visual_feats=visual_feats, visual_pos=visual_pos, attention_mask=attention_mask, visual_attention_mask=visual_attention_mask, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, masked_lm_labels=masked_lm_labels, obj_labels=obj_labels, matched_label=matched_label, ans=ans, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training, kwargs_call=kwargs)
A:transformers.models.lxmert.modeling_tf_lxmert.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.lxmert.modeling_tf_lxmert.inputs['attention_mask']->tensorflow.fill(input_shape, 1)
A:transformers.models.lxmert.modeling_tf_lxmert.inputs['token_type_ids']->tensorflow.fill(input_shape, 0)
A:transformers.models.lxmert.modeling_tf_lxmert.embedding_output->self.embeddings(inputs['input_ids'], inputs['token_type_ids'], inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.lxmert.modeling_tf_lxmert.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.lxmert.modeling_tf_lxmert.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.lxmert.modeling_tf_lxmert.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.lxmert.modeling_tf_lxmert.extended_visual_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_visual_attention_mask), ten_thousand_cst)
A:transformers.models.lxmert.modeling_tf_lxmert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, inputs['visual_feats'], inputs['visual_pos'], extended_visual_attention_mask, output_attentions=inputs['output_attentions'], training=inputs['training'])
A:transformers.models.lxmert.modeling_tf_lxmert.pooled_output->self.dense(first_token_tensor)
A:transformers.models.lxmert.modeling_tf_lxmert.self.lxmert->TFLxmertMainLayer(config, name='lxmert')
A:transformers.models.lxmert.modeling_tf_lxmert.outputs->self.lxmert(input_ids=inputs['input_ids'], visual_feats=inputs['visual_feats'], visual_pos=inputs['visual_pos'], attention_mask=inputs['attention_mask'], visual_attention_mask=inputs['visual_attention_mask'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.lxmert.modeling_tf_lxmert.self.transform_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.lxmert.modeling_tf_lxmert.self.transform->TFLxmertPredictionHeadTransform(config, name='transform')
A:transformers.models.lxmert.modeling_tf_lxmert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.lxmert.modeling_tf_lxmert.self.predictions->TFLxmertLMPredictionHead(config, input_embeddings, name='predictions')
A:transformers.models.lxmert.modeling_tf_lxmert.prediction_scores->self.predictions(sequence_output)
A:transformers.models.lxmert.modeling_tf_lxmert.self.seq_relationship->tensorflow.keras.layers.Dense(2, kernel_initializer=get_initializer(config.initializer_range), name='seq_relationship')
A:transformers.models.lxmert.modeling_tf_lxmert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.lxmert.modeling_tf_lxmert.self.activation->get_tf_activation('gelu')
A:transformers.models.lxmert.modeling_tf_lxmert.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='logit_fc_._2')
A:transformers.models.lxmert.modeling_tf_lxmert.self.dense_1->tensorflow.keras.layers.Dense(num_labels, kernel_initializer=get_initializer(config.initializer_range), name='logit_fc_._3')
A:transformers.models.lxmert.modeling_tf_lxmert.output[key]->self.decoder_dict[key](hidden_states)
A:transformers.models.lxmert.modeling_tf_lxmert.self.cls->TFLxmertPreTrainingHeads(config, self.lxmert.embeddings, name='cls')
A:transformers.models.lxmert.modeling_tf_lxmert.self.obj_predict_head->TFLxmertVisualObjHead(config, name='obj_predict_head')
A:transformers.models.lxmert.modeling_tf_lxmert.self.answer_head->TFLxmertVisualAnswerHead(config, self.num_qa_labels, name='answer_head')
A:transformers.models.lxmert.modeling_tf_lxmert.lxmert_output->self.lxmert(input_ids=inputs['input_ids'], visual_feats=inputs['visual_feats'], visual_pos=inputs['visual_pos'], attention_mask=inputs['attention_mask'], visual_attention_mask=inputs['visual_attention_mask'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.lxmert.modeling_tf_lxmert.(lang_prediction_scores, cross_relationship_score)->self.cls(lang_output, pooled_output)
A:transformers.models.lxmert.modeling_tf_lxmert.answer_score->self.answer_head(pooled_output)
A:transformers.models.lxmert.modeling_tf_lxmert.masked_lm_loss->self.loss_fcts['ce'](tf.reshape(inputs['masked_lm_labels'], [-1]), tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]))
A:transformers.models.lxmert.modeling_tf_lxmert.matched_loss->self.loss_fcts['ce'](tf.reshape(inputs['matched_label'], [-1]), tf.reshape(cross_relationship_score, [-1, 2]))
A:transformers.models.lxmert.modeling_tf_lxmert.visn_prediction_scores_dict->self.obj_predict_head(visual_output)
A:transformers.models.lxmert.modeling_tf_lxmert.visn_loss->tensorflow.reduce_mean(visn_loss)
A:transformers.models.lxmert.modeling_tf_lxmert.answer_loss->self.loss_fcts['ce'](tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels]))
transformers.TFLxmertForPreTraining(self,config,*inputs,**kwargs)
transformers.TFLxmertForPreTraining.call(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,masked_lm_labels=None,obj_labels=None,matched_label=None,ans=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFLxmertForPreTraining.dummy_inputs(self)
transformers.TFLxmertForPreTraining.get_lm_head(self)
transformers.TFLxmertForPreTraining.get_prefix_bias_name(self)
transformers.TFLxmertForPreTraining.serving_output(self,output)
transformers.TFLxmertForPreTrainingOutput(ModelOutput)
transformers.TFLxmertMainLayer(self,config,**kwargs)
transformers.TFLxmertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFLxmertMainLayer.call(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFLxmertMainLayer.dummy_inputs(self)
transformers.TFLxmertMainLayer.get_input_embeddings(self)
transformers.TFLxmertMainLayer.set_input_embeddings(self,value)
transformers.TFLxmertModel(self,config,*inputs,**kwargs)
transformers.TFLxmertModel.call(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFLxmertModel.serving_output(self,output)
transformers.TFLxmertModelOutput(ModelOutput)
transformers.TFLxmertPreTrainedModel(TFPreTrainedModel)
transformers.TFLxmertPreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.TFLxmertPreTrainedModel.serving(self,inputs)
transformers.TFLxmertVisualFeatureEncoder(self,config,**kwargs)
transformers.TFLxmertVisualFeatureEncoder.call(self,visn_input,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttention(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttention.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttention.call(self,hidden_states,context,attention_mask,output_attentions,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttention.transpose_for_scores(self,x,batch_size)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttentionOutput(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttentionOutput.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertAttentionOutput.call(self,hidden_states,input_tensor,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertCrossAttentionLayer(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertCrossAttentionLayer.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertCrossAttentionLayer.call(self,input_tensor,ctx_tensor,ctx_att_mask,output_attentions=False,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEmbeddings(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEmbeddings.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEmbeddings.build(self,input_shape)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEmbeddings.call(self,input_ids=None,token_type_ids=None,inputs_embeds=None,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEncoder(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEncoder.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertEncoder.call(self,lang_feats=None,lang_attention_mask=None,visual_feats=None,visual_pos=None,visual_attention_mask=None,output_attentions=None,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining(self,config,*inputs,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining.__init__(self,config,*inputs,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining.call(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,masked_lm_labels=None,obj_labels=None,matched_label=None,ans=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining.dummy_inputs(self)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining.get_lm_head(self)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining.get_prefix_bias_name(self)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTraining.serving_output(self,output)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput(ModelOutput)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertIntermediate(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertIntermediate.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertIntermediate.call(self,hidden_states)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead(self,config:LxmertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.__init__(self,config:LxmertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.build(self,input_shape:tf.TensorShape)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.get_bias(self)->Dict[str, tf.Variable]
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.set_bias(self,value:tf.Variable)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLMPredictionHead.set_output_embeddings(self,value:tf.Variable)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLayer(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLayer.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertLayer.call(self,hidden_states,attention_mask,output_attentions,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMLMHead(self,config:LxmertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMLMHead.__init__(self,config:LxmertConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMLMHead.call(self,sequence_output:tf.Tensor)->tf.Tensor
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer._prune_heads(self,heads_to_prune)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer.call(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer.dummy_inputs(self)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer.get_input_embeddings(self)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertMainLayer.set_input_embeddings(self,value)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModel(self,config,*inputs,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModel.__init__(self,config,*inputs,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModel.call(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModel.serving_output(self,output)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput(ModelOutput)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertOutput(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertOutput.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertOutput.call(self,hidden_states,input_tensor,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPooler(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPooler.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPooler.call(self,hidden_states)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPreTrainedModel(TFPreTrainedModel)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPreTrainedModel.serving(self,inputs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPreTrainingHeads(self,config,input_embeddings,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPreTrainingHeads.__init__(self,config,input_embeddings,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPreTrainingHeads.call(self,sequence_output,pooled_output)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPredictionHeadTransform(self,config:LxmertConfig,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPredictionHeadTransform.__init__(self,config:LxmertConfig,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertPredictionHeadTransform.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertSelfAttentionLayer(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertSelfAttentionLayer.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertSelfAttentionLayer.call(self,input_tensor,attention_mask,output_attentions,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualAnswerHead(self,config,num_labels,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualAnswerHead.__init__(self,config,num_labels,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualAnswerHead.call(self,hidden_states)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualFeatureEncoder(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualFeatureEncoder.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualFeatureEncoder.call(self,visn_input,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualObjHead(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualObjHead.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertVisualObjHead.call(self,hidden_states)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertXLayer(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertXLayer.__init__(self,config,**kwargs)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertXLayer.call(self,lang_feats,lang_attention_mask,visn_feats,visn_attention_mask,output_attentions,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertXLayer.cross_att(self,lang_input,lang_attention_mask,visn_input,visn_attention_mask,output_attentions,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertXLayer.output_fc(self,lang_input,visn_input,training=False)
transformers.models.lxmert.modeling_tf_lxmert.TFLxmertXLayer.self_att(self,lang_input,lang_attention_mask,visn_input,visn_attention_mask,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.lxmert.convert_lxmert_original_tf_checkpoint_to_pytorch.config->transformers.LxmertConfig.from_json_file(config_file)
A:transformers.models.lxmert.convert_lxmert_original_tf_checkpoint_to_pytorch.model->LxmertForPreTraining(config)
A:transformers.models.lxmert.convert_lxmert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.lxmert.convert_lxmert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.lxmert.convert_lxmert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/tokenization_lxmert_fast.py----------------------------------------
transformers.LxmertTokenizerFast(BertTokenizerFast)
transformers.models.lxmert.tokenization_lxmert_fast.LxmertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/modeling_lxmert.py----------------------------------------
A:transformers.models.lxmert.modeling_lxmert.logger->utils.logging.get_logger(__name__)
A:transformers.models.lxmert.modeling_lxmert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.lxmert.modeling_lxmert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.lxmert.modeling_lxmert.array->numpy.transpose(array)
A:transformers.models.lxmert.modeling_lxmert.name->name.split('/').split('/')
A:transformers.models.lxmert.modeling_lxmert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.lxmert.modeling_lxmert.pointer->getattr(pointer, 'weight')
A:transformers.models.lxmert.modeling_lxmert.num->int(scope_names[1])
A:transformers.models.lxmert.modeling_lxmert.pointer.data->torch.from_numpy(array)
A:transformers.models.lxmert.modeling_lxmert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
A:transformers.models.lxmert.modeling_lxmert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=0)
A:transformers.models.lxmert.modeling_lxmert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size, padding_idx=0)
A:transformers.models.lxmert.modeling_lxmert.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=1e-12)
A:transformers.models.lxmert.modeling_lxmert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.lxmert.modeling_lxmert.input_shape->input_ids.size()
A:transformers.models.lxmert.modeling_lxmert.position_ids->position_ids.unsqueeze(0).expand(input_shape).unsqueeze(0).expand(input_shape)
A:transformers.models.lxmert.modeling_lxmert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.lxmert.modeling_lxmert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.lxmert.modeling_lxmert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.lxmert.modeling_lxmert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.lxmert.modeling_lxmert.embeddings->self.dropout(embeddings)
A:transformers.models.lxmert.modeling_lxmert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.lxmert.modeling_lxmert.self.query->torch.nn.Linear(config.hidden_size, self.head_size)
A:transformers.models.lxmert.modeling_lxmert.self.key->torch.nn.Linear(ctx_dim, self.head_size)
A:transformers.models.lxmert.modeling_lxmert.self.value->torch.nn.Linear(ctx_dim, self.head_size)
A:transformers.models.lxmert.modeling_lxmert.x->self.visn_layer_norm(x)
A:transformers.models.lxmert.modeling_lxmert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.lxmert.modeling_lxmert.mixed_key_layer->self.key(context)
A:transformers.models.lxmert.modeling_lxmert.mixed_value_layer->self.value(context)
A:transformers.models.lxmert.modeling_lxmert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.lxmert.modeling_lxmert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.models.lxmert.modeling_lxmert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.models.lxmert.modeling_lxmert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.lxmert.modeling_lxmert.attention_probs->self.dropout(attention_probs)
A:transformers.models.lxmert.modeling_lxmert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.lxmert.modeling_lxmert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.lxmert.modeling_lxmert.hidden_states->self.transform(hidden_states)
A:transformers.models.lxmert.modeling_lxmert.self.att->LxmertAttention(config)
A:transformers.models.lxmert.modeling_lxmert.self.output->LxmertOutput(config)
A:transformers.models.lxmert.modeling_lxmert.output->self.dropout(output)
A:transformers.models.lxmert.modeling_lxmert.attention_output->self.output(output[0], input_tensor)
A:transformers.models.lxmert.modeling_lxmert.self.self->LxmertAttention(config)
A:transformers.models.lxmert.modeling_lxmert.self.attention->LxmertSelfAttentionLayer(config)
A:transformers.models.lxmert.modeling_lxmert.self.intermediate->LxmertIntermediate(config)
A:transformers.models.lxmert.modeling_lxmert.outputs->self.attention(hidden_states, attention_mask, output_attentions=output_attentions)
A:transformers.models.lxmert.modeling_lxmert.intermediate_output->self.intermediate(attention_output)
A:transformers.models.lxmert.modeling_lxmert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.lxmert.modeling_lxmert.self.visual_attention->LxmertCrossAttentionLayer(config)
A:transformers.models.lxmert.modeling_lxmert.self.lang_self_att->LxmertSelfAttentionLayer(config)
A:transformers.models.lxmert.modeling_lxmert.self.visn_self_att->LxmertSelfAttentionLayer(config)
A:transformers.models.lxmert.modeling_lxmert.self.lang_inter->LxmertIntermediate(config)
A:transformers.models.lxmert.modeling_lxmert.self.lang_output->LxmertOutput(config)
A:transformers.models.lxmert.modeling_lxmert.self.visn_inter->LxmertIntermediate(config)
A:transformers.models.lxmert.modeling_lxmert.self.visn_output->LxmertOutput(config)
A:transformers.models.lxmert.modeling_lxmert.lang_att_output->self.lang_self_att(lang_input, lang_attention_mask, output_attentions=False)
A:transformers.models.lxmert.modeling_lxmert.visual_att_output->self.visn_self_att(visual_input, visual_attention_mask, output_attentions=False)
A:transformers.models.lxmert.modeling_lxmert.lang_inter_output->self.lang_inter(lang_input)
A:transformers.models.lxmert.modeling_lxmert.visual_inter_output->self.visn_inter(visual_input)
A:transformers.models.lxmert.modeling_lxmert.lang_output->self.lang_output(lang_inter_output, lang_input)
A:transformers.models.lxmert.modeling_lxmert.visual_output->self.visn_output(visual_inter_output, visual_input)
A:transformers.models.lxmert.modeling_lxmert.(lang_att_output, visual_att_output)->self.self_att(lang_att_output[0], lang_attention_mask, visual_att_output[0], visual_attention_mask)
A:transformers.models.lxmert.modeling_lxmert.(lang_output, visual_output)->self.output_fc(lang_att_output, visual_att_output)
A:transformers.models.lxmert.modeling_lxmert.self.visn_fc->LxmertVisualFeatureEncoder(config)
A:transformers.models.lxmert.modeling_lxmert.self.visn_layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=1e-12)
A:transformers.models.lxmert.modeling_lxmert.self.box_fc->torch.nn.Linear(pos_dim, config.hidden_size)
A:transformers.models.lxmert.modeling_lxmert.self.box_layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=1e-12)
A:transformers.models.lxmert.modeling_lxmert.y->self.box_layer_norm(y)
A:transformers.models.lxmert.modeling_lxmert.self.layer->torch.nn.ModuleList([LxmertLayer(config) for _ in range(self.num_l_layers)])
A:transformers.models.lxmert.modeling_lxmert.self.x_layers->torch.nn.ModuleList([LxmertXLayer(config) for _ in range(self.num_x_layers)])
A:transformers.models.lxmert.modeling_lxmert.self.r_layers->torch.nn.ModuleList([LxmertLayer(config) for _ in range(self.num_r_layers)])
A:transformers.models.lxmert.modeling_lxmert.visual_feats->self.visn_fc(visual_feats, visual_pos)
A:transformers.models.lxmert.modeling_lxmert.l_outputs->layer_module(lang_feats, lang_attention_mask, output_attentions=output_attentions)
A:transformers.models.lxmert.modeling_lxmert.v_outputs->layer_module(visual_feats, visual_attention_mask, output_attentions=output_attentions)
A:transformers.models.lxmert.modeling_lxmert.x_outputs->layer_module(lang_feats, lang_attention_mask, visual_feats, visual_attention_mask, output_attentions=output_attentions)
A:transformers.models.lxmert.modeling_lxmert.self.activation->torch.nn.Tanh()
A:transformers.models.lxmert.modeling_lxmert.pooled_output->self.pooler(lang_output)
A:transformers.models.lxmert.modeling_lxmert.self.transform->LxmertPredictionHeadTransform(config)
A:transformers.models.lxmert.modeling_lxmert.self.decoder->torch.nn.Linear(lxmert_model_embedding_weights.size(1), lxmert_model_embedding_weights.size(0), bias=False)
A:transformers.models.lxmert.modeling_lxmert.self.bias->torch.nn.Parameter(torch.zeros(lxmert_model_embedding_weights.size(0)))
A:transformers.models.lxmert.modeling_lxmert.self.logit_fc->torch.nn.Sequential(nn.Linear(hid_dim, hid_dim * 2), GeLU(), nn.LayerNorm(hid_dim * 2, eps=1e-12), nn.Linear(hid_dim * 2, num_labels))
A:transformers.models.lxmert.modeling_lxmert.self.decoder_dict->torch.nn.ModuleDict({key: nn.Linear(config.hidden_size, self.visual_losses[key]['num']) for key in self.visual_losses})
A:transformers.models.lxmert.modeling_lxmert.output[key]->self.decoder_dict[key](hidden_states)
A:transformers.models.lxmert.modeling_lxmert.self.predictions->LxmertLMPredictionHead(config, lxmert_model_embedding_weights)
A:transformers.models.lxmert.modeling_lxmert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.models.lxmert.modeling_lxmert.prediction_scores->self.predictions(sequence_output)
A:transformers.models.lxmert.modeling_lxmert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.lxmert.modeling_lxmert.self.embeddings->LxmertEmbeddings(config)
A:transformers.models.lxmert.modeling_lxmert.self.encoder->LxmertEncoder(config)
A:transformers.models.lxmert.modeling_lxmert.self.pooler->LxmertPooler(config)
A:transformers.models.lxmert.modeling_lxmert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.lxmert.modeling_lxmert.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.lxmert.modeling_lxmert.extended_visual_attention_mask->extended_visual_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.lxmert.modeling_lxmert.embedding_output->self.embeddings(input_ids, token_type_ids, inputs_embeds)
A:transformers.models.lxmert.modeling_lxmert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, visual_feats=visual_feats, visual_pos=visual_pos, visual_attention_mask=extended_visual_attention_mask, output_attentions=output_attentions)
A:transformers.models.lxmert.modeling_lxmert.self.lxmert->LxmertModel(config)
A:transformers.models.lxmert.modeling_lxmert.self.cls->LxmertPreTrainingHeads(config, self.lxmert.embeddings.word_embeddings.weight)
A:transformers.models.lxmert.modeling_lxmert.self.obj_predict_head->LxmertVisualObjHead(config)
A:transformers.models.lxmert.modeling_lxmert.self.answer_head->LxmertVisualAnswerHead(config, self.num_qa_labels)
A:transformers.models.lxmert.modeling_lxmert.cur_qa_logit_layer->self.get_qa_logit_layer()
A:transformers.models.lxmert.modeling_lxmert.new_qa_logit_layer->torch.nn.Linear(hidden_dim, num_labels, bias=False)
A:transformers.models.lxmert.modeling_lxmert.(cur_qa_labels, hidden_dim)->self.get_qa_logit_layer().weight.size()
A:transformers.models.lxmert.modeling_lxmert.num_labels_to_copy->min(cur_qa_labels, num_labels)
A:transformers.models.lxmert.modeling_lxmert.labels->kwargs.pop('masked_lm_labels')
A:transformers.models.lxmert.modeling_lxmert.lxmert_output->self.lxmert(input_ids=input_ids, visual_feats=visual_feats, visual_pos=visual_pos, token_type_ids=token_type_ids, attention_mask=attention_mask, visual_attention_mask=visual_attention_mask, inputs_embeds=inputs_embeds, output_hidden_states=output_hidden_states, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.models.lxmert.modeling_lxmert.(lang_prediction_scores, cross_relationship_score)->self.cls(lang_output, pooled_output)
A:transformers.models.lxmert.modeling_lxmert.answer_score->self.answer_head(pooled_output)
A:transformers.models.lxmert.modeling_lxmert.masked_lm_loss->self.loss_fcts['ce'](lang_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.lxmert.modeling_lxmert.matched_loss->self.loss_fcts['ce'](cross_relationship_score.view(-1, 2), matched_label.view(-1))
A:transformers.models.lxmert.modeling_lxmert.total_visual_loss->torch.tensor(0.0, device=input_ids.device)
A:transformers.models.lxmert.modeling_lxmert.visual_prediction_scores_dict->self.obj_predict_head(visual_output)
A:transformers.models.lxmert.modeling_lxmert.visual_loss->visual_loss.mean(1).mean(1)
A:transformers.models.lxmert.modeling_lxmert.answer_loss->self.loss_fcts['ce'](answer_score.view(-1, self.num_qa_labels), ans.view(-1))
A:transformers.models.lxmert.modeling_lxmert.self.loss->CrossEntropyLoss()
A:transformers.models.lxmert.modeling_lxmert.loss->self.loss(answer_score.view(-1, self.num_qa_labels), labels.view(-1))
transformers.LxmertEncoder(self,config)
transformers.LxmertEncoder.forward(self,lang_feats,lang_attention_mask,visual_feats,visual_pos,visual_attention_mask=None,output_attentions=None)
transformers.LxmertForPreTraining(self,config)
transformers.LxmertForPreTraining._get_resized_qa_labels(self,cur_qa_logit_layer,num_labels)
transformers.LxmertForPreTraining._resize_qa_labels(self,num_labels)
transformers.LxmertForPreTraining._set_qa_logit_layer(self,qa_logit_layer)
transformers.LxmertForPreTraining.forward(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,obj_labels=None,matched_label=None,ans=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.LxmertForPreTraining.get_qa_logit_layer(self)->nn.Module
transformers.LxmertForPreTraining.resize_num_qa_labels(self,num_labels)
transformers.LxmertForPreTrainingOutput(ModelOutput)
transformers.LxmertForQuestionAnswering(self,config)
transformers.LxmertForQuestionAnswering._get_resized_qa_labels(self,cur_qa_logit_layer,num_labels)
transformers.LxmertForQuestionAnswering._resize_qa_labels(self,num_labels)
transformers.LxmertForQuestionAnswering._set_qa_logit_layer(self,qa_logit_layer)
transformers.LxmertForQuestionAnswering.forward(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LxmertForQuestionAnswering.get_qa_logit_layer(self)->nn.Module
transformers.LxmertForQuestionAnswering.resize_num_qa_labels(self,num_labels)
transformers.LxmertForQuestionAnsweringOutput(ModelOutput)
transformers.LxmertModel(self,config)
transformers.LxmertModel.forward(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LxmertModel.get_input_embeddings(self)
transformers.LxmertModel.set_input_embeddings(self,new_embeddings)
transformers.LxmertModelOutput(ModelOutput)
transformers.LxmertPreTrainedModel(PreTrainedModel)
transformers.LxmertPreTrainedModel._init_weights(self,module)
transformers.LxmertVisualFeatureEncoder(self,config)
transformers.LxmertVisualFeatureEncoder.forward(self,visual_feats,visual_pos)
transformers.LxmertXLayer(self,config)
transformers.LxmertXLayer.cross_att(self,lang_input,lang_attention_mask,visual_input,visual_attention_mask,output_x_attentions=False)
transformers.LxmertXLayer.forward(self,lang_feats,lang_attention_mask,visual_feats,visual_attention_mask,output_attentions=False)
transformers.LxmertXLayer.output_fc(self,lang_input,visual_input)
transformers.LxmertXLayer.self_att(self,lang_input,lang_attention_mask,visual_input,visual_attention_mask)
transformers.models.lxmert.modeling_lxmert.GeLU(self)
transformers.models.lxmert.modeling_lxmert.GeLU.__init__(self)
transformers.models.lxmert.modeling_lxmert.GeLU.forward(self,x)
transformers.models.lxmert.modeling_lxmert.LxmertAttention(self,config,ctx_dim=None)
transformers.models.lxmert.modeling_lxmert.LxmertAttention.__init__(self,config,ctx_dim=None)
transformers.models.lxmert.modeling_lxmert.LxmertAttention.forward(self,hidden_states,context,attention_mask=None,output_attentions=False)
transformers.models.lxmert.modeling_lxmert.LxmertAttention.transpose_for_scores(self,x)
transformers.models.lxmert.modeling_lxmert.LxmertAttentionOutput(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertAttentionOutput.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertAttentionOutput.forward(self,hidden_states,input_tensor)
transformers.models.lxmert.modeling_lxmert.LxmertCrossAttentionLayer(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertCrossAttentionLayer.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertCrossAttentionLayer.forward(self,input_tensor,ctx_tensor,ctx_att_mask=None,output_attentions=False)
transformers.models.lxmert.modeling_lxmert.LxmertEmbeddings(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertEmbeddings.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertEmbeddings.forward(self,input_ids,token_type_ids=None,inputs_embeds=None)
transformers.models.lxmert.modeling_lxmert.LxmertEncoder(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertEncoder.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertEncoder.forward(self,lang_feats,lang_attention_mask,visual_feats,visual_pos,visual_attention_mask=None,output_attentions=None)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining._get_resized_qa_labels(self,cur_qa_logit_layer,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining._resize_qa_labels(self,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining._set_qa_logit_layer(self,qa_logit_layer)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining.forward(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,obj_labels=None,matched_label=None,ans=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining.get_qa_logit_layer(self)->nn.Module
transformers.models.lxmert.modeling_lxmert.LxmertForPreTraining.resize_num_qa_labels(self,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput(ModelOutput)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering._get_resized_qa_labels(self,cur_qa_logit_layer,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering._resize_qa_labels(self,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering._set_qa_logit_layer(self,qa_logit_layer)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering.forward(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering.get_qa_logit_layer(self)->nn.Module
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnswering.resize_num_qa_labels(self,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput(ModelOutput)
transformers.models.lxmert.modeling_lxmert.LxmertIntermediate(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertIntermediate.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertIntermediate.forward(self,hidden_states)
transformers.models.lxmert.modeling_lxmert.LxmertLMPredictionHead(self,config,lxmert_model_embedding_weights)
transformers.models.lxmert.modeling_lxmert.LxmertLMPredictionHead.__init__(self,config,lxmert_model_embedding_weights)
transformers.models.lxmert.modeling_lxmert.LxmertLMPredictionHead.forward(self,hidden_states)
transformers.models.lxmert.modeling_lxmert.LxmertLayer(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertLayer.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertLayer.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.models.lxmert.modeling_lxmert.LxmertModel(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertModel.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertModel.forward(self,input_ids=None,visual_feats=None,visual_pos=None,attention_mask=None,visual_attention_mask=None,token_type_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.lxmert.modeling_lxmert.LxmertModel.get_input_embeddings(self)
transformers.models.lxmert.modeling_lxmert.LxmertModel.set_input_embeddings(self,new_embeddings)
transformers.models.lxmert.modeling_lxmert.LxmertModelOutput(ModelOutput)
transformers.models.lxmert.modeling_lxmert.LxmertOutput(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertOutput.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertOutput.forward(self,hidden_states,input_tensor)
transformers.models.lxmert.modeling_lxmert.LxmertPooler(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertPooler.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertPooler.forward(self,hidden_states)
transformers.models.lxmert.modeling_lxmert.LxmertPreTrainedModel(PreTrainedModel)
transformers.models.lxmert.modeling_lxmert.LxmertPreTrainedModel._init_weights(self,module)
transformers.models.lxmert.modeling_lxmert.LxmertPreTrainingHeads(self,config,lxmert_model_embedding_weights)
transformers.models.lxmert.modeling_lxmert.LxmertPreTrainingHeads.__init__(self,config,lxmert_model_embedding_weights)
transformers.models.lxmert.modeling_lxmert.LxmertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.models.lxmert.modeling_lxmert.LxmertPredictionHeadTransform(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertPredictionHeadTransform.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.lxmert.modeling_lxmert.LxmertSelfAttentionLayer(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertSelfAttentionLayer.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertSelfAttentionLayer.forward(self,input_tensor,attention_mask,output_attentions=False)
transformers.models.lxmert.modeling_lxmert.LxmertVisualAnswerHead(self,config,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertVisualAnswerHead.__init__(self,config,num_labels)
transformers.models.lxmert.modeling_lxmert.LxmertVisualAnswerHead.forward(self,hidden_states)
transformers.models.lxmert.modeling_lxmert.LxmertVisualFeatureEncoder(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertVisualFeatureEncoder.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertVisualFeatureEncoder.forward(self,visual_feats,visual_pos)
transformers.models.lxmert.modeling_lxmert.LxmertVisualObjHead(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertVisualObjHead.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertVisualObjHead.forward(self,hidden_states)
transformers.models.lxmert.modeling_lxmert.LxmertXLayer(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertXLayer.__init__(self,config)
transformers.models.lxmert.modeling_lxmert.LxmertXLayer.cross_att(self,lang_input,lang_attention_mask,visual_input,visual_attention_mask,output_x_attentions=False)
transformers.models.lxmert.modeling_lxmert.LxmertXLayer.forward(self,lang_feats,lang_attention_mask,visual_feats,visual_attention_mask,output_attentions=False)
transformers.models.lxmert.modeling_lxmert.LxmertXLayer.output_fc(self,lang_input,visual_input)
transformers.models.lxmert.modeling_lxmert.LxmertXLayer.self_att(self,lang_input,lang_attention_mask,visual_input,visual_attention_mask)
transformers.models.lxmert.modeling_lxmert.load_tf_weights_in_lxmert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/__init__.py----------------------------------------
A:transformers.models.lxmert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/lxmert/configuration_lxmert.py----------------------------------------
A:transformers.models.lxmert.configuration_lxmert.logger->utils.logging.get_logger(__name__)
transformers.LxmertConfig(self,vocab_size=30522,hidden_size=768,num_attention_heads=12,num_labels=2,num_qa_labels=9500,num_object_labels=1600,num_attr_labels=400,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,l_layers=9,x_layers=5,r_layers=5,visual_feat_dim=2048,visual_pos_dim=4,visual_loss_normalizer=6.67,task_matched=True,task_mask_lm=True,task_obj_predict=True,task_qa=True,visual_obj_loss=True,visual_attr_loss=True,visual_feat_loss=True,output_attentions=False,output_hidden_states=False,**kwargs)
transformers.models.lxmert.configuration_lxmert.LxmertConfig(self,vocab_size=30522,hidden_size=768,num_attention_heads=12,num_labels=2,num_qa_labels=9500,num_object_labels=1600,num_attr_labels=400,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,l_layers=9,x_layers=5,r_layers=5,visual_feat_dim=2048,visual_pos_dim=4,visual_loss_normalizer=6.67,task_matched=True,task_mask_lm=True,task_obj_predict=True,task_qa=True,visual_obj_loss=True,visual_attr_loss=True,visual_feat_loss=True,output_attentions=False,output_hidden_states=False,**kwargs)
transformers.models.lxmert.configuration_lxmert.LxmertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_attention_heads=12,num_labels=2,num_qa_labels=9500,num_object_labels=1600,num_attr_labels=400,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,l_layers=9,x_layers=5,r_layers=5,visual_feat_dim=2048,visual_pos_dim=4,visual_loss_normalizer=6.67,task_matched=True,task_mask_lm=True,task_obj_predict=True,task_qa=True,visual_obj_loss=True,visual_attr_loss=True,visual_feat_loss=True,output_attentions=False,output_hidden_states=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/flaubert/modeling_tf_flaubert.py----------------------------------------
A:transformers.models.flaubert.modeling_tf_flaubert.logger->utils.logging.get_logger(__name__)
A:transformers.models.flaubert.modeling_tf_flaubert.alen->tensorflow.range(slen)
A:transformers.models.flaubert.modeling_tf_flaubert.mask->tensorflow.cast(mask, dtype=tensor.dtype)
A:transformers.models.flaubert.modeling_tf_flaubert.attn_mask->tensorflow.less_equal(tf.tile(tf.reshape(alen, (1, 1, slen)), (bs, slen, 1)), tf.reshape(alen, (1, slen, 1)))
A:transformers.models.flaubert.modeling_tf_flaubert.inputs_list->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.models.flaubert.modeling_tf_flaubert.attns_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.models.flaubert.modeling_tf_flaubert.self.transformer->TFFlaubertMainLayer(config, name='transformer')
A:transformers.models.flaubert.modeling_tf_flaubert.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training, kwargs_call=kwargs)
A:transformers.models.flaubert.modeling_tf_flaubert.outputs->self.pred_layer(output)
A:transformers.models.flaubert.modeling_tf_flaubert.NEW_ID->itertools.count()
A:transformers.models.flaubert.modeling_tf_flaubert.self.layer_id->next(TFFlaubertMultiHeadAttention.NEW_ID)
A:transformers.models.flaubert.modeling_tf_flaubert.self.q_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='q_lin')
A:transformers.models.flaubert.modeling_tf_flaubert.self.k_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='k_lin')
A:transformers.models.flaubert.modeling_tf_flaubert.self.v_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='v_lin')
A:transformers.models.flaubert.modeling_tf_flaubert.self.out_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='out_lin')
A:transformers.models.flaubert.modeling_tf_flaubert.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.flaubert.modeling_tf_flaubert.self.pruned_heads->set()
A:transformers.models.flaubert.modeling_tf_flaubert.(bs, qlen, dim)->shape_list(input)
A:transformers.models.flaubert.modeling_tf_flaubert.q->tensorflow.multiply(q, tf.math.rsqrt(f_dim_per_head))
A:transformers.models.flaubert.modeling_tf_flaubert.k->tensorflow.cast(k, dtype=q.dtype)
A:transformers.models.flaubert.modeling_tf_flaubert.v->tensorflow.concat([v_, v], axis=2)
A:transformers.models.flaubert.modeling_tf_flaubert.f_dim_per_head->tensorflow.cast(dim_per_head, dtype=q.dtype)
A:transformers.models.flaubert.modeling_tf_flaubert.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.models.flaubert.modeling_tf_flaubert.weights->self.dropout(weights, training=training)
A:transformers.models.flaubert.modeling_tf_flaubert.context->unshape(context)
A:transformers.models.flaubert.modeling_tf_flaubert.self.lin1->tensorflow.keras.layers.Dense(dim_hidden, kernel_initializer=get_initializer(config.init_std), name='lin1')
A:transformers.models.flaubert.modeling_tf_flaubert.self.lin2->tensorflow.keras.layers.Dense(out_dim, kernel_initializer=get_initializer(config.init_std), name='lin2')
A:transformers.models.flaubert.modeling_tf_flaubert.x->self.dropout(x, training=training)
A:transformers.models.flaubert.modeling_tf_flaubert.self.layerdrop->getattr(config, 'layerdrop', 0.0)
A:transformers.models.flaubert.modeling_tf_flaubert.self.pre_norm->getattr(config, 'pre_norm', False)
A:transformers.models.flaubert.modeling_tf_flaubert.self.embeddings->TFSharedEmbeddings(self.n_words, self.dim, initializer_range=config.embed_init_std, name='embeddings')
A:transformers.models.flaubert.modeling_tf_flaubert.self.layer_norm_emb->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm_emb')
A:transformers.models.flaubert.modeling_tf_flaubert.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.dim], initializer=get_initializer(self.embed_init_std))
A:transformers.models.flaubert.modeling_tf_flaubert.self.lang_embeddings->self.add_weight(name='embeddings', shape=[self.n_langs, self.dim], initializer=get_initializer(self.embed_init_std))
A:transformers.models.flaubert.modeling_tf_flaubert.(bs, slen)->shape_list(inputs['input_ids'])
A:transformers.models.flaubert.modeling_tf_flaubert.inputs['lengths']->tensorflow.convert_to_tensor([slen] * bs)
A:transformers.models.flaubert.modeling_tf_flaubert.(mask, attn_mask)->get_masks(slen, inputs['lengths'], self.causal, padding_mask=inputs['attention_mask'])
A:transformers.models.flaubert.modeling_tf_flaubert.inputs['position_ids']->tensorflow.tile(inputs['position_ids'], (bs, 1))
A:transformers.models.flaubert.modeling_tf_flaubert.inputs['inputs_embeds']->self.embeddings(inputs['input_ids'])
A:transformers.models.flaubert.modeling_tf_flaubert.tensor->self.layer_norm2[i](tensor)
A:transformers.models.flaubert.modeling_tf_flaubert.dropout_probability->random.uniform(0, 1)
A:transformers.models.flaubert.modeling_tf_flaubert.attn_outputs->self.attentions[i](tensor_normalized, attn_mask, None, inputs['cache'], inputs['head_mask'][i], inputs['output_attentions'], training=inputs['training'])
A:transformers.models.flaubert.modeling_tf_flaubert.attn->self.dropout(attn, training=inputs['training'])
A:transformers.models.flaubert.modeling_tf_flaubert.tensor_normalized->self.layer_norm2[i](tensor)
A:transformers.models.flaubert.modeling_tf_flaubert.self.bias->self.add_weight(shape=(self.n_words,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.flaubert.modeling_tf_flaubert.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.models.flaubert.modeling_tf_flaubert.self.pred_layer->TFFlaubertPredLayer(config, self.transformer.embeddings, name='pred_layer_._proj')
A:transformers.models.flaubert.modeling_tf_flaubert.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], langs=inputs['langs'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], lengths=inputs['lengths'], cache=inputs['cache'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
transformers.TFFlaubertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFFlaubertForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFFlaubertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFFlaubertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFFlaubertModel(self,config,*inputs,**kwargs)
transformers.TFFlaubertModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFFlaubertModel.serving_output(self,output)
transformers.TFFlaubertPreTrainedModel(TFPreTrainedModel)
transformers.TFFlaubertPreTrainedModel.dummy_inputs(self)
transformers.TFFlaubertWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFFlaubertWithLMHeadModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFFlaubertWithLMHeadModel.get_lm_head(self)
transformers.TFFlaubertWithLMHeadModel.get_prefix_bias_name(self)
transformers.TFFlaubertWithLMHeadModel.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.TFFlaubertWithLMHeadModel.serving_output(self,output)
transformers.TFFlaubertWithLMHeadModelOutput(ModelOutput)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMainLayer(self,config,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMainLayer.__init__(self,config,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMainLayer.build(self,input_shape)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMainLayer.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMainLayer.get_input_embeddings(self)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMainLayer.set_input_embeddings(self,value)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertModel(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertModel.__init__(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertModel.serving_output(self,output)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMultiHeadAttention(self,n_heads,dim,config,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMultiHeadAttention.__init__(self,n_heads,dim,config,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMultiHeadAttention.call(self,input,mask,kv,cache,head_mask,output_attentions,training=False)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertMultiHeadAttention.prune_heads(self,heads)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPreTrainedModel(TFPreTrainedModel)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPreTrainedModel.dummy_inputs(self)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer(self,config,input_embeddings,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.__init__(self,config,input_embeddings,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.build(self,input_shape)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.call(self,hidden_states)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.get_bias(self)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.get_output_embeddings(self)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.set_bias(self,value)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertPredLayer.set_output_embeddings(self,value)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertTransformerFFN(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertTransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertTransformerFFN.call(self,input,training=False)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.call(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.get_lm_head(self)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.get_prefix_bias_name(self)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.serving_output(self,output)
transformers.models.flaubert.modeling_tf_flaubert.TFFlaubertWithLMHeadModelOutput(ModelOutput)
transformers.models.flaubert.modeling_tf_flaubert.get_masks(slen,lengths,causal,padding_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/flaubert/modeling_flaubert.py----------------------------------------
A:transformers.models.flaubert.modeling_flaubert.logger->utils.logging.get_logger(__name__)
A:transformers.models.flaubert.modeling_flaubert.self.layerdrop->getattr(config, 'layerdrop', 0.0)
A:transformers.models.flaubert.modeling_flaubert.self.pre_norm->getattr(config, 'pre_norm', False)
A:transformers.models.flaubert.modeling_flaubert.(bs, slen)->input_ids.size()
A:transformers.models.flaubert.modeling_flaubert.lengths->torch.tensor([slen] * bs, device=device)
A:transformers.models.flaubert.modeling_flaubert.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.models.flaubert.modeling_flaubert.position_ids->position_ids.unsqueeze(0).expand((bs, slen)).unsqueeze(0).expand((bs, slen))
A:transformers.models.flaubert.modeling_flaubert.head_mask->self.get_head_mask(head_mask, self.config.n_layers)
A:transformers.models.flaubert.modeling_flaubert.inputs_embeds->self.embeddings(input_ids)
A:transformers.models.flaubert.modeling_flaubert.tensor->self.layer_norm2[i](tensor)
A:transformers.models.flaubert.modeling_flaubert.dropout_probability->random.uniform(0, 1)
A:transformers.models.flaubert.modeling_flaubert.attn_outputs->self.attentions[i](tensor_normalized, attn_mask, cache=cache, head_mask=head_mask[i])
A:transformers.models.flaubert.modeling_flaubert.attn->torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)
A:transformers.models.flaubert.modeling_flaubert.tensor_normalized->self.layer_norm2[i](tensor)
A:transformers.models.flaubert.modeling_flaubert.self.transformer->FlaubertModel(config)
transformers.FlaubertForMultipleChoice(self,config)
transformers.FlaubertForQuestionAnswering(self,config)
transformers.FlaubertForQuestionAnsweringSimple(self,config)
transformers.FlaubertForSequenceClassification(self,config)
transformers.FlaubertForTokenClassification(self,config)
transformers.FlaubertModel(self,config)
transformers.FlaubertModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FlaubertWithLMHeadModel(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForMultipleChoice(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForMultipleChoice.__init__(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnswering(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnswering.__init__(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringSimple(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringSimple.__init__(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForSequenceClassification(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForSequenceClassification.__init__(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForTokenClassification(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertForTokenClassification.__init__(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertModel(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertModel.__init__(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.flaubert.modeling_flaubert.FlaubertWithLMHeadModel(self,config)
transformers.models.flaubert.modeling_flaubert.FlaubertWithLMHeadModel.__init__(self,config)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/flaubert/configuration_flaubert.py----------------------------------------
A:transformers.models.flaubert.configuration_flaubert.logger->utils.logging.get_logger(__name__)
transformers.FlaubertConfig(self,layerdrop=0.0,pre_norm=False,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.models.flaubert.configuration_flaubert.FlaubertConfig(self,layerdrop=0.0,pre_norm=False,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.models.flaubert.configuration_flaubert.FlaubertConfig.__init__(self,layerdrop=0.0,pre_norm=False,pad_token_id=2,bos_token_id=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/flaubert/tokenization_flaubert.py----------------------------------------
A:transformers.models.flaubert.tokenization_flaubert.logger->utils.logging.get_logger(__name__)
A:transformers.models.flaubert.tokenization_flaubert.text->self.moses_tokenize(text, lang=lang)
transformers.FlaubertTokenizer(self,do_lowercase=False,**kwargs)
transformers.FlaubertTokenizer._tokenize(self,text,bypass_tokenizer=False)
transformers.FlaubertTokenizer.preprocess_text(self,text)
transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer(self,do_lowercase=False,**kwargs)
transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer.__init__(self,do_lowercase=False,**kwargs)
transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer._tokenize(self,text,bypass_tokenizer=False)
transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer.preprocess_text(self,text)
transformers.models.flaubert.tokenization_flaubert.convert_to_unicode(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/flaubert/__init__.py----------------------------------------
A:transformers.models.flaubert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_prophetnet/tokenization_xlm_prophetnet.py----------------------------------------
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.vocab->collections.OrderedDict()
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.tokens->reader.readlines()
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.token->token.rstrip('\n').rstrip('\n')
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.state->self.__dict__.copy()
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.spm_id->self.sp_model.PieceToId(token)
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.XLMProphetNetTokenizer(self,vocab_file,bos_token='[SEP]',eos_token='[SEP]',sep_token='[SEP]',unk_token='[UNK]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.XLMProphetNetTokenizer.__getstate__(self)
transformers.XLMProphetNetTokenizer.__setstate__(self,d)
transformers.XLMProphetNetTokenizer._convert_id_to_token(self,index)
transformers.XLMProphetNetTokenizer._convert_token_to_id(self,token)
transformers.XLMProphetNetTokenizer._tokenize(self,text:str)->str
transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMProphetNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMProphetNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLMProphetNetTokenizer.get_vocab(self)
transformers.XLMProphetNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.XLMProphetNetTokenizer.vocab_size(self)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer(self,vocab_file,bos_token='[SEP]',eos_token='[SEP]',sep_token='[SEP]',unk_token='[UNK]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.__getstate__(self)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.__init__(self,vocab_file,bos_token='[SEP]',eos_token='[SEP]',sep_token='[SEP]',unk_token='[UNK]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.__setstate__(self,d)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer._convert_id_to_token(self,index)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer._convert_token_to_id(self,token)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer._tokenize(self,text:str)->str
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.get_vocab(self)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.XLMProphetNetTokenizer.vocab_size(self)
transformers.models.xlm_prophetnet.tokenization_xlm_prophetnet.load_vocab(vocab_file)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_prophetnet/modeling_xlm_prophetnet.py----------------------------------------
A:transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.logger->utils.logging.get_logger(__name__)
transformers.XLMProphetNetDecoder(ProphetNetDecoder)
transformers.XLMProphetNetEncoder(ProphetNetEncoder)
transformers.XLMProphetNetForCausalLM(ProphetNetForCausalLM)
transformers.XLMProphetNetForConditionalGeneration(ProphetNetForConditionalGeneration)
transformers.XLMProphetNetModel(ProphetNetModel)
transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetDecoder(ProphetNetDecoder)
transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetEncoder(ProphetNetEncoder)
transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetForCausalLM(ProphetNetForCausalLM)
transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetForConditionalGeneration(ProphetNetForConditionalGeneration)
transformers.models.xlm_prophetnet.modeling_xlm_prophetnet.XLMProphetNetModel(ProphetNetModel)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_prophetnet/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlm_prophetnet/configuration_xlm_prophetnet.py----------------------------------------
A:transformers.models.xlm_prophetnet.configuration_xlm_prophetnet.logger->utils.logging.get_logger(__name__)
transformers.XLMProphetNetConfig(ProphetNetConfig)
transformers.models.xlm_prophetnet.configuration_xlm_prophetnet.XLMProphetNetConfig(ProphetNetConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/vit/convert_vit_timm_to_pytorch.py----------------------------------------
A:transformers.models.vit.convert_vit_timm_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.vit.convert_vit_timm_to_pytorch.in_proj_weight->timm_model.state_dict().pop(f'blocks.{i}.attn.qkv.weight')
A:transformers.models.vit.convert_vit_timm_to_pytorch.in_proj_bias->timm_model.state_dict().pop(f'blocks.{i}.attn.qkv.bias')
A:transformers.models.vit.convert_vit_timm_to_pytorch.val->dct.pop(old)
A:transformers.models.vit.convert_vit_timm_to_pytorch.im->PIL.Image.open(requests.get(url, stream=True).raw)
A:transformers.models.vit.convert_vit_timm_to_pytorch.config->ViTConfig()
A:transformers.models.vit.convert_vit_timm_to_pytorch.config.patch_size->int(vit_name[-6:-4])
A:transformers.models.vit.convert_vit_timm_to_pytorch.config.image_size->int(vit_name[-3:])
A:transformers.models.vit.convert_vit_timm_to_pytorch.timm_model->timm.create_model(vit_name, pretrained=True)
A:transformers.models.vit.convert_vit_timm_to_pytorch.state_dict->timm.create_model(vit_name, pretrained=True).state_dict()
A:transformers.models.vit.convert_vit_timm_to_pytorch.rename_keys->create_rename_keys(config, base_model)
A:transformers.models.vit.convert_vit_timm_to_pytorch.model->ViTForImageClassification(config).eval()
A:transformers.models.vit.convert_vit_timm_to_pytorch.feature_extractor->ViTFeatureExtractor(size=config.image_size)
A:transformers.models.vit.convert_vit_timm_to_pytorch.encoding->feature_extractor(images=prepare_img(), return_tensors='pt')
A:transformers.models.vit.convert_vit_timm_to_pytorch.outputs->model(pixel_values)
A:transformers.models.vit.convert_vit_timm_to_pytorch.timm_pooled_output->timm.create_model(vit_name, pretrained=True).forward_features(pixel_values)
A:transformers.models.vit.convert_vit_timm_to_pytorch.timm_logits->timm_model(pixel_values)
A:transformers.models.vit.convert_vit_timm_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.vit.convert_vit_timm_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.vit.convert_vit_timm_to_pytorch.convert_vit_checkpoint(vit_name,pytorch_dump_folder_path)
transformers.models.vit.convert_vit_timm_to_pytorch.create_rename_keys(config,base_model=False)
transformers.models.vit.convert_vit_timm_to_pytorch.prepare_img()
transformers.models.vit.convert_vit_timm_to_pytorch.read_in_q_k_v(state_dict,config,base_model=False)
transformers.models.vit.convert_vit_timm_to_pytorch.remove_classification_head_(state_dict)
transformers.models.vit.convert_vit_timm_to_pytorch.rename_key(dct,old,new)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/vit/modeling_vit.py----------------------------------------
A:transformers.models.vit.modeling_vit.logger->utils.logging.get_logger(__name__)
A:transformers.models.vit.modeling_vit.self.cls_token->torch.nn.Parameter(torch.zeros(1, 1, config.hidden_size))
A:transformers.models.vit.modeling_vit.self.patch_embeddings->PatchEmbeddings(image_size=config.image_size, patch_size=config.patch_size, num_channels=config.num_channels, embed_dim=config.hidden_size)
A:transformers.models.vit.modeling_vit.self.position_embeddings->torch.nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))
A:transformers.models.vit.modeling_vit.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.vit.modeling_vit.embeddings->self.dropout(embeddings)
A:transformers.models.vit.modeling_vit.cls_tokens->self.cls_token.expand(batch_size, -1, -1)
A:transformers.models.vit.modeling_vit.image_size->to_2tuple(image_size)
A:transformers.models.vit.modeling_vit.patch_size->to_2tuple(patch_size)
A:transformers.models.vit.modeling_vit.self.projection->torch.nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)
A:transformers.models.vit.modeling_vit.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.vit.modeling_vit.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.vit.modeling_vit.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.vit.modeling_vit.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.vit.modeling_vit.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.vit.modeling_vit.mixed_query_layer->self.query(hidden_states)
A:transformers.models.vit.modeling_vit.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.vit.modeling_vit.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.vit.modeling_vit.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.vit.modeling_vit.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.vit.modeling_vit.attention_probs->self.dropout(attention_probs)
A:transformers.models.vit.modeling_vit.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.vit.modeling_vit.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.vit.modeling_vit.hidden_states->self.dropout(hidden_states)
A:transformers.models.vit.modeling_vit.self.attention->ViTAttention(config)
A:transformers.models.vit.modeling_vit.self.output->ViTOutput(config)
A:transformers.models.vit.modeling_vit.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.vit.modeling_vit.(heads, index)->find_pruneable_heads_and_indices(heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads)
A:transformers.models.vit.modeling_vit.self.attention.query->prune_linear_layer(self.attention.query, index)
A:transformers.models.vit.modeling_vit.self.attention.key->prune_linear_layer(self.attention.key, index)
A:transformers.models.vit.modeling_vit.self.attention.value->prune_linear_layer(self.attention.value, index)
A:transformers.models.vit.modeling_vit.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.vit.modeling_vit.self_outputs->self.attention(hidden_states, head_mask, output_attentions)
A:transformers.models.vit.modeling_vit.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.vit.modeling_vit.self.intermediate->ViTIntermediate(config)
A:transformers.models.vit.modeling_vit.self.layernorm_before->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.vit.modeling_vit.self.layernorm_after->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.vit.modeling_vit.self_attention_outputs->self.attention(self.layernorm_before(hidden_states), head_mask, output_attentions=output_attentions)
A:transformers.models.vit.modeling_vit.layer_output->self.output(intermediate_output)
A:transformers.models.vit.modeling_vit.intermediate_output->self.intermediate(attention_output)
A:transformers.models.vit.modeling_vit.self.layer->torch.nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.vit.modeling_vit.layer_outputs->layer_module(hidden_states, layer_head_mask, output_attentions)
A:transformers.models.vit.modeling_vit.self.embeddings->ViTEmbeddings(config)
A:transformers.models.vit.modeling_vit.self.encoder->ViTEncoder(config)
A:transformers.models.vit.modeling_vit.self.layernorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.vit.modeling_vit.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.vit.modeling_vit.embedding_output->self.embeddings(pixel_values)
A:transformers.models.vit.modeling_vit.encoder_outputs->self.encoder(embedding_output, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.vit.modeling_vit.sequence_output->self.layernorm(sequence_output)
A:transformers.models.vit.modeling_vit.self.activation->torch.nn.Tanh()
A:transformers.models.vit.modeling_vit.pooled_output->self.activation(pooled_output)
A:transformers.models.vit.modeling_vit.self.vit->ViTModel(config, add_pooling_layer=False)
A:transformers.models.vit.modeling_vit.outputs->self.vit(pixel_values, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.vit.modeling_vit.logits->self.classifier(sequence_output[:, 0, :])
A:transformers.models.vit.modeling_vit.loss_fct->CrossEntropyLoss()
A:transformers.models.vit.modeling_vit.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
transformers.ViTForImageClassification(self,config)
transformers.ViTForImageClassification.forward(self,pixel_values=None,head_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ViTModel(self,config,add_pooling_layer=True)
transformers.ViTModel._prune_heads(self,heads_to_prune)
transformers.ViTModel.forward(self,pixel_values=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ViTModel.get_input_embeddings(self)
transformers.ViTPreTrainedModel(PreTrainedModel)
transformers.ViTPreTrainedModel._init_weights(self,module)
transformers.models.vit.modeling_vit.PatchEmbeddings(self,image_size=224,patch_size=16,num_channels=3,embed_dim=768)
transformers.models.vit.modeling_vit.PatchEmbeddings.__init__(self,image_size=224,patch_size=16,num_channels=3,embed_dim=768)
transformers.models.vit.modeling_vit.PatchEmbeddings.forward(self,pixel_values)
transformers.models.vit.modeling_vit.ViTAttention(self,config)
transformers.models.vit.modeling_vit.ViTAttention.__init__(self,config)
transformers.models.vit.modeling_vit.ViTAttention.forward(self,hidden_states,head_mask=None,output_attentions=False)
transformers.models.vit.modeling_vit.ViTAttention.prune_heads(self,heads)
transformers.models.vit.modeling_vit.ViTEmbeddings(self,config)
transformers.models.vit.modeling_vit.ViTEmbeddings.__init__(self,config)
transformers.models.vit.modeling_vit.ViTEmbeddings.forward(self,pixel_values)
transformers.models.vit.modeling_vit.ViTEncoder(self,config)
transformers.models.vit.modeling_vit.ViTEncoder.__init__(self,config)
transformers.models.vit.modeling_vit.ViTEncoder.forward(self,hidden_states,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.vit.modeling_vit.ViTForImageClassification(self,config)
transformers.models.vit.modeling_vit.ViTForImageClassification.__init__(self,config)
transformers.models.vit.modeling_vit.ViTForImageClassification.forward(self,pixel_values=None,head_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.vit.modeling_vit.ViTIntermediate(self,config)
transformers.models.vit.modeling_vit.ViTIntermediate.__init__(self,config)
transformers.models.vit.modeling_vit.ViTIntermediate.forward(self,hidden_states)
transformers.models.vit.modeling_vit.ViTLayer(self,config)
transformers.models.vit.modeling_vit.ViTLayer.__init__(self,config)
transformers.models.vit.modeling_vit.ViTLayer.feed_forward_chunk(self,attention_output)
transformers.models.vit.modeling_vit.ViTLayer.forward(self,hidden_states,head_mask=None,output_attentions=False)
transformers.models.vit.modeling_vit.ViTModel(self,config,add_pooling_layer=True)
transformers.models.vit.modeling_vit.ViTModel.__init__(self,config,add_pooling_layer=True)
transformers.models.vit.modeling_vit.ViTModel._prune_heads(self,heads_to_prune)
transformers.models.vit.modeling_vit.ViTModel.forward(self,pixel_values=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.vit.modeling_vit.ViTModel.get_input_embeddings(self)
transformers.models.vit.modeling_vit.ViTOutput(self,config)
transformers.models.vit.modeling_vit.ViTOutput.__init__(self,config)
transformers.models.vit.modeling_vit.ViTOutput.forward(self,hidden_states,input_tensor)
transformers.models.vit.modeling_vit.ViTPooler(self,config)
transformers.models.vit.modeling_vit.ViTPooler.__init__(self,config)
transformers.models.vit.modeling_vit.ViTPooler.forward(self,hidden_states)
transformers.models.vit.modeling_vit.ViTPreTrainedModel(PreTrainedModel)
transformers.models.vit.modeling_vit.ViTPreTrainedModel._init_weights(self,module)
transformers.models.vit.modeling_vit.ViTSelfAttention(self,config)
transformers.models.vit.modeling_vit.ViTSelfAttention.__init__(self,config)
transformers.models.vit.modeling_vit.ViTSelfAttention.forward(self,hidden_states,head_mask=None,output_attentions=False)
transformers.models.vit.modeling_vit.ViTSelfAttention.transpose_for_scores(self,x)
transformers.models.vit.modeling_vit.ViTSelfOutput(self,config)
transformers.models.vit.modeling_vit.ViTSelfOutput.__init__(self,config)
transformers.models.vit.modeling_vit.ViTSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.vit.modeling_vit.to_2tuple(x)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/vit/configuration_vit.py----------------------------------------
A:transformers.models.vit.configuration_vit.logger->utils.logging.get_logger(__name__)
transformers.ViTConfig(self,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.0,initializer_range=0.02,layer_norm_eps=1e-12,is_encoder_decoder=False,image_size=224,patch_size=16,num_channels=3,**kwargs)
transformers.models.vit.configuration_vit.ViTConfig(self,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.0,initializer_range=0.02,layer_norm_eps=1e-12,is_encoder_decoder=False,image_size=224,patch_size=16,num_channels=3,**kwargs)
transformers.models.vit.configuration_vit.ViTConfig.__init__(self,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.0,initializer_range=0.02,layer_norm_eps=1e-12,is_encoder_decoder=False,image_size=224,patch_size=16,num_channels=3,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/vit/__init__.py----------------------------------------
A:transformers.models.vit.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/vit/modeling_flax_vit.py----------------------------------------
A:transformers.models.vit.modeling_flax_vit.self.projection->flax.linen.Conv(self.config.hidden_size, kernel_size=(patch_size, patch_size), strides=(patch_size, patch_size), padding='VALID', dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.vit.modeling_flax_vit.x->self.projection(pixel_values)
A:transformers.models.vit.modeling_flax_vit.self.cls_token->self.param('cls_token', nn.initializers.zeros, (1, 1, self.config.hidden_size))
A:transformers.models.vit.modeling_flax_vit.self.patch_embeddings->FlaxPatchEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.self.position_embeddings->self.param('position_embeddings', nn.initializers.zeros, (1, num_patches + 1, self.config.hidden_size))
A:transformers.models.vit.modeling_flax_vit.self.dropout->flax.linen.Dropout(rate=self.config.hidden_dropout_prob)
A:transformers.models.vit.modeling_flax_vit.embeddings->self.dropout(embeddings, deterministic=deterministic)
A:transformers.models.vit.modeling_flax_vit.cls_tokens->jax.numpy.broadcast_to(self.cls_token, (batch_size, 1, self.config.hidden_size))
A:transformers.models.vit.modeling_flax_vit.self.query->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.vit.modeling_flax_vit.self.key->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.vit.modeling_flax_vit.self.value->flax.linen.Dense(self.config.hidden_size, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.vit.modeling_flax_vit.query_states->self.query(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.vit.modeling_flax_vit.value_states->self.value(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.vit.modeling_flax_vit.key_states->self.key(hidden_states).reshape(hidden_states.shape[:2] + (self.config.num_attention_heads, head_dim))
A:transformers.models.vit.modeling_flax_vit.dropout_rng->self.make_rng('dropout')
A:transformers.models.vit.modeling_flax_vit.attn_weights->dot_product_attention_weights(query_states, key_states, dropout_rng=dropout_rng, dropout_rate=self.config.attention_probs_dropout_prob, broadcast_dropout=True, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.vit.modeling_flax_vit.attn_output->attn_output.reshape(attn_output.shape[:2] + (-1,)).reshape(attn_output.shape[:2] + (-1,))
A:transformers.models.vit.modeling_flax_vit.self.dense->flax.linen.Dense(self.config.hidden_size, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype), dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.hidden_states->self.layernorm(hidden_states)
A:transformers.models.vit.modeling_flax_vit.self.attention->FlaxViTAttention(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.self.output->FlaxViTOutput(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.attn_outputs->self.attention(hidden_states, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.vit.modeling_flax_vit.self.intermediate->FlaxViTIntermediate(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.self.layernorm_before->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.self.layernorm_after->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.attention_outputs->self.attention(self.layernorm_before(hidden_states), deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.vit.modeling_flax_vit.layer_output->self.layernorm_after(attention_output)
A:transformers.models.vit.modeling_flax_vit.layer_outputs->layer(hidden_states, deterministic=deterministic, output_attentions=output_attentions)
A:transformers.models.vit.modeling_flax_vit.self.layer->FlaxViTLayerCollection(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.cls_hidden_state->self.dense(cls_hidden_state)
A:transformers.models.vit.modeling_flax_vit.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.vit.modeling_flax_vit.pixel_values->jax.numpy.transpose(pixel_values, (0, 2, 3, 1))
A:transformers.models.vit.modeling_flax_vit.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.vit.modeling_flax_vit.self.embeddings->FlaxViTEmbeddings(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.self.encoder->FlaxViTEncoder(self.config, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.self.layernorm->flax.linen.LayerNorm(epsilon=self.config.layer_norm_eps, dtype=self.dtype)
A:transformers.models.vit.modeling_flax_vit.outputs->self.vit(pixel_values, deterministic=deterministic, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.vit.modeling_flax_vit.self.vit->FlaxViTModule(config=self.config, dtype=self.dtype, add_pooling_layer=False)
A:transformers.models.vit.modeling_flax_vit.self.classifier->flax.linen.Dense(self.config.num_labels, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.initializer_range, self.dtype))
A:transformers.models.vit.modeling_flax_vit.logits->self.classifier(hidden_states[:, 0, :])
transformers.FlaxViTForImageClassification(FlaxViTPreTrainedModel)
transformers.FlaxViTForImageClassificationModule(self,pixel_values=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FlaxViTForImageClassificationModule.setup(self)
transformers.FlaxViTModel(FlaxViTPreTrainedModel)
transformers.FlaxViTPreTrainedModel(self,config:ViTConfig,input_shape=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxViTPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.vit.modeling_flax_vit.FlaxPatchEmbeddings(self,pixel_values)
transformers.models.vit.modeling_flax_vit.FlaxPatchEmbeddings.__call__(self,pixel_values)
transformers.models.vit.modeling_flax_vit.FlaxPatchEmbeddings.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTAttention(self,hidden_states,deterministic=True,output_attentions:bool=False)
transformers.models.vit.modeling_flax_vit.FlaxViTAttention.__call__(self,hidden_states,deterministic=True,output_attentions:bool=False)
transformers.models.vit.modeling_flax_vit.FlaxViTAttention.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTEmbeddings(self,pixel_values,deterministic=True)
transformers.models.vit.modeling_flax_vit.FlaxViTEmbeddings.__call__(self,pixel_values,deterministic=True)
transformers.models.vit.modeling_flax_vit.FlaxViTEmbeddings.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTEncoder(self,hidden_states,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTEncoder.__call__(self,hidden_states,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTEncoder.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTForImageClassification(FlaxViTPreTrainedModel)
transformers.models.vit.modeling_flax_vit.FlaxViTForImageClassificationModule(self,pixel_values=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.vit.modeling_flax_vit.FlaxViTForImageClassificationModule.__call__(self,pixel_values=None,deterministic:bool=True,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.vit.modeling_flax_vit.FlaxViTForImageClassificationModule.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTIntermediate(self,hidden_states)
transformers.models.vit.modeling_flax_vit.FlaxViTIntermediate.__call__(self,hidden_states)
transformers.models.vit.modeling_flax_vit.FlaxViTIntermediate.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTLayer(self,hidden_states,deterministic:bool=True,output_attentions:bool=False)
transformers.models.vit.modeling_flax_vit.FlaxViTLayer.__call__(self,hidden_states,deterministic:bool=True,output_attentions:bool=False)
transformers.models.vit.modeling_flax_vit.FlaxViTLayer.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTLayerCollection(self,hidden_states,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTLayerCollection.__call__(self,hidden_states,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTLayerCollection.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTModel(FlaxViTPreTrainedModel)
transformers.models.vit.modeling_flax_vit.FlaxViTModule(self,pixel_values,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTModule.__call__(self,pixel_values,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTModule.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTOutput(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTOutput.__call__(self,hidden_states,attention_output,deterministic:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTOutput.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTPooler(self,hidden_states)
transformers.models.vit.modeling_flax_vit.FlaxViTPooler.__call__(self,hidden_states)
transformers.models.vit.modeling_flax_vit.FlaxViTPooler.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTPreTrainedModel(self,config:ViTConfig,input_shape=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.vit.modeling_flax_vit.FlaxViTPreTrainedModel.__init__(self,config:ViTConfig,input_shape=None,seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.vit.modeling_flax_vit.FlaxViTPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.vit.modeling_flax_vit.FlaxViTSelfAttention(self,hidden_states,deterministic:bool=True,output_attentions:bool=False)
transformers.models.vit.modeling_flax_vit.FlaxViTSelfAttention.__call__(self,hidden_states,deterministic:bool=True,output_attentions:bool=False)
transformers.models.vit.modeling_flax_vit.FlaxViTSelfAttention.setup(self)
transformers.models.vit.modeling_flax_vit.FlaxViTSelfOutput(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTSelfOutput.__call__(self,hidden_states,input_tensor,deterministic:bool=True)
transformers.models.vit.modeling_flax_vit.FlaxViTSelfOutput.setup(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/vit/feature_extraction_vit.py----------------------------------------
A:transformers.models.vit.feature_extraction_vit.logger->utils.logging.get_logger(__name__)
A:transformers.models.vit.feature_extraction_vit.is_batched->bool(isinstance(images, (list, tuple)) and (isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0])))
A:transformers.models.vit.feature_extraction_vit.encoded_inputs->BatchFeature(data=data, tensor_type=return_tensors)
transformers.ViTFeatureExtractor(self,do_resize=True,size=224,resample=Image.BILINEAR,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.vit.feature_extraction_vit.ViTFeatureExtractor(self,do_resize=True,size=224,resample=Image.BILINEAR,do_normalize=True,image_mean=None,image_std=None,**kwargs)
transformers.models.vit.feature_extraction_vit.ViTFeatureExtractor.__init__(self,do_resize=True,size=224,resample=Image.BILINEAR,do_normalize=True,image_mean=None,image_std=None,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/megatron_bert/configuration_megatron_bert.py----------------------------------------
A:transformers.models.megatron_bert.configuration_megatron_bert.logger->utils.logging.get_logger(__name__)
transformers.MegatronBertConfig(self,vocab_size=29056,hidden_size=1024,num_hidden_layers=24,num_attention_heads=16,intermediate_size=4096,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)
transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig(self,vocab_size=29056,hidden_size=1024,num_hidden_layers=24,num_attention_heads=16,intermediate_size=4096,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)
transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig.__init__(self,vocab_size=29056,hidden_size=1024,num_hidden_layers=24,num_attention_heads=16,intermediate_size=4096,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,position_embedding_type='absolute',use_cache=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/megatron_bert/modeling_megatron_bert.py----------------------------------------
A:transformers.models.megatron_bert.modeling_megatron_bert.logger->utils.logging.get_logger(__name__)
A:transformers.models.megatron_bert.modeling_megatron_bert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.models.megatron_bert.modeling_megatron_bert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.megatron_bert.modeling_megatron_bert.array->numpy.transpose(array)
A:transformers.models.megatron_bert.modeling_megatron_bert.name->name.split('/').split('/')
A:transformers.models.megatron_bert.modeling_megatron_bert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.models.megatron_bert.modeling_megatron_bert.pointer->getattr(pointer, 'weight')
A:transformers.models.megatron_bert.modeling_megatron_bert.num->int(scope_names[1])
A:transformers.models.megatron_bert.modeling_megatron_bert.pointer.data->torch.from_numpy(array)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.megatron_bert.modeling_megatron_bert.input_shape->torch.cat([input_ids, dummy_token], dim=1).size()
A:transformers.models.megatron_bert.modeling_megatron_bert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.megatron_bert.modeling_megatron_bert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.megatron_bert.modeling_megatron_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.megatron_bert.modeling_megatron_bert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.megatron_bert.modeling_megatron_bert.embeddings->self.dropout(embeddings)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.distance_embedding->torch.nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.megatron_bert.modeling_megatron_bert.mixed_query_layer->self.query(hidden_states)
A:transformers.models.megatron_bert.modeling_megatron_bert.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.megatron_bert.modeling_megatron_bert.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.megatron_bert.modeling_megatron_bert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.megatron_bert.modeling_megatron_bert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.megatron_bert.modeling_megatron_bert.position_ids_l->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
A:transformers.models.megatron_bert.modeling_megatron_bert.position_ids_r->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
A:transformers.models.megatron_bert.modeling_megatron_bert.positional_embedding->positional_embedding.to(dtype=query_layer.dtype).to(dtype=query_layer.dtype)
A:transformers.models.megatron_bert.modeling_megatron_bert.relative_position_scores->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.megatron_bert.modeling_megatron_bert.relative_position_scores_query->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.megatron_bert.modeling_megatron_bert.relative_position_scores_key->torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
A:transformers.models.megatron_bert.modeling_megatron_bert.attention_probs->self.dropout(attention_probs)
A:transformers.models.megatron_bert.modeling_megatron_bert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.megatron_bert.modeling_megatron_bert.hidden_states->self.decoder(hidden_states)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.ln->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.self->MegatronBertSelfAttention(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.output->MegatronBertOutput(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.megatron_bert.modeling_megatron_bert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.megatron_bert.modeling_megatron_bert.ln_outputs->self.ln(hidden_states)
A:transformers.models.megatron_bert.modeling_megatron_bert.self_outputs->self.self(ln_outputs, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.megatron_bert.modeling_megatron_bert.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.attention->MegatronBertAttention(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.crossattention->MegatronBertAttention(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.intermediate->MegatronBertIntermediate(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.megatron_bert.modeling_megatron_bert.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.megatron_bert.modeling_megatron_bert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.ln_output->self.ln(attention_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.intermediate_output->self.intermediate(ln_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.layer->torch.nn.ModuleList([MegatronBertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.megatron_bert.modeling_megatron_bert.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.activation->torch.nn.Tanh()
A:transformers.models.megatron_bert.modeling_megatron_bert.pooled_output->self.dropout(pooled_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.transform->MegatronBertPredictionHeadTransform(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.megatron_bert.modeling_megatron_bert.self.predictions->MegatronBertLMPredictionHead(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.prediction_scores->self.cls(sequence_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.models.megatron_bert.modeling_megatron_bert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.embeddings->MegatronBertEmbeddings(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.encoder->MegatronBertEncoder(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.attention_mask->torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)
A:transformers.models.megatron_bert.modeling_megatron_bert.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.models.megatron_bert.modeling_megatron_bert.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.models.megatron_bert.modeling_megatron_bert.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.models.megatron_bert.modeling_megatron_bert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.models.megatron_bert.modeling_megatron_bert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, past_key_values_length=past_key_values_length)
A:transformers.models.megatron_bert.modeling_megatron_bert.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.bert->MegatronBertModel(config, add_pooling_layer=False)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.cls->MegatronBertOnlyNSPHead(config)
A:transformers.models.megatron_bert.modeling_megatron_bert.outputs->self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.megatron_bert.modeling_megatron_bert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.models.megatron_bert.modeling_megatron_bert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.megatron_bert.modeling_megatron_bert.next_sentence_loss->loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))
A:transformers.models.megatron_bert.modeling_megatron_bert.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.models.megatron_bert.modeling_megatron_bert.labels->kwargs.pop('next_sentence_label')
A:transformers.models.megatron_bert.modeling_megatron_bert.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.megatron_bert.modeling_megatron_bert.dummy_token->torch.full((effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.models.megatron_bert.modeling_megatron_bert.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.models.megatron_bert.modeling_megatron_bert.seq_relationship_scores->self.cls(pooled_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.megatron_bert.modeling_megatron_bert.logits->self.qa_outputs(sequence_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.megatron_bert.modeling_megatron_bert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.megatron_bert.modeling_megatron_bert.sequence_output->self.dropout(sequence_output)
A:transformers.models.megatron_bert.modeling_megatron_bert.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.megatron_bert.modeling_megatron_bert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.megatron_bert.modeling_megatron_bert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.megatron_bert.modeling_megatron_bert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.megatron_bert.modeling_megatron_bert.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.megatron_bert.modeling_megatron_bert.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.megatron_bert.modeling_megatron_bert.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.megatron_bert.modeling_megatron_bert.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.megatron_bert.modeling_megatron_bert.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.megatron_bert.modeling_megatron_bert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.megatron_bert.modeling_megatron_bert.end_loss->loss_fct(end_logits, end_positions)
transformers.MegatronBertForCausalLM(self,config)
transformers.MegatronBertForCausalLM._reorder_cache(self,past,beam_idx)
transformers.MegatronBertForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertForCausalLM.get_output_embeddings(self)
transformers.MegatronBertForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.MegatronBertForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.MegatronBertForMaskedLM(self,config)
transformers.MegatronBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertForMaskedLM.get_output_embeddings(self)
transformers.MegatronBertForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.MegatronBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.MegatronBertForMultipleChoice(self,config)
transformers.MegatronBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertForNextSentencePrediction(self,config)
transformers.MegatronBertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.MegatronBertForPreTraining(self,config,add_binary_head=True)
transformers.MegatronBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertForPreTraining.get_output_embeddings(self)
transformers.MegatronBertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.MegatronBertForPreTrainingOutput(ModelOutput)
transformers.MegatronBertForQuestionAnswering(self,config)
transformers.MegatronBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertForSequenceClassification(self,config)
transformers.MegatronBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertForTokenClassification(self,config)
transformers.MegatronBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertModel(self,config,add_pooling_layer=True)
transformers.MegatronBertModel._prune_heads(self,heads_to_prune)
transformers.MegatronBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MegatronBertModel.get_input_embeddings(self)
transformers.MegatronBertModel.set_input_embeddings(self,value)
transformers.MegatronBertPreTrainedModel(PreTrainedModel)
transformers.MegatronBertPreTrainedModel._init_weights(self,module)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertAttention(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertAttention.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertAttention.prune_heads(self,heads)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertEmbeddings(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertEmbeddings.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None,past_key_values_length=0)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertEncoder(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertEncoder.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM._reorder_cache(self,past,beam_idx)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM.get_output_embeddings(self)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,**model_kwargs)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMaskedLM(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMaskedLM.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMaskedLM.get_output_embeddings(self)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMultipleChoice(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMultipleChoice.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForNextSentencePrediction(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForNextSentencePrediction.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForPreTraining(self,config,add_binary_head=True)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForPreTraining.__init__(self,config,add_binary_head=True)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForPreTraining.get_output_embeddings(self)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForPreTraining.set_output_embeddings(self,new_embeddings)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForPreTrainingOutput(ModelOutput)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForQuestionAnswering(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForQuestionAnswering.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForSequenceClassification(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForSequenceClassification.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForTokenClassification(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForTokenClassification.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertIntermediate(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertIntermediate.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertIntermediate.forward(self,hidden_states)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLMPredictionHead(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLMPredictionHead.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLMPredictionHead.forward(self,hidden_states)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLayer(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLayer.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLayer.feed_forward_chunk(self,attention_output)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertModel(self,config,add_pooling_layer=True)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertModel.__init__(self,config,add_pooling_layer=True)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertModel._prune_heads(self,heads_to_prune)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertModel.get_input_embeddings(self)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertModel.set_input_embeddings(self,value)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOnlyMLMHead(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOnlyMLMHead.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOnlyMLMHead.forward(self,sequence_output)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOnlyNSPHead(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOnlyNSPHead.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOnlyNSPHead.forward(self,pooled_output)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOutput(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOutput.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertOutput.forward(self,hidden_states,input_tensor)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPooler(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPooler.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPooler.forward(self,hidden_states)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPreTrainedModel(PreTrainedModel)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPreTrainedModel._init_weights(self,module)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPreTrainingHeads(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPreTrainingHeads.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPredictionHeadTransform(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPredictionHeadTransform.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertPredictionHeadTransform.forward(self,hidden_states)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfAttention(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfAttention.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfAttention.transpose_for_scores(self,x)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfOutput(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfOutput.__init__(self,config)
transformers.models.megatron_bert.modeling_megatron_bert.MegatronBertSelfOutput.forward(self,hidden_states,residual)
transformers.models.megatron_bert.modeling_megatron_bert.load_tf_weights_in_megatron_bert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/megatron_bert/convert_megatron_bert_checkpoint.py----------------------------------------
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.msg->fmt.format(name)
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.layer_re->re.compile('layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)')
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.m->re.compile('layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)').match(key)
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.layer_idx->int(m.group(1))
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.op_name->re.compile('layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)').match(key).group(2)
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.weight_or_bias->re.compile('layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)').match(key).group(3)
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.parser->argparse.ArgumentParser()
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.args->argparse.ArgumentParser().parse_args()
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.basename->os.path.dirname(args.path_to_checkpoint)
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.input_state_dict->torch.load(pytorch_dict, map_location='cpu')
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.(output_state_dict, output_config)->convert_megatron_checkpoint(args, input_state_dict)
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.output_config_file->os.path.join(basename, 'config.json')
A:transformers.models.megatron_bert.convert_megatron_bert_checkpoint.output_checkpoint_file->os.path.join(basename, 'pytorch_model.bin')
transformers.models.megatron_bert.convert_megatron_bert_checkpoint.convert_megatron_checkpoint(args,input_state_dict)
transformers.models.megatron_bert.convert_megatron_bert_checkpoint.main()
transformers.models.megatron_bert.convert_megatron_bert_checkpoint.recursive_print(name,val,spaces=0)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/megatron_bert/__init__.py----------------------------------------
A:transformers.models.megatron_bert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/modeling_tf_xlnet.py----------------------------------------
A:transformers.models.xlnet.modeling_tf_xlnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlnet.modeling_tf_xlnet.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.xlnet.modeling_tf_xlnet.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.xlnet.modeling_tf_xlnet.initializer->get_initializer(self.initializer_range)
A:transformers.models.xlnet.modeling_tf_xlnet.self.q->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='q')
A:transformers.models.xlnet.modeling_tf_xlnet.self.k->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='k')
A:transformers.models.xlnet.modeling_tf_xlnet.self.v->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='v')
A:transformers.models.xlnet.modeling_tf_xlnet.self.o->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='o')
A:transformers.models.xlnet.modeling_tf_xlnet.self.r->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='r')
A:transformers.models.xlnet.modeling_tf_xlnet.self.r_r_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_r_bias')
A:transformers.models.xlnet.modeling_tf_xlnet.self.r_s_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_s_bias')
A:transformers.models.xlnet.modeling_tf_xlnet.self.r_w_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_w_bias')
A:transformers.models.xlnet.modeling_tf_xlnet.self.seg_embed->self.add_weight(shape=(2, self.n_head, self.d_head), initializer=initializer, trainable=True, name='seg_embed')
A:transformers.models.xlnet.modeling_tf_xlnet.x_size->shape_list(x)
A:transformers.models.xlnet.modeling_tf_xlnet.x->tensorflow.reshape(x, (x_size[0], x_size[1] - 1, x_size[2], x_size[3]))
A:transformers.models.xlnet.modeling_tf_xlnet.ac->tensorflow.einsum('ibnd,jbnd->ijbn', q_head + self.r_w_bias, k_head_h)
A:transformers.models.xlnet.modeling_tf_xlnet.bd->self.rel_shift(bd, klen=shape_list(ac)[1])
A:transformers.models.xlnet.modeling_tf_xlnet.ef->tensorflow.einsum('ijbs,ibns->ijbn', seg_mat, ef)
A:transformers.models.xlnet.modeling_tf_xlnet.attn_prob->self.dropout(attn_prob, training=training)
A:transformers.models.xlnet.modeling_tf_xlnet.attn_vec->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_h, head_mask, output_attentions, training=training)
A:transformers.models.xlnet.modeling_tf_xlnet.attn_out->self.dropout(attn_out, training=training)
A:transformers.models.xlnet.modeling_tf_xlnet.output->self.call(inputs)
A:transformers.models.xlnet.modeling_tf_xlnet.cat->tensorflow.concat([mems, h], axis=0)
A:transformers.models.xlnet.modeling_tf_xlnet.k_head_h->tensorflow.einsum('ibh,hnd->ibnd', cat, self.k)
A:transformers.models.xlnet.modeling_tf_xlnet.v_head_h->tensorflow.einsum('ibh,hnd->ibnd', cat, self.v)
A:transformers.models.xlnet.modeling_tf_xlnet.k_head_r->tensorflow.einsum('ibh,hnd->ibnd', r, self.r)
A:transformers.models.xlnet.modeling_tf_xlnet.q_head_h->tensorflow.einsum('ibh,hnd->ibnd', h, self.q)
A:transformers.models.xlnet.modeling_tf_xlnet.attn_vec_h->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_h, head_mask, output_attentions, training=training)
A:transformers.models.xlnet.modeling_tf_xlnet.output_h->self.dropout(word_emb_k, training=inputs['training'])
A:transformers.models.xlnet.modeling_tf_xlnet.q_head_g->tensorflow.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)
A:transformers.models.xlnet.modeling_tf_xlnet.attn_vec_g->self.rel_attn_core(q_head_g, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_g, head_mask, output_attentions, training=training)
A:transformers.models.xlnet.modeling_tf_xlnet.output_g->self.dropout(word_emb_q, training=inputs['training'])
A:transformers.models.xlnet.modeling_tf_xlnet.self.layer_1->tensorflow.keras.layers.Dense(config.d_inner, kernel_initializer=get_initializer(config.initializer_range), name='layer_1')
A:transformers.models.xlnet.modeling_tf_xlnet.self.layer_2->tensorflow.keras.layers.Dense(config.d_model, kernel_initializer=get_initializer(config.initializer_range), name='layer_2')
A:transformers.models.xlnet.modeling_tf_xlnet.self.activation_function->get_tf_activation(config.ff_activation)
A:transformers.models.xlnet.modeling_tf_xlnet.self.rel_attn->TFXLNetRelativeAttention(config, name='rel_attn')
A:transformers.models.xlnet.modeling_tf_xlnet.self.ff->TFXLNetFeedForward(config, name='ff')
A:transformers.models.xlnet.modeling_tf_xlnet.outputs->self.transformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], mems=inputs['mems'], perm_mask=inputs['perm_mask'], target_mapping=inputs['target_mapping'], token_type_ids=inputs['token_type_ids'], input_mask=inputs['input_mask'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], use_mems=inputs['use_mems'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.xlnet.modeling_tf_xlnet.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.xlnet.modeling_tf_xlnet.hidden_states->tuple((tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states))
A:transformers.models.xlnet.modeling_tf_xlnet.self.word_embedding->TFSharedEmbeddings(config.vocab_size, config.d_model, initializer_range=config.initializer_range, name='word_embedding')
A:transformers.models.xlnet.modeling_tf_xlnet.self.mask_emb->self.add_weight(shape=(1, 1, self.d_model), initializer=initializer, trainable=True, name='mask_emb')
A:transformers.models.xlnet.modeling_tf_xlnet.attn_mask->tensorflow.cast(attn_mask > 0, dtype=attn_mask.dtype)
A:transformers.models.xlnet.modeling_tf_xlnet.mask_u->tensorflow.matrix_band_part(attn_mask, 0, -1)
A:transformers.models.xlnet.modeling_tf_xlnet.mask_dia->tensorflow.matrix_band_part(attn_mask, 0, 0)
A:transformers.models.xlnet.modeling_tf_xlnet.attn_mask_pad->tensorflow.zeros([qlen, mlen])
A:transformers.models.xlnet.modeling_tf_xlnet.ret->tensorflow.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)
A:transformers.models.xlnet.modeling_tf_xlnet.mask_l->tensorflow.matrix_band_part(attn_mask, -1, 0)
A:transformers.models.xlnet.modeling_tf_xlnet.sinusoid_inp->tensorflow.einsum('i,d->id', pos_seq, inv_freq)
A:transformers.models.xlnet.modeling_tf_xlnet.pos_emb->self.dropout(pos_emb, training=inputs['training'])
A:transformers.models.xlnet.modeling_tf_xlnet.freq_seq->tensorflow.range(0, self.d_model, 2.0)
A:transformers.models.xlnet.modeling_tf_xlnet.fwd_pos_seq->tensorflow.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)
A:transformers.models.xlnet.modeling_tf_xlnet.bwd_pos_seq->tensorflow.clip_by_value(bwd_pos_seq, -self.clamp_len, self.clamp_len)
A:transformers.models.xlnet.modeling_tf_xlnet.fwd_pos_emb->self.positional_embedding(fwd_pos_seq, inv_freq)
A:transformers.models.xlnet.modeling_tf_xlnet.bwd_pos_emb->self.positional_embedding(bwd_pos_seq, inv_freq)
A:transformers.models.xlnet.modeling_tf_xlnet.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, use_mems=use_mems, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, start_positions=start_positions, end_positions=end_positions, training=training, kwargs_call=kwargs)
A:transformers.models.xlnet.modeling_tf_xlnet.inputs['input_ids']->tensorflow.transpose(inputs['input_ids'], perm=(1, 0))
A:transformers.models.xlnet.modeling_tf_xlnet.inputs['inputs_embeds']->tensorflow.transpose(inputs['inputs_embeds'], perm=(1, 0, 2))
A:transformers.models.xlnet.modeling_tf_xlnet.one_cst->tensorflow.constant(1.0)
A:transformers.models.xlnet.modeling_tf_xlnet.mems_mask->tensorflow.zeros([shape_list(data_mask)[0], mlen, bsz])
A:transformers.models.xlnet.modeling_tf_xlnet.data_mask->tensorflow.concat([mems_mask, data_mask], axis=1)
A:transformers.models.xlnet.modeling_tf_xlnet.non_tgt_mask->tensorflow.cast(attn_mask + non_tgt_mask[:, :, None, None] > 0, dtype=non_tgt_mask.dtype)
A:transformers.models.xlnet.modeling_tf_xlnet.word_emb_k->self.word_embedding(inputs['input_ids'])
A:transformers.models.xlnet.modeling_tf_xlnet.word_emb_q->tensorflow.tile(self.mask_emb, [shape_list(inputs['target_mapping'])[0], bsz, 1])
A:transformers.models.xlnet.modeling_tf_xlnet.mem_pad->tensorflow.zeros([mlen, bsz], dtype=inputs['token_type_ids'].dtype)
A:transformers.models.xlnet.modeling_tf_xlnet.cat_ids->tensorflow.concat([mem_pad, inputs['token_type_ids']], 0)
A:transformers.models.xlnet.modeling_tf_xlnet.seg_mat->tensorflow.one_hot(seg_mat, 2)
A:transformers.models.xlnet.modeling_tf_xlnet.attentions->tuple((tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions))
A:transformers.models.xlnet.modeling_tf_xlnet.self.transformer->TFXLNetMainLayer(config, name='transformer')
A:transformers.models.xlnet.modeling_tf_xlnet.self.lm_loss->TFXLNetLMHead(config, self.transformer.word_embedding, name='lm_loss')
A:transformers.models.xlnet.modeling_tf_xlnet.dummy_token->tensorflow.zeros((effective_batch_size, 1), dtype=inputs.dtype)
A:transformers.models.xlnet.modeling_tf_xlnet.perm_mask->tensorflow.concat([perm_mask, perm_mask_seq_end], axis=-1)
A:transformers.models.xlnet.modeling_tf_xlnet.perm_mask_seq_end->tensorflow.ones((effective_batch_size, sequence_length, 1))
A:transformers.models.xlnet.modeling_tf_xlnet.target_mapping->tensorflow.concat([target_mapping, target_mapping_seq_end], axis=-1)
A:transformers.models.xlnet.modeling_tf_xlnet.target_mapping_seq_end->tensorflow.ones((effective_batch_size, 1, 1))
A:transformers.models.xlnet.modeling_tf_xlnet.inputs['mems']->tuple((layer_past[:-offset, :, :] for layer_past in past))
A:transformers.models.xlnet.modeling_tf_xlnet.transformer_outputs->self.transformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], mems=inputs['mems'], perm_mask=inputs['perm_mask'], target_mapping=inputs['target_mapping'], token_type_ids=inputs['token_type_ids'], input_mask=inputs['input_mask'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], use_mems=inputs['use_mems'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.xlnet.modeling_tf_xlnet.logits->self.qa_outputs(sequence_output)
A:transformers.models.xlnet.modeling_tf_xlnet.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.models.xlnet.modeling_tf_xlnet.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.initializer_range, name='sequence_summary')
A:transformers.models.xlnet.modeling_tf_xlnet.self.logits_proj->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='logits_proj')
A:transformers.models.xlnet.modeling_tf_xlnet.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.xlnet.modeling_tf_xlnet.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.xlnet.modeling_tf_xlnet.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.xlnet.modeling_tf_xlnet.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.xlnet.modeling_tf_xlnet.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.xlnet.modeling_tf_xlnet.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFXLNetForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFXLNetForMultipleChoice.call(self,input_ids=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLNetForMultipleChoice.dummy_inputs(self)
transformers.TFXLNetForMultipleChoice.serving(self,inputs)
transformers.TFXLNetForMultipleChoice.serving_output(self,output)
transformers.TFXLNetForMultipleChoiceOutput(ModelOutput)
transformers.TFXLNetForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFXLNetForQuestionAnsweringSimple.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFXLNetForQuestionAnsweringSimple.serving_output(self,output)
transformers.TFXLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.TFXLNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFXLNetForSequenceClassification.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLNetForSequenceClassification.serving_output(self,output)
transformers.TFXLNetForSequenceClassificationOutput(ModelOutput)
transformers.TFXLNetForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFXLNetForTokenClassification.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLNetForTokenClassification.serving_output(self,output)
transformers.TFXLNetForTokenClassificationOutput(ModelOutput)
transformers.TFXLNetLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFXLNetLMHeadModel.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFXLNetLMHeadModel.get_lm_head(self)
transformers.TFXLNetLMHeadModel.get_prefix_bias_name(self)
transformers.TFXLNetLMHeadModel.prepare_inputs_for_generation(self,inputs,past,use_mems=None,**kwargs)
transformers.TFXLNetLMHeadModel.serving_output(self,output)
transformers.TFXLNetLMHeadModelOutput(ModelOutput)
transformers.TFXLNetMainLayer(self,config,**kwargs)
transformers.TFXLNetMainLayer._prune_heads(self,heads_to_prune)
transformers.TFXLNetMainLayer.build(self,input_shape)
transformers.TFXLNetMainLayer.cache_mem(self,curr_out,prev_mem)
transformers.TFXLNetMainLayer.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFXLNetMainLayer.create_mask(self,qlen,mlen)
transformers.TFXLNetMainLayer.get_input_embeddings(self)
transformers.TFXLNetMainLayer.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.TFXLNetMainLayer.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.TFXLNetMainLayer.set_input_embeddings(self,value)
transformers.TFXLNetModel(self,config,*inputs,**kwargs)
transformers.TFXLNetModel.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFXLNetModel.serving_output(self,output)
transformers.TFXLNetModelOutput(ModelOutput)
transformers.TFXLNetPreTrainedModel(TFPreTrainedModel)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetFeedForward(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetFeedForward.__init__(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetFeedForward.call(self,inp,training=False)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoice.call(self,input_ids=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoice.dummy_inputs(self)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoice.serving(self,inputs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoice.serving_output(self,output)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput(ModelOutput)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.serving_output(self,output)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassification.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassification.serving_output(self,output)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput(ModelOutput)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassification.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassification.serving_output(self,output)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput(ModelOutput)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead(self,config,input_embeddings,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.build(self,input_shape)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.call(self,hidden_states)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.get_bias(self)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.get_output_embeddings(self)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.set_bias(self,value)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHead.set_output_embeddings(self,value)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel.get_lm_head(self)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel.get_prefix_bias_name(self)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel.prepare_inputs_for_generation(self,inputs,past,use_mems=None,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModel.serving_output(self,output)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLMHeadModelOutput(ModelOutput)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLayer(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLayer.__init__(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetLayer.call(self,output_h,output_g,non_tgt_mask,attn_mask,pos_emb,seg_mat,mems,target_mapping,head_mask,output_attentions,training=False)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.__init__(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer._prune_heads(self,heads_to_prune)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.build(self,input_shape)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.cache_mem(self,curr_out,prev_mem)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.create_mask(self,qlen,mlen)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.get_input_embeddings(self)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetMainLayer.set_input_embeddings(self,value)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModel(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModel.__init__(self,config,*inputs,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModel.call(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModel.serving_output(self,output)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetModelOutput(ModelOutput)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetPreTrainedModel(TFPreTrainedModel)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.__init__(self,config,**kwargs)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.build(self,input_shape)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.call(self,h,g,attn_mask_h,attn_mask_g,r,seg_mat,mems,target_mapping,head_mask,output_attentions,training=False)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.post_attention(self,h,attn_vec,residual=True,training=False)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.prune_heads(self,heads)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.rel_attn_core(self,q_head,k_head_h,v_head_h,k_head_r,seg_mat,attn_mask,head_mask,output_attentions,training=False)
transformers.models.xlnet.modeling_tf_xlnet.TFXLNetRelativeAttention.rel_shift(self,x,klen=-1)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/tokenization_xlnet_fast.py----------------------------------------
A:transformers.models.xlnet.tokenization_xlnet_fast.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlnet.tokenization_xlnet_fast.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.XLNetTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.XLNetTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLNetTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLNetTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast(self,vocab_file,tokenizer_file=None,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.__init__(self,vocab_file,tokenizer_file=None,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/configuration_xlnet.py----------------------------------------
A:transformers.models.xlnet.configuration_xlnet.logger->utils.logging.get_logger(__name__)
transformers.XLNetConfig(self,vocab_size=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=512,reuse_len=None,use_mems_eval=True,use_mems_train=False,bi_data=False,clamp_len=-1,same_length=False,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,pad_token_id=5,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.XLNetConfig.hidden_size(self)
transformers.XLNetConfig.max_position_embeddings(self)
transformers.XLNetConfig.n_token(self)
transformers.XLNetConfig.n_token(self,value)
transformers.XLNetConfig.num_attention_heads(self)
transformers.XLNetConfig.num_hidden_layers(self)
transformers.models.xlnet.configuration_xlnet.XLNetConfig(self,vocab_size=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=512,reuse_len=None,use_mems_eval=True,use_mems_train=False,bi_data=False,clamp_len=-1,same_length=False,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,pad_token_id=5,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.__init__(self,vocab_size=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=512,reuse_len=None,use_mems_eval=True,use_mems_train=False,bi_data=False,clamp_len=-1,same_length=False,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,pad_token_id=5,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.hidden_size(self)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.max_position_embeddings(self)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.n_token(self)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.n_token(self,value)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.num_attention_heads(self)
transformers.models.xlnet.configuration_xlnet.XLNetConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/tokenization_xlnet.py----------------------------------------
A:transformers.models.xlnet.tokenization_xlnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlnet.tokenization_xlnet.self.sp_model->sentencepiece.SentencePieceProcessor(**self.sp_model_kwargs)
A:transformers.models.xlnet.tokenization_xlnet.state->self.__dict__.copy()
A:transformers.models.xlnet.tokenization_xlnet.outputs->outputs.lower().lower()
A:transformers.models.xlnet.tokenization_xlnet.text->self.preprocess_text(text)
A:transformers.models.xlnet.tokenization_xlnet.pieces->self.sp_model.encode(text, out_type=str)
A:transformers.models.xlnet.tokenization_xlnet.cur_pieces->self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, ''))
A:transformers.models.xlnet.tokenization_xlnet.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.models.xlnet.tokenization_xlnet.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
transformers.XLNetTokenizer(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.XLNetTokenizer.__getstate__(self)
transformers.XLNetTokenizer.__setstate__(self,d)
transformers.XLNetTokenizer._convert_id_to_token(self,index)
transformers.XLNetTokenizer._convert_token_to_id(self,token)
transformers.XLNetTokenizer._tokenize(self,text:str)->List[str]
transformers.XLNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLNetTokenizer.get_vocab(self)
transformers.XLNetTokenizer.preprocess_text(self,inputs)
transformers.XLNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.XLNetTokenizer.vocab_size(self)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.__getstate__(self)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.__init__(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],sp_model_kwargs:Optional[Dict[str,Any]]=None,**kwargs)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.__setstate__(self,d)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer._convert_id_to_token(self,index)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer._convert_token_to_id(self,token)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer._tokenize(self,text:str)->List[str]
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.get_vocab(self)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.preprocess_text(self,inputs)
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/__init__.py----------------------------------------
A:transformers.models.xlnet.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.config->transformers.XLNetConfig.from_json_file(bert_config_file)
A:transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.model->XLNetLMHeadModel(config)
A:transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.pytorch_config_dump_path->os.path.join(pytorch_dump_folder_path, CONFIG_NAME)
A:transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.xlnet.convert_xlnet_original_tf_checkpoint_to_pytorch.convert_xlnet_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_folder_path,finetuning_task=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/xlnet/modeling_xlnet.py----------------------------------------
A:transformers.models.xlnet.modeling_xlnet.logger->utils.logging.get_logger(__name__)
A:transformers.models.xlnet.modeling_xlnet.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.xlnet.modeling_xlnet.array->numpy.transpose(array)
A:transformers.models.xlnet.modeling_xlnet.tf_to_pt_map->build_tf_xlnet_to_pytorch_map(model, config, tf_weights)
A:transformers.models.xlnet.modeling_xlnet.p_i.data->torch.from_numpy(arr_i)
A:transformers.models.xlnet.modeling_xlnet.pointer.data->torch.from_numpy(array)
A:transformers.models.xlnet.modeling_xlnet.self.q->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.k->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.v->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.o->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.r->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.r_r_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.r_s_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.r_w_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.seg_embed->torch.nn.Parameter(torch.FloatTensor(2, self.n_head, self.d_head))
A:transformers.models.xlnet.modeling_xlnet.self.layer_norm->torch.nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)
A:transformers.models.xlnet.modeling_xlnet.self.dropout->torch.nn.Dropout(config.dropout)
A:transformers.models.xlnet.modeling_xlnet.x->torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
A:transformers.models.xlnet.modeling_xlnet.ac->torch.einsum('ibnd,jbnd->bnij', q_head + self.r_w_bias, k_head_h)
A:transformers.models.xlnet.modeling_xlnet.bd->self.rel_shift_bnij(bd, klen=ac.shape[3])
A:transformers.models.xlnet.modeling_xlnet.ef->torch.einsum('ijbs,ibns->bnij', seg_mat, ef)
A:transformers.models.xlnet.modeling_xlnet.attn_prob->self.dropout(attn_prob)
A:transformers.models.xlnet.modeling_xlnet.attn_vec->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.models.xlnet.modeling_xlnet.attn_out->self.dropout(attn_out)
A:transformers.models.xlnet.modeling_xlnet.output->self.sequence_summary(output)
A:transformers.models.xlnet.modeling_xlnet.cat->torch.cat([mems, h], dim=0)
A:transformers.models.xlnet.modeling_xlnet.k_head_h->torch.einsum('ibh,hnd->ibnd', cat, self.k)
A:transformers.models.xlnet.modeling_xlnet.v_head_h->torch.einsum('ibh,hnd->ibnd', cat, self.v)
A:transformers.models.xlnet.modeling_xlnet.k_head_r->torch.einsum('ibh,hnd->ibnd', r.type(self.r.dtype), self.r)
A:transformers.models.xlnet.modeling_xlnet.q_head_h->torch.einsum('ibh,hnd->ibnd', h, self.q)
A:transformers.models.xlnet.modeling_xlnet.attn_vec_h->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.models.xlnet.modeling_xlnet.output_h->self.dropout(word_emb_k)
A:transformers.models.xlnet.modeling_xlnet.q_head_g->torch.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)
A:transformers.models.xlnet.modeling_xlnet.attn_vec_g->self.rel_attn_core(q_head_g, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_g, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.models.xlnet.modeling_xlnet.output_g->self.dropout(word_emb_q)
A:transformers.models.xlnet.modeling_xlnet.self.layer_1->torch.nn.Linear(config.d_model, config.d_inner)
A:transformers.models.xlnet.modeling_xlnet.self.layer_2->torch.nn.Linear(config.d_inner, config.d_model)
A:transformers.models.xlnet.modeling_xlnet.self.rel_attn->XLNetRelativeAttention(config)
A:transformers.models.xlnet.modeling_xlnet.self.ff->XLNetFeedForward(config)
A:transformers.models.xlnet.modeling_xlnet.outputs->self.transformer(input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, use_mems=use_mems, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, **kwargs)
A:transformers.models.xlnet.modeling_xlnet.output_x->self.ff(output_x)
A:transformers.models.xlnet.modeling_xlnet.self.word_embedding->torch.nn.Embedding(config.vocab_size, config.d_model)
A:transformers.models.xlnet.modeling_xlnet.self.mask_emb->torch.nn.Parameter(torch.FloatTensor(1, 1, config.d_model))
A:transformers.models.xlnet.modeling_xlnet.self.layer->torch.nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])
A:transformers.models.xlnet.modeling_xlnet.attn_mask->(attn_mask > 0).to(dtype_float)
A:transformers.models.xlnet.modeling_xlnet.mask_up->torch.triu(attn_mask, diagonal=1)
A:transformers.models.xlnet.modeling_xlnet.attn_mask_pad->torch.zeros([qlen, mlen])
A:transformers.models.xlnet.modeling_xlnet.ret->ret.to(self.device).to(self.device)
A:transformers.models.xlnet.modeling_xlnet.mask_lo->torch.tril(attn_mask, diagonal=-1)
A:transformers.models.xlnet.modeling_xlnet.sinusoid_inp->torch.einsum('i,d->id', pos_seq, inv_freq)
A:transformers.models.xlnet.modeling_xlnet.pos_emb->self.dropout(pos_emb)
A:transformers.models.xlnet.modeling_xlnet.freq_seq->torch.arange(0, self.d_model, 2.0, dtype=torch.float)
A:transformers.models.xlnet.modeling_xlnet.fwd_pos_seq->fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len).clamp(-self.clamp_len, self.clamp_len)
A:transformers.models.xlnet.modeling_xlnet.bwd_pos_seq->bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len).clamp(-self.clamp_len, self.clamp_len)
A:transformers.models.xlnet.modeling_xlnet.fwd_pos_emb->self.positional_embedding(fwd_pos_seq, inv_freq)
A:transformers.models.xlnet.modeling_xlnet.bwd_pos_emb->self.positional_embedding(bwd_pos_seq, inv_freq)
A:transformers.models.xlnet.modeling_xlnet.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.models.xlnet.modeling_xlnet.inputs_embeds->inputs_embeds.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.models.xlnet.modeling_xlnet.mems_mask->torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)
A:transformers.models.xlnet.modeling_xlnet.data_mask->torch.cat([mems_mask, data_mask], dim=1)
A:transformers.models.xlnet.modeling_xlnet.non_tgt_mask->(attn_mask + non_tgt_mask[:, :, None, None] > 0).to(attn_mask)
A:transformers.models.xlnet.modeling_xlnet.word_emb_k->self.word_embedding(input_ids)
A:transformers.models.xlnet.modeling_xlnet.word_emb_q->self.mask_emb.expand(target_mapping.shape[0], bsz, -1)
A:transformers.models.xlnet.modeling_xlnet.mem_pad->torch.zeros([mlen, bsz], dtype=torch.long, device=device)
A:transformers.models.xlnet.modeling_xlnet.cat_ids->torch.cat([mem_pad, token_type_ids], dim=0)
A:transformers.models.xlnet.modeling_xlnet.seg_mat->torch.nn.functional.one_hot(seg_mat, num_classes=2).to(dtype_float)
A:transformers.models.xlnet.modeling_xlnet.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.models.xlnet.modeling_xlnet.hidden_states->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states))
A:transformers.models.xlnet.modeling_xlnet.attentions->tuple((t.permute(2, 3, 0, 1).contiguous() for t in attentions))
A:transformers.models.xlnet.modeling_xlnet.self.transformer->XLNetModel(config)
A:transformers.models.xlnet.modeling_xlnet.self.lm_loss->torch.nn.Linear(config.d_model, config.vocab_size, bias=True)
A:transformers.models.xlnet.modeling_xlnet.dummy_token->torch.zeros((effective_batch_size, 1), dtype=torch.long, device=input_ids.device)
A:transformers.models.xlnet.modeling_xlnet.perm_mask->torch.zeros((effective_batch_size, sequence_length, sequence_length), dtype=torch.float, device=input_ids.device)
A:transformers.models.xlnet.modeling_xlnet.target_mapping->torch.zeros((effective_batch_size, 1, sequence_length), dtype=torch.float, device=input_ids.device)
A:transformers.models.xlnet.modeling_xlnet.inputs['mems']->tuple((layer_past[:-offset, :, :] for layer_past in past))
A:transformers.models.xlnet.modeling_xlnet.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, use_mems=use_mems, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, **kwargs)
A:transformers.models.xlnet.modeling_xlnet.logits->self.qa_outputs(sequence_output)
A:transformers.models.xlnet.modeling_xlnet.loss_fct->CrossEntropyLoss()
A:transformers.models.xlnet.modeling_xlnet.loss->loss_fct(reshaped_logits, labels.view(-1))
A:transformers.models.xlnet.modeling_xlnet.self.sequence_summary->SequenceSummary(config)
A:transformers.models.xlnet.modeling_xlnet.self.logits_proj->torch.nn.Linear(config.d_model, 1)
A:transformers.models.xlnet.modeling_xlnet.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.xlnet.modeling_xlnet.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.models.xlnet.modeling_xlnet.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.xlnet.modeling_xlnet.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.models.xlnet.modeling_xlnet.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.xlnet.modeling_xlnet.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.models.xlnet.modeling_xlnet.start_logits->self.start_logits(hidden_states, p_mask=p_mask)
A:transformers.models.xlnet.modeling_xlnet.end_logits->self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)
A:transformers.models.xlnet.modeling_xlnet.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.xlnet.modeling_xlnet.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.xlnet.modeling_xlnet.ignored_index->self.start_logits(hidden_states, p_mask=p_mask).size(1)
A:transformers.models.xlnet.modeling_xlnet.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.xlnet.modeling_xlnet.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.xlnet.modeling_xlnet.self.start_logits->PoolerStartLogits(config)
A:transformers.models.xlnet.modeling_xlnet.self.end_logits->PoolerEndLogits(config)
A:transformers.models.xlnet.modeling_xlnet.self.answer_class->PoolerAnswerClass(config)
A:transformers.models.xlnet.modeling_xlnet.cls_logits->self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)
A:transformers.models.xlnet.modeling_xlnet.loss_fct_cls->torch.nn.BCEWithLogitsLoss()
A:transformers.models.xlnet.modeling_xlnet.cls_loss->loss_fct_cls(cls_logits, is_impossible)
A:transformers.models.xlnet.modeling_xlnet.(bsz, slen, hsz)->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states)).size()
A:transformers.models.xlnet.modeling_xlnet.start_log_probs->torch.nn.functional.softmax(start_logits, dim=-1)
A:transformers.models.xlnet.modeling_xlnet.(start_top_log_probs, start_top_index)->torch.topk(start_log_probs, self.start_n_top, dim=-1)
A:transformers.models.xlnet.modeling_xlnet.start_top_index_exp->start_top_index.unsqueeze(-1).expand(-1, -1, hsz)
A:transformers.models.xlnet.modeling_xlnet.start_states->torch.einsum('blh,bl->bh', hidden_states, start_log_probs)
A:transformers.models.xlnet.modeling_xlnet.hidden_states_expanded->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states)).unsqueeze(2).expand_as(start_states)
A:transformers.models.xlnet.modeling_xlnet.end_log_probs->torch.nn.functional.softmax(end_logits, dim=1)
A:transformers.models.xlnet.modeling_xlnet.(end_top_log_probs, end_top_index)->torch.topk(end_log_probs, self.end_n_top, dim=1)
A:transformers.models.xlnet.modeling_xlnet.end_top_log_probs->end_top_log_probs.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.models.xlnet.modeling_xlnet.end_top_index->end_top_index.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
transformers.XLNetForMultipleChoice(self,config)
transformers.XLNetForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetForMultipleChoiceOutput(ModelOutput)
transformers.XLNetForQuestionAnswering(self,config)
transformers.XLNetForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetForQuestionAnsweringOutput(ModelOutput)
transformers.XLNetForQuestionAnsweringSimple(self,config)
transformers.XLNetForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.XLNetForSequenceClassification(self,config)
transformers.XLNetForSequenceClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetForSequenceClassificationOutput(ModelOutput)
transformers.XLNetForTokenClassification(self,config)
transformers.XLNetForTokenClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetForTokenClassificationOutput(ModelOutput)
transformers.XLNetLMHeadModel(self,config)
transformers.XLNetLMHeadModel._reorder_cache(mems:List[torch.Tensor],beam_idx:torch.Tensor)->List[torch.Tensor]
transformers.XLNetLMHeadModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetLMHeadModel.get_output_embeddings(self)
transformers.XLNetLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,use_mems=None,**kwargs)
transformers.XLNetLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.XLNetLMHeadModelOutput(ModelOutput)
transformers.XLNetModel(self,config)
transformers.XLNetModel._prune_heads(self,heads_to_prune)
transformers.XLNetModel.cache_mem(self,curr_out,prev_mem)
transformers.XLNetModel.create_mask(self,qlen,mlen)
transformers.XLNetModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.XLNetModel.get_input_embeddings(self)
transformers.XLNetModel.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.XLNetModel.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.XLNetModel.set_input_embeddings(self,new_embeddings)
transformers.XLNetModelOutput(ModelOutput)
transformers.XLNetPreTrainedModel(PreTrainedModel)
transformers.XLNetPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_xlnet(model,config,tf_path)
transformers.models.xlnet.modeling_xlnet.XLNetFeedForward(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetFeedForward.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetFeedForward.forward(self,inp)
transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoice(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoice.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetForMultipleChoiceOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnswering(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnswering.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimple(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimple.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassification(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassification.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassification(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassification.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetForTokenClassificationOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel._reorder_cache(mems:List[torch.Tensor],beam_idx:torch.Tensor)->List[torch.Tensor]
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel.get_output_embeddings(self)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,use_mems=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModel.set_output_embeddings(self,new_embeddings)
transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetLayer(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetLayer.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetLayer.ff_chunk(self,output_x)
transformers.models.xlnet.modeling_xlnet.XLNetLayer.forward(self,output_h,output_g,attn_mask_h,attn_mask_g,r,seg_mat,mems=None,target_mapping=None,head_mask=None,output_attentions=False)
transformers.models.xlnet.modeling_xlnet.XLNetModel(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetModel.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetModel._prune_heads(self,heads_to_prune)
transformers.models.xlnet.modeling_xlnet.XLNetModel.cache_mem(self,curr_out,prev_mem)
transformers.models.xlnet.modeling_xlnet.XLNetModel.create_mask(self,qlen,mlen)
transformers.models.xlnet.modeling_xlnet.XLNetModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_mems=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.models.xlnet.modeling_xlnet.XLNetModel.get_input_embeddings(self)
transformers.models.xlnet.modeling_xlnet.XLNetModel.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.models.xlnet.modeling_xlnet.XLNetModel.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.models.xlnet.modeling_xlnet.XLNetModel.set_input_embeddings(self,new_embeddings)
transformers.models.xlnet.modeling_xlnet.XLNetModelOutput(ModelOutput)
transformers.models.xlnet.modeling_xlnet.XLNetPreTrainedModel(PreTrainedModel)
transformers.models.xlnet.modeling_xlnet.XLNetPreTrainedModel._init_weights(self,module)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.__init__(self,config)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.forward(self,h,g,attn_mask_h,attn_mask_g,r,seg_mat,mems=None,target_mapping=None,head_mask=None,output_attentions=False)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.post_attention(self,h,attn_vec,residual=True)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.prune_heads(self,heads)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.rel_attn_core(self,q_head,k_head_h,v_head_h,k_head_r,seg_mat=None,attn_mask=None,head_mask=None,output_attentions=False)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.rel_shift(x,klen=-1)
transformers.models.xlnet.modeling_xlnet.XLNetRelativeAttention.rel_shift_bnij(x,klen=-1)
transformers.models.xlnet.modeling_xlnet.build_tf_xlnet_to_pytorch_map(model,config,tf_weights=None)
transformers.models.xlnet.modeling_xlnet.load_tf_weights_in_xlnet(model,config,tf_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/fsmt/modeling_fsmt.py----------------------------------------
A:transformers.models.fsmt.modeling_fsmt.logger->utils.logging.get_logger(__name__)
A:transformers.models.fsmt.modeling_fsmt.arange->arange.unsqueeze(-1).unsqueeze(-1)
A:transformers.models.fsmt.modeling_fsmt.mask->tensor.ne(padding_idx).int()
A:transformers.models.fsmt.modeling_fsmt.decoder_input_ids->shift_tokens_right(input_ids, pad_token_id)
A:transformers.models.fsmt.modeling_fsmt.(bsz, tgt_len)->shift_tokens_right(input_ids, pad_token_id).size()
A:transformers.models.fsmt.modeling_fsmt.decoder_padding_mask->invert_mask(decoder_padding_mask)
A:transformers.models.fsmt.modeling_fsmt.causal_mask->triu_onnx(fill_with_neg_inf(torch.zeros(tgt_len, tgt_len)), 1).to(dtype=causal_mask_dtype, device=decoder_input_ids.device)
A:transformers.models.fsmt.modeling_fsmt.input_ids->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)
A:transformers.models.fsmt.modeling_fsmt.lin_layer->torch.nn.Linear(vocab_size, emb_size, bias=False)
A:transformers.models.fsmt.modeling_fsmt.prev_output_tokens->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).clone()
A:transformers.models.fsmt.modeling_fsmt.index_of_eos->(input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)
A:transformers.models.fsmt.modeling_fsmt.prev_output_tokens[:, 0]->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).gather(1, index_of_eos).squeeze()
A:transformers.models.fsmt.modeling_fsmt.padding_mask->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).eq(padding_idx)
A:transformers.models.fsmt.modeling_fsmt.self.self_attn->Attention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout)
A:transformers.models.fsmt.modeling_fsmt.self.self_attn_layer_norm->LayerNorm(self.embed_dim)
A:transformers.models.fsmt.modeling_fsmt.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.fsmt.modeling_fsmt.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.fsmt.modeling_fsmt.self.final_layer_norm->LayerNorm(self.embed_dim)
A:transformers.models.fsmt.modeling_fsmt.(x, attn_weights)->self.self_attn(query=x, key=x, key_padding_mask=encoder_padding_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.fsmt.modeling_fsmt.x->self.output_projection(x)
A:transformers.models.fsmt.modeling_fsmt.self.embed_positions->SinusoidalPositionalEmbedding(config.max_position_embeddings + self.padding_idx + 1, embed_dim, self.padding_idx)
A:transformers.models.fsmt.modeling_fsmt.self.layers->torch.nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.fsmt.modeling_fsmt.attention_mask->invert_mask(attention_mask)
A:transformers.models.fsmt.modeling_fsmt.embed_pos->self.embed_positions(input_ids)
A:transformers.models.fsmt.modeling_fsmt.dropout_probability->random.uniform(0, 1)
A:transformers.models.fsmt.modeling_fsmt.(x, attn)->encoder_layer(x, attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, output_attentions=output_attentions)
A:transformers.models.fsmt.modeling_fsmt.self.encoder_attn->Attention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, encoder_decoder_attention=True)
A:transformers.models.fsmt.modeling_fsmt.self.encoder_attn_layer_norm->LayerNorm(self.embed_dim)
A:transformers.models.fsmt.modeling_fsmt.(x, self_attn_weights)->self.self_attn(query=x, key=x, layer_state=layer_state, key_padding_mask=decoder_padding_mask, attn_mask=causal_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.fsmt.modeling_fsmt.(x, cross_attn_weights)->self.encoder_attn(query=x, key=encoder_hidden_states, key_padding_mask=encoder_attn_mask, layer_state=layer_state, layer_head_mask=cross_attn_layer_head_mask, output_attentions=output_attentions)
A:transformers.models.fsmt.modeling_fsmt.self.output_projection->torch.nn.Linear(self.embed_tokens.weight.shape[1], self.embed_tokens.weight.shape[0], bias=False)
A:transformers.models.fsmt.modeling_fsmt.encoder_padding_mask->invert_mask(encoder_padding_mask)
A:transformers.models.fsmt.modeling_fsmt.positions->self.make_positions(input, self.padding_idx)
A:transformers.models.fsmt.modeling_fsmt.encoder_hidden_states->encoder_hidden_states.transpose(0, 1).transpose(0, 1)
A:transformers.models.fsmt.modeling_fsmt.(x, layer_self_attn, layer_past, layer_cross_attn)->decoder_layer(x, encoder_hidden_states, encoder_attn_mask=encoder_padding_mask, decoder_padding_mask=decoder_padding_mask, layer_state=layer_state, causal_mask=decoder_causal_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, output_attentions=output_attentions)
A:transformers.models.fsmt.modeling_fsmt.attn_cache[k]->input_buffer_k.index_select(0, new_order)
A:transformers.models.fsmt.modeling_fsmt.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.fsmt.modeling_fsmt.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.fsmt.modeling_fsmt.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.fsmt.modeling_fsmt.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.fsmt.modeling_fsmt.(tgt_len, bsz, embed_dim)->query.size()
A:transformers.models.fsmt.modeling_fsmt.saved_state->layer_state.get(self.cache_key, {})
A:transformers.models.fsmt.modeling_fsmt.k->torch.cat([prev_key, k], dim=1)
A:transformers.models.fsmt.modeling_fsmt.v->torch.cat([prev_value, v], dim=1)
A:transformers.models.fsmt.modeling_fsmt.q->self._shape(q, tgt_len, bsz)
A:transformers.models.fsmt.modeling_fsmt.(k, v, key_padding_mask)->self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)
A:transformers.models.fsmt.modeling_fsmt.src_len->torch.cat([prev_key, k], dim=1).size(1)
A:transformers.models.fsmt.modeling_fsmt.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.fsmt.modeling_fsmt.reshaped->key_padding_mask.unsqueeze(1).unsqueeze(2)
A:transformers.models.fsmt.modeling_fsmt.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.fsmt.modeling_fsmt.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.fsmt.modeling_fsmt.attn_output->self.out_proj(attn_output)
A:transformers.models.fsmt.modeling_fsmt.prev_key->_prev_key.view(bsz * self.num_heads, -1, self.head_dim)
A:transformers.models.fsmt.modeling_fsmt.prev_value->_prev_value.view(bsz * self.num_heads, -1, self.head_dim)
A:transformers.models.fsmt.modeling_fsmt.new_key_padding_mask->torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)
A:transformers.models.fsmt.modeling_fsmt.encoder_embed_tokens->torch.nn.Embedding(config.src_vocab_size, config.d_model, padding_idx)
A:transformers.models.fsmt.modeling_fsmt.decoder_embed_tokens->torch.nn.Embedding(config.tgt_vocab_size, config.d_model, padding_idx)
A:transformers.models.fsmt.modeling_fsmt.self.encoder->FSMTEncoder(config, encoder_embed_tokens)
A:transformers.models.fsmt.modeling_fsmt.self.decoder->FSMTDecoder(config, decoder_embed_tokens)
A:transformers.models.fsmt.modeling_fsmt.(decoder_input_ids, decoder_padding_mask, causal_mask)->_prepare_fsmt_decoder_inputs(self.config, input_ids, decoder_input_ids=decoder_input_ids, decoder_padding_mask=decoder_attention_mask, causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)
A:transformers.models.fsmt.modeling_fsmt.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.fsmt.modeling_fsmt.decoder_outputs->self.decoder(decoder_input_ids, encoder_outputs[0], attention_mask, decoder_padding_mask, decoder_causal_mask=causal_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.fsmt.modeling_fsmt.base_model->FSMTModel(config)
A:transformers.models.fsmt.modeling_fsmt.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.fsmt.modeling_fsmt.outputs->self.model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, encoder_outputs=encoder_outputs, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.fsmt.modeling_fsmt.loss_fct->CrossEntropyLoss()
A:transformers.models.fsmt.modeling_fsmt.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.tgt_vocab_size), labels.view(-1))
A:transformers.models.fsmt.modeling_fsmt.weight->weight.to(self.weight.device).to(self.weight.device)
A:transformers.models.fsmt.modeling_fsmt.self.weight->torch.nn.Parameter(weight)
A:transformers.models.fsmt.modeling_fsmt.emb->torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
transformers.FSMTForConditionalGeneration(self,config:FSMTConfig)
transformers.FSMTForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.FSMTForConditionalGeneration.forward(self,input_ids,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FSMTForConditionalGeneration.get_encoder(self)
transformers.FSMTForConditionalGeneration.get_output_embeddings(self)
transformers.FSMTForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.FSMTForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.FSMTForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.FSMTModel(self,config:FSMTConfig)
transformers.FSMTModel.forward(self,input_ids,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Tuple]=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FSMTModel.get_input_embeddings(self)
transformers.FSMTModel.get_output_embeddings(self)
transformers.FSMTModel.set_input_embeddings(self,value)
transformers.FSMTModel.set_output_embeddings(self,value)
transformers.PretrainedFSMTModel(PreTrainedModel)
transformers.PretrainedFSMTModel._init_weights(self,module)
transformers.PretrainedFSMTModel.dummy_inputs(self)
transformers.models.fsmt.modeling_fsmt.Attention(self,embed_dim,num_heads,dropout=0.0,bias=True,encoder_decoder_attention=False)
transformers.models.fsmt.modeling_fsmt.Attention.__init__(self,embed_dim,num_heads,dropout=0.0,bias=True,encoder_decoder_attention=False)
transformers.models.fsmt.modeling_fsmt.Attention._shape(self,tensor,seq_len,bsz)
transformers.models.fsmt.modeling_fsmt.Attention._use_saved_state(self,k,v,saved_state,key_padding_mask,static_kv,bsz)
transformers.models.fsmt.modeling_fsmt.Attention.forward(self,query,key:Optional[Tensor],key_padding_mask:Optional[Tensor]=None,layer_state:Optional[Dict[str,Optional[Tensor]]]=None,attn_mask:Optional[Tensor]=None,layer_head_mask:Optional[Tensor]=None,output_attentions=False)->Tuple[Tensor, Optional[Tensor]]
transformers.models.fsmt.modeling_fsmt.DecoderLayer(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.DecoderLayer.__init__(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.DecoderLayer.forward(self,x,encoder_hidden_states,encoder_attn_mask=None,layer_state=None,causal_mask=None,layer_head_mask=None,cross_attn_layer_head_mask=None,decoder_padding_mask=None,output_attentions=False)
transformers.models.fsmt.modeling_fsmt.EncoderLayer(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.EncoderLayer.__init__(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.EncoderLayer.forward(self,x,encoder_padding_mask,layer_head_mask,output_attentions=False)
transformers.models.fsmt.modeling_fsmt.FSMTDecoder(self,config:FSMTConfig,embed_tokens:nn.Embedding)
transformers.models.fsmt.modeling_fsmt.FSMTDecoder.__init__(self,config:FSMTConfig,embed_tokens:nn.Embedding)
transformers.models.fsmt.modeling_fsmt.FSMTDecoder.forward(self,input_ids,encoder_hidden_states,encoder_padding_mask,decoder_padding_mask,decoder_causal_mask,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=False,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.fsmt.modeling_fsmt.FSMTEncoder(self,config:FSMTConfig,embed_tokens)
transformers.models.fsmt.modeling_fsmt.FSMTEncoder.__init__(self,config:FSMTConfig,embed_tokens)
transformers.models.fsmt.modeling_fsmt.FSMTEncoder.forward(self,input_ids,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.__init__(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.forward(self,input_ids,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.get_encoder(self)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.get_output_embeddings(self)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.fsmt.modeling_fsmt.FSMTForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.fsmt.modeling_fsmt.FSMTModel(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.FSMTModel.__init__(self,config:FSMTConfig)
transformers.models.fsmt.modeling_fsmt.FSMTModel.forward(self,input_ids,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Tuple]=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.fsmt.modeling_fsmt.FSMTModel.get_input_embeddings(self)
transformers.models.fsmt.modeling_fsmt.FSMTModel.get_output_embeddings(self)
transformers.models.fsmt.modeling_fsmt.FSMTModel.set_input_embeddings(self,value)
transformers.models.fsmt.modeling_fsmt.FSMTModel.set_output_embeddings(self,value)
transformers.models.fsmt.modeling_fsmt.PretrainedFSMTModel(PreTrainedModel)
transformers.models.fsmt.modeling_fsmt.PretrainedFSMTModel._init_weights(self,module)
transformers.models.fsmt.modeling_fsmt.PretrainedFSMTModel.dummy_inputs(self)
transformers.models.fsmt.modeling_fsmt.SinusoidalPositionalEmbedding(self,num_positions,embedding_dim,padding_idx)
transformers.models.fsmt.modeling_fsmt.SinusoidalPositionalEmbedding.__init__(self,num_positions,embedding_dim,padding_idx)
transformers.models.fsmt.modeling_fsmt.SinusoidalPositionalEmbedding.forward(self,input,incremental_state:Optional[Any]=None,timestep:Optional[Tensor]=None)
transformers.models.fsmt.modeling_fsmt.SinusoidalPositionalEmbedding.get_embedding(num_embeddings,embedding_dim,padding_idx)
transformers.models.fsmt.modeling_fsmt.SinusoidalPositionalEmbedding.make_positions(tensor,padding_idx:int)
transformers.models.fsmt.modeling_fsmt.SinusoidalPositionalEmbedding.make_weight(self,num_positions,embedding_dim,padding_idx)
transformers.models.fsmt.modeling_fsmt._check_shapes(shape_1,shape2)
transformers.models.fsmt.modeling_fsmt._get_shape(t)
transformers.models.fsmt.modeling_fsmt._make_linear_from_emb(emb)
transformers.models.fsmt.modeling_fsmt._prepare_fsmt_decoder_inputs(config,input_ids,decoder_input_ids=None,decoder_padding_mask=None,causal_mask_dtype=torch.float32)
transformers.models.fsmt.modeling_fsmt._reorder_buffer(attn_cache,new_order)
transformers.models.fsmt.modeling_fsmt.fill_with_neg_inf(t)
transformers.models.fsmt.modeling_fsmt.invert_mask(attention_mask)
transformers.models.fsmt.modeling_fsmt.make_padding_mask(input_ids,padding_idx=1)
transformers.models.fsmt.modeling_fsmt.shift_tokens_right(input_ids,pad_token_id)
transformers.models.fsmt.modeling_fsmt.triu_onnx(x,diagonal=0)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.d2->dict(((re.sub('@@$', '', k), v) if k.endswith('@@') else (re.sub('$', '</w>', k), v) for (k, v) in d.items()))
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.keep_keys->'<s> <pad> </s> <unk>'.split()
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.checkpoint_file->basename(fsmt_checkpoint_path)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.fsmt_folder_path->dirname(fsmt_checkpoint_path)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.models->cls.hub_models()
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.chkpt->fairseq.hub_utils.from_pretrained(fsmt_folder_path, checkpoint_file, data_name_or_path, archive_map=models, **kwargs)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.data_root->dirname(pytorch_dump_folder_path)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.model_dir->basename(pytorch_dump_folder_path)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.src_dict_file->os.path.join(fsmt_folder_path, f'dict.{src_lang}.txt')
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.tgt_dict_file->os.path.join(fsmt_folder_path, f'dict.{tgt_lang}.txt')
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.src_dict->fairseq.data.dictionary.Dictionary.load(src_dict_file)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.src_vocab->rewrite_dict_keys(src_dict.indices)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.src_vocab_size->len(src_vocab)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.src_vocab_file->os.path.join(pytorch_dump_folder_path, 'vocab-src.json')
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.tgt_dict->fairseq.data.dictionary.Dictionary.load(tgt_dict_file)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.tgt_vocab->rewrite_dict_keys(tgt_dict.indices)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.tgt_vocab_size->len(tgt_vocab)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.tgt_vocab_file->os.path.join(pytorch_dump_folder_path, 'vocab-tgt.json')
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.merges_file->os.path.join(pytorch_dump_folder_path, VOCAB_FILES_NAMES['merges_file'])
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.fsmt_merges_file->os.path.join(fsmt_folder_path, fn)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.merges->re.sub(' \\d+$', '', merges, 0, re.M)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.fsmt_model_config_file->os.path.join(pytorch_dump_folder_path, 'config.json')
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.fsmt_tokenizer_config_file->os.path.join(pytorch_dump_folder_path, TOKENIZER_CONFIG_FILE)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.model_state_dict->OrderedDict((('model.' + k, v) for (k, v) in model_state_dict.items()))
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.config->transformers.FSMTConfig.from_pretrained(pytorch_dump_folder_path)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.model_new->FSMTForConditionalGeneration(config)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path,pytorch_dump_folder_path)
transformers.models.fsmt.convert_fsmt_original_pytorch_checkpoint_to_pytorch.rewrite_dict_keys(d)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/fsmt/configuration_fsmt.py----------------------------------------
A:transformers.models.fsmt.configuration_fsmt.logger->utils.logging.get_logger(__name__)
A:transformers.models.fsmt.configuration_fsmt.self.decoder->DecoderConfig(vocab_size=tgt_vocab_size, bos_token_id=eos_token_id)
A:transformers.models.fsmt.configuration_fsmt.output->copy.deepcopy(self.__dict__)
A:transformers.models.fsmt.configuration_fsmt.output['decoder']->self.decoder.to_dict()
transformers.FSMTConfig(self,langs=['en','de'],src_vocab_size=42024,tgt_vocab_size=42024,activation_function='relu',d_model=1024,max_length=200,max_position_embeddings=1024,encoder_ffn_dim=4096,encoder_layers=12,encoder_attention_heads=16,encoder_layerdrop=0.0,decoder_ffn_dim=4096,decoder_layers=12,decoder_attention_heads=16,decoder_layerdrop=0.0,attention_dropout=0.0,dropout=0.1,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,is_encoder_decoder=True,scale_embedding=True,tie_word_embeddings=False,num_beams=5,length_penalty=1.0,early_stopping=False,use_cache=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,forced_eos_token_id=2,**common_kwargs)
transformers.FSMTConfig.hidden_size(self)->int
transformers.FSMTConfig.num_attention_heads(self)->int
transformers.FSMTConfig.to_dict(self)
transformers.models.fsmt.configuration_fsmt.DecoderConfig(self,vocab_size=0,bos_token_id=0)
transformers.models.fsmt.configuration_fsmt.DecoderConfig.__init__(self,vocab_size=0,bos_token_id=0)
transformers.models.fsmt.configuration_fsmt.FSMTConfig(self,langs=['en','de'],src_vocab_size=42024,tgt_vocab_size=42024,activation_function='relu',d_model=1024,max_length=200,max_position_embeddings=1024,encoder_ffn_dim=4096,encoder_layers=12,encoder_attention_heads=16,encoder_layerdrop=0.0,decoder_ffn_dim=4096,decoder_layers=12,decoder_attention_heads=16,decoder_layerdrop=0.0,attention_dropout=0.0,dropout=0.1,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,is_encoder_decoder=True,scale_embedding=True,tie_word_embeddings=False,num_beams=5,length_penalty=1.0,early_stopping=False,use_cache=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,forced_eos_token_id=2,**common_kwargs)
transformers.models.fsmt.configuration_fsmt.FSMTConfig.__init__(self,langs=['en','de'],src_vocab_size=42024,tgt_vocab_size=42024,activation_function='relu',d_model=1024,max_length=200,max_position_embeddings=1024,encoder_ffn_dim=4096,encoder_layers=12,encoder_attention_heads=16,encoder_layerdrop=0.0,decoder_ffn_dim=4096,decoder_layers=12,decoder_attention_heads=16,decoder_layerdrop=0.0,attention_dropout=0.0,dropout=0.1,activation_dropout=0.0,init_std=0.02,decoder_start_token_id=2,is_encoder_decoder=True,scale_embedding=True,tie_word_embeddings=False,num_beams=5,length_penalty=1.0,early_stopping=False,use_cache=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,forced_eos_token_id=2,**common_kwargs)
transformers.models.fsmt.configuration_fsmt.FSMTConfig.hidden_size(self)->int
transformers.models.fsmt.configuration_fsmt.FSMTConfig.num_attention_heads(self)->int
transformers.models.fsmt.configuration_fsmt.FSMTConfig.to_dict(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/fsmt/tokenization_fsmt.py----------------------------------------
A:transformers.models.fsmt.tokenization_fsmt.logger->utils.logging.get_logger(__name__)
A:transformers.models.fsmt.tokenization_fsmt.pairs->get_pairs(word)
A:transformers.models.fsmt.tokenization_fsmt.text->self.moses_detokenize(tokens, self.tgt_lang)
A:transformers.models.fsmt.tokenization_fsmt.cat->unicodedata.category(char)
A:transformers.models.fsmt.tokenization_fsmt.self.cache_moses_punct_normalizer->dict()
A:transformers.models.fsmt.tokenization_fsmt.self.cache_moses_tokenizer->dict()
A:transformers.models.fsmt.tokenization_fsmt.self.cache_moses_detokenizer->dict()
A:transformers.models.fsmt.tokenization_fsmt.self.encoder->json.load(src_vocab_handle)
A:transformers.models.fsmt.tokenization_fsmt.tgt_vocab->json.load(tgt_vocab_handle)
A:transformers.models.fsmt.tokenization_fsmt.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.fsmt.tokenization_fsmt.punct_normalizer->sacremoses.MosesPunctNormalizer(lang=lang)
A:transformers.models.fsmt.tokenization_fsmt.moses_tokenizer->sacremoses.MosesTokenizer(lang=lang)
A:transformers.models.fsmt.tokenization_fsmt.moses_detokenizer->sacremoses.MosesDetokenizer(lang=self.tgt_lang)
A:transformers.models.fsmt.tokenization_fsmt.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.fsmt.tokenization_fsmt.j->' '.join(word).index(first, i)
A:transformers.models.fsmt.tokenization_fsmt.new_word->tuple(new_word)
A:transformers.models.fsmt.tokenization_fsmt.word->' '.join(word)
A:transformers.models.fsmt.tokenization_fsmt.tokens->''.join(tokens).split()
A:transformers.models.fsmt.tokenization_fsmt.src_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['src_vocab_file'])
A:transformers.models.fsmt.tokenization_fsmt.tgt_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['tgt_vocab_file'])
A:transformers.models.fsmt.tokenization_fsmt.merges_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
transformers.FSMTTokenizer(self,langs=None,src_vocab_file=None,tgt_vocab_file=None,merges_file=None,do_lower_case=False,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',**kwargs)
transformers.FSMTTokenizer._convert_id_to_token(self,index)
transformers.FSMTTokenizer._convert_token_to_id(self,token)
transformers.FSMTTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.FSMTTokenizer.bpe(self,token)
transformers.FSMTTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.FSMTTokenizer.convert_tokens_to_string(self,tokens)
transformers.FSMTTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.FSMTTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.FSMTTokenizer.get_src_vocab(self)
transformers.FSMTTokenizer.get_tgt_vocab(self)
transformers.FSMTTokenizer.get_vocab(self)->Dict[str, int]
transformers.FSMTTokenizer.moses_detokenize(self,tokens,lang)
transformers.FSMTTokenizer.moses_pipeline(self,text,lang)
transformers.FSMTTokenizer.moses_punct_norm(self,text,lang)
transformers.FSMTTokenizer.moses_tokenize(self,text,lang)
transformers.FSMTTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.FSMTTokenizer.src_vocab_size(self)
transformers.FSMTTokenizer.tgt_vocab_size(self)
transformers.FSMTTokenizer.vocab_size(self)->int
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer(self,langs=None,src_vocab_file=None,tgt_vocab_file=None,merges_file=None,do_lower_case=False,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',**kwargs)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.__init__(self,langs=None,src_vocab_file=None,tgt_vocab_file=None,merges_file=None,do_lower_case=False,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',**kwargs)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer._convert_id_to_token(self,index)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer._convert_token_to_id(self,token)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.bpe(self,token)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.get_src_vocab(self)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.get_tgt_vocab(self)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.get_vocab(self)->Dict[str, int]
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.moses_detokenize(self,tokens,lang)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.moses_pipeline(self,text,lang)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.moses_punct_norm(self,text,lang)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.moses_tokenize(self,text,lang)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.src_vocab_size(self)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.tgt_vocab_size(self)
transformers.models.fsmt.tokenization_fsmt.FSMTTokenizer.vocab_size(self)->int
transformers.models.fsmt.tokenization_fsmt.get_pairs(word)
transformers.models.fsmt.tokenization_fsmt.remove_non_printing_char(text)
transformers.models.fsmt.tokenization_fsmt.replace_unicode_punct(text)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/fsmt/__init__.py----------------------------------------
A:transformers.models.fsmt.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt_neo/convert_gpt_neo_mesh_tf_to_pytorch.py----------------------------------------
A:transformers.models.gpt_neo.convert_gpt_neo_mesh_tf_to_pytorch.config_json->json.load(open(config_file, 'r'))
A:transformers.models.gpt_neo.convert_gpt_neo_mesh_tf_to_pytorch.config->GPTNeoConfig(hidden_size=config_json['n_embd'], num_layers=config_json['n_layer'], num_heads=config_json['n_head'], attention_types=config_json['attention_types'], max_position_embeddings=config_json['n_ctx'], resid_dropout=config_json['res_dropout'], embed_dropout=config_json['embed_dropout'], attention_dropout=config_json['attn_dropout'])
A:transformers.models.gpt_neo.convert_gpt_neo_mesh_tf_to_pytorch.model->GPTNeoForCausalLM(config)
A:transformers.models.gpt_neo.convert_gpt_neo_mesh_tf_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.gpt_neo.convert_gpt_neo_mesh_tf_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.gpt_neo.convert_gpt_neo_mesh_tf_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt_neo/configuration_gpt_neo.py----------------------------------------
A:transformers.models.gpt_neo.configuration_gpt_neo.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt_neo.configuration_gpt_neo.self.attention_layers->self.expand_attention_types_params(attention_types)
transformers.GPTNeoConfig(self,vocab_size=50257,max_position_embeddings=2048,hidden_size=2048,num_layers=24,attention_types=[[['global','local'],12]],num_heads=16,intermediate_size=None,window_size=256,activation_function='gelu_new',resid_dropout=0.0,embed_dropout=0.0,attention_dropout=0.0,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,gradient_checkpointing=False,use_cache=True,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.GPTNeoConfig.expand_attention_types_params(attention_types)
transformers.GPTNeoConfig.num_attention_heads(self)
transformers.GPTNeoConfig.num_hidden_layers(self)
transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig(self,vocab_size=50257,max_position_embeddings=2048,hidden_size=2048,num_layers=24,attention_types=[[['global','local'],12]],num_heads=16,intermediate_size=None,window_size=256,activation_function='gelu_new',resid_dropout=0.0,embed_dropout=0.0,attention_dropout=0.0,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,gradient_checkpointing=False,use_cache=True,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig.__init__(self,vocab_size=50257,max_position_embeddings=2048,hidden_size=2048,num_layers=24,attention_types=[[['global','local'],12]],num_heads=16,intermediate_size=None,window_size=256,activation_function='gelu_new',resid_dropout=0.0,embed_dropout=0.0,attention_dropout=0.0,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,gradient_checkpointing=False,use_cache=True,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig.expand_attention_types_params(attention_types)
transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig.num_attention_heads(self)
transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt_neo/modeling_gpt_neo.py----------------------------------------
A:transformers.models.gpt_neo.modeling_gpt_neo.logger->utils.logging.get_logger(__name__)
A:transformers.models.gpt_neo.modeling_gpt_neo.tf_path->os.path.abspath(gpt_neo_checkpoint_path)
A:transformers.models.gpt_neo.modeling_gpt_neo.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.models.gpt_neo.modeling_gpt_neo.array->array.transpose().transpose()
A:transformers.models.gpt_neo.modeling_gpt_neo.name->name.split('/').split('/')
A:transformers.models.gpt_neo.modeling_gpt_neo.scope_names->re.split('(\\d+)', m_name)
A:transformers.models.gpt_neo.modeling_gpt_neo.pointer->getattr(pointer, scope_names[0])
A:transformers.models.gpt_neo.modeling_gpt_neo.num->int(scope_names[1])
A:transformers.models.gpt_neo.modeling_gpt_neo.pointer.data->torch.from_numpy(array)
A:transformers.models.gpt_neo.modeling_gpt_neo.lin->torch.nn.Linear(embs.size()[1], embs.size()[0], bias=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.padded_tensor->padded_tensor.transpose(-2, -1).transpose(-2, -1)
A:transformers.models.gpt_neo.modeling_gpt_neo.(block_length, num_blocks)->self._get_block_length_and_num_blocks(full_seq_length, self.window_size)
A:transformers.models.gpt_neo.modeling_gpt_neo.indices->torch.arange(seq_length, dtype=torch.long, device=device).repeat(batch_size, 1)
A:transformers.models.gpt_neo.modeling_gpt_neo.query_indices->GPTNeoAttentionMixin._split_seq_length_dim_to(indices, num_blocks, block_length)
A:transformers.models.gpt_neo.modeling_gpt_neo.key_indices->GPTNeoAttentionMixin._look_back(indices, block_length, window_size, is_key_value=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.causal_mask->self.bias[:, :, key_length - query_length:key_length, :key_length].bool()
A:transformers.models.gpt_neo.modeling_gpt_neo.attention_mask->kwargs.get('attention_mask', None)
A:transformers.models.gpt_neo.modeling_gpt_neo.visible->torch.gt(relative_position, -window_size)
A:transformers.models.gpt_neo.modeling_gpt_neo.tensor->tensor.permute(0, 2, 1, 3).contiguous().permute(0, 2, 1, 3).contiguous()
A:transformers.models.gpt_neo.modeling_gpt_neo.query->self._split_heads(query, self.num_heads, self.head_dim)
A:transformers.models.gpt_neo.modeling_gpt_neo.key->self._split_heads(key, self.num_heads, self.head_dim)
A:transformers.models.gpt_neo.modeling_gpt_neo.attn_weights->attn_dropout(attn_weights)
A:transformers.models.gpt_neo.modeling_gpt_neo.attn_output->self.resid_dropout(attn_output)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.attn_dropout->torch.nn.Dropout(config.attention_dropout)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.resid_dropout->torch.nn.Dropout(config.resid_dropout)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.k_proj->torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.v_proj->torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.q_proj->torch.nn.Linear(self.embed_dim, self.embed_dim, bias=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.out_proj->torch.nn.Linear(self.embed_dim, self.embed_dim, bias=True)
A:transformers.models.gpt_neo.modeling_gpt_neo.value->self._split_heads(value, self.num_heads, self.head_dim)
A:transformers.models.gpt_neo.modeling_gpt_neo.(attn_output, attn_weights)->self._attn(query, key, value, causal_mask=attention_mask, masked_bias=self.masked_bias, attn_dropout=self.attn_dropout, head_mask=head_mask)
A:transformers.models.gpt_neo.modeling_gpt_neo.key_value_hidden_states->torch.cat([past, hidden_states], dim=1)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.attention->GPTNeoLocalSelfAttention(config)
A:transformers.models.gpt_neo.modeling_gpt_neo.outputs->block(hidden_states, layer_past=layer_past, attention_mask=attn_mask, head_mask=head_mask[i], use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.gpt_neo.modeling_gpt_neo.past->torch.cat([layer_past[0], hidden_states], dim=1)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.c_fc->torch.nn.Linear(embed_dim, intermediate_size)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.c_proj->torch.nn.Linear(intermediate_size, embed_dim)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.dropout->torch.nn.Dropout(config.resid_dropout)
A:transformers.models.gpt_neo.modeling_gpt_neo.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.ln_1->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.attn->GPTNeoAttention(config, layer_id)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.ln_2->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.mlp->GPTNeoMLP(inner_dim, config)
A:transformers.models.gpt_neo.modeling_gpt_neo.attn_outputs->self.attn(hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.models.gpt_neo.modeling_gpt_neo.feed_forward_hidden_states->self.mlp(hidden_states)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.wte->torch.nn.Embedding(config.vocab_size, self.embed_dim)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.wpe->torch.nn.Embedding(config.max_position_embeddings, self.embed_dim)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.drop->torch.nn.Dropout(config.embed_dropout)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.h->torch.nn.ModuleList([GPTNeoBlock(config, layer_id=i) for i in range(config.num_layers)])
A:transformers.models.gpt_neo.modeling_gpt_neo.self.ln_f->torch.nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)
A:transformers.models.gpt_neo.modeling_gpt_neo.input_shape->input_ids[:, -1].unsqueeze(-1).size()
A:transformers.models.gpt_neo.modeling_gpt_neo.input_ids->input_ids[:, -1].unsqueeze(-1)
A:transformers.models.gpt_neo.modeling_gpt_neo.token_type_ids->token_type_ids[:, -1].unsqueeze(-1)
A:transformers.models.gpt_neo.modeling_gpt_neo.position_ids->position_ids[:, -1].unsqueeze(-1)
A:transformers.models.gpt_neo.modeling_gpt_neo.past_key_values->tuple([None] * len(self.h))
A:transformers.models.gpt_neo.modeling_gpt_neo.past_length->past_key_values[0][0].size(-2)
A:transformers.models.gpt_neo.modeling_gpt_neo.global_attention_mask->global_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.gpt_neo.modeling_gpt_neo.local_attention_mask->GPTNeoAttentionMixin.create_local_attention_mask(batch_size, full_seq_length, self.config.window_size, device, attention_mask)
A:transformers.models.gpt_neo.modeling_gpt_neo.head_mask->self.get_head_mask(head_mask, self.config.num_layers)
A:transformers.models.gpt_neo.modeling_gpt_neo.inputs_embeds->self.wte(input_ids)
A:transformers.models.gpt_neo.modeling_gpt_neo.position_embeds->self.wpe(position_ids)
A:transformers.models.gpt_neo.modeling_gpt_neo.token_type_embeds->self.wte(token_type_ids)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.transformer->GPTNeoModel(config)
A:transformers.models.gpt_neo.modeling_gpt_neo.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.transformer_outputs->self.transformer(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.gpt_neo.modeling_gpt_neo.lm_logits->lm_logits.to(hidden_states.dtype).to(hidden_states.dtype)
A:transformers.models.gpt_neo.modeling_gpt_neo.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.models.gpt_neo.modeling_gpt_neo.shift_labels->labels[..., 1:].contiguous()
A:transformers.models.gpt_neo.modeling_gpt_neo.loss_fct->CrossEntropyLoss()
A:transformers.models.gpt_neo.modeling_gpt_neo.loss->loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.gpt_neo.modeling_gpt_neo.self.score->torch.nn.Linear(config.hidden_size, self.num_labels, bias=False)
A:transformers.models.gpt_neo.modeling_gpt_neo.logits->self.score(hidden_states)
transformers.GPTNeoForCausalLM(self,config)
transformers.GPTNeoForCausalLM._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.GPTNeoForCausalLM.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.GPTNeoForCausalLM.get_output_embeddings(self)
transformers.GPTNeoForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.GPTNeoForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.GPTNeoForSequenceClassification(self,config)
transformers.GPTNeoForSequenceClassification.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.GPTNeoModel(self,config)
transformers.GPTNeoModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.GPTNeoModel.get_input_embeddings(self)
transformers.GPTNeoModel.set_input_embeddings(self,new_embeddings)
transformers.GPTNeoPreTrainedModel(self,*inputs,**kwargs)
transformers.GPTNeoPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_gpt_neo(model,config,gpt_neo_checkpoint_path)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttention(self,config,layer_id=0)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttention.__init__(self,config,layer_id=0)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttention.forward(self,hidden_states,layer_past=None,attention_mask=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin._attn(self,query,key,value,causal_mask,masked_bias,attn_dropout,attention_mask=None,head_mask=None)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin._get_block_length_and_num_blocks(seq_length,window_size)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin._look_back(tensor,block_length,window_size,pad_value=0,is_key_value=True)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin._merge_heads(self,tensor,num_heads,attn_head_size)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin._split_heads(self,tensor,num_heads,attn_head_size)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin._split_seq_length_dim_to(tensors,dim_factor_1,dim_factor_2)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoAttentionMixin.create_local_attention_mask(batch_size,seq_length,window_size,device,attention_mask=None)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoBlock(self,config,layer_id)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoBlock.__init__(self,config,layer_id)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoBlock.forward(self,hidden_states,layer_past=None,attention_mask=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM.__init__(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM._reorder_cache(past:Tuple[Tuple[torch.Tensor]],beam_idx:torch.Tensor)->Tuple[Tuple[torch.Tensor]]
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM.get_output_embeddings(self)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForSequenceClassification(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForSequenceClassification.__init__(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForSequenceClassification.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoLocalSelfAttention(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoLocalSelfAttention.__init__(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoLocalSelfAttention.forward(self,hidden_states,attention_mask,layer_past=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoMLP(self,intermediate_size,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoMLP.__init__(self,intermediate_size,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoMLP.forward(self,hidden_states)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoModel(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoModel.__init__(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoModel.get_input_embeddings(self)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoModel.set_input_embeddings(self,new_embeddings)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoPreTrainedModel(self,*inputs,**kwargs)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoPreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoPreTrainedModel._init_weights(self,module)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoSelfAttention(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoSelfAttention.__init__(self,config)
transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoSelfAttention.forward(self,hidden_states,attention_mask=None,layer_past=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.models.gpt_neo.modeling_gpt_neo.load_tf_weights_in_gpt_neo(model,config,gpt_neo_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/gpt_neo/__init__.py----------------------------------------
A:transformers.models.gpt_neo.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/tokenization_longformer_fast.py----------------------------------------
A:transformers.models.longformer.tokenization_longformer_fast.logger->utils.logging.get_logger(__name__)
transformers.LongformerTokenizerFast(RobertaTokenizerFast)
transformers.models.longformer.tokenization_longformer_fast.LongformerTokenizerFast(RobertaTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py----------------------------------------
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.self.qa_outputs->torch.nn.Linear(self.model.config.hidden_size, self.num_labels)
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.longformer->transformers.LongformerModel.from_pretrained(longformer_model)
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.lightning_model->LightningModel(longformer)
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.ckpt->torch.load(longformer_question_answering_ckpt_path, map_location=torch.device('cpu'))
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.longformer_for_qa->transformers.LongformerForQuestionAnswering.from_pretrained(longformer_model)
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.LightningModel(self,model)
transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.LightningModel.__init__(self,model)
transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.LightningModel.forward(self)
transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch.convert_longformer_qa_checkpoint_to_pytorch(longformer_model:str,longformer_question_answering_ckpt_path:str,pytorch_dump_folder_path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/modeling_tf_longformer.py----------------------------------------
A:transformers.models.longformer.modeling_tf_longformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.longformer.modeling_tf_longformer.attention_mask->tensorflow.convert_to_tensor([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.models.longformer.modeling_tf_longformer.question_end_index->tensorflow.tile(question_end_index + 1, (1, input_ids_shape[1]))
A:transformers.models.longformer.modeling_tf_longformer.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), activation='tanh', name='dense')
A:transformers.models.longformer.modeling_tf_longformer.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.longformer.modeling_tf_longformer.self.act->get_tf_activation('gelu')
A:transformers.models.longformer.modeling_tf_longformer.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.longformer.modeling_tf_longformer.hidden_states->self.dropout(hidden_states, training=training)
A:transformers.models.longformer.modeling_tf_longformer.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.longformer.modeling_tf_longformer.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.longformer.modeling_tf_longformer.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.models.longformer.modeling_tf_longformer.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.longformer.modeling_tf_longformer.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.longformer.modeling_tf_longformer.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.longformer.modeling_tf_longformer.mask->tensorflow.cast(mask, dtype=attn_probs_from_global_key_trans.dtype)
A:transformers.models.longformer.modeling_tf_longformer.inputs_embeds->tensorflow.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda : inputs_embeds)
A:transformers.models.longformer.modeling_tf_longformer.token_type_ids->tensorflow.pad(token_type_ids, paddings, constant_values=0)
A:transformers.models.longformer.modeling_tf_longformer.position_ids->tensorflow.pad(position_ids, paddings, constant_values=pad_token_id)
A:transformers.models.longformer.modeling_tf_longformer.position_embeds->tensorflow.gather(params=self.position_embeddings, indices=position_ids)
A:transformers.models.longformer.modeling_tf_longformer.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.longformer.modeling_tf_longformer.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.longformer.modeling_tf_longformer.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.longformer.modeling_tf_longformer.pooled_output->self.dropout(pooled_output)
A:transformers.models.longformer.modeling_tf_longformer.self.head_dim->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.longformer.modeling_tf_longformer.self.query->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.longformer.modeling_tf_longformer.self.key->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.longformer.modeling_tf_longformer.self.value->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.longformer.modeling_tf_longformer.self.query_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='query_global')
A:transformers.models.longformer.modeling_tf_longformer.self.key_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='key_global')
A:transformers.models.longformer.modeling_tf_longformer.self.value_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='value_global')
A:transformers.models.longformer.modeling_tf_longformer.self.global_dropout->tensorflow.keras.layers.Dropout(config.attention_probs_dropout_prob)
A:transformers.models.longformer.modeling_tf_longformer.query_vectors->tensorflow.reshape(query_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.models.longformer.modeling_tf_longformer.key_vectors->tensorflow.reshape(key_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.models.longformer.modeling_tf_longformer.value_vectors->tensorflow.reshape(value_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.models.longformer.modeling_tf_longformer.(batch_size, seq_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.longformer.modeling_tf_longformer.attn_scores->tensorflow.concat((attn_probs_from_global_key, attn_scores), axis=-1)
A:transformers.models.longformer.modeling_tf_longformer.diagonal_mask->self._sliding_chunks_query_key_matmul(tf.ones(shape_list(attention_mask)), attention_mask, self.one_sided_attn_window_size)
A:transformers.models.longformer.modeling_tf_longformer.(max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero)->self._get_global_attn_indices(is_index_global_attn)
A:transformers.models.longformer.modeling_tf_longformer.attn_probs->tensorflow.where(masked_global_attn_index, tf.zeros(shape_list(masked_global_attn_index), dtype=attn_probs.dtype), attn_probs)
A:transformers.models.longformer.modeling_tf_longformer.masked_index->tensorflow.cond(is_global_attn, lambda : tf.tile(is_index_masked[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + max_num_global_attn_indices + 1)), lambda : tf.tile(is_index_masked[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + 1)))
A:transformers.models.longformer.modeling_tf_longformer.attn_output->tensorflow.tensor_scatter_nd_update(attn_output, is_index_global_attn_nonzero, nonzero_global_attn_output)
A:transformers.models.longformer.modeling_tf_longformer.(attn_output, global_attn_probs)->tensorflow.cond(is_global_attn, lambda : self._compute_global_attn_output_from_hidden(attn_output=attn_output, hidden_states=hidden_states, max_num_global_attn_indices=max_num_global_attn_indices, layer_head_mask=layer_head_mask, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero, is_index_masked=is_index_masked, training=training), lambda : (attn_output, tf.zeros((batch_size, self.num_heads, max_num_global_attn_indices, seq_len))))
A:transformers.models.longformer.modeling_tf_longformer.masked_global_attn_index->tensorflow.cond(is_global_attn, lambda : tf.tile(is_index_global_attn[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + max_num_global_attn_indices + 1)), lambda : tf.tile(is_index_global_attn[:, :, None, None], (1, 1, self.num_heads, self.one_sided_attn_window_size * 2 + 1)))
A:transformers.models.longformer.modeling_tf_longformer.(batch_size, seq_len, num_heads, head_dim)->shape_list(value)
A:transformers.models.longformer.modeling_tf_longformer.query->tensorflow.reshape(tf.transpose(query, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.models.longformer.modeling_tf_longformer.key->tensorflow.reshape(tf.transpose(key, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.models.longformer.modeling_tf_longformer.chunked_query->tensorflow.cast(chunked_query, dtype=chunked_key.dtype)
A:transformers.models.longformer.modeling_tf_longformer.chunked_key->self._chunk(key, window_overlap)
A:transformers.models.longformer.modeling_tf_longformer.chunked_attention_scores->tensorflow.einsum('bcxd,bcyd->bcxy', chunked_query, chunked_key)
A:transformers.models.longformer.modeling_tf_longformer.paddings->tensorflow.convert_to_tensor([[0, 0], [0, padding_len]])
A:transformers.models.longformer.modeling_tf_longformer.diagonal_chunked_attention_scores->self._pad_and_transpose_last_two_dims(chunked_attention_scores, paddings)
A:transformers.models.longformer.modeling_tf_longformer.diagonal_attn_scores_up_triang->tensorflow.concat([diagonal_chunked_attention_scores[:, :, :window_overlap, :window_overlap + 1], diagonal_chunked_attention_scores[:, -1:, window_overlap:, :window_overlap + 1]], axis=1)
A:transformers.models.longformer.modeling_tf_longformer.diagonal_attn_scores_low_triang->tensorflow.where(first_chunk_mask, diagonal_attn_scores_first_chunk, diagonal_attn_scores_low_triang)
A:transformers.models.longformer.modeling_tf_longformer.diagonal_attn_scores_first_chunk->tensorflow.concat([tf.roll(diagonal_chunked_attention_scores, shift=[1, window_overlap], axis=[2, 3])[:, :, :window_overlap, :window_overlap], tf.zeros((batch_size * num_heads, 1, window_overlap, window_overlap), dtype=diagonal_chunked_attention_scores.dtype)], axis=1)
A:transformers.models.longformer.modeling_tf_longformer.diagonal_attention_scores->self._mask_invalid_locations(diagonal_attention_scores, window_overlap)
A:transformers.models.longformer.modeling_tf_longformer.mask_2d_upper->tensorflow.reverse(tf.linalg.band_part(tf.ones(shape=(window_overlap, window_overlap + 1)), -1, 0), axis=[0])
A:transformers.models.longformer.modeling_tf_longformer.padding->tensorflow.convert_to_tensor([[0, shape_list(input_tensor)[1] - window_overlap], [0, shape_list(input_tensor)[3] - window_overlap - 1]])
A:transformers.models.longformer.modeling_tf_longformer.mask_2d->tensorflow.pad(mask_2d_upper, padding)
A:transformers.models.longformer.modeling_tf_longformer.mask_4d->tensorflow.tile(mask_2d[None, :, None, :], (shape_list(input_tensor)[0], 1, 1, 1))
A:transformers.models.longformer.modeling_tf_longformer.input_tensor->tensorflow.where(tf.math.greater(mask_4d, 0), inf_tensor, input_tensor)
A:transformers.models.longformer.modeling_tf_longformer.chunked_attn_probs->self._pad_and_diagonalize(chunked_attn_probs)
A:transformers.models.longformer.modeling_tf_longformer.value->tensorflow.reshape(tf.transpose(value, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.models.longformer.modeling_tf_longformer.padded_value->tensorflow.pad(value, paddings, constant_values=-1)
A:transformers.models.longformer.modeling_tf_longformer.chunked_value->tensorflow.reshape(chunked_value, (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim))
A:transformers.models.longformer.modeling_tf_longformer.context->tensorflow.transpose(tf.reshape(context, (batch_size, num_heads, seq_len, head_dim)), (0, 2, 1, 3))
A:transformers.models.longformer.modeling_tf_longformer.hidden_states_padded->tensorflow.reshape(hidden_states_padded, (batch_size, chunk_size, hidden_dim, seq_length))
A:transformers.models.longformer.modeling_tf_longformer.(batch_size, chunk_size, seq_length, hidden_dim)->shape_list(hidden_states_padded)
A:transformers.models.longformer.modeling_tf_longformer.(total_num_heads, num_chunks, window_overlap, hidden_dim)->shape_list(chunked_hidden_states)
A:transformers.models.longformer.modeling_tf_longformer.chunked_hidden_states->tensorflow.reshape(chunked_hidden_states, (batch_size, num_output_chunks, 2 * window_overlap, hidden_dim))
A:transformers.models.longformer.modeling_tf_longformer.(batch_size, seq_length, hidden_dim)->shape_list(hidden_states)
A:transformers.models.longformer.modeling_tf_longformer.num_global_attn_indices->tensorflow.cast(num_global_attn_indices, dtype=tf.constant(1).dtype)
A:transformers.models.longformer.modeling_tf_longformer.max_num_global_attn_indices->tensorflow.reduce_max(num_global_attn_indices)
A:transformers.models.longformer.modeling_tf_longformer.is_index_global_attn_nonzero->tensorflow.where(is_index_global_attn)
A:transformers.models.longformer.modeling_tf_longformer.is_local_index_global_attn_nonzero->tensorflow.where(is_local_index_global_attn)
A:transformers.models.longformer.modeling_tf_longformer.is_local_index_no_global_attn_nonzero->tensorflow.where(tf.math.logical_not(is_local_index_global_attn))
A:transformers.models.longformer.modeling_tf_longformer.global_key_vectors->self.reshape_and_transpose(global_key_vectors, batch_size)
A:transformers.models.longformer.modeling_tf_longformer.key_vectors_only_global->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_key_vectors, shape=(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim))
A:transformers.models.longformer.modeling_tf_longformer.attn_probs_from_global_key->tensorflow.transpose(attn_probs_from_global_key_trans, (0, 2, 3, 1))
A:transformers.models.longformer.modeling_tf_longformer.attn_probs_from_global_key_trans->tensorflow.tensor_scatter_nd_update(attn_probs_from_global_key_trans, is_local_index_no_global_attn_nonzero, mask)
A:transformers.models.longformer.modeling_tf_longformer.global_value_vectors->self.reshape_and_transpose(global_value_vectors, batch_size)
A:transformers.models.longformer.modeling_tf_longformer.value_vectors_only_global->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_value_vectors, shape=(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim))
A:transformers.models.longformer.modeling_tf_longformer.attn_output_only_global->tensorflow.einsum('blhs,bshd->blhd', attn_probs_only_global, value_vectors_only_global)
A:transformers.models.longformer.modeling_tf_longformer.attn_output_without_global->self._sliding_chunks_matmul_attn_probs_value(attn_probs_without_global, value_vectors, self.one_sided_attn_window_size)
A:transformers.models.longformer.modeling_tf_longformer.global_attn_hidden_states->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_attn_hidden_states, shape=(batch_size, max_num_global_attn_indices, self.embed_dim))
A:transformers.models.longformer.modeling_tf_longformer.global_query_vectors_only_global->self.reshape_and_transpose(global_query_vectors_only_global, batch_size)
A:transformers.models.longformer.modeling_tf_longformer.global_attn_scores->tensorflow.reshape(global_attn_scores, (batch_size * self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.models.longformer.modeling_tf_longformer.global_attn_scores_trans->tensorflow.tensor_scatter_nd_update(global_attn_scores_trans, is_local_index_no_global_attn_nonzero, global_attn_mask)
A:transformers.models.longformer.modeling_tf_longformer.global_attn_mask->tensorflow.cast(global_attn_mask, dtype=global_attn_scores_trans.dtype)
A:transformers.models.longformer.modeling_tf_longformer.attn_mask->tensorflow.tile(is_index_masked[:, None, None, :], (1, shape_list(global_attn_scores)[1], 1, 1))
A:transformers.models.longformer.modeling_tf_longformer.global_attn_probs_float->tensorflow.reshape(global_attn_probs_float, (batch_size * self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.models.longformer.modeling_tf_longformer.global_attn_probs->tensorflow.reshape(global_attn_probs, (batch_size, self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.models.longformer.modeling_tf_longformer.global_attn_output->tensorflow.reshape(global_attn_output, (batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim))
A:transformers.models.longformer.modeling_tf_longformer.nonzero_global_attn_output->tensorflow.reshape(nonzero_global_attn_output, (shape_list(is_local_index_global_attn_nonzero)[0], -1))
A:transformers.models.longformer.modeling_tf_longformer.self.self_attention->TFLongformerSelfAttention(config, layer_id, name='self')
A:transformers.models.longformer.modeling_tf_longformer.self.dense_output->TFLongformerSelfOutput(config, name='output')
A:transformers.models.longformer.modeling_tf_longformer.self_outputs->self.self_attention([hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn], training=training)
A:transformers.models.longformer.modeling_tf_longformer.attention_output->self.dense_output(self_outputs[0], hidden_states, training=training)
A:transformers.models.longformer.modeling_tf_longformer.self.attention->TFLongformerAttention(config, layer_id, name='attention')
A:transformers.models.longformer.modeling_tf_longformer.self.intermediate->TFLongformerIntermediate(config, name='intermediate')
A:transformers.models.longformer.modeling_tf_longformer.self.longformer_output->TFLongformerOutput(config, name='output')
A:transformers.models.longformer.modeling_tf_longformer.attention_outputs->self.attention([hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn], training=training)
A:transformers.models.longformer.modeling_tf_longformer.intermediate_output->self.intermediate(attention_output)
A:transformers.models.longformer.modeling_tf_longformer.layer_output->self.longformer_output(intermediate_output, attention_output, training=training)
A:transformers.models.longformer.modeling_tf_longformer.layer_outputs->layer_module([hidden_states, attention_mask, head_mask[idx] if head_mask is not None else None, is_index_masked, is_index_global_attn, is_global_attn], training=training)
A:transformers.models.longformer.modeling_tf_longformer.self.embeddings->TFLongformerEmbeddings(config, name='embeddings')
A:transformers.models.longformer.modeling_tf_longformer.self.encoder->TFLongformerEncoder(config, name='encoder')
A:transformers.models.longformer.modeling_tf_longformer.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, global_attention_mask=global_attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.longformer.modeling_tf_longformer.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.longformer.modeling_tf_longformer.inputs['attention_mask']->self._merge_to_attention_mask(inputs['attention_mask'], inputs['global_attention_mask'])
A:transformers.models.longformer.modeling_tf_longformer.inputs['token_type_ids']->tensorflow.fill(input_shape, 0)
A:transformers.models.longformer.modeling_tf_longformer.(padding_len, inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['position_ids'], inputs['inputs_embeds'])->self._pad_to_window_size(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], inputs_embeds=inputs['inputs_embeds'], pad_token_id=self.pad_token_id)
A:transformers.models.longformer.modeling_tf_longformer.is_index_masked->tensorflow.math.less(inputs['attention_mask'], 1)
A:transformers.models.longformer.modeling_tf_longformer.is_index_global_attn->tensorflow.math.greater(inputs['attention_mask'], 1)
A:transformers.models.longformer.modeling_tf_longformer.is_global_attn->tensorflow.math.reduce_any(is_index_global_attn)
A:transformers.models.longformer.modeling_tf_longformer.attention_mask_shape->shape_list(inputs['attention_mask'])
A:transformers.models.longformer.modeling_tf_longformer.extended_attention_mask->tensorflow.reshape(inputs['attention_mask'], (attention_mask_shape[0], attention_mask_shape[1], 1, 1))
A:transformers.models.longformer.modeling_tf_longformer.embedding_output->self.embeddings(inputs['input_ids'], inputs['position_ids'], inputs['token_type_ids'], inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.longformer.modeling_tf_longformer.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, padding_len=padding_len, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.longformer.modeling_tf_longformer.input_ids->tensorflow.convert_to_tensor(MULTIPLE_CHOICE_DUMMY_INPUTS)
A:transformers.models.longformer.modeling_tf_longformer.input_ids_padding->tensorflow.fill((batch_size, padding_len), self.pad_token_id)
A:transformers.models.longformer.modeling_tf_longformer.inputs_embeds_padding->self.embeddings(input_ids_padding)
A:transformers.models.longformer.modeling_tf_longformer.global_attention_mask->tensorflow.convert_to_tensor([[[0, 0, 0, 1], [0, 0, 0, 1]]] * 2)
A:transformers.models.longformer.modeling_tf_longformer.output->self.call(inputs)
A:transformers.models.longformer.modeling_tf_longformer.self.longformer->TFLongformerMainLayer(config=config, add_pooling_layer=False, name='longformer')
A:transformers.models.longformer.modeling_tf_longformer.outputs->self.longformer(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], head_mask=inputs['head_mask'], global_attention_mask=inputs['global_attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.longformer.modeling_tf_longformer.self.lm_head->TFLongformerLMHead(config, self.longformer.embeddings, name='lm_head')
A:transformers.models.longformer.modeling_tf_longformer.prediction_scores->self.lm_head(sequence_output, training=inputs['training'])
A:transformers.models.longformer.modeling_tf_longformer.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.models.longformer.modeling_tf_longformer.inputs['global_attention_mask']->tensorflow.tensor_scatter_nd_update(inputs['global_attention_mask'], indices, updates)
A:transformers.models.longformer.modeling_tf_longformer.sep_token_indices->tensorflow.cast(sep_token_indices, dtype=inputs['input_ids'].dtype)
A:transformers.models.longformer.modeling_tf_longformer.logits->self.classifier(sequence_output)
A:transformers.models.longformer.modeling_tf_longformer.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.models.longformer.modeling_tf_longformer.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.models.longformer.modeling_tf_longformer.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.models.longformer.modeling_tf_longformer.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.models.longformer.modeling_tf_longformer.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.models.longformer.modeling_tf_longformer.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.longformer.modeling_tf_longformer.updates->tensorflow.ones(shape_list(inputs['input_ids'])[0], dtype=tf.int32)
A:transformers.models.longformer.modeling_tf_longformer.indices->tensorflow.pad(tensor=tf.expand_dims(tf.range(shape_list(inputs['input_ids'])[0]), axis=1), paddings=[[0, 0], [0, 1]], constant_values=0)
A:transformers.models.longformer.modeling_tf_longformer.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.models.longformer.modeling_tf_longformer.sequence_output->self.dropout(sequence_output)
transformers.TFLongformerForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFLongformerForMaskedLM.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFLongformerForMaskedLM.get_lm_head(self)
transformers.TFLongformerForMaskedLM.get_prefix_bias_name(self)
transformers.TFLongformerForMaskedLM.serving_output(self,output)
transformers.TFLongformerForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFLongformerForMultipleChoice.call(self,input_ids=None,attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,global_attention_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFLongformerForMultipleChoice.dummy_inputs(self)
transformers.TFLongformerForMultipleChoice.serving(self,inputs)
transformers.TFLongformerForMultipleChoice.serving_output(self,output)
transformers.TFLongformerForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFLongformerForQuestionAnswering.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.TFLongformerForQuestionAnswering.serving_output(self,output)
transformers.TFLongformerForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFLongformerForSequenceClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,global_attention_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFLongformerForSequenceClassification.serving_output(self,output)
transformers.TFLongformerForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFLongformerForTokenClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,global_attention_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFLongformerForTokenClassification.serving_output(self,output)
transformers.TFLongformerModel(self,config,*inputs,**kwargs)
transformers.TFLongformerModel.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFLongformerModel.serving_output(self,output)
transformers.TFLongformerPreTrainedModel(TFPreTrainedModel)
transformers.TFLongformerPreTrainedModel.dummy_inputs(self)
transformers.TFLongformerPreTrainedModel.serving(self,inputs)
transformers.TFLongformerSelfAttention(self,config,layer_id,**kwargs)
transformers.TFLongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.TFLongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.TFLongformerSelfAttention._compute_global_attn_output_from_hidden(self,attn_output,hidden_states,max_num_global_attn_indices,layer_head_mask,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked,training)
transformers.TFLongformerSelfAttention._concat_with_global_key_attn_probs(self,attn_scores,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.TFLongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.TFLongformerSelfAttention._mask_invalid_locations(input_tensor,window_overlap)
transformers.TFLongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.TFLongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,paddings)
transformers.TFLongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs,value,window_overlap)
transformers.TFLongformerSelfAttention._sliding_chunks_query_key_matmul(self,query,key,window_overlap)
transformers.TFLongformerSelfAttention.call(self,inputs,training=False)
transformers.TFLongformerSelfAttention.reshape_and_transpose(self,vector,batch_size)
transformers.models.longformer.modeling_tf_longformer.TFLongformerAttention(self,config,layer_id=0,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerAttention.__init__(self,config,layer_id=0,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerAttention.call(self,inputs,training=False)
transformers.models.longformer.modeling_tf_longformer.TFLongformerAttention.prune_heads(self,heads)
transformers.models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutput(ModelOutput)
transformers.models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling(ModelOutput)
transformers.models.longformer.modeling_tf_longformer.TFLongformerClassificationHead(self,config,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerClassificationHead.__init__(self,config,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerClassificationHead.call(self,hidden_states,training=False)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings(self,config,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings.__init__(self,config,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,training=False)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings.create_position_ids_from_input_ids(self,input_ids)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder(self,config,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder.__init__(self,config,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder.call(self,hidden_states,attention_mask=None,head_mask=None,padding_len=0,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMaskedLM(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMaskedLM.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMaskedLM.get_lm_head(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMaskedLM.get_prefix_bias_name(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMaskedLM.serving_output(self,output)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMultipleChoice(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMultipleChoice.call(self,input_ids=None,attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,global_attention_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMultipleChoice.dummy_inputs(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMultipleChoice.serving(self,inputs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForMultipleChoice.serving_output(self,output)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForQuestionAnswering.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForQuestionAnswering.serving_output(self,output)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForSequenceClassification(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForSequenceClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,global_attention_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForSequenceClassification.serving_output(self,output)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForTokenClassification(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForTokenClassification.call(self,input_ids=None,attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,global_attention_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerForTokenClassification.serving_output(self,output)
transformers.models.longformer.modeling_tf_longformer.TFLongformerIntermediate(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerIntermediate.__init__(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead(self,config,input_embeddings,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.build(self,input_shape)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.call(self,hidden_states)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.get_bias(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.get_output_embeddings(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.set_bias(self,value)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLMHead.set_output_embeddings(self,value)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLayer(self,config,layer_id=0,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLayer.__init__(self,config,layer_id=0,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerLayer.call(self,inputs,training=False)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer(self,config,add_pooling_layer=True,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer.__init__(self,config,add_pooling_layer=True,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer._merge_to_attention_mask(attention_mask:tf.Tensor,global_attention_mask:tf.Tensor)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer._pad_to_window_size(self,input_ids,attention_mask,token_type_ids,position_ids,inputs_embeds,pad_token_id)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer._prune_heads(self,heads_to_prune)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer.get_input_embeddings(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer.set_input_embeddings(self,value)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput(ModelOutput)
transformers.models.longformer.modeling_tf_longformer.TFLongformerModel(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerModel.__init__(self,config,*inputs,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerModel.call(self,input_ids=None,attention_mask=None,head_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerModel.serving_output(self,output)
transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput(ModelOutput)
transformers.models.longformer.modeling_tf_longformer.TFLongformerOutput(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerOutput.__init__(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.longformer.modeling_tf_longformer.TFLongformerPooler(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerPooler.__init__(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerPooler.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.longformer.modeling_tf_longformer.TFLongformerPreTrainedModel(TFPreTrainedModel)
transformers.models.longformer.modeling_tf_longformer.TFLongformerPreTrainedModel.dummy_inputs(self)
transformers.models.longformer.modeling_tf_longformer.TFLongformerPreTrainedModel.serving(self,inputs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput(ModelOutput)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention(self,config,layer_id,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention.__init__(self,config,layer_id,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._compute_global_attn_output_from_hidden(self,attn_output,hidden_states,max_num_global_attn_indices,layer_head_mask,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked,training)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._concat_with_global_key_attn_probs(self,attn_scores,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._mask_invalid_locations(input_tensor,window_overlap)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,paddings)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs,value,window_overlap)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention._sliding_chunks_query_key_matmul(self,query,key,window_overlap)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention.call(self,inputs,training=False)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfAttention.reshape_and_transpose(self,vector,batch_size)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfOutput(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfOutput.__init__(self,config:LongformerConfig,**kwargs)
transformers.models.longformer.modeling_tf_longformer.TFLongformerSelfOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput(ModelOutput)
transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput(ModelOutput)
transformers.models.longformer.modeling_tf_longformer._compute_global_attention_mask(input_ids_shape,sep_token_indices,before_sep_token=True)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/configuration_longformer.py----------------------------------------
A:transformers.models.longformer.configuration_longformer.logger->utils.logging.get_logger(__name__)
transformers.LongformerConfig(self,attention_window:Union[List[int],int]=512,sep_token_id:int=2,**kwargs)
transformers.models.longformer.configuration_longformer.LongformerConfig(self,attention_window:Union[List[int],int]=512,sep_token_id:int=2,**kwargs)
transformers.models.longformer.configuration_longformer.LongformerConfig.__init__(self,attention_window:Union[List[int],int]=512,sep_token_id:int=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/modeling_longformer.py----------------------------------------
A:transformers.models.longformer.modeling_longformer.logger->utils.logging.get_logger(__name__)
A:transformers.models.longformer.modeling_longformer.sep_token_indices->(input_ids == sep_token_id).nonzero()
A:transformers.models.longformer.modeling_longformer.question_end_index->question_end_index.unsqueeze(dim=1).unsqueeze(dim=1)
A:transformers.models.longformer.modeling_longformer.attention_mask->self._merge_to_attention_mask(attention_mask, global_attention_mask)
A:transformers.models.longformer.modeling_longformer.mask->torch.nn.functional.pad(input_ids, (0, padding_len), value=pad_token_id).ne(padding_idx).int()
A:transformers.models.longformer.modeling_longformer.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.longformer.modeling_longformer.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.models.longformer.modeling_longformer.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.longformer.modeling_longformer.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.longformer.modeling_longformer.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.longformer.modeling_longformer.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.longformer.modeling_longformer.position_ids->torch.nn.functional.pad(position_ids, (0, padding_len), value=pad_token_id)
A:transformers.models.longformer.modeling_longformer.input_shape->torch.nn.functional.pad(input_ids, (0, padding_len), value=pad_token_id).size()
A:transformers.models.longformer.modeling_longformer.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.longformer.modeling_longformer.inputs_embeds->torch.cat([inputs_embeds, inputs_embeds_padding], dim=-2)
A:transformers.models.longformer.modeling_longformer.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.longformer.modeling_longformer.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.longformer.modeling_longformer.embeddings->self.dropout(embeddings)
A:transformers.models.longformer.modeling_longformer.self.head_dim->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.longformer.modeling_longformer.self.query->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.self.key->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.self.value->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.self.query_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.self.key_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.self.value_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.hidden_states->self.dropout(hidden_states)
A:transformers.models.longformer.modeling_longformer.query_vectors->query_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.longformer.modeling_longformer.key_vectors->key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.longformer.modeling_longformer.value_vectors->value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.longformer.modeling_longformer.(seq_len, batch_size, embed_dim)->self.dropout(hidden_states).size()
A:transformers.models.longformer.modeling_longformer.attn_scores->torch.cat((global_key_attn_scores, attn_scores), dim=-1)
A:transformers.models.longformer.modeling_longformer.float_mask->remove_from_windowed_attention_mask.type_as(query_vectors).masked_fill(remove_from_windowed_attention_mask, -10000.0)
A:transformers.models.longformer.modeling_longformer.diagonal_mask->self._sliding_chunks_query_key_matmul(float_mask.new_ones(size=float_mask.size()), float_mask, self.one_sided_attn_window_size)
A:transformers.models.longformer.modeling_longformer.(max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero)->self._get_global_attn_indices(is_index_global_attn)
A:transformers.models.longformer.modeling_longformer.global_key_attn_scores->self._concat_with_global_key_attn_probs(query_vectors=query_vectors, key_vectors=key_vectors, max_num_global_attn_indices=max_num_global_attn_indices, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero)
A:transformers.models.longformer.modeling_longformer.attn_probs->torch.nn.functional.dropout(attn_probs, p=self.dropout, training=self.training)
A:transformers.models.longformer.modeling_longformer.attn_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.longformer.modeling_longformer.(global_attn_output, global_attn_probs)->self._compute_global_attn_output_from_hidden(hidden_states=hidden_states, max_num_global_attn_indices=max_num_global_attn_indices, layer_head_mask=layer_head_mask, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero, is_index_masked=is_index_masked)
A:transformers.models.longformer.modeling_longformer.attn_output[is_index_global_attn_nonzero[::-1]]->nonzero_global_attn_output.view(len(is_local_index_global_attn_nonzero[0]), -1)
A:transformers.models.longformer.modeling_longformer.hidden_states_padded->hidden_states_padded.view(*hidden_states_padded.size()[:-2], hidden_states_padded.size(-1), hidden_states_padded.size(-2)).view(*hidden_states_padded.size()[:-2], hidden_states_padded.size(-1), hidden_states_padded.size(-2))
A:transformers.models.longformer.modeling_longformer.(total_num_heads, num_chunks, window_overlap, hidden_dim)->chunked_hidden_states.view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim).size()
A:transformers.models.longformer.modeling_longformer.chunked_hidden_states->chunked_hidden_states.view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim).view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim)
A:transformers.models.longformer.modeling_longformer.chunk_size->list(hidden_states.size())
A:transformers.models.longformer.modeling_longformer.chunk_stride->list(hidden_states.stride())
A:transformers.models.longformer.modeling_longformer.beginning_mask_2d->input_tensor.new_ones(affected_seq_len, affected_seq_len + 1).tril().flip(dims=[0])
A:transformers.models.longformer.modeling_longformer.ending_mask->ending_mask.expand(ending_input.size()).expand(ending_input.size())
A:transformers.models.longformer.modeling_longformer.beginning_mask->beginning_mask.expand(beginning_input.size()).expand(beginning_input.size())
A:transformers.models.longformer.modeling_longformer.(batch_size, seq_len, num_heads, head_dim)->value.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).size()
A:transformers.models.longformer.modeling_longformer.query->self._chunk(query, window_overlap)
A:transformers.models.longformer.modeling_longformer.key->self._chunk(key, window_overlap)
A:transformers.models.longformer.modeling_longformer.diagonal_chunked_attention_scores->self._pad_and_transpose_last_two_dims(diagonal_chunked_attention_scores, padding=(0, 0, 0, 1))
A:transformers.models.longformer.modeling_longformer.diagonal_attention_scores->diagonal_attention_scores.view(batch_size, num_heads, seq_len, 2 * window_overlap + 1).transpose(2, 1).view(batch_size, num_heads, seq_len, 2 * window_overlap + 1).transpose(2, 1)
A:transformers.models.longformer.modeling_longformer.chunked_attn_probs->self._pad_and_diagonalize(chunked_attn_probs)
A:transformers.models.longformer.modeling_longformer.value->value.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)
A:transformers.models.longformer.modeling_longformer.padded_value->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1)
A:transformers.models.longformer.modeling_longformer.chunked_value_stride->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1).stride()
A:transformers.models.longformer.modeling_longformer.chunked_value->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1).as_strided(size=chunked_value_size, stride=chunked_value_stride)
A:transformers.models.longformer.modeling_longformer.context->torch.einsum('bcwd,bcdh->bcwh', (chunked_attn_probs, chunked_value))
A:transformers.models.longformer.modeling_longformer.num_global_attn_indices->is_index_global_attn.long().sum(dim=1)
A:transformers.models.longformer.modeling_longformer.max_num_global_attn_indices->is_index_global_attn.long().sum(dim=1).max()
A:transformers.models.longformer.modeling_longformer.is_index_global_attn_nonzero->is_index_global_attn.nonzero(as_tuple=True)
A:transformers.models.longformer.modeling_longformer.is_local_index_global_attn_nonzero->is_local_index_global_attn.nonzero(as_tuple=True)
A:transformers.models.longformer.modeling_longformer.is_local_index_no_global_attn_nonzero->(is_local_index_global_attn == 0).nonzero(as_tuple=True)
A:transformers.models.longformer.modeling_longformer.key_vectors_only_global->key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).new_zeros(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim)
A:transformers.models.longformer.modeling_longformer.attn_probs_from_global_key->torch.einsum('blhd,bshd->blhs', (query_vectors, key_vectors_only_global))
A:transformers.models.longformer.modeling_longformer.attn_probs_only_global->torch.nn.functional.dropout(attn_probs, p=self.dropout, training=self.training).narrow(-1, 0, max_num_global_attn_indices)
A:transformers.models.longformer.modeling_longformer.value_vectors_only_global->value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).new_zeros(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim)
A:transformers.models.longformer.modeling_longformer.attn_output_only_global->torch.matmul(attn_probs_only_global.transpose(1, 2), value_vectors_only_global.transpose(1, 2)).transpose(1, 2)
A:transformers.models.longformer.modeling_longformer.attn_probs_without_global->torch.nn.functional.dropout(attn_probs, p=self.dropout, training=self.training).narrow(-1, max_num_global_attn_indices, attn_probs.size(-1) - max_num_global_attn_indices).contiguous()
A:transformers.models.longformer.modeling_longformer.attn_output_without_global->self._sliding_chunks_matmul_attn_probs_value(attn_probs_without_global, value_vectors, self.one_sided_attn_window_size)
A:transformers.models.longformer.modeling_longformer.global_attn_hidden_states->self.dropout(hidden_states).new_zeros(max_num_global_attn_indices, batch_size, self.embed_dim)
A:transformers.models.longformer.modeling_longformer.global_query_vectors_only_global->global_query_vectors_only_global.contiguous().view(max_num_global_attn_indices, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(max_num_global_attn_indices, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.longformer.modeling_longformer.global_key_vectors->global_key_vectors.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.longformer.modeling_longformer.global_value_vectors->global_value_vectors.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.models.longformer.modeling_longformer.global_attn_scores->global_attn_scores.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.models.longformer.modeling_longformer.global_attn_probs_float->global_attn_probs_float.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.models.longformer.modeling_longformer.global_attn_probs->global_attn_probs.view(batch_size, self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size, self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.models.longformer.modeling_longformer.global_attn_output->global_attn_output.view(batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim).view(batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim)
A:transformers.models.longformer.modeling_longformer.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.longformer.modeling_longformer.self.self->LongformerSelfAttention(config, layer_id)
A:transformers.models.longformer.modeling_longformer.self.output->LongformerOutput(config)
A:transformers.models.longformer.modeling_longformer.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.longformer.modeling_longformer.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.longformer.modeling_longformer.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.longformer.modeling_longformer.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.longformer.modeling_longformer.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.longformer.modeling_longformer.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.longformer.modeling_longformer.self_outputs->self.self(hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=output_attentions)
A:transformers.models.longformer.modeling_longformer.self.attention->LongformerAttention(config, layer_id)
A:transformers.models.longformer.modeling_longformer.self.intermediate->LongformerIntermediate(config)
A:transformers.models.longformer.modeling_longformer.self_attn_outputs->self.attention(hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=output_attentions)
A:transformers.models.longformer.modeling_longformer.layer_output->self.output(intermediate_output, attn_output)
A:transformers.models.longformer.modeling_longformer.intermediate_output->self.intermediate(attn_output)
A:transformers.models.longformer.modeling_longformer.self.layer->torch.nn.ModuleList([LongformerLayer(config, layer_id=i) for i in range(config.num_hidden_layers)])
A:transformers.models.longformer.modeling_longformer.is_global_attn->is_index_global_attn.flatten().any().item()
A:transformers.models.longformer.modeling_longformer.layer_outputs->layer_module(hidden_states, attention_mask=attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=output_attentions)
A:transformers.models.longformer.modeling_longformer.self.activation->torch.nn.Tanh()
A:transformers.models.longformer.modeling_longformer.pooled_output->self.dropout(pooled_output)
A:transformers.models.longformer.modeling_longformer.self.layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.longformer.modeling_longformer.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.longformer.modeling_longformer.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.longformer.modeling_longformer.x->self.decoder(x)
A:transformers.models.longformer.modeling_longformer.self.embeddings->LongformerEmbeddings(config)
A:transformers.models.longformer.modeling_longformer.self.encoder->LongformerEncoder(config)
A:transformers.models.longformer.modeling_longformer.input_ids->torch.nn.functional.pad(input_ids, (0, padding_len), value=pad_token_id)
A:transformers.models.longformer.modeling_longformer.input_ids_padding->torch.cat([inputs_embeds, inputs_embeds_padding], dim=-2).new_full((batch_size, padding_len), self.config.pad_token_id, dtype=torch.long)
A:transformers.models.longformer.modeling_longformer.inputs_embeds_padding->self.embeddings(input_ids_padding)
A:transformers.models.longformer.modeling_longformer.(padding_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds)->self._pad_to_window_size(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, pad_token_id=self.config.pad_token_id)
A:transformers.models.longformer.modeling_longformer.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.longformer.modeling_longformer.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.longformer.modeling_longformer.self.longformer->LongformerModel(config)
A:transformers.models.longformer.modeling_longformer.self.lm_head->LongformerLMHead(config)
A:transformers.models.longformer.modeling_longformer.outputs->self.longformer(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids, attention_mask=flat_attention_mask, global_attention_mask=flat_global_attention_mask, head_mask=head_mask, inputs_embeds=flat_inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.longformer.modeling_longformer.prediction_scores->self.lm_head(sequence_output)
A:transformers.models.longformer.modeling_longformer.loss_fct->CrossEntropyLoss()
A:transformers.models.longformer.modeling_longformer.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.longformer.modeling_longformer.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.models.longformer.modeling_longformer.global_attention_mask->torch.stack([_compute_global_attention_mask(input_ids[:, i], self.config.sep_token_id, before_sep_token=False) for i in range(num_choices)], dim=1)
A:transformers.models.longformer.modeling_longformer.logits->self.classifier(pooled_output)
A:transformers.models.longformer.modeling_longformer.loss->loss_fct(reshaped_logits, labels)
A:transformers.models.longformer.modeling_longformer.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.longformer.modeling_longformer.output->self.out_proj(hidden_states)
A:transformers.models.longformer.modeling_longformer.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.longformer.modeling_longformer.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.models.longformer.modeling_longformer.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.longformer.modeling_longformer.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.longformer.modeling_longformer.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.longformer.modeling_longformer.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.longformer.modeling_longformer.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.longformer.modeling_longformer.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.longformer.modeling_longformer.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.longformer.modeling_longformer.sequence_output->self.dropout(sequence_output)
A:transformers.models.longformer.modeling_longformer.active_logits->self.classifier(pooled_output).view(-1, self.num_labels)
A:transformers.models.longformer.modeling_longformer.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.models.longformer.modeling_longformer.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.LongformerForMaskedLM(self,config)
transformers.LongformerForMaskedLM.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForMaskedLM.get_output_embeddings(self)
transformers.LongformerForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.LongformerForMultipleChoice(self,config)
transformers.LongformerForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,labels=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForQuestionAnswering(self,config)
transformers.LongformerForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForSequenceClassification(self,config)
transformers.LongformerForSequenceClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForTokenClassification(self,config)
transformers.LongformerForTokenClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerModel(self,config,add_pooling_layer=True)
transformers.LongformerModel._merge_to_attention_mask(self,attention_mask:torch.Tensor,global_attention_mask:torch.Tensor)
transformers.LongformerModel._pad_to_window_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,token_type_ids:torch.Tensor,position_ids:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.LongformerModel._prune_heads(self,heads_to_prune)
transformers.LongformerModel.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerModel.get_input_embeddings(self)
transformers.LongformerModel.set_input_embeddings(self,value)
transformers.LongformerPreTrainedModel(PreTrainedModel)
transformers.LongformerPreTrainedModel._init_weights(self,module)
transformers.LongformerSelfAttention(self,config,layer_id)
transformers.LongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.LongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.LongformerSelfAttention._compute_global_attn_output_from_hidden(self,hidden_states,max_num_global_attn_indices,layer_head_mask,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked)
transformers.LongformerSelfAttention._concat_with_global_key_attn_probs(self,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.LongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.LongformerSelfAttention._mask_invalid_locations(input_tensor,affected_seq_len)->torch.Tensor
transformers.LongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.LongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,padding)
transformers.LongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs:torch.Tensor,value:torch.Tensor,window_overlap:int)
transformers.LongformerSelfAttention._sliding_chunks_query_key_matmul(self,query:torch.Tensor,key:torch.Tensor,window_overlap:int)
transformers.LongformerSelfAttention.forward(self,hidden_states,attention_mask=None,layer_head_mask=None,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=False)
transformers.models.longformer.modeling_longformer.LongformerAttention(self,config,layer_id=0)
transformers.models.longformer.modeling_longformer.LongformerAttention.__init__(self,config,layer_id=0)
transformers.models.longformer.modeling_longformer.LongformerAttention.forward(self,hidden_states,attention_mask=None,layer_head_mask=None,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=False)
transformers.models.longformer.modeling_longformer.LongformerAttention.prune_heads(self,heads)
transformers.models.longformer.modeling_longformer.LongformerBaseModelOutput(ModelOutput)
transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling(ModelOutput)
transformers.models.longformer.modeling_longformer.LongformerClassificationHead(self,config)
transformers.models.longformer.modeling_longformer.LongformerClassificationHead.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerClassificationHead.forward(self,hidden_states,**kwargs)
transformers.models.longformer.modeling_longformer.LongformerEmbeddings(self,config)
transformers.models.longformer.modeling_longformer.LongformerEmbeddings.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.models.longformer.modeling_longformer.LongformerEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.longformer.modeling_longformer.LongformerEncoder(self,config)
transformers.models.longformer.modeling_longformer.LongformerEncoder.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.longformer.modeling_longformer.LongformerForMaskedLM(self,config)
transformers.models.longformer.modeling_longformer.LongformerForMaskedLM.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerForMaskedLM.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.longformer.modeling_longformer.LongformerForMaskedLM.get_output_embeddings(self)
transformers.models.longformer.modeling_longformer.LongformerForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.longformer.modeling_longformer.LongformerForMultipleChoice(self,config)
transformers.models.longformer.modeling_longformer.LongformerForMultipleChoice.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,labels=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.longformer.modeling_longformer.LongformerForQuestionAnswering(self,config)
transformers.models.longformer.modeling_longformer.LongformerForQuestionAnswering.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.longformer.modeling_longformer.LongformerForSequenceClassification(self,config)
transformers.models.longformer.modeling_longformer.LongformerForSequenceClassification.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerForSequenceClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.longformer.modeling_longformer.LongformerForTokenClassification(self,config)
transformers.models.longformer.modeling_longformer.LongformerForTokenClassification.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerForTokenClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.longformer.modeling_longformer.LongformerIntermediate(self,config)
transformers.models.longformer.modeling_longformer.LongformerIntermediate.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerIntermediate.forward(self,hidden_states)
transformers.models.longformer.modeling_longformer.LongformerLMHead(self,config)
transformers.models.longformer.modeling_longformer.LongformerLMHead.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerLMHead.forward(self,features,**kwargs)
transformers.models.longformer.modeling_longformer.LongformerLayer(self,config,layer_id=0)
transformers.models.longformer.modeling_longformer.LongformerLayer.__init__(self,config,layer_id=0)
transformers.models.longformer.modeling_longformer.LongformerLayer.ff_chunk(self,attn_output)
transformers.models.longformer.modeling_longformer.LongformerLayer.forward(self,hidden_states,attention_mask=None,layer_head_mask=None,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=False)
transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput(ModelOutput)
transformers.models.longformer.modeling_longformer.LongformerModel(self,config,add_pooling_layer=True)
transformers.models.longformer.modeling_longformer.LongformerModel.__init__(self,config,add_pooling_layer=True)
transformers.models.longformer.modeling_longformer.LongformerModel._merge_to_attention_mask(self,attention_mask:torch.Tensor,global_attention_mask:torch.Tensor)
transformers.models.longformer.modeling_longformer.LongformerModel._pad_to_window_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,token_type_ids:torch.Tensor,position_ids:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.models.longformer.modeling_longformer.LongformerModel._prune_heads(self,heads_to_prune)
transformers.models.longformer.modeling_longformer.LongformerModel.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,head_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.longformer.modeling_longformer.LongformerModel.get_input_embeddings(self)
transformers.models.longformer.modeling_longformer.LongformerModel.set_input_embeddings(self,value)
transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput(ModelOutput)
transformers.models.longformer.modeling_longformer.LongformerOutput(self,config)
transformers.models.longformer.modeling_longformer.LongformerOutput.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerOutput.forward(self,hidden_states,input_tensor)
transformers.models.longformer.modeling_longformer.LongformerPooler(self,config)
transformers.models.longformer.modeling_longformer.LongformerPooler.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerPooler.forward(self,hidden_states)
transformers.models.longformer.modeling_longformer.LongformerPreTrainedModel(PreTrainedModel)
transformers.models.longformer.modeling_longformer.LongformerPreTrainedModel._init_weights(self,module)
transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput(ModelOutput)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention(self,config,layer_id)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention.__init__(self,config,layer_id)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._compute_global_attn_output_from_hidden(self,hidden_states,max_num_global_attn_indices,layer_head_mask,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._concat_with_global_key_attn_probs(self,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._mask_invalid_locations(input_tensor,affected_seq_len)->torch.Tensor
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,padding)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs:torch.Tensor,value:torch.Tensor,window_overlap:int)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention._sliding_chunks_query_key_matmul(self,query:torch.Tensor,key:torch.Tensor,window_overlap:int)
transformers.models.longformer.modeling_longformer.LongformerSelfAttention.forward(self,hidden_states,attention_mask=None,layer_head_mask=None,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=False)
transformers.models.longformer.modeling_longformer.LongformerSelfOutput(self,config)
transformers.models.longformer.modeling_longformer.LongformerSelfOutput.__init__(self,config)
transformers.models.longformer.modeling_longformer.LongformerSelfOutput.forward(self,hidden_states,input_tensor)
transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput(ModelOutput)
transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput(ModelOutput)
transformers.models.longformer.modeling_longformer._compute_global_attention_mask(input_ids,sep_token_id,before_sep_token=True)
transformers.models.longformer.modeling_longformer._get_question_end_index(input_ids,sep_token_id)
transformers.models.longformer.modeling_longformer.create_position_ids_from_input_ids(input_ids,padding_idx)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/__init__.py----------------------------------------
A:transformers.models.longformer.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/longformer/tokenization_longformer.py----------------------------------------
A:transformers.models.longformer.tokenization_longformer.logger->utils.logging.get_logger(__name__)
transformers.LongformerTokenizer(RobertaTokenizer)
transformers.models.longformer.tokenization_longformer.LongformerTokenizer(RobertaTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/layoutlm/modeling_tf_layoutlm.py----------------------------------------
A:transformers.models.layoutlm.modeling_tf_layoutlm.logger->utils.logging.get_logger(__name__)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.embeddings_sum->tensorflow.keras.layers.Add()
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.dropout->tensorflow.keras.layers.Dropout(rate=config.hidden_dropout_prob)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.weight->self.add_weight(name='weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.token_type_embeddings->self.add_weight(name='embeddings', shape=[self.type_vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.position_embeddings->self.add_weight(name='embeddings', shape=[self.max_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.x_position_embeddings->self.add_weight(name='embeddings', shape=[self.max_2d_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.y_position_embeddings->self.add_weight(name='embeddings', shape=[self.max_2d_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.h_position_embeddings->self.add_weight(name='embeddings', shape=[self.max_2d_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.w_position_embeddings->self.add_weight(name='embeddings', shape=[self.max_2d_position_embeddings, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.models.layoutlm.modeling_tf_layoutlm.inputs_embeds->tensorflow.gather(params=self.weight, indices=input_ids)
A:transformers.models.layoutlm.modeling_tf_layoutlm.token_type_ids->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.layoutlm.modeling_tf_layoutlm.position_ids->tensorflow.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)
A:transformers.models.layoutlm.modeling_tf_layoutlm.bboxbbox->tensorflow.fill(input_shape + [4], value=0)
A:transformers.models.layoutlm.modeling_tf_layoutlm.left_position_embeddings->tensorflow.gather(self.x_position_embeddings, bbox[:, :, 0])
A:transformers.models.layoutlm.modeling_tf_layoutlm.upper_position_embeddings->tensorflow.gather(self.y_position_embeddings, bbox[:, :, 1])
A:transformers.models.layoutlm.modeling_tf_layoutlm.right_position_embeddings->tensorflow.gather(self.x_position_embeddings, bbox[:, :, 2])
A:transformers.models.layoutlm.modeling_tf_layoutlm.lower_position_embeddings->tensorflow.gather(self.y_position_embeddings, bbox[:, :, 3])
A:transformers.models.layoutlm.modeling_tf_layoutlm.h_position_embeddings->tensorflow.gather(self.h_position_embeddings, bbox[:, :, 3] - bbox[:, :, 1])
A:transformers.models.layoutlm.modeling_tf_layoutlm.w_position_embeddings->tensorflow.gather(self.w_position_embeddings, bbox[:, :, 2] - bbox[:, :, 0])
A:transformers.models.layoutlm.modeling_tf_layoutlm.position_embeds->tensorflow.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))
A:transformers.models.layoutlm.modeling_tf_layoutlm.token_type_embeds->tensorflow.gather(params=self.token_type_embeddings, indices=token_type_ids)
A:transformers.models.layoutlm.modeling_tf_layoutlm.final_embeddings->self.dropout(inputs=final_embeddings, training=training)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.sqrt_att_head_size->math.sqrt(self.attention_head_size)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.query->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.key->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.value->tensorflow.keras.layers.Dense(units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.models.layoutlm.modeling_tf_layoutlm.tensor->tensorflow.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.models.layoutlm.modeling_tf_layoutlm.mixed_query_layer->self.query(inputs=hidden_states)
A:transformers.models.layoutlm.modeling_tf_layoutlm.mixed_key_layer->self.key(inputs=hidden_states)
A:transformers.models.layoutlm.modeling_tf_layoutlm.mixed_value_layer->self.value(inputs=hidden_states)
A:transformers.models.layoutlm.modeling_tf_layoutlm.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.models.layoutlm.modeling_tf_layoutlm.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.models.layoutlm.modeling_tf_layoutlm.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.models.layoutlm.modeling_tf_layoutlm.attention_scores->tensorflow.add(attention_scores, attention_mask)
A:transformers.models.layoutlm.modeling_tf_layoutlm.dk->tensorflow.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
A:transformers.models.layoutlm.modeling_tf_layoutlm.attention_probs->tensorflow.multiply(attention_probs, head_mask)
A:transformers.models.layoutlm.modeling_tf_layoutlm.attention_output->self.dense_output(hidden_states=self_outputs[0], input_tensor=input_tensor, training=training)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.dense->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.models.layoutlm.modeling_tf_layoutlm.hidden_states->tensorflow.nn.bias_add(value=hidden_states, bias=self.bias)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.self_attention->TFLayoutLMSelfAttention(config, name='self')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.dense_output->TFLayoutLMSelfOutput(config, name='output')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self_outputs->self.self_attention(hidden_states=input_tensor, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.attention->TFLayoutLMAttention(config, name='attention')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.intermediate->TFLayoutLMIntermediate(config, name='intermediate')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.bert_output->TFLayoutLMOutput(config, name='output')
A:transformers.models.layoutlm.modeling_tf_layoutlm.attention_outputs->self.attention(input_tensor=hidden_states, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions, training=training)
A:transformers.models.layoutlm.modeling_tf_layoutlm.intermediate_output->self.intermediate(hidden_states=attention_output)
A:transformers.models.layoutlm.modeling_tf_layoutlm.layer_output->self.bert_output(hidden_states=intermediate_output, input_tensor=attention_output, training=training)
A:transformers.models.layoutlm.modeling_tf_layoutlm.layer_outputs->layer_module(hidden_states=hidden_states, attention_mask=attention_mask, head_mask=head_mask[i], output_attentions=output_attentions, training=training)
A:transformers.models.layoutlm.modeling_tf_layoutlm.pooled_output->self.dropout(inputs=pooled_output, training=inputs['training'])
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.transform_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.transform->TFLayoutLMPredictionHeadTransform(config, name='transform')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.predictions->TFLayoutLMLMPredictionHead(config, input_embeddings, name='predictions')
A:transformers.models.layoutlm.modeling_tf_layoutlm.prediction_scores->self.mlm(sequence_output=sequence_output, training=inputs['training'])
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.embeddings->TFLayoutLMEmbeddings(config, name='embeddings')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.encoder->TFLayoutLMEncoder(config, name='encoder')
A:transformers.models.layoutlm.modeling_tf_layoutlm.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.layoutlm.modeling_tf_layoutlm.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.layoutlm.modeling_tf_layoutlm.inputs['attention_mask']->tensorflow.fill(dims=input_shape, value=1)
A:transformers.models.layoutlm.modeling_tf_layoutlm.inputs['token_type_ids']->tensorflow.fill(dims=input_shape, value=0)
A:transformers.models.layoutlm.modeling_tf_layoutlm.inputs['bbox']->tensorflow.fill(dims=input_shape + [4], value=0)
A:transformers.models.layoutlm.modeling_tf_layoutlm.embedding_output->self.embeddings(input_ids=inputs['input_ids'], bbox=inputs['bbox'], position_ids=inputs['position_ids'], token_type_ids=inputs['token_type_ids'], inputs_embeds=inputs['inputs_embeds'], training=inputs['training'])
A:transformers.models.layoutlm.modeling_tf_layoutlm.extended_attention_mask->tensorflow.multiply(tf.subtract(one_cst, extended_attention_mask), ten_thousand_cst)
A:transformers.models.layoutlm.modeling_tf_layoutlm.one_cst->tensorflow.constant(1.0, dtype=embedding_output.dtype)
A:transformers.models.layoutlm.modeling_tf_layoutlm.ten_thousand_cst->tensorflow.constant(-10000.0, dtype=embedding_output.dtype)
A:transformers.models.layoutlm.modeling_tf_layoutlm.encoder_outputs->self.encoder(hidden_states=embedding_output, attention_mask=extended_attention_mask, head_mask=inputs['head_mask'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.layoutlm->TFLayoutLMMainLayer(config, add_pooling_layer=True, name='layoutlm')
A:transformers.models.layoutlm.modeling_tf_layoutlm.outputs->self.layoutlm(input_ids=inputs['input_ids'], bbox=inputs['bbox'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.mlm->TFLayoutLMMLMHead(config, input_embeddings=self.layoutlm.embeddings, name='mlm___cls')
A:transformers.models.layoutlm.modeling_tf_layoutlm.self.classifier->tensorflow.keras.layers.Dense(units=config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.models.layoutlm.modeling_tf_layoutlm.logits->self.classifier(inputs=sequence_output)
A:transformers.models.layoutlm.modeling_tf_layoutlm.sequence_output->self.dropout(inputs=sequence_output, training=inputs['training'])
transformers.models.layoutlm.TFLayoutLMForMaskedLM(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.TFLayoutLMForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.TFLayoutLMForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.layoutlm.TFLayoutLMForMaskedLM.get_prefix_bias_name(self)->str
transformers.models.layoutlm.TFLayoutLMForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.layoutlm.TFLayoutLMForSequenceClassification(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.TFLayoutLMForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.TFLayoutLMForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.layoutlm.TFLayoutLMForTokenClassification(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.TFLayoutLMForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.TFLayoutLMForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.layoutlm.TFLayoutLMMainLayer(self,config:LayoutLMConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.layoutlm.TFLayoutLMMainLayer._prune_heads(self,heads_to_prune)
transformers.models.layoutlm.TFLayoutLMMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.layoutlm.TFLayoutLMMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.layoutlm.TFLayoutLMMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.layoutlm.TFLayoutLMModel(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.TFLayoutLMModel.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.layoutlm.TFLayoutLMModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.layoutlm.TFLayoutLMPreTrainedModel(TFPreTrainedModel)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMAttention(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMAttention.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMAttention.call(self,input_tensor:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMAttention.prune_heads(self,heads)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEmbeddings(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEmbeddings.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEmbeddings.build(self,input_shape:tf.TensorShape)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEmbeddings.call(self,input_ids:tf.Tensor=None,bbox:tf.Tensor=None,position_ids:tf.Tensor=None,token_type_ids:tf.Tensor=None,inputs_embeds:tf.Tensor=None,training:bool=False)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEncoder(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEncoder.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMEncoder.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,output_hidden_states:bool,return_dict:bool,training:bool=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForMaskedLM(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForMaskedLM.__init__(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForMaskedLM.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFMaskedLMOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForMaskedLM.get_lm_head(self)->tf.keras.layers.Layer
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForMaskedLM.get_prefix_bias_name(self)->str
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForMaskedLM.serving_output(self,output:TFMaskedLMOutput)->TFMaskedLMOutput
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForSequenceClassification(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForSequenceClassification.__init__(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForSequenceClassification.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForSequenceClassification.serving_output(self,output:TFSequenceClassifierOutput)->TFSequenceClassifierOutput
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForTokenClassification(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForTokenClassification.__init__(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForTokenClassification.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,labels:Optional[Union[np.ndarray,tf.Tensor]]=None,training:Optional[bool]=False,**kwargs)->Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMForTokenClassification.serving_output(self,output:TFTokenClassifierOutput)->TFTokenClassifierOutput
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMIntermediate(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMIntermediate.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMIntermediate.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead(self,config:LayoutLMConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.__init__(self,config:LayoutLMConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.build(self,input_shape:tf.TensorShape)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.get_bias(self)->Dict[str, tf.Variable]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.set_bias(self,value:tf.Variable)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLMPredictionHead.set_output_embeddings(self,value:tf.Variable)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLayer(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLayer.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMLMHead(self,config:LayoutLMConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMLMHead.__init__(self,config:LayoutLMConfig,input_embeddings:tf.keras.layers.Layer,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMLMHead.call(self,sequence_output:tf.Tensor)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMainLayer(self,config:LayoutLMConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMainLayer.__init__(self,config:LayoutLMConfig,add_pooling_layer:bool=True,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMainLayer._prune_heads(self,heads_to_prune)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMainLayer.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMainLayer.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMMainLayer.set_input_embeddings(self,value:tf.Variable)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMModel(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMModel.__init__(self,config:LayoutLMConfig,*inputs,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMModel.call(self,input_ids:Optional[TFModelInputType]=None,bbox:Optional[Union[np.ndarray,tf.Tensor]]=None,attention_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,token_type_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,position_ids:Optional[Union[np.ndarray,tf.Tensor]]=None,head_mask:Optional[Union[np.ndarray,tf.Tensor]]=None,inputs_embeds:Optional[Union[np.ndarray,tf.Tensor]]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs)->Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMModel.serving_output(self,output:TFBaseModelOutputWithPooling)->TFBaseModelOutputWithPooling
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMOutput(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMOutput.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPooler(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPooler.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPooler.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPreTrainedModel(TFPreTrainedModel)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPredictionHeadTransform(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPredictionHeadTransform.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMPredictionHeadTransform.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfAttention(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfAttention.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfAttention.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,head_mask:tf.Tensor,output_attentions:bool,training:bool=False)->Tuple[tf.Tensor]
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfAttention.transpose_for_scores(self,tensor:tf.Tensor,batch_size:int)->tf.Tensor
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfOutput(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfOutput.__init__(self,config:LayoutLMConfig,**kwargs)
transformers.models.layoutlm.modeling_tf_layoutlm.TFLayoutLMSelfOutput.call(self,hidden_states:tf.Tensor,input_tensor:tf.Tensor,training:bool=False)->tf.Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/layoutlm/configuration_layoutlm.py----------------------------------------
A:transformers.models.layoutlm.configuration_layoutlm.logger->utils.logging.get_logger(__name__)
transformers.LayoutLMConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,max_2d_position_embeddings=1024,**kwargs)
transformers.models.layoutlm.configuration_layoutlm.LayoutLMConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,max_2d_position_embeddings=1024,**kwargs)
transformers.models.layoutlm.configuration_layoutlm.LayoutLMConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,max_2d_position_embeddings=1024,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/layoutlm/modeling_layoutlm.py----------------------------------------
A:transformers.models.layoutlm.modeling_layoutlm.logger->utils.logging.get_logger(__name__)
A:transformers.models.layoutlm.modeling_layoutlm.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.models.layoutlm.modeling_layoutlm.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.x_position_embeddings->torch.nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.y_position_embeddings->torch.nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.h_position_embeddings->torch.nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.w_position_embeddings->torch.nn.Embedding(config.max_2d_position_embeddings, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.layoutlm.modeling_layoutlm.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.models.layoutlm.modeling_layoutlm.input_shape->input_ids.size()
A:transformers.models.layoutlm.modeling_layoutlm.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.models.layoutlm.modeling_layoutlm.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.models.layoutlm.modeling_layoutlm.position_embeddings->self.position_embeddings(position_ids)
A:transformers.models.layoutlm.modeling_layoutlm.left_position_embeddings->self.x_position_embeddings(bbox[:, :, 0])
A:transformers.models.layoutlm.modeling_layoutlm.upper_position_embeddings->self.y_position_embeddings(bbox[:, :, 1])
A:transformers.models.layoutlm.modeling_layoutlm.right_position_embeddings->self.x_position_embeddings(bbox[:, :, 2])
A:transformers.models.layoutlm.modeling_layoutlm.lower_position_embeddings->self.y_position_embeddings(bbox[:, :, 3])
A:transformers.models.layoutlm.modeling_layoutlm.h_position_embeddings->self.h_position_embeddings(bbox[:, :, 3] - bbox[:, :, 1])
A:transformers.models.layoutlm.modeling_layoutlm.w_position_embeddings->self.w_position_embeddings(bbox[:, :, 2] - bbox[:, :, 0])
A:transformers.models.layoutlm.modeling_layoutlm.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.models.layoutlm.modeling_layoutlm.embeddings->self.dropout(embeddings)
A:transformers.models.layoutlm.modeling_layoutlm.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.models.layoutlm.modeling_layoutlm.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.models.layoutlm.modeling_layoutlm.self.position_embedding_type->getattr(config, 'position_embedding_type', 'absolute')
A:transformers.models.layoutlm.modeling_layoutlm.self.distance_embedding->torch.nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
A:transformers.models.layoutlm.modeling_layoutlm.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.models.layoutlm.modeling_layoutlm.mixed_query_layer->self.query(hidden_states)
A:transformers.models.layoutlm.modeling_layoutlm.key_layer->self.transpose_for_scores(self.key(hidden_states))
A:transformers.models.layoutlm.modeling_layoutlm.value_layer->self.transpose_for_scores(self.value(hidden_states))
A:transformers.models.layoutlm.modeling_layoutlm.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.models.layoutlm.modeling_layoutlm.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.models.layoutlm.modeling_layoutlm.position_ids_l->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
A:transformers.models.layoutlm.modeling_layoutlm.position_ids_r->torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
A:transformers.models.layoutlm.modeling_layoutlm.positional_embedding->positional_embedding.to(dtype=query_layer.dtype).to(dtype=query_layer.dtype)
A:transformers.models.layoutlm.modeling_layoutlm.relative_position_scores->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.layoutlm.modeling_layoutlm.relative_position_scores_query->torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
A:transformers.models.layoutlm.modeling_layoutlm.relative_position_scores_key->torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
A:transformers.models.layoutlm.modeling_layoutlm.attention_probs->self.dropout(attention_probs)
A:transformers.models.layoutlm.modeling_layoutlm.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.models.layoutlm.modeling_layoutlm.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.models.layoutlm.modeling_layoutlm.hidden_states->self.decoder(hidden_states)
A:transformers.models.layoutlm.modeling_layoutlm.self.self->LayoutLMSelfAttention(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.output->LayoutLMOutput(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.models.layoutlm.modeling_layoutlm.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.models.layoutlm.modeling_layoutlm.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.models.layoutlm.modeling_layoutlm.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.models.layoutlm.modeling_layoutlm.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.models.layoutlm.modeling_layoutlm.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.models.layoutlm.modeling_layoutlm.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.layoutlm.modeling_layoutlm.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.models.layoutlm.modeling_layoutlm.self.attention->LayoutLMAttention(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.crossattention->LayoutLMAttention(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.intermediate->LayoutLMIntermediate(config)
A:transformers.models.layoutlm.modeling_layoutlm.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
A:transformers.models.layoutlm.modeling_layoutlm.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, cross_attn_past_key_value, output_attentions)
A:transformers.models.layoutlm.modeling_layoutlm.layer_output->self.output(intermediate_output, attention_output)
A:transformers.models.layoutlm.modeling_layoutlm.intermediate_output->self.intermediate(attention_output)
A:transformers.models.layoutlm.modeling_layoutlm.self.layer->torch.nn.ModuleList([LayoutLMLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.layoutlm.modeling_layoutlm.layer_outputs->layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
A:transformers.models.layoutlm.modeling_layoutlm.self.activation->torch.nn.Tanh()
A:transformers.models.layoutlm.modeling_layoutlm.pooled_output->self.dropout(pooled_output)
A:transformers.models.layoutlm.modeling_layoutlm.self.transform->LayoutLMPredictionHeadTransform(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.layoutlm.modeling_layoutlm.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.models.layoutlm.modeling_layoutlm.self.predictions->LayoutLMLMPredictionHead(config)
A:transformers.models.layoutlm.modeling_layoutlm.prediction_scores->self.cls(sequence_output)
A:transformers.models.layoutlm.modeling_layoutlm.self.embeddings->LayoutLMEmbeddings(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.encoder->LayoutLMEncoder(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.pooler->LayoutLMPooler(config)
A:transformers.models.layoutlm.modeling_layoutlm.attention_mask->torch.ones(input_shape, device=device)
A:transformers.models.layoutlm.modeling_layoutlm.bbox->torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)
A:transformers.models.layoutlm.modeling_layoutlm.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.models.layoutlm.modeling_layoutlm.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.models.layoutlm.modeling_layoutlm.embedding_output->self.embeddings(input_ids=input_ids, bbox=bbox, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.models.layoutlm.modeling_layoutlm.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.layoutlm.modeling_layoutlm.self.layoutlm->LayoutLMModel(config)
A:transformers.models.layoutlm.modeling_layoutlm.self.cls->LayoutLMOnlyMLMHead(config)
A:transformers.models.layoutlm.modeling_layoutlm.outputs->self.layoutlm(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.layoutlm.modeling_layoutlm.loss_fct->CrossEntropyLoss()
A:transformers.models.layoutlm.modeling_layoutlm.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.layoutlm.modeling_layoutlm.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.layoutlm.modeling_layoutlm.logits->self.classifier(sequence_output)
A:transformers.models.layoutlm.modeling_layoutlm.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.models.layoutlm.modeling_layoutlm.sequence_output->self.dropout(sequence_output)
transformers.LayoutLMForMaskedLM(self,config)
transformers.LayoutLMForMaskedLM.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LayoutLMForMaskedLM.get_input_embeddings(self)
transformers.LayoutLMForMaskedLM.get_output_embeddings(self)
transformers.LayoutLMForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.LayoutLMForSequenceClassification(self,config)
transformers.LayoutLMForSequenceClassification.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LayoutLMForSequenceClassification.get_input_embeddings(self)
transformers.LayoutLMForTokenClassification(self,config)
transformers.LayoutLMForTokenClassification.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LayoutLMForTokenClassification.get_input_embeddings(self)
transformers.LayoutLMModel(self,config)
transformers.LayoutLMModel._prune_heads(self,heads_to_prune)
transformers.LayoutLMModel.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LayoutLMModel.get_input_embeddings(self)
transformers.LayoutLMModel.set_input_embeddings(self,value)
transformers.LayoutLMPreTrainedModel(PreTrainedModel)
transformers.LayoutLMPreTrainedModel._init_weights(self,module)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMAttention(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMAttention.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMAttention.prune_heads(self,heads)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMEmbeddings(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMEmbeddings.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMEmbeddings.forward(self,input_ids=None,bbox=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMEncoder(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMEncoder.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForMaskedLM(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForMaskedLM.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForMaskedLM.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForMaskedLM.get_input_embeddings(self)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForMaskedLM.get_output_embeddings(self)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForMaskedLM.set_output_embeddings(self,new_embeddings)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForSequenceClassification(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForSequenceClassification.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForSequenceClassification.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForSequenceClassification.get_input_embeddings(self)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForTokenClassification(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForTokenClassification.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForTokenClassification.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMForTokenClassification.get_input_embeddings(self)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMIntermediate(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMIntermediate.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMIntermediate.forward(self,hidden_states)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLMPredictionHead(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLMPredictionHead.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLMPredictionHead.forward(self,hidden_states)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLayer(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLayer.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLayer.feed_forward_chunk(self,attention_output)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMModel(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMModel.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMModel._prune_heads(self,heads_to_prune)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMModel.forward(self,input_ids=None,bbox=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMModel.get_input_embeddings(self)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMModel.set_input_embeddings(self,value)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMOnlyMLMHead(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMOnlyMLMHead.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMOnlyMLMHead.forward(self,sequence_output)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMOutput(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMOutput.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMOutput.forward(self,hidden_states,input_tensor)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPooler(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPooler.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPooler.forward(self,hidden_states)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPreTrainedModel(PreTrainedModel)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPreTrainedModel._init_weights(self,module)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPredictionHeadTransform(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPredictionHeadTransform.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMPredictionHeadTransform.forward(self,hidden_states)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfAttention(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfAttention.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,past_key_value=None,output_attentions=False)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfAttention.transpose_for_scores(self,x)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfOutput(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfOutput.__init__(self,config)
transformers.models.layoutlm.modeling_layoutlm.LayoutLMSelfOutput.forward(self,hidden_states,input_tensor)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/layoutlm/__init__.py----------------------------------------
A:transformers.models.layoutlm.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/layoutlm/tokenization_layoutlm.py----------------------------------------
A:transformers.models.layoutlm.tokenization_layoutlm.logger->utils.logging.get_logger(__name__)
transformers.LayoutLMTokenizer(BertTokenizer)
transformers.models.layoutlm.tokenization_layoutlm.LayoutLMTokenizer(BertTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/layoutlm/tokenization_layoutlm_fast.py----------------------------------------
A:transformers.models.layoutlm.tokenization_layoutlm_fast.logger->utils.logging.get_logger(__name__)
transformers.LayoutLMTokenizerFast(BertTokenizerFast)
transformers.models.layoutlm.tokenization_layoutlm_fast.LayoutLMTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/phobert/tokenization_phobert.py----------------------------------------
A:transformers.models.phobert.tokenization_phobert.logger->utils.logging.get_logger(__name__)
A:transformers.models.phobert.tokenization_phobert.pairs->get_pairs(word)
A:transformers.models.phobert.tokenization_phobert.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.models.phobert.tokenization_phobert.word->'@@ '.join(word)
A:transformers.models.phobert.tokenization_phobert.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.models.phobert.tokenization_phobert.j->'@@ '.join(word).index(first, i)
A:transformers.models.phobert.tokenization_phobert.new_word->tuple(new_word)
A:transformers.models.phobert.tokenization_phobert.words->re.findall('\\S+\\n?', text)
A:transformers.models.phobert.tokenization_phobert.out_string->' '.join(tokens).replace('@@ ', '').strip()
A:transformers.models.phobert.tokenization_phobert.out_vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.phobert.tokenization_phobert.out_merge_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['merges_file'])
A:transformers.models.phobert.tokenization_phobert.lines->f.readlines()
A:transformers.models.phobert.tokenization_phobert.line->lineTmp.strip()
A:transformers.models.phobert.tokenization_phobert.idx->lineTmp.strip().rfind(' ')
A:transformers.models.phobert.tokenization_phobert.self.encoder[word]->len(self.encoder)
transformers.PhobertTokenizer(self,vocab_file,merges_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.PhobertTokenizer._convert_id_to_token(self,index)
transformers.PhobertTokenizer._convert_token_to_id(self,token)
transformers.PhobertTokenizer._tokenize(self,text)
transformers.PhobertTokenizer.add_from_file(self,f)
transformers.PhobertTokenizer.bpe(self,token)
transformers.PhobertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.PhobertTokenizer.convert_tokens_to_string(self,tokens)
transformers.PhobertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.PhobertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PhobertTokenizer.get_vocab(self)
transformers.PhobertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.PhobertTokenizer.vocab_size(self)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer(self,vocab_file,merges_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.__init__(self,vocab_file,merges_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer._convert_id_to_token(self,index)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer._convert_token_to_id(self,token)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer._tokenize(self,text)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.add_from_file(self,f)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.bpe(self,token)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.convert_tokens_to_string(self,tokens)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.get_vocab(self)
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.phobert.tokenization_phobert.PhobertTokenizer.vocab_size(self)
transformers.models.phobert.tokenization_phobert.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/phobert/__init__.py----------------------------------------
A:transformers.models.phobert.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/modeling_wav2vec2.py----------------------------------------
A:transformers.models.wav2vec2.modeling_wav2vec2.logger->utils.logging.get_logger(__name__)
A:transformers.models.wav2vec2.modeling_wav2vec2.num_masked_spans->max(num_masked_spans, min_masks)
A:transformers.models.wav2vec2.modeling_wav2vec2.spec_aug_mask->spec_aug_mask.scatter(1, spec_aug_mask_idxs, True).scatter(1, spec_aug_mask_idxs, True)
A:transformers.models.wav2vec2.modeling_wav2vec2.uniform_dist->torch.ones((batch_size, sequence_length - (mask_length - 1)), device=device)
A:transformers.models.wav2vec2.modeling_wav2vec2.spec_aug_mask_idxs->spec_aug_mask_idxs.unsqueeze(dim=-1).expand((batch_size, num_masked_spans, mask_length)).reshape(batch_size, num_masked_spans * mask_length).unsqueeze(dim=-1).expand((batch_size, num_masked_spans, mask_length)).reshape(batch_size, num_masked_spans * mask_length)
A:transformers.models.wav2vec2.modeling_wav2vec2.offsets->torch.arange(mask_length, device=device)[None, None, :].expand((batch_size, num_masked_spans, mask_length)).reshape(batch_size, num_masked_spans * mask_length)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.conv->torch.nn.utils.weight_norm(self.conv, name='weight', dim=2)
A:transformers.models.wav2vec2.modeling_wav2vec2.hidden_states->self.dropout(hidden_states)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.padding->Wav2Vec2SamePadLayer(config.num_conv_pos_embeddings)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.conv_layers->torch.nn.ModuleList(conv_layers)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.projection->torch.nn.Linear(config.conv_dim[-1], config.hidden_size)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.dropout->torch.nn.Dropout(config.final_dropout)
A:transformers.models.wav2vec2.modeling_wav2vec2.norm_hidden_states->self.layer_norm(hidden_states)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.out_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.wav2vec2.modeling_wav2vec2.(bsz, tgt_len, embed_dim)->self.dropout(hidden_states).size()
A:transformers.models.wav2vec2.modeling_wav2vec2.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.wav2vec2.modeling_wav2vec2.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.wav2vec2.modeling_wav2vec2.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.wav2vec2.modeling_wav2vec2.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.wav2vec2.modeling_wav2vec2.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.wav2vec2.modeling_wav2vec2.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.wav2vec2.modeling_wav2vec2.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.wav2vec2.modeling_wav2vec2.attn_output->self.out_proj(attn_output)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.intermediate_dropout->torch.nn.Dropout(config.activation_dropout)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.intermediate_dense->torch.nn.Linear(config.hidden_size, config.intermediate_size)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.output_dense->torch.nn.Linear(config.intermediate_size, config.hidden_size)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.output_dropout->torch.nn.Dropout(config.hidden_dropout)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.attention->Wav2Vec2Attention(embed_dim=config.hidden_size, num_heads=config.num_attention_heads, dropout=config.attention_dropout, is_decoder=False)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.feed_forward->Wav2Vec2FeedForward(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.final_layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.models.wav2vec2.modeling_wav2vec2.(hidden_states, attn_weights, _)->self.attention(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.pos_conv_embed->Wav2Vec2PositionalConvEmbedding(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.layers->torch.nn.ModuleList([Wav2Vec2EncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)])
A:transformers.models.wav2vec2.modeling_wav2vec2.attention_mask->attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool().flip([-1]).cumsum(-1).flip([-1]).bool()
A:transformers.models.wav2vec2.modeling_wav2vec2.position_embeddings->self.pos_conv_embed(hidden_states)
A:transformers.models.wav2vec2.modeling_wav2vec2.deepspeed_zero3_is_enabled->is_deepspeed_zero3_enabled()
A:transformers.models.wav2vec2.modeling_wav2vec2.dropout_probability->numpy.random.uniform(0, 1)
A:transformers.models.wav2vec2.modeling_wav2vec2.layer_outputs->layer(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.codevectors->torch.nn.Parameter(torch.FloatTensor(1, self.num_groups * self.num_vars, config.codevector_dim // self.num_groups))
A:transformers.models.wav2vec2.modeling_wav2vec2.self.weight_proj->torch.nn.Linear(config.conv_dim[-1], self.num_groups * self.num_vars)
A:transformers.models.wav2vec2.modeling_wav2vec2.mask_extended->mask.flatten()[:, None, None].expand(probs.shape)
A:transformers.models.wav2vec2.modeling_wav2vec2.probs->torch.where(mask_extended, probs, torch.zeros_like(probs))
A:transformers.models.wav2vec2.modeling_wav2vec2.marginal_probs->torch.where(mask_extended, probs, torch.zeros_like(probs)).mean(dim=0)
A:transformers.models.wav2vec2.modeling_wav2vec2.perplexity->self._compute_perplexity(codevector_probs, mask_time_indices)
A:transformers.models.wav2vec2.modeling_wav2vec2.codevector_probs->codevector_probs.view(batch_size * sequence_length, -1).view(batch_size * sequence_length, -1)
A:transformers.models.wav2vec2.modeling_wav2vec2.codevector_soft_dist->torch.softmax(hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1)
A:transformers.models.wav2vec2.modeling_wav2vec2.codevector_idx->self.dropout(hidden_states).argmax(dim=-1)
A:transformers.models.wav2vec2.modeling_wav2vec2.codevectors->codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1).sum(-2).view(batch_size, sequence_length, -1)
A:transformers.models.wav2vec2.modeling_wav2vec2.input_lengths->self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.feature_extractor->Wav2Vec2FeatureExtractor(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.feature_projection->Wav2Vec2FeatureProjection(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.masked_spec_embed->torch.nn.Parameter(torch.FloatTensor(config.hidden_size).uniform_())
A:transformers.models.wav2vec2.modeling_wav2vec2.self.encoder->Wav2Vec2Encoder(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.hidden_states[mask_time_indices]->self.masked_spec_embed.to(hidden_states.dtype)
A:transformers.models.wav2vec2.modeling_wav2vec2.(batch_size, sequence_length, hidden_size)->self.dropout(hidden_states).size()
A:transformers.models.wav2vec2.modeling_wav2vec2.mask_time_indices->mask_time_indices.to(torch.bool).to(torch.bool)
A:transformers.models.wav2vec2.modeling_wav2vec2.mask_feature_indices->_compute_mask_indices((batch_size, hidden_size), mask_prob=self.config.mask_feature_prob, mask_length=self.config.mask_feature_length, device=hidden_states.device)
A:transformers.models.wav2vec2.modeling_wav2vec2.extract_features->self.dropout_features(outputs[1])
A:transformers.models.wav2vec2.modeling_wav2vec2.output_lengths->self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)
A:transformers.models.wav2vec2.modeling_wav2vec2.(hidden_states, extract_features)->self.feature_projection(extract_features)
A:transformers.models.wav2vec2.modeling_wav2vec2.encoder_outputs->self.encoder(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.wav2vec2->Wav2Vec2Model(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.dropout_features->torch.nn.Dropout(config.feat_quantizer_dropout)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.quantizer->Wav2Vec2GumbelVectorQuantizer(config)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.project_q->torch.nn.Linear(config.codevector_dim, config.proj_codevector_dim)
A:transformers.models.wav2vec2.modeling_wav2vec2.self.project_hid->torch.nn.Linear(config.hidden_size, config.proj_codevector_dim)
A:transformers.models.wav2vec2.modeling_wav2vec2.features->features.view(-1, hidden_size).view(-1, hidden_size)
A:transformers.models.wav2vec2.modeling_wav2vec2.sampled_negative_indices->torch.randint(low=0, high=sequence_length - 1, size=(batch_size, num_negatives * sequence_length), device=features.device)
A:transformers.models.wav2vec2.modeling_wav2vec2.feature_indices->torch.arange(sequence_length, device=features.device)[:, None].expand(sequence_length, num_negatives).flatten()
A:transformers.models.wav2vec2.modeling_wav2vec2.sampled_negatives->sampled_negatives.view(batch_size, sequence_length, num_negatives, hidden_size).permute(2, 0, 1, 3).view(batch_size, sequence_length, num_negatives, hidden_size).permute(2, 0, 1, 3)
A:transformers.models.wav2vec2.modeling_wav2vec2.target_features->torch.cat([target_features, negative_features], dim=0)
A:transformers.models.wav2vec2.modeling_wav2vec2.logits->self.lm_head(hidden_states)
A:transformers.models.wav2vec2.modeling_wav2vec2.outputs->self.wav2vec2(input_values, attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.wav2vec2.modeling_wav2vec2.transformer_features->self.project_hid(outputs[0])
A:transformers.models.wav2vec2.modeling_wav2vec2.(quantized_features, codevector_perplexity)->self.quantizer(extract_features, mask_time_indices)
A:transformers.models.wav2vec2.modeling_wav2vec2.quantized_features->self.project_q(quantized_features)
A:transformers.models.wav2vec2.modeling_wav2vec2.negative_quantized_features->self._sample_negatives(quantized_features, self.config.num_negatives)
A:transformers.models.wav2vec2.modeling_wav2vec2.neg_is_pos->(quantized_features == negative_quantized_features).all(-1)
A:transformers.models.wav2vec2.modeling_wav2vec2.logits[1:][neg_is_pos]->float('-inf')
A:transformers.models.wav2vec2.modeling_wav2vec2.preds->self.lm_head(hidden_states).transpose(0, 2).reshape(-1, logits.size(0))
A:transformers.models.wav2vec2.modeling_wav2vec2.target->((1 - mask_time_indices.long()) * -100).transpose(0, 1).flatten()
A:transformers.models.wav2vec2.modeling_wav2vec2.contrastive_loss->torch.nn.functional.cross_entropy(preds.float(), target, reduction='sum')
A:transformers.models.wav2vec2.modeling_wav2vec2.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size)
A:transformers.models.wav2vec2.modeling_wav2vec2.target_lengths->labels_mask.sum(-1)
A:transformers.models.wav2vec2.modeling_wav2vec2.flattened_targets->labels.masked_select(labels_mask)
A:transformers.models.wav2vec2.modeling_wav2vec2.log_probs->torch.nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)
A:transformers.models.wav2vec2.modeling_wav2vec2.loss->torch.nn.functional.ctc_loss(log_probs, flattened_targets, input_lengths, target_lengths, blank=self.config.pad_token_id, reduction=self.config.ctc_loss_reduction, zero_infinity=self.config.ctc_zero_infinity)
transformers.Wav2Vec2ForCTC(self,config)
transformers.Wav2Vec2ForCTC.forward(self,input_values,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.Wav2Vec2ForCTC.freeze_feature_extractor(self)
transformers.Wav2Vec2ForMaskedLM(self,config)
transformers.Wav2Vec2ForMaskedLM.forward(self,input_values,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.Wav2Vec2ForPreTraining(self,config:Wav2Vec2Config)
transformers.Wav2Vec2ForPreTraining._sample_negatives(features:torch.FloatTensor,num_negatives:int)
transformers.Wav2Vec2ForPreTraining.compute_contrastive_logits(target_features:torch.FloatTensor,negative_features:torch.FloatTensor,predicted_features:torch.FloatTensor,temperature:int=1)
transformers.Wav2Vec2ForPreTraining.forward(self,input_values,attention_mask=None,mask_time_indices=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.Wav2Vec2ForPreTraining.freeze_feature_extractor(self)
transformers.Wav2Vec2ForPreTraining.set_gumbel_temperature(self,temperature:int)
transformers.Wav2Vec2ForPreTrainingOutput(ModelOutput)
transformers.Wav2Vec2Model(self,config:Wav2Vec2Config)
transformers.Wav2Vec2Model._mask_hidden_states(self,hidden_states:torch.FloatTensor,mask_time_indices:Optional[torch.FloatTensor]=None)
transformers.Wav2Vec2Model.forward(self,input_values,attention_mask=None,mask_time_indices=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.Wav2Vec2PreTrainedModel(PreTrainedModel)
transformers.Wav2Vec2PreTrainedModel._get_feat_extract_output_lengths(self,input_lengths:Union[torch.LongTensor,int])
transformers.Wav2Vec2PreTrainedModel._init_weights(self,module)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Attention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2BaseModelOutput(ModelOutput)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder.forward(self,hidden_states,attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm.forward(self,hidden_states,attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=True)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureExtractor(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureExtractor.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureExtractor._freeze_parameters(self)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureExtractor.forward(self,input_values)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.forward(self,input_values,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.freeze_feature_extractor(self)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForMaskedLM(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForMaskedLM.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForMaskedLM.forward(self,input_values,attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining(self,config:Wav2Vec2Config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.__init__(self,config:Wav2Vec2Config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining._sample_negatives(features:torch.FloatTensor,num_negatives:int)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.compute_contrastive_logits(target_features:torch.FloatTensor,negative_features:torch.FloatTensor,predicted_features:torch.FloatTensor,temperature:int=1)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.forward(self,input_values,attention_mask=None,mask_time_indices=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.freeze_feature_extractor(self)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.set_gumbel_temperature(self,temperature:int)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput(ModelOutput)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer(self,config,layer_id=0)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer.__init__(self,config,layer_id=0)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer._compute_perplexity(probs,mask=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer.forward(self,hidden_states,mask_time_indices=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer.set_temperature(self,temperature:int)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer(self,config,layer_id=0)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer.__init__(self,config,layer_id=0)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model(self,config:Wav2Vec2Config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model.__init__(self,config:Wav2Vec2Config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states(self,hidden_states:torch.FloatTensor,mask_time_indices:Optional[torch.FloatTensor]=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model.forward(self,input_values,attention_mask=None,mask_time_indices=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer(self,config,layer_id=0)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer.__init__(self,config,layer_id=0)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding.__init__(self,config)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PreTrainedModel(PreTrainedModel)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PreTrainedModel._get_feat_extract_output_lengths(self,input_lengths:Union[torch.LongTensor,int])
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PreTrainedModel._init_weights(self,module)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer(self,num_conv_pos_embeddings)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer.__init__(self,num_conv_pos_embeddings)
transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer.forward(self,hidden_states)
transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices(shape:Tuple[int,int],mask_prob:float,mask_length:int,device:torch.device,min_masks:int=0)->torch.tensor


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/processing_wav2vec2.py----------------------------------------
A:transformers.models.wav2vec2.processing_wav2vec2.feature_extractor->feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.models.wav2vec2.processing_wav2vec2.tokenizer->tokenization_wav2vec2.Wav2Vec2CTCTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)
transformers.Wav2Vec2Processor(self,feature_extractor,tokenizer)
transformers.Wav2Vec2Processor.as_target_processor(self)
transformers.Wav2Vec2Processor.batch_decode(self,*args,**kwargs)
transformers.Wav2Vec2Processor.decode(self,*args,**kwargs)
transformers.Wav2Vec2Processor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.Wav2Vec2Processor.pad(self,*args,**kwargs)
transformers.Wav2Vec2Processor.save_pretrained(self,save_directory)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor(self,feature_extractor,tokenizer)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.__init__(self,feature_extractor,tokenizer)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.as_target_processor(self)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.batch_decode(self,*args,**kwargs)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.decode(self,*args,**kwargs)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.pad(self,*args,**kwargs)
transformers.models.wav2vec2.processing_wav2vec2.Wav2Vec2Processor.save_pretrained(self,save_directory)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/configuration_wav2vec2.py----------------------------------------
A:transformers.models.wav2vec2.configuration_wav2vec2.logger->utils.logging.get_logger(__name__)
A:transformers.models.wav2vec2.configuration_wav2vec2.self.conv_dim->list(conv_dim)
A:transformers.models.wav2vec2.configuration_wav2vec2.self.conv_stride->list(conv_stride)
A:transformers.models.wav2vec2.configuration_wav2vec2.self.conv_kernel->list(conv_kernel)
A:transformers.models.wav2vec2.configuration_wav2vec2.self.num_feat_extract_layers->len(self.conv_dim)
transformers.Wav2Vec2Config(self,vocab_size=32,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout=0.1,activation_dropout=0.1,attention_dropout=0.1,feat_proj_dropout=0.1,feat_quantizer_dropout=0.0,final_dropout=0.1,layerdrop=0.1,initializer_range=0.02,layer_norm_eps=1e-05,feat_extract_norm='group',feat_extract_activation='gelu',conv_dim=(512,512,512,512,512,512,512),conv_stride=(5,2,2,2,2,2,2),conv_kernel=(10,3,3,3,3,2,2),conv_bias=False,num_conv_pos_embeddings=128,num_conv_pos_embedding_groups=16,do_stable_layer_norm=False,apply_spec_augment=True,mask_time_prob=0.05,mask_time_length=10,mask_feature_prob=0.0,mask_feature_length=10,num_codevectors_per_group=320,num_codevector_groups=2,contrastive_logits_temperature=0.1,num_negatives=100,codevector_dim=256,proj_codevector_dim=256,diversity_loss_weight=0.1,ctc_loss_reduction='sum',ctc_zero_infinity=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config(self,vocab_size=32,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout=0.1,activation_dropout=0.1,attention_dropout=0.1,feat_proj_dropout=0.1,feat_quantizer_dropout=0.0,final_dropout=0.1,layerdrop=0.1,initializer_range=0.02,layer_norm_eps=1e-05,feat_extract_norm='group',feat_extract_activation='gelu',conv_dim=(512,512,512,512,512,512,512),conv_stride=(5,2,2,2,2,2,2),conv_kernel=(10,3,3,3,3,2,2),conv_bias=False,num_conv_pos_embeddings=128,num_conv_pos_embedding_groups=16,do_stable_layer_norm=False,apply_spec_augment=True,mask_time_prob=0.05,mask_time_length=10,mask_feature_prob=0.0,mask_feature_length=10,num_codevectors_per_group=320,num_codevector_groups=2,contrastive_logits_temperature=0.1,num_negatives=100,codevector_dim=256,proj_codevector_dim=256,diversity_loss_weight=0.1,ctc_loss_reduction='sum',ctc_zero_infinity=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config.__init__(self,vocab_size=32,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout=0.1,activation_dropout=0.1,attention_dropout=0.1,feat_proj_dropout=0.1,feat_quantizer_dropout=0.0,final_dropout=0.1,layerdrop=0.1,initializer_range=0.02,layer_norm_eps=1e-05,feat_extract_norm='group',feat_extract_activation='gelu',conv_dim=(512,512,512,512,512,512,512),conv_stride=(5,2,2,2,2,2,2),conv_kernel=(10,3,3,3,3,2,2),conv_bias=False,num_conv_pos_embeddings=128,num_conv_pos_embedding_groups=16,do_stable_layer_norm=False,apply_spec_augment=True,mask_time_prob=0.05,mask_time_length=10,mask_feature_prob=0.0,mask_feature_length=10,num_codevectors_per_group=320,num_codevector_groups=2,contrastive_logits_temperature=0.1,num_negatives=100,codevector_dim=256,proj_codevector_dim=256,diversity_loss_weight=0.1,ctc_loss_reduction='sum',ctc_zero_infinity=False,gradient_checkpointing=False,pad_token_id=0,bos_token_id=1,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.logger->transformers.logging.get_logger(__name__)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.hf_pointer->getattr(hf_pointer, attribute)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.fairseq_dict->fairseq_model.state_dict()
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.mapped_key->mapped_key.replace('*', layer_index).replace('*', layer_index)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.items->name.split('.')
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.layer_id->int(items[0])
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.type_id->int(items[1])
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.config->Wav2Vec2Config()
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.target_dict->fairseq.data.Dictionary.load(dict_path)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.config.vocab_size->len(target_dict.symbols)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.vocab_path->os.path.join(pytorch_dump_folder_path, 'vocab.json')
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.tokenizer->Wav2Vec2CTCTokenizer(vocab_path, unk_token=target_dict.unk_word, pad_token=target_dict.pad_word, bos_token=target_dict.bos_word, eos_token=target_dict.eos_word, word_delimiter_token='|', do_lower_case=False)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.feature_extractor->Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0, do_normalize=True, return_attention_mask=return_attention_mask)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.processor->Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.hf_wav2vec->Wav2Vec2ForPreTraining(config)
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.(model, _, _)->fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path])
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.model->model[0].eval()
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.convert_wav2vec2_checkpoint(checkpoint_path,pytorch_dump_folder_path,config_path=None,dict_path=None,is_finetuned=True)
transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.load_conv_layer(full_name,value,feature_extractor,unused_weights,use_group_norm)
transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.recursively_load_weights(fairseq_model,hf_model,is_headless)
transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.set_recursively(hf_pointer,key,value,full_name,weight_type)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/feature_extraction_wav2vec2.py----------------------------------------
A:transformers.models.wav2vec2.feature_extraction_wav2vec2.logger->utils.logging.get_logger(__name__)
A:transformers.models.wav2vec2.feature_extraction_wav2vec2.is_batched->bool(isinstance(raw_speech, (list, tuple)) and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list))))
A:transformers.models.wav2vec2.feature_extraction_wav2vec2.raw_speech->self.zero_mean_unit_var_norm(raw_speech)
A:transformers.models.wav2vec2.feature_extraction_wav2vec2.encoded_inputs->BatchFeature({'input_values': raw_speech})
A:transformers.models.wav2vec2.feature_extraction_wav2vec2.padded_inputs->self.pad(encoded_inputs, padding=padding, max_length=max_length, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask, return_tensors=return_tensors)
transformers.Wav2Vec2FeatureExtractor(self,feature_size=1,sampling_rate=16000,padding_value=0.0,return_attention_mask=False,do_normalize=True,**kwargs)
transformers.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm(input_values:List[np.ndarray])->List[np.ndarray]
transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor(self,feature_size=1,sampling_rate=16000,padding_value=0.0,return_attention_mask=False,do_normalize=True,**kwargs)
transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.__init__(self,feature_size=1,sampling_rate=16000,padding_value=0.0,return_attention_mask=False,do_normalize=True,**kwargs)
transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm(input_values:List[np.ndarray])->List[np.ndarray]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/modeling_tf_wav2vec2.py----------------------------------------
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.logger->utils.logging.get_logger(__name__)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.signature->dict(inspect.signature(func).parameters)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.parameter_names->list(signature.keys())
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.output['input_values']->tensorflow.convert_to_tensor(np.random.rand(1, 16000), tf.float32).pop('inputs')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.output['past_key_values']->tensorflow.convert_to_tensor(np.random.rand(1, 16000), tf.float32).pop('decoder_cached_states')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.output[name]->kwargs.pop(name, signature[name].default)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.(_, indices)->tensorflow.nn.top_k(distribution + z, num_samples)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.indices_shape->shape_list(batch_indices)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.broad_casted_batch_dims->tensorflow.reshape(tf.broadcast_to(tf.expand_dims(tf.range(indices_shape[0]), axis=-1), indices_shape), [1, -1])
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.pair_indices->tensorflow.transpose(tf.concat([broad_casted_batch_dims, tf.reshape(batch_indices, [1, -1])], 0))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.num_masked_spans->max(num_masked_spans, min_masks)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.spec_aug_mask->_scatter_values_on_batch_indices(tf.ones_like(spec_aug_mask_idxs), spec_aug_mask_idxs, spec_aug_mask.shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.uniform_dist->tensorflow.ones((batch_size, sequence_length - (mask_length - 1)))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.spec_aug_mask_idxs->tensorflow.reshape(spec_aug_mask_idxs, (batch_size, num_masked_spans * mask_length))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.offsets->tensorflow.reshape(offsets, (batch_size, num_masked_spans * mask_length))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.one_cst->tensorflow.constant(1.0)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.beta_initializer->tensorflow.keras.initializers.get(beta_initializer)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.gamma_initializer->tensorflow.keras.initializers.get(gamma_initializer)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.beta_regularizer->tensorflow.keras.regularizers.get(beta_regularizer)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.gamma_regularizer->tensorflow.keras.regularizers.get(gamma_regularizer)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.beta_constraint->tensorflow.keras.constraints.get(beta_constraint)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.gamma_constraint->tensorflow.keras.constraints.get(gamma_constraint)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.input_shape->tensorflow.keras.backend.int_shape(inputs)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.tensor_input_shape->tensorflow.shape(inputs)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.(reshaped_inputs, group_shape)->self._reshape_into_groups(inputs, input_shape, tensor_input_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.normalized_inputs->tensorflow.nn.batch_normalization(reshaped_inputs, mean=mean, variance=variance, scale=gamma, offset=beta, variance_epsilon=self.epsilon)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.outputs->self.wav2vec2(input_values=inputs['input_values'], attention_mask=inputs['attention_mask'], token_type_ids=inputs['token_type_ids'], position_ids=inputs['position_ids'], head_mask=inputs['head_mask'], inputs_embeds=inputs['inputs_embeds'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.base_config->super().get_config()
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.group_shape->tensorflow.keras.backend.int_shape(reshaped_inputs)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.reshaped_inputs->tensorflow.reshape(inputs, group_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.group_reduction_axes->list(range(1, len(group_shape)))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.(mean, variance)->tensorflow.nn.moments(reshaped_inputs, group_reduction_axes, keepdims=True)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.(gamma, beta)->self._get_reshaped_weights(input_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.broadcast_shape->self._create_broadcast_shape(input_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.gamma->tensorflow.reshape(self.gamma, broadcast_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.beta->tensorflow.reshape(self.beta, broadcast_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.input_spec->tensorflow.keras.layers.InputSpec(ndim=len(input_shape), axes={self.axis: dim})
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.gamma->self.add_weight(shape=shape, name='gamma', initializer=self.gamma_initializer, regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.beta->self.add_weight(shape=shape, name='beta', initializer=self.beta_initializer, regularizer=self.beta_regularizer, constraint=self.beta_constraint)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.kernel_norm_axes->tensorflow.constant([0, 1])
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.kernel_norm->tensorflow.sqrt(tf.reduce_sum(tf.square(self.weight_v), axis=self.kernel_norm_axes))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.kernel->tensorflow.Variable(tf.transpose(self.kernel), name='weight_v', trainable=True)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.weight_g->self.add_weight(name='weight_g', shape=(int(self.weight_v.shape[self.filter_axis]), 1, 1), initializer='ones', dtype=self.weight_v.dtype, trainable=True)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.bias->self.add_weight(name='bias', shape=(self.filters,), initializer='zeros', trainable=True)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.padded_inputs->tensorflow.pad(inputs, ((0, 0), (self.explicit_padding, self.explicit_padding), (0, 0)))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.output->self.call(input_values=inputs, training=False)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.conv->TFWav2Vec2WeightNormConv1D(filters=config.hidden_size, kernel_size=config.num_conv_pos_embeddings, groups=config.num_conv_pos_embedding_groups, explicit_padding=config.num_conv_pos_embeddings // 2, name='conv')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.activation->get_tf_activation(config.feat_extract_activation)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.hidden_states->self.dropout(hidden_states, training=inputs['training'])
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.padding->TFWav2Vec2SamePadLayer(config.num_conv_pos_embeddings)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.projection->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), bias_initializer='zeros', name='projection')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.dropout->tensorflow.keras.layers.Dropout(config.final_dropout)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.attention_mask->tensorflow.sequence_mask(output_lengths, dtype=hidden_states.dtype)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.attn_output->self.out_proj(attn_output)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.intermediate_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.intermediate_dense->tensorflow.keras.layers.Dense(units=config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), bias_initializer='zeros', name='intermediate_dense')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.intermediate_act_fn->get_tf_activation(config.hidden_act)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.output_dense->tensorflow.keras.layers.Dense(units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), bias_initializer='zeros', name='output_dense')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.output_dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.attention->TFWav2Vec2Attention(embed_dim=config.hidden_size, num_heads=config.num_attention_heads, dropout=config.attention_dropout, is_decoder=False, name='attention')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.feed_forward->TFWav2Vec2FeedForward(config, name='feed_forward')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='final_layer_norm')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.(hidden_states, attn_weights, _)->self.attention(hidden_states, attention_mask=attention_mask, training=training)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.pos_conv_embed->TFWav2Vec2PositionalConvEmbedding(config, name='pos_conv_embed')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.position_embeddings->self.pos_conv_embed(hidden_states)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.dropout_probability->numpy.random.uniform(0, 1)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.layer_outputs->layer_module(hidden_states=hidden_states, attention_mask=attention_mask, output_attentions=output_attentions, training=training)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.feature_extractor->TFWav2Vec2FeatureExtractor(config, name='feature_extractor')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.feature_projection->TFWav2Vec2FeatureProjection(config, name='feature_projection')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.encoder->TFWav2Vec2Encoder(config, name='encoder')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.masked_spec_embed->self.add_weight(shape=(self.config.hidden_size,), initializer='uniform', trainable=True, name='masked_spec_embed')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.input_lengths->self.wav2vec2._get_feat_extract_output_lengths(tf.reduce_sum(attention_mask, axis=-1))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.mask_time_indices->kwargs.get('mask_time_indices', None)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.mask_feature_indices->_compute_mask_indices((batch_size, hidden_size), mask_prob=self.config.mask_feature_prob, mask_length=self.config.mask_feature_length)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.inputs->input_values_processing(func=self.call, config=self.config, input_values=input_values, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training, kwargs_call=kwargs)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.output_lengths->self._get_feat_extract_output_lengths(tf.reduce_sum(inputs['attention_mask'], -1))
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.encoder_outputs->self.encoder(hidden_states, attention_mask=attention_mask, output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.input_values->tensorflow.convert_to_tensor(np.random.rand(1, 16000), tf.float32)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.wav2vec2->TFWav2Vec2MainLayer(config, name='wav2vec2')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.self.lm_head->tensorflow.keras.layers.Dense(config.vocab_size, name='lm_head')
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.logits->self.lm_head(hidden_states)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.labels_mask->tensorflow.cast(labels >= 0, tf.int32)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.target_lengths->tensorflow.reduce_sum(labels_mask, axis=-1)
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.flattened_labels->tensorflow.reshape(flattened_labels, [labels.shape[0], -1])
A:transformers.models.wav2vec2.modeling_tf_wav2vec2.loss->tensorflow.reduce_mean(loss)
transformers.TFWav2Vec2ForCTC(self,config:Wav2Vec2Config,*inputs,**kwargs)
transformers.TFWav2Vec2ForCTC.call(self,input_values:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,position_ids:Optional[tf.Tensor]=None,head_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=None,labels:Optional[tf.Tensor]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs:Any)->Union[TFCausalLMOutput, Tuple[tf.Tensor]]
transformers.TFWav2Vec2ForCTC.freeze_feature_extractor(self)
transformers.TFWav2Vec2ForCTC.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.TFWav2Vec2Model(self,config:Wav2Vec2Config,*inputs,**kwargs)
transformers.TFWav2Vec2Model.call(self,input_values:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,position_ids:Optional[tf.Tensor]=None,head_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs:Any)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.TFWav2Vec2Model.serving_output(self,output)
transformers.TFWav2Vec2PreTrainedModel(TFPreTrainedModel)
transformers.TFWav2Vec2PreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.TFWav2Vec2PreTrainedModel.serving(self,inputs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Attention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Attention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Attention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Attention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Encoder(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Encoder.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Encoder.call(self,hidden_states:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=False,output_hidden_states:Optional[bool]=False,return_dict:Optional[bool]=True,training:Optional[bool]=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderLayer(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderLayer.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=False,training:bool=False)->Tuple[tf.Tensor]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderLayerStableLayerNorm(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderLayerStableLayerNorm.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderLayerStableLayerNorm.call(self,hidden_states:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=False,training:bool=False)->Tuple[tf.Tensor]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderStableLayerNorm(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderStableLayerNorm.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2EncoderStableLayerNorm.call(self,hidden_states:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=False,output_hidden_states:Optional[bool]=False,return_dict:Optional[bool]=True,training:Optional[bool]=False)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeatureExtractor(self,config:Wav2Vec2Config,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeatureExtractor.__init__(self,config:Wav2Vec2Config,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeatureExtractor.call(self,input_values)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeatureProjection(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeatureProjection.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeatureProjection.call(self,hidden_states:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeedForward(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeedForward.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2FeedForward.call(self,hidden_states:tf.Tensor,training:bool=False)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2ForCTC(self,config:Wav2Vec2Config,*inputs,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2ForCTC.__init__(self,config:Wav2Vec2Config,*inputs,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2ForCTC.call(self,input_values:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,position_ids:Optional[tf.Tensor]=None,head_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=None,labels:Optional[tf.Tensor]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:Optional[bool]=False,**kwargs:Any)->Union[TFCausalLMOutput, Tuple[tf.Tensor]]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2ForCTC.freeze_feature_extractor(self)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2ForCTC.serving_output(self,output:TFCausalLMOutput)->TFCausalLMOutput
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm(self,groups:int=32,axis:int=-1,epsilon:float=0.001,center:bool=True,scale:bool=True,beta_initializer:tf.keras.initializers.Initializer='zeros',gamma_initializer:tf.keras.initializers.Initializer='ones',beta_regularizer:tf.keras.regularizers.Regularizer=None,gamma_regularizer:tf.keras.regularizers.Regularizer=None,beta_constraint:tf.keras.constraints.Constraint=None,gamma_constraint:tf.keras.constraints.Constraint=None,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm.__init__(self,groups:int=32,axis:int=-1,epsilon:float=0.001,center:bool=True,scale:bool=True,beta_initializer:tf.keras.initializers.Initializer='zeros',gamma_initializer:tf.keras.initializers.Initializer='ones',beta_regularizer:tf.keras.regularizers.Regularizer=None,gamma_regularizer:tf.keras.regularizers.Regularizer=None,beta_constraint:tf.keras.constraints.Constraint=None,gamma_constraint:tf.keras.constraints.Constraint=None,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._add_beta_weight(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._add_gamma_weight(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._apply_normalization(self,reshaped_inputs,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._check_axis(self)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._check_if_input_shape_is_none(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._check_size_of_dimensions(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._create_broadcast_shape(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._create_input_spec(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._get_reshaped_weights(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._reshape_into_groups(self,inputs,input_shape,tensor_input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm._set_number_of_groups_for_instance_norm(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm.build(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm.call(self,inputs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm.compute_output_shape(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNorm.get_config(self)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNormConvLayer(self,config:Wav2Vec2Config,layer_id:int=0,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNormConvLayer.__init__(self,config:Wav2Vec2Config,layer_id:int=0,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2GroupNormConvLayer.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2LayerNormConvLayer(self,config:Wav2Vec2Config,layer_id:int=0,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2LayerNormConvLayer.__init__(self,config:Wav2Vec2Config,layer_id:int=0,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2LayerNormConvLayer.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2MainLayer(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2MainLayer.__init__(self,config:Wav2Vec2Config,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2MainLayer._get_feat_extract_output_lengths(self,input_lengths:tf.Tensor)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2MainLayer._mask_hidden_states(self,hidden_states:tf.Tensor,mask_time_indices:Optional[tf.Tensor]=None,training:bool=False)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2MainLayer.build(self,input_shape:tf.TensorShape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2MainLayer.call(self,input_values:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,position_ids:Optional[tf.Tensor]=None,head_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:Optional[tf.Tensor]=None,output_hidden_states:Optional[tf.Tensor]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Model(self,config:Wav2Vec2Config,*inputs,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Model.__init__(self,config:Wav2Vec2Config,*inputs,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Model.call(self,input_values:tf.Tensor,attention_mask:Optional[tf.Tensor]=None,token_type_ids:Optional[tf.Tensor]=None,position_ids:Optional[tf.Tensor]=None,head_mask:Optional[tf.Tensor]=None,inputs_embeds:Optional[tf.Tensor]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,training:bool=False,**kwargs:Any)->Union[TFBaseModelOutput, Tuple[tf.Tensor]]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2Model.serving_output(self,output)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2NoLayerNormConvLayer(self,config:Wav2Vec2Config,layer_id:int=0,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2NoLayerNormConvLayer.__init__(self,config:Wav2Vec2Config,layer_id:int=0,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2NoLayerNormConvLayer.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2PositionalConvEmbedding(self,config:Wav2Vec2Config,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2PositionalConvEmbedding.__init__(self,config:Wav2Vec2Config,**kwargs:Any)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2PositionalConvEmbedding.call(self,hidden_states:tf.Tensor)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2PreTrainedModel(TFPreTrainedModel)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2PreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2PreTrainedModel.serving(self,inputs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2SamePadLayer(self,num_conv_pos_embeddings,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2SamePadLayer.__init__(self,num_conv_pos_embeddings,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2SamePadLayer.call(self,hidden_states)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2WeightNormConv1D(self,filters,kernel_size,groups,explicit_padding,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2WeightNormConv1D.__init__(self,filters,kernel_size,groups,explicit_padding,**kwargs)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2WeightNormConv1D._init_norm(self)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2WeightNormConv1D._normalize_kernel(self)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2WeightNormConv1D.build(self,input_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.TFWav2Vec2WeightNormConv1D.call(self,inputs)
transformers.models.wav2vec2.modeling_tf_wav2vec2._compute_mask_indices(shape:Tuple[int,int],mask_prob:float,mask_length:int,min_masks:int=0)->tf.Tensor
transformers.models.wav2vec2.modeling_tf_wav2vec2._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.wav2vec2.modeling_tf_wav2vec2._sample_without_replacement(distribution,num_samples)
transformers.models.wav2vec2.modeling_tf_wav2vec2._scatter_values_on_batch_indices(values,batch_indices,output_shape)
transformers.models.wav2vec2.modeling_tf_wav2vec2.input_values_processing(func,config,input_values,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/__init__.py----------------------------------------
A:transformers.models.wav2vec2.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/wav2vec2/tokenization_wav2vec2.py----------------------------------------
A:transformers.models.wav2vec2.tokenization_wav2vec2.logger->utils.logging.get_logger(__name__)
A:transformers.models.wav2vec2.tokenization_wav2vec2.self.encoder->json.load(vocab_handle)
A:transformers.models.wav2vec2.tokenization_wav2vec2.self._word_delimiter_token->self.convert_tokens_to_ids(value)
A:transformers.models.wav2vec2.tokenization_wav2vec2.text->self.convert_tokens_to_string(result)
A:transformers.models.wav2vec2.tokenization_wav2vec2.result->self.decoder.get(index, self.unk_token)
A:transformers.models.wav2vec2.tokenization_wav2vec2.filtered_tokens->self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
A:transformers.models.wav2vec2.tokenization_wav2vec2.string->string.lower().lower()
A:transformers.models.wav2vec2.tokenization_wav2vec2.clean_text->self.clean_up_tokenization(text)
A:transformers.models.wav2vec2.tokenization_wav2vec2.vocab_file->os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + VOCAB_FILES_NAMES['vocab_file'])
A:transformers.models.wav2vec2.tokenization_wav2vec2.token->token.lower().lower()
A:transformers.models.wav2vec2.tokenization_wav2vec2.added_tok_encoder->dict(((tok, len(self) + i) for (i, tok) in enumerate(tokens_to_add)))
A:transformers.models.wav2vec2.tokenization_wav2vec2.is_batched->bool(isinstance(raw_speech, (list, tuple)) and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list))))
A:transformers.models.wav2vec2.tokenization_wav2vec2.raw_speech->numpy.asarray(raw_speech)
A:transformers.models.wav2vec2.tokenization_wav2vec2.encoded_inputs->BatchEncoding({'input_values': raw_speech})
A:transformers.models.wav2vec2.tokenization_wav2vec2.padded_inputs->self.pad(encoded_inputs, padding=padding, max_length=max_length, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=self.return_attention_mask, return_tensors=return_tensors, verbose=verbose)
transformers.Wav2Vec2CTCTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',word_delimiter_token='|',do_lower_case=False,**kwargs)
transformers.Wav2Vec2CTCTokenizer._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.Wav2Vec2CTCTokenizer._convert_id_to_token(self,index:int)->str
transformers.Wav2Vec2CTCTokenizer._convert_token_to_id(self,token:str)->int
transformers.Wav2Vec2CTCTokenizer._decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,group_tokens:bool=True,spaces_between_special_tokens:bool=False)->str
transformers.Wav2Vec2CTCTokenizer._tokenize(self,text,**kwargs)
transformers.Wav2Vec2CTCTokenizer.convert_tokens_to_string(self,tokens:List[str],group_tokens:bool=True,spaces_between_special_tokens:bool=False)->str
transformers.Wav2Vec2CTCTokenizer.get_vocab(self)->Dict
transformers.Wav2Vec2CTCTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.Wav2Vec2CTCTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.Wav2Vec2CTCTokenizer.vocab_size(self)->int
transformers.Wav2Vec2CTCTokenizer.word_delimiter_token(self)->str
transformers.Wav2Vec2CTCTokenizer.word_delimiter_token(self,value)
transformers.Wav2Vec2CTCTokenizer.word_delimiter_token_id(self)->Optional[int]
transformers.Wav2Vec2CTCTokenizer.word_delimiter_token_id(self,value)
transformers.Wav2Vec2Tokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',word_delimiter_token='|',do_lower_case=False,do_normalize=False,return_attention_mask=False,**kwargs)
transformers.Wav2Vec2Tokenizer._convert_id_to_token(self,index:int)->str
transformers.Wav2Vec2Tokenizer._convert_token_to_id(self,token:str)->int
transformers.Wav2Vec2Tokenizer._decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.Wav2Vec2Tokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.Wav2Vec2Tokenizer.get_vocab(self)->Dict
transformers.Wav2Vec2Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.Wav2Vec2Tokenizer.vocab_size(self)->int
transformers.Wav2Vec2Tokenizer.word_delimiter_token(self)->str
transformers.Wav2Vec2Tokenizer.word_delimiter_token(self,value)
transformers.Wav2Vec2Tokenizer.word_delimiter_token_id(self)->Optional[int]
transformers.Wav2Vec2Tokenizer.word_delimiter_token_id(self,value)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',word_delimiter_token='|',do_lower_case=False,**kwargs)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',word_delimiter_token='|',do_lower_case=False,**kwargs)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer._convert_id_to_token(self,index:int)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer._convert_token_to_id(self,token:str)->int
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer._decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,group_tokens:bool=True,spaces_between_special_tokens:bool=False)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer._tokenize(self,text,**kwargs)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.convert_tokens_to_string(self,tokens:List[str],group_tokens:bool=True,spaces_between_special_tokens:bool=False)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.get_vocab(self)->Dict
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.prepare_for_tokenization(self,text,is_split_into_words=False,**kwargs)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.vocab_size(self)->int
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.word_delimiter_token(self)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.word_delimiter_token(self,value)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.word_delimiter_token_id(self)->Optional[int]
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2CTCTokenizer.word_delimiter_token_id(self,value)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',word_delimiter_token='|',do_lower_case=False,do_normalize=False,return_attention_mask=False,**kwargs)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',unk_token='<unk>',pad_token='<pad>',word_delimiter_token='|',do_lower_case=False,do_normalize=False,return_attention_mask=False,**kwargs)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer._convert_id_to_token(self,index:int)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer._convert_token_to_id(self,token:str)->int
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer._decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True,**kwargs)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.get_vocab(self)->Dict
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.save_vocabulary(self,save_directory:str,filename_prefix:Optional[str]=None)->Tuple[str]
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.vocab_size(self)->int
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.word_delimiter_token(self)->str
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.word_delimiter_token(self,value)
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.word_delimiter_token_id(self)->Optional[int]
transformers.models.wav2vec2.tokenization_wav2vec2.Wav2Vec2Tokenizer.word_delimiter_token_id(self,value)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/modeling_tf_bart.py----------------------------------------
A:transformers.models.bart.modeling_tf_bart.logger->utils.logging.get_logger(__name__)
A:transformers.models.bart.modeling_tf_bart.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.models.bart.modeling_tf_bart.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.models.bart.modeling_tf_bart.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))
A:transformers.models.bart.modeling_tf_bart.mask_cond->tensorflow.range(shape_list(mask)[-1])
A:transformers.models.bart.modeling_tf_bart.mask->tensorflow.cast(mask, dtype=one_cst.dtype)
A:transformers.models.bart.modeling_tf_bart.one_cst->tensorflow.constant(1.0)
A:transformers.models.bart.modeling_tf_bart.expanded_mask->tensorflow.tile(mask[:, None, None, :], (1, 1, tgt_len, 1))
A:transformers.models.bart.modeling_tf_bart.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.bart.modeling_tf_bart.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.models.bart.modeling_tf_bart.self.k_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='k_proj')
A:transformers.models.bart.modeling_tf_bart.self.q_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='q_proj')
A:transformers.models.bart.modeling_tf_bart.self.v_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='v_proj')
A:transformers.models.bart.modeling_tf_bart.self.out_proj->tensorflow.keras.layers.Dense(embed_dim, use_bias=bias, name='out_proj')
A:transformers.models.bart.modeling_tf_bart.(bsz, tgt_len, embed_dim)->shape_list(hidden_states)
A:transformers.models.bart.modeling_tf_bart.key_states->tensorflow.reshape(key_states, proj_shape)
A:transformers.models.bart.modeling_tf_bart.value_states->tensorflow.reshape(value_states, proj_shape)
A:transformers.models.bart.modeling_tf_bart.query_states->tensorflow.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)
A:transformers.models.bart.modeling_tf_bart.attn_weights->tensorflow.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))
A:transformers.models.bart.modeling_tf_bart.attention_mask->_expand_mask(inputs['attention_mask'])
A:transformers.models.bart.modeling_tf_bart.attn_probs->self.dropout(attn_weights, training=training)
A:transformers.models.bart.modeling_tf_bart.attn_output->self.out_proj(attn_output)
A:transformers.models.bart.modeling_tf_bart.self.self_attn->TFBartAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, name='self_attn', is_decoder=True)
A:transformers.models.bart.modeling_tf_bart.self.self_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='self_attn_layer_norm')
A:transformers.models.bart.modeling_tf_bart.self.activation_fn->get_tf_activation(config.activation_function)
A:transformers.models.bart.modeling_tf_bart.self.activation_dropout->tensorflow.keras.layers.Dropout(config.activation_dropout)
A:transformers.models.bart.modeling_tf_bart.self.fc1->tensorflow.keras.layers.Dense(config.decoder_ffn_dim, name='fc1')
A:transformers.models.bart.modeling_tf_bart.self.fc2->tensorflow.keras.layers.Dense(self.embed_dim, name='fc2')
A:transformers.models.bart.modeling_tf_bart.self.final_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='final_layer_norm')
A:transformers.models.bart.modeling_tf_bart.(hidden_states, self_attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.bart.modeling_tf_bart.hidden_states->self.dropout(hidden_states, training=inputs['training'])
A:transformers.models.bart.modeling_tf_bart.self.encoder_attn->TFBartAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, name='encoder_attn', is_decoder=True)
A:transformers.models.bart.modeling_tf_bart.self.encoder_attn_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='encoder_attn_layer_norm')
A:transformers.models.bart.modeling_tf_bart.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask)
A:transformers.models.bart.modeling_tf_bart.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value)
A:transformers.models.bart.modeling_tf_bart.input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.bart.modeling_tf_bart.decoder_input_ids->tensorflow.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)
A:transformers.models.bart.modeling_tf_bart.output->self.call(inputs)
A:transformers.models.bart.modeling_tf_bart.self.embed_positions->TFBartLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model, name='embed_positions')
A:transformers.models.bart.modeling_tf_bart.self.layernorm_embedding->tensorflow.keras.layers.LayerNormalization(epsilon=1e-05, name='layernorm_embedding')
A:transformers.models.bart.modeling_tf_bart.inputs->input_processing(func=self.call, config=self.config, input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, head_mask=head_mask, decoder_head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, encoder_outputs=encoder_outputs, past_key_values=past_key_values, inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, labels=labels, training=training, kwargs_call=kwargs)
A:transformers.models.bart.modeling_tf_bart.input_shape->shape_list(inputs['input_ids'])
A:transformers.models.bart.modeling_tf_bart.embed_pos->self.embed_positions(input_shape)
A:transformers.models.bart.modeling_tf_bart.dropout_probability->random.uniform(0, 1)
A:transformers.models.bart.modeling_tf_bart.(hidden_states, attn)->encoder_layer(hidden_states, attention_mask, inputs['head_mask'][idx] if inputs['head_mask'] is not None else None)
A:transformers.models.bart.modeling_tf_bart.combined_attention_mask->_expand_mask(tf.ones((input_shape[0], input_shape[1] + past_key_values_length)), tgt_len=input_shape[-1])
A:transformers.models.bart.modeling_tf_bart.inputs['encoder_attention_mask']->_expand_mask(inputs['encoder_attention_mask'], tgt_len=input_shape[-1])
A:transformers.models.bart.modeling_tf_bart.(hidden_states, layer_self_attn, layer_cross_attn, present_key_value)->decoder_layer(hidden_states, attention_mask=combined_attention_mask, encoder_hidden_states=inputs['encoder_hidden_states'], encoder_attention_mask=inputs['encoder_attention_mask'], layer_head_mask=inputs['head_mask'][idx] if inputs['head_mask'] is not None else None, cross_attn_layer_head_mask=inputs['cross_attn_head_mask'][idx] if inputs['cross_attn_head_mask'] is not None else None, past_key_value=past_key_value)
A:transformers.models.bart.modeling_tf_bart.all_self_attns->list(all_self_attns)
A:transformers.models.bart.modeling_tf_bart.all_cross_attns->list(all_cross_attns)
A:transformers.models.bart.modeling_tf_bart.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name='model.shared')
A:transformers.models.bart.modeling_tf_bart.embed_tokens->TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)
A:transformers.models.bart.modeling_tf_bart.self.encoder->TFBartEncoder(config, embed_tokens, name='encoder')
A:transformers.models.bart.modeling_tf_bart.self.decoder->TFBartDecoder(config, embed_tokens, name='decoder')
A:transformers.models.bart.modeling_tf_bart.inputs['decoder_input_ids']->shift_tokens_right(inputs['labels'], self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.bart.modeling_tf_bart.inputs['encoder_outputs']->inputs['encoder_outputs'].to_tuple().to_tuple()
A:transformers.models.bart.modeling_tf_bart.decoder_outputs->self.decoder(inputs['decoder_input_ids'], attention_mask=inputs['decoder_attention_mask'], encoder_hidden_states=inputs['encoder_outputs'][0], encoder_attention_mask=inputs['attention_mask'], head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.bart.modeling_tf_bart.self.model->TFBartMainLayer(config, load_weight_prefix=load_weight_prefix, name='model')
A:transformers.models.bart.modeling_tf_bart.outputs->self.model(inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=inputs['decoder_input_ids'], encoder_outputs=inputs['encoder_outputs'], decoder_attention_mask=inputs['decoder_attention_mask'], head_mask=inputs['head_mask'], decoder_head_mask=inputs['decoder_head_mask'], cross_attn_head_mask=inputs['cross_attn_head_mask'], past_key_values=inputs['past_key_values'], inputs_embeds=inputs['inputs_embeds'], decoder_inputs_embeds=inputs['decoder_inputs_embeds'], use_cache=inputs['use_cache'], output_attentions=inputs['output_attentions'], output_hidden_states=inputs['output_hidden_states'], return_dict=inputs['return_dict'], training=inputs['training'])
A:transformers.models.bart.modeling_tf_bart.self.final_logits_bias->self.add_weight(name='final_logits_bias', shape=[1, config.vocab_size], initializer='zeros', trainable=False)
A:transformers.models.bart.modeling_tf_bart.inputs['labels']->tensorflow.where(inputs['labels'] == self.config.pad_token_id, tf.fill(shape_list(inputs['labels']), -100), inputs['labels'])
A:transformers.models.bart.modeling_tf_bart.lm_logits->self.model.shared(outputs[0], mode='linear')
A:transformers.models.bart.modeling_tf_bart.encoder_outputs->TFBaseModelOutput(last_hidden_state=encoder_outputs)
transformers.TFBartForConditionalGeneration(self,config,load_weight_prefix=None,*inputs,**kwargs)
transformers.TFBartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.TFBartForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFBartForConditionalGeneration.get_bias(self)
transformers.TFBartForConditionalGeneration.get_decoder(self)
transformers.TFBartForConditionalGeneration.get_encoder(self)
transformers.TFBartForConditionalGeneration.get_output_embeddings(self)
transformers.TFBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.TFBartForConditionalGeneration.serving_output(self,output)
transformers.TFBartForConditionalGeneration.set_bias(self,value)
transformers.TFBartForConditionalGeneration.set_output_embeddings(self,value)
transformers.TFBartModel(self,config:BartConfig,load_weight_prefix=None,*inputs,**kwargs)
transformers.TFBartModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFBartModel.get_decoder(self)
transformers.TFBartModel.get_encoder(self)
transformers.TFBartModel.serving_output(self,output)
transformers.TFBartPretrainedModel(TFPreTrainedModel)
transformers.TFBartPretrainedModel.dummy_inputs(self)
transformers.TFBartPretrainedModel.serving(self,inputs)
transformers.models.bart.modeling_tf_bart.TFBartAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartAttention._shape(self,tensor:tf.Tensor,seq_len:int,bsz:int)
transformers.models.bart.modeling_tf_bart.TFBartAttention.call(self,hidden_states:tf.Tensor,key_value_states:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[Tuple[tf.Tensor]]]=None,attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,training=False)->Tuple[tf.Tensor, Optional[tf.Tensor]]
transformers.models.bart.modeling_tf_bart.TFBartDecoder(self,config:BartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartDecoder.__init__(self,config:BartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartDecoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartDecoder.get_embed_tokens(self)
transformers.models.bart.modeling_tf_bart.TFBartDecoder.set_embed_tokens(self,embed_tokens)
transformers.models.bart.modeling_tf_bart.TFBartDecoderLayer(self,config:BartConfig,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartDecoderLayer.__init__(self,config:BartConfig,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartDecoderLayer.call(self,hidden_states,attention_mask:Optional[tf.Tensor]=None,encoder_hidden_states:Optional[tf.Tensor]=None,encoder_attention_mask:Optional[tf.Tensor]=None,layer_head_mask:Optional[tf.Tensor]=None,cross_attn_layer_head_mask:Optional[tf.Tensor]=None,past_key_value:Optional[Tuple[tf.Tensor]]=None,training=False)->Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]
transformers.models.bart.modeling_tf_bart.TFBartEncoder(self,config:BartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartEncoder.__init__(self,config:BartConfig,embed_tokens:Optional[TFSharedEmbeddings]=None,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartEncoder.call(self,input_ids=None,inputs_embeds=None,attention_mask=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartEncoder.get_embed_tokens(self)
transformers.models.bart.modeling_tf_bart.TFBartEncoder.set_embed_tokens(self,embed_tokens)
transformers.models.bart.modeling_tf_bart.TFBartEncoderLayer(self,config:BartConfig,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartEncoderLayer.__init__(self,config:BartConfig,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartEncoderLayer.call(self,hidden_states:tf.Tensor,attention_mask:tf.Tensor,layer_head_mask:tf.Tensor,training=False)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration(self,config,load_weight_prefix=None,*inputs,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.__init__(self,config,load_weight_prefix=None,*inputs,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[TFBaseModelOutput]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.get_bias(self)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.get_decoder(self)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.get_encoder(self)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.get_output_embeddings(self)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,**kwargs)->Dict
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.serving_output(self,output)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.set_bias(self,value)
transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration.set_output_embeddings(self,value)
transformers.models.bart.modeling_tf_bart.TFBartLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartLearnedPositionalEmbedding.call(self,input_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.bart.modeling_tf_bart.TFBartMainLayer(self,config:BartConfig,load_weight_prefix=None,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartMainLayer.__init__(self,config:BartConfig,load_weight_prefix=None,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartMainLayer.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartMainLayer.get_input_embeddings(self)
transformers.models.bart.modeling_tf_bart.TFBartMainLayer.set_input_embeddings(self,new_embeddings)
transformers.models.bart.modeling_tf_bart.TFBartModel(self,config:BartConfig,load_weight_prefix=None,*inputs,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartModel.__init__(self,config:BartConfig,load_weight_prefix=None,*inputs,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartModel.call(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs:Optional[Union[Tuple,TFBaseModelOutput]]=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.models.bart.modeling_tf_bart.TFBartModel.get_decoder(self)
transformers.models.bart.modeling_tf_bart.TFBartModel.get_encoder(self)
transformers.models.bart.modeling_tf_bart.TFBartModel.serving_output(self,output)
transformers.models.bart.modeling_tf_bart.TFBartPretrainedModel(TFPreTrainedModel)
transformers.models.bart.modeling_tf_bart.TFBartPretrainedModel.dummy_inputs(self)
transformers.models.bart.modeling_tf_bart.TFBartPretrainedModel.serving(self,inputs)
transformers.models.bart.modeling_tf_bart._expand_mask(mask:tf.Tensor,tgt_len:Optional[int]=None,past_key_values_length:int=0)
transformers.models.bart.modeling_tf_bart._make_causal_mask(input_ids_shape:tf.TensorShape,past_key_values_length:int=0)
transformers.models.bart.modeling_tf_bart.shift_tokens_right(input_ids:tf.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/configuration_bart.py----------------------------------------
A:transformers.models.bart.configuration_bart.logger->utils.logging.get_logger(__name__)
transformers.BartConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,use_cache=True,num_labels=3,pad_token_id=1,bos_token_id=0,eos_token_id=2,is_encoder_decoder=True,decoder_start_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.BartConfig.hidden_size(self)->int
transformers.BartConfig.num_attention_heads(self)->int
transformers.models.bart.configuration_bart.BartConfig(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,use_cache=True,num_labels=3,pad_token_id=1,bos_token_id=0,eos_token_id=2,is_encoder_decoder=True,decoder_start_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.models.bart.configuration_bart.BartConfig.__init__(self,vocab_size=50265,max_position_embeddings=1024,encoder_layers=12,encoder_ffn_dim=4096,encoder_attention_heads=16,decoder_layers=12,decoder_ffn_dim=4096,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,activation_function='gelu',d_model=1024,dropout=0.1,attention_dropout=0.0,activation_dropout=0.0,init_std=0.02,classifier_dropout=0.0,scale_embedding=False,gradient_checkpointing=False,use_cache=True,num_labels=3,pad_token_id=1,bos_token_id=0,eos_token_id=2,is_encoder_decoder=True,decoder_start_token_id=2,forced_eos_token_id=2,**kwargs)
transformers.models.bart.configuration_bart.BartConfig.hidden_size(self)->int
transformers.models.bart.configuration_bart.BartConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.val->dct.pop(old)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.sd->torch.load(checkpoint_path, map_location='cpu')
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.hub_interface->torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.lin_layer->torch.nn.Linear(vocab_size, emb_size, bias=False)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.bart->load_xsum_checkpoint(checkpoint_path)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.hf_checkpoint_name->checkpoint_path.replace('.', '-')
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.config->transformers.BartConfig.from_pretrained(hf_checkpoint_name)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.tokens->load_xsum_checkpoint(checkpoint_path).encode(SAMPLE_TEXT).unsqueeze(0)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.tokens2->transformers.BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.state_dict->load_xsum_checkpoint(checkpoint_path).model.state_dict()
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.model->BartForConditionalGeneration(config).eval()
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.fairseq_output->load_xsum_checkpoint(checkpoint_path).extract_features(tokens)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.model.lm_head->make_linear_from_emb(model.model.shared)
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.convert_bart_checkpoint(checkpoint_path,pytorch_dump_folder_path,hf_checkpoint_name=None)
transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.load_xsum_checkpoint(checkpoint_path)
transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.make_linear_from_emb(emb)
transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.remove_ignore_keys_(state_dict)
transformers.models.bart.convert_bart_original_pytorch_checkpoint_to_pytorch.rename_key(dct,old,new)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/modeling_flax_bart.py----------------------------------------
A:transformers.models.bart.modeling_flax_bart.logger->utils.logging.get_logger(__name__)
A:transformers.models.bart.modeling_flax_bart.shifted_input_ids->jax.numpy.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)
A:transformers.models.bart.modeling_flax_bart.dense->partial(nn.Dense, self.embed_dim, use_bias=self.bias, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.self.out_proj->flax.linen.Dense(self.num_classes, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.self.dropout_layer->flax.linen.Dropout(rate=self.config.dropout)
A:transformers.models.bart.modeling_flax_bart.self.causal_mask->make_causal_mask(jnp.ones((1, self.config.max_position_embeddings), dtype='bool'), dtype='bool')
A:transformers.models.bart.modeling_flax_bart.is_initialized->self.has_variable('cache', 'cached_key')
A:transformers.models.bart.modeling_flax_bart.cached_key->self.variable('cache', 'cached_key', jnp.zeros, key.shape, key.dtype)
A:transformers.models.bart.modeling_flax_bart.cached_value->self.variable('cache', 'cached_value', jnp.zeros, value.shape, value.dtype)
A:transformers.models.bart.modeling_flax_bart.cache_index->self.variable('cache', 'cache_index', lambda : jnp.array(0, dtype=jnp.int32))
A:transformers.models.bart.modeling_flax_bart.key->jax.lax.dynamic_update_slice(cached_key.value, key, indices)
A:transformers.models.bart.modeling_flax_bart.value->jax.lax.dynamic_update_slice(cached_value.value, value, indices)
A:transformers.models.bart.modeling_flax_bart.pad_mask->jax.numpy.broadcast_to(jnp.arange(max_length) < cur_index + num_updated_cache_vectors, tuple(batch_dims) + (1, num_updated_cache_vectors, max_length))
A:transformers.models.bart.modeling_flax_bart.attention_mask->jax.numpy.ones_like(input_ids)
A:transformers.models.bart.modeling_flax_bart.query_states->self._split_heads(query_states)
A:transformers.models.bart.modeling_flax_bart.key_states->self._split_heads(key_states)
A:transformers.models.bart.modeling_flax_bart.value_states->self._split_heads(value_states)
A:transformers.models.bart.modeling_flax_bart.causal_mask->jax.numpy.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])
A:transformers.models.bart.modeling_flax_bart.(key_states, value_states, attention_mask)->self._concatenate_to_cache(key_states, value_states, query_states, attention_mask)
A:transformers.models.bart.modeling_flax_bart.attention_bias->jax.lax.select(attention_mask > 0, jnp.full(attention_mask.shape, 0.0).astype(self.dtype), jnp.full(attention_mask.shape, float('-inf')).astype(self.dtype))
A:transformers.models.bart.modeling_flax_bart.dropout_rng->self.make_rng('dropout')
A:transformers.models.bart.modeling_flax_bart.attn_weights->dot_product_attention_weights(query_states, key_states, bias=attention_bias, dropout_rng=dropout_rng, dropout_rate=self.dropout, broadcast_dropout=True, deterministic=deterministic, dtype=self.dtype, precision=None)
A:transformers.models.bart.modeling_flax_bart.attn_output->self.out_proj(attn_output)
A:transformers.models.bart.modeling_flax_bart.self.self_attn->FlaxBartAttention(config=self.config, embed_dim=self.embed_dim, num_heads=self.config.decoder_attention_heads, dropout=self.config.attention_dropout, causal=True)
A:transformers.models.bart.modeling_flax_bart.self.self_attn_layer_norm->flax.linen.LayerNorm(dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.self.acticvation_dropout_layer->flax.linen.Dropout(rate=self.config.activation_dropout)
A:transformers.models.bart.modeling_flax_bart.self.fc1->flax.linen.Dense(self.config.encoder_ffn_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.self.fc2->flax.linen.Dense(self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.self.final_layer_norm->flax.linen.LayerNorm(dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.(hidden_states, attn_weights)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask)
A:transformers.models.bart.modeling_flax_bart.hidden_states->self.dropout_layer(hidden_states, deterministic=deterministic)
A:transformers.models.bart.modeling_flax_bart.dropout_probability->random.uniform(0, 1)
A:transformers.models.bart.modeling_flax_bart.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, init_cache=init_cache, output_attentions=output_attentions, deterministic=deterministic)
A:transformers.models.bart.modeling_flax_bart.self.encoder_attn->FlaxBartAttention(config=self.config, embed_dim=self.embed_dim, num_heads=self.config.decoder_attention_heads, dropout=self.config.attention_dropout)
A:transformers.models.bart.modeling_flax_bart.self.encoder_attn_layer_norm->flax.linen.LayerNorm(dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.(hidden_states, self_attn_weights)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, init_cache=init_cache)
A:transformers.models.bart.modeling_flax_bart.(hidden_states, cross_attn_weights)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask)
A:transformers.models.bart.modeling_flax_bart.self.dense->flax.linen.Dense(self.inner_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.self.dropout->flax.linen.Dropout(rate=self.pooler_dropout)
A:transformers.models.bart.modeling_flax_bart.self.embed_tokens->flax.linen.Embed(self.config.vocab_size, embed_dim, embedding_init=jax.nn.initializers.normal(self.config.init_std, self.dtype), dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.self.embed_positions->flax.linen.Embed(self.config.max_position_embeddings + self.offset, embed_dim, embedding_init=jax.nn.initializers.normal(self.config.init_std, self.dtype), dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.self.layers->FlaxBartDecoderLayerCollection(self.config, self.dtype)
A:transformers.models.bart.modeling_flax_bart.self.layernorm_embedding->flax.linen.LayerNorm(dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.input_ids->jax.ops.index_update(input_ids, (..., -1), self.config.eos_token_id)
A:transformers.models.bart.modeling_flax_bart.embed_pos->self.embed_positions(position_ids + self.offset)
A:transformers.models.bart.modeling_flax_bart.outputs->self.model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, position_ids=position_ids, decoder_position_ids=decoder_position_ids, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, deterministic=deterministic)
A:transformers.models.bart.modeling_flax_bart.positions->self.embed_positions(position_ids + self.offset)
A:transformers.models.bart.modeling_flax_bart.self.shared->flax.linen.Embed(self.config.vocab_size, self.config.d_model, embedding_init=jax.nn.initializers.normal(self.config.init_std, self.dtype), dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.self.encoder->FlaxBartEncoder(self.config, dtype=self.dtype, embed_tokens=self.shared)
A:transformers.models.bart.modeling_flax_bart.self.decoder->FlaxBartDecoder(self.config, dtype=self.dtype, embed_tokens=self.shared)
A:transformers.models.bart.modeling_flax_bart.encoder_outputs->self.encoder(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, deterministic=deterministic)
A:transformers.models.bart.modeling_flax_bart.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, position_ids=decoder_position_ids, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, deterministic=deterministic)
A:transformers.models.bart.modeling_flax_bart.module->self.module_class(config=config, dtype=dtype, **kwargs)
A:transformers.models.bart.modeling_flax_bart.decoder_attention_mask->jax.numpy.ones((batch_size, sequence_length))
A:transformers.models.bart.modeling_flax_bart.position_ids->jax.numpy.broadcast_to(jnp.arange(seq_length, dtype='i4')[None, :], (batch_size, seq_length))
A:transformers.models.bart.modeling_flax_bart.decoder_position_ids->jax.numpy.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))
A:transformers.models.bart.modeling_flax_bart.(params_rng, dropout_rng)->jax.random.split(rng)
A:transformers.models.bart.modeling_flax_bart.decoder_input_ids->shift_tokens_right(input_ids, self.config.pad_token_id, decoder_start_token_id=self.config.decoder_start_token_id)
A:transformers.models.bart.modeling_flax_bart.decoder_module->self.module_class(config=config, dtype=dtype, **kwargs)._get_decoder_module()
A:transformers.models.bart.modeling_flax_bart.init_variables->self.module.init(jax.random.PRNGKey(0), decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, decoder_position_ids=decoder_position_ids, encoder_hidden_states=encoder_outputs[0], init_cache=True, method=_decoder_forward)
A:transformers.models.bart.modeling_flax_bart.encode_module->self.module_class(config=config, dtype=dtype, **kwargs)._get_encoder_module()
A:transformers.models.bart.modeling_flax_bart.encoder_attention_mask->jax.numpy.ones((batch_size, sequence_length))
A:transformers.models.bart.modeling_flax_bart.outputs['past_key_values']->unfreeze(past['cache'])
A:transformers.models.bart.modeling_flax_bart.self.model->FlaxBartModule(config=self.config, dtype=self.dtype)
A:transformers.models.bart.modeling_flax_bart.self.lm_head->flax.linen.Dense(self.model.shared.num_embeddings, use_bias=False, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.self.final_logits_bias->self.param('final_logits_bias', self.bias_init, (1, self.model.shared.num_embeddings))
A:transformers.models.bart.modeling_flax_bart.lm_logits->self.module_class(config=config, dtype=dtype, **kwargs).lm_head(hidden_states)
A:transformers.models.bart.modeling_flax_bart.past_key_values->self.init_cache(batch_size, max_length, encoder_outputs)
A:transformers.models.bart.modeling_flax_bart.extended_attention_mask->jax.lax.dynamic_update_slice(extended_attention_mask, decoder_attention_mask, (0, 0))
A:transformers.models.bart.modeling_flax_bart.self.classification_head->FlaxBartClassificationHead(config=self.config, inner_dim=self.config.d_model, num_classes=self.num_labels if self.num_labels is not None else self.config.num_labels, pooler_dropout=self.config.classifier_dropout)
A:transformers.models.bart.modeling_flax_bart.eos_mask->jax.numpy.where(eos_mask_noised == eos_mask_noised.max(1).reshape(-1, 1), 1, 0)
A:transformers.models.bart.modeling_flax_bart.sentence_representation->jax.numpy.einsum('ijk, ij -> ijk', hidden_states, eos_mask).sum(1)
A:transformers.models.bart.modeling_flax_bart.logits->self.qa_outputs(sequence_output)
A:transformers.models.bart.modeling_flax_bart.self.qa_outputs->flax.linen.Dense(self.num_labels, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype))
A:transformers.models.bart.modeling_flax_bart.(start_logits, end_logits)->jax.numpy.split(logits, logits.shape[-1], axis=-1)
A:transformers.models.bart.modeling_flax_bart.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.models.bart.modeling_flax_bart.end_logits->end_logits.squeeze(-1).squeeze(-1)
transformers.FlaxBartForConditionalGeneration(FlaxBartPreTrainedModel)
transformers.FlaxBartForConditionalGeneration.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,decoder_position_ids:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,deterministic:bool=True,params:dict=None,dropout_rng:PRNGKey=None)
transformers.FlaxBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,max_length,attention_mask:Optional[jnp.DeviceArray]=None,decoder_attention_mask:Optional[jnp.DeviceArray]=None,encoder_outputs=None,**kwargs)
transformers.FlaxBartForConditionalGeneration.update_inputs_for_generation(self,model_outputs,model_kwargs)
transformers.FlaxBartForConditionalGenerationModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.FlaxBartForConditionalGenerationModule._get_decoder_module(self)
transformers.FlaxBartForConditionalGenerationModule._get_encoder_module(self)
transformers.FlaxBartForConditionalGenerationModule.setup(self)
transformers.FlaxBartForQuestionAnswering(FlaxBartPreTrainedModel)
transformers.FlaxBartForQuestionAnsweringModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.FlaxBartForQuestionAnsweringModule._get_decoder_module(self)
transformers.FlaxBartForQuestionAnsweringModule._get_encoder_module(self)
transformers.FlaxBartForQuestionAnsweringModule.setup(self)
transformers.FlaxBartForSequenceClassification(FlaxBartPreTrainedModel)
transformers.FlaxBartForSequenceClassificationModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.FlaxBartForSequenceClassificationModule._get_decoder_module(self)
transformers.FlaxBartForSequenceClassificationModule._get_encoder_module(self)
transformers.FlaxBartForSequenceClassificationModule.setup(self)
transformers.FlaxBartModel(FlaxBartPreTrainedModel)
transformers.FlaxBartPreTrainedModel(self,config:BartConfig,input_shape:Tuple[int]=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.FlaxBartPreTrainedModel.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,decoder_position_ids:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.FlaxBartPreTrainedModel.encode(self,input_ids:jnp.ndarray,attention_mask:Optional[jnp.ndarray]=None,position_ids:Optional[jnp.ndarray]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.FlaxBartPreTrainedModel.init_cache(self,batch_size,max_length,encoder_outputs)
transformers.FlaxBartPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.bart.modeling_flax_bart.FlaxBartAttention(self,hidden_states:jnp.ndarray,key_value_states:Optional[jnp.ndarray]=None,attention_mask:Optional[jnp.ndarray]=None,init_cache:bool=False,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartAttention.__call__(self,hidden_states:jnp.ndarray,key_value_states:Optional[jnp.ndarray]=None,attention_mask:Optional[jnp.ndarray]=None,init_cache:bool=False,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartAttention._concatenate_to_cache(self,key,value,query,attention_mask)
transformers.models.bart.modeling_flax_bart.FlaxBartAttention._merge_heads(self,hidden_states)
transformers.models.bart.modeling_flax_bart.FlaxBartAttention._split_heads(self,hidden_states)
transformers.models.bart.modeling_flax_bart.FlaxBartAttention.setup(self)->None
transformers.models.bart.modeling_flax_bart.FlaxBartClassificationHead(self,hidden_states:jnp.ndarray,deterministic:bool)
transformers.models.bart.modeling_flax_bart.FlaxBartClassificationHead.__call__(self,hidden_states:jnp.ndarray,deterministic:bool)
transformers.models.bart.modeling_flax_bart.FlaxBartClassificationHead.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoder(self,input_ids,attention_mask,position_ids,encoder_hidden_states:Optional[jnp.ndarray]=None,encoder_attention_mask:Optional[jnp.ndarray]=None,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoder.__call__(self,input_ids,attention_mask,position_ids,encoder_hidden_states:Optional[jnp.ndarray]=None,encoder_attention_mask:Optional[jnp.ndarray]=None,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoder.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoderLayer(self,hidden_states:jnp.ndarray,attention_mask:jnp.ndarray,encoder_hidden_states:Optional[jnp.ndarray]=None,encoder_attention_mask:Optional[jnp.ndarray]=None,init_cache:bool=False,output_attentions:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoderLayer.__call__(self,hidden_states:jnp.ndarray,attention_mask:jnp.ndarray,encoder_hidden_states:Optional[jnp.ndarray]=None,encoder_attention_mask:Optional[jnp.ndarray]=None,init_cache:bool=False,output_attentions:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoderLayer.setup(self)->None
transformers.models.bart.modeling_flax_bart.FlaxBartDecoderLayerCollection(self,hidden_states,attention_mask,encoder_hidden_states:Optional[jnp.ndarray]=None,encoder_attention_mask:Optional[jnp.ndarray]=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoderLayerCollection.__call__(self,hidden_states,attention_mask,encoder_hidden_states:Optional[jnp.ndarray]=None,encoder_attention_mask:Optional[jnp.ndarray]=None,deterministic:bool=True,init_cache:bool=False,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartDecoderLayerCollection.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoder(self,input_ids,attention_mask,position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoder.__call__(self,input_ids,attention_mask,position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoder.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoderLayer(self,hidden_states:jnp.ndarray,attention_mask:jnp.ndarray,output_attentions:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoderLayer.__call__(self,hidden_states:jnp.ndarray,attention_mask:jnp.ndarray,output_attentions:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoderLayer.setup(self)->None
transformers.models.bart.modeling_flax_bart.FlaxBartEncoderLayerCollection(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoderLayerCollection.__call__(self,hidden_states,attention_mask,deterministic:bool=True,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartEncoderLayerCollection.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGeneration(FlaxBartPreTrainedModel)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGeneration.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,decoder_position_ids:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,deterministic:bool=True,params:dict=None,dropout_rng:PRNGKey=None)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,max_length,attention_mask:Optional[jnp.DeviceArray]=None,decoder_attention_mask:Optional[jnp.DeviceArray]=None,encoder_outputs=None,**kwargs)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGeneration.update_inputs_for_generation(self,model_outputs,model_kwargs)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGenerationModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGenerationModule.__call__(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGenerationModule._get_decoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGenerationModule._get_encoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForConditionalGenerationModule.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForQuestionAnswering(FlaxBartPreTrainedModel)
transformers.models.bart.modeling_flax_bart.FlaxBartForQuestionAnsweringModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartForQuestionAnsweringModule.__call__(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartForQuestionAnsweringModule._get_decoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForQuestionAnsweringModule._get_encoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForQuestionAnsweringModule.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForSequenceClassification(FlaxBartPreTrainedModel)
transformers.models.bart.modeling_flax_bart.FlaxBartForSequenceClassificationModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartForSequenceClassificationModule.__call__(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartForSequenceClassificationModule._get_decoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForSequenceClassificationModule._get_encoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartForSequenceClassificationModule.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartModel(FlaxBartPreTrainedModel)
transformers.models.bart.modeling_flax_bart.FlaxBartModule(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartModule.__call__(self,input_ids,attention_mask,decoder_input_ids,decoder_attention_mask,position_ids,decoder_position_ids,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=True,deterministic:bool=True)
transformers.models.bart.modeling_flax_bart.FlaxBartModule._get_decoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartModule._get_encoder_module(self)
transformers.models.bart.modeling_flax_bart.FlaxBartModule.setup(self)
transformers.models.bart.modeling_flax_bart.FlaxBartPreTrainedModel(self,config:BartConfig,input_shape:Tuple[int]=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.bart.modeling_flax_bart.FlaxBartPreTrainedModel.__init__(self,config:BartConfig,input_shape:Tuple[int]=(1,1),seed:int=0,dtype:jnp.dtype=jnp.float32,**kwargs)
transformers.models.bart.modeling_flax_bart.FlaxBartPreTrainedModel.decode(self,decoder_input_ids,encoder_outputs,encoder_attention_mask:Optional[jnp.ndarray]=None,decoder_attention_mask:Optional[jnp.ndarray]=None,decoder_position_ids:Optional[jnp.ndarray]=None,past_key_values:dict=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.models.bart.modeling_flax_bart.FlaxBartPreTrainedModel.encode(self,input_ids:jnp.ndarray,attention_mask:Optional[jnp.ndarray]=None,position_ids:Optional[jnp.ndarray]=None,output_attentions:Optional[bool]=None,output_hidden_states:Optional[bool]=None,return_dict:Optional[bool]=None,train:bool=False,params:dict=None,dropout_rng:PRNGKey=None)
transformers.models.bart.modeling_flax_bart.FlaxBartPreTrainedModel.init_cache(self,batch_size,max_length,encoder_outputs)
transformers.models.bart.modeling_flax_bart.FlaxBartPreTrainedModel.init_weights(self,rng:jax.random.PRNGKey,input_shape:Tuple)->FrozenDict
transformers.models.bart.modeling_flax_bart.shift_tokens_right(input_ids:jnp.ndarray,pad_token_id:int,decoder_start_token_id:int)->jnp.ndarray


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/tokenization_bart.py----------------------------------------
A:transformers.models.bart.tokenization_bart.logger->utils.logging.get_logger(__name__)
transformers.BartTokenizer(RobertaTokenizer)
transformers.models.bart.tokenization_bart.BartTokenizer(RobertaTokenizer)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/tokenization_bart_fast.py----------------------------------------
A:transformers.models.bart.tokenization_bart_fast.logger->utils.logging.get_logger(__name__)
transformers.BartTokenizerFast(RobertaTokenizerFast)
transformers.models.bart.tokenization_bart_fast.BartTokenizerFast(RobertaTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/modeling_bart.py----------------------------------------
A:transformers.models.bart.modeling_bart.logger->utils.logging.get_logger(__name__)
A:transformers.models.bart.modeling_bart.shifted_input_ids->input_ids.view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.models.bart.modeling_bart.shifted_input_ids[:, 1:]->input_ids[:, :-1].clone()
A:transformers.models.bart.modeling_bart.mask->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
A:transformers.models.bart.modeling_bart.mask_cond->torch.arange(mask.size(-1))
A:transformers.models.bart.modeling_bart.(bsz, src_len)->torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1).size()
A:transformers.models.bart.modeling_bart.expanded_mask->mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
A:transformers.models.bart.modeling_bart.positions->self.embed_positions(input_shape, past_key_values_length)
A:transformers.models.bart.modeling_bart.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.bart.modeling_bart.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.bart.modeling_bart.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.models.bart.modeling_bart.self.out_proj->torch.nn.Linear(inner_dim, num_classes)
A:transformers.models.bart.modeling_bart.(bsz, tgt_len, embed_dim)->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training).size()
A:transformers.models.bart.modeling_bart.key_states->key_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.bart.modeling_bart.value_states->value_states.view(*proj_shape).view(*proj_shape)
A:transformers.models.bart.modeling_bart.query_states->self._shape(query_states, tgt_len, bsz).view(*proj_shape)
A:transformers.models.bart.modeling_bart.src_len->key_states.view(*proj_shape).view(*proj_shape).size(1)
A:transformers.models.bart.modeling_bart.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len)
A:transformers.models.bart.modeling_bart.attn_weights_reshaped->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz * self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.models.bart.modeling_bart.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.models.bart.modeling_bart.attn_output->self.out_proj(attn_output)
A:transformers.models.bart.modeling_bart.self.self_attn->BartAttention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.bart.modeling_bart.self.self_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.bart.modeling_bart.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.models.bart.modeling_bart.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.models.bart.modeling_bart.self.final_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.bart.modeling_bart.(hidden_states, attn_weights, _)->self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.bart.modeling_bart.hidden_states->torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
A:transformers.models.bart.modeling_bart.self.encoder_attn->BartAttention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, is_decoder=True)
A:transformers.models.bart.modeling_bart.self.encoder_attn_layer_norm->torch.nn.LayerNorm(self.embed_dim)
A:transformers.models.bart.modeling_bart.(hidden_states, self_attn_weights, present_key_value)->self.self_attn(hidden_states=hidden_states, past_key_value=self_attn_past_key_value, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
A:transformers.models.bart.modeling_bart.(hidden_states, cross_attn_weights, cross_attn_present_key_value)->self.encoder_attn(hidden_states=hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_attention_mask, layer_head_mask=cross_attn_layer_head_mask, past_key_value=cross_attn_past_key_value, output_attentions=output_attentions)
A:transformers.models.bart.modeling_bart.self.dense->torch.nn.Linear(input_dim, inner_dim)
A:transformers.models.bart.modeling_bart.self.dropout->torch.nn.Dropout(p=pooler_dropout)
A:transformers.models.bart.modeling_bart.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.models.bart.modeling_bart.self.embed_tokens->torch.nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)
A:transformers.models.bart.modeling_bart.self.embed_positions->BartLearnedPositionalEmbedding(config.max_position_embeddings, config.d_model)
A:transformers.models.bart.modeling_bart.self.layers->torch.nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.models.bart.modeling_bart.self.layernorm_embedding->torch.nn.LayerNorm(config.d_model)
A:transformers.models.bart.modeling_bart.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.models.bart.modeling_bart.embed_pos->self.embed_positions(input_shape)
A:transformers.models.bart.modeling_bart.attention_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_ones(input_ids.shape)
A:transformers.models.bart.modeling_bart.dropout_probability->random.uniform(0, 1)
A:transformers.models.bart.modeling_bart.layer_outputs->decoder_layer(hidden_states, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, layer_head_mask=head_mask[idx] if head_mask is not None else None, cross_attn_layer_head_mask=cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None, past_key_value=past_key_value, output_attentions=output_attentions, use_cache=use_cache)
A:transformers.models.bart.modeling_bart.combined_attention_mask->_make_causal_mask(input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length).to(self.device)
A:transformers.models.bart.modeling_bart.expanded_attn_mask->_expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.bart.modeling_bart.encoder_attention_mask->_expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
A:transformers.models.bart.modeling_bart.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.models.bart.modeling_bart.self.encoder->BartEncoder(config, self.shared)
A:transformers.models.bart.modeling_bart.self.decoder->BartDecoder(config)
A:transformers.models.bart.modeling_bart.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)
A:transformers.models.bart.modeling_bart.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.models.bart.modeling_bart.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states=encoder_outputs[0], encoder_attention_mask=attention_mask, head_mask=decoder_head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=decoder_inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bart.modeling_bart.self.model->BartDecoderWrapper(config)
A:transformers.models.bart.modeling_bart.self.lm_head->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.models.bart.modeling_bart.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.models.bart.modeling_bart.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.models.bart.modeling_bart.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.models.bart.modeling_bart.outputs->self.model.decoder(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, head_mask=head_mask, cross_attn_head_mask=cross_attn_head_mask, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.models.bart.modeling_bart.loss_fct->CrossEntropyLoss()
A:transformers.models.bart.modeling_bart.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.bart.modeling_bart.self.classification_head->BartClassificationHead(config.d_model, config.d_model, config.num_labels, config.classifier_dropout)
A:transformers.models.bart.modeling_bart.eos_mask->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).eq(self.config.eos_token_id)
A:transformers.models.bart.modeling_bart.logits->self.lm_head(outputs[0])
A:transformers.models.bart.modeling_bart.loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.models.bart.modeling_bart.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.models.bart.modeling_bart.(start_logits, end_logits)->self.lm_head(outputs[0]).split(1, dim=-1)
A:transformers.models.bart.modeling_bart.start_logits->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.bart.modeling_bart.end_logits->end_logits.squeeze(-1).contiguous().squeeze(-1).contiguous()
A:transformers.models.bart.modeling_bart.start_positions->start_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.bart.modeling_bart.end_positions->end_positions.clamp(0, ignored_index).clamp(0, ignored_index)
A:transformers.models.bart.modeling_bart.ignored_index->start_logits.squeeze(-1).contiguous().squeeze(-1).contiguous().size(1)
A:transformers.models.bart.modeling_bart.start_loss->loss_fct(start_logits, start_positions)
A:transformers.models.bart.modeling_bart.end_loss->loss_fct(end_logits, end_positions)
A:transformers.models.bart.modeling_bart.config->copy.deepcopy(config)
transformers.BartForCausalLM(self,config)
transformers.BartForCausalLM._reorder_cache(past,beam_idx)
transformers.BartForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartForCausalLM.get_decoder(self)
transformers.BartForCausalLM.get_input_embeddings(self)
transformers.BartForCausalLM.get_output_embeddings(self)
transformers.BartForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.BartForCausalLM.set_decoder(self,decoder)
transformers.BartForCausalLM.set_input_embeddings(self,value)
transformers.BartForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.BartForConditionalGeneration(self,config:BartConfig)
transformers.BartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.BartForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.BartForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartForConditionalGeneration.get_decoder(self)
transformers.BartForConditionalGeneration.get_encoder(self)
transformers.BartForConditionalGeneration.get_output_embeddings(self)
transformers.BartForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.BartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.BartForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.BartForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.BartForQuestionAnswering(self,config)
transformers.BartForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartForSequenceClassification(self,config:BartConfig,**kwargs)
transformers.BartForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartModel(self,config:BartConfig)
transformers.BartModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartModel.get_decoder(self)
transformers.BartModel.get_encoder(self)
transformers.BartModel.get_input_embeddings(self)
transformers.BartModel.set_input_embeddings(self,value)
transformers.BartPretrainedModel(PreTrainedModel)
transformers.BartPretrainedModel._init_weights(self,module)
transformers.BartPretrainedModel.dummy_inputs(self)
transformers.PretrainedBartModel(BartPretrainedModel)
transformers.PretrainedBartModel.__init_subclass__(self)
transformers.models.bart.modeling_bart.BartAttention(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.bart.modeling_bart.BartAttention.__init__(self,embed_dim:int,num_heads:int,dropout:float=0.0,is_decoder:bool=False,bias:bool=True)
transformers.models.bart.modeling_bart.BartAttention._shape(self,tensor:torch.Tensor,seq_len:int,bsz:int)
transformers.models.bart.modeling_bart.BartAttention.forward(self,hidden_states:torch.Tensor,key_value_states:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,output_attentions:bool=False)->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
transformers.models.bart.modeling_bart.BartClassificationHead(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.bart.modeling_bart.BartClassificationHead.__init__(self,input_dim:int,inner_dim:int,num_classes:int,pooler_dropout:float)
transformers.models.bart.modeling_bart.BartClassificationHead.forward(self,hidden_states:torch.Tensor)
transformers.models.bart.modeling_bart.BartDecoder(self,config:BartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bart.modeling_bart.BartDecoder.__init__(self,config:BartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask(self,attention_mask,input_shape,inputs_embeds,past_key_values_length)
transformers.models.bart.modeling_bart.BartDecoder.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartDecoder.get_input_embeddings(self)
transformers.models.bart.modeling_bart.BartDecoder.set_input_embeddings(self,value)
transformers.models.bart.modeling_bart.BartDecoderLayer(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartDecoderLayer.__init__(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartDecoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:Optional[torch.Tensor]=None,encoder_hidden_states:Optional[torch.Tensor]=None,encoder_attention_mask:Optional[torch.Tensor]=None,layer_head_mask:Optional[torch.Tensor]=None,cross_attn_layer_head_mask:Optional[torch.Tensor]=None,past_key_value:Optional[Tuple[torch.Tensor]]=None,output_attentions:Optional[bool]=False,use_cache:Optional[bool]=True)
transformers.models.bart.modeling_bart.BartDecoderWrapper(self,config)
transformers.models.bart.modeling_bart.BartDecoderWrapper.__init__(self,config)
transformers.models.bart.modeling_bart.BartDecoderWrapper.forward(self,*args,**kwargs)
transformers.models.bart.modeling_bart.BartEncoder(self,config:BartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bart.modeling_bart.BartEncoder.__init__(self,config:BartConfig,embed_tokens:Optional[nn.Embedding]=None)
transformers.models.bart.modeling_bart.BartEncoder.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartEncoderLayer(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartEncoderLayer.__init__(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartEncoderLayer.forward(self,hidden_states:torch.Tensor,attention_mask:torch.Tensor,layer_head_mask:torch.Tensor,output_attentions:bool=False)
transformers.models.bart.modeling_bart.BartForCausalLM(self,config)
transformers.models.bart.modeling_bart.BartForCausalLM.__init__(self,config)
transformers.models.bart.modeling_bart.BartForCausalLM._reorder_cache(past,beam_idx)
transformers.models.bart.modeling_bart.BartForCausalLM.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,head_mask=None,cross_attn_head_mask=None,past_key_values=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartForCausalLM.get_decoder(self)
transformers.models.bart.modeling_bart.BartForCausalLM.get_input_embeddings(self)
transformers.models.bart.modeling_bart.BartForCausalLM.get_output_embeddings(self)
transformers.models.bart.modeling_bart.BartForCausalLM.prepare_inputs_for_generation(self,input_ids,past=None,attention_mask=None,use_cache=None,**kwargs)
transformers.models.bart.modeling_bart.BartForCausalLM.set_decoder(self,decoder)
transformers.models.bart.modeling_bart.BartForCausalLM.set_input_embeddings(self,value)
transformers.models.bart.modeling_bart.BartForCausalLM.set_output_embeddings(self,new_embeddings)
transformers.models.bart.modeling_bart.BartForConditionalGeneration(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.__init__(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.models.bart.modeling_bart.BartForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int)->None
transformers.models.bart.modeling_bart.BartForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.get_decoder(self)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.get_encoder(self)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.get_output_embeddings(self)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.prepare_decoder_input_ids_from_labels(self,labels:torch.Tensor)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past=None,attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,use_cache=None,encoder_outputs=None,**kwargs)
transformers.models.bart.modeling_bart.BartForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.models.bart.modeling_bart.BartForConditionalGeneration.set_output_embeddings(self,new_embeddings)
transformers.models.bart.modeling_bart.BartForQuestionAnswering(self,config)
transformers.models.bart.modeling_bart.BartForQuestionAnswering.__init__(self,config)
transformers.models.bart.modeling_bart.BartForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,start_positions=None,end_positions=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartForSequenceClassification(self,config:BartConfig,**kwargs)
transformers.models.bart.modeling_bart.BartForSequenceClassification.__init__(self,config:BartConfig,**kwargs)
transformers.models.bart.modeling_bart.BartForSequenceClassification.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,inputs_embeds=None,decoder_inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int)
transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int)
transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding.forward(self,input_ids_shape:torch.Size,past_key_values_length:int=0)
transformers.models.bart.modeling_bart.BartModel(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartModel.__init__(self,config:BartConfig)
transformers.models.bart.modeling_bart.BartModel.forward(self,input_ids=None,attention_mask=None,decoder_input_ids=None,decoder_attention_mask=None,head_mask=None,decoder_head_mask=None,cross_attn_head_mask=None,encoder_outputs=None,past_key_values=None,inputs_embeds=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.models.bart.modeling_bart.BartModel.get_decoder(self)
transformers.models.bart.modeling_bart.BartModel.get_encoder(self)
transformers.models.bart.modeling_bart.BartModel.get_input_embeddings(self)
transformers.models.bart.modeling_bart.BartModel.set_input_embeddings(self,value)
transformers.models.bart.modeling_bart.BartPretrainedModel(PreTrainedModel)
transformers.models.bart.modeling_bart.BartPretrainedModel._init_weights(self,module)
transformers.models.bart.modeling_bart.BartPretrainedModel.dummy_inputs(self)
transformers.models.bart.modeling_bart.PretrainedBartModel(BartPretrainedModel)
transformers.models.bart.modeling_bart.PretrainedBartModel.__init_subclass__(self)
transformers.models.bart.modeling_bart._expand_mask(mask:torch.Tensor,dtype:torch.dtype,tgt_len:Optional[int]=None)
transformers.models.bart.modeling_bart._make_causal_mask(input_ids_shape:torch.Size,dtype:torch.dtype,past_key_values_length:int=0)
transformers.models.bart.modeling_bart.shift_tokens_right(input_ids:torch.Tensor,pad_token_id:int,decoder_start_token_id:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/bart/__init__.py----------------------------------------
A:transformers.models.bart.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/cpm/tokenization_cpm.py----------------------------------------
A:transformers.models.cpm.tokenization_cpm.logger->utils.logging.get_logger(__name__)
A:transformers.models.cpm.tokenization_cpm.self.translator->str.maketrans(' \n', '')
A:transformers.models.cpm.tokenization_cpm.text->text.replace(' ', '').replace('', ' ').replace('', '\n').replace(' ', '').replace('', ' ').replace('', '\n')
transformers.CpmTokenizer(self,*args,**kwargs)
transformers.CpmTokenizer._decode(self,*args,**kwargs)
transformers.CpmTokenizer._tokenize(self,text,*args,**kwargs)
transformers.models.cpm.tokenization_cpm.CpmTokenizer(self,*args,**kwargs)
transformers.models.cpm.tokenization_cpm.CpmTokenizer.__init__(self,*args,**kwargs)
transformers.models.cpm.tokenization_cpm.CpmTokenizer._decode(self,*args,**kwargs)
transformers.models.cpm.tokenization_cpm.CpmTokenizer._tokenize(self,text,*args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/models/cpm/__init__.py----------------------------------------
A:transformers.models.cpm.__init__.sys.modules[__name__]->_LazyModule(__name__, _import_structure)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/data_collator.py----------------------------------------
A:transformers.data.data_collator.InputDataClass->NewType('InputDataClass', Any)
A:transformers.data.data_collator.DataCollator->NewType('DataCollator', Callable[[List[InputDataClass]], Dict[str, torch.Tensor]])
A:transformers.data.data_collator.batch['labels']->torch.tensor([f['label_ids'] for f in features], dtype=dtype)
A:transformers.data.data_collator.batch[k]->torch.tensor([f[k] for f in features])
A:transformers.data.data_collator.batch->_collate_batch(examples, self.tokenizer)
A:transformers.data.data_collator.length_of_first->examples[0].size(0)
A:transformers.data.data_collator.are_tensors_same_length->all((x.size(0) == length_of_first for x in examples))
A:transformers.data.data_collator.max_length->max((x.size(0) for x in examples))
A:transformers.data.data_collator.result->examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)
A:transformers.data.data_collator.max_label_length->max((len(l) for l in labels))
A:transformers.data.data_collator.features->self.tokenizer.pad(features, padding=self.padding, max_length=self.max_length, pad_to_multiple_of=self.pad_to_multiple_of, return_tensors='pt')
A:transformers.data.data_collator.decoder_input_ids->self.model.prepare_decoder_input_ids_from_labels(labels=features['labels'])
A:transformers.data.data_collator.special_tokens_mask->torch.tensor([self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()], dtype=torch.bool)
A:transformers.data.data_collator.(batch['input_ids'], batch['labels'])->self.mask_tokens(batch['input_ids'], special_tokens_mask=special_tokens_mask)
A:transformers.data.data_collator.labels->inputs.clone()
A:transformers.data.data_collator.probability_matrix->torch.full(labels.shape, self.mlm_probability)
A:transformers.data.data_collator.masked_indices->torch.full(labels.shape, 0, dtype=torch.bool)
A:transformers.data.data_collator.inputs[indices_replaced]->self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)
A:transformers.data.data_collator.random_words->torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)
A:transformers.data.data_collator.batch_input->_collate_batch(input_ids, self.tokenizer)
A:transformers.data.data_collator.token->self.tokenizer._convert_id_to_token(id)
A:transformers.data.data_collator.ref_pos->tolist(e['chinese_ref'])
A:transformers.data.data_collator.len_seq->len(e['input_ids'])
A:transformers.data.data_collator.batch_mask->_collate_batch(mask_labels, self.tokenizer)
A:transformers.data.data_collator.(inputs, labels)->self.mask_tokens(batch_input, batch_mask)
A:transformers.data.data_collator.num_to_predict->min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))
A:transformers.data.data_collator.covered_indexes->set()
A:transformers.data.data_collator.padding_mask->inputs.clone().eq(self.tokenizer.pad_token_id)
A:transformers.data.data_collator.input_ids->_collate_batch(input_ids, self.tokenizer)
A:transformers.data.data_collator.(input_ids, labels, attention_mask)->self.mask_tokens(input_ids)
A:transformers.data.data_collator.token_type_ids->pad_sequence(token_type_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)
A:transformers.data.data_collator.sentence_order_label->torch.stack(sop_label_list)
A:transformers.data.data_collator.attention_mask->(~masked_indices).float()
A:transformers.data.data_collator.attention_padding_mask->inputs.clone().eq(self.tokenizer.pad_token_id)
A:transformers.data.data_collator.(inputs, perm_mask, target_mapping, labels)->self.mask_tokens(batch)
A:transformers.data.data_collator.target_mapping->torch.zeros((labels.size(0), labels.size(1), labels.size(1)), dtype=torch.float32)
A:transformers.data.data_collator.max_len->inputs.clone().size(1)
A:transformers.data.data_collator.span_length->torch.randint(1, self.max_span_length + 1, (1,)).item()
A:transformers.data.data_collator.context_length->int(span_length / self.plm_probability)
A:transformers.data.data_collator.target_mapping[i]->torch.eye(labels.size(1))
A:transformers.data.data_collator.perm_mask->torch.zeros((labels.size(0), labels.size(1), labels.size(1)), dtype=torch.float32)
A:transformers.data.data_collator.perm_index->torch.flatten(perm_index.transpose(0, 1))
transformers.DataCollatorForLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.DataCollatorForLanguageModeling.__post_init__(self)
transformers.DataCollatorForLanguageModeling.mask_tokens(self,inputs:torch.Tensor,special_tokens_mask:Optional[torch.Tensor]=None)->Tuple[torch.Tensor, torch.Tensor]
transformers.DataCollatorForPermutationLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.DataCollatorForPermutationLanguageModeling.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
transformers.DataCollatorForSOP(self,*args,**kwargs)
transformers.DataCollatorForSOP.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
transformers.DataCollatorForSeq2Seq(self,features)
transformers.DataCollatorForTokenClassification(self,features)
transformers.DataCollatorForWholeWordMask(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.DataCollatorForWholeWordMask._whole_word_mask(self,input_tokens:List[str],max_predictions=512)
transformers.DataCollatorForWholeWordMask.mask_tokens(self,inputs:torch.Tensor,mask_labels:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
transformers.DataCollatorWithPadding(self,features:List[Dict[str,Union[List[int],torch.Tensor]]])
transformers.data.data_collator.DataCollatorForLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForLanguageModeling.__call__(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForLanguageModeling.__post_init__(self)
transformers.data.data_collator.DataCollatorForLanguageModeling.mask_tokens(self,inputs:torch.Tensor,special_tokens_mask:Optional[torch.Tensor]=None)->Tuple[torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.__call__(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorForSOP(self,*args,**kwargs)
transformers.data.data_collator.DataCollatorForSOP.__init__(self,*args,**kwargs)
transformers.data.data_collator.DataCollatorForSOP.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorForSeq2Seq(self,features)
transformers.data.data_collator.DataCollatorForSeq2Seq.__call__(self,features)
transformers.data.data_collator.DataCollatorForTokenClassification(self,features)
transformers.data.data_collator.DataCollatorForTokenClassification.__call__(self,features)
transformers.data.data_collator.DataCollatorForWholeWordMask(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForWholeWordMask.__call__(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForWholeWordMask._whole_word_mask(self,input_tokens:List[str],max_predictions=512)
transformers.data.data_collator.DataCollatorForWholeWordMask.mask_tokens(self,inputs:torch.Tensor,mask_labels:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorWithPadding(self,features:List[Dict[str,Union[List[int],torch.Tensor]]])
transformers.data.data_collator.DataCollatorWithPadding.__call__(self,features:List[Dict[str,Union[List[int],torch.Tensor]]])
transformers.data.data_collator._collate_batch(examples,tokenizer,pad_to_multiple_of:Optional[int]=None)
transformers.data.data_collator.default_data_collator(features:List[InputDataClass])->Dict[str, torch.Tensor]
transformers.data.data_collator.tolist(x:Union[List[Any],torch.Tensor])
transformers.default_data_collator(features:List[InputDataClass])->Dict[str, torch.Tensor]


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/test_generation_utils.py----------------------------------------
A:transformers.data.test_generation_utils.config->models.marian.MarianConfig.from_pretrained('sshleifer/tiny-marian-en-de')
A:transformers.data.test_generation_utils.input_ids->torch.arange(0, 96, 1).view((8, 12))
A:transformers.data.test_generation_utils.scores->torch.rand((8, 300))
A:transformers.data.test_generation_utils.output->model.postprocess_next_token_scores(scores, input_ids, 0, bad_words_ids, 13, 15, config.max_length, config.eos_token_id, config.repetition_penalty, 32, 5)
A:transformers.data.test_generation_utils.length_bad_word->random.randint(1, 4)
A:transformers.data.test_generation_utils._->model.postprocess_next_token_scores(scores, input_ids, 0, bad_words_ids, 13, 15, config.max_length, config.eos_token_id, config.repetition_penalty, 32, 5)
transformers.data.test_generation_utils.GenerationUtilsTest(unittest.TestCase)
transformers.data.test_generation_utils.GenerationUtilsTest.config(self)
transformers.data.test_generation_utils.GenerationUtilsTest.model(self)
transformers.data.test_generation_utils.GenerationUtilsTest.test_postprocess_next_token_scores(self)
transformers.data.test_generation_utils.GenerationUtilsTest.test_postprocess_next_token_scores_large_bad_words_list(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/processors/glue.py----------------------------------------
A:transformers.data.processors.glue.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.glue.processor->glue_processors[task]()
A:transformers.data.processors.glue.features->glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)
A:transformers.data.processors.glue.label->d.pop('label')
A:transformers.data.processors.glue.label_list->glue_processors[task]().get_labels()
A:transformers.data.processors.glue.batch_encoding->tokenizer([(example.text_a, example.text_b) for example in examples], max_length=max_length, padding='max_length', truncation=True)
A:transformers.data.processors.glue.feature->InputFeatures(**inputs, label=labels[i])
transformers.data.processors.glue.ColaProcessor(self,*args,**kwargs)
transformers.data.processors.glue.ColaProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.ColaProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.ColaProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.ColaProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.ColaProcessor.get_labels(self)
transformers.data.processors.glue.ColaProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.ColaProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.MnliMismatchedProcessor(self,*args,**kwargs)
transformers.data.processors.glue.MnliMismatchedProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.MnliMismatchedProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MnliMismatchedProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor(self,*args,**kwargs)
transformers.data.processors.glue.MnliProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.MnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.MnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.MnliProcessor.get_labels(self)
transformers.data.processors.glue.MnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor(self,*args,**kwargs)
transformers.data.processors.glue.MrpcProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.MrpcProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.MrpcProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.MrpcProcessor.get_labels(self)
transformers.data.processors.glue.MrpcProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.OutputMode(Enum)
transformers.data.processors.glue.QnliProcessor(self,*args,**kwargs)
transformers.data.processors.glue.QnliProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.QnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.QnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.QnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.QnliProcessor.get_labels(self)
transformers.data.processors.glue.QnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.QnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor(self,*args,**kwargs)
transformers.data.processors.glue.QqpProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.QqpProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.QqpProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.QqpProcessor.get_labels(self)
transformers.data.processors.glue.QqpProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor(self,*args,**kwargs)
transformers.data.processors.glue.RteProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.RteProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.RteProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.RteProcessor.get_labels(self)
transformers.data.processors.glue.RteProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor(self,*args,**kwargs)
transformers.data.processors.glue.Sst2Processor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.Sst2Processor._create_examples(self,lines,set_type)
transformers.data.processors.glue.Sst2Processor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.Sst2Processor.get_labels(self)
transformers.data.processors.glue.Sst2Processor.get_test_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor.get_train_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor(self,*args,**kwargs)
transformers.data.processors.glue.StsbProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.StsbProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.StsbProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.StsbProcessor.get_labels(self)
transformers.data.processors.glue.StsbProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor(self,*args,**kwargs)
transformers.data.processors.glue.WnliProcessor.__init__(self,*args,**kwargs)
transformers.data.processors.glue.WnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.WnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.WnliProcessor.get_labels(self)
transformers.data.processors.glue.WnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue._glue_convert_examples_to_features(examples:List[InputExample],tokenizer:PreTrainedTokenizer,max_length:Optional[int]=None,task=None,label_list=None,output_mode=None)
transformers.data.processors.glue.glue_convert_examples_to_features(examples:Union[List[InputExample],'tf.data.Dataset'],tokenizer:PreTrainedTokenizer,max_length:Optional[int]=None,task=None,label_list=None,output_mode=None)
transformers.glue_convert_examples_to_features(examples:Union[List[InputExample],'tf.data.Dataset'],tokenizer:PreTrainedTokenizer,max_length:Optional[int]=None,task=None,label_list=None,output_mode=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/processors/utils.py----------------------------------------
A:transformers.data.processors.utils.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.utils.processor->cls(**kwargs)
A:transformers.data.processors.utils.lines->self._read_tsv(file_name)
A:transformers.data.processors.utils.added_labels->set()
A:transformers.data.processors.utils.self.labels->list(set(self.labels).union(added_labels))
A:transformers.data.processors.utils.input_ids->tokenizer.encode(example.text_a, add_special_tokens=True, max_length=min(max_length, tokenizer.max_len))
A:transformers.data.processors.utils.batch_length->max((len(input_ids) for input_ids in all_input_ids))
A:transformers.data.processors.utils.label->float(example.label)
A:transformers.data.processors.utils.dataset->TensorDataset(all_input_ids, all_attention_mask, all_labels)
A:transformers.data.processors.utils.all_input_ids->torch.tensor([f.input_ids for f in features], dtype=torch.long)
A:transformers.data.processors.utils.all_attention_mask->torch.tensor([f.attention_mask for f in features], dtype=torch.long)
A:transformers.data.processors.utils.all_labels->torch.tensor([f.label for f in features], dtype=torch.float)
transformers.DataProcessor
transformers.DataProcessor._read_tsv(cls,input_file,quotechar=None)
transformers.DataProcessor.get_dev_examples(self,data_dir)
transformers.DataProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.DataProcessor.get_labels(self)
transformers.DataProcessor.get_test_examples(self,data_dir)
transformers.DataProcessor.get_train_examples(self,data_dir)
transformers.DataProcessor.tfds_map(self,example)
transformers.InputExample
transformers.InputExample.to_json_string(self)
transformers.InputFeatures
transformers.InputFeatures.to_json_string(self)
transformers.SingleSentenceClassificationProcessor(self,labels=None,examples=None,mode='classification',verbose=False)
transformers.SingleSentenceClassificationProcessor.__getitem__(self,idx)
transformers.SingleSentenceClassificationProcessor.__len__(self)
transformers.SingleSentenceClassificationProcessor.add_examples(self,texts_or_text_and_labels,labels=None,ids=None,overwrite_labels=False,overwrite_examples=False)
transformers.SingleSentenceClassificationProcessor.add_examples_from_csv(self,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,overwrite_labels=False,overwrite_examples=False)
transformers.SingleSentenceClassificationProcessor.create_from_csv(cls,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,**kwargs)
transformers.SingleSentenceClassificationProcessor.create_from_examples(cls,texts_or_text_and_labels,labels=None,**kwargs)
transformers.SingleSentenceClassificationProcessor.get_features(self,tokenizer,max_length=None,pad_on_left=False,pad_token=0,mask_padding_with_zero=True,return_tensors=None)
transformers.data.processors.utils.DataProcessor
transformers.data.processors.utils.DataProcessor._read_tsv(cls,input_file,quotechar=None)
transformers.data.processors.utils.DataProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.utils.DataProcessor.get_labels(self)
transformers.data.processors.utils.DataProcessor.get_test_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.get_train_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.tfds_map(self,example)
transformers.data.processors.utils.InputExample
transformers.data.processors.utils.InputExample.to_json_string(self)
transformers.data.processors.utils.InputFeatures
transformers.data.processors.utils.InputFeatures.to_json_string(self)
transformers.data.processors.utils.SingleSentenceClassificationProcessor(self,labels=None,examples=None,mode='classification',verbose=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.__getitem__(self,idx)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.__init__(self,labels=None,examples=None,mode='classification',verbose=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.__len__(self)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.add_examples(self,texts_or_text_and_labels,labels=None,ids=None,overwrite_labels=False,overwrite_examples=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.add_examples_from_csv(self,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,overwrite_labels=False,overwrite_examples=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.create_from_csv(cls,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,**kwargs)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.create_from_examples(cls,texts_or_text_and_labels,labels=None,**kwargs)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.get_features(self,tokenizer,max_length=None,pad_on_left=False,pad_token=0,mask_padding_with_zero=True,return_tensors=None)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/processors/squad.py----------------------------------------
A:transformers.data.processors.squad.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.squad.tok_answer_text->' '.join(tokenizer.tokenize(orig_answer_text))
A:transformers.data.processors.squad.text_span->' '.join(doc_tokens[new_start:new_end + 1])
A:transformers.data.processors.squad.actual_text->' '.join(example.doc_tokens[start_position:end_position + 1])
A:transformers.data.processors.squad.cleaned_answer_text->' '.join(whitespace_tokenize(example.answer_text))
A:transformers.data.processors.squad.sub_tokens->tokenizer.tokenize(token)
A:transformers.data.processors.squad.(tok_start_position, tok_end_position)->_improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text)
A:transformers.data.processors.squad.truncated_query->tokenizer.encode(example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
A:transformers.data.processors.squad.tokenizer_type->type(tokenizer).__name__.replace('Tokenizer', '').lower()
A:transformers.data.processors.squad.encoded_dict->tokenizer.encode_plus(texts, pairs, truncation=truncation, padding=padding_strategy, max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
A:transformers.data.processors.squad.paragraph_len->min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
A:transformers.data.processors.squad.tokens->tokenizer.convert_ids_to_tokens(non_padded_ids)
A:transformers.data.processors.squad.is_max_context->_new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
A:transformers.data.processors.squad.cls_index->span['input_ids'].index(tokenizer.cls_token_id)
A:transformers.data.processors.squad.p_mask->numpy.ones_like(span['token_type_ids'])
A:transformers.data.processors.squad.pad_token_indices->numpy.where(span['input_ids'] == tokenizer.pad_token_id)
A:transformers.data.processors.squad.special_token_indices->numpy.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
A:transformers.data.processors.squad.threads->min(threads, cpu_count())
A:transformers.data.processors.squad.annotate_->partial(squad_convert_example_to_features, max_seq_length=max_seq_length, doc_stride=doc_stride, max_query_length=max_query_length, padding_strategy=padding_strategy, is_training=is_training)
A:transformers.data.processors.squad.features->list(tqdm(p.imap(annotate_, examples, chunksize=32), total=len(examples), desc='convert squad examples to features', disable=not tqdm_enabled))
A:transformers.data.processors.squad.all_input_ids->torch.tensor([f.input_ids for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_attention_masks->torch.tensor([f.attention_mask for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_token_type_ids->torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_cls_index->torch.tensor([f.cls_index for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_p_mask->torch.tensor([f.p_mask for f in features], dtype=torch.float)
A:transformers.data.processors.squad.all_is_impossible->torch.tensor([f.is_impossible for f in features], dtype=torch.float)
A:transformers.data.processors.squad.all_feature_index->torch.arange(all_input_ids.size(0), dtype=torch.long)
A:transformers.data.processors.squad.dataset->TensorDataset(all_input_ids, all_attention_masks, all_token_type_ids, all_start_positions, all_end_positions, all_cls_index, all_p_mask, all_is_impossible)
A:transformers.data.processors.squad.all_start_positions->torch.tensor([f.start_position for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_end_positions->torch.tensor([f.end_position for f in features], dtype=torch.long)
A:transformers.data.processors.squad.answer->tensor_dict['answers']['text'][0].numpy().decode('utf-8')
A:transformers.data.processors.squad.answer_start->tensor_dict['answers']['answer_start'][0].numpy()
A:transformers.data.processors.squad.is_impossible->qa.get('is_impossible', False)
A:transformers.data.processors.squad.example->SquadExample(qas_id=qas_id, question_text=question_text, context_text=context_text, answer_text=answer_text, start_position_character=start_position_character, title=title, is_impossible=is_impossible, answers=answers)
transformers.SquadExample(self,qas_id,question_text,context_text,answer_text,start_position_character,title,answers=[],is_impossible=False)
transformers.SquadFeatures(self,input_ids,attention_mask,token_type_ids,cls_index,p_mask,example_index,unique_id,paragraph_len,token_is_max_context,tokens,token_to_orig_map,start_position,end_position,is_impossible,qas_id:str=None,encoding:BatchEncoding=None)
transformers.SquadV1Processor(SquadProcessor)
transformers.SquadV2Processor(SquadProcessor)
transformers.data.processors.squad.SquadExample(self,qas_id,question_text,context_text,answer_text,start_position_character,title,answers=[],is_impossible=False)
transformers.data.processors.squad.SquadExample.__init__(self,qas_id,question_text,context_text,answer_text,start_position_character,title,answers=[],is_impossible=False)
transformers.data.processors.squad.SquadFeatures(self,input_ids,attention_mask,token_type_ids,cls_index,p_mask,example_index,unique_id,paragraph_len,token_is_max_context,tokens,token_to_orig_map,start_position,end_position,is_impossible,qas_id:str=None,encoding:BatchEncoding=None)
transformers.data.processors.squad.SquadFeatures.__init__(self,input_ids,attention_mask,token_type_ids,cls_index,p_mask,example_index,unique_id,paragraph_len,token_is_max_context,tokens,token_to_orig_map,start_position,end_position,is_impossible,qas_id:str=None,encoding:BatchEncoding=None)
transformers.data.processors.squad.SquadProcessor(DataProcessor)
transformers.data.processors.squad.SquadProcessor._create_examples(self,input_data,set_type)
transformers.data.processors.squad.SquadProcessor._get_example_from_tensor_dict(self,tensor_dict,evaluate=False)
transformers.data.processors.squad.SquadProcessor.get_dev_examples(self,data_dir,filename=None)
transformers.data.processors.squad.SquadProcessor.get_examples_from_dataset(self,dataset,evaluate=False)
transformers.data.processors.squad.SquadProcessor.get_train_examples(self,data_dir,filename=None)
transformers.data.processors.squad.SquadResult(self,unique_id,start_logits,end_logits,start_top_index=None,end_top_index=None,cls_logits=None)
transformers.data.processors.squad.SquadResult.__init__(self,unique_id,start_logits,end_logits,start_top_index=None,end_top_index=None,cls_logits=None)
transformers.data.processors.squad.SquadV1Processor(SquadProcessor)
transformers.data.processors.squad.SquadV2Processor(SquadProcessor)
transformers.data.processors.squad._check_is_max_context(doc_spans,cur_span_index,position)
transformers.data.processors.squad._improve_answer_span(doc_tokens,input_start,input_end,tokenizer,orig_answer_text)
transformers.data.processors.squad._is_whitespace(c)
transformers.data.processors.squad._new_check_is_max_context(doc_spans,cur_span_index,position)
transformers.data.processors.squad.squad_convert_example_to_features(example,max_seq_length,doc_stride,max_query_length,padding_strategy,is_training)
transformers.data.processors.squad.squad_convert_example_to_features_init(tokenizer_for_convert:PreTrainedTokenizerBase)
transformers.data.processors.squad.squad_convert_examples_to_features(examples,tokenizer,max_seq_length,doc_stride,max_query_length,is_training,padding_strategy='max_length',return_dataset=False,threads=1,tqdm_enabled=True)
transformers.squad_convert_examples_to_features(examples,tokenizer,max_seq_length,doc_stride,max_query_length,is_training,padding_strategy='max_length',return_dataset=False,threads=1,tqdm_enabled=True)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/processors/xnli.py----------------------------------------
A:transformers.data.processors.xnli.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.xnli.lines->self._read_tsv(os.path.join(data_dir, 'XNLI-1.0/xnli.test.tsv'))
transformers.data.processors.xnli.XnliProcessor(self,language,train_language=None)
transformers.data.processors.xnli.XnliProcessor.__init__(self,language,train_language=None)
transformers.data.processors.xnli.XnliProcessor.get_labels(self)
transformers.data.processors.xnli.XnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.xnli.XnliProcessor.get_train_examples(self,data_dir)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/processors/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/metrics/__init__.py----------------------------------------
A:transformers.data.metrics.__init__.acc->simple_accuracy(preds, labels)
A:transformers.data.metrics.__init__.f1->f1_score(y_true=labels, y_pred=preds)
transformers.data.metrics.__init__.acc_and_f1(preds,labels)
transformers.data.metrics.__init__.glue_compute_metrics(task_name,preds,labels)
transformers.data.metrics.__init__.pearson_and_spearman(preds,labels)
transformers.data.metrics.__init__.simple_accuracy(preds,labels)
transformers.data.metrics.__init__.xnli_compute_metrics(task_name,preds,labels)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/metrics/squad_metrics.py----------------------------------------
A:transformers.data.metrics.squad_metrics.logger->utils.logging.get_logger(__name__)
A:transformers.data.metrics.squad_metrics.regex->re.compile('\\b(a|an|the)\\b', re.UNICODE)
A:transformers.data.metrics.squad_metrics.exclude->set(string.punctuation)
A:transformers.data.metrics.squad_metrics.gold_toks->get_tokens(a_gold)
A:transformers.data.metrics.squad_metrics.pred_toks->get_tokens(a_pred)
A:transformers.data.metrics.squad_metrics.num_same->sum(common.values())
A:transformers.data.metrics.squad_metrics.exact_scores[qas_id]->max((compute_exact(a, prediction) for a in gold_answers))
A:transformers.data.metrics.squad_metrics.f1_scores[qas_id]->max((compute_f1(a, prediction) for a in gold_answers))
A:transformers.data.metrics.squad_metrics.new_scores[qid]->float(not qid_to_has_ans[qid])
A:transformers.data.metrics.squad_metrics.total->len(qid_list)
A:transformers.data.metrics.squad_metrics.num_no_ans->sum((1 for k in qid_to_has_ans if not qid_to_has_ans[k]))
A:transformers.data.metrics.squad_metrics.qid_list->sorted(na_probs, key=lambda k: na_probs[k])
A:transformers.data.metrics.squad_metrics.(best_exact, exact_thresh, has_ans_exact)->find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(best_f1, f1_thresh, has_ans_f1)->find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(best_exact, exact_thresh)->find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(best_f1, f1_thresh)->find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(exact, f1)->get_raw_scores(examples, preds)
A:transformers.data.metrics.squad_metrics.exact_threshold->apply_no_ans_threshold(exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)
A:transformers.data.metrics.squad_metrics.f1_threshold->apply_no_ans_threshold(f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)
A:transformers.data.metrics.squad_metrics.evaluation->make_eval_dict(exact_threshold, f1_threshold)
A:transformers.data.metrics.squad_metrics.has_ans_eval->make_eval_dict(exact_threshold, f1_threshold, qid_list=has_answer_qids)
A:transformers.data.metrics.squad_metrics.no_ans_eval->make_eval_dict(exact_threshold, f1_threshold, qid_list=no_answer_qids)
A:transformers.data.metrics.squad_metrics.ns_to_s_map->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.ns_text->''.join(ns_chars)
A:transformers.data.metrics.squad_metrics.tokenizer->BasicTokenizer(do_lower_case=do_lower_case)
A:transformers.data.metrics.squad_metrics.tok_text->' '.join(tok_text.split())
A:transformers.data.metrics.squad_metrics.start_position->' '.join(tok_text.split()).find(pred_text)
A:transformers.data.metrics.squad_metrics.(orig_ns_text, orig_ns_to_s_map)->_strip_spaces(orig_text)
A:transformers.data.metrics.squad_metrics.(tok_ns_text, tok_ns_to_s_map)->_strip_spaces(tok_text)
A:transformers.data.metrics.squad_metrics.index_and_score->sorted(enumerate(logits), key=lambda x: x[1], reverse=True)
A:transformers.data.metrics.squad_metrics.x->math.exp(score - max_score)
A:transformers.data.metrics.squad_metrics.example_index_to_features->collections.defaultdict(list)
A:transformers.data.metrics.squad_metrics._PrelimPrediction->collections.namedtuple('PrelimPrediction', ['feature_index', 'start_index', 'end_index', 'start_log_prob', 'end_log_prob'])
A:transformers.data.metrics.squad_metrics.all_predictions->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.all_nbest_json->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.scores_diff_json->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.start_indexes->_get_best_indexes(result.start_logits, n_best_size)
A:transformers.data.metrics.squad_metrics.end_indexes->_get_best_indexes(result.end_logits, n_best_size)
A:transformers.data.metrics.squad_metrics.prelim_predictions->sorted(prelim_predictions, key=lambda x: x.start_log_prob + x.end_log_prob, reverse=True)
A:transformers.data.metrics.squad_metrics._NbestPrediction->collections.namedtuple('NbestPrediction', ['text', 'start_log_prob', 'end_log_prob'])
A:transformers.data.metrics.squad_metrics.orig_text->' '.join(orig_tokens)
A:transformers.data.metrics.squad_metrics.final_text->get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)
A:transformers.data.metrics.squad_metrics.probs->_compute_softmax(total_scores)
A:transformers.data.metrics.squad_metrics.output->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.score_null->min(score_null, cur_null_score)
transformers.data.metrics.squad_metrics._compute_softmax(scores)
transformers.data.metrics.squad_metrics._get_best_indexes(logits,n_best_size)
transformers.data.metrics.squad_metrics.apply_no_ans_threshold(scores,na_probs,qid_to_has_ans,na_prob_thresh)
transformers.data.metrics.squad_metrics.compute_exact(a_gold,a_pred)
transformers.data.metrics.squad_metrics.compute_f1(a_gold,a_pred)
transformers.data.metrics.squad_metrics.compute_predictions_log_probs(all_examples,all_features,all_results,n_best_size,max_answer_length,output_prediction_file,output_nbest_file,output_null_log_odds_file,start_n_top,end_n_top,version_2_with_negative,tokenizer,verbose_logging)
transformers.data.metrics.squad_metrics.compute_predictions_logits(all_examples,all_features,all_results,n_best_size,max_answer_length,do_lower_case,output_prediction_file,output_nbest_file,output_null_log_odds_file,verbose_logging,version_2_with_negative,null_score_diff_threshold,tokenizer)
transformers.data.metrics.squad_metrics.find_all_best_thresh(main_eval,preds,exact_raw,f1_raw,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.find_all_best_thresh_v2(main_eval,preds,exact_raw,f1_raw,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.find_best_thresh(preds,scores,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.find_best_thresh_v2(preds,scores,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.get_final_text(pred_text,orig_text,do_lower_case,verbose_logging=False)
transformers.data.metrics.squad_metrics.get_raw_scores(examples,preds)
transformers.data.metrics.squad_metrics.get_tokens(s)
transformers.data.metrics.squad_metrics.make_eval_dict(exact_scores,f1_scores,qid_list=None)
transformers.data.metrics.squad_metrics.merge_eval(main_eval,new_eval,prefix)
transformers.data.metrics.squad_metrics.normalize_answer(s)
transformers.data.metrics.squad_metrics.squad_evaluate(examples,preds,no_answer_probs=None,no_answer_probability_threshold=1.0)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/datasets/glue.py----------------------------------------
A:transformers.data.datasets.glue.logger->utils.logging.get_logger(__name__)
A:transformers.data.datasets.glue.self.task_name->self.task_name.lower()
A:transformers.data.datasets.glue.self.processor->glue_processors[args.task_name]()
A:transformers.data.datasets.glue.cached_features_file->os.path.join(cache_dir if cache_dir is not None else args.data_dir, f'cached_{mode.value}_{tokenizer.__class__.__name__}_{args.max_seq_length}_{args.task_name}')
A:transformers.data.datasets.glue.label_list->self.processor.get_labels()
A:transformers.data.datasets.glue.start->time.time()
A:transformers.data.datasets.glue.self.features->glue_convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=self.output_mode)
A:transformers.data.datasets.glue.examples->self.processor.get_train_examples(args.data_dir)
transformers.GlueDataTrainingArguments
transformers.GlueDataTrainingArguments.__post_init__(self)
transformers.GlueDataset(self,args:GlueDataTrainingArguments,tokenizer:PreTrainedTokenizerBase,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,cache_dir:Optional[str]=None)
transformers.GlueDataset.__getitem__(self,i)->InputFeatures
transformers.GlueDataset.__len__(self)
transformers.GlueDataset.get_labels(self)
transformers.data.datasets.glue.GlueDataTrainingArguments
transformers.data.datasets.glue.GlueDataTrainingArguments.__post_init__(self)
transformers.data.datasets.glue.GlueDataset(self,args:GlueDataTrainingArguments,tokenizer:PreTrainedTokenizerBase,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,cache_dir:Optional[str]=None)
transformers.data.datasets.glue.GlueDataset.__getitem__(self,i)->InputFeatures
transformers.data.datasets.glue.GlueDataset.__init__(self,args:GlueDataTrainingArguments,tokenizer:PreTrainedTokenizerBase,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,cache_dir:Optional[str]=None)
transformers.data.datasets.glue.GlueDataset.__len__(self)
transformers.data.datasets.glue.GlueDataset.get_labels(self)
transformers.data.datasets.glue.Split(Enum)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/datasets/language_modeling.py----------------------------------------
A:transformers.data.datasets.language_modeling.logger->utils.logging.get_logger(__name__)
A:transformers.data.datasets.language_modeling.(directory, filename)->os.path.split(file_path)
A:transformers.data.datasets.language_modeling.cached_features_file->os.path.join(directory, f'cached_nsp_{tokenizer.__class__.__name__}_{block_size}_{filename}')
A:transformers.data.datasets.language_modeling.start->time.time()
A:transformers.data.datasets.language_modeling.self.examples->pickle.load(handle)
A:transformers.data.datasets.language_modeling.text->f.read()
A:transformers.data.datasets.language_modeling.tokenized_text->tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))
A:transformers.data.datasets.language_modeling.batch_encoding->tokenizer(data, add_special_tokens=True, truncation=True, max_length=block_size)
A:transformers.data.datasets.language_modeling.data->f.readlines()
A:transformers.data.datasets.language_modeling.n->len(self.examples)
A:transformers.data.datasets.language_modeling.self.examples[i]['chinese_ref']->torch.tensor(ref[i], dtype=torch.long)
A:transformers.data.datasets.language_modeling.file_path->os.path.join(file_dir, file_name)
A:transformers.data.datasets.language_modeling.original_lines->f.readlines()
A:transformers.data.datasets.language_modeling.examples->self.create_examples_from_document(document, block_size, tokenizer)
A:transformers.data.datasets.language_modeling.target_seq_length->random.randint(2, max_num_tokens)
A:transformers.data.datasets.language_modeling.a_end->random.randint(1, len(current_chunk) - 1)
A:transformers.data.datasets.language_modeling.input_ids->self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)
A:transformers.data.datasets.language_modeling.token_type_ids->self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)
A:transformers.data.datasets.language_modeling.line->line.strip().strip()
A:transformers.data.datasets.language_modeling.tokens->tokenizer.convert_tokens_to_ids(tokens)
A:transformers.data.datasets.language_modeling.random_document_index->random.randint(0, len(self.documents) - 1)
A:transformers.data.datasets.language_modeling.random_start->random.randint(0, len(random_document) - 1)
transformers.LineByLineTextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int)
transformers.LineByLineTextDataset.__getitem__(self,i)->Dict[str, torch.tensor]
transformers.LineByLineTextDataset.__len__(self)
transformers.LineByLineWithRefDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,ref_path:str)
transformers.LineByLineWithRefDataset.__getitem__(self,i)->Dict[str, torch.tensor]
transformers.LineByLineWithRefDataset.__len__(self)
transformers.LineByLineWithSOPTextDataset(self,tokenizer:PreTrainedTokenizer,file_dir:str,block_size:int)
transformers.LineByLineWithSOPTextDataset.__getitem__(self,i)->Dict[str, torch.tensor]
transformers.LineByLineWithSOPTextDataset.__len__(self)
transformers.LineByLineWithSOPTextDataset.create_examples_from_document(self,document,block_size,tokenizer,short_seq_prob=0.1)
transformers.TextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False,cache_dir:Optional[str]=None)
transformers.TextDataset.__getitem__(self,i)->torch.Tensor
transformers.TextDataset.__len__(self)
transformers.TextDatasetForNextSentencePrediction(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False,short_seq_probability=0.1,nsp_probability=0.5)
transformers.TextDatasetForNextSentencePrediction.__getitem__(self,i)
transformers.TextDatasetForNextSentencePrediction.__len__(self)
transformers.TextDatasetForNextSentencePrediction.create_examples_from_document(self,document:List[List[int]],doc_index:int,block_size:int)
transformers.data.datasets.language_modeling.LineByLineTextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int)
transformers.data.datasets.language_modeling.LineByLineTextDataset.__getitem__(self,i)->Dict[str, torch.tensor]
transformers.data.datasets.language_modeling.LineByLineTextDataset.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int)
transformers.data.datasets.language_modeling.LineByLineTextDataset.__len__(self)
transformers.data.datasets.language_modeling.LineByLineWithRefDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,ref_path:str)
transformers.data.datasets.language_modeling.LineByLineWithRefDataset.__getitem__(self,i)->Dict[str, torch.tensor]
transformers.data.datasets.language_modeling.LineByLineWithRefDataset.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,ref_path:str)
transformers.data.datasets.language_modeling.LineByLineWithRefDataset.__len__(self)
transformers.data.datasets.language_modeling.LineByLineWithSOPTextDataset(self,tokenizer:PreTrainedTokenizer,file_dir:str,block_size:int)
transformers.data.datasets.language_modeling.LineByLineWithSOPTextDataset.__getitem__(self,i)->Dict[str, torch.tensor]
transformers.data.datasets.language_modeling.LineByLineWithSOPTextDataset.__init__(self,tokenizer:PreTrainedTokenizer,file_dir:str,block_size:int)
transformers.data.datasets.language_modeling.LineByLineWithSOPTextDataset.__len__(self)
transformers.data.datasets.language_modeling.LineByLineWithSOPTextDataset.create_examples_from_document(self,document,block_size,tokenizer,short_seq_prob=0.1)
transformers.data.datasets.language_modeling.TextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False,cache_dir:Optional[str]=None)
transformers.data.datasets.language_modeling.TextDataset.__getitem__(self,i)->torch.Tensor
transformers.data.datasets.language_modeling.TextDataset.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False,cache_dir:Optional[str]=None)
transformers.data.datasets.language_modeling.TextDataset.__len__(self)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False,short_seq_probability=0.1,nsp_probability=0.5)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.__getitem__(self,i)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False,short_seq_probability=0.1,nsp_probability=0.5)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.__len__(self)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.create_examples_from_document(self,document:List[List[int]],doc_index:int,block_size:int)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/datasets/squad.py----------------------------------------
A:transformers.data.datasets.squad.logger->utils.logging.get_logger(__name__)
A:transformers.data.datasets.squad.MODEL_CONFIG_CLASSES->list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())
A:transformers.data.datasets.squad.MODEL_TYPES->tuple((conf.model_type for conf in MODEL_CONFIG_CLASSES))
A:transformers.data.datasets.squad.cached_features_file->os.path.join(cache_dir if cache_dir is not None else args.data_dir, f'cached_{mode.value}_{tokenizer.__class__.__name__}_{args.max_seq_length}_{version_tag}')
A:transformers.data.datasets.squad.start->time.time()
A:transformers.data.datasets.squad.self.old_features->torch.load(cached_features_file)
A:transformers.data.datasets.squad.self.dataset->self.old_features.get('dataset', None)
A:transformers.data.datasets.squad.self.examples->self.processor.get_train_examples(args.data_dir)
A:transformers.data.datasets.squad.(self.features, self.dataset)->squad_convert_examples_to_features(examples=self.examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=mode == Split.train, threads=args.threads, return_dataset=dataset_format)
A:transformers.data.datasets.squad.input_ids->torch.tensor(feature.input_ids, dtype=torch.long)
A:transformers.data.datasets.squad.attention_mask->torch.tensor(feature.attention_mask, dtype=torch.long)
A:transformers.data.datasets.squad.token_type_ids->torch.tensor(feature.token_type_ids, dtype=torch.long)
A:transformers.data.datasets.squad.cls_index->torch.tensor(feature.cls_index, dtype=torch.long)
A:transformers.data.datasets.squad.p_mask->torch.tensor(feature.p_mask, dtype=torch.float)
A:transformers.data.datasets.squad.is_impossible->torch.tensor(feature.is_impossible, dtype=torch.float)
A:transformers.data.datasets.squad.start_positions->torch.tensor(feature.start_position, dtype=torch.long)
A:transformers.data.datasets.squad.end_positions->torch.tensor(feature.end_position, dtype=torch.long)
transformers.SquadDataTrainingArguments
transformers.SquadDataset(self,args:SquadDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,is_language_sensitive:Optional[bool]=False,cache_dir:Optional[str]=None,dataset_format:Optional[str]='pt')
transformers.SquadDataset.__getitem__(self,i)->Dict[str, torch.Tensor]
transformers.SquadDataset.__len__(self)
transformers.data.datasets.squad.Split(Enum)
transformers.data.datasets.squad.SquadDataTrainingArguments
transformers.data.datasets.squad.SquadDataset(self,args:SquadDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,is_language_sensitive:Optional[bool]=False,cache_dir:Optional[str]=None,dataset_format:Optional[str]='pt')
transformers.data.datasets.squad.SquadDataset.__getitem__(self,i)->Dict[str, torch.Tensor]
transformers.data.datasets.squad.SquadDataset.__init__(self,args:SquadDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,is_language_sensitive:Optional[bool]=False,cache_dir:Optional[str]=None,dataset_format:Optional[str]='pt')
transformers.data.datasets.squad.SquadDataset.__len__(self)


----------------------------------------/home/zhang/Packages/transformers/transformers4.8.2/data/datasets/__init__.py----------------------------------------

